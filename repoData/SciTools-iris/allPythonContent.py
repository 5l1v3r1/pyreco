__FILENAME__ = anomaly_log_colouring
"""
Colouring anomaly data with logarithmic scaling
===============================================

In this example, we need to plot anomaly data where the values have a
"logarithmic" significance  -- i.e. we want to give approximately equal ranges
of colour between data values of, say, 1 and 10 as between 10 and 100.

As the data range also contains zero, that obviously does not suit a simple
logarithmic interpretation.  However, values of less than a certain absolute
magnitude may be considered "not significant", so we put these into a separate
"zero band" which is plotted in white.

To do this, we create a custom value mapping function (normalization) using
the matplotlib Norm class `matplotlib.colours.SymLogNorm
<http://matplotlib.org/api/colors_api.html#matplotlib.colors.SymLogNorm>`_.
We use this to make a cell-filled pseudocolour plot with a colorbar.

NOTE: By "pseudocolour", we mean that each data point is drawn as a "cell"
region on the plot, coloured according to its data value.
This is provided in Iris by the functions :meth:`iris.plot.pcolor` and
:meth:`iris.plot.pcolormesh`, which call the underlying matplotlib
functions of the same names (i.e. `matplotlib.pyplot.pcolor
<http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.pcolor>`_
and  `matplotlib.pyplot.pcolormesh
<http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.pcolormesh>`_).
See also: http://en.wikipedia.org/wiki/False_color#Pseudocolor.

"""
import cartopy.crs as ccrs
import iris
import iris.coord_categorisation
import iris.plot as iplt
import matplotlib.pyplot as plt
import matplotlib.colors as mcols
import matplotlib.ticker as mticks


def main():
    # Load a sample air temperatures sequence.
    file_path = iris.sample_data_path('E1_north_america.nc')
    temperatures = iris.load_cube(file_path)

    # Create a year-number coordinate from the time information.
    iris.coord_categorisation.add_year(temperatures, 'time')

    # Create a sample anomaly field for one chosen year, by extracting that
    # year and subtracting the time mean.
    sample_year = 1982
    year_temperature = temperatures.extract(iris.Constraint(year=sample_year))
    time_mean = temperatures.collapsed('time', iris.analysis.MEAN)
    anomaly = year_temperature - time_mean

    # Construct a plot title string explaining which years are involved.
    years = temperatures.coord('year').points
    plot_title = 'Temperature anomaly'
    plot_title += '\n{} differences from {}-{} average.'.format(
        sample_year, years[0], years[-1])

    # Define scaling levels for the logarithmic colouring.
    minimum_log_level = 0.1
    maximum_scale_level = 3.0

    # Use a standard colour map which varies blue-white-red.
    # For suitable options, see the 'Diverging colormaps' section in:
    # http://matplotlib.org/examples/color/colormaps_reference.html
    anom_cmap = 'bwr'

    # Create a 'logarithmic' data normalization.
    anom_norm = mcols.SymLogNorm(linthresh=minimum_log_level,
                                 linscale=0,
                                 vmin=-maximum_scale_level,
                                 vmax=maximum_scale_level)
    # Setting "linthresh=minimum_log_level" makes its non-logarithmic
    # data range equal to our 'zero band'.
    # Setting "linscale=0" maps the whole zero band to the middle colour value
    # (i.e. 0.5), which is the neutral point of a "diverging" style colormap.

    # Create an Axes, specifying the map projection.
    plt.axes(projection=ccrs.LambertConformal())

    # Make a pseudocolour plot using this colour scheme.
    mesh = iplt.pcolormesh(anomaly, cmap=anom_cmap, norm=anom_norm)

    # Add a colourbar, with extensions to show handling of out-of-range values.
    bar = plt.colorbar(mesh, orientation='horizontal', extend='both')

    # Set some suitable fixed "logarithmic" colourbar tick positions.
    tick_levels = [-3, -1, -0.3, 0.0, 0.3, 1, 3]
    bar.set_ticks(tick_levels)

    # Modify the tick labels so that the centre one shows "+/-<minumum-level>".
    tick_levels[3] = r'$\pm${:g}'.format(minimum_log_level)
    bar.set_ticklabels(tick_levels)

    # Label the colourbar to show the units.
    bar.set_label('[{}, log scale]'.format(anomaly.units))

    # Add coastlines and a title.
    plt.gca().coastlines()
    plt.title(plot_title)

    # Display the result.
    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = atlantic_profiles
"""
Oceanographic profiles and T-S diagrams
=======================================

This example demonstrates how to plot vertical profiles of different
variables in the same axes, and how to make a scatter plot of two
variables. There is an oceanographic theme but the same techniques are
equally applicable to atmospheric or other kinds of data.

The data used are profiles of potential temperature and salinity in the
Equatorial and South Atlantic, output from an ocean model.

The y-axis of the first plot produced will be automatically inverted due to the
presence of the attribute positive=down on the depth coordinate. This means
depth values intuitively increase downward on the y-axis.

"""
import iris
import iris.iterate
import iris.plot as iplt
import matplotlib.pyplot as plt


def main():

    # Load the gridded temperature and salinity data.
    fname = iris.sample_data_path('atlantic_profiles.nc')
    cubes = iris.load(fname)
    theta, = cubes.extract('sea_water_potential_temperature')
    salinity, = cubes.extract('sea_water_practical_salinity')

    # Extract profiles of temperature and salinity from a particular point in
    # the southern portion of the domain, and limit the depth of the profile
    # to 1000m.
    lon_cons = iris.Constraint(longitude=330.5)
    lat_cons = iris.Constraint(latitude=lambda l: -10 < l < -9)
    depth_cons = iris.Constraint(depth=lambda d: d <= 1000)
    theta_1000m = theta.extract(depth_cons & lon_cons & lat_cons)
    salinity_1000m = salinity.extract(depth_cons & lon_cons & lat_cons)

    # Plot these profiles on the same set of axes. In each case we call plot
    # with two arguments, the cube followed by the depth coordinate. Putting
    # them in this order places the depth coordinate on the y-axis.
    # The first plot is in the default axes. We'll use the same color for the
    # curve and its axes/tick labels.
    fig = plt.figure(figsize=(5, 6))
    temperature_color = (.3, .4, .5)
    ax1 = plt.gca()
    iplt.plot(theta_1000m, theta_1000m.coord('depth'), linewidth=2,
              color=temperature_color, alpha=.75)
    ax1.set_xlabel('Potential Temperature / K', color=temperature_color)
    ax1.set_ylabel('Depth / m')
    for ticklabel in ax1.get_xticklabels():
        ticklabel.set_color(temperature_color)
    # To plot salinity in the same axes we use twiny(). We'll use a different
    # color to identify salinity.
    salinity_color = (.6, .1, .15)
    ax2 = plt.gca().twiny()
    iplt.plot(salinity_1000m, salinity_1000m.coord('depth'), linewidth=2,
              color=salinity_color, alpha=.75)
    ax2.set_xlabel('Salinity / PSU', color=salinity_color)
    for ticklabel in ax2.get_xticklabels():
        ticklabel.set_color(salinity_color)
    plt.tight_layout()
    plt.show()

    # Now plot a T-S diagram using scatter. We'll use all the profiles here,
    # and each point will be coloured according to its depth.
    plt.figure(figsize=(6, 6))
    depth_values = theta.coord('depth').points
    for s, t in iris.iterate.izip(salinity, theta, coords='depth'):
        iplt.scatter(s, t, c=depth_values, marker='+', cmap='RdYlBu_r')
    ax = plt.gca()
    ax.set_xlabel('Salinity / PSU')
    ax.set_ylabel('Potential Temperature / K')
    cb = plt.colorbar(orientation='horizontal')
    cb.set_label('Depth / m')
    plt.tight_layout()
    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = COP_1d_plot
"""
Global average annual temperature plot
======================================

Produces a time-series plot of North American temperature forecasts for 2 different emission scenarios.
Constraining data to a limited spatial area also features in this example.

The data used comes from the HadGEM2-AO model simulations for the A1B and E1 scenarios, both of which
were derived using the IMAGE Integrated Assessment Model (Johns et al. 2010; Lowe et al. 2009).

References
----------

   Johns T.C., et al. (2010) Climate change under aggressive mitigation: The ENSEMBLES multi-model
   experiment. Climate Dynamics (submitted)

   Lowe J.A., C.D. Hewitt, D.P. Van Vuuren, T.C. Johns, E. Stehfest, J-F. Royer, and P. van der Linden, 2009.
   New Study For Climate Modeling, Analyses, and Scenarios. Eos Trans. AGU, Vol 90, No. 21.

.. seealso::

    Further details on the aggregation functionality being used in this example can be found in
    :ref:`cube-statistics`.

"""
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.plot as iplt
import iris.quickplot as qplt

import iris.analysis.cartography
import matplotlib.dates as mdates


def main():
    # Load data into three Cubes, one for each set of NetCDF files.
    e1 = iris.load_cube(iris.sample_data_path('E1_north_america.nc'))

    a1b = iris.load_cube(iris.sample_data_path('A1B_north_america.nc'))

    # load in the global pre-industrial mean temperature, and limit the domain
    # to the same North American region that e1 and a1b are at.
    north_america = iris.Constraint(longitude=lambda v: 225 <= v <= 315,
                                    latitude=lambda v: 15 <= v <= 60)
    pre_industrial = iris.load_cube(iris.sample_data_path('pre-industrial.pp'),
                                    north_america)

    # Generate area-weights array. As e1 and a1b are on the same grid we can
    # do this just once and re-use. This method requires bounds on lat/lon
    # coords, so let's add some in sensible locations using the "guess_bounds"
    # method.
    e1.coord('latitude').guess_bounds()
    e1.coord('longitude').guess_bounds()
    e1_grid_areas = iris.analysis.cartography.area_weights(e1)
    pre_industrial.coord('latitude').guess_bounds()
    pre_industrial.coord('longitude').guess_bounds()
    pre_grid_areas = iris.analysis.cartography.area_weights(pre_industrial)

    # Perform the area-weighted mean for each of the datasets using the
    # computed grid-box areas.
    pre_industrial_mean = pre_industrial.collapsed(['latitude', 'longitude'],
                                                   iris.analysis.MEAN,
                                                   weights=pre_grid_areas)
    e1_mean = e1.collapsed(['latitude', 'longitude'],
                           iris.analysis.MEAN,
                           weights=e1_grid_areas)
    a1b_mean = a1b.collapsed(['latitude', 'longitude'],
                             iris.analysis.MEAN,
                             weights=e1_grid_areas)

    # Show ticks 30 years apart
    plt.gca().xaxis.set_major_locator(mdates.YearLocator(30))

    # Plot the datasets
    qplt.plot(e1_mean, label='E1 scenario', lw=1.5, color='blue')
    qplt.plot(a1b_mean, label='A1B-Image scenario', lw=1.5, color='red')

    # Draw a horizontal line showing the pre-industrial mean
    plt.axhline(y=pre_industrial_mean.data, color='gray', linestyle='dashed',
                label='pre-industrial', lw=1.5)

    # Establish where r and t have the same data, i.e. the observations
    common = np.where(a1b_mean.data == e1_mean.data)[0]
    observed = a1b_mean[common]

    # Plot the observed data
    qplt.plot(observed, label='observed', color='black', lw=1.5)

    # Add a legend and title
    plt.legend(loc="upper left")
    plt.title('North American mean air temperature', fontsize=18)

    plt.xlabel('Time / year')

    plt.grid()

    iplt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = COP_maps
"""
Global average annual temperature maps
======================================

Produces maps of global temperature forecasts from the A1B and E1 scenarios.

The data used comes from the HadGEM2-AO model simulations for the A1B and E1 scenarios, both of which were derived using the IMAGE
Integrated Assessment Model (Johns et al. 2010; Lowe et al. 2009).

References
----------

    Johns T.C., et al. (2010) Climate change under aggressive mitigation: The ENSEMBLES multi-model experiment. Climate
    Dynamics (submitted)

    Lowe J.A., C.D. Hewitt, D.P. Van Vuuren, T.C. Johns, E. Stehfest, J-F. Royer, and P. van der Linden, 2009. New
    Study For Climate Modeling, Analyses, and Scenarios. Eos Trans. AGU, Vol 90, No. 21.


"""
import os.path
import itertools
import matplotlib.pyplot as plt
import numpy as np

import iris
import iris.coords as coords
import iris.plot as iplt


def cop_metadata_callback(cube, field, filename):
    """ A function which adds an "Experiment" coordinate which comes from the filename. """

    # Extract the experiment name (such as a1b or e1) from the filename (in this case it is just the parent folder's name)
    containing_folder = os.path.dirname(filename)
    experiment_label = os.path.basename(containing_folder)

    # Create a coordinate with the experiment label in it
    exp_coord = coords.AuxCoord(experiment_label, long_name='Experiment', units='no_unit')

    # and add it to the cube
    cube.add_aux_coord(exp_coord)


def main():
    # Load e1 and a1 using the callback to update the metadata
    e1 = iris.load_cube(iris.sample_data_path('E1.2098.pp'),
                        callback=cop_metadata_callback)
    a1b = iris.load_cube(iris.sample_data_path('A1B.2098.pp'),
                         callback=cop_metadata_callback)

    # Load the global average data and add an 'Experiment' coord it
    global_avg = iris.load_cube(iris.sample_data_path('pre-industrial.pp'))

    # Define evenly spaced contour levels: -2.5, -1.5, ... 15.5, 16.5 with the specific colours
    levels = np.arange(20) - 2.5
    red = np.array([0, 0, 221, 239, 229, 217, 239, 234, 228, 222, 205, 196, 161, 137, 116, 89, 77, 60, 51]) / 256.
    green = np.array([16, 217, 242, 243, 235, 225, 190, 160, 128, 87, 72, 59, 33, 21, 29, 30, 30, 29, 26]) / 256.
    blue = np.array([255, 255, 243, 169, 99, 51, 63, 37, 39, 21, 27, 23, 22, 26, 29, 28, 27, 25, 22]) / 256.

    # Put those colours into an array which can be passed to conourf as the specific colours for each level
    colors = np.array([red, green, blue]).T

    # Subtract the global


    # Iterate over each latitude longitude slice for both e1 and a1b scenarios simultaneously
    for e1_slice, a1b_slice in itertools.izip(e1.slices(['latitude', 'longitude']), a1b.slices(['latitude', 'longitude'])):

        time_coord = a1b_slice.coord('time')

        # Calculate the difference from the mean
        delta_e1 = e1_slice - global_avg
        delta_a1b = a1b_slice - global_avg

        # Make a wider than normal figure to house two maps side-by-side
        fig = plt.figure(figsize=(12, 5))

        # Get the time datetime from the coordinate
        time = time_coord.units.num2date(time_coord.points[0])
        # Set a title for the entire figure, giving the time in a nice format of "MonthName Year". Also, set the y value for the
        # title so that it is not tight to the top of the plot.
        fig.suptitle('Annual Temperature Predictions for ' + time.strftime("%Y"), y=0.9, fontsize=18)

        # Add the first subplot showing the E1 scenario
        plt.subplot(121)
        plt.title('HadGEM2 E1 Scenario',  fontsize=10)
        iplt.contourf(delta_e1, levels, colors=colors, linewidth=0, extend='both')
        plt.gca().coastlines()
        # get the current axes' subplot for use later on
        plt1_ax = plt.gca()

        # Add the second subplot showing the A1B scenario
        plt.subplot(122)
        plt.title('HadGEM2 A1B-Image Scenario',  fontsize=10)
        contour_result = iplt.contourf(delta_a1b, levels, colors=colors, linewidth=0, extend='both')
        plt.gca().coastlines()
        # get the current axes' subplot for use later on
        plt2_ax = plt.gca()


        # Now add a colourbar who's leftmost point is the same as the leftmost point of the left hand plot
        # and rightmost point is the rightmost point of the right hand plot

        # Get the positions of the 2nd plot and the left position of the 1st plot
        left, bottom, width, height = plt2_ax.get_position().bounds
        first_plot_left = plt1_ax.get_position().bounds[0]

        # the width of the colorbar should now be simple
        width = left - first_plot_left + width

        # Add axes to the figure, to place the colour bar
        colorbar_axes = fig.add_axes([first_plot_left, bottom + 0.07, width, 0.03])

        # Add the colour bar
        cbar = plt.colorbar(contour_result, colorbar_axes, orientation='horizontal')

        # Label the colour bar and add ticks
        cbar.set_label(e1_slice.units)
        cbar.ax.tick_params(length=0)

        plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = cross_section
"""
Cross section plots
===================

This example demonstrates contour plots of a cross-sectioned multi-dimensional cube which features
a hybrid height vertical coordinate system.

"""

import matplotlib.pyplot as plt

import iris
import iris.quickplot as qplt


def main():
    fname = iris.sample_data_path('hybrid_height.nc')
    theta = iris.load_cube(fname)

    # Extract a single height vs longitude cross-section. N.B. This could easily be changed to
    # extract a specific slice, or even to loop over *all* cross section slices.
    cross_section = theta.slices(['grid_longitude', 'model_level_number']).next()

    qplt.contourf(cross_section, coords=['grid_longitude', 'altitude'])
    plt.show()

    # Now do the equivalent plot, only against model level
    plt.figure()

    qplt.contourf(cross_section, coords=['grid_longitude', 'model_level_number'])
    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = custom_aggregation
"""
Calculating a custom statistic
==============================

This example shows how to define and use a custom
:class:`iris.analysis.Aggregator`, that provides a new statistical operator for
use with cube aggregation functions such as :meth:`~iris.cube.Cube.collapsed`,
:meth:`~iris.cube.Cube.aggregated_by` or
:meth:`~iris.cube.Cube.rolling_window`.

In this case, we have a 240-year sequence of yearly average surface temperature
over North America, and we want to calculate in how many years these exceed a
certain temperature over a spell of 5 years or more.

"""
import matplotlib.pyplot as plt
import numpy as np

import iris
from iris.analysis import Aggregator
import iris.quickplot as qplt
from iris.util import rolling_window


# Define a function to perform the custom statistical operation.
# Note: in order to meet the requirements of iris.analysis.Aggregator, it must
# do the calculation over an arbitrary (given) data axis.
def count_spells(data, threshold, axis, spell_length):
    """
    Function to calculate the number of points in a sequence where the value
    has exceeded a threshold value for at least a certain number of timepoints.

    Generalised to operate on multiple time sequences arranged on a specific
    axis of a multidimensional array.

    Args:

    * data (array):
        raw data to be compared with value threshold.

    * threshold (float):
        threshold point for 'significant' datapoints.

    * axis (int):
        number of the array dimension mapping the time sequences.
        (Can also be negative, e.g. '-1' means last dimension)

    * spell_length (int):
        number of consecutive times at which value > threshold to "count".

    """
    if axis < 0:
        # just cope with negative axis numbers
        axis += data.ndim
    # Threshold the data to find the 'significant' points.
    data_hits = data > threshold
    # Make an array with data values "windowed" along the time axis.
    hit_windows = rolling_window(data_hits, window=spell_length, axis=axis)
    # Find the windows "full of True-s" (along the added 'window axis').
    full_windows = np.all(hit_windows, axis=axis+1)
    # Count points fulfilling the condition (along the time axis).
    spell_point_counts = np.sum(full_windows, axis=axis, dtype=int)
    return spell_point_counts


def main():
    # Load the whole time-sequence as a single cube.
    file_path = iris.sample_data_path('E1_north_america.nc')
    cube = iris.load_cube(file_path)

    # Make an aggregator from the user function.
    SPELL_COUNT = Aggregator('spell_count',
                             count_spells,
                             units_func=lambda units: 1)

    # Define the parameters of the test.
    threshold_temperature = 280.0
    spell_years = 5

    # Calculate the statistic.
    warm_periods = cube.collapsed('time', SPELL_COUNT,
                                  threshold=threshold_temperature,
                                  spell_length=spell_years)
    warm_periods.rename('Number of 5-year warm spells in 240 years')

    # Plot the results.
    qplt.contourf(warm_periods, cmap='RdYlBu_r')
    plt.gca().coastlines()
    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = custom_file_loading
"""
Loading a cube from a custom file format
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This example shows how a custom text file can be loaded using the standard Iris load mechanism.

The first stage in the process is to define an Iris :class:`FormatSpecification <iris.io.format_picker.FormatSpecification>` for the file format.
To create a format specification we need to define the following:

* format_name - Some text that describes the format specification we are creating
* file_element - FileElement object describing the element which identifies
                 this FormatSpecification.
    Possible values are:
    
    ``iris.io.format_picker.MagicNumber(n, o)`` - The n bytes from the file \
    at offset o.
    
    ``iris.io.format_picker.FileExtension()`` - The file's extension.
    
    ``iris.io.format_picker.LeadingLine()``  - The first line of the file.

* file_element_value - The value that the file_element should take if a file matches this FormatSpecification
* handler (optional) - A generator function that will be called when the file specification has been identified. This function is
  provided by the user and provides the means to parse the whole file. If no handler function is provided, then identification
  is still possible without any handling.

  The handler function must define the following arguments:

  * list of filenames to process
  * callback function - An optional function to filter/alter the Iris cubes returned

  The handler function must be defined as generator which yields each cube as they are produced.

* priority (optional) - Integer giving a priority for considering this specification where higher priority means sooner consideration

In the following example, the function :func:`load_NAME_III` has been defined to handle the loading of the raw data from the custom file format.
This function is called from :func:`NAME_to_cube` which uses this data to create and yield Iris cubes.

In the ``main()`` function the filenames are loaded via the ``iris.load_cube`` function which automatically
invokes the ``FormatSpecification`` we defined. The cube returned from the load function is then used to produce a plot.

"""
import datetime

import matplotlib.pyplot as plt
import numpy as np

import iris
import iris.coords as icoords
import iris.coord_systems as icoord_systems
import iris.fileformats
import iris.io.format_picker as format_picker
import iris.plot as iplt


UTC_format = '%H%M%Z %d/%m/%Y'


def load_NAME_III(filename):
    """
    Loads the Met Office's NAME III grid output files returning headers, column definitions and data arrays as 3 separate lists.

    """

    # loading a file gives a generator of lines which can be progressed using the next() method.
    # This will come in handy as we wish to progress through the file line by line.
    file_handle = file(filename)

    # define a dictionary which can hold the header metadata about this file
    headers = {}

    # skip the NAME header of the file which looks something like 'NAME III (version X.X.X)'
    file_handle.next()

    # read the next 16 lines of header information, putting the form "header name:    header value" into a dictionary
    for _ in range(16):
        header_name, header_value = file_handle.next().split(':')

        # strip off any spurious space characters in the header name and value
        header_name = header_name.strip()
        header_value = header_value.strip()

        # cast some headers into floats or integers if they match a given header name
        if header_name in ['X grid origin', 'Y grid origin', 'X grid resolution', 'Y grid resolution']:
            header_value = float(header_value)
        elif header_name in ['X grid size', 'Y grid size', 'Number of fields']:
            header_value = int(header_value)
        elif header_name in ['Run time', 'Start of release', 'End of release']:
            # convert the time to python datetimes
            header_value = datetime.datetime.strptime(header_value, UTC_format)

        headers[header_name] = header_value

    # skip the next blank line in the file.
    file_handle.next()

    # Read the next 7 lines of column definitions
    column_headings = {}
    for column_header_name in ['species_category', 'species', 'cell_measure', 'quantity', 'unit', 'z_level', 'time']:
        column_headings[column_header_name] = [col.strip() for col in file_handle.next().split(',')][:-1]

    # convert the time to python datetimes
    new_time_column_header = []
    for i, t in enumerate(column_headings['time']):
        # the first 4 columns aren't time at all, so don't convert them to datetimes
        if i >= 4:
            new_time_column_header.append(datetime.datetime.strptime(t, UTC_format))
        else:
            new_time_column_header.append(t)
    column_headings['time'] = new_time_column_header

    # skip the blank line after the column headers
    file_handle.next()

    # make a list of data arrays to hold the data for each column
    data_shape = (headers['Y grid size'], headers['X grid size'])
    data_arrays = [np.zeros(data_shape, dtype=np.float32) for i in range(headers['Number of fields'])]

    # iterate over the remaining lines which represent the data in a column form
    for line in file_handle:

        # split the line by comma, removing the last empty column caused by the trailing comma
        vals = line.split(',')[:-1]

        # cast the x and y grid positions to floats and convert them to zero based indices
        # (the numbers are 1 based grid positions where 0.5 represents half a grid point.)
        x = float(vals[0]) - 1.5
        y = float(vals[1]) - 1.5

        # populate the data arrays (i.e. all columns but the leading 4)
        for i, data_array in enumerate(data_arrays):
            data_array[y, x] = float(vals[i + 4])

    return headers, column_headings, data_arrays


def NAME_to_cube(filenames, callback):
    """Returns a generator of cubes given a list of filenames and a callback."""

    for filename in filenames:
        header, column_headings, data_arrays = load_NAME_III(filename)

        for i, data_array in enumerate(data_arrays):
            # turn the dictionary of column headers with a list of header information for each field into a dictionary of
            # headers for just this field. Ignore the first 4 columns of grid position (data was located with the data array).
            field_headings = dict([(k, v[i + 4]) for k, v in column_headings.iteritems()])

            # make an cube
            cube = iris.cube.Cube(data_array)

            # define the name and unit
            name = ('%s %s' % (field_headings['species'], field_headings['quantity'])).upper().replace(' ', '_')
            cube.rename(name)
            # Some units are badly encoded in the file, fix this by putting a space in between. (if gs is not found, then the
            # string will be returned unchanged)
            cube.units = field_headings['unit'].replace('gs', 'g s')

            # define and add the singular coordinates of the field (flight level, time etc.)
            cube.add_aux_coord(icoords.AuxCoord(field_headings['z_level'], long_name='flight_level', units='1'))

            # define the time unit and use it to serialise the datetime for the time coordinate
            time_unit = iris.unit.Unit('hours since epoch', calendar=iris.unit.CALENDAR_GREGORIAN)
            time_coord = icoords.AuxCoord(time_unit.date2num(field_headings['time']), standard_name='time', units=time_unit)
            cube.add_aux_coord(time_coord)

            # build a coordinate system which can be referenced by latitude and longitude coordinates
            lat_lon_coord_system = icoord_systems.GeogCS(6371229)

            # build regular latitude and longitude coordinates which have bounds
            start = header['X grid origin'] + header['X grid resolution']
            step = header['X grid resolution']
            count = header['X grid size']
            pts = start + np.arange(count, dtype=np.float32) * step
            lon_coord = icoords.DimCoord(pts, standard_name='longitude', units='degrees', coord_system=lat_lon_coord_system)
            lon_coord.guess_bounds()

            start = header['Y grid origin'] + header['Y grid resolution']
            step = header['Y grid resolution']
            count = header['Y grid size']
            pts = start + np.arange(count, dtype=np.float32) * step
            lat_coord = icoords.DimCoord(pts, standard_name='latitude', units='degrees', coord_system=lat_lon_coord_system)
            lat_coord.guess_bounds()

            # add the latitude and longitude coordinates to the cube, with mappings to data dimensions
            cube.add_dim_coord(lat_coord, 0)
            cube.add_dim_coord(lon_coord, 1)

            # implement standard iris callback capability. Although callbacks are not used in this example, the standard
            # mechanism for a custom loader to implement a callback is shown:
            cube = iris.io.run_callback(callback, cube, [header, field_headings, data_array], filename)

            # yield the cube created (the loop will continue when the next() element is requested)
            yield cube


# Create a format_picker specification of the NAME file format giving it a
# priority greater than the built in NAME loader.
_NAME_III_spec = format_picker.FormatSpecification('Name III', format_picker.LeadingLine(),
                                      lambda line: line.startswith("NAME III"), NAME_to_cube,
                                      priority=6)

# Register the NAME loader with iris
iris.fileformats.FORMAT_AGENT.add_spec(_NAME_III_spec)



# ---------------------------------------------
# |          Using the new loader             |
# ---------------------------------------------

def main():
    fname = iris.sample_data_path('NAME_output.txt')

    boundary_volc_ash_constraint = iris.Constraint('VOLCANIC_ASH_AIR_CONCENTRATION', flight_level='From FL000 - FL200')

    # Callback shown as None to illustrate where a cube-level callback function would be used if required
    cube = iris.load_cube(fname, boundary_volc_ash_constraint, callback=None)

    # draw contour levels for the data (the top level is just a catch-all)
    levels = (0.0002, 0.002, 0.004, 1e10)
    cs = iplt.contourf(cube, levels=levels,
                        colors=('#80ffff', '#939598', '#e00404'),
                        )

    # draw a black outline at the lowest contour to highlight affected areas
    iplt.contour(cube, levels=(levels[0], 100),
                 colors='black')

    # set an extent and a background image for the map
    ax = plt.gca()
    ax.set_extent((-90, 20, 20, 75))
    ax.stock_img('ne_shaded')

    # make a legend, with custom labels, for the coloured contour set
    artists, _ = cs.legend_elements()
    labels = [
              r'$%s < x \leq %s$' % (levels[0], levels[1]),
              r'$%s < x \leq %s$' % (levels[1], levels[2]),
              r'$x > %s$' % levels[2]]
    ax.legend(artists, labels, title='Ash concentration / g m-3', loc='upper left')

    time = cube.coord('time')
    time_date = time.units.num2date(time.points[0]).strftime(UTC_format)
    plt.title('Volcanic ash concentration forecast\nvalid at %s' % time_date)

    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = deriving_phenomena
"""
Deriving Exner Pressure and Air Temperature
===========================================

This example shows some processing of cubes in order to derive further related
cubes; in this case the derived cubes are Exner pressure and air temperature
which are calculated by combining air pressure, air potential temperature and
specific humidity. Finally, the two new cubes are presented side-by-side in a
plot.

"""
import matplotlib.pyplot as plt
import matplotlib.ticker

import iris
import iris.coords as coords
import iris.iterate
import iris.quickplot as qplt


def limit_colorbar_ticks(contour_object):
    """
    Takes a contour object which has an associated colorbar and limits the
    number of ticks on the colorbar to 4.

    """
    # Under Matplotlib v1.2.x the colorbar attribute of a contour object is
    # a tuple containing the colorbar and an axes object, whereas under
    # Matplotlib v1.3.x it is simply the colorbar.
    try:
        colorbar = contour_object.colorbar[0]
    except AttributeError:
        colorbar = contour_object.colorbar

    colorbar.locator = matplotlib.ticker.MaxNLocator(4)
    colorbar.update_ticks()


def main():
    fname = iris.sample_data_path('colpex.pp')

    # The list of phenomena of interest
    phenomena = ['air_potential_temperature', 'air_pressure']

    # Define the constraint on standard name and model level
    constraints = [iris.Constraint(phenom, model_level_number=1) for
                   phenom in phenomena]

    air_potential_temperature, air_pressure = iris.load_cubes(fname,
                                                              constraints)

    # Define a coordinate which represents 1000 hPa
    p0 = coords.AuxCoord(1000, long_name='P0', units='hPa')
    # Convert reference pressure 'p0' into the same units as 'air_pressure'
    p0.convert_units(air_pressure.units)

    # Calculate Exner pressure
    exner_pressure = (air_pressure / p0) ** (287.05 / 1005.0)
    # Set the name (the unit is scalar)
    exner_pressure.rename('exner_pressure')

    # Calculate air_temp
    air_temperature = exner_pressure * air_potential_temperature
    # Set the name (the unit is K)
    air_temperature.rename('air_temperature')

    # Now create an iterator which will give us lat lon slices of
    # exner pressure and air temperature in the form
    # (exner_slice, air_temp_slice).
    lat_lon_slice_pairs = iris.iterate.izip(exner_pressure,
                                            air_temperature,
                                            coords=['grid_latitude',
                                                    'grid_longitude'])

    plt.figure(figsize=(8, 4))
    for exner_slice, air_temp_slice in lat_lon_slice_pairs:
        plt.subplot(121)
        cont = qplt.contourf(exner_slice)

        # The default colorbar has a few too many ticks on it, causing text to
        # overlap. Therefore, limit the number of ticks.
        limit_colorbar_ticks(cont)

        plt.subplot(122)
        cont = qplt.contourf(air_temp_slice)
        limit_colorbar_ticks(cont)
        plt.show()

        # For the purposes of this example, break after the first loop - we
        # only want to demonstrate the first plot.
        break


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = global_map
"""
Quickplot of a 2d cube on a map
===============================

This example demonstrates a contour plot of global air temperature.
The plot title and the labels for the axes are automatically derived from the metadata.

"""
import cartopy.crs as ccrs
import matplotlib.pyplot as plt

import iris
import iris.quickplot as qplt


def main():
    fname = iris.sample_data_path('air_temp.pp')
    temperature = iris.load_cube(fname)

    # Plot #1: contourf with axes longitude from -180 to 180
    fig = plt.figure(figsize=(12, 5))
    plt.subplot(121)
    qplt.contourf(temperature, 15)
    plt.gca().coastlines()

    # Plot #2: contourf with axes longitude from 0 to 360
    proj = ccrs.PlateCarree(central_longitude=-180.0)
    ax = plt.subplot(122, projection=proj)
    qplt.contourf(temperature, 15)
    plt.gca().coastlines()
    plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = hovmoller
"""
Hovmoller diagram of monthly surface temperature
================================================

This example demonstrates the creation of a Hovmoller diagram with fine control over plot ticks and labels.
The data comes from the Met Office OSTIA project and has been pre-processed to calculate the monthly mean sea
surface temperature.

"""
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

import iris
import iris.quickplot as qplt
import iris.unit


def main():
    fname = iris.sample_data_path('ostia_monthly.nc')

    # load a single cube of surface temperature between +/- 5 latitude
    cube = iris.load_cube(fname, iris.Constraint('surface_temperature', latitude=lambda v: -5 < v < 5))

    # Take the mean over latitude
    cube = cube.collapsed('latitude', iris.analysis.MEAN)

    # Now that we have our data in a nice way, lets create the plot
    # contour with 20 levels
    qplt.contourf(cube, 20)

    # Put a custom label on the y axis
    plt.ylabel('Time / years')

    # Stop matplotlib providing clever axes range padding
    plt.axis('tight')

    # As we are plotting annual variability, put years as the y ticks
    plt.gca().yaxis.set_major_locator(mdates.YearLocator())

    # And format the ticks to just show the year
    plt.gca().yaxis.set_major_formatter(mdates.DateFormatter('%Y'))

    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = lagged_ensemble
"""
Seasonal ensemble model plots
=============================

This example demonstrates the loading of a lagged ensemble dataset from the GloSea4 model, which is then used to
produce two types of plot:

 * The first shows the "postage stamp" style image with an array of 14 images, one for each ensemble member with
   a shared colorbar. (The missing image in this example represents ensemble member number 6 which was a failed run)

 * The second plot shows the data limited to a region of interest, in this case a region defined for forecasting
   ENSO (El Nino-Southern Oscillation), which, for the purposes of this example, has had the ensemble mean subtracted
   from each ensemble member to give an anomaly surface temperature. In practice a better approach would be to take the
   climatological mean, calibrated to the model, from each ensemble member.

"""
import matplotlib.pyplot as plt
import numpy as np

import iris
import iris.plot as iplt


def realization_metadata(cube, field, fname):
    """
    A function which modifies the cube's metadata to add a "realization" (ensemble member) coordinate from the filename if one
    doesn't already exist in the cube.

    """
    # add an ensemble member coordinate if one doesn't already exist
    if not cube.coords('realization'):
        # the ensemble member is encoded in the filename as *_???.pp where ??? is the ensemble member
        realization_number = fname[-6:-3]

        import iris.coords
        realization_coord = iris.coords.AuxCoord(np.int32(realization_number), 'realization')
        cube.add_aux_coord(realization_coord)


def main():
    # extract surface temperature cubes which have an ensemble member coordinate, adding appropriate lagged ensemble metadata
    surface_temp = iris.load_cube(iris.sample_data_path('GloSea4', 'ensemble_???.pp'),
                  iris.Constraint('surface_temperature', realization=lambda value: True),
                  callback=realization_metadata,
                  )

    # ----------------------------------------------------------------------------------------------------------------
    # Plot #1: Ensemble postage stamps
    # ----------------------------------------------------------------------------------------------------------------

    # for the purposes of this example, take the last time element of the cube
    last_timestep = surface_temp[:, -1, :, :]

    # Make 50 evenly spaced levels which span the dataset
    contour_levels = np.linspace(np.min(last_timestep.data), np.max(last_timestep.data), 50)

    # Create a wider than normal figure to support our many plots
    plt.figure(figsize=(12, 6), dpi=100)

    # Also manually adjust the spacings which are used when creating subplots
    plt.gcf().subplots_adjust(hspace=0.05, wspace=0.05, top=0.95, bottom=0.05, left=0.075, right=0.925)

    # iterate over all possible latitude longitude slices
    for cube in last_timestep.slices(['latitude', 'longitude']):

        # get the ensemble member number from the ensemble coordinate
        ens_member = cube.coord('realization').points[0]

        # plot the data in a 4x4 grid, with each plot's position in the grid being determined by ensemble member number
        # the special case for the 13th ensemble member is to have the plot at the bottom right
        if ens_member == 13:
            plt.subplot(4, 4, 16)
        else:
            plt.subplot(4, 4, ens_member+1)

        cf = iplt.contourf(cube, contour_levels)

        # add coastlines
        plt.gca().coastlines()

    # make an axes to put the shared colorbar in
    colorbar_axes = plt.gcf().add_axes([0.35, 0.1, 0.3, 0.05])
    colorbar = plt.colorbar(cf, colorbar_axes, orientation='horizontal')
    colorbar.set_label('%s' % last_timestep.units)

    # limit the colorbar to 8 tick marks
    import matplotlib.ticker
    colorbar.locator = matplotlib.ticker.MaxNLocator(8)
    colorbar.update_ticks()

    # get the time for the entire plot
    time_coord = last_timestep.coord('time')
    time = time_coord.units.num2date(time_coord.bounds[0, 0])

    # set a global title for the postage stamps with the date formated by "monthname year"
    plt.suptitle('Surface temperature ensemble forecasts for %s' % time.strftime('%B %Y'))

    iplt.show()


    # ----------------------------------------------------------------------------------------------------------------
    # Plot #2: ENSO plumes
    # ----------------------------------------------------------------------------------------------------------------

    # Nino 3.4 lies between: 170W and 120W, 5N and 5S, so define a constraint which matches this
    nino_3_4_constraint = iris.Constraint(longitude=lambda v: -170+360 <= v <= -120+360, latitude=lambda v: -5 <= v <= 5)

    nino_cube = surface_temp.extract(nino_3_4_constraint)

    # Subsetting a circular longitude coordinate always results in a circular coordinate, so set the coordinate to be non-circular
    nino_cube.coord('longitude').circular = False

    # Calculate the horizontal mean for the nino region
    mean = nino_cube.collapsed(['latitude', 'longitude'], iris.analysis.MEAN)

    # Calculate the ensemble mean of the horizontal mean. To do this, remove the "forecast_period" and
    # "forecast_reference_time" coordinates which span both "relalization" and "time".
    mean.remove_coord("forecast_reference_time")
    mean.remove_coord("forecast_period")
    ensemble_mean = mean.collapsed('realization', iris.analysis.MEAN)

    # take the ensemble mean from each ensemble member
    mean -= ensemble_mean.data

    plt.figure()

    for ensemble_member in mean.slices(['time']):
        # draw each ensemble member as a dashed line in black
        iplt.plot(ensemble_member, '--k')

    plt.title('Mean temperature anomaly for ENSO 3.4 region')
    plt.xlabel('Time')
    plt.ylabel('Temperature anomaly / K')

    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = lineplot_with_legend
"""
Multi-line temperature profile plot
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

"""
import matplotlib.pyplot as plt

import iris
import iris.quickplot as qplt


def main():
    fname = iris.sample_data_path('air_temp.pp')

    # Load exactly one cube from the given file.
    temperature = iris.load_cube(fname)

    # We only want a small number of latitudes, so filter some out
    # using "extract".
    temperature = temperature.extract(
        iris.Constraint(latitude=lambda cell: 68 <= cell < 78))

    for cube in temperature.slices('longitude'):

        # Create a string label to identify this cube (i.e. latitude: value).
        cube_label = 'latitude: %s' % cube.coord('latitude').points[0]

        # Plot the cube, and associate it with a label.
        qplt.plot(cube, label=cube_label)

    # Add the legend with 2 columns.
    plt.legend(ncol=2)

    # Put a grid on the plot.
    plt.grid(True)

    # Tell matplotlib not to extend the plot axes range to nicely
    # rounded numbers.
    plt.axis('tight')

    # Finally, show it.
    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = orca_projection
"""
Tru-Polar Grid Projected Ploting
================================

This example demonstrates cell plots of data on the semi-structured ORCA2 model
grid.

First, the data is projected into the PlateCarree coordinate reference system.

Second four pcolormesh plots are created from this projected dataset,
using different projections for the output image.

"""

import matplotlib
import matplotlib.pyplot as plt

import cartopy.crs as ccrs
import iris
import iris.analysis.cartography
import iris.quickplot as qplt


def main():
    # Load data
    filepath = iris.sample_data_path('orca2_votemper.nc')
    cube = iris.load_cube(filepath)

    # Choose plot projections
    projections = {}
    projections['Mollweide'] = ccrs.Mollweide()
    projections['PlateCarree'] = ccrs.PlateCarree()
    projections['NorthPolarStereo'] = ccrs.NorthPolarStereo()
    projections['Orthographic'] = ccrs.Orthographic(central_longitude=-90,
                                                    central_latitude=45)

    pcarree = projections['PlateCarree']
    # Transform cube to target projection
    new_cube, extent = iris.analysis.cartography.project(cube, pcarree,
                                                         nx=400, ny=200)

    # Plot data in each projection
    for name in sorted(projections):
        fig = plt.figure()
        fig.suptitle('ORCA2 Data Projected to {}'.format(name))
        # Set up axes and title
        ax = plt.subplot(projection=projections[name])
        # Set limits
        ax.set_global()
        # plot with Iris quickplot pcolormesh
        qplt.pcolormesh(new_cube)
        # Draw coastlines
        ax.coastlines()

        plt.show()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = polar_stereo
"""
Example of a polar stereographic plot
=====================================

Demonstrates plotting data that are defined on a polar stereographic
projection.

"""

import matplotlib.pyplot as plt

import iris
import iris.quickplot as qplt


def main():
    file_path = iris.sample_data_path('polar_stereo.grib2')
    cube = iris.load_cube(file_path)
    qplt.contourf(cube)
    ax = plt.gca()
    ax.coastlines()
    ax.gridlines()
    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = projections_and_annotations
"""
Plotting in different projections
=================================

This example shows how to overlay data and graphics in different projections,
demonstrating various features of Iris, Cartopy and matplotlib.

We wish to overlay two datasets, defined on different rotated-pole grids.
To display both together, we make a pseudocoloured plot of the first, overlaid
with contour lines from the second.
We also add some lines and text annotations drawn in various projections.

We plot these over a specified region, in two different map projections.

"""
import cartopy.crs as ccrs
import iris
import iris.plot as iplt
import numpy as np
import matplotlib.pyplot as plt


# Define a Cartopy 'ordinary' lat-lon coordinate reference system.
crs_latlon = ccrs.PlateCarree()


def make_plot(projection_name, projection_crs):

    # Create a matplotlib Figure.
    fig = plt.figure()

    # Add a matplotlib Axes, specifying the required display projection.
    # NOTE: specifying 'projection' (a "cartopy.crs.Projection") makes the
    # resulting Axes a "cartopy.mpl.geoaxes.GeoAxes", which supports plotting
    # in different coordinate systems.
    ax = plt.axes(projection=projection_crs)

    # Set display limits to include a set region of latitude * longitude.
    # (Note: Cartopy-specific).
    ax.set_extent((-80.0, 20.0, 10.0, 80.0), crs=crs_latlon)

    # Add coastlines and meridians/parallels (Cartopy-specific).
    ax.coastlines(linewidth=0.75, color='navy')
    ax.gridlines(crs=crs_latlon, linestyle='-')

    # Plot the first dataset as a pseudocolour filled plot.
    maindata_filepath = iris.sample_data_path('rotated_pole.nc')
    main_data = iris.load_cube(maindata_filepath)
    # NOTE: iplt.pcolormesh calls "pyplot.pcolormesh", passing in a coordinate
    # system with the 'transform' keyword:  This enables the Axes (a cartopy
    # GeoAxes) to reproject the plot into the display projection.
    iplt.pcolormesh(main_data, cmap='RdBu_r')

    # Overplot the other dataset (which has a different grid), as contours.
    overlay_filepath = iris.sample_data_path('space_weather.nc')
    overlay_data = iris.load_cube(overlay_filepath, 'total electron content')
    # NOTE: as above, "iris.plot.contour" calls "pyplot.contour" with a
    # 'transform' keyword, enabling Cartopy reprojection.
    iplt.contour(overlay_data, 20,
                 linewidths=2.0, colors='darkgreen', linestyles='-')

    # Draw a margin line, some way in from the border of the 'main' data...
    # First calculate rectangle corners, 7% in from each corner of the data.
    x_coord, y_coord = main_data.coord(axis='x'), main_data.coord(axis='y')
    x_start, x_end = np.min(x_coord.points), np.max(x_coord.points)
    y_start, y_end = np.min(y_coord.points), np.max(y_coord.points)
    margin = 0.07
    margin_fractions = np.array([margin, 1.0 - margin])
    x_lower, x_upper = x_start + (x_end - x_start) * margin_fractions
    y_lower, y_upper = y_start + (y_end - y_start) * margin_fractions
    box_x_points = x_lower + (x_upper - x_lower) * np.array([0, 1, 1, 0, 0])
    box_y_points = y_lower + (y_upper - y_lower) * np.array([0, 0, 1, 1, 0])
    # Get the Iris coordinate sytem of the X coordinate (Y should be the same).
    cs_data1 = x_coord.coord_system
    # Construct an equivalent Cartopy coordinate reference system ("crs").
    crs_data1 = cs_data1.as_cartopy_crs()
    # Draw the rectangle in this crs, with matplotlib "pyplot.plot".
    # NOTE: the 'transform' keyword specifies a non-display coordinate system
    # for the plot points (as used by the "iris.plot" functions).
    plt.plot(box_x_points, box_y_points, transform=crs_data1,
             linewidth=2.0, color='white', linestyle='--')

    # Mark some particular places with a small circle and a name label...
    # Define some test points with latitude and longitude coordinates.
    city_data = [('London', 51.5072, 0.1275),
                 ('Halifax, NS', 44.67, -63.61),
                 ('Reykjavik', 64.1333, -21.9333)]
    # Place a single marker point and a text annotation at each place.
    for name, lat, lon in city_data:
        plt.plot(lon, lat, marker='o', markersize=7.0, markeredgewidth=2.5,
                 markerfacecolor='black', markeredgecolor='white',
                 transform=crs_latlon)
        # NOTE: the "plt.annotate call" does not have a "transform=" keyword,
        # so for this one we transform the coordinates with a Cartopy call.
        at_x, at_y = ax.projection.transform_point(lon, lat,
                                                   src_crs=crs_latlon)
        plt.annotate(
            name, xy=(at_x, at_y), xytext=(30, 20), textcoords='offset points',
            color='black', backgroundcolor='white', size='large',
            arrowprops=dict(arrowstyle='->', color='white', linewidth=2.5))

    # Add a title, and display.
    plt.title('A pseudocolour plot on the {} projection,\n'
              'with overlaid contours.'.format(projection_name))
    plt.show()


def main():
    # Demonstrate with two different display projections.
    make_plot('Equidistant Cylindrical', ccrs.PlateCarree())
    make_plot('North Polar Stereographic', ccrs.NorthPolarStereo())


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = rotated_pole_mapping
"""
Rotated pole mapping
=====================

This example uses several visualisation methods to achieve an array of
differing images, including:

 * Visualisation of point based data
 * Contouring of point based data
 * Block plot of contiguous bounded data
 * Non native projection and a Natural Earth shaded relief image underlay

"""
import cartopy.crs as ccrs
import matplotlib.pyplot as plt

import iris
import iris.plot as iplt
import iris.quickplot as qplt
import iris.analysis.cartography


def main():
    fname = iris.sample_data_path('rotated_pole.nc')
    air_pressure = iris.load_cube(fname)

    # Plot #1: Point plot showing data values & a colorbar
    plt.figure()
    points = qplt.points(air_pressure, c=air_pressure.data)
    cb = plt.colorbar(points, orientation='horizontal')
    cb.set_label(air_pressure.units)
    plt.gca().coastlines()
    plt.show()

    # Plot #2: Contourf of the point based data
    plt.figure()
    qplt.contourf(air_pressure, 15)
    plt.gca().coastlines()
    plt.show()

    # Plot #3: Contourf overlayed by coloured point data
    plt.figure()
    qplt.contourf(air_pressure)
    iplt.points(air_pressure, c=air_pressure.data)
    plt.gca().coastlines()
    plt.show()

    # For the purposes of this example, add some bounds to the latitude
    # and longitude
    air_pressure.coord('grid_latitude').guess_bounds()
    air_pressure.coord('grid_longitude').guess_bounds()

    # Plot #4: Block plot
    plt.figure()
    plt.axes(projection=ccrs.PlateCarree())
    iplt.pcolormesh(air_pressure)
    plt.gca().stock_img()
    plt.gca().coastlines()
    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = SOI_filtering
"""
Applying a filter to a time-series
==================================

This example demonstrates low pass filtering a time-series by applying a
weighted running mean over the time dimension.

The time-series used is the Darwin-only Southern Oscillation index (SOI),
which is filtered using two different Lanczos filters, one to filter out
time-scales of less than two years and one to filter out time-scales of
less than 7 years.

References
----------

    Duchon C. E. (1979) Lanczos Filtering in One and Two Dimensions.
    Journal of Applied Meteorology, Vol 18, pp 1016-1022.

    Trenberth K. E. (1984) Signal Versus Noise in the Southern Oscillation.
    Monthly Weather Review, Vol 112, pp 326-332

"""
import numpy as np
import matplotlib.pyplot as plt
import iris
import iris.plot as iplt


def low_pass_weights(window, cutoff):
    """Calculate weights for a low pass Lanczos filter.

    Args:

    window: int
        The length of the filter window.

    cutoff: float
        The cutoff frequency in inverse time steps.

    """
    order = ((window - 1) // 2 ) + 1
    nwts = 2 * order + 1
    w = np.zeros([nwts])
    n = nwts // 2
    w[n] = 2 * cutoff
    k = np.arange(1., n)
    sigma = np.sin(np.pi * k / n) * n / (np.pi * k)
    firstfactor = np.sin(2. * np.pi * cutoff * k) / (np.pi * k)
    w[n-1:0:-1] = firstfactor * sigma
    w[n+1:-1] = firstfactor * sigma
    return w[1:-1]


def main():

    # Load the monthly-valued Southern Oscillation Index (SOI) time-series.
    fname = iris.sample_data_path('SOI_Darwin.nc')
    soi = iris.load_cube(fname)

    # Window length for filters.
    window = 121

    # Construct 2-year (24-month) and 7-year (84-month) low pass filters
    # for the SOI data which is monthly.
    wgts24 = low_pass_weights(window, 1. / 24.) 
    wgts84 = low_pass_weights(window, 1. / 84.)

    # Apply each filter using the rolling_window method used with the weights
    # keyword argument. A weighted sum is required because the magnitude of
    # the weights are just as important as their relative sizes.
    soi24 = soi.rolling_window('time',
                               iris.analysis.SUM,
                               len(wgts24),
                               weights=wgts24)
    soi84 =  soi.rolling_window('time',
                                iris.analysis.SUM,
                                len(wgts84),
                                weights=wgts84)

    # Plot the SOI time series and both filtered versions.
    plt.figure(figsize=(9, 4))
    iplt.plot(soi, color='0.7', linewidth=1., linestyle='-',
              alpha=1., label='no filter')
    iplt.plot(soi24, color='b', linewidth=2., linestyle='-',
              alpha=.7, label='2-year filter')
    iplt.plot(soi84, color='r', linewidth=2., linestyle='-',
              alpha=.7, label='7-year filter')
    plt.ylim([-4, 4])
    plt.title('Southern Oscillation Index (Darwin Only)')
    plt.xlabel('Time')
    plt.ylabel('SOI')
    plt.legend(fontsize=10)
    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = TEC
"""
Ionosphere space weather
========================

This space weather example plots a filled contour of rotated pole point
data with a shaded relief image underlay. The plot shows aggregated
vertical electron content in the ionosphere.

The plot exhibits an interesting outline effect due to excluding data
values below a certain threshold.

"""

import matplotlib.pyplot as plt
import numpy.ma as ma

import iris
import iris.quickplot as qplt


def main():
    # Load the "total electron content" cube.
    filename = iris.sample_data_path('space_weather.nc')
    cube = iris.load_cube(filename, 'total electron content')

    # Explicitly mask negative electron content.
    cube.data = ma.masked_less(cube.data, 0)

    # Plot the cube using one hundred colour levels.
    qplt.contourf(cube, 100)
    plt.title('Total Electron Content')
    plt.xlabel('longitude / degrees')
    plt.ylabel('latitude / degrees')
    plt.gca().stock_img()
    plt.gca().coastlines()
    plt.show()


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = extest_util
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

"""
Provides context managers which are fundamental to the ability
to run the example tests.

"""


import contextlib
import os.path
import sys

import matplotlib.pyplot as plt

import iris.plot as iplt
import iris.quickplot as qplt
from iris.tests import _DEFAULT_IMAGE_TOLERANCE


EXAMPLE_DIRECTORY = os.path.join(os.path.dirname(os.path.dirname(__file__)),
                                 'example_code', 'graphics')


@contextlib.contextmanager
def add_examples_to_path():
    """
    Creates a context manager which can be used to add the iris examples
    to the PYTHONPATH. The examples are only importable throughout the lifetime
    of this context manager.

    """
    orig_sys_path = sys.path
    sys.path = sys.path[:]
    sys.path.append(EXAMPLE_DIRECTORY)
    yield
    sys.path = orig_sys_path


@contextlib.contextmanager
def show_replaced_by_check_graphic(test_case, tol=_DEFAULT_IMAGE_TOLERANCE):
    """
    Creates a context manager which can be used to replace the functionality
    of matplotlib.pyplot.show with a function which calls the check_graphic
    method on the given test_case (iris.tests.IrisTest.check_graphic).

    """
    def replacement_show():
        # form a closure on test_case and tolerance
        test_case.check_graphic(tol=tol)

    orig_show = plt.show
    plt.show = iplt.show = qplt.show = replacement_show
    yield
    plt.show = iplt.show = qplt.show = orig_show

########NEW FILE########
__FILENAME__ = test_anomaly_log_colouring
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import anomaly_log_colouring


class TestAnomalyLogColouring(tests.GraphicsTest):
    """Test the anomaly colouring example code."""
    def test_anomaly_log_colouring(self):
        with extest_util.show_replaced_by_check_graphic(self):
            anomaly_log_colouring.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_atlantic_profiles
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import atlantic_profiles


class TestAtlanticProfiles(tests.GraphicsTest):
    """Test the atlantic_profiles example code."""
    def test_atlantic_profiles(self):
        with extest_util.show_replaced_by_check_graphic(self, tol=14.0):
            atlantic_profiles.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_COP_1d_plot
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import COP_1d_plot


class TestCOP1DPlot(tests.GraphicsTest):
    """Test the COP_1d_plot example code."""
    def test_COP_1d_plot(self):
        with extest_util.show_replaced_by_check_graphic(self):
            COP_1d_plot.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_COP_maps
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import COP_maps


class TestCOPMaps(tests.GraphicsTest):
    """Test the COP_maps example code."""
    def test_cop_maps(self):
        with extest_util.show_replaced_by_check_graphic(self):
            COP_maps.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_cross_section
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import cross_section


class TestCrossSection(tests.GraphicsTest):
    """Test the cross_section example code."""
    def test_cross_section(self):
        with extest_util.show_replaced_by_check_graphic(self):
            cross_section.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_custom_aggregation
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import custom_aggregation


class TestCustomAggregation(tests.GraphicsTest):
    """Test the custom aggregation example code."""
    def test_custom_aggregation(self):
        with extest_util.show_replaced_by_check_graphic(self):
            custom_aggregation.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_custom_file_loading
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import custom_file_loading


class TestCustomFileLoading(tests.GraphicsTest):
    """Test the custom_file_loading example code."""
    def test_custom_file_loading(self):
        with extest_util.show_replaced_by_check_graphic(self):
            custom_file_loading.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_deriving_phenomena
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import deriving_phenomena


class TestDerivingPhenomena(tests.GraphicsTest):
    """Test the deriving_phenomena example code."""
    def test_deriving_phenomena(self):
        with extest_util.show_replaced_by_check_graphic(self):
            deriving_phenomena.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_global_map
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import global_map


class TestGlobalMap(tests.GraphicsTest):
    """Test the global_map example code."""
    def test_global_map(self):
        with extest_util.show_replaced_by_check_graphic(self):
            global_map.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_hovmoller
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import hovmoller


class TestGlobalMap(tests.GraphicsTest):
    """Test the hovmoller example code."""
    def test_hovmoller(self):
        with extest_util.show_replaced_by_check_graphic(self):
            hovmoller.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_lagged_ensemble
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import lagged_ensemble


class TestLaggedEnsemble(tests.GraphicsTest):
    """Test the lagged ensemble example code."""
    def test_lagged_ensemble(self):
        with extest_util.show_replaced_by_check_graphic(self):
            lagged_ensemble.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_lineplot_with_legend
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import lineplot_with_legend


class TestLineplotWithLegend(tests.GraphicsTest):
    """Test the lineplot_with_legend example code."""
    def test_lineplot_with_legend(self):
        with extest_util.show_replaced_by_check_graphic(self):
            lineplot_with_legend.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_orca_projection
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import orca_projection


class TestOrcaProjection(tests.GraphicsTest):
    """Test the orca projection example code."""
    def test_orca_projection(self):
        with extest_util.show_replaced_by_check_graphic(self):
            orca_projection.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_polar_stereo
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised
# before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import polar_stereo


class TestPolarStereo(tests.GraphicsTest):
    """Test the polar_stereo example code."""
    def test_polar_stereo(self):
        with extest_util.show_replaced_by_check_graphic(self):
            polar_stereo.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_projections_and_annotations
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import projections_and_annotations


class TestProjectionsAndAnnotations(tests.GraphicsTest):
    """Test the atlantic_profiles example code."""
    def test_projections_and_annotations(self):
        with extest_util.show_replaced_by_check_graphic(self):
            projections_and_annotations.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_rotated_pole_mapping
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import rotated_pole_mapping


class TestRotatedPoleMapping(tests.GraphicsTest):
    """Test the rotated_pole_mapping example code."""
    def test_rotated_pole_mapping(self):
        with extest_util.show_replaced_by_check_graphic(self):
            rotated_pole_mapping.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_SOI_filtering
# (C) British Crown Copyright 2012 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import SOI_filtering


class TestSOIFiltering(tests.GraphicsTest):
    """Test the SOI_filtering example code."""
    def test_soi_filtering(self):
        with extest_util.show_replaced_by_check_graphic(self):
            SOI_filtering.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_TEC
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# Import Iris tests first so that some things can be initialised before importing anything else.
import iris.tests as tests

import extest_util

with extest_util.add_examples_to_path():
    import TEC


class TestTEC(tests.GraphicsTest):
    """Test the TEC example code."""
    def test_TEC(self):
        with extest_util.show_replaced_by_check_graphic(self):
            TEC.main()


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = conf
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# -*- coding: utf-8 -*-
#
# Iris documentation build configuration file, created by
# sphinx-quickstart on Tue May 25 13:26:23 2010.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.append(os.path.abspath('sphinxext'))

# add some sample files from the developers guide..
sys.path.append(os.path.abspath(os.path.join('developers_guide')))


# -- General configuration -----------------------------------------------------

# Temporary value for use by LaTeX and 'man' output.
# Deleted at the end of the module.
_authors = ('Byron Blay', 'Ed Campbell', 'Philip Elson', 'Richard Hattersley',
            'Bill Little')

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc',
              'sphinx.ext.coverage',
              'sphinx.ext.pngmath',
              'sphinx.ext.autosummary',
              'sphinx.ext.graphviz',
              'sphinx.ext.intersphinx',
              'sphinx.ext.doctest',
              'matplotlib.sphinxext.mathmpl',
              'matplotlib.sphinxext.only_directives',

              #'matplotlib.sphinxext.plot_directive',
              'plot_directive',

              # better class documentation
              'custom_class_autodoc',

              # Data instance __repr__ filter.
              'custom_data_autodoc',

              'gen_example_directory',
              'generate_package_rst',
              'gen_gallery',
              ]

# list of packages to document
autopackage_name = ['iris']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'contents'

# General information about the project.
project = u'Iris'
# define the copyright information for latex builds. Note, for html builds,
# the copyright exists directly inside "_templates/layout.html"
copyright = u'British Crown Copyright 2010 - 2013, Met Office'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
import iris
# The short X.Y version.
if iris.__version__ == 'dev':
    version = 'dev'
else:
    # major.feature(.minor)-dev -> major.minor
    version = '.'.join(iris.__version__.split('-')[0].split('.')[:2])
# The full version, including alpha/beta/rc tags.
release = iris.__version__

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['sphinxext', 'build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# Definer the default highlight language. This also allows the >>> removal
# javascript (copybutton.js) to function.
highlight_language = 'python'

# A list of ignored prefixes for module index sorting.
modindex_common_prefix = ['iris']

intersphinx_mapping = {
   'python': ('http://docs.python.org/2.7', None),
   'numpy': ('http://docs.scipy.org/doc/numpy/', None),
   'scipy': ('http://docs.scipy.org/doc/scipy/reference/', None),
   'matplotlib': ('http://matplotlib.sourceforge.net/', None),
}


# -- Doctest ------------------------------------------------------------------

doctest_global_setup = 'import iris'

# -- Autodoc ------------------------------------------------------------------

autodoc_member_order = 'groupwise'
autodoc_default_flags = ['show-inheritance']

# include the __init__ method when documenting classes
# document the init/new method at the top level of the class documentation rather than displaying the class docstring
autoclass_content='init'

# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
html_theme = 'default'
html_theme = 'sphinxdoc'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
html_additional_pages = {'index': 'index.html', 'gallery':'gallery.html'}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
html_show_sphinx = False

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'Irisdoc'

html_use_modindex = False


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('contents', 'Iris.tex', u'Iris Documentation', ' \\and '.join(_authors), 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True
latex_elements = {}
latex_elements['docclass'] = 'MO_report'

# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'iris', u'Iris Documentation', _authors, 1)
]

##########################
# plot directive options #
##########################

plot_formats = [('png', 100),
                #('hires.png', 200), ('pdf', 250)
                ]





# Delete the temporary value.
del _authors

########NEW FILE########
__FILENAME__ = docstrings_attribute
class ExampleClass(object):
    """
    Class Summary

    """
    def __init__(self, arg1, arg2):
        """
        Purpose section description.

        Description section text.

        Args:

        * arg1 (int):
            First argument description.
        * arg2 (float):
            Second argument description.

        Returns:
            Boolean.

        """
        self.a = arg1
        'Attribute arg1 docstring.'
        self.b = arg2
        'Attribute arg2 docstring.'

    @property
    def square(self):
        """
        *(read-only)* Purpose section description.

        Returns:
            int.

        """
        return self.a*self.a

########NEW FILE########
__FILENAME__ = docstrings_sample_routine
def sample_routine(arg1, arg2, kwarg1='foo', kwarg2=None):
    """
    Purpose section text goes here.

    Description section longer text goes here.

    Args:

    * arg1 (numpy.ndarray):
        First argument description.
    * arg2 (numpy.ndarray):
        Second argument description.

    Kwargs:

    * kwarg1 (string):
        The first keyword argument. This argument description
        can be multi-lined.
    * kwarg2 (Boolean or None):
        The second keyword argument.

    Returns:
        numpy.ndarray of arg1 * arg2

    """
    pass

########NEW FILE########
__FILENAME__ = gitwash_dumper
#!/usr/bin/env python
''' Checkout gitwash repo into directory and do search replace on name '''

import os
from os.path import join as pjoin
import shutil
import sys
import re
import glob
import fnmatch
import tempfile
from subprocess import call
from optparse import OptionParser

verbose = False


def clone_repo(url, branch):
    cwd = os.getcwd()
    tmpdir = tempfile.mkdtemp()
    try:
        cmd = 'git clone %s %s' % (url, tmpdir)
        call(cmd, shell=True)
        os.chdir(tmpdir)
        cmd = 'git checkout %s' % branch
        call(cmd, shell=True)
    except:
        shutil.rmtree(tmpdir)
        raise
    finally:
        os.chdir(cwd)
    return tmpdir


def cp_files(in_path, globs, out_path):
    try:
        os.makedirs(out_path)
    except OSError:
        pass
    out_fnames = []
    for in_glob in globs:
        in_glob_path = pjoin(in_path, in_glob)
        for in_fname in glob.glob(in_glob_path):
            out_fname = in_fname.replace(in_path, out_path)
            pth, _ = os.path.split(out_fname)
            if not os.path.isdir(pth):
                os.makedirs(pth)
            shutil.copyfile(in_fname, out_fname)
            out_fnames.append(out_fname)
    return out_fnames


def filename_search_replace(sr_pairs, filename, backup=False):
    ''' Search and replace for expressions in files

    '''
    in_txt = open(filename, 'rt').read(-1)
    out_txt = in_txt[:]
    for in_exp, out_exp in sr_pairs:
        in_exp = re.compile(in_exp)
        out_txt = in_exp.sub(out_exp, out_txt)
    if in_txt == out_txt:
        return False
    open(filename, 'wt').write(out_txt)
    if backup:
        open(filename + '.bak', 'wt').write(in_txt)
    return True


def copy_replace(replace_pairs,
                 repo_path,
                 out_path,
                 cp_globs=('*',),
                 rep_globs=('*',),
                 renames = ()):
    out_fnames = cp_files(repo_path, cp_globs, out_path)
    renames = [(re.compile(in_exp), out_exp) for in_exp, out_exp in renames]
    fnames = []
    for rep_glob in rep_globs:
        fnames += fnmatch.filter(out_fnames, rep_glob)
    if verbose:
        print '\n'.join(fnames)
    for fname in fnames:
        filename_search_replace(replace_pairs, fname, False)
        for in_exp, out_exp in renames:
            new_fname, n = in_exp.subn(out_exp, fname)
            if n:
                os.rename(fname, new_fname)
                break


def make_link_targets(proj_name,
                      user_name,
                      repo_name,
                      known_link_fname,
                      out_link_fname,
                      url=None,
                      ml_url=None):
    """ Check and make link targets

    If url is None or ml_url is None, check if there are links present for these
    in `known_link_fname`.  If not, raise error.  The check is:

    Look for a target `proj_name`.
    Look for a target `proj_name` + ' mailing list'

    Also, look for a target `proj_name` + 'github'.  If this exists, don't write
    this target into the new file below.

    If we are writing any of the url, ml_url, or github address, then write new
    file with these links, of form:

    .. _`proj_name`
    .. _`proj_name`: url
    .. _`proj_name` mailing list: url
    """
    link_contents = open(known_link_fname, 'rt').readlines()
    have_url = not url is None
    have_ml_url = not ml_url is None
    have_gh_url = None
    for line in link_contents:
        if not have_url:
            match = re.match(r'..\s+_`%s`:\s+' % proj_name, line)
            if match:
                have_url = True
        if not have_ml_url:
            match = re.match(r'..\s+_`%s mailing list`:\s+' % proj_name, line)
            if match:
                have_ml_url = True
        if not have_gh_url:
            match = re.match(r'..\s+_`%s github`:\s+' % proj_name, line)
            if match:
                have_gh_url = True
    if not have_url or not have_ml_url:
        raise RuntimeError('Need command line or known project '
                           'and / or mailing list URLs')
    lines = []
    if not url is None:
        lines.append('.. _`%s`: %s\n' % (proj_name, url))
    if not have_gh_url:
        gh_url = 'http://github.com/%s/%s\n' % (user_name, repo_name)
        lines.append('.. _`%s github`: %s\n' % (proj_name, gh_url))
    if not ml_url is None:
        lines.append('.. _`%s mailing list`: %s\n' % (proj_name, ml_url))
    if len(lines) == 0:
        # Nothing to do
        return
    # A neat little header line
    lines = ['.. %s\n' % proj_name] + lines
    out_links = open(out_link_fname, 'wt')
    out_links.writelines(lines)
    out_links.close()


USAGE = ''' <output_directory> <project_name>

If not set with options, the repository name is the same as the <project
name>

If not set with options, the main github user is the same as the
repository name.'''


GITWASH_CENTRAL = 'git://github.com/matthew-brett/gitwash.git'
GITWASH_BRANCH = 'master'


def main():
    parser = OptionParser()
    parser.set_usage(parser.get_usage().strip() + USAGE)
    parser.add_option("--repo-name", dest="repo_name",
                      help="repository name - e.g. nitime",
                      metavar="REPO_NAME")
    parser.add_option("--github-user", dest="main_gh_user",
                      help="github username for main repo - e.g fperez",
                      metavar="MAIN_GH_USER")
    parser.add_option("--gitwash-url", dest="gitwash_url",
                      help="URL to gitwash repository - default %s"
                      % GITWASH_CENTRAL,
                      default=GITWASH_CENTRAL,
                      metavar="GITWASH_URL")
    parser.add_option("--gitwash-branch", dest="gitwash_branch",
                      help="branch in gitwash repository - default %s"
                      % GITWASH_BRANCH,
                      default=GITWASH_BRANCH,
                      metavar="GITWASH_BRANCH")
    parser.add_option("--source-suffix", dest="source_suffix",
                      help="suffix of ReST source files - default '.rst'",
                      default='.rst',
                      metavar="SOURCE_SUFFIX")
    parser.add_option("--project-url", dest="project_url",
                      help="URL for project web pages",
                      default=None,
                      metavar="PROJECT_URL")
    parser.add_option("--project-ml-url", dest="project_ml_url",
                      help="URL for project mailing list",
                      default=None,
                      metavar="PROJECT_ML_URL")
    (options, args) = parser.parse_args()
    if len(args) < 2:
        parser.print_help()
        sys.exit()
    out_path, project_name = args
    if options.repo_name is None:
        options.repo_name = project_name
    if options.main_gh_user is None:
        options.main_gh_user = options.repo_name
    repo_path = clone_repo(options.gitwash_url, options.gitwash_branch)
    try:
        copy_replace((('PROJECTNAME', project_name),
                      ('REPONAME', options.repo_name),
                      ('MAIN_GH_USER', options.main_gh_user)),
                     repo_path,
                     out_path,
                     cp_globs=(pjoin('gitwash', '*'),),
                     rep_globs=('*.rst',),
                     renames=(('\.rst$', options.source_suffix),))
        make_link_targets(project_name,
                          options.main_gh_user,
                          options.repo_name,
                          pjoin(out_path, 'gitwash', 'known_projects.inc'),
                          pjoin(out_path, 'gitwash', 'this_project.inc'),
                          options.project_url,
                          options.project_ml_url)
    finally:
        shutil.rmtree(repo_path)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = custom_class_autodoc
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


from sphinx.ext import autodoc
from sphinx.ext.autodoc import *

import inspect


class ClassWithConstructorDocumenter(autodoc.ClassDocumenter):
    priority = 1000000

    def get_object_members(self, want_all):
        r,f = autodoc.ClassDocumenter.get_object_members(self, want_all)
        #print 'CALLED OBJECT MEMBERS....', r, f
        return r, f

#   def filter_members(self, members, want_all):
#      res = autodoc.ClassDocumenter.filter_members(self, members, want_all)
#      for (membername, member) in members:
#         if membername in ['__init__', '__new__']:
#            # final argument is "isattr" - no its not an attribute it is a contructor method.
#            res.insert(0, [membername, member, False])
#      return res

    @staticmethod
    def can_document_member(member, mname, isattr, self):
        #print ' asked me if I can document....', member, mname, isattr, self
        #print ' gave them :', autodoc.ClassDocumenter.can_document_member(member, mname, isattr, self)
        return autodoc.ClassDocumenter.can_document_member(member, mname, isattr, self)

    def get_doc(self, encoding=None):
        content = self.env.config.autoclass_content

        docstrings = []
        docstring = self.get_attr(self.object, '__doc__', None)
        if docstring:
            docstrings.append(docstring)

        # for classes, what the "docstring" is can be controlled via a
        # config value; the default is only the class docstring
        if content in ('both', 'init'):
            constructor = self.get_constructor()
            if constructor:
                initdocstring = self.get_attr( constructor, '__doc__', None)
            else:
                initdocstring = None
            if initdocstring:
                if content == 'init':
                    docstrings = [initdocstring]
                else:
                    docstrings.append(initdocstring)

        return [prepare_docstring(force_decode(docstring, encoding))
                for docstring in docstrings]

    def get_constructor(self):
        # for classes, the relevant signature is the __init__ method's
        initmeth = self.get_attr(self.object, '__new__', None)

        if initmeth is None or initmeth is object.__new__ or not \
               (inspect.ismethod(initmeth) or inspect.isfunction(initmeth)):
           initmeth = None

        if initmeth is None:
            initmeth = self.get_attr(self.object, '__init__', None)

        if initmeth is None or initmeth is object.__init__ or initmeth is object.__new__ or not \
               (inspect.ismethod(initmeth) or inspect.isfunction(initmeth)):
            initmeth = None

        return initmeth


    def format_args(self):
        initmeth = self.get_constructor()
        #print 'DOING FORMAT ARGS: ', initmeth, self.object
        try:
            argspec = inspect.getargspec(initmeth)
        except TypeError:
            # still not possible: happens e.g. for old-style classes
            # with __init__ in C
            return None
        if argspec[0] and argspec[0][0] in ('cls', 'self'):
            del argspec[0][0]
        return inspect.formatargspec(*argspec)


def setup(app):
    app.add_autodocumenter(ClassWithConstructorDocumenter)

########NEW FILE########
__FILENAME__ = custom_data_autodoc
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


from sphinx.ext.autodoc import DataDocumenter, ModuleLevelDocumenter
from sphinx.util.inspect import safe_repr

from iris.analysis import Aggregator


class IrisDataDocumenter(DataDocumenter):
    priority = 0

    def add_directive_header(self, sig):
        ModuleLevelDocumenter.add_directive_header(self, sig)
        if not self.options.annotation:
            try:
                objrepr = safe_repr(self.object)
            except ValueError:
                pass
            else:
                if not (objrepr.startswith('<') and objrepr.endswith('>')):
                    self.add_line(u'   :annotation: = ' + objrepr, '<autodoc>')
                else:
                    self.add_line(u'   :annotation:', '<autodoc>')
        elif self.options.annotation is object():
            pass
        else:
            self.add_line(u'   :annotation: %s' % self.options.annotation,
                          '<autodoc>')


def handler(app, what, name, obj, options, signature, return_annotation):
    if what == 'data':
        if isinstance(obj, object) and issubclass(obj.__class__, Aggregator):
            signature = '()'
            return_annotation = '{} instance.'.format(obj.__class__.__name__)
    return signature, return_annotation


def setup(app):
    app.add_autodocumenter(IrisDataDocumenter)
    app.connect('autodoc-process-signature', handler)

########NEW FILE########
__FILENAME__ = generate_package_rst
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


import os
import sys
import re
import inspect


document_dict = {
                     # Use autoclass for classes
                     'class': '''
%(object_docstring)s

..

    .. autoclass:: %(object_name)s
        :members:
        :undoc-members:

''',
                     'function': '''
.. autofunction:: %(object_name)s

''',
                     # For everything else, let automodule do some magic...
                     None: '''

.. autodata:: %(object_name)s

'''}


horizontal_sep = """
.. raw:: html

    <p class="hr_p"><a href="#">&uarr;&#32&#32 top &#32&#32&uarr;</a></p>
    <!--
-----------

.. raw:: html

    -->

"""


def lookup_object_type(obj):
        if inspect.isclass(obj):
            return 'class'
        elif inspect.isfunction(obj):
            return 'function'
        else:
            return None


def auto_doc_module(file_path, import_name, root_package, package_toc=None, title=None):
    r = __import__(import_name)
    r = sys.modules[import_name]
    elems = dir(r)

    if '__all__' in elems:
        document_these = r.__all__
        document_these = [[obj, r.__getattribute__(obj)] for obj in document_these]
    else:
        document_these = [[obj, r.__getattribute__(obj)] for obj in elems if not obj.startswith('_') and not inspect.ismodule(r.__getattribute__(obj))]
        is_from_this_module = lambda x, this_module=r.__name__: hasattr(x[1], '__module__') and x[1].__module__ == r.__name__

        document_these = filter(is_from_this_module, document_these)
        sort_order = {'class': 2, 'function':1, None:0}
        # sort them according to sort_order dict
        document_these = sorted(document_these, key=lambda x: sort_order.get(lookup_object_type(x[1]),0))

    tmp = ''
    for element, obj in document_these:
        tmp += horizontal_sep + document_dict[lookup_object_type(obj)] % {'object_name': import_name + '.' + element,
                                                         'object_name_header_line':'+' * len(import_name + '.' + element),
                                                         'object_docstring': inspect.getdoc(obj),
                                                         }

    module_elements = '\n'.join([' * :py:obj:`%s`' % (element) for element, obj in document_these])

    tmp = r'''.. _%(import_name)s:

%(title_underline)s
%(title)s
%(title_underline)s

%(sidebar)s

.. currentmodule:: %(root_package)s

.. automodule:: %(import_name)s

In this module:

%(module_elements)s


''' + tmp
    if package_toc:
       sidebar = """
.. sidebar:: Modules in this package

%(package_toc_tree)s

    """ % {'package_toc_tree': package_toc}
    else:
       sidebar = ''

    return tmp % {'title': title or import_name, 'title_underline': '=' * len(title or import_name), 'import_name': import_name, 'root_package':root_package, 'sidebar':sidebar, 'module_elements': module_elements}


def auto_doc_package(file_path, import_name, root_package, sub_packages):
    max_depth = 1 if import_name == 'iris' else 2
    package_toc = '\n      '.join(sub_packages)
    package_toc = '''
   .. toctree::
      :maxdepth: %d
      :titlesonly:

      %s


''' % (max_depth, package_toc)

    if '.' in import_name:
        title = None
    else:
        title = import_name.capitalize() + ' reference documentation'

    return auto_doc_module(file_path, import_name, root_package, package_toc=package_toc, title=title)


def auto_package_build(app):

    root_package = app.config.autopackage_name
    if root_package is None:
        raise ValueError('set the autopackage_name variable in the conf.py file')

    if not isinstance(root_package, list):
        raise ValueError("autopackage was expecting a list of packages to document e.g. ['itertools']")

    for package in root_package:
        do_package(package)


def do_package(package_name):

    out_dir = package_name + os.path.sep

    # import the root package. If this fails then an import error will be raised.
    module = __import__(package_name)
    root_package = package_name
    rootdir = os.path.dirname(module.__file__)


    package_folder = []
    module_folders = {}
    for root, subFolders, files in os.walk(rootdir):
        for fname in files:
            name, ext = os.path.splitext(fname)

            # skip some non-relevant files
            if ( fname.startswith('.') or fname.startswith('#') or re.search("^_[^_]", fname) or
                 fname.find('.svn')>=0 or not (ext in ['.py', '.so']) ):
                continue

            rel_path = root_package + os.path.join(root, fname).split(rootdir)[-1]
            mod_folder = root_package + os.path.join(root).split(rootdir)[-1].replace('/','.')

            # only add to package folder list if it contains an __init__ script
            if name == '__init__':
                package_folder.append([mod_folder, rel_path])
            else:
                import_name = mod_folder + '.' + name
                module_folders.setdefault(mod_folder, []).append([import_name, rel_path])
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)

    for package, package_path in package_folder:
       if '._' in package or 'test' in package:
            continue

       sub_packages = (spackage for spackage, spackage_path in package_folder if spackage != package and spackage.startswith(package))
       paths = [os.path.join(*spackage.rsplit('.', 2)[-2:None])+'.rst' for spackage in sub_packages]
       paths.extend( [os.path.join(os.path.basename(os.path.dirname(path)), os.path.splitext(os.path.basename(path))[0]) for imp_name, path in module_folders.get(package, [])])
       paths.sort()
       doc = auto_doc_package(package_path, package, root_package, paths)

       package_dir = out_dir + package.replace('.', os.path.sep)
       if not os.path.exists(package_dir):
           os.makedirs(out_dir + package.replace('.', os.path.sep))

       out_path = package_dir + '.rst'
       if not os.path.exists(out_path) or doc != ''.join(file(out_path, 'r').readlines()):
            print 'creating out of date/non-existant document %s' % out_path
            file(out_path, 'w').write(doc)

       for import_name, module_path in module_folders.get(package, []):
         doc = auto_doc_module(module_path, import_name, root_package)
         out_path = out_dir + import_name.replace('.', os.path.sep) + '.rst'
         if not os.path.exists(out_path) or doc != ''.join(file(out_path, 'r').readlines()):
            print 'creating out of date/non-existant document %s' % out_path
            file(out_path, 'w').write(doc)


def setup(app):
    app.connect('builder-inited', auto_package_build)
    app.add_config_value('autopackage_name', None, 'env')

########NEW FILE########
__FILENAME__ = gen_example_directory
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


"""
Generate the rst files for the examples
"""
import os
import re
import sys


def out_of_date(original, derived):
    """
    Returns True if derivative is out-of-date wrt original,
    both of which are full file paths.

    TODO: this check isn't adequate in some cases.  Eg, if we discover
    a bug when building the examples, the original and derived will be
    unchanged but we still want to force a rebuild.
    """
    return (not os.path.exists(derived) or
            os.stat(derived).st_mtime < os.stat(original).st_mtime)


docstring_regex = re.compile(r'[\'\"]{3}(.*?)[\'\"]{3}', re.DOTALL)


noplot_regex = re.compile(r"#\s*-\*-\s*noplot\s*-\*-")


def generate_example_rst(app):
    # example code can be found at the same level as the documentation src folder
    rootdir = os.path.join(os.path.dirname(app.builder.srcdir), 'example_code')

    # examples are build as a subfolder of the src folder
    exampledir = os.path.join(app.builder.srcdir, 'examples')

    if not os.path.exists(exampledir):
        os.makedirs(exampledir)

    datad = {}
    for root, subFolders, files in os.walk(rootdir):
        for fname in files:
            if ( fname.startswith('.') or fname.startswith('#') or fname.startswith('_') or
                 fname.find('.svn')>=0 or not fname.endswith('.py') ):
                continue

            fullpath = os.path.join(root,fname)
            contents = file(fullpath).read()
            # indent
            relpath = os.path.split(root)[-1]
            datad.setdefault(relpath, []).append((fullpath, fname, contents))

    subdirs = datad.keys()
    subdirs.sort()

    fhindex = file(os.path.join(exampledir, 'index.rst'), 'w')
    fhindex.write("""\
Iris examples
=============

.. toctree::
    :maxdepth: 2

""")

    for subdir in subdirs:
        rstdir = os.path.join(exampledir, subdir)
        if not os.path.exists(rstdir):
            os.makedirs(rstdir)

        outputdir = os.path.join(app.builder.outdir, 'examples')
        if not os.path.exists(outputdir):
            os.makedirs(outputdir)

        outputdir = os.path.join(outputdir, subdir)
        if not os.path.exists(outputdir):
            os.makedirs(outputdir)

        subdirIndexFile = os.path.join(rstdir, 'index.rst')
        fhsubdirIndex = file(subdirIndexFile, 'w')
        fhindex.write('    %s/index.rst\n\n' % subdir)

        subdir_root_path = os.path.join(rootdir, subdir)

        # use the __init__.py file's docstring for the subdir example page (if __init__ exists)
        if os.path.exists(os.path.join(subdir_root_path, '__init__.py')):
            import imp
            mod = imp.load_source(subdir, os.path.join(subdir_root_path, '__init__.py'))
            fhsubdirIndex.writelines(mod.__doc__)
        else:
            fhsubdirIndex.writelines(['Examples in %s\n' % subdir, '='*50])

        # append the code to produce the toctree
        fhsubdirIndex.write("""\

.. toctree::
    :maxdepth: 1

""")

        sys.stdout.write(subdir + ", ")
        sys.stdout.flush()

        data = datad[subdir]
        data.sort()

        for fullpath, fname, contents in data:
            basename, ext = os.path.splitext(fname)
            outputfile = os.path.join(outputdir, fname)
            #thumbfile = os.path.join(thumb_dir, '%s.png'%basename)
            #print '    static_dir=%s, basename=%s, fullpath=%s, fname=%s, thumb_dir=%s, thumbfile=%s'%(static_dir, basename, fullpath, fname, thumb_dir, thumbfile)

            rstfile = '%s.rst'%basename
            outrstfile = os.path.join(rstdir, rstfile)

            fhsubdirIndex.write('    %s\n'%rstfile)

            if not out_of_date(fullpath, outrstfile):
                continue

            fh = file(outrstfile, 'w')
            fh.write('.. _%s-%s:\n\n'%(subdir, basename))

            docstring_results = docstring_regex.search(contents)
            if docstring_results is not None:
                fh.write( docstring_results.group(1) )
            else:
                title = '%s example code: %s'%(subdir, fname)
                #title = '<img src=%s> %s example code: %s'%(thumbfile, subdir, fname)
                fh.write(title + '\n')
                fh.write('='*len(title) + '\n\n')

            if not noplot_regex.search(contents):
                fh.write("\n\n.. plot:: %s\n\n::\n\n" % fullpath)
            else:
                fh.write("[`source code <%s>`_]\n\n::\n\n" % fname)
                # write the py file contents (we didnt need to do this for plot as the plot directive does this for us.
                fhstatic = file(outputfile, 'w')
                fhstatic.write(contents)
                fhstatic.close()

            # indent the contents
            contents = '\n'.join(['    %s'%row.rstrip() for row in contents.split('\n')])
            fh.write(contents)
            fh.write('\n\n\n\n\n')

            fh.close()

        fhsubdirIndex.close()

    fhindex.close()

def setup(app):
    app.connect('builder-inited', generate_example_rst)

########NEW FILE########
__FILENAME__ = gen_gallery
#
#    (C) Copyright 2010 MATPLOTLIB (vn 1.0.1)
#

# generate a thumbnail gallery of examples
template = """\
{%% extends "layout.html" %%}
{%% set title = "Thumbnail gallery" %%}


{%% block body %%}

<h3>Click on any image to see full size image and source code</h3>
<br/>

%s
{%% endblock %%}
"""

import os, glob, re, warnings
import matplotlib.image as image

multiimage = re.compile('(.*)_\d\d')

def make_thumbnail(args):
    image.thumbnail(args[0], args[1], 0.4)

def out_of_date(original, derived):
    return (not os.path.exists(derived) or
            os.stat(derived).st_mtime < os.stat(original).st_mtime)

def gen_gallery(app, doctree):
    if app.builder.name != 'html':
        return

    outdir = app.builder.outdir

    rootdir = os.path.join('plot_directive','example_code')

    # images we want to skip for the gallery because they are an unusual
    # size that doesn't layout well in a table, or because they may be
    # redundant with other images or uninteresting
    skips = set([
        'mathtext_examples',
        'matshow_02',
        'matshow_03',
        'matplotlib_icon',
        ])

    data = []
    thumbnails = {}

    for subdir in ('graphics', ):
        origdir = os.path.join(os.path.dirname(outdir), rootdir, subdir)
        thumbdir = os.path.join(outdir, rootdir, subdir, 'thumbnails')
        if not os.path.exists(thumbdir):
            os.makedirs(thumbdir)
        for filename in sorted(glob.glob(os.path.join(origdir, '*.png'))):
            if filename.endswith("hires.png"):
                continue

            path, filename = os.path.split(filename)
            basename, ext = os.path.splitext(filename)
            if basename in skips:
                continue

            # Create thumbnails based on images in tmpdir, and place
            # them within the build tree
            orig_path = str(os.path.join(origdir, filename))
            thumb_path = str(os.path.join(thumbdir, filename))
            if out_of_date(orig_path, thumb_path) or True:
                thumbnails[orig_path] = thumb_path

            m = multiimage.match(basename)
            if m is None:
                pyfile = '%s.py'%basename
            else:
                basename = m.group(1)
                pyfile = '%s.py'%basename

            data.append((subdir, basename,
                         os.path.join(rootdir, subdir, 'thumbnails', filename)))

    link_template = """\
    <a href="%(href)s"><img src="%(thumb_file)s" border="0" alt="%(alternative_text)s"/></a>
    """

    random_image_content_template = '''
// This file was automatically generated by gen_gallery.py & should not be modified directly.

images = new Array();

%s

'''

    random_image_template = "['%(thumbfile)s', '%(full_image)s', '%(link)s', 'Iris examples.'];"
    random_image_join = 'images[%s] = %s'

    if len(data) == 0:
        warnings.warn("No thumbnails were found")
        return

    rows = []
    random_image = []
    for (subdir, basename, thumbfile) in data:
        if thumbfile is not None:
            link = 'examples/%s/%s.html#%s'%(subdir, basename, os.path.splitext(os.path.basename(thumbfile))[0].replace('_', '-'))
            rows.append(link_template % {'href': link, 'thumb_file': thumbfile, 'alternative_text': basename})
            random_image.append(random_image_template % {'link':link, 'thumbfile':thumbfile, 'basename':basename, 'full_image':'_images/' + os.path.basename(thumbfile)} )

    random_image_content = random_image_content_template % '\n'.join([random_image_join % (i, line) for i, line in enumerate(random_image)])
    random_image_script_path = os.path.join(app.builder.srcdir, '_static', 'random_image.js')
    file(random_image_script_path, 'w').write(random_image_content)


    # Only write out the file if the contents have actually changed.
    # Otherwise, this triggers a full rebuild of the docs
    content = template%'\n'.join(rows)
    gallery_path = os.path.join(app.builder.srcdir, '_templates', 'gallery.html')
    if os.path.exists(gallery_path):
        fh = file(gallery_path, 'r')
        regenerate = fh.read() != content
        fh.close()
    else:
        regenerate = True
    if regenerate:
        fh = file(gallery_path, 'w')
        fh.write(content)
        fh.close()

    try:
        import multiprocessing
        app.builder.info("generating thumbnails... ", nonl=True)
        pool = multiprocessing.Pool()
        pool.map(make_thumbnail, thumbnails.iteritems())
        pool.close()
        pool.join()
        app.builder.info("done")

    except ImportError:
        for key in app.builder.status_iterator(
            thumbnails.iterkeys(), "generating thumbnails... ",
            length=len(thumbnails)):
            image.thumbnail(key, thumbnails[key], 0.3)

def setup(app):
    app.connect('env-updated', gen_gallery)

########NEW FILE########
__FILENAME__ = gen_rst
#
#    (C) Copyright 2010 MATPLOTLIB (vn 1.0.1)
#
"""
generate the rst files for the examples by iterating over the pylab examples
"""
import os
import re
import sys
fileList = []

def out_of_date(original, derived):
    """
    Returns True if derivative is out-of-date wrt original,
    both of which are full file paths.

    TODO: this check isn't adequate in some cases.  Eg, if we discover
    a bug when building the examples, the original and derived will be
    unchanged but we still want to force a rebuild.
    """
    return (not os.path.exists(derived) or
            os.stat(derived).st_mtime < os.stat(original).st_mtime)

noplot_regex = re.compile(r"#\s*-\*-\s*noplot\s*-\*-")

def generate_example_rst(app):
    rootdir = os.path.join(os.path.dirname(app.builder.srcdir), 'example_code')
    exampledir = os.path.join(app.builder.srcdir, 'examples')
    if not os.path.exists(exampledir):
        os.makedirs(exampledir)

    datad = {}
    for root, subFolders, files in os.walk(rootdir):
        for fname in files:
            if ( fname.startswith('.') or fname.startswith('#') or fname.startswith('_') or
                 fname.find('.svn')>=0 or not fname.endswith('.py') ):
                continue

            fullpath = os.path.join(root,fname)
            contents = file(fullpath).read()
            # indent
            relpath = os.path.split(root)[-1]
            datad.setdefault(relpath, []).append((fullpath, fname, contents))

    subdirs = datad.keys()
    subdirs.sort()

    fhindex = file(os.path.join(exampledir, 'index.rst'), 'w')
    fhindex.write("""\
.. _examples-index:

####################
Iris Examples
####################

.. toctree::
    :maxdepth: 2

""")

    for subdir in subdirs:
        rstdir = os.path.join(exampledir, subdir)
        if not os.path.exists(rstdir):
            os.makedirs(rstdir)

        outputdir = os.path.join(app.builder.outdir, 'examples')
        if not os.path.exists(outputdir):
            os.makedirs(outputdir)

        outputdir = os.path.join(outputdir, subdir)
        if not os.path.exists(outputdir):
            os.makedirs(outputdir)

        subdirIndexFile = os.path.join(rstdir, 'index.rst')
        fhsubdirIndex = file(subdirIndexFile, 'w')
        fhindex.write('    %s/index.rst\n\n'%subdir)

        fhsubdirIndex.write("""\
.. _%s-examples-index:

##############################################
%s Examples
##############################################

.. toctree::
    :maxdepth: 1

"""%(subdir, subdir))

        sys.stdout.write(subdir + ", ")
        sys.stdout.flush()

        data = datad[subdir]
        data.sort()

        for fullpath, fname, contents in data:
            basename, ext = os.path.splitext(fname)
            outputfile = os.path.join(outputdir, fname)
            #thumbfile = os.path.join(thumb_dir, '%s.png'%basename)
            #print '    static_dir=%s, basename=%s, fullpath=%s, fname=%s, thumb_dir=%s, thumbfile=%s'%(static_dir, basename, fullpath, fname, thumb_dir, thumbfile)

            rstfile = '%s.rst'%basename
            outrstfile = os.path.join(rstdir, rstfile)

            fhsubdirIndex.write('    %s\n'%rstfile)

            if not out_of_date(fullpath, outrstfile):
                continue

            fh = file(outrstfile, 'w')
            fh.write('.. _%s-%s:\n\n'%(subdir, basename))
            title = '%s example code: %s'%(subdir, fname)
            #title = '<img src=%s> %s example code: %s'%(thumbfile, subdir, fname)


            fh.write(title + '\n')
            fh.write('='*len(title) + '\n\n')

            do_plot = (subdir in ('graphics',
                                  ) and
                       not noplot_regex.search(contents))

            if do_plot:
                fh.write("\n\n.. plot:: %s\n\n::\n\n" % fullpath)
            else:
                fh.write("[`source code <%s>`_]\n\n::\n\n" % fname)
                fhstatic = file(outputfile, 'w')
                fhstatic.write(contents)
                fhstatic.close()

            # indent the contents
            contents = '\n'.join(['    %s'%row.rstrip() for row in contents.split('\n')])
            fh.write(contents)

            #fh.write('\n\nKeywords: python, matplotlib, pylab, example, codex (see :ref:`how-to-search-examples`)')
            fh.write('\n\n')

            fh.close()

        fhsubdirIndex.close()

    fhindex.close()

    print

def setup(app):
    app.connect('builder-inited', generate_example_rst)

########NEW FILE########
__FILENAME__ = plot_directive
#
#    (C) Copyright 2010 MATPLOTLIB (vn 1.0.1)
#

"""A special directive for including a matplotlib plot.

The source code for the plot may be included in one of two ways:

  1. A path to a source file as the argument to the directive::

       .. plot:: path/to/plot.py

     When a path to a source file is given, the content of the
     directive may optionally contain a caption for the plot::

       .. plot:: path/to/plot.py

          This is the caption for the plot

     Additionally, one my specify the name of a function to call (with
     no arguments) immediately after importing the module::

       .. plot:: path/to/plot.py plot_function1

  2. Included as inline content to the directive::

     .. plot::

        import matplotlib.pyplot as plt
        import matplotlib.image as mpimg
        import numpy as np
        img = mpimg.imread('_static/stinkbug.png')
        imgplot = plt.imshow(img)

In HTML output, `plot` will include a .png file with a link to a high-res
.png and .pdf.  In LaTeX output, it will include a .pdf.

To customize the size of the plot, this directive supports all of the
options of the `image` directive, except for `target` (since plot will
add its own target).  These include `alt`, `height`, `width`, `scale`,
`align` and `class`.

Additionally, if the `:include-source:` option is provided, the
literal source will be displayed inline in the text, (as well as a
link to the source in HTML).  If this source file is in a non-UTF8 or
non-ASCII encoding, the encoding must be specified using the
`:encoding:` option.

The set of file formats to generate can be specified with the
`plot_formats` configuration variable.


Error handling:

Any errors generated during the running of the code are emitted as warnings
using the Python `warnings` module, using a custom category called
`PlotWarning`.  To turn the warnings into fatal errors that stop the
documentation build, after adjusting your `sys.path` in your `conf.py` Sphinx
configuration file, use::

    import plot_directive
    warnings.simplefilter('error', plot_directive.PlotWarning)
"""

import sys, os, shutil, imp, warnings, cStringIO, re
try:
    from hashlib import md5
except ImportError:
    from md5 import md5

from docutils.parsers.rst import directives
try:
    # docutils 0.4
    from docutils.parsers.rst.directives.images import align
except ImportError:
    # docutils 0.5
    from docutils.parsers.rst.directives.images import Image
    align = Image.align
import sphinx

sphinx_version = sphinx.__version__.split(".")
# The split is necessary for sphinx beta versions where the string is
# '6b1'
sphinx_version = tuple([int(re.split('[a-z]', x)[0])
                        for x in sphinx_version[:2]])

import matplotlib
import matplotlib.cbook as cbook
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import matplotlib.image as image
from matplotlib import _pylab_helpers


class PlotWarning(Warning):
    """Warning category for all warnings generated by this directive.

    By printing our warnings with this category, it becomes possible to turn
    them into errors by using in your conf.py::

      warnings.simplefilter('error', plot_directive.PlotWarning)

    This way, you can ensure that your docs only build if all your examples
    actually run successfully.
    """
    pass


# os.path.relpath is new in Python 2.6
if hasattr(os.path, 'relpath'):
    relpath = os.path.relpath
else:
    # This code is snagged from Python 2.6

    def relpath(target, base=os.curdir):
        """
        Return a relative path to the target from either the current dir or an optional base dir.
        Base can be a directory specified either as absolute or relative to current dir.
        """

        if not os.path.exists(target):
            raise OSError, 'Target does not exist: '+target

        if not os.path.isdir(base):
            raise OSError, 'Base is not a directory or does not exist: '+base

        base_list = (os.path.abspath(base)).split(os.sep)
        target_list = (os.path.abspath(target)).split(os.sep)

        # On the windows platform the target may be on a completely
        # different drive from the base.
        if os.name in ['nt','dos','os2'] and base_list[0] <> target_list[0]:
            raise OSError, 'Target is on a different drive to base. Target: '+target_list[0].upper()+', base: '+base_list[0].upper()

        # Starting from the filepath root, work out how much of the
        # filepath is shared by base and target.
        for i in range(min(len(base_list), len(target_list))):
            if base_list[i] <> target_list[i]: break
        else:
            # If we broke out of the loop, i is pointing to the first
            # differing path elements.  If we didn't break out of the
            # loop, i is pointing to identical path elements.
            # Increment i so that in all cases it points to the first
            # differing path elements.
            i+=1

        rel_list = [os.pardir] * (len(base_list)-i) + target_list[i:]
        if rel_list:
            return os.path.join(*rel_list)
        else:
            return ""

template = """
.. htmlonly::

   %(links)s

   .. figure:: %(prefix)s%(tmpdir)s/%(outname)s.png
%(options)s

%(caption)s

.. latexonly::
   .. figure:: %(prefix)s%(tmpdir)s/%(outname)s.pdf
%(options)s

%(caption)s

"""

exception_template = """
.. htmlonly::

   [`source code <%(linkdir)s/%(basename)s.py>`__]

Exception occurred rendering plot.

"""

template_content_indent = '      '

def out_of_date(original, derived):
    """
    Returns True if derivative is out-of-date wrt original,
    both of which are full file paths.
    """
    return (not os.path.exists(derived) or
            (os.path.exists(original) and
             os.stat(derived).st_mtime < os.stat(original).st_mtime))

def run_code(plot_path, function_name, plot_code):
    """
    Import a Python module from a path, and run the function given by
    name, if function_name is not None.
    """
    # Change the working directory to the directory of the example, so
    # it can get at its data files, if any.  Add its path to sys.path
    # so it can import any helper modules sitting beside it.
    if plot_code is not None:
#        exec_code = 'import numpy as np; import matplotlib.pyplot as plt\n%s'%plot_code
        exec_code = plot_code
        exec(exec_code)
    else:
        pwd = os.getcwd()
        path, fname = os.path.split(plot_path)
        sys.path.insert(0, os.path.abspath(path))
        stdout = sys.stdout
        sys.stdout = cStringIO.StringIO()
        os.chdir(path)
        fd = None
        try:
            fd = open(fname)
            module = imp.load_module(
                "__main__", fd, fname, ('py', 'r', imp.PY_SOURCE))
        finally:
            del sys.path[0]
            os.chdir(pwd)
            sys.stdout = stdout
            if fd is not None:
                fd.close()

        if function_name is not None:
            getattr(module, function_name)()

def run_savefig(plot_path, basename, tmpdir, destdir, formats):
    """
    Once a plot script has been imported, this function runs savefig
    on all of the figures in all of the desired formats.
    """
    fig_managers = _pylab_helpers.Gcf.get_all_fig_managers()
    for i, figman in enumerate(fig_managers):
        for j, (format, dpi) in enumerate(formats):
            if len(fig_managers) == 1:
                outname = basename
            else:
                outname = "%s_%02d" % (basename, i)
            outname = outname + "." + format
            outpath = os.path.join(tmpdir, outname)
            try:
                figman.canvas.figure.savefig(outpath, dpi=dpi)
            except:
                s = cbook.exception_to_str("Exception saving plot %s" % plot_path)
                warnings.warn(s, PlotWarning)
                return 0
            if j > 0:
                shutil.copyfile(outpath, os.path.join(destdir, outname))

    return len(fig_managers)

def clear_state():
    plt.close('all')
    matplotlib.rcdefaults()
    # Set a default figure size that doesn't overflow typical browser
    # windows.  The script is free to override it if necessary.
    matplotlib.rcParams['figure.figsize'] = (8, 6.5)

def render_figures(plot_path, function_name, plot_code, tmpdir, destdir,
                   formats):
    """
    Run a pyplot script and save the low and high res PNGs and a PDF
    in outdir.
    """
    plot_path = str(plot_path)  # todo, why is unicode breaking this
    basedir, fname = os.path.split(plot_path)
    basename, ext = os.path.splitext(fname)

    all_exists = True

    # Look for single-figure output files first
    for format, dpi in formats:
        outname = os.path.join(tmpdir, '%s.%s' % (basename, format))
        if out_of_date(plot_path, outname):
            all_exists = False
            break

    if all_exists:
        return 1

    # Then look for multi-figure output files, assuming
    # if we have some we have all...
    i = 0
    while True:
        all_exists = True
        for format, dpi in formats:
            outname = os.path.join(
                tmpdir, '%s_%02d.%s' % (basename, i, format))
            if out_of_date(plot_path, outname):
                all_exists = False
                break
        if all_exists:
            i += 1
        else:
            break

    if i != 0:
        return i

    # We didn't find the files, so build them

    clear_state()
    try:
        run_code(plot_path, function_name, plot_code)
    except:
        s = cbook.exception_to_str("Exception running plot %s" % plot_path)
        warnings.warn(s, PlotWarning)
        return 0

    num_figs = run_savefig(plot_path, basename, tmpdir, destdir, formats)

    if '__plot__' in sys.modules:
        del sys.modules['__plot__']

    return num_figs

def _plot_directive(plot_path, basedir, function_name, plot_code, caption,
                    options, state_machine):
    formats = setup.config.plot_formats
    if type(formats) == str:
        formats = eval(formats)

    fname = os.path.basename(plot_path)
    basename, ext = os.path.splitext(fname)

    # Get the directory of the rst file, and determine the relative
    # path from the resulting html file to the plot_directive links
    # (linkdir).  This relative path is used for html links *only*,
    # and not the embedded image.  That is given an absolute path to
    # the temporary directory, and then sphinx moves the file to
    # build/html/_images for us later.
    rstdir, rstfile = os.path.split(state_machine.document.attributes['source'])
    outdir = os.path.join('plot_directive', basedir)
    reldir = relpath(setup.confdir, rstdir)
    linkdir = os.path.join(reldir, outdir)

    # tmpdir is where we build all the output files.  This way the
    # plots won't have to be redone when generating latex after html.

    # Prior to Sphinx 0.6, absolute image paths were treated as
    # relative to the root of the filesystem.  0.6 and after, they are
    # treated as relative to the root of the documentation tree.  We
    # need to support both methods here.
    tmpdir = os.path.join('build', outdir)
    tmpdir = os.path.join(os.path.dirname(setup.app.builder.outdir), 'plot_directive')
    tmpdir = os.path.join(tmpdir, outdir)
    tmpdir = os.path.abspath(tmpdir)
    if sphinx_version < (0, 6):
        prefix = ''
    else:
        prefix = '/'
    if not os.path.exists(tmpdir):
        cbook.mkdirs(tmpdir)

    # destdir is the directory within the output to store files
    # that we'll be linking to -- not the embedded images.
    destdir = os.path.abspath(os.path.join(setup.app.builder.outdir, outdir))
    if not os.path.exists(destdir):
        cbook.mkdirs(destdir)

    # Properly indent the caption
    caption = '\n'.join(template_content_indent + line.strip()
                        for line in caption.split('\n'))

    # Generate the figures, and return the number of them
    num_figs = render_figures(plot_path, function_name, plot_code, tmpdir,
                              destdir, formats)

    # Now start generating the lines of output
    lines = []

    if plot_code is None:
        shutil.copyfile(plot_path, os.path.join(destdir, fname))

    if options.has_key('include-source'):
        if plot_code is None:
            if sphinx_version > (1,):
                include_prefix = '/'
            else:
                include_prefix = setup.app.builder.srcdir

            lines.extend(
                ['.. include:: %s' % os.path.join(include_prefix, plot_path),
                 '    :literal:'])
            if options.has_key('encoding'):
                lines.append('    :encoding: %s' % options['encoding'])
                del options['encoding']
        else:
            lines.extend(['::', ''])
            lines.extend(['    %s' % row.rstrip()
                          for row in plot_code.split('\n')])
        lines.append('')
        del options['include-source']
    else:
        lines = []

    if num_figs > 0:
        options = ['%s:%s: %s' % (template_content_indent, key, val)
                   for key, val in options.items()]
        options = "\n".join(options)

        for i in range(num_figs):
            if num_figs == 1:
                outname = basename
            else:
                outname = "%s_%02d" % (basename, i)

            # Copy the linked-to files to the destination within the build tree,
            # and add a link for them
            links = []
            for format, dpi in formats[1:]:
                links.append('`%s <%s/%s.%s>`__' % (format, linkdir, outname, format))
            if plot_code is None:
                links.append('`source code <%(linkdir)s/%(basename)s.py>`__')
            if len(links):
                links = ('.. _%s: \n\n' % outname) + '[%s]' % (', '.join(links) % locals())
            else:
                links = ''

            lines.extend((template % locals()).split('\n'))
    else:
        lines.extend((exception_template % locals()).split('\n'))

    if len(lines):
        state_machine.insert_input(
            lines, state_machine.input_lines.source(0))

    return []

def plot_directive(name, arguments, options, content, lineno,
                   content_offset, block_text, state, state_machine):
    """
    Handle the arguments to the plot directive.  The real work happens
    in _plot_directive.
    """
    # The user may provide a filename *or* Python code content, but not both
    if len(arguments):
        plot_path = directives.uri(arguments[0])
        basedir = relpath(os.path.dirname(plot_path), setup.app.builder.srcdir)

        # If there is content, it will be passed as a caption.

        # Indent to match expansion below.  XXX - The number of spaces matches
        # that of the 'options' expansion further down.  This should be moved
        # to common code to prevent them from diverging accidentally.
        caption = '\n'.join(content)

        # If the optional function name is provided, use it
        if len(arguments) == 2:
            function_name = arguments[1]
        else:
            function_name = None

        return _plot_directive(plot_path, basedir, function_name, None, caption,
                               options, state_machine)
    else:
        plot_code = '\n'.join(content)

        # Since we don't have a filename, use a hash based on the content
        plot_path = md5(plot_code).hexdigest()[-10:]

        return _plot_directive(plot_path, 'inline', None, plot_code, '', options,
                               state_machine)

def mark_plot_labels(app, document):
    """
    To make plots referenceable, we need to move the reference from
    the "htmlonly" (or "latexonly") node to the actual figure node
    itself.
    """
    for name, explicit in document.nametypes.iteritems():
        if not explicit:
            continue
        labelid = document.nameids[name]
        if labelid is None:
            continue
        node = document.ids[labelid]
        if node.tagname in ('html_only', 'latex_only'):
            for n in node:
                if n.tagname == 'figure':
                    sectname = name
                    for c in n:
                        if c.tagname == 'caption':
                            sectname = c.astext()
                            break

                    node['ids'].remove(labelid)
                    node['names'].remove(name)
                    n['ids'].append(labelid)
                    n['names'].append(name)
                    document.settings.env.labels[name] = \
                        document.settings.env.docname, labelid, sectname
                    break

def setup(app):
    setup.app = app
    setup.config = app.config
    setup.confdir = app.confdir

    options = {'alt': directives.unchanged,
               'height': directives.length_or_unitless,
               'width': directives.length_or_percentage_or_unitless,
               'scale': directives.nonnegative_int,
               'align': align,
               'class': directives.class_option,
               'include-source': directives.flag,
               'encoding': directives.encoding }

    app.add_directive('plot', plot_directive, True, (0, 2, 0), **options)
    app.add_config_value(
        'plot_formats',
        [('png', 100), ('hires.png', 200), ('pdf', 250)],
        True)

    app.connect('doctree-read', mark_plot_labels)

########NEW FILE########
__FILENAME__ = 1d_quickplot_simple
import matplotlib.pyplot as plt

import iris
import iris.quickplot as qplt


fname = iris.sample_data_path('air_temp.pp')
temperature = iris.load_cube(fname)

# Take a 1d slice using array style indexing.
temperature_1d = temperature[5, :]

qplt.plot(temperature_1d)
plt.show()

########NEW FILE########
__FILENAME__ = 1d_simple
import matplotlib.pyplot as plt

import iris
import iris.plot as iplt


fname = iris.sample_data_path('air_temp.pp')
temperature = iris.load_cube(fname)

# Take a 1d slice using array style indexing.
temperature_1d = temperature[5, :]

iplt.plot(temperature_1d)
plt.show()

########NEW FILE########
__FILENAME__ = 1d_with_legend
import matplotlib.pyplot as plt

import iris
import iris.plot as iplt


fname = iris.sample_data_path('air_temp.pp')

# Load exactly one cube from the given file
temperature = iris.load_cube(fname)

# We are only interested in a small number of longitudes (the 4 after and including the 5th element), so index them out
temperature = temperature[5:9, :]

for cube in temperature.slices('longitude'):

    # Create a string label to identify this cube (i.e. latitude: value)
    cube_label = 'latitude: %s' % cube.coord('latitude').points[0]

    # Plot the cube, and associate it with a label
    iplt.plot(cube, label=cube_label )

#match the longitude range to global
max_lon = temperature.coord('longitude').points.max()
min_lon = temperature.coord('longitude').points.min()
plt.xlim(min_lon, max_lon)

# Add the legend with 2 columns
plt.legend(ncol=2)

# Put a grid on the plot
plt.grid(True)

# Provide some axis labels
plt.ylabel('Temerature / kelvin')
plt.xlabel('Longitude / degrees')

# And a sensible title
plt.suptitle('Air Temperature', fontsize=20, y=0.9)

# Finally, show it.
plt.show()

########NEW FILE########
__FILENAME__ = brewer
import matplotlib.pyplot as plt
import numpy as np

import iris.palette


a = np.linspace(0, 1, 256).reshape(1,-1)
a = np.vstack((a,a))

maps = sorted(iris.palette.CMAP_BREWER)
nmaps = len(maps)

fig = plt.figure(figsize=(7, 10))
fig.subplots_adjust(top=0.99, bottom=0.01, left=0.2, right=0.99)
for i,m in enumerate(maps):
    ax = plt.subplot(nmaps, 1, i+1)
    plt.axis("off")
    plt.imshow(a, aspect='auto', cmap=plt.get_cmap(m), origin='lower')
    pos = list(ax.get_position().bounds)
    fig.text(pos[0] - 0.01, pos[1], m, fontsize=8,
             horizontalalignment='right')

plt.show()

########NEW FILE########
__FILENAME__ = cube_blockplot
import matplotlib.pyplot as plt

import iris
import iris.quickplot as qplt


# Load the data for a single value of model level number.
fname = iris.sample_data_path('hybrid_height.nc')
temperature_cube = iris.load_cube(
    fname, iris.Constraint(model_level_number=1))

# Draw the block plot.
qplt.pcolormesh(temperature_cube)

plt.show()

########NEW FILE########
__FILENAME__ = cube_brewer_cite_contourf
import matplotlib.cm as mpl_cm
import matplotlib.pyplot as plt

import iris
import iris.quickplot as qplt
import iris.plot as iplt


fname = iris.sample_data_path('air_temp.pp')
temperature_cube = iris.load_cube(fname)

# Load a Cynthia Brewer palette.
brewer_cmap = mpl_cm.get_cmap('brewer_Purples_09')

# Draw the contours, with n-levels set for the map colours (9).
# NOTE: needed as the map is non-interpolated, but matplotlib does not provide
# any special behaviour for these.
qplt.contourf(temperature_cube, brewer_cmap.N, cmap=brewer_cmap)

# Add a citation to the plot.
iplt.citation(iris.plot.BREWER_CITE)

# Add coastlines to the map created by contourf.
plt.gca().coastlines()

plt.show()

########NEW FILE########
__FILENAME__ = cube_brewer_contourf
import matplotlib.cm as mpl_cm
import matplotlib.pyplot as plt

import iris
import iris.quickplot as qplt

fname = iris.sample_data_path('air_temp.pp')
temperature_cube = iris.load_cube(fname)

# Load a Cynthia Brewer palette.
brewer_cmap = mpl_cm.get_cmap('brewer_OrRd_09')

# Draw the contours, with n-levels set for the map colours (9).
# NOTE: needed as the map is non-interpolated, but matplotlib does not provide
# any special behaviour for these.
qplt.contourf(temperature_cube, brewer_cmap.N, cmap=brewer_cmap)

# Add coastlines to the map created by contourf.
plt.gca().coastlines()

plt.show()

########NEW FILE########
__FILENAME__ = cube_contour
import matplotlib.pyplot as plt

import iris
import iris.quickplot as qplt


fname = iris.sample_data_path('air_temp.pp')
temperature_cube = iris.load_cube(fname)

# Add a contour, and put the result in a variable called contour.
contour = qplt.contour(temperature_cube)

# Add coastlines to the map created by contour.
plt.gca().coastlines()

# Add contour labels based on the contour we have just created.
plt.clabel(contour)

plt.show()

########NEW FILE########
__FILENAME__ = cube_contourf
import matplotlib.pyplot as plt

import iris
import iris.quickplot as qplt


fname = iris.sample_data_path('air_temp.pp')
temperature_cube = iris.load_cube(fname)

# Draw the contour with 25 levels.
qplt.contourf(temperature_cube, 25)

# Add coastlines to the map created by contourf.
plt.gca().coastlines()

plt.show()

########NEW FILE########
__FILENAME__ = calculus
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Calculus operations on :class:`iris.cube.Cube` instances.

See also: :mod:`NumPy <numpy>`.

"""

from __future__ import division
import re
import warnings

import numpy as np

import iris.cube
import iris.coords
import iris.coord_systems
import iris.analysis
import iris.analysis.maths
import iris.analysis.cartography
from iris.util import delta


__all__ = ['cube_delta', 'differentiate', 'curl']


def _construct_delta_coord(coord):
    """
    Return a coordinate of deltas between the given coordinate's points. If the original coordinate has length n
    and is circular then the result will be a coordinate of length n, otherwise the result will be
    of length n-1.

    """
    if coord.ndim != 1:
        raise iris.exceptions.CoordinateMultiDimError(coord)
    circular = getattr(coord, 'circular', False)
    if coord.shape == (1,) and not circular:
        raise ValueError('Cannot take interval differences of a single valued coordinate.')

    if circular:
        circular_kwd = coord.units.modulus or True
    else:
        circular_kwd = False

    if coord.bounds is not None:
        bounds = iris.util.delta(coord.bounds, 0, circular=circular_kwd)
    else:
        bounds = None

    points = iris.util.delta(coord.points, 0, circular=circular_kwd)
    new_coord = iris.coords.AuxCoord.from_coord(coord).copy(points, bounds)
    new_coord.rename('change_in_%s' % new_coord.name())

    return new_coord


def _construct_midpoint_coord(coord, circular=None):
    """
    Return a coordinate of mid-points from the given coordinate. If the given coordinate has length n
    and the circular flag set then the result will be a coordinate of length n, otherwise the result will be
    of length n-1.

    """
    if circular and not hasattr(coord, 'circular'):
        raise ValueError("Cannot produce circular midpoint from a coord without the circular attribute ")

    if circular is None:
        circular = getattr(coord, 'circular', False)
    elif circular != getattr(coord, 'circular', False):
        warnings.warn("circular flag and Coord.circular attribute do not match")

    if coord.ndim != 1:
        raise iris.exceptions.CoordinateMultiDimError(coord)
    if coord.shape == (1,) and not circular:
        raise ValueError('Cannot take the midpoints of a single valued coordinate.')

    # Calculate the delta of the coordinate (this deals with circularity nicely)
    mid_point_coord = _construct_delta_coord(coord)

    # if the coord is circular then include the last one, else, just take 0:-1
    circular_slice = slice(0, -1 if not circular else None)

    if coord.bounds is not None:
        axis_delta = mid_point_coord.bounds
        mid_point_bounds = axis_delta * 0.5 + coord.bounds[circular_slice, :]
    else:
        mid_point_bounds = None

    # Get the deltas
    axis_delta = mid_point_coord.points
    # Add half of the deltas to the original points
    # if the coord is circular then include the last one, else, just take 0:-1
    mid_point_points = axis_delta * 0.5 + coord.points[circular_slice]

    try: # try creating a coordinate of the same type as before, otherwise, make an AuxCoord
        mid_point_coord = coord.from_coord(coord).copy(mid_point_points, mid_point_bounds)
    except ValueError:
        mid_point_coord = iris.coords.AuxCoord.from_coord(coord).copy(mid_point_points, mid_point_bounds)

    return mid_point_coord


def cube_delta(cube, coord):
    """
    Given a cube calculate the difference between each value in the given coord's direction.


    Args:

    * coord
        either a Coord instance or the unique name of a coordinate in the cube.
        If a Coord instance is provided, it does not necessarily have to exist in the cube.

    Example usage::

        change_in_temperature_wrt_pressure = cube_delta(temperature_cube, 'pressure')

    .. note:: Missing data support not yet implemented.

    """
    # handle the case where a user passes a coordinate name
    if isinstance(coord, basestring):
        coord = cube.coord(coord)

    if coord.ndim != 1:
        raise iris.exceptions.CoordinateMultiDimError(coord)

    # Try and get a coord dim
    delta_dims = cube.coord_dims(coord.name())
    if (coord.shape[0] == 1 and not getattr(coord, 'circular', False)) or not delta_dims:
        raise ValueError('Cannot calculate delta over "%s" as it has length of 1.' % coord.name())
    delta_dim = delta_dims[0]

    # Calculate the actual delta, taking into account whether the given coordinate is circular
    delta_cube_data = delta(cube.data, delta_dim, circular=getattr(coord, 'circular', False))

    # If the coord/dim is circular there is no change in cube shape
    if getattr(coord, 'circular', False):
        delta_cube = cube.copy(data=delta_cube_data)
    else:
        # Subset the cube to the appropriate new shape by knocking off the last row of the delta dimension
        subset_slice = [slice(None,None)] * cube.ndim
        subset_slice[delta_dim] = slice(None, -1)
        delta_cube = cube[tuple(subset_slice)]
        delta_cube.data = delta_cube_data

    # Replace the delta_dim coords with midpoints (no shape change if circular).
    for cube_coord in cube.coords(dimensions=delta_dim):
        delta_cube.replace_coord(_construct_midpoint_coord(cube_coord, circular=getattr(coord, 'circular', False)))

    delta_cube.rename('change_in_%s_wrt_%s' % (delta_cube.name(), coord.name()))

    return delta_cube


def differentiate(cube, coord_to_differentiate):
    r"""
    Calculate the differential of a given cube with respect to the coord_to_differentiate.

    Args:

    * coord_to_differentiate:
        Either a Coord instance or the unique name of a coordinate which exists in the cube.
        If a Coord instance is provided, it does not necessarily have to exist on the cube.

    Example usage::

        u_wind_acceleration = differentiate(u_wind_cube, 'forecast_time')

    The algorithm used is equivalent to:

    .. math::

        d_i = \frac{v_{i+1}-v_i}{c_{i+1}-c_i}

    Where ``d`` is the differential, ``v`` is the data value, ``c`` is the coordinate value and ``i`` is the index in the differential
    direction. Hence, in a normal situation if a cube has a shape (x: n; y: m) differentiating with respect to x will result in a cube
    of shape (x: n-1; y: m) and differentiating with respect to y will result in (x: n; y: m-1). If the coordinate to differentiate is
    :attr:`circular <iris.coords.DimCoord.circular>` then the resultant shape will be the same as the input cube.


    .. note:: Difference method used is the same as :func:`cube_delta` and therefore has the same limitations.

    .. note:: Spherical differentiation does not occur in this routine.

    """
    # Get the delta cube in the required differential direction.
    # This operation results in a copy of the original cube.
    delta_cube = cube_delta(cube, coord_to_differentiate)

    if isinstance(coord_to_differentiate, basestring):
        coord = cube.coord(coord_to_differentiate)
    else:
        coord = coord_to_differentiate

    delta_coord = _construct_delta_coord(coord)
    delta_dim = cube.coord_dims(coord.name())[0]

    # calculate delta_cube / delta_coord to give the differential.
    delta_cube = iris.analysis.maths.divide(delta_cube, delta_coord, delta_dim)

    # Update the standard name
    delta_cube.rename(('derivative_of_%s_wrt_%s' % (cube.name(), coord.name())) )
    return delta_cube


def _curl_subtract(a, b):
    """
    Simple wrapper to :func:`iris.analysis.maths.subtract` to subtract two cubes, which deals with None in a way that makes sense in the context of curl.

    """
    # We are definitely dealing with cubes or None - otherwise we have a programmer error...
    assert isinstance(a, iris.cube.Cube) or a is None
    assert isinstance(b, iris.cube.Cube) or b is None

    if a is None and b is None:
        return None
    elif a is None:
        c = b.copy(data = 0 - b.data)
        return c
    elif b is None:
        return a.copy()
    else:
        return iris.analysis.maths.subtract(a, b)


def _curl_differentiate(cube, coord):
    """
    Simple wrapper to :func:`differentiate` to differentiate a cube and deal with None in a way that makes sense in the context of curl.

    """
    # We are definitely dealing with cubes/coords or None - otherwise we have a programmer error...
    assert isinstance(cube, iris.cube.Cube) or cube is None
    assert isinstance(coord, iris.coords.Coord) or coord is None

    if cube is None:
        return None
    if coord.ndim != 1:
        raise iris.exceptions.CoordinateMultiDimError(coord)
    if coord.shape[0] <= 1:
        return None

    return differentiate(cube, coord)


def _curl_regrid(cube, prototype):
    """
    Simple wrapper to :ref`iris.cube.Cube.regridded` to deal with None in a way that makes sense in the context of curl.

    """
    # We are definitely dealing with cubes or None - otherwise we have a programmer error...
    assert isinstance(cube, iris.cube.Cube) or cube is None
    assert isinstance(prototype, iris.cube.Cube)

    if cube is None:
        return None
    # #301 use of resample would be better here.
    return cube.regridded(prototype)


def _copy_cube_transformed(src_cube, data, coord_func):
    """
    Returns a new cube based on the src_cube, but with the given data,
    and with the coordinates transformed via coord_func.

    The data must have the same number of dimensions as the source cube.

    """
    assert src_cube.ndim == data.ndim

    # Start with just the metadata and the data...
    new_cube = iris.cube.Cube(data)
    new_cube.metadata = src_cube.metadata

    # ... and then create all the coordinates.

    # Record a mapping from old coordinate IDs to new coordinates,
    # for subsequent use in creating updated aux_factories.
    coord_mapping = {}

    def copy_coords(source_coords, add_method):
        for coord in source_coords:
            new_coord = coord_func(coord)
            add_method(new_coord, src_cube.coord_dims(coord))
            coord_mapping[id(coord)] = new_coord

    copy_coords(src_cube.dim_coords, new_cube.add_dim_coord)
    copy_coords(src_cube.aux_coords, new_cube.add_aux_coord)

    for factory in src_cube.aux_factories:
        new_cube.add_aux_factory(factory.updated(coord_mapping))

    return new_cube


def _curl_change_z(src_cube, z_coord, prototype_diff):
    # New data
    ind = [slice(None, None)] * src_cube.ndim
    z_dim = src_cube.coord_dims(z_coord)[0]
    ind[z_dim] = slice(-1, None)
    new_data = np.append(src_cube.data, src_cube.data[tuple(ind)], z_dim)

    # The existing z_coord doesn't fit the new data so make a
    # new cube using the prototype z_coord.
    local_z_coord = src_cube.coord(z_coord)
    new_local_z_coord = prototype_diff.coord(z_coord).copy()
    def coord_func(coord):
        if coord is local_z_coord:
            new_coord = new_local_z_coord
        else:
            new_coord = coord.copy()
        return new_coord
    result = _copy_cube_transformed(src_cube, new_data, coord_func)
    return result


def _coord_sin(coord):
    """
    Return a coordinate which represents sin(coord).

    Args:

    * coord
        Coord instance with values in either degrees or radians

    """
    return _trig_method(coord, np.sin)


def _coord_cos(coord):
    """
    Return a coordinate which represents cos(coord).

    Args:

    * coord
        Coord instance with values in either degrees or radians

    """
    return _trig_method(coord, np.cos)


def _trig_method(coord, trig_function):
    """
    Return a coordinate which represents trig_function(coord).

    Args:

    * coord
        Coord instance with points values in either degrees or radians
    * trig_function
        Reference to a trigonometric function e.g. numpy.sin

    """
    # If we are in degrees create a copy that is in radians.
    if coord.units == 'degrees':
        coord = coord.copy()
        coord.convert_units('radians')

    trig_coord = iris.coords.AuxCoord.from_coord(coord)
    trig_coord.points = trig_function(coord.points)
    if coord.has_bounds():
        trig_coord.bounds = trig_function(coord.bounds)
    trig_coord.units = '1'
    trig_coord.rename('{}({})'.format(trig_function.__name__, coord.name()))

    return trig_coord


def curl(i_cube, j_cube, k_cube=None, ignore=None):
    r'''
    Calculate the 3d curl of the given vector of cubes.

    Args:

    * i_cube
        The i cube of the vector to operate on
    * j_cube
        The j cube of the vector to operate on

    Kwargs:

    * k_cube
        The k cube of the vector to operate on

    Return (i_cmpt_curl_cube, j_cmpt_curl_cube, k_cmpt_curl_cube)

    The calculation of curl is dependent on the type of :func:`iris.coord_systems.CoordSystem` in the cube:

        Cartesian curl

            The Cartesian curl is defined as:

            .. math::

                \nabla\times \vec u = (\frac{\delta w}{\delta y} - \frac{\delta v}{\delta z}) \vec a_i - (\frac{\delta w}{\delta x} - \frac{\delta u}{\delta z})\vec a_j + (\frac{\delta v}{\delta x} - \frac{\delta u}{\delta y})\vec a_k

        Spherical curl

            When spherical calculus is used, i_cube is the phi vector component (e.g. eastward), j_cube is the theta component
            (e.g. northward) and k_cube is the radial component.

            The spherical curl is defined as:

            .. math::

                \nabla\times \vec A = \frac{1}{r cos \theta}(\frac{\delta}{\delta \theta}(\vec A_\phi cos \theta) - \frac{\delta \vec A_\theta}{\delta \phi}) \vec r + \frac{1}{r}(\frac{1}{cos \theta} \frac{\delta \vec A_r}{\delta \phi} - \frac{\delta}{\delta r} (r \vec A_\phi))\vec \theta + \frac{1}{r}(\frac{\delta}{\delta r}(r \vec A_\theta) - \frac{\delta \vec A_r}{\delta \theta}) \vec \phi

            where phi is longitude, theta is latitude.

    '''
    if ignore is not None:
        ignore = None
        warnings.warn('The ignore keyword to iris.analysis.calculus.curl is deprecated, ignoring is now done automatically.')

    # Get the vector quantity names (i.e. ['easterly', 'northerly', 'vertical'])
    vector_quantity_names, phenomenon_name = spatial_vectors_with_phenom_name(i_cube, j_cube, k_cube)

    cubes = filter(None, [i_cube, j_cube, k_cube])

    # get the names of all coords binned into useful comparison groups
    coord_comparison = iris.analysis.coord_comparison(*cubes)

    bad_coords = coord_comparison['ungroupable_and_dimensioned']
    if bad_coords:
        raise ValueError("Coordinates found in one cube that describe a data dimension which weren't in the other "
                         "cube (%s), try removing this coordinate."  % ', '.join([group.name() for group in bad_coords]))

    bad_coords = coord_comparison['resamplable']
    if bad_coords:
        raise ValueError('Some coordinates are different (%s), consider resampling.' % ', '.join([group.name() for group in bad_coords]))

    ignore_string = ''
    if coord_comparison['ignorable']:
        ignore_string = ' (ignoring %s)' % ', '.join([group.name() for group in bad_coords])

    # Get the dim_coord, or None if none exist, for the xyz dimensions
    x_coord = i_cube.coord(axis='X')
    y_coord = i_cube.coord(axis='Y')
    z_coord = i_cube.coord(axis='Z')

    y_dim = i_cube.coord_dims(y_coord)[0]

    horiz_cs = i_cube.coord_system('CoordSystem')

    # Planar (non spherical) coords?
    ellipsoidal = isinstance(horiz_cs, (iris.coord_systems.GeogCS, iris.coord_systems.RotatedGeogCS))
    if not ellipsoidal:

        # TODO Implement some mechanism for conforming to a common grid
        dj_dx = _curl_differentiate(j_cube, x_coord)
        prototype_diff = dj_dx

        # i curl component (dk_dy - dj_dz)
        dk_dy = _curl_differentiate(k_cube, y_coord)
        dk_dy = _curl_regrid(dk_dy, prototype_diff)
        dj_dz = _curl_differentiate(j_cube, z_coord)
        dj_dz = _curl_regrid(dj_dz, prototype_diff)

        # TODO Implement resampling in the vertical (which regridding does not support).
        if dj_dz is not None and dj_dz.data.shape != prototype_diff.data.shape:
            dj_dz = _curl_change_z(dj_dz, z_coord, prototype_diff)

        i_cmpt = _curl_subtract(dk_dy, dj_dz)
        dj_dz = dk_dy = None

        # j curl component (di_dz - dk_dx)
        di_dz = _curl_differentiate(i_cube, z_coord)
        di_dz = _curl_regrid(di_dz, prototype_diff)

        # TODO Implement resampling in the vertical (which regridding does not support).
        if di_dz is not None and di_dz.data.shape != prototype_diff.data.shape:
            di_dz = _curl_change_z(di_dz, z_coord, prototype_diff)

        dk_dx = _curl_differentiate(k_cube, x_coord)
        dk_dx = _curl_regrid(dk_dx, prototype_diff)
        j_cmpt = _curl_subtract(di_dz, dk_dx)
        di_dz = dk_dx = None

        # k curl component ( dj_dx - di_dy)
        di_dy = _curl_differentiate(i_cube, y_coord)
        di_dy = _curl_regrid(di_dy, prototype_diff)
        # Since prototype_diff == dj_dx we don't need to recalculate dj_dx
#        dj_dx = _curl_differentiate(j_cube, x_coord)
#        dj_dx = _curl_regrid(dj_dx, prototype_diff)
        k_cmpt = _curl_subtract(dj_dx, di_dy)
        di_dy = dj_dx = None

        result = [i_cmpt, j_cmpt, k_cmpt]

    # Spherical coords (GeogCS or RotatedGeogCS).
    else:
        # A_\phi = i ; A_\theta = j ; A_\r = k
        # theta = lat ; phi = long ;
        # r_cmpt = 1/ ( r * cos(lat) ) * ( d/dtheta ( i_cube * sin( lat ) ) - d_j_cube_dphi )
        # phi_cmpt = 1/r * ( d/dr (r * j_cube) - d_k_cube_dtheta)
        # theta_cmpt = 1/r * ( 1/cos(lat) * d_k_cube_dphi - d/dr (r * i_cube)
        if y_coord.name() != 'latitude' or x_coord.name() != 'longitude':
            raise ValueError('Expecting latitude as the y coord and longitude as the x coord for spherical curl.')

        # Get the radius of the earth - and check for sphericity
        ellipsoid = horiz_cs
        if isinstance(horiz_cs, iris.coord_systems.RotatedGeogCS):
            ellipsoid = horiz_cs.ellipsoid
        if ellipsoid:
            # TODO: Add a test for this
            r = ellipsoid.semi_major_axis
            r_unit = iris.unit.Unit("m")
            spherical = (ellipsoid.inverse_flattening == 0.0)
        else:
            r = iris.analysis.cartography.DEFAULT_SPHERICAL_EARTH_RADIUS
            r_unit = iris.analysis.cartography.DEFAULT_SPHERICAL_EARTH_RADIUS_UNIT
            spherical = True

        if not spherical:
            raise ValueError("Cannot take the curl over a non-spherical ellipsoid.")

        lon_coord = x_coord.copy()
        lat_coord = y_coord.copy()
        lon_coord.convert_units('radians')
        lat_coord.convert_units('radians')
        lat_cos_coord = _coord_cos(lat_coord)

        # TODO Implement some mechanism for conforming to a common grid
        temp = iris.analysis.maths.multiply(i_cube, lat_cos_coord, y_dim)
        dicos_dtheta = _curl_differentiate(temp, lat_coord)
        prototype_diff = dicos_dtheta

        # r curl component:  1/ ( r * cos(lat) ) * ( dicos_dtheta - d_j_cube_dphi )
        # Since prototype_diff == dicos_dtheta we don't need to recalculate dicos_dtheta
        d_j_cube_dphi = _curl_differentiate(j_cube, lon_coord)
        d_j_cube_dphi = _curl_regrid(d_j_cube_dphi, prototype_diff)
        new_lat_coord = d_j_cube_dphi.coord('latitude')
        new_lat_cos_coord = _coord_cos(new_lat_coord)
        lat_dim = d_j_cube_dphi.coord_dims(new_lat_coord)[0]
        r_cmpt = iris.analysis.maths.divide(_curl_subtract(dicos_dtheta, d_j_cube_dphi), r * new_lat_cos_coord, dim=lat_dim)
        r_cmpt.units = r_cmpt.units / r_unit
        d_j_cube_dphi = dicos_dtheta = None

        # phi curl component: 1/r * ( drj_dr - d_k_cube_dtheta)
        drj_dr = _curl_differentiate(r * j_cube, z_coord)
        if drj_dr is not None:
            drj_dr.units = drj_dr.units * r_unit
        drj_dr = _curl_regrid(drj_dr, prototype_diff)
        d_k_cube_dtheta = _curl_differentiate(k_cube, lat_coord)
        d_k_cube_dtheta = _curl_regrid(d_k_cube_dtheta, prototype_diff)
        if drj_dr is None and d_k_cube_dtheta is None:
            phi_cmpt = None
        else:
            phi_cmpt = 1/r * _curl_subtract(drj_dr, d_k_cube_dtheta)
            phi_cmpt.units = phi_cmpt.units / r_unit

        drj_dr = d_k_cube_dtheta = None

        # theta curl component: 1/r * ( 1/cos(lat) * d_k_cube_dphi - dri_dr )
        d_k_cube_dphi = _curl_differentiate(k_cube, lon_coord)
        d_k_cube_dphi = _curl_regrid(d_k_cube_dphi, prototype_diff)
        if d_k_cube_dphi is not None:
            d_k_cube_dphi = iris.analysis.maths.divide(d_k_cube_dphi, lat_cos_coord)
        dri_dr = _curl_differentiate(r * i_cube, z_coord)
        if dri_dr is not None:
            dri_dr.units = dri_dr.units * r_unit
        dri_dr = _curl_regrid(dri_dr, prototype_diff)
        if d_k_cube_dphi is None and dri_dr is None:
            theta_cmpt = None
        else:
            theta_cmpt = 1/r * _curl_subtract(d_k_cube_dphi, dri_dr)
            theta_cmpt.units = theta_cmpt.units / r_unit
        d_k_cube_dphi = dri_dr = None

        result = [phi_cmpt, theta_cmpt, r_cmpt]

    for direction, cube in zip(vector_quantity_names, result):
        if cube is not None:
            cube.rename('%s curl of %s' % (direction, phenomenon_name))

    return result


def spatial_vectors_with_phenom_name(i_cube, j_cube, k_cube=None):
    """
    Given 2 or 3 spatially dependent cubes, return a list of the spatial coordinate names with appropriate phenomenon name.

    This routine is designed to identify the vector quantites which each of the cubes provided represent
    and return a list of their 3d spatial dimension names and associated phenomenon.
    For example, given a cube of "u wind" and "v wind" the return value would be (['u', 'v', 'w'], 'wind')::

        >>> spatial_vectors_with_phenom_name(u_wind_cube, v_wind_cube) #doctest: +SKIP
        (['u', 'v', 'w'], 'wind')

    """
    directional_names = (('u', 'v', 'w'), ('x', 'y', 'z'), ('i', 'j', 'k'),
                         ('eastward', 'northward', 'upward'),
                         ('easterly', 'northerly', 'vertical'), ('easterly', 'northerly', 'radial'))

    # Create a list of the standard_names of our incoming cubes (excluding the k_cube if it is None)
    cube_standard_names = [cube.name() for cube in (i_cube, j_cube, k_cube) if cube is not None]

    # Define a regular expr which represents (direction, phenomenon) from the standard name of a cube
    # e.g from "w wind" -> ("w", "wind")
    vector_qty = re.compile(r'([^\W_]+)[\W_]+(.*)')

    # Make a dictionary of {direction: phenomenon quantity}
    cube_directions, cube_phenomena = zip( *[re.match(vector_qty, std_name).groups() for std_name in cube_standard_names] )

    # Check that there is only one distinct phenomenon
    if len(set(cube_phenomena)) != 1:
        raise ValueError('Vector phenomenon name not consistent between vector cubes. Got '
                         'cube phenomena: %s; from standard names: %s.' % \
                         (', '.join(cube_phenomena), ', '.join(cube_standard_names))
                         )

    # Get the appropriate direction list from the cube_directions we have got from the standard name
    direction = None
    for possible_direction in directional_names:
        # if this possible direction (minus the k_cube if it is none) matches direction from the given cubes use it.
        if possible_direction[0:len(cube_directions)] == cube_directions:
            direction = possible_direction

    # If we didn't get a match, raise an Exception
    if direction is None:
        direction_string = '; '.join((', '.join(possible_direction) for possible_direction in directional_names))
        raise ValueError('%s are not recognised vector cube_directions. Possible cube_directions are: %s.' % \
                         (cube_directions, direction_string) )

    return (direction, cube_phenomena[0])

########NEW FILE########
__FILENAME__ = cartography
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Various utilities and numeric transformations relevant to cartography.

"""
import copy
import itertools
import math
import warnings

import numpy as np
import numpy.ma as ma

import cartopy.img_transform
import cartopy.crs as ccrs
import iris.analysis
import iris.coords
import iris.coord_systems
import iris.exceptions
import iris.unit


# This value is used as a fall-back if the cube does not define the earth
DEFAULT_SPHERICAL_EARTH_RADIUS = 6367470
# TODO: This should not be necessary, as CF is always in meters
DEFAULT_SPHERICAL_EARTH_RADIUS_UNIT = iris.unit.Unit('m')


def wrap_lons(lons, base, period):
    """
    Wrap longitude values into the range between base and base+period.

    .. testsetup::

        import numpy as np
        from iris.analysis.cartography import wrap_lons

    For example:
        >>> print wrap_lons(np.array([185, 30, -200, 75]), -180, 360)
        [-175.   30.  160.   75.]

    """
    # It is important to use 64bit floating precision when changing a floats
    # numbers range.
    lons = lons.astype(np.float64)
    return ((lons - base + period * 2) % period) + base


def unrotate_pole(rotated_lons, rotated_lats, pole_lon, pole_lat):
    """
    Convert rotated-pole lons and lats to unrotated ones.

    Example::

        lons, lats = unrotate_pole(grid_lons, grid_lats, pole_lon, pole_lat)

    .. note:: Uses proj.4 to perform the conversion.

    """
    src_proj = ccrs.RotatedGeodetic(pole_longitude=pole_lon,
                                    pole_latitude=pole_lat)
    target_proj = ccrs.Geodetic()
    res = target_proj.transform_points(x=rotated_lons, y=rotated_lats,
                                       src_crs=src_proj)
    unrotated_lon = res[..., 0]
    unrotated_lat = res[..., 1]

    return unrotated_lon, unrotated_lat


def rotate_pole(lons, lats, pole_lon, pole_lat):
    """
    Convert arrays of lons and lats to ones on a rotated pole.

    Example::

        grid_lons, grid_lats = rotate_pole(lons, lats, pole_lon, pole_lat)

    .. note:: Uses proj.4 to perform the conversion.

    """
    src_proj = ccrs.Geodetic()
    target_proj = ccrs.RotatedGeodetic(pole_longitude=pole_lon,
                                       pole_latitude=pole_lat)
    res = target_proj.transform_points(x=lons, y=lats,
                                       src_crs=src_proj)
    rotated_lon = res[..., 0]
    rotated_lat = res[..., 1]

    return rotated_lon, rotated_lat


def _get_lat_lon_coords(cube):
    lat_coords = filter(lambda coord: "latitude" in coord.name(),
                        cube.coords())
    lon_coords = filter(lambda coord: "longitude" in coord.name(),
                        cube.coords())
    if len(lat_coords) > 1 or len(lon_coords) > 1:
        raise ValueError(
            "Calling _get_lat_lon_coords() with multiple lat or lon coords"
            " is currently disallowed")
    lat_coord = lat_coords[0]
    lon_coord = lon_coords[0]
    return (lat_coord, lon_coord)


def _xy_range(cube, mode=None):
    """
    Return the x & y range of this Cube.

    Args:

        * cube - The cube for which to calculate xy extents.

    Kwargs:

        * mode - If the coordinate has bounds, set this to specify the
                 min/max calculation.
                 Set to iris.coords.POINT_MODE or iris.coords.BOUND_MODE.

    """
    # Helpful error if we have an inappropriate CoordSystem
    cs = cube.coord_system("CoordSystem")
    cs_valid_types = (iris.coord_systems.GeogCS,
                      iris.coord_systems.RotatedGeogCS)
    if ((cs is not None) and not isinstance(cs, cs_valid_types)):
        raise ValueError(
            "Latlon coords cannot be found with {0}.".format(type(cs)))

    x_coord, y_coord = cube.coord(axis="X"), cube.coord(axis="Y")
    cs = cube.coord_system('CoordSystem')

    if x_coord.has_bounds() != x_coord.has_bounds():
        raise ValueError(
            'Cannot get the range of the x and y coordinates if they do '
            'not have the same presence of bounds.')

    if x_coord.has_bounds():
        if mode not in [iris.coords.POINT_MODE, iris.coords.BOUND_MODE]:
            raise ValueError(
                'When the coordinate has bounds, please specify "mode".')
        _mode = mode
    else:
        _mode = iris.coords.POINT_MODE

    # Get the x and y grids
    if isinstance(cs, iris.coord_systems.RotatedGeogCS):
        if _mode == iris.coords.POINT_MODE:
            x, y = get_xy_grids(cube)
        else:
            x, y = get_xy_contiguous_bounded_grids(cube)
    else:
        if _mode == iris.coords.POINT_MODE:
            x = x_coord.points
            y = y_coord.points
        else:
            x = x_coord.bounds
            y = y_coord.bounds

    # Get the x and y range
    if getattr(x_coord, 'circular', False):
        x_range = (np.min(x), np.min(x) + x_coord.units.modulus)
    else:
        x_range = (np.min(x), np.max(x))

    y_range = (np.min(y), np.max(y))

    return (x_range, y_range)


def get_xy_grids(cube):
    """
    Return 2D X and Y points for a given cube.

    Args:

        * cube - The cube for which to generate 2D X and Y points.

    Example::

        x, y = get_xy_grids(cube)

    """
    x_coord, y_coord = cube.coord(axis="X"), cube.coord(axis="Y")

    x = x_coord.points
    y = y_coord.points

    if x.ndim == y.ndim == 1:
        # Convert to 2D.
        x, y = np.meshgrid(x, y)
    elif x.ndim == y.ndim == 2:
        # They are already in the correct shape.
        pass
    else:
        raise ValueError("Expected 1D or 2D XY coords")

    return (x, y)


def get_xy_contiguous_bounded_grids(cube):
    """
    Return 2d arrays for x and y bounds.

    Returns array of shape (n+1, m+1).

    Example::

        xs, ys = get_xy_contiguous_bounded_grids(cube)

    """
    x_coord, y_coord = cube.coord(axis="X"), cube.coord(axis="Y")

    x = x_coord.contiguous_bounds()
    y = y_coord.contiguous_bounds()
    x, y = np.meshgrid(x, y)

    return (x, y)


def _quadrant_area(radian_colat_bounds, radian_lon_bounds, radius_of_earth):
    """Calculate spherical segment areas.

    - radian_colat_bounds    -- [n,2] array of colatitude bounds (radians)
    - radian_lon_bounds      -- [n,2] array of longitude bounds (radians)
    - radius_of_earth        -- radius of the earth
                                (currently assumed spherical)

    Area weights are calculated for each lat/lon cell as:

        .. math::

            r^2 (lon_1 - lon_0) ( cos(colat_0) - cos(colat_1))

    The resulting array will have a shape of
    *(radian_colat_bounds.shape[0], radian_lon_bounds.shape[0])*

    """
    # ensure pairs of bounds
    if (radian_colat_bounds.shape[-1] != 2 or
            radian_lon_bounds.shape[-1] != 2 or
            radian_colat_bounds.ndim != 2 or
            radian_lon_bounds.ndim != 2):
        raise ValueError("Bounds must be [n,2] array")

    # fill in a new array of areas
    radius_sqr = radius_of_earth ** 2
    areas = np.ndarray((radian_colat_bounds.shape[0],
                        radian_lon_bounds.shape[0]))
    # we use abs because backwards bounds (min > max) give negative areas.
    for j in range(radian_colat_bounds.shape[0]):
        areas[j, :] = [(radius_sqr * math.cos(radian_colat_bounds[j, 0]) *
                       (radian_lon_bounds[i, 1] - radian_lon_bounds[i, 0])) -
                       (radius_sqr * math.cos(radian_colat_bounds[j, 1]) *
                        (radian_lon_bounds[i, 1] - radian_lon_bounds[i, 0]))
                       for i in range(radian_lon_bounds.shape[0])]

    return np.abs(areas)


def area_weights(cube, normalize=False):
    """
    Returns an array of area weights, with the same dimensions as the cube.

    This is a 2D lat/lon area weights array, repeated over the non lat/lon
    dimensions.

    Args:

    * cube (:class:`iris.cube.Cube`):
        The cube to calculate area weights for.

    Kwargs:

    * normalize (False/True):
        If False, weights are grid cell areas. If True, weights are grid
        cell areas divided by the total grid area.

    The cube must have coordinates 'latitude' and 'longitude' with bounds.

    Area weights are calculated for each lat/lon cell as:

        .. math::

            r^2 cos(lat_0) (lon_1 - lon_0) - r^2 cos(lat_1) (lon_1 - lon_0)

    Currently, only supports a spherical datum.
    Uses earth radius from the cube, if present and spherical.
    Defaults to iris.analysis.cartography.DEFAULT_SPHERICAL_EARTH_RADIUS.

    """
    # Get the radius of the earth
    cs = cube.coord_system("CoordSystem")
    if isinstance(cs, iris.coord_systems.GeogCS):
        if cs.inverse_flattening != 0.0:
            warnings.warn("Assuming spherical earth from ellipsoid.")
        radius_of_earth = cs.semi_major_axis
    elif (isinstance(cs, iris.coord_systems.RotatedGeogCS) and
            (cs.ellipsoid is not None)):
        if cs.ellipsoid.inverse_flattening != 0.0:
            warnings.warn("Assuming spherical earth from ellipsoid.")
        radius_of_earth = cs.ellipsoid.semi_major_axis
    else:
        warnings.warn("Using DEFAULT_SPHERICAL_EARTH_RADIUS.")
        radius_of_earth = DEFAULT_SPHERICAL_EARTH_RADIUS

    # Get the lon and lat coords and axes
    try:
        lat, lon = _get_lat_lon_coords(cube)
    except IndexError:
        raise ValueError('Cannot get latitude/longitude '
                         'coordinates from cube {!r}.'.format(cube.name()))

    if lat.ndim > 1:
        raise iris.exceptions.CoordinateMultiDimError(lat)
    if lon.ndim > 1:
        raise iris.exceptions.CoordinateMultiDimError(lon)

    lat_dim = cube.coord_dims(lat)
    lat_dim = lat_dim[0] if lat_dim else None

    lon_dim = cube.coord_dims(lon)
    lon_dim = lon_dim[0] if lon_dim else None

    if not (lat.has_bounds() and lon.has_bounds()):
        msg = "Coordinates {!r} and {!r} must have bounds to determine " \
              "the area weights.".format(lat.name(), lon.name())
        raise ValueError(msg)

    # Convert from degrees to radians
    lat = lat.copy()
    lat.convert_units('radians')
    lon = lon.copy()
    lon.convert_units('radians')

    # Create 2D weights from bounds.
    # Use the geographical area as the weight for each cell
    # Convert latitudes to co-latitude. I.e from -90 --> +90  to  0 --> pi
    ll_weights = _quadrant_area(lat.bounds + np.pi / 2.,
                                lon.bounds, radius_of_earth)

    # Normalize the weights if necessary.
    if normalize:
        ll_weights /= ll_weights.sum()

    # Now we create an array of weights for each cell. This process will
    # handle adding the required extra dimensions and also take care of
    # the order of dimensions.
    broadcast_dims = filter(lambda x: x is not None, (lat_dim, lon_dim))
    wshape = []
    for idim, dim in zip((0, 1), (lat_dim, lon_dim)):
        if dim is not None:
            wshape.append(ll_weights.shape[idim])
    ll_weights = ll_weights.reshape(wshape)
    broad_weights = iris.util.broadcast_to_shape(ll_weights,
                                                 cube.shape,
                                                 broadcast_dims)

    return broad_weights


def cosine_latitude_weights(cube):
    """
    Returns an array of latitude weights, with the same dimensions as
    the cube. The weights are the cosine of latitude.

    These are n-dimensional latitude weights repeated over the dimensions
    not covered by the latitude coordinate.

    The cube must have a coordinate with 'latitude' in the name. Out of
    range values (greater than 90 degrees or less than -90 degrees) will
    be clipped to the valid range.

    Weights are calculated for each latitude as:

        .. math::

           w_l = \cos \phi_l

    Examples:

    Compute weights suitable for averaging type operations::

        from iris.analysis.cartography import cosine_latitude_weights
        cube = iris.load_cube(iris.sample_data_path('air_temp.pp'))
        weights = cosine_latitude_weights(cube)

    Compute weights suitable for EOF analysis (or other covariance type
    analyses)::

        import numpy as np
        from iris.analysis.cartography import cosine_latitude_weights
        cube = iris.load_cube(iris.sample_data_path('air_temp.pp'))
        weights = np.sqrt(cosine_latitude_weights(cube))

    """
    # Find all latitude coordinates, we want one and only one.
    lat_coords = filter(lambda coord: "latitude" in coord.name(),
                        cube.coords())
    if len(lat_coords) > 1:
        raise ValueError("Multiple latitude coords are currently disallowed.")
    try:
        lat = lat_coords[0]
    except IndexError:
        raise ValueError('Cannot get latitude '
                         'coordinate from cube {!r}.'.format(cube.name()))

    # Get the dimension position(s) of the latitude coordinate.
    lat_dims = cube.coord_dims(lat)

    # Convert to radians.
    lat = lat.copy()
    lat.convert_units('radians')

    # Compute the weights as the cosine of latitude. In some cases,
    # particularly when working in 32-bit precision, the latitude values can
    # extend beyond the allowable range of [-pi/2, pi/2] due to numerical
    # precision. We first check for genuinely out of range values, and issue a
    # warning if these are found. Then the cosine is computed and clipped to
    # the valid range [0, 1].
    threshold = np.deg2rad(0.001)  # small value for grid resolution
    if np.any(lat.points < -np.pi / 2. - threshold) or \
            np.any(lat.points > np.pi / 2. + threshold):
        warnings.warn('Out of range latitude values will be '
                      'clipped to the valid range.',
                      UserWarning)
    points = lat.points
    l_weights = np.cos(points).clip(0., 1.)

    # Create weights for each grid point. This operation handles adding extra
    # dimensions and also the order of the dimensions.
    broadcast_dims = filter(lambda x: x is not None, lat_dims)
    wshape = []
    for idim, dim in enumerate(lat_dims):
        if dim is not None:
            wshape.append(l_weights.shape[idim])
    l_weights = l_weights.reshape(wshape)
    broad_weights = iris.util.broadcast_to_shape(l_weights,
                                                 cube.shape,
                                                 broadcast_dims)

    return broad_weights


def project(cube, target_proj, nx=None, ny=None):
    """
    Nearest neighbour regrid to a specified target projection.

    Return a new cube that is the result of projecting a cube with 1 or 2
    dimensional latitude-longitude coordinates from its coordinate system into
    a specified projection e.g. Robinson or Polar Stereographic.
    This function is intended to be used in cases where the cube's coordinates
    prevent one from directly visualising the data, e.g. when the longitude
    and latitude are two dimensional and do not make up a regular grid.

    Args:
        * cube
            An instance of :class:`iris.cube.Cube`.
        * target_proj
            An instance of the Cartopy Projection class, or an instance of
            :class:`iris.coord_systems.CoordSystem` from which a projection
            will be obtained.
    Kwargs:
        * nx
            Desired number of sample points in the x direction for a domain
            covering the globe.
        * ny
            Desired number of sample points in the y direction for a domain
            covering the globe.

    Returns:
        An instance of :class:`iris.cube.Cube` and a list describing the
        extent of the projection.

    .. note::

        This function assumes global data and will if necessary extrapolate
        beyond the geographical extent of the source cube using a nearest
        neighbour approach. nx and ny then include those points which are
        outside of the target projection.

    .. note::

        Masked arrays are handled by passing their masked status to the
        resulting nearest neighbour values.  If masked, the value in the
        resulting cube is set to 0.

    .. warning::

        This function uses a nearest neighbour approach rather than any form
        of linear/non-linear interpolation to determine the data value of each
        cell in the resulting cube. Consequently it may have an adverse effect
        on the statistics of the data e.g. the mean and standard deviation
        will not be preserved.

    """
    try:
        lat_coord, lon_coord = _get_lat_lon_coords(cube)
    except IndexError:
        raise ValueError('Cannot get latitude/longitude '
                         'coordinates from cube {!r}.'.format(cube.name()))

    if lat_coord.coord_system != lon_coord.coord_system:
        raise ValueError('latitude and longitude coords appear to have '
                         'different coordinates systems.')

    if lon_coord.units != 'degrees':
        lon_coord = lon_coord.copy()
        lon_coord.convert_units('degrees')
    if lat_coord.units != 'degrees':
        lat_coord = lat_coord.copy()
        lat_coord.convert_units('degrees')

    # Determine source coordinate system
    if lat_coord.coord_system is None:
        # Assume WGS84 latlon if unspecified
        warnings.warn('Coordinate system of latitude and longitude '
                      'coordinates is not specified. Assuming WGS84 Geodetic.')
        orig_cs = iris.coord_systems.GeogCS(semi_major_axis=6378137.0,
                                            inverse_flattening=298.257223563)
    else:
        orig_cs = lat_coord.coord_system

    # Convert to cartopy crs
    source_cs = orig_cs.as_cartopy_crs()

    # Obtain coordinate arrays (ignoring bounds) and convert to 2d
    # if not already.
    source_x = lon_coord.points
    source_y = lat_coord.points
    if source_x.ndim != 2 or source_y.ndim != 2:
        source_x, source_y = np.meshgrid(source_x, source_y)

    # Calculate target grid
    target_cs = None
    if isinstance(target_proj, iris.coord_systems.CoordSystem):
        target_cs = target_proj
        target_proj = target_proj.as_cartopy_projection()

    # Resolution of new grid
    if nx is None:
        nx = source_x.shape[1]
    if ny is None:
        ny = source_x.shape[0]

    target_x, target_y, extent = cartopy.img_transform.mesh_projection(
        target_proj, nx, ny)

    # Determine dimension mappings - expect either 1d or 2d
    if lat_coord.ndim != lon_coord.ndim:
        raise ValueError("The latitude and longitude coordinates have "
                         "different dimensionality.")

    latlon_ndim = lat_coord.ndim
    lon_dims = cube.coord_dims(lon_coord)
    lat_dims = cube.coord_dims(lat_coord)

    if latlon_ndim == 1:
        xdim = lon_dims[0]
        ydim = lat_dims[0]
    elif latlon_ndim == 2:
        if lon_dims != lat_dims:
            raise ValueError("The 2d latitude and longitude coordinates "
                             "correspond to different dimensions.")
        # If coords are 2d assume that grid is ordered such that x corresponds
        # to the last dimension (shortest stride).
        xdim = lon_dims[1]
        ydim = lon_dims[0]
    else:
        raise ValueError('Expected the latitude and longitude coordinates '
                         'to have 1 or 2 dimensions, got {} and '
                         '{}.'.format(lat_coord.ndim, lon_coord.ndim))

    # Create array to store regridded data
    new_shape = list(cube.shape)
    new_shape[xdim] = nx
    new_shape[ydim] = ny
    new_data = ma.zeros(new_shape, cube.data.dtype)

    # Create iterators to step through cube data in lat long slices
    new_shape[xdim] = 1
    new_shape[ydim] = 1
    index_it = np.ndindex(*new_shape)
    if lat_coord.ndim == 1 and lon_coord.ndim == 1:
        slice_it = cube.slices([lat_coord, lon_coord])
    elif lat_coord.ndim == 2 and lon_coord.ndim == 2:
        slice_it = cube.slices(lat_coord)
    else:
        raise ValueError('Expected the latitude and longitude coordinates '
                         'to have 1 or 2 dimensions, got {} and '
                         '{}.'.format(lat_coord.ndim, lon_coord.ndim))

#    # Mask out points outside of extent in source_cs - disabled until
#    # a way to specify global/limited extent is agreed upon and code
#    # is generalised to handle -180 to +180, 0 to 360 and >360 longitudes.
#    source_desired_xy = source_cs.transform_points(target_proj,
#                                                   target_x.flatten(),
#                                                   target_y.flatten())
#    if np.any(source_x < 0.0) and np.any(source_x > 180.0):
#        raise ValueError('Unable to handle range of longitude.')
#    # This does not work in all cases e.g. lon > 360
#    if np.any(source_x > 180.0):
#        source_desired_x = (source_desired_xy[:, 0].reshape(ny, nx) +
#                            360.0) % 360.0
#    else:
#        source_desired_x = source_desired_xy[:, 0].reshape(ny, nx)
#    source_desired_y = source_desired_xy[:, 1].reshape(ny, nx)
#    outof_extent_points = ((source_desired_x < source_x.min()) |
#                           (source_desired_x > source_x.max()) |
#                           (source_desired_y < source_y.min()) |
#                           (source_desired_y > source_y.max()))
#    # Make array a mask by default (rather than a single bool) to allow mask
#    # to be assigned to slices.
#    new_data.mask = np.zeros(new_shape)

    # Step through cube data, regrid onto desired projection and insert results
    # in new_data array
    for index, ll_slice in itertools.izip(index_it, slice_it):
        # Regrid source data onto target grid
        index = list(index)
        index[xdim] = slice(None, None)
        index[ydim] = slice(None, None)
        new_data[index] = cartopy.img_transform.regrid(ll_slice.data,
                                                       source_x, source_y,
                                                       source_cs,
                                                       target_proj,
                                                       target_x, target_y)

#    # Mask out points beyond extent
#    new_data[index].mask[outof_extent_points] = True

    # Remove mask if it is unnecessary
    if not np.any(new_data.mask):
        new_data = new_data.data

    # Create new cube
    new_cube = iris.cube.Cube(new_data)

    # Add new grid coords
    x_coord = iris.coords.DimCoord(
        target_x[0, :], 'projection_x_coordinate',
        coord_system=copy.copy(target_cs))
    y_coord = iris.coords.DimCoord(
        target_y[:, 0], 'projection_y_coordinate',
        coord_system=copy.copy(target_cs))

    new_cube.add_dim_coord(x_coord, xdim)
    new_cube.add_dim_coord(y_coord, ydim)

    # Add resampled lat/lon in original coord system
    source_desired_xy = source_cs.transform_points(target_proj,
                                                   target_x.flatten(),
                                                   target_y.flatten())
    new_lon_points = source_desired_xy[:, 0].reshape(ny, nx)
    new_lat_points = source_desired_xy[:, 1].reshape(ny, nx)
    new_lon_coord = iris.coords.AuxCoord(new_lon_points,
                                         standard_name='longitude',
                                         units='degrees',
                                         coord_system=orig_cs)
    new_lat_coord = iris.coords.AuxCoord(new_lat_points,
                                         standard_name='latitude',
                                         units='degrees',
                                         coord_system=orig_cs)
    new_cube.add_aux_coord(new_lon_coord, [ydim, xdim])
    new_cube.add_aux_coord(new_lat_coord, [ydim, xdim])

    coords_to_ignore = set()
    coords_to_ignore.update(cube.coords(contains_dimension=xdim))
    coords_to_ignore.update(cube.coords(contains_dimension=ydim))
    for coord in cube.dim_coords:
        if coord not in coords_to_ignore:
            new_cube.add_dim_coord(coord.copy(), cube.coord_dims(coord))
    for coord in cube.aux_coords:
        if coord not in coords_to_ignore:
            new_cube.add_aux_coord(coord.copy(), cube.coord_dims(coord))
    discarded_coords = coords_to_ignore.difference([lat_coord, lon_coord])
    if discarded_coords:
        warnings.warn('Discarding coordinates that share dimensions with '
                      '{} and {}: {}'.format(lat_coord.name(),
                                             lon_coord.name(),
                                             [coord.name() for
                                              coord in discarded_coords]))

    # TODO handle derived coords/aux_factories

    # Copy metadata across
    new_cube.metadata = cube.metadata

    return new_cube, extent

########NEW FILE########
__FILENAME__ = geometry
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Various utilities related to geometric operations.

.. note::
    This module requires :mod:`shapely`.

"""

from shapely.geometry import Polygon

import numpy as np

import iris.exceptions


def geometry_area_weights(cube, geometry, normalize=False):
    """
    Returns the array of weights corresponding to the area of overlap between
    the cells of cube's horizontal grid, and the given shapely geometry.

    The returned array is suitable for use with :const:`iris.analysis.MEAN`.

    The cube must have bounded horizontal coordinates.

    .. note::
        This routine works in Euclidean space. Area calculations do not
        account for the curvature of the Earth. And care must be taken to
        ensure any longitude values are expressed over a suitable interval.

    Args:

    * cube (:class:`iris.cube.Cube`):
        A Cube containing a bounded, horizontal grid definition.
    * geometry (a shapely geometry instance):
        The geometry of interest. To produce meaningful results this geometry
        must have a non-zero area. Typically a Polygon or MultiPolygon.

    Kwargs:

    * normalize:
        Calculate each individual cell weight as the cell area overlap between
        the cell and the given shapely geometry divided by the total cell area.
        Default is False.

    """
    # Validate the input parameters
    if not cube.coords(axis='x') or not cube.coords(axis='y'):
        raise ValueError('The cube must contain x and y axes.')

    x_coords = cube.coords(axis='x')
    y_coords = cube.coords(axis='y')
    if len(x_coords) != 1 or len(y_coords) != 1:
        raise ValueError('The cube must contain one, and only one, coordinate for each of the x and y axes.')

    x_coord = x_coords[0]
    y_coord = y_coords[0]
    if not (x_coord.has_bounds() and y_coord.has_bounds()):
        raise ValueError('Both horizontal coordinates must have bounds.')

    if x_coord.ndim != 1:
        raise iris.exceptions.CoordinateMultiDimError(x_coord)
    if y_coord.ndim != 1:
        raise iris.exceptions.CoordinateMultiDimError(y_coord)

    # Figure out the shape of the horizontal dimensions
    shape = [1] * len(cube.shape)
    x_dim = cube.coord_dims(x_coord)[0]
    y_dim = cube.coord_dims(y_coord)[0]
    shape[x_dim] = x_coord.shape[0]
    shape[y_dim] = y_coord.shape[0]
    weights = np.empty(shape, np.float32)

    # Calculate the area weights
    x_bounds = x_coord.bounds
    y_bounds = y_coord.bounds
    for nd_index in np.ndindex(weights.shape):
        xi = nd_index[x_dim]
        yi = nd_index[y_dim]
        x0, x1 = x_bounds[xi]
        y0, y1 = y_bounds[yi]
        polygon = Polygon([(x0, y0), (x0, y1), (x1, y1), (x1, y0)])
        if normalize:
            weights[nd_index] = polygon.intersection(geometry).area / polygon.area
        else:
            weights[nd_index] = polygon.intersection(geometry).area

    # Fix for the limitation of iris.analysis.MEAN weights handling.
    # Broadcast the array to the full shape of the cube
    weights = np.broadcast_arrays(weights, cube.data)[0]

    return weights

########NEW FILE########
__FILENAME__ = interpolate
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Interpolation and re-gridding routines.

See also: :mod:`NumPy <numpy>`, and :ref:`SciPy <scipy:modindex>`.

"""
import collections
import warnings

import numpy as np
import numpy.ma as ma
import scipy
import scipy.spatial
from scipy.interpolate.interpolate import interp1d

from iris.analysis import Linear
import iris.cube
import iris.coord_systems
import iris.coords
import iris.exceptions


def _ll_to_cart(lon, lat):
    # Based on cartopy.img_transform.ll_to_cart()
    x = np.sin(np.deg2rad(90 - lat)) * np.cos(np.deg2rad(lon))
    y = np.sin(np.deg2rad(90 - lat)) * np.sin(np.deg2rad(lon))
    z = np.cos(np.deg2rad(90 - lat))
    return (x, y, z)

def _cartesian_sample_points(sample_points, sample_point_coord_names):
    # Replace geographic latlon with cartesian xyz.
    # Generates coords suitable for nearest point calculations with scipy.spatial.cKDTree.
    #
    # Input:
    # sample_points[coord][datum] : list of sample_positions for each datum, formatted for fast use of _ll_to_cart()
    # sample_point_coord_names[coord] : list of n coord names
    #
    # Output:
    # list of [x,y,z,t,etc] positions, formatted for kdtree

    # Find lat and lon coord indices
    i_lat = i_lon = None
    i_non_latlon = range(len(sample_point_coord_names))
    for i, name in enumerate(sample_point_coord_names):
        if "latitude" in name:
            i_lat = i
            i_non_latlon.remove(i_lat)
        if "longitude" in name:
            i_lon = i
            i_non_latlon.remove(i_lon)

    if i_lat is None or i_lon is None:
        return sample_points.transpose()

    num_points = len(sample_points[0])
    cartesian_points = [None] * num_points

    # Get the point coordinates without the latlon
    for p in range(num_points):
        cartesian_points[p] = [sample_points[c][p] for c in i_non_latlon]

    # Add cartesian xyz coordinates from latlon
    x, y, z = _ll_to_cart(sample_points[i_lon], sample_points[i_lat])
    for p in range(num_points):
        cartesian_point = cartesian_points[p]
        cartesian_point.append(x[p])
        cartesian_point.append(y[p])
        cartesian_point.append(z[p])

    return cartesian_points


def nearest_neighbour_indices(cube, sample_points):
    """
    Returns the indices to select the data value(s) closest to the given coordinate point values.

    The sample_points mapping does not have to include coordinate values corresponding to all data
    dimensions. Any dimensions unspecified will default to a full slice.

    For example:

        >>> cube = iris.load_cube(iris.sample_data_path('ostia_monthly.nc'))
        >>> iris.analysis.interpolate.nearest_neighbour_indices(cube, [('latitude', 0), ('longitude', 10)])
        (slice(None, None, None), 9, 12)
        >>> iris.analysis.interpolate.nearest_neighbour_indices(cube, [('latitude', 0)])
        (slice(None, None, None), 9, slice(None, None, None))

    Args:

    * cube:
        An :class:`iris.cube.Cube`.
    * sample_points
        A list of tuple pairs mapping coordinate instances or unique coordinate names in the cube to point values.

    Returns:
        The tuple of indices which will select the point in the cube closest to the supplied coordinate values.

    .. note::

        Nearest neighbour interpolation of multidimensional coordinates is not
        yet supported.

    """
    if isinstance(sample_points, dict):
        warnings.warn('Providing a dictionary to specify points is deprecated. Please provide a list of (coordinate, values) pairs.')
        sample_points = sample_points.items()

    if sample_points:
        try:
            coord, values = sample_points[0]
        except ValueError:
            raise ValueError('Sample points must be a list of (coordinate, value) pairs. Got %r.' % sample_points)

    points = []
    for coord, values in sample_points:
        if isinstance(coord, basestring):
            coord = cube.coord(coord)
        else:
            coord = cube.coord(coord)
        points.append((coord, values))
    sample_points = points

    # Build up a list of indices to span the cube.
    indices = [slice(None, None)] * cube.ndim
    
    # Build up a dictionary which maps the cube's data dimensions to a list (which will later
    # be populated by coordinates in the sample points list)
    dim_to_coord_map = {}
    for i in range(cube.ndim):
        dim_to_coord_map[i] = []

    # Iterate over all of the specifications provided by sample_points
    for coord, point in sample_points:
        data_dim = cube.coord_dims(coord)

        # If no data dimension then we don't need to make any modifications to indices.
        if not data_dim:
            continue
        elif len(data_dim) > 1:
            raise iris.exceptions.CoordinateMultiDimError("Nearest neighbour interpolation of multidimensional "
                                                          "coordinates is not supported.")
        data_dim = data_dim[0]

        dim_to_coord_map[data_dim].append(coord)

        #calculate the nearest neighbour
        min_index = coord.nearest_neighbour_index(point)

        if getattr(coord, 'circular', False):
            warnings.warn("Nearest neighbour on a circular coordinate may not be picking the nearest point.", DeprecationWarning)

        # If the dimension has already been interpolated then assert that the index from this coordinate
        # agrees with the index already calculated, otherwise we have a contradicting specification
        if indices[data_dim] != slice(None, None) and min_index != indices[data_dim]:
            raise ValueError('The coordinates provided (%s) over specify dimension %s.' %
                                        (', '.join([coord.name() for coord in dim_to_coord_map[data_dim]]), data_dim))

        indices[data_dim] = min_index

    return tuple(indices)


def _nearest_neighbour_indices_ndcoords(cube, sample_point, cache=None):
    """
    See documentation for :func:`iris.analysis.interpolate.nearest_neighbour_indices`.

    This function is adapted for points sampling a multi-dimensional coord,
    and can currently only do nearest neighbour interpolation.

    Because this function can be slow for multidimensional coordinates,
    a 'cache' dictionary can be provided by the calling code.

    """

    # Developer notes:
    # A "sample space cube" is made which only has the coords and dims we are sampling on.
    # We get the nearest neighbour using this sample space cube.

    if isinstance(sample_point, dict):
        warnings.warn('Providing a dictionary to specify points is deprecated. Please provide a list of (coordinate, values) pairs.')
        sample_point = sample_point.items()

    if sample_point:
        try:
            coord, value = sample_point[0]
        except ValueError:
            raise ValueError('Sample points must be a list of (coordinate, value) pairs. Got %r.' % sample_point)

    # Convert names to coords in sample_point
    point = []
    ok_coord_ids = set(map(id, cube.dim_coords + cube.aux_coords))
    for coord, value in sample_point:
        if isinstance(coord, basestring):
            coord = cube.coord(coord)
        else:
            coord = cube.coord(coord)
        if id(coord) not in ok_coord_ids:
            msg = ('Invalid sample coordinate {!r}: derived coordinates are'
                   ' not allowed.'.format(coord.name()))
            raise ValueError(msg)
        point.append((coord, value))

    # Reformat sample_point for use in _cartesian_sample_points(), below.
    sample_point = np.array([[value] for coord, value in point])
    sample_point_coords = [coord for coord, value in point]
    sample_point_coord_names = [coord.name() for coord, value in point]

    # Which dims are we sampling?
    sample_dims = set()
    for coord in sample_point_coords:
        for dim in cube.coord_dims(coord):
            sample_dims.add(dim)
    sample_dims = sorted(list(sample_dims))

    # Extract a sub cube that lives in just the sampling space.
    sample_space_slice = [0] * cube.ndim
    for sample_dim in sample_dims:
        sample_space_slice[sample_dim] = slice(None, None)
    sample_space_slice = tuple(sample_space_slice)
    sample_space_cube = cube[sample_space_slice]

    #...with just the sampling coords
    for coord in sample_space_cube.coords():
        if not coord.name() in sample_point_coord_names:
            sample_space_cube.remove_coord(coord)

    # Order the sample point coords according to the sample space cube coords
    sample_space_coord_names = [coord.name() for coord in sample_space_cube.coords()]
    new_order = [sample_space_coord_names.index(name) for name in sample_point_coord_names]
    sample_point = np.array([sample_point[i] for i in new_order])
    sample_point_coord_names = [sample_point_coord_names[i] for i in new_order]

    # Convert the sample point to cartesian coords.
    # If there is no latlon within the coordinate there will be no change.
    # Otherwise, geographic latlon is replaced with cartesian xyz.
    cartesian_sample_point = _cartesian_sample_points(sample_point, sample_point_coord_names)[0]

    sample_space_coords = sample_space_cube.dim_coords + sample_space_cube.aux_coords
    sample_space_coords_and_dims = [(coord, sample_space_cube.coord_dims(coord)) for coord in sample_space_coords]

    if cache is not None and cube in cache:
        kdtree = cache[cube]
    else:
        # Create a "sample space position" for each datum: sample_space_data_positions[coord_index][datum_index]
        sample_space_data_positions = np.empty((len(sample_space_coords_and_dims), sample_space_cube.data.size), dtype=float)
        for d, ndi in enumerate(np.ndindex(sample_space_cube.data.shape)):
            for c, (coord, coord_dims) in enumerate(sample_space_coords_and_dims):
                # Index of this datum along this coordinate (could be nD).
                keys = tuple(ndi[ind] for ind in coord_dims) if coord_dims else slice(None, None)
                # Position of this datum along this coordinate.
                sample_space_data_positions[c][d] = coord.points[keys]

        # Convert to cartesian coordinates. Flatten for kdtree compatibility.
        cartesian_space_data_coords = _cartesian_sample_points(sample_space_data_positions, sample_point_coord_names)

        # Get the nearest datum index to the sample point. This is the goal of the function.
        kdtree = scipy.spatial.cKDTree(cartesian_space_data_coords)

    cartesian_distance, datum_index = kdtree.query(cartesian_sample_point)
    sample_space_ndi = np.unravel_index(datum_index, sample_space_cube.data.shape)

    # Turn sample_space_ndi into a main cube slice.
    # Map sample cube to main cube dims and leave the rest as a full slice.
    main_cube_slice = [slice(None, None)] * cube.ndim
    for sample_coord, sample_coord_dims in sample_space_coords_and_dims:
        # Find the coord in the main cube
        main_coord = cube.coord(sample_coord.name())
        main_coord_dims = cube.coord_dims(main_coord)
        # Mark the nearest data index/indices with respect to this coord
        for sample_i, main_i in zip(sample_coord_dims, main_coord_dims):
            main_cube_slice[main_i] = sample_space_ndi[sample_i]


    # Update cache
    if cache is not None:
        cache[cube] = kdtree

    return tuple(main_cube_slice)


def extract_nearest_neighbour(cube, sample_points):
    """
    Returns a new cube using data value(s) closest to the given coordinate point values.

    The sample_points mapping does not have to include coordinate values corresponding to all data
    dimensions. Any dimensions unspecified will default to a full slice.

    For example:

        >>> cube = iris.load_cube(iris.sample_data_path('ostia_monthly.nc'))
        >>> iris.analysis.interpolate.extract_nearest_neighbour(cube, [('latitude', 0), ('longitude', 10)])
        <iris 'Cube' of surface_temperature / (K) (time: 54)>
        >>> iris.analysis.interpolate.extract_nearest_neighbour(cube, [('latitude', 0)])
        <iris 'Cube' of surface_temperature / (K) (time: 54; longitude: 432)>

    Args:

    * cube:
        An :class:`iris.cube.Cube`.
    * sample_points
        A list of tuple pairs mapping coordinate instances or unique coordinate names in the cube to point values.

    Returns:
        A cube that represents uninterpolated data as near to the given points as possible.

    """
    return cube[nearest_neighbour_indices(cube, sample_points)]


def nearest_neighbour_data_value(cube, sample_points):
    """
    Returns the data value closest to the given coordinate point values.

    The sample_points mapping must include coordinate values corresponding to all data
    dimensions.

    For example:

        >>> cube = iris.load_cube(iris.sample_data_path('air_temp.pp'))
        >>> iris.analysis.interpolate.nearest_neighbour_data_value(cube, [('latitude', 0), ('longitude', 10)])
        299.21564
        >>> iris.analysis.interpolate.nearest_neighbour_data_value(cube, [('latitude', 0)])
        Traceback (most recent call last):
        ...
        ValueError: The sample points [('latitude', 0)] was not specific enough to return a single value from the cube.


    Args:

    * cube:
        An :class:`iris.cube.Cube`.
    * sample_points
        A list of tuple pairs mapping coordinate instances or unique coordinate names in the cube to point values.

    Returns:
        The data value at the point in the cube closest to the supplied coordinate values.

    """
    indices = nearest_neighbour_indices(cube, sample_points)
    for ind in indices:
        if isinstance(ind, slice):
            raise ValueError('The sample points given (%s) were not specific enough to return a '
                             'single value from the cube.' % sample_points)

    return cube.data[indices]


def regrid(source_cube, grid_cube, mode='bilinear', **kwargs):
    """
    Returns a new cube with values derived from the source_cube on the horizontal grid specified
    by the grid_cube.

    Fundamental input requirements:
        1) Both cubes must have a CoordSystem.
        2) The source 'x' and 'y' coordinates must not share data dimensions with any other coordinates.

    In addition, the algorithm currently used requires:
        3) Both CS instances must be compatible:
            i.e. of the same type, with the same attribute values, and with compatible coordinates.
        4) No new data dimensions can be created.
        5) Source cube coordinates to map to a single dimension.

    Args:

    * source_cube:
        An instance of :class:`iris.cube.Cube` which supplies the source data and metadata.
    * grid_cube:
        An instance of :class:`iris.cube.Cube` which supplies the horizontal grid definition.

    Kwargs:

    * mode (string):
        Regridding interpolation algorithm to be applied, which may be one of the following:

            * 'bilinear' for bi-linear interpolation (default), see :func:`iris.analysis.interpolate.linear`.
            * 'nearest' for nearest neighbour interpolation.

    Returns:
        A new :class:`iris.cube.Cube` instance.

    .. note::

        The masked status of values are currently ignored.  See :func:\
`~iris.experimental.regrid.regrid_bilinear_rectilinear_src_and_grid`
        for regrid support with mask awareness.

    """
    # Condition 1
    source_cs = source_cube.coord_system(iris.coord_systems.CoordSystem)
    grid_cs = grid_cube.coord_system(iris.coord_systems.CoordSystem)
    if (source_cs is None) != (grid_cs is None):
        raise ValueError("The source and grid cubes must both have a CoordSystem or both have None.")

    # Condition 2: We can only have one x coordinate and one y coordinate with the source CoordSystem, and those coordinates
    # must be the only ones occupying their respective dimension
    source_x = source_cube.coord(axis='x', coord_system=source_cs)
    source_y = source_cube.coord(axis='y', coord_system=source_cs)

    source_x_dims = source_cube.coord_dims(source_x)
    source_y_dims = source_cube.coord_dims(source_y)

    source_x_dim = None
    if source_x_dims:
        if len(source_x_dims) > 1:
            raise ValueError('The source x coordinate may not describe more than one data dimension.')
        source_x_dim = source_x_dims[0]
        dim_sharers = ', '.join([coord.name() for coord in source_cube.coords(contains_dimension=source_x_dim) if coord is not source_x])
        if dim_sharers:
            raise ValueError('No coordinates may share a dimension (dimension %s) with the x '
                             'coordinate, but (%s) do.' % (source_x_dim, dim_sharers))

    source_y_dim = None
    if source_y_dims:
        if len(source_y_dims) > 1:
            raise ValueError('The source y coordinate may not describe more than one data dimension.')
        source_y_dim = source_y_dims[0]
        dim_sharers = ', '.join([coord.name() for coord in source_cube.coords(contains_dimension=source_y_dim) if coord is not source_y])
        if dim_sharers:
            raise ValueError('No coordinates may share a dimension (dimension %s) with the y '
                             'coordinate, but (%s) do.' % (source_y_dim, dim_sharers))

    if source_x_dim is not None and source_y_dim == source_x_dim:
        raise ValueError('The source x and y coords may not describe the same data dimension.')


    # Condition 3
    # Check for compatible horizontal CSs. Currently that means they're exactly the same except for the coordinate
    # values.
    # The same kind of CS ...
    compatible = (source_cs == grid_cs)
    if compatible:
        grid_x = grid_cube.coord(axis='x', coord_system=grid_cs)
        grid_y = grid_cube.coord(axis='y', coord_system=grid_cs)
        compatible = source_x.is_compatible(grid_x) and \
            source_y.is_compatible(grid_y)
    if not compatible:
        raise ValueError("The new grid must be defined on the same coordinate system, and have the same coordinate "
                         "metadata, as the source.")

    # Condition 4
    if grid_cube.coord_dims(grid_x) and not source_x_dims or \
            grid_cube.coord_dims(grid_y) and not source_y_dims:
        raise ValueError("The new grid must not require additional data dimensions.")

    x_coord = grid_x.copy()
    y_coord = grid_y.copy()


    #
    # Adjust the data array to match the new grid.
    #

    # get the new shape of the data
    new_shape = list(source_cube.shape)
    if source_x_dims:
        new_shape[source_x_dims[0]] = grid_x.shape[0]
    if source_y_dims:
        new_shape[source_y_dims[0]] = grid_y.shape[0]

    new_data = np.empty(new_shape, dtype=source_cube.data.dtype)

    # Prepare the index pattern which will be used to insert a single "column" of data.
    # NB. A "column" is a slice constrained to a single XY point, which therefore extends over *all* the other axes.
    # For an XYZ cube this means a column only extends over Z and corresponds to the normal definition of "column".
    indices = [slice(None, None)] * new_data.ndim

    if mode == 'bilinear':
        # Perform bilinear interpolation, passing through any keywords.
        points_dict = [(source_x, list(x_coord.points)), (source_y, list(y_coord.points))]
        new_data = linear(source_cube, points_dict, **kwargs).data
    else:
        # Perform nearest neighbour interpolation on each column in turn.
        for iy, y in enumerate(y_coord.points):
            for ix, x in enumerate(x_coord.points):
                column_pos = [(source_x,  x), (source_y, y)]
                column_data = extract_nearest_neighbour(source_cube, column_pos).data
                if source_y_dim is not None:
                    indices[source_y_dim] = iy
                if source_x_dim is not None:
                    indices[source_x_dim] = ix
                new_data[tuple(indices)] = column_data

    # Special case to make 0-dimensional results take the same form as NumPy
    if new_data.shape == ():
        new_data = new_data.flat[0]

    # Start with just the metadata and the re-sampled data...
    new_cube = iris.cube.Cube(new_data)
    new_cube.metadata = source_cube.metadata

    # ... and then copy across all the unaffected coordinates.

    # Record a mapping from old coordinate IDs to new coordinates,
    # for subsequent use in creating updated aux_factories.
    coord_mapping = {}

    def copy_coords(source_coords, add_method):
        for coord in source_coords:
            if coord is source_x or coord is source_y:
                continue
            dims = source_cube.coord_dims(coord)
            new_coord = coord.copy()
            add_method(new_coord, dims)
            coord_mapping[id(coord)] = new_coord

    copy_coords(source_cube.dim_coords, new_cube.add_dim_coord)
    copy_coords(source_cube.aux_coords, new_cube.add_aux_coord)

    for factory in source_cube.aux_factories:
        new_cube.add_aux_factory(factory.updated(coord_mapping))

    # Add the new coords
    if source_x in source_cube.dim_coords:
        new_cube.add_dim_coord(x_coord, source_x_dim)
    else:
        new_cube.add_aux_coord(x_coord, source_x_dims)

    if source_y in source_cube.dim_coords:
        new_cube.add_dim_coord(y_coord, source_y_dim)
    else:
        new_cube.add_aux_coord(y_coord, source_y_dims)

    return new_cube


def regrid_to_max_resolution(cubes, **kwargs):
    """
    Returns all the cubes re-gridded to the highest horizontal resolution.

    Horizontal resolution is defined by the number of grid points/cells covering the horizontal plane.
    See :func:`iris.analysis.interpolation.regrid` regarding mode of interpolation.

    Args:

    * cubes:
        An iterable of :class:`iris.cube.Cube` instances.

    Returns:
        A list of new :class:`iris.cube.Cube` instances.

    """
    # TODO: This could be significantly improved for readability and functionality.
    resolution = lambda cube_: (cube_.shape[cube_.coord_dims(cube_.coord(axis="x"))[0]]) * (cube_.shape[cube_.coord_dims(cube_.coord(axis="y"))[0]])
    grid_cube = max(cubes, key=resolution)
    return [cube.regridded(grid_cube, **kwargs) for cube in cubes]


def linear(cube, sample_points, extrapolation_mode='linear'):
    """
    Return a cube of the linearly interpolated points given the desired
    sample points.

    Given a list of tuple pairs mapping coordinates (or coordinate names)
    to their desired values, return a cube with linearly interpolated values.
    If more than one coordinate is specified, the linear interpolation will be
    carried out in sequence, thus providing n-linear interpolation
    (bi-linear, tri-linear, etc.).

    If the input cube's data is masked, the result cube will have a data
    mask interpolated to the new sample points

    .. testsetup::

        import numpy as np

    For example:

        >>> cube = iris.load_cube(iris.sample_data_path('air_temp.pp'))
        >>> sample_points = [('latitude', np.linspace(-90, 90, 10)),
        ...                  ('longitude', np.linspace(-180, 180, 20))]
        >>> iris.analysis.interpolate.linear(cube, sample_points)
        <iris 'Cube' of air_temperature / (K) (latitude: 10; longitude: 20)>

    .. note::

        By definition, linear interpolation requires all coordinates to
        be 1-dimensional.

    .. note::

        If a specified coordinate is single valued its value will be
        extrapolated to the desired sample points by assuming a gradient of
        zero.

    Args:

    * cube
        The cube to be interpolated.

    * sample_points
        List of one or more tuple pairs mapping coordinate to desired
        points to interpolate. Points may be a scalar or a numpy array
        of values.  Multi-dimensional coordinates are not supported.

    Kwargs:

    * extrapolation_mode - string - one of 'linear', 'nan' or 'error'

        * If 'linear' the point will be calculated by extending the
          gradient of closest two points.
        * If 'nan' the extrapolation point will be put as a NAN.
        * If 'error' a value error will be raised notifying of the
          attempted extrapolation.

    .. note::

        If the source cube's data, or any of its resampled coordinates,
        have an integer data type they will be promoted to a floating
        point data type in the result.

    """
    if isinstance(sample_points, dict):
        sample_points = sample_points.items()
    
    # catch the case where a user passes a single (coord/name, value) pair rather than a list of pairs
    if sample_points and not (isinstance(sample_points[0], collections.Container) and not isinstance(sample_points[0], basestring)):
        raise TypeError('Expecting the sample points to be a list of tuple pairs representing (coord, points), got a list of %s.' % type(sample_points[0]))

    scheme = Linear(extrapolation_mode)
    return cube.interpolate(scheme, sample_points)


def _interp1d_rolls_y():
    """
    Determines if :class:`scipy.interpolate.interp1d` rolls its array `y` by
    comparing the shape of y passed into interp1d to the shape of its internal
    representation of y.

    SciPy v0.13.x+ no longer rolls the axis of its internal representation
    of y so we test for this occurring to prevent us subsequently
    extrapolating along the wrong axis.

    For further information on this change see, for example:
        * https://github.com/scipy/scipy/commit/0d906d0fc54388464603c63119b9e35c9a9c4601
          (the commit that introduced the change in behaviour).
        * https://github.com/scipy/scipy/issues/2621
          (a discussion on the change - note the issue is not resolved
          at time of writing).

    """
    y = np.arange(12).reshape(3, 4)
    f = interp1d(np.arange(3), y, axis=0)
    # If the initial shape of y and the shape internal to interp1d are *not*
    # the same then scipy.interp1d rolls y.
    return y.shape != f.y.shape


class Linear1dExtrapolator(object):
    """
    Extension class to :class:`scipy.interpolate.interp1d` to provide linear extrapolation.

    See also: :mod:`scipy.interpolate`.

    """
    roll_y = _interp1d_rolls_y()

    def __init__(self, interpolator):
        """
        Given an already created :class:`scipy.interpolate.interp1d` instance, return a callable object
        which supports linear extrapolation.

        """
        self._interpolator = interpolator
        self.x = interpolator.x
        # Store the y values given to the interpolator.
        self.y = interpolator.y
        """
        The y values given to the interpolator object.

        .. note:: These are stored with the interpolator.axis last.

        """
        # Roll interpolator.axis to the end if scipy no longer does it for us.
        if not self.roll_y:
            self.y = np.rollaxis(self.y, self._interpolator.axis, self.y.ndim)

    def all_points_in_range(self, requested_x):
        """Given the x points, do all of the points sit inside the interpolation range."""
        test = (requested_x >= self.x[0]) & (requested_x <= self.x[-1])
        if isinstance(test, np.ndarray):
            test = test.all()
        return test

    def __call__(self, requested_x):
        if not self.all_points_in_range(requested_x):
            # cast requested_x to a numpy array if it is not already.
            if not isinstance(requested_x, np.ndarray):
                requested_x = np.array(requested_x)

            # we need to catch the special case of providing a single value...
            remember_that_i_was_0d = requested_x.ndim == 0

            requested_x = requested_x.flatten()

            gt = np.where(requested_x > self.x[-1])[0]
            lt = np.where(requested_x < self.x[0])[0]
            ok = np.where( (requested_x >= self.x[0]) & (requested_x <= self.x[-1]) )[0]

            data_shape = list(self.y.shape)
            data_shape[-1] = len(requested_x)
            result = np.empty(data_shape, dtype=self._interpolator(self.x[0]).dtype)

            # Make a variable to represent the slice into the resultant data. (This will be updated in each of gt, lt & ok)
            interpolator_result_index = [slice(None, None)] * self.y.ndim

            if len(ok) != 0:
                interpolator_result_index[-1] = ok

                r = self._interpolator(requested_x[ok])
                # Reshape the properly formed array to put the interpolator.axis last i.e. dims 0, 1, 2 -> 0, 2, 1 if axis = 1
                axes = range(r.ndim)
                del axes[self._interpolator.axis]
                axes.append(self._interpolator.axis)

                result[interpolator_result_index] = r.transpose(axes)

            if len(lt) != 0:
                interpolator_result_index[-1] = lt

                grad = (self.y[..., 1:2] - self.y[..., 0:1]) / (self.x[1] - self.x[0])
                result[interpolator_result_index] = self.y[..., 0:1] + (requested_x[lt] - self.x[0]) * grad

            if len(gt) != 0:
                interpolator_result_index[-1] = gt

                grad = (self.y[..., -1:] - self.y[..., -2:-1]) / (self.x[-1] - self.x[-2])
                result[interpolator_result_index] = self.y[..., -1:] + (requested_x[gt] - self.x[-1]) * grad

            axes = range(len(interpolator_result_index))
            axes.insert(self._interpolator.axis, axes.pop(axes[-1]))
            result = result.transpose(axes)

            if remember_that_i_was_0d:
                new_shape = list(result.shape)
                del new_shape[self._interpolator.axis]
                result = result.reshape(new_shape)

            return result
        else:
            return self._interpolator(requested_x)

########NEW FILE########
__FILENAME__ = maths
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Basic mathematical and statistical operations.

"""
from __future__ import division
import warnings
import math
import operator

import numpy as np

import iris.analysis
import iris.coords
import iris.cube
import iris.exceptions
import iris.util


def abs(cube, in_place=False):
    """
    Calculate the absolute values of the data in the Cube provided.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.

    Kwargs:

    * in_place:
        Whether to create a new Cube, or alter the given "cube".

    Returns:
        An instance of :class:`iris.cube.Cube`.

    """
    return _math_op_common(cube, np.abs, cube.units, in_place=in_place)


def intersection_of_cubes(cube, other_cube):
    """
    Return the two Cubes of intersection given two Cubes.

    .. note:: The intersection of cubes function will ignore all single valued
        coordinates in checking the intersection.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.
    * other_cube:
        An instance of :class:`iris.cube.Cube`.

    Returns:
        A pair of :class:`iris.cube.Cube` instances in a tuple corresponding
        to the original cubes restricted to their intersection.

    """
    # Take references of the original cubes (which will be copied when slicing later)
    new_cube_self = cube
    new_cube_other = other_cube

    # This routine has not been written to cope with multi-dimensional coordinates.
    for coord in cube.coords() + other_cube.coords():
        if coord.ndim != 1:
            raise iris.exceptions.CoordinateMultiDimError(coord)

    coord_comp = iris.analysis.coord_comparison(cube, other_cube)

    if coord_comp['ungroupable_and_dimensioned']:
        raise ValueError('Cubes do not share all coordinates in common, cannot intersect.')

    # cubes must have matching coordinates
    for coord in cube.coords():
        other_coord = other_cube.coord(coord)

        # Only intersect coordinates which are different, single values coordinates may differ.
        if coord.shape[0] > 1 and coord != other_coord:
            intersected_coord = coord.intersect(other_coord)
            new_cube_self = new_cube_self.subset(intersected_coord)
            new_cube_other = new_cube_other.subset(intersected_coord)

    return new_cube_self, new_cube_other


def _assert_is_cube(cube):
    if not isinstance(cube, iris.cube.Cube):
        raise TypeError('The "cube" argument must be an instance of '
                        'iris.cube.Cube.')


def _assert_compatible(cube, other):
    """Checks to see if cube.data and another array can be broadcast to the same shape using ``numpy.broadcast_arrays``."""
    # This code previously returned broadcasted versions of the cube data and the other array.
    # As numpy.broadcast_arrays does not work with masked arrays (it returns them as ndarrays) operations
    # involving masked arrays would be broken.

    try:
        data_view, other_view = np.broadcast_arrays(cube.data, other)
    except ValueError, err:
        # re-raise
        raise ValueError("The array was not broadcastable to the cube's data shape. The error message from numpy when broadcasting:\n%s\n"
                         "The cube's shape was %s and the array's shape was %s" % (err, cube.shape, other.shape))

    if cube.shape != data_view.shape:
        raise ValueError("The array operation would increase the dimensionality of the cube. The new cubes data would "
                         "have had to become: %s" % (data_view.shape, ))


def _assert_matching_units(cube, other, operation_noun):
    """
    Check that the units of the cube and the other item are the same, or if
    the other does not have a unit, skip this test
    """
    if cube.units != getattr(other, 'units', cube.units):
        raise iris.exceptions.NotYetImplementedError(
            'Differing units (%s & %s) %s not implemented' %
            (cube.units, other.units, operation_noun))


def add(cube, other, dim=None, ignore=True, in_place=False):
    """
    Calculate the sum of two cubes, or the sum of a cube and a coordinate or scalar
    value.

    When summing two cubes, they must both have the same coordinate systems & data resolution.

    When adding a coordinate to a cube, they must both share the same number of elements
    along a shared axis.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.
    * other:
        An instance of :class:`iris.cube.Cube` or :class:`iris.coords.Coord`,
        or a number or :class:`numpy.ndarray`.

    Kwargs:

    * dim:
        If supplying a coord with no match on the cube, you must supply the dimension to process.
    * in_place:
        Whether to create a new Cube, or alter the given "cube".

    Returns:
        An instance of :class:`iris.cube.Cube`.

    """
    op = operator.iadd if in_place else operator.add
    return _add_subtract_common(op, 'addition', 'added', cube, other, dim=dim,
                                ignore=ignore, in_place=in_place)


def subtract(cube, other, dim=None, ignore=True, in_place=False):
    """
    Calculate the difference between two cubes, or the difference between
    a cube and a coordinate or scalar value.

    When subtracting two cubes, they must both have the same coordinate systems & data resolution.

    When subtracting a coordinate to a cube, they must both share the same number of elements
    along a shared axis.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.
    * other:
        An instance of :class:`iris.cube.Cube` or :class:`iris.coords.Coord`,
        or a number or :class:`numpy.ndarray`.

    Kwargs:

    * dim:
        If supplying a coord with no match on the cube, you must supply the dimension to process.
    * in_place:
        Whether to create a new Cube, or alter the given "cube".

    Returns:
        An instance of :class:`iris.cube.Cube`.

    """
    op = operator.isub if in_place else operator.sub
    return _add_subtract_common(op, 'subtraction', 'subtracted', cube, other,
                                dim=dim, ignore=ignore, in_place=in_place)


def _add_subtract_common(operation_function, operation_noun,
                         operation_past_tense, cube, other, dim=None,
                         ignore=True, in_place=False):
    """
    Function which shares common code between addition and subtraction of cubes.

    operation_function   - function which does the operation (e.g. numpy.subtract)
    operation_symbol     - the textual symbol of the operation (e.g. '-')
    operation_noun       - the noun of the operation (e.g. 'subtraction')
    operation_past_tense - the past tense of the operation (e.g. 'subtracted')

    """
    _assert_is_cube(cube)
    _assert_matching_units(cube, other, operation_noun)

    if isinstance(other, iris.cube.Cube):
        # get a coordinate comparison of this cube and the cube to do the
        # operation with
        coord_comp = iris.analysis.coord_comparison(cube, other)

        # provide a deprecation warning if the ignore keyword has been set
        if ignore is not True:
            warnings.warn('The "ignore" keyword has been deprecated in '
                          'add/subtract. This functionality is now automatic. '
                          'The provided value to "ignore" has been ignored, '
                          'and has been automatically calculated.')

        bad_coord_grps = (coord_comp['ungroupable_and_dimensioned']
                          + coord_comp['resamplable'])
        if bad_coord_grps:
            raise ValueError('This operation cannot be performed as there are '
                             'differing coordinates (%s) remaining '
                             'which cannot be ignored.'
                             % ', '.join({coord_grp.name() for coord_grp
                                          in bad_coord_grps}))
    else:
        coord_comp = None

    new_cube = _binary_op_common(operation_function, operation_noun, cube,
                                 other, cube.units, dim, in_place)

    if coord_comp:
        # If a coordinate is to be ignored - remove it
        ignore = filter(None, [coord_grp[0] for coord_grp
                               in coord_comp['ignorable']])
        for coord in ignore:
            new_cube.remove_coord(coord)

    return new_cube


def multiply(cube, other, dim=None, in_place=False):
    """
    Calculate the product of a cube and another cube or coordinate.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.
    * other:
        An instance of :class:`iris.cube.Cube` or :class:`iris.coords.Coord`,
        or a number or :class:`numpy.ndarray`.

    Kwargs:

    * dim:
        If supplying a coord with no match on the cube, you must supply the dimension to process.

    Returns:
        An instance of :class:`iris.cube.Cube`.

    """
    _assert_is_cube(cube)
    other_unit = getattr(other, 'units', '1')
    new_unit = cube.units * other_unit
    op = operator.imul if in_place else operator.mul
    return _binary_op_common(op, 'multiplication', cube, other, new_unit, dim,
                             in_place=in_place)


def divide(cube, other, dim=None, in_place=False):
    """
    Calculate the division of a cube by a cube or coordinate.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.
    * other:
        An instance of :class:`iris.cube.Cube` or :class:`iris.coords.Coord`,
        or a number or :class:`numpy.ndarray`.

    Kwargs:

    * dim:
        If supplying a coord with no match on the cube, you must supply the dimension to process.

    Returns:
        An instance of :class:`iris.cube.Cube`.

    """
    _assert_is_cube(cube)
    other_unit = getattr(other, 'units', '1')
    new_unit = cube.units / other_unit
    op = operator.idiv if in_place else operator.div
    return _binary_op_common(op, 'divison', cube, other, new_unit, dim,
                             in_place=in_place)


def exponentiate(cube, exponent, in_place=False):
    """
    Returns the result of the given cube to the power of a scalar.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.
    * exponent:
        The integer or floating point exponent.

        .. note:: When applied to the cube's unit, the exponent must result in a unit
            that can be described using only integer powers of the basic units.

            e.g. Unit('meter^-2 kilogram second^-1')

    Kwargs:

    * in_place:
        Whether to create a new Cube, or alter the given "cube".

    Returns:
        An instance of :class:`iris.cube.Cube`.

    """
    _assert_is_cube(cube)
    def power(data, out=None):
        return np.power(data, exponent, out)
    return _math_op_common(cube, power, cube.units ** exponent,
                           in_place=in_place)


def exp(cube, in_place=False):
    """
    Calculate the exponential (exp(x)) of the cube.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.

    .. note::

        Taking an exponential will return a cube with dimensionless units.

    Kwargs:

    * in_place:
        Whether to create a new Cube, or alter the given "cube".

    Returns:
        An instance of :class:`iris.cube.Cube`.

    """
    return _math_op_common(cube, np.exp, iris.unit.Unit('1'),
                           in_place=in_place)


def log(cube, in_place=False):
    """
    Calculate the natural logarithm (base-e logarithm) of the cube.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.

    Kwargs:

    * in_place:
        Whether to create a new Cube, or alter the given "cube".

    Returns:
        An instance of :class:`iris.cube.Cube`.

    """
    return _math_op_common(cube, np.log, cube.units.log(math.e),
                           in_place=in_place)


def log2(cube, in_place=False):
    """
    Calculate the base-2 logarithm of the cube.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.

    Kwargs:

    * in_place:
        Whether to create a new Cube, or alter the given "cube".

    Returns:
        An instance of :class:`iris.cube.Cube`.

    """
    return _math_op_common(cube, np.log2, cube.units.log(2),
                           in_place=in_place)


def log10(cube, in_place=False):
    """
    Calculate the base-10 logarithm of the cube.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.

    Kwargs:

    * in_place:
        Whether to create a new Cube, or alter the given "cube".

    Returns:
        An instance of :class:`iris.cube.Cube`.

    """
    return _math_op_common(cube, np.log10, cube.units.log(10),
                           in_place=in_place)


def _binary_op_common(operation_function, operation_noun, cube, other,
                      new_unit, dim=None, in_place=False):
    """
    Function which shares common code between binary operations.

    operation_function   - function which does the operation (e.g. numpy.divide)
    operation_noun       - the noun of the operation (e.g. 'division')
    cube                 - the cube whose data is used as the first argument
                           to `operation_function`
    other                - the cube, coord, ndarray or number whose data is
                           used as the second argument
    new_unit             - unit for the resulting quantity
    dim                  - dimension along which to apply `other` if it's a
                           coordinate that is not found in `cube`
    in_place             - whether or not to apply the operation in place to
                           `cube` and `cube.data`
    """
    _assert_is_cube(cube)

    if isinstance(other, iris.coords.Coord):
        other = _broadcast_cube_coord_data(cube, other, operation_noun, dim)
    elif isinstance(other, iris.cube.Cube):
        try:
            np.broadcast_arrays(cube.data, other.data)
        except ValueError:
            other = iris.util.as_compatible_shape(other, cube).data
        else:
            other = other.data

    # don't worry about checking for other data types (such as scalars or
    # np.ndarrays) because _assert_compatible validates that they are broadcast
    # compatible with cube.data
    _assert_compatible(cube, other)

    def unary_func(x):
        ret = operation_function(x, other)
        if ret is NotImplemented:
            # explicitly raise the TypeError, so it gets raised even if, for
            # example, `iris.analysis.maths.multiply(cube, other)` is called
            # directly instead of `cube * other`
            raise TypeError('cannot %s %r and %r objects' %
                            (operation_function.__name__, type(x).__name__,
                             type(other).__name__))
        return ret
    return _math_op_common(cube, unary_func, new_unit, in_place)


def _broadcast_cube_coord_data(cube, other, operation_noun, dim=None):
    # What dimension are we processing?
    data_dimension = None
    if dim is not None:
        # Ensure the given dim matches the coord
        if other in cube.coords() and cube.coord_dims(other) != [dim]:
            raise ValueError("dim provided does not match dim found for coord")
        data_dimension = dim
    else:
        # Try and get a coord dim
        if other.shape != (1,):
            try:
                coord_dims = cube.coord_dims(other)
                data_dimension = coord_dims[0] if coord_dims else None
            except iris.exceptions.CoordinateNotFoundError:
                raise ValueError("Could not determine dimension for %s. "
                                 "Use %s(cube, coord, dim=dim)"
                                 % (operation_noun, operation_noun))

    if other.ndim != 1:
        raise iris.exceptions.CoordinateMultiDimError(other)

    if other.has_bounds():
        warnings.warn('%s by a bounded coordinate not well defined, ignoring '
                      'bounds.' % operation_noun)

    points = other.points

    # If the `data_dimension` is defined then shape the provided points for
    # proper array broadcasting
    if data_dimension is not None:
        points_shape = [1] * cube.ndim
        points_shape[data_dimension] = -1
        points = points.reshape(points_shape)

    return points


def _math_op_common(cube, operation_function, new_unit, in_place=False):
    _assert_is_cube(cube)
    if in_place:
        new_cube = cube
        try:
            operation_function(new_cube.data, out=new_cube.data)
        except TypeError:
            # Non ufunc function
            operation_function(new_cube.data)
    else:
        new_cube = cube.copy(data=operation_function(cube.data))
    iris.analysis.clear_phenomenon_identity(new_cube)
    new_cube.units = new_unit
    return new_cube

########NEW FILE########
__FILENAME__ = stats
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Statistical operations between cubes.

"""

import numpy as np

import iris


def _get_calc_view(cube_a, cube_b, corr_coords):
    """
    This function takes two cubes and returns cubes which are
    flattened so that efficient comparisons can be performed
    between the two.

    Args:

    * cube_a:
        First cube of data
    * cube_b:
        Second cube of data, compatible with cube_a
    * corr_coords:
        Names of the dimension coordinates over which
        to calculate correlations.

    Returns:

    * reshaped_a/reshaped_b:
        The data arrays of cube_a/cube_b reshaped
        so that the dimensions to be compared are
        flattened into the 0th dimension of the
        return array and other dimensions are
        preserved.
    * res_ind:
        The indices of the dimensions that we
        are not comparing, in terms of cube_a/cube_b

    """

    # Following lists to be filled with:
    # indices of dimension we are not comparing
    res_ind = []
    # indices of dimensions we are comparing
    slice_ind = []
    for i, c in enumerate(cube_a.dim_coords):
        if not c.name() in corr_coords:
            res_ind.append(i)
        else:
            slice_ind.append(i)

    # sanitise input
    dim_coord_names = [c.name() for c in cube_a.dim_coords]
    if corr_coords is None:
        corr_coords = dim_coord_names

    if ([c.name() for c in cube_a.dim_coords] !=
            [c.name() for c in cube_b.dim_coords]):
        raise ValueError("Cubes are incompatible.")

    for c in corr_coords:
        if c not in dim_coord_names:
            raise ValueError("%s coord "
                             "does not exist in cube." % c)

    # Reshape data to be data to correlate in 0th dim and
    # other grid points in 1st dim.
    # Transpose to group the correlation data dims before the
    # grid point dims.
    data_a = cube_a.data.view()
    data_b = cube_b.data.view()
    dim_i_len = np.prod(np.array(cube_a.shape)[slice_ind])
    dim_j_len = np.prod(np.array(cube_a.shape)[res_ind])
    reshaped_a = data_a.transpose(slice_ind+res_ind)\
                       .reshape(dim_i_len, dim_j_len)
    reshaped_b = data_b.transpose(slice_ind+res_ind)\
                       .reshape(dim_i_len, dim_j_len)

    return reshaped_a, reshaped_b, res_ind


def pearsonr(cube_a, cube_b, corr_coords=None):
    """
    Calculates the n-D Pearson's r correlation
    cube over the dimensions associated with the
    given coordinates.

    Returns a cube of the correlation between the two
    cubes along the dimensions of the given
    coordinates, at each point in the remaining
    dimensions of the cubes.

    For example providing two time/altitude/latitude/longitude
    cubes and corr_coords of 'latitude' and 'longitude' will result
    in a time/altitude cube describing the latitude/longitude
    (i.e. pattern) correlation at each time/altitude point.

    Args:

    * cube_a, cube_b (cubes):
        Between which the correlation field will be calculated.
        Cubes should be the same shape and have the
        same dimension coordinates.
    * corr_coords (list of str):
        The cube coordinate names over which to calculate
        correlations. If no names are provided then
        correlation will be calculated over all cube
        dimensions.

    Returns:
        Cube of correlations.

    Reference:
        http://www.statsoft.com/textbook/glosp.html#Pearson%20Correlation

    """

    # If no coords passed then set to all coords of cube.
    if corr_coords is None:
        corr_coords = [c.name() for c in cube_a.dim_coords]

    vec_a, vec_b, res_ind = _get_calc_view(cube_a,
                                           cube_b,
                                           corr_coords)

    sa = vec_a - np.mean(vec_a, 0)
    sb = vec_b - np.mean(vec_b, 0)
    flat_corrs = np.sum((sa*sb), 0)/np.sqrt(np.sum(sa**2, 0)*np.sum(sb**2, 0))

    corrs = flat_corrs.reshape([cube_a.shape[i] for i in res_ind])

    # Construct cube to hold correlation results.
    corrs_cube = iris.cube.Cube(corrs)
    corrs_cube.long_name = "Pearson's r"
    corrs_cube.units = "1"
    for i, dim in enumerate(res_ind):
        c = cube_a.dim_coords[dim]
        corrs_cube.add_dim_coord(c, i)

    return corrs_cube

########NEW FILE########
__FILENAME__ = trajectory
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Defines a Trajectory class, and a routine to extract a sub-cube along a trajectory.

"""

import math

import numpy as np

import iris.coord_systems
import iris.coords
import iris.analysis


class _Segment(object):
    """A single trajectory line segment: Two points, as described in the Trajectory class."""
    def __init__(self, p0, p1):
        #check keys
        if sorted(p0.keys()) != sorted(p1.keys()):
            raise ValueError("keys do not match")

        self.pts = [p0, p1]

        #calculate our length
        squares = 0
        for key in self.pts[0].keys():
            delta = self.pts[1][key] - self.pts[0][key]
            squares += delta * delta
        self.length = math.sqrt(squares)


class Trajectory(object):
    """A series of given waypoints with pre-calculated sample points."""

    def __init__(self, waypoints, sample_count=10):
        """
        Defines a trajectory using a sequence of waypoints.

        For example::

            waypoints = [{'latitude': 45, 'longitude': -60}, {'latitude': 45, 'longitude': 0}]
            Trajectory(waypoints)

        .. note:: All the waypoint dictionaries must contain the same coordinate names.

        Args:

        * waypoints
            A sequence of dictionaries, mapping coordinate names to values.

        Kwargs:

        * sample_count
            The number of sample positions to use along the trajectory.

        """
        self.waypoints = waypoints
        self.sample_count = sample_count

        # create line segments from the waypoints
        segments = [_Segment(self.waypoints[i], self.waypoints[i+1]) for i in range(len(self.waypoints) - 1)]

        # calculate our total length
        self.length = sum([seg.length for seg in segments])

        # generate our sampled points
        self.sampled_points = []
        sample_step = self.length / (self.sample_count - 1)

        #start with the first segment
        cur_seg_i = 0
        cur_seg = segments[cur_seg_i]
        len_accum = cur_seg.length
        for p in range(self.sample_count):

            # calculate the sample position along our total length
            sample_at_len = p * sample_step

            # skip forward to the containing segment
            while(len_accum < sample_at_len and cur_seg_i < len(segments)):
                cur_seg_i += 1
                cur_seg = segments[cur_seg_i]
                len_accum += cur_seg.length

            # how far through the segment is our sample point?
            seg_start_len = len_accum - cur_seg.length
            seg_frac = (sample_at_len-seg_start_len) / cur_seg.length

            # sample each coordinate in this segment, to create a new sampled point
            new_sampled_point = {}
            for key in cur_seg.pts[0].keys():
                seg_coord_delta = cur_seg.pts[1][key] - cur_seg.pts[0][key]
                new_sampled_point.update({key: cur_seg.pts[0][key] + seg_frac*seg_coord_delta})

            # add this new sampled point
            self.sampled_points.append(new_sampled_point)

    def __repr__(self):
        return 'Trajectory(%s, sample_count=%s)' % (self.waypoints, self.sample_count)


def interpolate(cube, sample_points, method=None):
    """
    Extract a sub-cube at the given n-dimensional points.

    Args:

    * cube
        The source Cube.

    * sample_points
        A sequence of coordinate (name) - values pairs.

    Kwargs:

    * method
        Request "linear" interpolation (default) or "nearest" neighbour.
        Only nearest neighbour is available when specifying multi-dimensional coordinates.


    For example::
    
        sample_points = [('latitude', [45, 45, 45]), ('longitude', [-60, -50, -40])]
        interpolated_cube = interpolate(cube, sample_points)

    """
    if method not in [None, "linear", "nearest"]:
        raise ValueError("Unhandled interpolation specified : %s" % method)

    # Convert any coordinate names to coords
    points = []
    for coord, values in sample_points:
        if isinstance(coord, basestring):
            coord = cube.coord(coord)
        points.append((coord, values))
    sample_points = points

    # Do all value sequences have the same number of values?
    coord, values = sample_points[0]
    trajectory_size = len(values)
    for coord, values in sample_points[1:]:
        if len(values) != trajectory_size:
            raise ValueError('Lengths of coordinate values are inconsistent.')

    # Which dimensions are we squishing into the last dimension?
    squish_my_dims = set()
    for coord, values in sample_points:
        dims = cube.coord_dims(coord)
        for dim in dims:
            squish_my_dims.add(dim)

    # Derive the new cube's shape by filtering out all the dimensions we're about to sample,
    # and then adding a new dimension to accommodate all the sample points.
    remaining = [(dim, size) for dim, size in enumerate(cube.shape) if dim not in squish_my_dims]
    new_data_shape = [size for dim, size in remaining]
    new_data_shape.append(trajectory_size)

    # Start with empty data and then fill in the "column" of values for each trajectory point.
    new_cube = iris.cube.Cube(np.empty(new_data_shape))
    new_cube.metadata = cube.metadata

    # Derive the mapping from the non-trajectory source dimensions to their
    # corresponding destination dimensions.
    remaining_dims = [dim for dim, size in remaining]
    dimension_remap = {dim: i for i, dim in enumerate(remaining_dims)}

    # Record a mapping from old coordinate IDs to new coordinates,
    # for subsequent use in creating updated aux_factories.
    coord_mapping = {}

    # Create all the non-squished coords
    for coord in cube.dim_coords:
        src_dims = cube.coord_dims(coord)
        if squish_my_dims.isdisjoint(src_dims):
            dest_dims = [dimension_remap[dim] for dim in src_dims]
            new_coord = coord.copy()
            new_cube.add_dim_coord(new_coord, dest_dims)
            coord_mapping[id(coord)] = new_coord
    for coord in cube.aux_coords:
        src_dims = cube.coord_dims(coord)
        if squish_my_dims.isdisjoint(src_dims):
            dest_dims = [dimension_remap[dim] for dim in src_dims]
            new_coord = coord.copy()
            new_cube.add_aux_coord(new_coord, dest_dims)
            coord_mapping[id(coord)] = new_coord

    # Create all the squished (non derived) coords, not filled in yet.
    trajectory_dim = len(remaining_dims)
    for coord in cube.dim_coords + cube.aux_coords:
        src_dims = cube.coord_dims(coord)
        if not squish_my_dims.isdisjoint(src_dims):
            points = np.array([coord.points.flatten()[0]] * trajectory_size)
            new_coord = iris.coords.AuxCoord(points,
                                             standard_name=coord.standard_name,
                                             long_name=coord.long_name,
                                             units=coord.units,
                                             bounds=None,
                                             attributes=coord.attributes,
                                             coord_system=coord.coord_system)
            new_cube.add_aux_coord(new_coord, trajectory_dim)
            coord_mapping[id(coord)] = new_coord

    for factory in cube.aux_factories:
        new_cube.add_aux_factory(factory.updated(coord_mapping))

    # Are the given coords all 1-dimensional? (can we do linear interp?)
    for coord, values in sample_points:
        if coord.ndim > 1:
            if method == "linear":
                raise iris.exceptions.CoordinateMultiDimError("Cannot currently perform linear interpolation for multi-dimensional coordinates.")
            method = "nearest"
            break

    # Use a cache with _nearest_neighbour_indices_ndcoords()
    cache = {}

    for i in range(trajectory_size):
        point = [(coord, values[i]) for coord, values in sample_points]

        if method in ["linear", None]:
            column = iris.analysis.interpolate.linear(cube, point)
            new_cube.data[..., i] = column.data
        elif method == "nearest":
            column_index = iris.analysis.interpolate._nearest_neighbour_indices_ndcoords(cube, point, cache=cache)
            column = cube[column_index]
            new_cube.data[..., i] = column.data

        # Fill in the empty squashed (non derived) coords.
        for column_coord in column.dim_coords + column.aux_coords:
            src_dims = cube.coord_dims(column_coord)
            if not squish_my_dims.isdisjoint(src_dims):
                if len(column_coord.points) != 1:
                    raise Exception("Expected to find exactly one point. Found %d" % len(column_coord.points))
                new_cube.coord(column_coord.name()).points[i] = column_coord.points[0]

    return new_cube

########NEW FILE########
__FILENAME__ = _interpolator
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
from itertools import product

import numpy as np
import numpy.ma as ma
from numpy.lib.stride_tricks import as_strided

from iris.analysis._scipy_interpolate import _RegularGridInterpolator
from iris.analysis.cartography import wrap_lons as wrap_circular_points
from iris.coords import DimCoord, AuxCoord
import iris.cube


_DEFAULT_DTYPE = np.float16

_LINEAR_EXTRAPOLATION_MODES = {
    'linear': (False, None),
    'error': (True, None),
    'nan': (False, np.nan)
}


def _extend_circular_coord(coord, points):
    """
    Return coordinates points with a shape extended by one
    This is common when dealing with circular coordinates.

    """
    modulus = np.array(coord.units.modulus or 0,
                       dtype=coord.dtype)
    points = np.append(points, points[0] + modulus)
    return points


def _extend_circular_coord_and_data(coord, data, coord_dim):
    """
    Return coordinate points and a data array with a shape extended by one
    in the coord_dim axis. This is common when dealing with circular
    coordinates.

    """
    points = _extend_circular_coord(coord, coord.points)
    data = _extend_circular_data(data, coord_dim)
    return points, data


def _extend_circular_data(data, coord_dim):
    coord_slice_in_cube = [slice(None)] * data.ndim
    coord_slice_in_cube[coord_dim] = slice(0, 1)

    # TODO: Restore this code after resolution of the following issue:
    # https://github.com/numpy/numpy/issues/478
    # data = np.append(cube.data,
    #                  cube.data[tuple(coord_slice_in_cube)],
    #                  axis=sample_dim)
    # This is the alternative, temporary workaround.
    # It doesn't use append on an nD mask.
    if not (isinstance(data, ma.MaskedArray) and
            not isinstance(data.mask, np.ndarray)) or \
            len(data.mask.shape) == 0:
        data = np.append(data,
                         data[tuple(coord_slice_in_cube)],
                         axis=coord_dim)
    else:
        new_data = np.append(data.data,
                             data.data[tuple(coord_slice_in_cube)],
                             axis=coord_dim)
        new_mask = np.append(data.mask,
                             data.mask[tuple(coord_slice_in_cube)],
                             axis=coord_dim)
        data = ma.array(new_data, mask=new_mask)
    return data


class LinearInterpolator(object):
    """
    This class provides support for performing linear interpolation over
    one or more orthogonal dimensions.

    """
    def __init__(self, src_cube, coords, extrapolation_mode='linear'):
        """
        Perform linear interpolation over one or more orthogonal coordinates.

        Args:

        * src_cube:
            The :class:`iris.cube.Cube` which is to be interpolated.
        * coords:
            The names or coordinate instances which are to be
            interpolated over

        Kwargs:

        * extrapolation_mode:
            Must be one of the following strings:

              * 'linear' - The extrapolation points will be calculated by
                extending the gradient of closest two points.
              * 'nan' - The extrapolation points will be be set to NAN.
              * 'error' - An exception will be raised, notifying an
                attempt to extrapolate.

            Default mode of extrapolation is 'linear'

        """
        # Snapshot the state of the cube to ensure that the interpolator
        # is impervious to external changes to the original source cube.
        self._src_cube = src_cube.copy()
        # Coordinates defining the dimensions to be interpolated.
        self._src_coords = [self._src_cube.coord(coord) for coord in coords]
        # The extrapolation mode.
        if extrapolation_mode not in _LINEAR_EXTRAPOLATION_MODES:
            msg = 'Extrapolation mode {!r} not supported.'
            raise ValueError(msg.format(extrapolation_mode))
        self._mode = extrapolation_mode
        # The point values defining the dimensions to be interpolated.
        self._src_points = []
        # A list of flags indicating dimensions that need to be reversed.
        self._coord_decreasing = []
        # The cube dimensions to be interpolated over.
        self._interp_dims = []
        # meta-data to support circular data-sets.
        self._circulars = []
        # Instance of the interpolator that performs the actual interpolation.
        self._interpolator = None

        # Perform initial start-up configuration and validation.
        self._setup()

    @property
    def cube(self):
        return self._src_cube

    @property
    def coords(self):
        return self._src_coords

    @property
    def extrapolation_mode(self):
        return self._mode

    def _account_for_circular(self, points, data):
        """
        Extend the given data array, and re-centralise coordinate points
        for circular (1D) coordinates.

        """
        for (circular, modulus, index, dim, offset) in self._circulars:
            if modulus:
                # Map all the requested values into the range of the source
                # data (centred over the centre of the source data to allow
                # extrapolation where required).
                points[:, index] = wrap_circular_points(points[:, index],
                                                        offset, modulus)

            # Also extend data if circular (to match the coord points, which
            # 'setup' already extended).
            if circular:
                data = _extend_circular_data(data, dim)

        return points, data

    def _account_for_inverted(self, data):
        if np.any(self._coord_decreasing):
            dim_slices = [slice(None)] * data.ndim
            for interp_dim, flip in zip(self._interp_dims,
                                        self._coord_decreasing):
                if flip:
                    dim_slices[interp_dim] = slice(-1, None, -1)
            data = data[dim_slices]
        return data

    def _interpolate(self, data, interp_points):
        """
        Interpolate a data array over N dimensions.

        Create and cache the underlying interpolator instance before invoking
        it to perform interpolation over the data at the given coordinate point
        values.

        * data (ndarray):
            A data array, to be interpolated in its first 'N' dimensions.

        * interp_points (ndarray):
            An array of interpolation coordinate values.
            Its shape is (..., N) where N is the number of interpolation
            dimensions.
            "interp_points[..., i]" are interpolation point values for the i'th
            coordinate, which is mapped to the i'th data dimension.
            The other (leading) dimensions index over the different required
            sample points.

        Returns:

            A :class:`np.ndarray`.  Its shape is "points_shape + extra_shape",
            where "extra_shape" is the remaining non-interpolated dimensions of
            the data array (i.e. 'data.shape[N:]'), and "points_shape" is the
            leading dimensions of interp_points,
            (i.e. 'interp_points.shape[:-1]').

        """
        dtype = self._interpolated_dtype(data.dtype)
        if data.dtype != dtype:
            # Perform dtype promotion.
            data = data.astype(dtype)

        if self._interpolator is None:
            # Cache the interpolator instance.
            bounds_error, fill_value = _LINEAR_EXTRAPOLATION_MODES[self._mode]
            self._interpolator = _RegularGridInterpolator(self._src_points,
                                                          data,
                                                          bounds_error=False,
                                                          fill_value=None)
            # The constructor of the _RegularGridInterpolator class does
            # some unnecessary checks on these values, so we set them
            # afterwards instead. Sneaky. ;-)
            self._interpolator.bounds_error = bounds_error
            self._interpolator.fill_value = fill_value
        else:
            self._interpolator.values = data

        result = self._interpolator(interp_points)

        if result.dtype != data.dtype:
            # Cast the data dtype to be as expected. Note that, the dtype
            # of the interpolated result is influenced by the dtype of the
            # interpolation points.
            result = result.astype(data.dtype)

        return result

    def _resample_coord(self, sample_points, coord, coord_dims):
        """
        Interpolate the given coordinate at the provided sample points.

        """
        # NB. This section is ripe for improvement:
        # - Internally self._points() expands coord.points to the same
        #   N-dimensional shape as the cube's data, but it doesn't
        #   collapse it again before returning so we have to do that
        #   here.
        # - By expanding to N dimensions self._points() is doing
        #   unnecessary work.
        data = self._points(sample_points, coord.points, coord_dims)
        index = tuple(0 if dim not in coord_dims else slice(None)
                      for dim in range(self._src_cube.ndim))
        new_points = data[index]
        # Watch out for DimCoord instances that are no longer monotonic
        # after the resampling.
        try:
            new_coord = coord.copy(new_points)
        except ValueError:
            aux_coord = AuxCoord.from_coord(coord)
            new_coord = aux_coord.copy(new_points)
        return new_coord

    def _setup(self):
        """
        Perform initial start-up configuration and validation based on the
        cube and the specified coordinates to be interpolated over.

        """
        # Pre-calculate control data for each interpolation coordinate.
        self._src_points = []
        self._coord_decreasing = []
        self._circulars = []
        self._interp_dims = []
        for index, coord in enumerate(self._src_coords):
            coord_dims = self._src_cube.coord_dims(coord)
            coord_points = coord.points

            # Record if coord is descending-order, and adjust points.
            # (notes copied from pelson :-
            #    Force all coordinates to be monotonically increasing.
            #    Generally this isn't always necessary for a rectilinear
            #    interpolator, but it is a common requirement.)
            decreasing = (coord.ndim == 1 and
                          # NOTE: this clause avoids an error when > 1D,
                          # as '_validate' raises a more readable error.
                          coord_points.size > 1 and
                          coord_points[1] < coord_points[0])
            self._coord_decreasing.append(decreasing)
            if decreasing:
                coord_points = coord_points[::-1]

            # Record info if coord is circular, and adjust points.
            circular = getattr(coord, 'circular', False)
            modulus = getattr(coord.units, 'modulus', 0)
            if circular or modulus:
                # Only DimCoords can be circular.
                if circular:
                    coord_points = _extend_circular_coord(coord, coord_points)
                offset = ((coord_points.max() + coord_points.min() - modulus)
                          * 0.5)
                self._circulars.append((circular, modulus,
                                        index, coord_dims[0],
                                        offset))

            self._src_points.append(coord_points)

            # Record any interpolation cube dims we haven't already seen.
            coord_dims = [c for c in coord_dims
                          if c not in self._interp_dims]
            self._interp_dims += coord_dims

        self._validate()

    def _validate(self):
        """
        Perform all sanity checks to ensure that the interpolation request
        over the cube with the specified coordinates is valid and can be
        performed.

        """
        if len(set(self._interp_dims)) != len(self._src_coords):
            raise ValueError('Coordinates repeat a data dimension - the '
                             'interpolation would be over-specified.')

        for coord in self._src_coords:
            if coord.ndim != 1:
                raise ValueError('Interpolation coords must be 1-d for '
                                 'rectilinear interpolation.')

            if not isinstance(coord, DimCoord):
                # Check monotonic.
                if not iris.util.monotonic(coord.points, strict=True):
                    msg = 'Cannot interpolate over the non-' \
                        'monotonic coordinate {}.'
                    raise ValueError(msg.format(coord.name()))

    def _interpolated_dtype(self, dtype):
        """
        Determine the minimum base dtype required by the
        underlying interpolator.

        """
        return np.result_type(_DEFAULT_DTYPE, dtype)

    def _points(self, sample_points, data, data_dims=None):
        """
        Interpolate the given data values at the specified list of orthogonal
        (coord, points) pairs.

        Args:

        * sample_points:
            A list of N iterables, where N is the number of coordinates
            passed to the constructor.
            [sample_values_for_coord_0, sample_values_for_coord_1, ...]
        * data:
            The data to interpolate - not necessarily the data from the cube
            that was used to construct this interpolator. If the data has
            fewer dimensions, then data_dims must be defined.

        Kwargs:

        * data_dims:
            The dimensions of the given data array in terms of the original
            cube passed through to this interpolator's constructor. If None,
            the data dimensions must map one-to-one onto the increasing
            dimension order of the cube.

        Returns:
            An :class:`~numpy.ndarray` or :class:`~numpy.ma.MaskedArray`
            instance of the interpolated data.

        """
        dims = range(self._src_cube.ndim)
        data_dims = data_dims or dims

        if len(data_dims) != data.ndim:
            msg = 'Data being interpolated is not consistent with ' \
                'the data passed through.'
            raise ValueError(msg)

        if sorted(data_dims) != list(data_dims):
            # To do this, a pre & post transpose will be necessary.
            msg = 'Currently only increasing data_dims is supported.'
            raise NotImplementedError(msg)

        # Broadcast the data into the shape of the original cube.
        if data_dims != range(self._src_cube.ndim):
            strides = list(data.strides)
            for dim in range(self._src_cube.ndim):
                if dim not in data_dims:
                    strides.insert(dim, 0)
            data = as_strided(data, strides=strides,
                              shape=self._src_cube.shape)

        data = self._account_for_inverted(data)
        # Calculate the transpose order to shuffle the interpolated dimensions
        # to the lower dimensions for the interpolation algorithm. Then the
        # transpose order to restore the dimensions to their original
        # positions.
        di = self._interp_dims
        ds = sorted(dims, key=lambda d: d not in di)
        dmap = {d: di.index(d) if d in di else ds.index(d) for d in dims}
        interp_order, _ = zip(*sorted(dmap.items(), key=lambda (x, fx):  fx))
        _, src_order = zip(*sorted(dmap.items(), key=lambda (x, fx): x))

        # Prepare the sample points for interpolation and calculate the
        # shape of the interpolated result.
        interp_points = []
        interp_shape = []
        for index, points in enumerate(sample_points):
            dtype = self._interpolated_dtype(self._src_points[index].dtype)
            points = np.array(points, dtype=dtype, ndmin=1)
            interp_points.append(points)
            interp_shape.append(points.size)

        interp_shape.extend(length for dim, length in enumerate(data.shape) if
                            dim not in di)

        # Convert the interpolation points into a cross-product array
        # with shape (n_cross_points, n_dims)
        interp_points = np.asarray([pts for pts in product(*interp_points)])

        # Adjust for circularity.
        interp_points, data = self._account_for_circular(interp_points, data)

        if interp_order != dims:
            # Transpose data in preparation for interpolation.
            data = np.transpose(data, interp_order)

        # Interpolate and reshape the data ...
        result = self._interpolate(data, interp_points)

        if isinstance(data, ma.MaskedArray) and \
                not isinstance(data.mask, ma.MaskType):
            mask = self._interpolate(data.mask, interp_points)
            result = ma.asarray(result)
            result.mask = mask > 0

        result = result.reshape(interp_shape)

        if src_order != dims:
            # Restore the interpolated result to the original
            # source cube dimensional order.
            result = np.transpose(result, src_order)

        return result

    def __call__(self, sample_points, collapse_scalar=True):
        """
        Construct a cube from the specified orthogonal interpolation points.

        Args:

        * sample_points:
            A list of N iterables, where N is the number of coordinates
            passed to the constructor.
            [sample_values_for_coord_0, sample_values_for_coord_1, ...]

        Kwargs:

        * collapse_scalar:
            Whether to collapse the dimension of the scalar sample points
            in the resulting cube. Default is True.

        Returns:
            A cube interpolated at the given sample points. The dimensionality
            of the cube will be the number of original cube dimensions minus
            the number of scalar coordinates, if collapse_scalar is True.

        """
        if len(sample_points) != len(self._src_coords):
            msg = 'Expected sample points for {} coordinates, got {}.'
            raise ValueError(msg.format(len(self._src_coords),
                                        len(sample_points)))

        data = self._src_cube.data
        # Interpolate the cube payload.
        interpolated_data = self._points(sample_points, data)

        if collapse_scalar:
            # When collapse_scalar is True, keep track of the dimensions for
            # which sample points is scalar : We will remove these dimensions
            # later on.
            _new_scalar_dims = []
            for dim, points in zip(self._interp_dims, sample_points):
                if np.array(points).ndim == 0:
                    _new_scalar_dims.append(dim)

        cube = self._src_cube
        new_cube = iris.cube.Cube(interpolated_data)
        new_cube.metadata = cube.metadata

        def construct_new_coord_given_points(coord, points):
            # Handle what was previously a DimCoord which may no longer be
            # monotonic.
            try:
                return coord.copy(points)
            except ValueError:
                return AuxCoord.from_coord(coord).copy(points)

        # Keep track of id(coord) -> new_coord for aux factory construction
        # later on.
        coord_mapping = {}
        dims_with_dim_coords = []

        def construct_new_coord(coord):
            dims = cube.coord_dims(coord)
            try:
                index = self._src_coords.index(coord)
                new_points = sample_points[index]
                new_coord = construct_new_coord_given_points(coord, new_points)
                # isinstance not possible here as a dimension coordinate can be
                # mapped to the aux coordinates of a cube.
                if coord in cube.aux_coords:
                    dims = [self._interp_dims[index]]
            except ValueError:
                if set(dims).intersection(set(self._interp_dims)):
                    # Interpolate the coordinate payload.
                    new_coord = self._resample_coord(sample_points, coord,
                                                     dims)
                else:
                    new_coord = coord.copy()
            return new_coord, dims

        def gen_new_cube():
            if (isinstance(new_coord, DimCoord) and len(dims) > 0
                    and dims[0] not in dims_with_dim_coords):
                new_cube._add_unique_dim_coord(new_coord, dims)
                dims_with_dim_coords.append(dims[0])
            else:
                new_cube._add_unique_aux_coord(new_coord, dims)
            coord_mapping[id(coord)] = new_coord

        # Copy/interpolate the coordinates.
        for coord in (cube.dim_coords + cube.aux_coords):
            new_coord, dims = construct_new_coord(coord)
            gen_new_cube()

        for factory in self._src_cube.aux_factories:
            new_cube.add_aux_factory(factory.updated(coord_mapping))

        if collapse_scalar and _new_scalar_dims:
            dim_slices = [0 if dim in _new_scalar_dims else slice(None)
                          for dim in range(new_cube.ndim)]
            new_cube = new_cube[tuple(dim_slices)]

        return new_cube

########NEW FILE########
__FILENAME__ = _scipy_interpolate
import numpy as np


# ============================================================================
# |                        Copyright SciPy                                   |
# | Code from this point unto the termination banner is copyright SciPy.     |
# | License details can be found at scipy.org/scipylib/license.html          |
# ============================================================================

# Source: https://github.com/scipy/scipy/blob/b94a5d5ccc08dddbc88453477ff2625\
# 9aeaafb32/scipy/interpolate/interpnd.pyx#L167
def _ndim_coords_from_arrays(points, ndim=None):
    """
    Convert a tuple of coordinate arrays to a (..., ndim)-shaped array.

    """
    if isinstance(points, tuple) and len(points) == 1:
        # handle argument tuple
        points = points[0]
    if isinstance(points, tuple):
        p = np.broadcast_arrays(*points)
        for j in xrange(1, len(p)):
            if p[j].shape != p[0].shape:
                raise ValueError(
                    "coordinate arrays do not have the same shape")
        points = np.empty(p[0].shape + (len(points),), dtype=float)
        for j, item in enumerate(p):
            points[..., j] = item
    else:
        points = np.asanyarray(points)
        # XXX Feed back to scipy.
        if points.ndim <= 1:
            if ndim is None:
                points = points.reshape(-1, 1)
            else:
                points = points.reshape(-1, ndim)
    return points


# source: https://github.com/scipy/scipy/blob/b94a5d5ccc08dddbc88453477ff2625\
# 9aeaafb32/scipy/interpolate/interpolate.py#L1400
class _RegularGridInterpolator(object):

    """
    Interpolation on a regular grid in arbitrary dimensions

    The data must be defined on a regular grid; the grid spacing however may be
    uneven.  Linear and nearest-neighbour interpolation are supported. After
    setting up the interpolator object, the interpolation method (*linear* or
    *nearest*) may be chosen at each evaluation.

    .. versionadded:: 0.14

    Parameters
    ----------
    points : tuple of ndarray of float, with shapes (m1, ), ..., (mn, )
        The points defining the regular grid in n dimensions.

    values : array_like, shape (m1, ..., mn, ...)
        The data on the regular grid in n dimensions.

    method : str
        The method of interpolation to perform. Supported are "linear" and
        "nearest". This parameter will become the default for the object's
        ``__call__`` method.

    bounds_error : bool, optional
        If True, when interpolated values are requested outside of the
        domain of the input data, a ValueError is raised.
        If False, then `fill_value` is used.

    fill_value : number, optional
        If provided, the value to use for points outside of the
        interpolation domain. If None, values outside
        the domain are extrapolated.

    Methods
    -------
    __call__

    Notes
    -----
    Contrary to LinearNDInterpolator and NearestNDInterpolator, this class
    avoids expensive triangulation of the input data by taking advantage of the
    regular grid structure.

    """
    # this class is based on code originally programmed by Johannes Buchner,
    # see https://github.com/JohannesBuchner/regulargrid

    def __init__(self, points, values, method="linear", bounds_error=True,
                 fill_value=np.nan):
        if method not in ["linear", "nearest"]:
            raise ValueError("Method '%s' is not defined" % method)
        self.method = method
        self.bounds_error = bounds_error

        if not hasattr(values, 'ndim'):
            # allow reasonable duck-typed values
            values = np.asarray(values)

        if len(points) > values.ndim:
            raise ValueError("There are %d point arrays, but values has %d "
                             "dimensions" % (len(points), values.ndim))

        if hasattr(values, 'dtype') and hasattr(values, 'astype'):
            if not np.issubdtype(values.dtype, np.inexact):
                values = values.astype(float)

        self.fill_value = fill_value
        if fill_value is not None:
            if hasattr(values, 'dtype') and not np.can_cast(fill_value,
                                                            values.dtype):
                raise ValueError("fill_value must be either 'None' or "
                                 "of a type compatible with values")

        for i, p in enumerate(points):
            if not np.all(np.diff(p) > 0.):
                raise ValueError("The points in dimension %d must be strictly "
                                 "ascending" % i)
            if not np.asarray(p).ndim == 1:
                raise ValueError("The points in dimension %d must be "
                                 "1-dimensional" % i)
            if not values.shape[i] == len(p):
                raise ValueError("There are %d points and %d values in "
                                 "dimension %d" % (len(p), values.shape[i], i))
        self.grid = tuple([np.asarray(p) for p in points])
        self.values = values

    def __call__(self, xi, method=None):
        """
        Interpolation at coordinates

        Parameters
        ----------
        xi : ndarray of shape (..., ndim)
            The coordinates to sample the gridded data at

        method : str
            The method of interpolation to perform. Supported are "linear" and
            "nearest".

        """
        method = self.method if method is None else method
        if method not in ["linear", "nearest"]:
            raise ValueError("Method '%s' is not defined" % method)

        ndim = len(self.grid)
        xi = _ndim_coords_from_arrays(xi, ndim=ndim)
        if xi.shape[-1] != len(self.grid):
            raise ValueError("The requested sample points xi have dimension "
                             "%d, but this RegularGridInterpolator has "
                             "dimension %d" % (xi.shape[1], ndim))

        xi_shape = xi.shape
        xi = xi.reshape(-1, xi_shape[-1])

        if self.bounds_error:
            for i, p in enumerate(xi.T):
                if not np.logical_and(np.all(self.grid[i][0] <= p),
                                      np.all(p <= self.grid[i][-1])):
                    raise ValueError("One of the requested xi is out of "
                                     "bounds in dimension %d" % i)

        indices, norm_distances, out_of_bounds = self._find_indices(xi.T)
        if method == "linear":
            result = self._evaluate_linear(
                indices, norm_distances, out_of_bounds)
        elif method == "nearest":
            result = self._evaluate_nearest(
                indices, norm_distances, out_of_bounds)
        if not self.bounds_error and self.fill_value is not None:
            result[out_of_bounds] = self.fill_value

        return result.reshape(xi_shape[:-1] + self.values.shape[ndim:])

    def _evaluate_linear(self, indices, norm_distances, out_of_bounds):
        # slice for broadcasting over trailing dimensions in self.values
        vslice = (slice(None),) + (np.newaxis,) * \
            (self.values.ndim - len(indices))

        # find relevant values
        # each i and i+1 represents a edge
        import itertools
        edges = itertools.product(*[[i, i + 1] for i in indices])
        values = 0.
        for edge_indices in edges:
            weight = 1.
            for ei, i, yi in zip(edge_indices, indices, norm_distances):
                weight *= np.where(ei == i, 1 - yi, yi)
            values += np.asarray(self.values[edge_indices]) * weight[vslice]
        return values

    def _evaluate_nearest(self, indices, norm_distances, out_of_bounds):
        idx_res = []
        for i, yi in zip(indices, norm_distances):
            idx_res.append(np.where(yi <= .5, i, i + 1))
        return self.values[idx_res]

    def _find_indices(self, xi):
        # find relevant edges between which xi are situated
        indices = []
        # compute distance to lower edge in unity units
        norm_distances = []
        # check for out of bounds xi
        out_of_bounds = np.zeros((xi.shape[1]), dtype=bool)
        # iterate through dimensions
        for x, grid in zip(xi, self.grid):
            i = np.searchsorted(grid, x) - 1
            i[i < 0] = 0
            i[i > grid.size - 2] = grid.size - 2
            indices.append(i)
            # TODO: Add this to scipy's version.
            if grid.size == 1:
                norm_distances.append(x - grid[i])
            else:
                norm_distances.append((x - grid[i]) /
                                      (grid[i + 1] - grid[i]))
            if not self.bounds_error:
                out_of_bounds += x < grid[0]
                out_of_bounds += x > grid[-1]
        return indices, norm_distances, out_of_bounds

# ============================================================================
# |                        END SciPy copyright                               |
# ============================================================================

########NEW FILE########
__FILENAME__ = aux_factory
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Definitions of derived coordinates.

"""

from abc import ABCMeta, abstractmethod, abstractproperty
import warnings
import zlib

import numpy as np

from iris._cube_coord_common import CFVariableMixin
import iris.coords
import iris.unit
import iris.util


class LazyArray(object):
    """
    Represents a simplified NumPy array which is only computed on demand.

    It provides the :meth:`view()` and :meth:`reshape()` methods so it
    can be used in place of a standard NumPy array under some
    circumstances.

    The first use of either of these methods causes the array to be
    computed and cached for any subsequent access.

    """
    def __init__(self, shape, func):
        """
        Args:

        * shape (tuple):
            The shape of the array which will be created.
        * func:
            The function which will be called to supply the real array.

        """
        self.shape = tuple(shape)
        self._func = func
        self._array = None

    def __repr__(self):
        return '<LazyArray(shape={})>'.format(self.shape)

    def _cached_array(self):
        if self._array is None:
            self._array = self._func()
            del self._func
        return self._array

    def reshape(self, *args, **kwargs):
        """
        Returns a view of this array with the given shape.

        See :meth:`numpy.ndarray.reshape()` for argument details.

        """
        return self._cached_array().reshape(*args, **kwargs)

    def to_xml_attr(self):
        """
        Returns a string describing this array, suitable for use in CML.

        """
        crc = zlib.crc32(np.array(self._cached_array(), order='C'))
        return 'LazyArray(shape={}, checksum={})'.format(self.shape, crc)

    def view(self, *args, **kwargs):
        """
        Returns a view of this array.

        See :meth:`numpy.ndarray.view()` for argument details.

        """
        return self._cached_array().view(*args, **kwargs)


class AuxCoordFactory(CFVariableMixin):
    """
    Represents a "factory" which can manufacture an additional auxiliary
    coordinate on demand, by combining the values of other coordinates.

    Each concrete subclass represents a specific formula for deriving
    values from other coordinates.

    The `standard_name`, `long_name`, `var_name`, `units`, `attributes` and
    `coord_system` of the factory are used to set the corresponding
    properties of the resulting auxiliary coordinates.

    """
    __metaclass__ = ABCMeta

    def __init__(self):
        #: Descriptive name of the coordinate made by the factory
        self.long_name = None

        #: CF variable name of the coordinate made by the factory
        self.var_name = None

        #: Coordinate system (if any) of the coordinate made by the factory
        self.coord_system = None

    @abstractproperty
    def dependencies(self):
        """
        Returns a dictionary mapping from constructor argument names to
        the corresponding coordinates.

        """

    def _as_defn(self):
        defn = iris.coords.CoordDefn(self.standard_name, self.long_name,
                                     self.var_name, self.units,
                                     self.attributes, self.coord_system)
        return defn

    @abstractmethod
    def make_coord(self, coord_dims_func):
        """
        Returns a new :class:`iris.coords.AuxCoord` as defined by this
        factory.

        Args:

        * coord_dims_func:
            A callable which can return the list of dimensions relevant
            to a given coordinate.
            See :meth:`iris.cube.Cube.coord_dims()`.

        """

    @abstractmethod
    def update(self, old_coord, new_coord=None):
        """
        Notifies the factory of a removal/replacement of a dependency.

        Args:

        * old_coord:
            The dependency coordinate to be removed/replaced.
        * new_coord:
            If None, the dependency using old_coord is removed, otherwise
            the dependency is updated to use new_coord.

        """

    def __repr__(self):
        def arg_text(item):
            key, coord = item
            return '{}={}'.format(key, str(coord and repr(coord.name())))
        items = self.dependencies.items()
        items.sort(key=lambda item: item[0])
        args = map(arg_text, items)
        return '<{}({})>'.format(type(self).__name__, ', '.join(args))

    def derived_dims(self, coord_dims_func):
        """
        Returns the virtual dim-mapping for the derived coordinate.

        Args:

        * coord_dims_func:
            A callable which can return the list of dimensions relevant
            to a given coordinate.
            See :meth:`iris.cube.Cube.coord_dims()`.

        """
        # Which dimensions are relevant?
        # e.g. If sigma -> [1] and orog -> [2, 3] then result = [1, 2, 3]
        derived_dims = set()
        for coord in self.dependencies.itervalues():
            if coord:
                derived_dims.update(coord_dims_func(coord))

        # Apply a fixed order so we know how to map dependency dims to
        # our own dims (and so the Cube can map them to Cube dims).
        derived_dims = tuple(sorted(derived_dims))
        return derived_dims

    def updated(self, new_coord_mapping):
        """
        Creates a new instance of this factory where the dependencies
        are replaced according to the given mapping.

        Args:

        * new_coord_mapping:
            A dictionary mapping from the object IDs potentially used
            by this factory, to the coordinate objects that should be
            used instead.

        """
        new_dependencies = {}
        for key, coord in self.dependencies.iteritems():
            if coord:
                coord = new_coord_mapping[id(coord)]
            new_dependencies[key] = coord
        return type(self)(**new_dependencies)

    def xml_element(self, doc):
        """
        Returns a DOM element describing this coordinate factory.

        """
        element = doc.createElement('coordFactory')
        for key, coord in self.dependencies.iteritems():
            element.setAttribute(key, coord._xml_id())
        element.appendChild(self.make_coord().xml_element(doc))
        return element

    def _dependency_dims(self, coord_dims_func):
        dependency_dims = {}
        for key, coord in self.dependencies.iteritems():
            if coord:
                dependency_dims[key] = coord_dims_func(coord)
        return dependency_dims

    def _nd_bounds(self, coord, dims, ndim):
        """
        Returns the coord's bounds in Cube-orientation and
        broadcastable to N dimensions.

        Example:
            coord.shape == (70,)
            coord.nbounds = 2
            dims == [3]
            ndim == 5
        results in:
            nd_bounds.shape == (1, 1, 1, 70, 1, 2)

        """
        # Transpose to be consistent with the Cube.
        sorted_pairs = sorted(enumerate(dims), key=lambda pair: pair[1])
        transpose_order = [pair[0] for pair in sorted_pairs] + [len(dims)]
        bounds = coord.bounds
        if dims:
            bounds = bounds.transpose(transpose_order)

        # Figure out the n-dimensional shape.
        nd_shape = [1] * ndim + [coord.nbounds]
        for dim, size in zip(dims, coord.shape):
            nd_shape[dim] = size
        bounds.shape = tuple(nd_shape)
        return bounds

    def _nd_points(self, coord, dims, ndim):
        """
        Returns the coord's points in Cube-orientation and
        broadcastable to N dimensions.

        Example:
            coord.shape == (4, 3)
            dims == [3, 2]
            ndim == 5
        results in:
            nd_points.shape == (1, 1, 3, 4, 1)

        """
        # Transpose to be consistent with the Cube.
        sorted_pairs = sorted(enumerate(dims), key=lambda pair: pair[1])
        transpose_order = [pair[0] for pair in sorted_pairs]
        points = coord.points
        if dims:
            points = points.transpose(transpose_order)

        # Figure out the n-dimensional shape.
        nd_shape = [1] * ndim
        for dim, size in zip(dims, coord.shape):
            nd_shape[dim] = size
        points.shape = tuple(nd_shape)
        return points

    def _remap(self, dependency_dims, derived_dims):
        if derived_dims:
            ndim = max(derived_dims) + 1
        else:
            ndim = 1

        nd_points_by_key = {}
        for key, coord in self.dependencies.iteritems():
            if coord:
                # Get the points as consistent with the Cube.
                nd_points = self._nd_points(coord, dependency_dims[key], ndim)

                # Restrict to just the dimensions relevant to the
                # derived coord. NB. These are always in Cube-order, so
                # no transpose is needed.
                shape = []
                for dim in derived_dims:
                    shape.append(nd_points.shape[dim])
                # Ensure the array always has at least one dimension to be
                # compatible with normal coordinates.
                if not derived_dims:
                    shape.append(1)
                nd_points.shape = shape
            else:
                # If no coord, treat value as zero.
                # Use a float16 to provide `shape` attribute and avoid
                # promoting other arguments to a higher precision.
                nd_points = np.float16(0)

            nd_points_by_key[key] = nd_points
        return nd_points_by_key

    def _remap_with_bounds(self, dependency_dims, derived_dims):
        if derived_dims:
            ndim = max(derived_dims) + 1
        else:
            ndim = 1

        nd_values_by_key = {}
        for key, coord in self.dependencies.iteritems():
            if coord:
                # Get the bounds or points as consistent with the Cube.
                if coord.nbounds:
                    nd_values = self._nd_bounds(coord, dependency_dims[key],
                                                ndim)
                else:
                    nd_values = self._nd_points(coord, dependency_dims[key],
                                                ndim)

                # Restrict to just the dimensions relevant to the
                # derived coord. NB. These are always in Cube-order, so
                # no transpose is needed.
                shape = []
                for dim in derived_dims:
                    shape.append(nd_values.shape[dim])
                # Ensure the array always has at least one dimension to be
                # compatible with normal coordinates.
                if not derived_dims:
                    shape.append(1)
                # Add on the N-bounds dimension
                if coord.nbounds:
                    shape.append(nd_values.shape[-1])
                else:
                    # NB. For a non-bounded coordinate we still need an
                    # extra dimension to make the shape compatible, so
                    # we just add an extra 1.
                    shape.append(1)
                nd_values.shape = shape
            else:
                # If no coord, treat value as zero.
                # Use a float16 to provide `shape` attribute and avoid
                # promoting other arguments to a higher precision.
                nd_values = np.float16(0)

            nd_values_by_key[key] = nd_values
        return nd_values_by_key

    def _shape(self, nd_values_by_key):
        nd_values = sorted(nd_values_by_key.values(),
                           key=lambda value: value.ndim)
        shape = list(nd_values.pop().shape)
        for array in nd_values:
            for i, size in enumerate(array.shape):
                if size > 1:
                    # NB. If there's an inconsistency it can only come
                    # from a mismatch in the number of bounds (the Cube
                    # ensures the other dimensions must match).
                    # But we can't afford to raise an error now - it'd
                    # break Cube.derived_coords. Instead, we let the
                    # error happen when the derived coordinate's bounds
                    # are accessed.
                    shape[i] = size
        return shape


class HybridHeightFactory(AuxCoordFactory):
    """
    Defines a hybrid-height coordinate factory with the formula:
        z = a + b * orog

    """
    def __init__(self, delta=None, sigma=None, orography=None):
        """
        Creates a hybrid-height coordinate factory with the formula:
            z = a + b * orog

        At least one of `delta` or `orography` must be provided.

        Args:

        * delta: Coord
            The coordinate providing the `a` term.
        * sigma: Coord
            The coordinate providing the `b` term.
        * orography: Coord
            The coordinate providing the `orog` term.

        """
        super(HybridHeightFactory, self).__init__()

        if delta and delta.nbounds not in (0, 2):
            raise ValueError('Invalid delta coordinate: must have either 0 or'
                             ' 2 bounds.')
        if sigma and sigma.nbounds not in (0, 2):
            raise ValueError('Invalid sigma coordinate: must have either 0 or'
                             ' 2 bounds.')
        if orography and orography.nbounds:
            msg = 'Orography coordinate {!r} has bounds.' \
                  ' These will be disregarded.'.format(orography.name())
            warnings.warn(msg, UserWarning, stacklevel=2)

        self.delta = delta
        self.sigma = sigma
        self.orography = orography

        self.standard_name = 'altitude'
        if delta is None and orography is None:
            raise ValueError('Unable to determine units: no delta or orography'
                             ' available.')
        if delta and orography and delta.units != orography.units:
            raise ValueError('Incompatible units: delta and orography must'
                             ' have the same units.')
        self.units = (delta and delta.units) or orography.units
        if not self.units.is_convertible('m'):
            raise ValueError('Invalid units: delta and/or orography'
                             ' must be expressed in length units.')
        self.attributes = {'positive': 'up'}

    @property
    def dependencies(self):
        """
        Returns a dictionary mapping from constructor argument names to
        the corresponding coordinates.

        """
        return {'delta': self.delta, 'sigma': self.sigma,
                'orography': self.orography}

    def _derive(self, delta, sigma, orography):
        temp = delta + sigma * orography
        return temp

    def make_coord(self, coord_dims_func):
        """
        Returns a new :class:`iris.coords.AuxCoord` as defined by this
        factory.

        Args:

        * coord_dims_func:
            A callable which can return the list of dimensions relevant
            to a given coordinate.
            See :meth:`iris.cube.Cube.coord_dims()`.

        """
        # Which dimensions are relevant?
        derived_dims = self.derived_dims(coord_dims_func)

        dependency_dims = self._dependency_dims(coord_dims_func)

        # Build a "lazy" points array.
        nd_points_by_key = self._remap(dependency_dims, derived_dims)

        # Define the function here to obtain a closure.
        def calc_points():
            return self._derive(nd_points_by_key['delta'],
                                nd_points_by_key['sigma'],
                                nd_points_by_key['orography'])
        shape = self._shape(nd_points_by_key)
        points = LazyArray(shape, calc_points)

        bounds = None
        if ((self.delta and self.delta.nbounds) or
                (self.sigma and self.sigma.nbounds)):
            # Build a "lazy" bounds array.
            nd_values_by_key = self._remap_with_bounds(dependency_dims,
                                                       derived_dims)

            # Define the function here to obtain a closure.
            def calc_bounds():
                delta = nd_values_by_key['delta']
                sigma = nd_values_by_key['sigma']
                orography = nd_values_by_key['orography']
                ok_bound_shapes = [(), (1,), (2,)]
                if delta.shape[-1:] not in ok_bound_shapes:
                    raise ValueError('Invalid delta coordinate bounds.')
                if sigma.shape[-1:] not in ok_bound_shapes:
                    raise ValueError('Invalid sigma coordinate bounds.')
                if orography.shape[-1:] not in [(), (1,)]:
                    warnings.warn('Orography coordinate has bounds. '
                                  'These are being disregarded.',
                                  UserWarning, stacklevel=2)
                    orography_pts = nd_points_by_key['orography']
                    orography_pts_shape = list(orography_pts.shape)
                    orography = orography_pts.reshape(
                        orography_pts_shape.append(1))
                return self._derive(delta, sigma, orography)
            b_shape = self._shape(nd_values_by_key)
            bounds = LazyArray(b_shape, calc_bounds)

        hybrid_height = iris.coords.AuxCoord(points,
                                             standard_name=self.standard_name,
                                             long_name=self.long_name,
                                             var_name=self.var_name,
                                             units=self.units,
                                             bounds=bounds,
                                             attributes=self.attributes,
                                             coord_system=self.coord_system)
        return hybrid_height

    def update(self, old_coord, new_coord=None):
        """
        Notifies the factory of the removal/replacement of a coordinate
        which might be a dependency.

        Args:

        * old_coord:
            The coordinate to be removed/replaced.
        * new_coord:
            If None, any dependency using old_coord is removed, othewise
            any dependency using old_coord is updated to use new_coord.

        """
        if self.delta is old_coord:
            if new_coord and new_coord.nbounds not in (0, 2):
                raise ValueError('Invalid delta coordinate:'
                                 ' must have either 0 or 2 bounds.')
            self.delta = new_coord
        elif self.sigma is old_coord:
            if new_coord and new_coord.nbounds not in (0, 2):
                raise ValueError('Invalid sigma coordinate:'
                                 ' must have either 0 or 2 bounds.')
            self.sigma = new_coord
        elif self.orography is old_coord:
            if new_coord and new_coord.nbounds:
                msg = 'Orography coordinate {!r} has bounds.' \
                      ' These will be disregarded.'.format(new_coord.name())
                warnings.warn(msg, UserWarning, stacklevel=2)
            self.orography = new_coord


class HybridPressureFactory(AuxCoordFactory):
    """
    Defines a hybrid-pressure coordinate factory with the formula:
        p = ap + b * ps

    """
    def __init__(self, delta=None, sigma=None, surface_air_pressure=None):
        """
        Creates a hybrid-height coordinate factory with the formula:
            p = ap + b * ps

        At least one of `delta` or `surface_air_pressure` must be provided.

        Args:

        * delta: Coord
            The coordinate providing the `ap` term.
        * sigma: Coord
            The coordinate providing the `b` term.
        * surface_air_pressure: Coord
            The coordinate providing the `ps` term.

        """
        super(HybridPressureFactory, self).__init__()

        # Check that provided coords meet necessary conditions.
        self._check_dependencies(delta, sigma, surface_air_pressure)

        self.delta = delta
        self.sigma = sigma
        self.surface_air_pressure = surface_air_pressure

        self.standard_name = 'air_pressure'
        self.attributes = {}

    @property
    def units(self):
        if self.delta is not None:
            units = self.delta.units
        else:
            units = self.surface_air_pressure.units
        return units

    @staticmethod
    def _check_dependencies(delta, sigma,
                            surface_air_pressure):
        # Check for sufficient coordinates.
        if (delta is None and (sigma is None or
                               surface_air_pressure is None)):
            msg = 'Unable to contruct hybrid pressure coordinate factory ' \
                  'due to insufficient source coordinates.'
            raise ValueError(msg)

        # Check bounds.
        if delta and delta.nbounds not in (0, 2):
            raise ValueError('Invalid delta coordinate: must have either 0 or'
                             ' 2 bounds.')
        if sigma and sigma.nbounds not in (0, 2):
            raise ValueError('Invalid sigma coordinate: must have either 0 or'
                             ' 2 bounds.')
        if surface_air_pressure and surface_air_pressure.nbounds:
            msg = 'Surface pressure coordinate {!r} has bounds. These will' \
                  ' be disregarded.'.format(surface_air_pressure.name())
            warnings.warn(msg, UserWarning, stacklevel=2)

        # Check units.
        if sigma is not None and not sigma.units.is_dimensionless():
            raise ValueError('Invalid units: sigma must be dimensionless.')
        if delta is not None and surface_air_pressure is not None and \
                delta.units != surface_air_pressure.units:
            msg = 'Incompatible units: delta and ' \
                  'surface_air_pressure must have the same units.'
            raise ValueError(msg)

        if delta is not None:
            units = delta.units
        else:
            units = surface_air_pressure.units

        if not units.is_convertible('Pa'):
            msg = 'Invalid units: delta and ' \
                'surface_air_pressure must have units of pressure.'
            raise ValueError(msg)

    @property
    def dependencies(self):
        """
        Returns a dictionary mapping from constructor argument names to
        the corresponding coordinates.

        """
        return {'delta': self.delta, 'sigma': self.sigma,
                'surface_air_pressure': self.surface_air_pressure}

    def _derive(self, delta, sigma, surface_air_pressure):
        temp = delta + sigma * surface_air_pressure
        return temp

    def make_coord(self, coord_dims_func):
        """
        Returns a new :class:`iris.coords.AuxCoord` as defined by this
        factory.

        Args:

        * coord_dims_func:
            A callable which can return the list of dimensions relevant
            to a given coordinate.
            See :meth:`iris.cube.Cube.coord_dims()`.

        """
        # Which dimensions are relevant?
        derived_dims = self.derived_dims(coord_dims_func)

        dependency_dims = self._dependency_dims(coord_dims_func)

        # Build a "lazy" points array.
        nd_points_by_key = self._remap(dependency_dims, derived_dims)

        # Define the function here to obtain a closure.
        def calc_points():
            return self._derive(nd_points_by_key['delta'],
                                nd_points_by_key['sigma'],
                                nd_points_by_key['surface_air_pressure'])
        shape = self._shape(nd_points_by_key)
        points = LazyArray(shape, calc_points)

        bounds = None
        if ((self.delta and self.delta.nbounds) or
                (self.sigma and self.sigma.nbounds)):
            # Build a "lazy" bounds array.
            nd_values_by_key = self._remap_with_bounds(dependency_dims,
                                                       derived_dims)

            # Define the function here to obtain a closure.
            def calc_bounds():
                delta = nd_values_by_key['delta']
                sigma = nd_values_by_key['sigma']
                surface_air_pressure = nd_values_by_key['surface_air_pressure']
                ok_bound_shapes = [(), (1,), (2,)]
                if delta.shape[-1:] not in ok_bound_shapes:
                    raise ValueError('Invalid delta coordinate bounds.')
                if sigma.shape[-1:] not in ok_bound_shapes:
                    raise ValueError('Invalid sigma coordinate bounds.')
                if surface_air_pressure.shape[-1:] not in [(), (1,)]:
                    warnings.warn('Surface pressure coordinate has bounds. '
                                  'These are being disregarded.')
                    surface_air_pressure_pts = nd_points_by_key[
                        'surface_air_pressure']
                    surface_air_pressure_pts_shape = list(
                        surface_air_pressure_pts.shape)
                    surface_air_pressure = surface_air_pressure_pts.reshape(
                        surface_air_pressure_pts_shape.append(1))
                return self._derive(delta, sigma, surface_air_pressure)
            b_shape = self._shape(nd_values_by_key)
            bounds = LazyArray(b_shape, calc_bounds)

        hybrid_pressure = iris.coords.AuxCoord(
            points, standard_name=self.standard_name, long_name=self.long_name,
            var_name=self.var_name, units=self.units, bounds=bounds,
            attributes=self.attributes, coord_system=self.coord_system)
        return hybrid_pressure

    def update(self, old_coord, new_coord=None):
        """
        Notifies the factory of the removal/replacement of a coordinate
        which might be a dependency.

        Args:

        * old_coord:
            The coordinate to be removed/replaced.
        * new_coord:
            If None, any dependency using old_coord is removed, othewise
            any dependency using old_coord is updated to use new_coord.

        """
        new_dependencies = self.dependencies
        for name, coord in self.dependencies.items():
            if old_coord is coord:
                new_dependencies[name] = new_coord
                try:
                    self._check_dependencies(**new_dependencies)
                except ValueError as e:
                    msg = 'Failed to update dependencies. ' + e.message
                    raise ValueError(msg)
                else:
                    setattr(self, name, new_coord)
                break


class OceanSigmaZFactory(AuxCoordFactory):
    """Defines an ocean sigma over z coordinate factory."""

    def __init__(self, sigma=None, eta=None, depth=None,
                 depth_c=None, nsigma=None, zlev=None):
        """
        Creates a ocean sigma over z coordinate factory with the formula:

        if k < nsigma:
            z(n, k, j, i) = eta(n, j, i) + sigma(k) *
                             (min(depth_c, depth(j, i)) + eta(n, j, i))

        if k >= nsigma:
            z(n, k, j, i) = zlev(k)

        The `zlev` and 'nsigma' coordinates must be provided, and at least
        either `eta`, or 'sigma' and `depth` and `depth_c` coordinates.

        """
        super(OceanSigmaZFactory, self).__init__()

        # Check that provided coordinates meet necessary conditions.
        self._check_dependencies(sigma, eta, depth, depth_c, nsigma, zlev)

        self.sigma = sigma
        self.eta = eta
        self.depth = depth
        self.depth_c = depth_c
        self.nsigma = nsigma
        self.zlev = zlev

        self.standard_name = 'sea_surface_height_above_reference_ellipsoid'
        self.attributes = {'positive': 'up'}

    @property
    def units(self):
        return self.zlev.units

    @staticmethod
    def _check_dependencies(sigma, eta, depth, depth_c, nsigma, zlev):
        # Check for sufficient factory coordinates.
        if zlev is None:
            raise ValueError('Unable to determine units: '
                             'no zlev coordinate available.')
        if nsigma is None:
            raise ValueError('Missing nsigma coordinate.')

        if eta is None and (sigma is None or depth_c is None or
                            depth is None):
            msg = 'Unable to construct ocean sigma over z coordinate ' \
                'factory due to insufficient source coordinates.'
            raise ValueError(msg)

        # Check bounds and shape.
        for coord, term in ((sigma, 'sigma'), (zlev, 'zlev')):
            if coord is not None and coord.nbounds not in (0, 2):
                msg = 'Invalid {} coordinate {!r}: must have either ' \
                    '0 or 2 bounds.'.format(term, coord.name())
                raise ValueError(msg)

        if sigma and sigma.nbounds != zlev.nbounds:
            msg = 'The sigma coordinate {!r} and zlev coordinate {!r} ' \
                'must be equally bounded.'.format(sigma.name(), zlev.name())
            raise ValueError(msg)

        coords = ((eta, 'eta'), (depth, 'depth'),
                  (depth_c, 'depth_c'), (nsigma, 'nsigma'))
        for coord, term in coords:
            if coord is not None and coord.nbounds:
                msg = 'The {} coordinate {!r} has bounds. ' \
                    'These are being disregarded.'.format(term, coord.name())
                warnings.warn(msg, UserWarning, stacklevel=2)

        for coord, term in ((depth_c, 'depth_c'), (nsigma, 'nsigma')):
            if coord is not None and coord.shape != (1,):
                msg = 'Expected scalar {} coordinate {!r}: ' \
                    'got shape {!r}.'.format(term, coord.name(), coord.shape)
                raise ValueError(msg)

        # Check units.
        if not zlev.units.is_convertible('m'):
            msg = 'Invalid units: zlev coordinate {!r} ' \
                'must have units of distance.'.format(zlev.name())
            raise ValueError(msg)

        if sigma is not None and not sigma.units.is_dimensionless():
            msg = 'Invalid units: sigma coordinate {!r} ' \
                'must be dimensionless.'.format(sigma.name())
            raise ValueError(msg)

        coords = ((eta, 'eta'), (depth_c, 'depth_c'), (depth, 'depth'))
        for coord, term in coords:
            if coord is not None and coord.units != zlev.units:
                msg = 'Incompatible units: {} coordinate {!r} and zlev ' \
                    'coordinate {!r} must have ' \
                    'the same units.'.format(term, coord.name(), zlev.name())
                raise ValueError(msg)

    @property
    def dependencies(self):
        """
        Returns a dictionary mapping from constructor argument names to
        the corresponding coordinates.

        """
        return dict(sigma=self.sigma, eta=self.eta, depth=self.depth,
                    depth_c=self.depth_c, nsigma=self.nsigma, zlev=self.zlev)

    def _derive(self, sigma, eta, depth, depth_c,
                nsigma, zlev, shape, nsigma_slice):
        # Perform the ocean sigma over z coordinate nsigma slice.
        if eta.ndim:
            eta = eta[nsigma_slice]
        if sigma.ndim:
            sigma = sigma[nsigma_slice]
        if depth.ndim:
            depth = depth[nsigma_slice]
        # Note that, this performs a point-wise minimum.
        temp = eta + sigma * (np.minimum(depth_c, depth) + eta)
        # Calculate the final derived result.
        result = np.ones(shape, dtype=temp.dtype) * zlev
        result[nsigma_slice] = temp

        return result

    def make_coord(self, coord_dims_func):
        """
        Returns a new :class:`iris.coords.AuxCoord` as defined by this factory.

        Args:

        * coord_dims_func:
            A callable which can return the list of dimesions relevant
            to a given coordinate. See :meth:`iris.cube.Cube.coord_dims()`.

        """
        # Determine the relevant dimensions.
        derived_dims = self.derived_dims(coord_dims_func)
        dependency_dims = self._dependency_dims(coord_dims_func)

        # Build a "lazy" points array.
        nd_points_by_key = self._remap(dependency_dims, derived_dims)
        points_shape = self._shape(nd_points_by_key)

        # Calculate the nsigma slice.
        nsigma_slice = [slice(None)] * len(derived_dims)
        dim, = dependency_dims['zlev']
        index = derived_dims.index(dim)
        nsigma_slice[index] = slice(0, int(nd_points_by_key['nsigma']))

        # Define the function here to obtain a closure.
        def calc_points():
            return self._derive(nd_points_by_key['sigma'],
                                nd_points_by_key['eta'],
                                nd_points_by_key['depth'],
                                nd_points_by_key['depth_c'],
                                nd_points_by_key['nsigma'],
                                nd_points_by_key['zlev'],
                                points_shape,
                                nsigma_slice)

        points = LazyArray(points_shape, calc_points)

        bounds = None
        if self.zlev.nbounds or (self.sigma and self.sigma.nbounds):
            # Build a "lazy" bounds array.
            nd_values_by_key = self._remap_with_bounds(dependency_dims,
                                                       derived_dims)
            bounds_shape = self._shape(nd_values_by_key)
            nsigma_slice_bounds = nsigma_slice + [slice(None)]

            # Define the function here to obtain a closure.
            def calc_bounds():
                valid_shapes = [(), (1,), (2,)]
                for key in ('sigma', 'zlev'):
                    if nd_values_by_key[key].shape[-1:] not in valid_shapes:
                        name = self.dependencies[key].name()
                        msg = 'Invalid bounds for {} ' \
                            'coordinate {!r}.'.format(key, name)
                        raise ValueError(msg)
                valid_shapes.pop()
                for key in ('eta', 'depth', 'depth_c', 'nsigma'):
                    if nd_values_by_key[key].shape[-1:] not in valid_shapes:
                        name = self.dependencies[key].name()
                        msg = 'The {} coordinate {!r} has bounds. ' \
                            'These are being disregarded.'.format(key, name)
                        warnings.warn(msg, UserWarning, stacklevel=2)
                        # Swap bounds with points.
                        shape = list(nd_points_by_key[key].shape)
                        bounds = nd_points_by_key[key].reshape(shape.append(1))
                        nd_values_by_key[key] = bounds
                return self._derive(nd_values_by_key['sigma'],
                                    nd_values_by_key['eta'],
                                    nd_values_by_key['depth'],
                                    nd_values_by_key['depth_c'],
                                    nd_values_by_key['nsigma'],
                                    nd_values_by_key['zlev'],
                                    bounds_shape,
                                    nsigma_slice_bounds)

            bounds = LazyArray(bounds_shape, calc_bounds)

        coord = iris.coords.AuxCoord(points,
                                     standard_name=self.standard_name,
                                     long_name=self.long_name,
                                     var_name=self.var_name,
                                     units=self.units,
                                     bounds=bounds,
                                     attributes=self.attributes,
                                     coord_system=self.coord_system)
        return coord

    def update(self, old_coord, new_coord=None):
        """
        Notifies the factory of the removal/replacement of a coordinate
        which might be a dependency.

        Args:

        * old_coord:
            The coordinate to be removed/replaced.
        * new_coord:
            If None, any dependency using old_coord is removed, othewise
            any dependency using old_coord is updated to use new_coord.

        """
        new_dependencies = self.dependencies
        for name, coord in self.dependencies.items():
            if old_coord is coord:
                new_dependencies[name] = new_coord
                try:
                    self._check_dependencies(**new_dependencies)
                except ValueError as e:
                    msg = 'Failed to update dependencies. ' + e.message
                    raise ValueError(msg)
                else:
                    setattr(self, name, new_coord)
                break

########NEW FILE########
__FILENAME__ = config
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Provides access to Iris-specific configuration values.

The default configuration values can be overridden by creating the file
``iris/etc/site.cfg``. If it exists, this file must conform to the format
defined by :mod:`ConfigParser`.

----------

.. py:data:: iris.config.SAMPLE_DATA_DIR

    Local directory where sample data exists. Defaults to "sample_data"
    sub-directory of the Iris package install directory. The sample data
    directory supports the Iris gallery. Directory contents accessed via
    :func:`iris.sample_data_path`.

.. py:data:: iris.config.TEST_DATA_DIR

    Local directory where test data exists.  Defaults to "test_data"
    sub-directory of the Iris package install directory. The test data
    directory supports the subset of Iris unit tests that require data.
    Directory contents accessed via :func:`iris.tests.get_data_path`.

.. py:data:: iris.config.PALETTE_PATH

    The full path to the Iris palette configuration directory

.. py:data:: iris.config.RULE_LOG_DIR

    The [optional] full path to the rule logging directory used by
    :func:`iris.fileformats.pp.load()` and
    :func:`iris.fileformats.pp.save()`.

.. py:data:: iris.config.RULE_LOG_IGNORE

    The [optional] list of users to ignore when logging rules.

.. py:data:: iris.config.IMPORT_LOGGER

    The [optional] name of the logger to notify when first imported.

----------
"""

import ConfigParser
import os.path
import warnings


# Returns simple string options
def get_option(section, option, default=None):
    """
    Returns the option value for the given section, or the default value
    if the section/option is not present.

    """
    value = default
    if config.has_option(section, option):
        value = config.get(section, option)
    return value


# Returns directory path options
def get_dir_option(section, option, default=None):
    """
    Returns the directory path from the given option and section, or
    returns the given default value if the section/option is not present
    or does not represent a valid directory.

    """
    path = default
    if config.has_option(section, option):
        c_path = config.get(section, option)
        if os.path.isdir(c_path):
            path = c_path
        else:
            msg = 'Ignoring config item {!r}:{!r} (section:option) as {!r}' \
                  ' is not a valid directory path.'
            warnings.warn(msg.format(section, option, c_path))
    return path


# Figure out the full path to the "iris" package.
ROOT_PATH = os.path.abspath(os.path.dirname(__file__))

# The full path to the configuration directory of the active Iris instance.
CONFIG_PATH = os.path.join(ROOT_PATH, 'etc')

# Load the optional "site.cfg" file if it exists.
config = ConfigParser.SafeConfigParser()
config.read([os.path.join(CONFIG_PATH, 'site.cfg')])


##################
# Resource options
_RESOURCE_SECTION = 'Resources'


SAMPLE_DATA_DIR = get_dir_option(
    _RESOURCE_SECTION, 'sample_data_dir',
    default=os.path.join(os.path.dirname(__file__), 'sample_data'))

TEST_DATA_DIR = get_dir_option(_RESOURCE_SECTION, 'test_data_dir',
                               default=os.path.join(os.path.dirname(__file__),
                                                    'test_data'))

# Override the data repository if the appropriate environment variable
# has been set.  This is used in setup.py in the TestRunner command to
# enable us to simulate the absence of external data.
if os.environ.get("override_test_data_repository"):
    TEST_DATA_DIR = None

PALETTE_PATH = get_dir_option(_RESOURCE_SECTION, 'palette_path',
                              os.path.join(CONFIG_PATH, 'palette'))


#################
# Logging options
_LOGGING_SECTION = 'Logging'


RULE_LOG_DIR = get_dir_option(_LOGGING_SECTION, 'rule_dir')


RULE_LOG_IGNORE = get_option(_LOGGING_SECTION, 'rule_ignore')


IMPORT_LOGGER = get_option(_LOGGING_SECTION, 'import_logger')

########NEW FILE########
__FILENAME__ = coords
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Definitions of coordinates.

"""
from __future__ import division

from abc import ABCMeta, abstractproperty
import collections
import copy
from itertools import chain, izip_longest
import operator
import warnings
import zlib

import netcdftime
import numpy as np

import iris.aux_factory
import iris.exceptions
import iris.time
import iris.unit
import iris.util

from iris._cube_coord_common import CFVariableMixin
from iris.util import is_regular


class CoordDefn(collections.namedtuple('CoordDefn',
                                       ['standard_name', 'long_name',
                                        'var_name', 'units',
                                        'attributes', 'coord_system'])):
    """
    Criterion for identifying a specific type of :class:`DimCoord` or
    :class:`AuxCoord` based on its metadata.

    """
    def name(self, default='unknown'):
        """
        Returns a human-readable name.

        First it tries self.standard_name, then it tries the 'long_name'
        attribute, then the 'var_name' attribute, before falling back to
        the value of `default` (which itself defaults to 'unknown').

        """
        return self.standard_name or self.long_name or self.var_name or default


class CoordExtent(collections.namedtuple('_CoordExtent', ['name_or_coord',
                                                          'minimum',
                                                          'maximum',
                                                          'min_inclusive',
                                                          'max_inclusive'])):
    """Defines a range of values for a coordinate."""

    def __new__(cls, name_or_coord, minimum, maximum,
                min_inclusive=True, max_inclusive=True):
        """
        Create a CoordExtent for the specified coordinate and range of
        values.

        Args:

        * name_or_coord
            Either a coordinate name or a coordinate, as defined in
            :meth:`iris.cube.Cube.coords()`.

        * minimum
            The minimum value of the range to select.

        * maximum
            The maximum value of the range to select.

        Kwargs:

        * min_inclusive
            If True, coordinate values equal to `minimum` will be included
            in the selection. Default is True.

        * max_inclusive
            If True, coordinate values equal to `maximum` will be included
            in the selection. Default is True.

        """
        return super(CoordExtent, cls).__new__(cls, name_or_coord, minimum,
                                               maximum, min_inclusive,
                                               max_inclusive)


# Coordinate cell styles. Used in plot and cartography.
POINT_MODE = 0
BOUND_MODE = 1

BOUND_POSITION_START = 0
BOUND_POSITION_MIDDLE = 0.5
BOUND_POSITION_END = 1


# Private named tuple class for coordinate groups.
_GroupbyItem = collections.namedtuple('GroupbyItem',
                                      'groupby_point, groupby_slice')


class Cell(collections.namedtuple('Cell', ['point', 'bound'])):
    """
    An immutable representation of a single cell of a coordinate, including the
    sample point and/or boundary position.

    Notes on cell comparison:

    Cells are compared in two ways, depending on whether they are
    compared to another Cell, or to a number/string.

    Cell-Cell comparison is defined to produce a strict ordering. If
    two cells are not exactly equal (i.e. including whether they both
    define bounds or not) then they will have a consistent relative
    order.

    Cell-number and Cell-string comparison is defined to support
    Constraint matching. The number/string will equal the Cell if, and
    only if, it is within the Cell (including on the boundary). The
    relative comparisons (lt, le, ..) are defined to be consistent with
    this interpretation. So for a given value `n` and Cell `cell`, only
    one of the following can be true:

    |    n < cell
    |    n == cell
    |    n > cell

    Similarly, `n <= cell` implies either `n < cell` or `n == cell`.
    And `n >= cell` implies either `n > cell` or `n == cell`.

    """

    # This subclass adds no attributes.
    __slots__ = ()

    def __new__(cls, point=None, bound=None):
        """
        Construct a Cell from point or point-and-bound information.

        """
        if point is None:
            raise ValueError('Point must be defined.')

        if bound is not None:
            bound = tuple(bound)

        if isinstance(point, np.ndarray):
            point = tuple(point.flatten())

        if isinstance(point, (tuple, list)):
            if len(point) != 1:
                raise ValueError('Point may only be a list or tuple if it has '
                                 'length 1.')
            point = point[0]

        return super(Cell, cls).__new__(cls, point, bound)

    def __mod__(self, mod):
        point = self.point
        bound = self.bound
        if point is not None:
            point = point % mod
        if bound is not None:
            bound = tuple([val % mod for val in bound])
        return Cell(point, bound)

    def __add__(self, mod):
        point = self.point
        bound = self.bound
        if point is not None:
            point = point + mod
        if bound is not None:
            bound = tuple([val + mod for val in bound])
        return Cell(point, bound)

    def __eq__(self, other):
        """
        Compares Cell equality depending on the type of the object to be
        compared.

        """
        if isinstance(other, (int, float, np.number)) or \
                hasattr(other, 'timetuple'):
            if self.bound is not None:
                return self.contains_point(other)
            else:
                return self.point == other
        elif isinstance(other, Cell):
            return (self.point == other.point) and (self.bound == other.bound)
        elif (isinstance(other, basestring) and self.bound is None and
              isinstance(self.point, basestring)):
            return self.point == other
        else:
            return NotImplemented

    # Must supply __ne__, Python does not defer to __eq__ for negative equality
    def __ne__(self, other):
        result = self.__eq__(other)
        if result is not NotImplemented:
            result = not result
        return result

    def __common_cmp__(self, other, operator_method):
        """
        Common method called by the rich comparison operators. The method of
        checking equality depends on the type of the object to be compared.

        Cell vs Cell comparison is used to define a strict order.
        Non-Cell vs Cell comparison is used to define Constraint matching.

        """
        if not (isinstance(other, (int, float, np.number, Cell)) or
                hasattr(other, 'timetuple')):
            raise TypeError("Unexpected type of other "
                            "{}.".format(type(other)))
        if operator_method not in (operator.gt, operator.lt,
                                   operator.ge, operator.le):
            raise ValueError("Unexpected operator_method")

        # Prevent silent errors resulting from missing netcdftime
        # behaviour.
        if (isinstance(other, netcdftime.datetime) or
                (isinstance(self.point, netcdftime.datetime) and
                 not isinstance(other, iris.time.PartialDateTime))):
            raise TypeError('Cannot determine the order of '
                            'netcdftime.datetime objects')

        if isinstance(other, Cell):
            # Cell vs Cell comparison for providing a strict sort order
            if self.bound is None:
                if other.bound is None:
                    # Point vs point
                    # - Simple ordering
                    result = operator_method(self.point, other.point)
                else:
                    # Point vs point-and-bound
                    # - Simple ordering of point values, but if the two
                    #   points are equal, we make the arbitrary choice
                    #   that the point-only Cell is defined as less than
                    #   the point-and-bound Cell.
                    if self.point == other.point:
                        result = operator_method in (operator.lt, operator.le)
                    else:
                        result = operator_method(self.point, other.point)
            else:
                if other.bound is None:
                    # Point-and-bound vs point
                    # - Simple ordering of point values, but if the two
                    #   points are equal, we make the arbitrary choice
                    #   that the point-only Cell is defined as less than
                    #   the point-and-bound Cell.
                    if self.point == other.point:
                        result = operator_method in (operator.gt, operator.ge)
                    else:
                        result = operator_method(self.point, other.point)
                else:
                    # Point-and-bound vs point-and-bound
                    # - Primarily ordered on minimum-bound. If the
                    #   minimum-bounds are equal, then ordered on
                    #   maximum-bound. If the maximum-bounds are also
                    #   equal, then ordered on point values.
                    if self.bound[0] == other.bound[0]:
                        if self.bound[1] == other.bound[1]:
                            result = operator_method(self.point, other.point)
                        else:
                            result = operator_method(self.bound[1],
                                                     other.bound[1])
                    else:
                        result = operator_method(self.bound[0], other.bound[0])
        else:
            # Cell vs number (or string, or datetime-like) for providing
            # Constraint behaviour.
            if self.bound is None:
                # Point vs number
                # - Simple matching
                me = self.point
            else:
                if hasattr(other, 'timetuple'):
                    raise TypeError('Cannot determine whether a point lies '
                                    'within a bounded region for '
                                    'datetime-like objects.')
                # Point-and-bound vs number
                # - Match if "within" the Cell
                if operator_method in [operator.gt, operator.le]:
                    me = min(self.bound)
                else:
                    me = max(self.bound)
            result = operator_method(me, other)

        return result

    def __ge__(self, other):
        return self.__common_cmp__(other, operator.ge)

    def __le__(self, other):
        return self.__common_cmp__(other, operator.le)

    def __gt__(self, other):
        return self.__common_cmp__(other, operator.gt)

    def __lt__(self, other):
        return self.__common_cmp__(other, operator.lt)

    def __str__(self):
        if self.bound is not None:
            return repr(self)
        else:
            return str(self.point)

    def contains_point(self, point):
        """
        For a bounded cell, returns whether the given point lies within the
        bounds.

        .. note:: The test carried out is equivalent to min(bound)
                  <= point <= max(bound).

        """
        if self.bound is None:
            raise ValueError('Point cannot exist inside an unbounded cell.')
        if hasattr(point, 'timetuple') or np.any([hasattr(val, 'timetuple') for
                                                  val in self.bound]):
            raise TypeError('Cannot determine whether a point lies within '
                            'a bounded region for datetime-like objects.')

        return np.min(self.bound) <= point <= np.max(self.bound)


class Coord(CFVariableMixin):
    """
    Abstract superclass for coordinates.

    """
    __metaclass__ = ABCMeta

    _MODE_ADD = 1
    _MODE_SUB = 2
    _MODE_MUL = 3
    _MODE_DIV = 4
    _MODE_RDIV = 5
    _MODE_SYMBOL = {_MODE_ADD: '+', _MODE_SUB: '-',
                    _MODE_MUL: '*', _MODE_DIV: '/',
                    _MODE_RDIV: '/'}

    def __init__(self, points, standard_name=None, long_name=None,
                 var_name=None, units='1', bounds=None, attributes=None,
                 coord_system=None):

        """
        Constructs a single coordinate.

        Args:

        * points:
            The values (or value in the case of a scalar coordinate) of the
            coordinate for each cell.

        Kwargs:

        * standard_name:
            CF standard name of coordinate
        * long_name:
            Descriptive name of coordinate
        * var_name:
            CF variable name of coordinate
        * units
            The :class:`~iris.unit.Unit` of the coordinate's values.
            Can be a string, which will be converted to a Unit object.
        * bounds
            An array of values describing the bounds of each cell. Given n
            bounds for each cell, the shape of the bounds array should be
            points.shape + (n,). For example, a 1d coordinate with 100 points
            and two bounds per cell would have a bounds array of shape
            (100, 2)

        * attributes
            A dictionary containing other cf and user-defined attributes.
        * coord_system
            A :class:`~iris.coord_systems.CoordSystem`,
            e.g. a :class:`~iris.coord_systems.GeogCS` for a longitude Coord.

        """
        #: CF standard name of the quantity that the coordinate represents.
        self.standard_name = standard_name

        #: Descriptive name of the coordinate.
        self.long_name = long_name

        #: The CF variable name for the coordinate.
        self.var_name = var_name

        #: Unit of the quantity that the coordinate represents.
        self.units = units

        #: Other attributes, including user specified attributes that
        #: have no meaning to Iris.
        self.attributes = attributes

        #: Relevant CoordSystem (if any).
        self.coord_system = coord_system

        self.points = points
        self.bounds = bounds

    def __getitem__(self, key):
        """
        Returns a new Coord whose values are obtained by conventional array
        indexing.

        .. note::

            Indexing of a circular coordinate results in a non-circular
            coordinate if the overall shape of the coordinate changes after
            indexing.

        """
        # Turn the key(s) into a full slice spec - i.e. one entry for
        # each dimension of the coord.
        full_slice = iris.util._build_full_slice_given_keys(key, self.ndim)

        # If it's a "null" indexing operation (e.g. coord[:, :]) then
        # we can preserve deferred loading by avoiding promoting _points
        # and _bounds to full ndarray instances.
        def is_full_slice(s):
            return isinstance(s, slice) and s == slice(None, None)
        if all(is_full_slice(s) for s in full_slice):
            points = self._points
            bounds = self._bounds
        else:
            points = self.points
            bounds = self.bounds

            # Make indexing on the cube column based by using the
            # column_slices_generator (potentially requires slicing the
            # data multiple times).
            _, slice_gen = iris.util.column_slices_generator(full_slice,
                                                             self.ndim)
            for keys in slice_gen:
                if points is not None:
                    points = points[keys]
                    if points.shape and min(points.shape) == 0:
                        raise IndexError('Cannot index with zero length '
                                         'slice.')
                if bounds is not None:
                    bounds = bounds[keys + (Ellipsis, )]

        new_coord = self.copy(points=points, bounds=bounds)
        return new_coord

    def copy(self, points=None, bounds=None):
        """
        Returns a copy of this coordinate.

        Kwargs:

        * points: A points array for the new coordinate.
                  This may be a different shape to the points of the coordinate
                  being copied.

        * bounds: A bounds array for the new coordinate.
                  Given n bounds for each cell, the shape of the bounds array
                  should be points.shape + (n,). For example, a 1d coordinate
                  with 100 points and two bounds per cell would have a bounds
                  array of shape (100, 2).

        .. note:: If the points argument is specified and bounds are not, the
                  resulting coordinate will have no bounds.

        """

        if points is None and bounds is not None:
            raise ValueError('If bounds are specified, points must also be '
                             'specified')

        new_coord = copy.deepcopy(self)
        if points is not None:
            # Explicitly not using the points property as we don't want the
            # shape the new points to be constrained by the shape of
            # self.points
            new_coord._points = None
            new_coord.points = points
            # Regardless of whether bounds are provided as an argument, new
            # points will result in new bounds, discarding those copied from
            # self.
            new_coord.bounds = bounds

        return new_coord

    @abstractproperty
    def points(self):
        """Property containing the points values as a numpy array"""

    @abstractproperty
    def bounds(self):
        """Property containing the bound values as a numpy array"""

    def _repr_other_metadata(self):
        fmt = ''
        if self.long_name:
            fmt = ', long_name={self.long_name!r}'
        if self.var_name:
            fmt += ', var_name={self.var_name!r}'
        if len(self.attributes) > 0:
            fmt += ', attributes={self.attributes}'
        if self.coord_system:
            fmt += ', coord_system={self.coord_system}'
        result = fmt.format(self=self)
        return result

    def _str_dates(self, dates_as_numbers):
        date_obj_array = self.units.num2date(dates_as_numbers)
        kwargs = {'separator': ', ', 'prefix': '      '}
        try:
            # With NumPy 1.7 we need to ask for 'str' formatting.
            result = np.core.arrayprint.array2string(
                date_obj_array, formatter={'numpystr': str}, **kwargs)
        except TypeError:
            # But in 1.6 we don't need to ask, and the option doesn't
            # even exist!
            result = np.core.arrayprint.array2string(date_obj_array, **kwargs)
        return result

    def __str__(self):
        if self.units.is_time_reference():
            fmt = '{cls}({points}{bounds}' \
                  ', standard_name={self.standard_name!r}' \
                  ', calendar={self.units.calendar!r}{other_metadata})'
            points = self._str_dates(self.points)
            bounds = ''
            if self.bounds is not None:
                bounds = ', bounds=' + self._str_dates(self.bounds)
            result = fmt.format(self=self, cls=type(self).__name__,
                                points=points, bounds=bounds,
                                other_metadata=self._repr_other_metadata())
        else:
            result = repr(self)
        return result

    def __repr__(self):
        fmt = '{cls}({self.points!r}{bounds}' \
              ', standard_name={self.standard_name!r}, units={self.units!r}' \
              '{other_metadata})'
        bounds = ''
        if self.bounds is not None:
            bounds = ', bounds=' + repr(self.bounds)
        result = fmt.format(self=self, cls=type(self).__name__,
                            bounds=bounds,
                            other_metadata=self._repr_other_metadata())
        return result

    def __eq__(self, other):
        eq = NotImplemented
        # If the other object has a means of getting its definition, and
        # whether or not it has_points and has_bounds, then do the
        # comparison, otherwise return a NotImplemented to let Python try to
        # resolve the operator elsewhere.
        if hasattr(other, '_as_defn'):
            # metadata comparison
            eq = self._as_defn() == other._as_defn()
            # points comparison
            if eq:
                eq = iris.util.array_equal(self.points, other.points)
            # bounds comparison
            if eq:
                if self.bounds is not None and other.bounds is not None:
                    eq = iris.util.array_equal(self.bounds, other.bounds)
                else:
                    eq = self.bounds is None and other.bounds is None

        return eq

    # Must supply __ne__, Python does not defer to __eq__ for negative equality
    def __ne__(self, other):
        result = self.__eq__(other)
        if result is not NotImplemented:
            result = not result
        return result

    def _as_defn(self):
        defn = CoordDefn(self.standard_name, self.long_name, self.var_name,
                         self.units, self.attributes, self.coord_system)
        return defn

    def __binary_operator__(self, other, mode_constant):
        """
        Common code which is called by add, sub, mult and div

        Mode constant is one of ADD, SUB, MUL, DIV, RDIV

        .. note::

            The unit is *not* changed when doing scalar operations on a
            coordinate. This means that a coordinate which represents
            "10 meters" when multiplied by a scalar i.e. "1000" would result
            in a coordinate of "10000 meters". An alternative approach could
            be taken to multiply the *unit* by 1000 and the resultant
            coordinate would represent "10 kilometers".

        """
        if isinstance(other, Coord):
            raise iris.exceptions.NotYetImplementedError(
                'coord %s coord' % Coord._MODE_SYMBOL[mode_constant])

        elif isinstance(other, (int, float, np.number)):

            if mode_constant == Coord._MODE_ADD:
                points = self.points + other
            elif mode_constant == Coord._MODE_SUB:
                points = self.points - other
            elif mode_constant == Coord._MODE_MUL:
                points = self.points * other
            elif mode_constant == Coord._MODE_DIV:
                points = self.points / other
            elif mode_constant == Coord._MODE_RDIV:
                points = other / self.points

            if self.bounds is not None:
                if mode_constant == Coord._MODE_ADD:
                    bounds = self.bounds + other
                elif mode_constant == Coord._MODE_SUB:
                    bounds = self.bounds - other
                elif mode_constant == Coord._MODE_MUL:
                    bounds = self.bounds * other
                elif mode_constant == Coord._MODE_DIV:
                    bounds = self.bounds / other
                elif mode_constant == Coord._MODE_RDIV:
                    bounds = other / self.bounds
            else:
                bounds = None
            new_coord = self.copy(points, bounds)
            return new_coord
        else:
            return NotImplemented

    def __add__(self, other):
        return self.__binary_operator__(other, Coord._MODE_ADD)

    def __sub__(self, other):
        return self.__binary_operator__(other, Coord._MODE_SUB)

    def __mul__(self, other):
        return self.__binary_operator__(other, Coord._MODE_MUL)

    def __div__(self, other):
        return self.__binary_operator__(other, Coord._MODE_DIV)

    def __truediv__(self, other):
        return self.__binary_operator__(other, Coord._MODE_DIV)

    def __radd__(self, other):
        return self + other

    def __rsub__(self, other):
        return (-self) + other

    def __rdiv__(self, other):
        return self.__binary_operator__(other, Coord._MODE_RDIV)

    def __rtruediv__(self, other):
        return self.__binary_operator__(other, Coord._MODE_RDIV)

    def __rmul__(self, other):
        return self * other

    def __neg__(self):
        return self.copy(-self.points, -self.bounds if self.bounds is not
                         None else None)

    def convert_units(self, unit):
        """
        Change the coordinate's units, converting the values in its points
        and bounds arrays.

        For example, if a coordinate's :attr:`~iris.coords.Coord.units`
        attribute is set to radians then::

            coord.convert_units('degrees')

        will change the coordinate's
        :attr:`~iris.coords.Coord.units` attribute to degrees and
        multiply each value in :attr:`~iris.coords.Coord.points` and
        :attr:`~iris.coords.Coord.bounds` by 180.0/:math:`\pi`.

        """
        # If the coord has units convert the values in points (and bounds if
        # present).
        if not self.units.is_unknown():
            self.points = self.units.convert(self.points, unit)
            if self.bounds is not None:
                self.bounds = self.units.convert(self.bounds, unit)
        self.units = unit

    def cells(self):
        """
        Returns an iterable of Cell instances for this Coord.

        For example::

           for cell in self.cells():
              ...

        """
        return _CellIterator(self)

    def _sanity_check_contiguous(self):
        if self.ndim != 1:
            raise iris.exceptions.CoordinateMultiDimError(
                'Invalid operation for {!r}. Contiguous bounds are not defined'
                ' for multi-dimensional coordinates.'.format(self.name()))
        if self.nbounds != 2:
            raise ValueError(
                'Invalid operation for {!r}, with {} bounds. Contiguous bounds'
                ' are only defined for coordinates with 2 bounds.'.format(
                    self.name(), self.nbounds))

    def is_contiguous(self, rtol=1e-05, atol=1e-08):
        """
        Return True if, and only if, this Coord is bounded with contiguous
        bounds to within the specified relative and absolute tolerances.

        Args:

        * rtol:
            The relative tolerance parameter (default is 1e-05).
        * atol:
            The absolute tolerance parameter (default is 1e-08).

        Returns:
            Boolean.

        """
        if self.bounds is not None:
            self._sanity_check_contiguous()
            return np.allclose(self.bounds[1:, 0], self.bounds[:-1, 1],
                               rtol=rtol, atol=atol)
        else:
            return False

    def contiguous_bounds(self):
        """
        Returns the N+1 bound values for a contiguous bounded coordinate
        of length N.

        .. note::

            If the coordinate is does not have bounds, this method will
            return bounds positioned halfway between the coordinate's points.

        """
        if self.bounds is None:
            warnings.warn('Coordinate {!r} is not bounded, guessing '
                          'contiguous bounds.'.format(self.name()))
            bounds = self._guess_bounds()
        else:
            self._sanity_check_contiguous()
            bounds = self.bounds

        c_bounds = np.resize(bounds[:, 0], bounds.shape[0] + 1)
        c_bounds[-1] = bounds[-1, 1]
        return c_bounds

    def is_monotonic(self):
        """Return True if, and only if, this Coord is monotonic."""

        if self.ndim != 1:
            raise iris.exceptions.CoordinateMultiDimError(self)

        if self.shape == (1,):
            return True

        if self.points is not None:
            if not iris.util.monotonic(self.points, strict=True):
                return False

        if self.bounds is not None:
            for b_index in xrange(self.nbounds):
                if not iris.util.monotonic(self.bounds[..., b_index],
                                           strict=True):
                    return False

        return True

    def is_compatible(self, other, ignore=None):
        """
        Return whether the coordinate is compatible with another.

        Compatibility is determined by comparing
        :meth:`iris.coords.Coord.name()`, :attr:`iris.coords.Coord.units`,
        :attr:`iris.coords.Coord.coord_system` and
        :attr:`iris.coords.Coord.attributes` that are present in both objects.

        Args:

        * other:
            An instance of :class:`iris.coords.Coord` or
            :class:`iris.coords.CoordDefn`.
        * ignore:
           A single attribute key or iterable of attribute keys to ignore when
           comparing the coordinates. Default is None. To ignore all
           attributes, set this to other.attributes.

        Returns:
           Boolean.

        """
        compatible = (self.name() == other.name() and
                      self.units == other.units and
                      self.coord_system == other.coord_system)

        if compatible:
            common_keys = set(self.attributes).intersection(other.attributes)
            if ignore is not None:
                if isinstance(ignore, basestring):
                    ignore = (ignore,)
                common_keys = common_keys.difference(ignore)
            for key in common_keys:
                if np.any(self.attributes[key] != other.attributes[key]):
                    compatible = False
                    break

        return compatible

    @property
    def dtype(self):
        """
        Abstract property which returns the Numpy data type of the Coordinate.

        """
        return self.points.dtype

    @property
    def ndim(self):
        """
        Return the number of dimensions of the coordinate (not including the
        bounded dimension).

        """
        return len(self.shape)

    @property
    def nbounds(self):
        """
        Return the number of bounds that this coordinate has (0 for no bounds).

        """
        nbounds = 0
        if self.bounds is not None:
            nbounds = self.bounds.shape[-1]
        return nbounds

    def has_bounds(self):
        return self.bounds is not None

    @property
    def shape(self):
        """The fundamental shape of the Coord, expressed as a tuple."""
        # Access the underlying _points attribute to avoid triggering
        # a deferred load unnecessarily.
        return self._points.shape

    def cell(self, index):
        """
        Return the single :class:`Cell` instance which results from slicing the
        points/bounds with the given index.

        .. note::

            If `iris.FUTURE.cell_datetime_objects` is True, then this
            method will return Cell objects whose `points` and `bounds`
            attributes contain either datetime.datetime instances or
            netcdftime.datetime instances (depending on the calendar).

        """
        index = iris.util._build_full_slice_given_keys(index, self.ndim)

        point = tuple(np.array(self.points[index], ndmin=1).flatten())
        if len(point) != 1:
            raise IndexError('The index %s did not uniquely identify a single '
                             'point to create a cell with.' % (index, ))

        bound = None
        if self.bounds is not None:
            bound = tuple(np.array(self.bounds[index], ndmin=1).flatten())

        if iris.FUTURE.cell_datetime_objects:
            if self.units.is_time_reference():
                point = self.units.num2date(point)
                if bound is not None:
                    bound = self.units.num2date(bound)

        return Cell(point, bound)

    def collapsed(self, dims_to_collapse=None):
        """
        Returns a copy of this coordinate which has been collapsed along
        the specified dimensions.

        Replaces the points & bounds with a simple bounded region.

        """
        if isinstance(dims_to_collapse, (int, np.integer)):
            dims_to_collapse = [dims_to_collapse]

        if dims_to_collapse is not None and \
                set(range(self.ndim)) != set(dims_to_collapse):
            raise ValueError('Cannot partially collapse a coordinate (%s).'
                             % self.name())

        if np.issubdtype(self.dtype, np.str):
            # Collapse the coordinate by serializing the points and
            # bounds as strings.
            serialize = lambda x: '|'.join([str(i) for i in x.flatten()])
            bounds = None
            if self.bounds is not None:
                shape = self.bounds.shape[1:]
                bounds = []
                for index in np.ndindex(shape):
                    index_slice = (slice(None),) + tuple(index)
                    bounds.append(serialize(self.bounds[index_slice]))
                dtype = np.dtype('S{}'.format(max(map(len, bounds))))
                bounds = np.array(bounds, dtype=dtype).reshape((1,) + shape)
            points = serialize(self.points)
            dtype = np.dtype('S{}'.format(len(points)))
            # Create the new collapsed coordinate.
            coord = self.copy(points=np.array(points, dtype=dtype),
                              bounds=bounds)
        else:
            # Collapse the coordinate by calculating the bounded extremes.
            if self.ndim > 1:
                msg = 'Collapsing a multi-dimensional coordinate. ' \
                    'Metadata may not be fully descriptive for {!r}.'
                warnings.warn(msg.format(self.name()))
            elif not self.is_contiguous():
                msg = 'Collapsing a non-contiguous coordinate. ' \
                    'Metadata may not be fully descriptive for {!r}.'
                warnings.warn(msg.format(self.name()))

            # Create bounds for the new collapsed coordinate.
            item = self.bounds if self.bounds is not None else self.points
            lower, upper = np.min(item), np.max(item)
            bounds_dtype = item.dtype
            bounds = [lower, upper]
            # Create points for the new collapsed coordinate.
            points_dtype = self.points.dtype
            points = [(lower + upper) * 0.5]

            # Create the new collapsed coordinate.
            coord = self.copy(points=np.array(points, dtype=points_dtype),
                              bounds=np.array(bounds, dtype=bounds_dtype))
        return coord

    def _guess_bounds(self, bound_position=0.5):
        """
        Return bounds for this coordinate based on its points.

        Kwargs:

        * bound_position - The desired position of the bounds relative to the
                           position of the points.

        Returns:
            A numpy array of shape (len(self.points), 2).

        .. note::

            This method only works for coordinates with ``coord.ndim == 1``.

        """
        # XXX Consider moving into DimCoord
        # ensure we have monotonic points
        if not self.is_monotonic():
            raise ValueError("Need monotonic points to generate bounds for %s"
                             % self.name())

        if self.ndim != 1:
            raise iris.exceptions.CoordinateMultiDimError(self)

        if self.shape[0] < 2:
            raise ValueError('Cannot guess bounds for a coordinate of length '
                             '1.')

        if self.bounds is not None:
            raise ValueError('Coord already has bounds. Remove the bounds '
                             'before guessing new ones.')

        diffs = np.diff(self.points)

        diffs = np.insert(diffs, 0, diffs[0])
        diffs = np.append(diffs, diffs[-1])

        min_bounds = self.points - diffs[:-1] * bound_position
        max_bounds = self.points + diffs[1:] * (1 - bound_position)

        bounds = np.array([min_bounds, max_bounds]).transpose()

        return bounds

    def guess_bounds(self, bound_position=0.5):
        """
        Add contiguous bounds to a coordinate, calculated from its points.

        Puts a cell boundary at the specified fraction between each point and
        the next, plus extrapolated lowermost and uppermost bound points, so
        that each point lies within a cell.

        With regularly spaced points, the resulting bounds will also be
        regular, and all points lie at the same position within their cell.
        With irregular points, the first and last cells are given the same
        widths as the ones next to them.

        Kwargs:

        * bound_position - The desired position of the bounds relative to the
                           position of the points.

        .. note::

            An error is raised if the coordinate already has bounds, is not
            one-dimensional, or is not monotonic.

        .. note::

            Unevenly spaced values, such from a wrapped longitude range, can
            produce unexpected results :  In such cases you should assign
            suitable values directly to the bounds property, instead.

        """
        self.bounds = self._guess_bounds(bound_position)

    def intersect(self, other, return_indices=False):
        """
        Returns a new coordinate from the intersection of two coordinates.

        Both coordinates must be compatible as defined by
        :meth:`~iris.coords.Coord.is_compatible`.

        Kwargs:

        * return_indices:
            If True, changes the return behaviour to return the intersection
            indices for the "self" coordinate.

        """
        if not self.is_compatible(other):
            msg = 'The coordinates cannot be intersected. They are not ' \
                  'compatible because of differing metadata.'
            raise ValueError(msg)

        # Cache self.cells for speed. We can also use the index operation on a
        # list conveniently.
        self_cells = [cell for cell in self.cells()]

        # Maintain a list of indices on self for which cells exist in both self
        # and other.
        self_intersect_indices = []
        for cell in other.cells():
            try:
                self_intersect_indices.append(self_cells.index(cell))
            except ValueError:
                pass

        if return_indices is False and self_intersect_indices == []:
            raise ValueError('No intersection between %s coords possible.' %
                             self.name())

        self_intersect_indices = np.array(self_intersect_indices)

        # Return either the indices, or a Coordinate instance of the
        # intersection.
        if return_indices:
            return self_intersect_indices
        else:
            return self[self_intersect_indices]

    def nearest_neighbour_index(self, point):
        """
        Returns the index of the cell nearest to the given point.

        Only works for one-dimensional coordinates.

        .. note:: If the coordinate contains bounds, these will be used to
            determine the nearest neighbour instead of the point values.

        .. note:: For circular coordinates, the 'nearest' point can wrap around
            to the other end of the values.

        """
        points = self.points
        bounds = self.bounds if self.has_bounds() else np.array([])
        if self.ndim != 1:
            raise ValueError('Nearest-neighbour is currently limited'
                             ' to one-dimensional coordinates.')
        do_circular = getattr(self, 'circular', False)
        if do_circular:
            wrap_modulus = self.units.modulus
            # wrap 'point' to a range based on lowest points or bounds value.
            wrap_origin = np.min(np.hstack((points, bounds.flatten())))
            point = wrap_origin + (point - wrap_origin) % wrap_modulus

        # Calculate the nearest neighbour.
        # The algorithm:  given a single value (V),
        #   if coord has bounds,
        #     make bounds cells complete and non-overlapping
        #     return first cell containing V
        #   else (no bounds),
        #     find the point which is closest to V
        #     or if two are equally close, return the lowest index
        if self.has_bounds():
            # make bounds ranges complete+separate, so point is in at least one
            bounds = bounds.copy()
            # sort the bounds cells by their centre values
            sort_inds = np.argsort(np.mean(bounds, axis=1))
            bounds = bounds[sort_inds]
            # replace all adjacent bounds with their averages
            mid_bounds = 0.5 * (bounds[:-1, 1] + bounds[1:, 0])
            bounds[:-1, 1] = mid_bounds
            bounds[1:, 0] = mid_bounds
            # if point lies beyond either end, fix the end cell to include it
            bounds[0, 0] = min(point, bounds[0, 0])
            bounds[-1, 1] = max(point, bounds[-1, 1])
            # get index of first-occurring cell that contains the point
            inside_cells = np.logical_and(point >= np.min(bounds, axis=1),
                                          point <= np.max(bounds, axis=1))
            result_index = np.where(inside_cells)[0][0]
            # return the original index of the cell (before the bounds sort)
            result_index = sort_inds[result_index]

        # Or, if no bounds, we always have points ...
        else:
            if do_circular:
                # add an extra, wrapped max point (simpler than bounds case)
                # NOTE: circular implies a DimCoord, so *must* be monotonic
                if points[-1] >= points[0]:
                    # ascending value order : add wrapped lowest value to end
                    index_offset = 0
                    points = np.hstack((points, points[0] + wrap_modulus))
                else:
                    # descending order : add wrapped lowest value at start
                    index_offset = 1
                    points = np.hstack((points[-1] + wrap_modulus, points))
            # return index of first-occurring nearest point
            distances = np.abs(points - point)
            result_index = np.where(distances == np.min(distances))[0][0]
            if do_circular:
                # convert index back from circular-adjusted points
                result_index = (result_index - index_offset) % self.shape[0]

        return result_index

    def xml_element(self, doc):
        """Return a DOM element describing this Coord."""
        # Create the XML element as the camelCaseEquivalent of the
        # class name.
        element_name = type(self).__name__
        element_name = element_name[0].lower() + element_name[1:]
        element = doc.createElement(element_name)

        element.setAttribute('id', self._xml_id())

        if self.standard_name:
            element.setAttribute('standard_name', str(self.standard_name))
        if self.long_name:
            element.setAttribute('long_name', str(self.long_name))
        if self.var_name:
            element.setAttribute('var_name', str(self.var_name))
        element.setAttribute('units', repr(self.units))

        if self.attributes:
            attributes_element = doc.createElement('attributes')
            for name in sorted(self.attributes.iterkeys()):
                attribute_element = doc.createElement('attribute')
                attribute_element.setAttribute('name', name)
                attribute_element.setAttribute('value',
                                               str(self.attributes[name]))
                attributes_element.appendChild(attribute_element)
            element.appendChild(attributes_element)

        # Add a coord system sub-element?
        if self.coord_system:
            element.appendChild(self.coord_system.xml_element(doc))

        # Add the values
        element.setAttribute('value_type', str(self._value_type_name()))
        element.setAttribute('shape', str(self.shape))
        if hasattr(self.points, 'to_xml_attr'):
            element.setAttribute('points', self.points.to_xml_attr())
        else:
            element.setAttribute('points', iris.util.format_array(self.points))

        if self.bounds is not None:
            if hasattr(self.bounds, 'to_xml_attr'):
                element.setAttribute('bounds', self.bounds.to_xml_attr())
            else:
                element.setAttribute('bounds',
                                     iris.util.format_array(self.bounds))

        return element

    def _xml_id(self):
        # Returns a consistent, unique string identifier for this coordinate.
        unique_value = (self.standard_name, self.long_name, self.units,
                        tuple(sorted(self.attributes.items())),
                        self.coord_system)
        # Mask to ensure consistency across Python versions & platforms.
        crc = zlib.crc32(str(unique_value)) & 0xffffffff
        # 'L' added by 32-bit systems.
        return hex(crc).lstrip('0x').rstrip('L')

    def _value_type_name(self):
        """
        A simple, readable name for the data type of the Coord point/bound
        values.

        """
        values = self.points
        value_type_name = values.dtype.name
        if self.points.dtype.kind == 'S':
            value_type_name = 'string'
        elif self.points.dtype.kind == 'U':
            value_type_name = 'unicode'
        return value_type_name


class DimCoord(Coord):
    """
    A coordinate that is 1D, numeric, and strictly monotonic.

    """
    @staticmethod
    def from_coord(coord):
        """Create a new DimCoord from the given coordinate."""
        return DimCoord(coord.points, standard_name=coord.standard_name,
                        long_name=coord.long_name, var_name=coord.var_name,
                        units=coord.units, bounds=coord.bounds,
                        attributes=coord.attributes,
                        coord_system=copy.deepcopy(coord.coord_system),
                        circular=getattr(coord, 'circular', False))

    @classmethod
    def from_regular(cls, zeroth, step, count, standard_name=None,
                     long_name=None, var_name=None, units='1', attributes=None,
                     coord_system=None, circular=False, with_bounds=False):
        """
        Create a :class:`DimCoord` with regularly spaced points, and
        optionally bounds.

        The majority of the arguments are defined as for
        :meth:`Coord.__init__`, but those which differ are defined below.

        Args:

        * zeroth:
            The value *prior* to the first point value.
        * step:
            The numeric difference between successive point values.
        * count:
            The number of point values.

        Kwargs:

        * with_bounds:
            If True, the resulting DimCoord will possess bound values
            which are equally spaced around the points. Otherwise no
            bounds values will be defined. Defaults to False.

        """
        coord = DimCoord.__new__(cls)

        coord.standard_name = standard_name
        coord.long_name = long_name
        coord.var_name = var_name
        coord.units = units
        coord.attributes = attributes
        coord.coord_system = coord_system
        coord.circular = circular

        points = (zeroth+step) + step*np.arange(count, dtype=np.float32)
        points.flags.writeable = False
        coord._points = points
        if not is_regular(coord) and count > 1:
            points = (zeroth+step) + step*np.arange(count, dtype=np.float64)
            points.flags.writeable = False
            coord._points = points

        if with_bounds:
            delta = 0.5 * step
            bounds = np.concatenate([[points - delta], [points + delta]]).T
            bounds.flags.writeable = False
            coord._bounds = bounds
        else:
            coord._bounds = None

        return coord

    def __init__(self, points, standard_name=None, long_name=None,
                 var_name=None, units='1', bounds=None, attributes=None,
                 coord_system=None, circular=False):
        """
        Create a 1D, numeric, and strictly monotonic :class:`Coord` with
        read-only points and bounds.

        """
        Coord.__init__(self, points, standard_name=standard_name,
                       long_name=long_name, var_name=var_name,
                       units=units, bounds=bounds, attributes=attributes,
                       coord_system=coord_system)

        #: Whether the coordinate wraps by ``coord.units.modulus``.
        self.circular = bool(circular)

    def __eq__(self, other):
        # TODO investigate equality of AuxCoord and DimCoord if circular is
        # False.
        result = NotImplemented
        if isinstance(other, DimCoord):
            result = (Coord.__eq__(self, other) and self.circular ==
                      other.circular)
        return result

    # The __ne__ operator from Coord implements the not __eq__ method.

    def __getitem__(self, key):
        coord = super(DimCoord, self).__getitem__(key)
        coord.circular = self.circular and coord.shape == self.shape
        return coord

    def collapsed(self, dims_to_collapse=None):
        coord = Coord.collapsed(self, dims_to_collapse=dims_to_collapse)
        if self.circular and self.units.modulus is not None:
            bnds = coord.bounds.copy()
            bnds[0, 1] = coord.bounds[0, 0] + self.units.modulus
            coord.bounds = bnds
            coord.points = np.array(np.sum(coord.bounds) * 0.5,
                                    dtype=self.points.dtype)
        # XXX This isn't actually correct, but is ported from the old world.
        coord.circular = False
        return coord

    def _repr_other_metadata(self):
        result = Coord._repr_other_metadata(self)
        if self.circular:
            result += ', circular=%r' % self.circular
        return result

    @property
    def points(self):
        """The local points values as a read-only NumPy array."""
        points = self._points.view()
        return points

    @points.setter
    def points(self, points):
        points = np.array(points, ndmin=1)
        # If points are already defined for this coordinate,
        if hasattr(self, '_points') and self._points is not None:
            # Check that setting these points wouldn't change self.shape
            if points.shape != self.shape:
                raise ValueError("New points shape must match existing points "
                                 "shape.")

        # Checks for 1d, numeric, monotonic
        if points.ndim != 1:
            raise ValueError('The points array must be 1-dimensional.')
        if not np.issubdtype(points.dtype, np.number):
            raise ValueError('The points array must be numeric.')
        if len(points) > 1 and not iris.util.monotonic(points, strict=True):
            raise ValueError('The points array must be strictly monotonic.')
        # Make the array read-only.
        points.flags.writeable = False

        self._points = points

    @property
    def bounds(self):
        """
        The bounds values as a read-only NumPy array, or None if no
        bounds have been set.

        """
        bounds = None
        if self._bounds is not None:
            bounds = self._bounds.view()
        return bounds

    @bounds.setter
    def bounds(self, bounds):
        if bounds is not None:
            # Ensure the bounds are a compatible shape.
            bounds = np.array(bounds, ndmin=2)
            if self.shape != bounds.shape[:-1]:
                raise ValueError(
                    "The shape of the bounds array should be "
                    "points.shape + (n_bounds,)")
            # Checks for numeric and monotonic
            if not np.issubdtype(bounds.dtype, np.number):
                raise ValueError('The bounds array must be numeric.')

            n_bounds = bounds.shape[-1]
            n_points = bounds.shape[0]
            if n_points > 1:

                directions = set()
                for b_index in xrange(n_bounds):
                    monotonic, direction = iris.util.monotonic(
                        bounds[:, b_index], strict=True, return_direction=True)
                    if not monotonic:
                        raise ValueError('The bounds array must be strictly '
                                         'monotonic.')
                    directions.add(direction)

                if len(directions) != 1:
                    raise ValueError('The direction of monotonicity must be '
                                     'consistent across all bounds')

            # Ensure the array is read-only.
            bounds.flags.writeable = False

        self._bounds = bounds

    def is_monotonic(self):
        return True

    def xml_element(self, doc):
        """Return DOM element describing this :class:`iris.coords.DimCoord`."""
        element = super(DimCoord, self).xml_element(doc)
        if self.circular:
            element.setAttribute('circular', str(self.circular))
        return element


class AuxCoord(Coord):
    """A CF auxiliary coordinate."""
    @staticmethod
    def from_coord(coord):
        """Create a new AuxCoord from the given coordinate."""
        new_coord = AuxCoord(coord.points, standard_name=coord.standard_name,
                             long_name=coord.long_name,
                             var_name=coord.var_name,
                             units=coord.units, bounds=coord.bounds,
                             attributes=coord.attributes,
                             coord_system=copy.deepcopy(coord.coord_system))

        return new_coord

    def _sanitise_array(self, src, ndmin):
        # Ensure the array is writeable.
        # NB. Returns the *same object* if src is already writeable.
        result = np.require(src, requirements='W')
        # Ensure the array has enough dimensions.
        # NB. Returns the *same object* if result.ndim >= ndmin
        result = np.array(result, ndmin=ndmin, copy=False)
        # We don't need to copy the data, but we do need to have our
        # own view so we can control the shape, etc.
        result = result.view()
        return result

    @property
    def points(self):
        """Property containing the points values as a numpy array"""
        return self._points.view()

    @points.setter
    def points(self, points):
        # Set the points to a new array - as long as it's the same shape.

        # With the exception of LazyArrays ensure points is a numpy array with
        # ndmin of 1.
        # This will avoid Scalar coords with points of shape () rather than the
        # desired (1,)
        #   ... could change to: points = lazy.array(points, ndmin=1)
        if not isinstance(points, iris.aux_factory.LazyArray):
            points = self._sanitise_array(points, 1)
        # If points are already defined for this coordinate,
        if hasattr(self, '_points') and self._points is not None:
            # Check that setting these points wouldn't change self.shape
            if points.shape != self.shape:
                raise ValueError("New points shape must match existing points "
                                 "shape.")

        self._points = points

    @property
    def bounds(self):
        """
        Property containing the bound values, as a numpy array,
        or None if no bound values are defined.

        .. note:: The shape of the bound array should be: ``points.shape +
            (n_bounds, )``.

        """
        if self._bounds is not None:
            bounds = self._bounds.view()
        else:
            bounds = None

        return bounds

    @bounds.setter
    def bounds(self, bounds):
        # Ensure the bounds are a compatible shape.
        if bounds is not None:
            if not isinstance(bounds, iris.aux_factory.LazyArray):
                bounds = self._sanitise_array(bounds, 2)
            # NB. Use _points to avoid triggering any lazy array.
            if self._points.shape != bounds.shape[:-1]:
                raise ValueError("Bounds shape must be compatible with points "
                                 "shape.")
        self._bounds = bounds


class CellMethod(iris.util._OrderedHashable):
    """
    Represents a sub-cell pre-processing operation.

    """

    # Declare the attribute names relevant to the _OrderedHashable behaviour.
    _names = ('method', 'coord_names', 'intervals', 'comments')

    #: The name of the operation that was applied. e.g. "mean", "max", etc.
    method = None

    #: The tuple of coordinate names over which the operation was applied.
    coord_names = None

    #: A description of the original intervals over which the operation
    #: was applied.
    intervals = None

    #: Additional comments.
    comments = None

    def __init__(self, method, coords=None, intervals=None, comments=None):
        """
        Args:

        * method:
            The name of the operation.

        Kwargs:

        * coords:
            A single instance or sequence of :class:`.Coord` instances or
            coordinate names.

        * intervals:
            A single string, or a sequence strings, describing the intervals
            within the cell method.

        * comments:
            A single string, or a sequence strings, containing any additional
            comments.

        """
        if not isinstance(method, basestring):
            raise TypeError("'method' must be a string - got a '%s'" %
                            type(method))

        _coords = []
        if coords is None:
            pass
        elif isinstance(coords, Coord):
            _coords.append(coords.name())
        elif isinstance(coords, basestring):
            _coords.append(coords)
        else:
            normalise = (lambda coord: coord.name() if
                         isinstance(coord, Coord) else coord)
            _coords.extend([normalise(coord) for coord in coords])

        _intervals = []
        if intervals is None:
            pass
        elif isinstance(intervals, basestring):
            _intervals = [intervals]
        else:
            _intervals.extend(intervals)

        _comments = []
        if comments is None:
            pass
        elif isinstance(comments, basestring):
            _comments = [comments]
        else:
            _comments.extend(comments)

        self._init(method, tuple(_coords), tuple(_intervals), tuple(_comments))

    def __str__(self):
        """Return a custom string representation of CellMethod"""
        # Group related coord names intervals and comments together
        cell_components = izip_longest(self.coord_names, self.intervals,
                                       self.comments, fillvalue="")

        collection_summaries = []
        cm_summary = "%s: " % self.method

        for coord_name, interval, comment in cell_components:
            other_info = ", ".join(filter(None, chain((interval, comment))))
            if other_info:
                coord_summary = "%s (%s)" % (coord_name, other_info)
            else:
                coord_summary = "%s" % coord_name

            collection_summaries.append(coord_summary)

        return cm_summary + ", ".join(collection_summaries)

    def __add__(self, other):
        # Disable the default tuple behaviour of tuple concatenation
        raise NotImplementedError()

    def xml_element(self, doc):
        """
        Return a dom element describing itself

        """
        cellMethod_xml_element = doc.createElement('cellMethod')
        cellMethod_xml_element.setAttribute('method', self.method)

        for coord_name, interval, comment in map(None, self.coord_names,
                                                 self.intervals,
                                                 self.comments):
            coord_xml_element = doc.createElement('coord')
            if coord_name is not None:
                coord_xml_element.setAttribute('name', coord_name)
                if interval is not None:
                    coord_xml_element.setAttribute('interval', interval)
                if comment is not None:
                    coord_xml_element.setAttribute('comment', comment)
                cellMethod_xml_element.appendChild(coord_xml_element)

        return cellMethod_xml_element


# See Coord.cells() for the description/context.
class _CellIterator(collections.Iterator):
    def __init__(self, coord):
        self._coord = coord
        if coord.ndim != 1:
            raise iris.exceptions.CoordinateMultiDimError(coord)
        self._indices = iter(xrange(coord.shape[0]))

    def next(self):
        # NB. When self._indices runs out it will raise StopIteration for us.
        i = self._indices.next()
        return self._coord.cell(i)


# See ExplicitCoord._group() for the description/context.
class _GroupIterator(collections.Iterator):
    def __init__(self, points):
        self._points = points
        self._start = 0

    def next(self):
        num_points = len(self._points)
        if self._start >= num_points:
            raise StopIteration

        stop = self._start + 1
        m = self._points[self._start]
        while stop < num_points and self._points[stop] == m:
            stop += 1

        group = _GroupbyItem(m, slice(self._start, stop))
        self._start = stop
        return group

########NEW FILE########
__FILENAME__ = coord_categorisation
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Cube functions for coordinate categorisation.

All the functions provided here add a new coordinate to a cube.
    * The function :func:`add_categorised_coord` performs a generic
      coordinate categorisation.
    * The other functions all implement specific common cases
      (e.g. :func:`add_day_of_month`).
      Currently, these are all calendar functions, so they only apply to
      "Time coordinates".

"""

import calendar
import collections

import numpy as np

import iris.coords


def add_categorised_coord(cube, name, from_coord, category_function,
                          units='1'):
    """
    Add a new coordinate to a cube, by categorising an existing one.

    Make a new :class:`iris.coords.AuxCoord` from mapped values, and add
    it to the cube.

    Args:

    * cube (:class:`iris.cube.Cube`):
        the cube containing 'from_coord'.  The new coord will be added into it.
    * name (string):
        name of the created coordinate
    * from_coord (:class:`iris.coords.Coord` or string):
        coordinate in 'cube', or the name of one
    * category_function (callable):
        function(coordinate, value), returning a category value for a
        coordinate point-value

    Kwargs:

    * units:
        units of the category value, typically 'no_unit' or '1'.
    """
    # Interpret coord, if given as a name
    if isinstance(from_coord, basestring):
        from_coord = cube.coord(from_coord)

    if len(cube.coords(name)) > 0:
        msg = 'A coordinate "%s" already exists in the cube.' % name
        raise ValueError(msg)

    # Construct new coordinate by mapping values, using numpy.vectorize to
    # support multi-dimensional coords.
    # Test whether the result contains strings. If it does we must manually
    # force the dtype because of a numpy bug (see numpy #3270 on GitHub).
    result = category_function(from_coord, from_coord.points.ravel()[0])
    if isinstance(result, basestring):
        str_vectorised_fn = np.vectorize(category_function, otypes=[object])
        vectorised_fn = lambda *args: str_vectorised_fn(*args).astype('|S64')
    else:
        vectorised_fn = np.vectorize(category_function)
    new_coord = iris.coords.AuxCoord(vectorised_fn(from_coord,
                                                   from_coord.points),
                                     units=units,
                                     attributes=from_coord.attributes.copy())
    new_coord.rename(name)

    # Add into the cube
    cube.add_aux_coord(new_coord, cube.coord_dims(from_coord))


# ======================================
# Specific functions for particular purposes
#
# NOTE: all the existing ones are calendar operations, so are for 'Time'
# coordinates only
#

# Private "helper" function
def _pt_date(coord, time):
    """
    Return the date of a time-coordinate point.

    Args:

    * coord (Coord):
        coordinate (must be Time-type)
    * time (float):
        value of a coordinate point

    Returns:
        datetime.date
    """
    # NOTE: All of the currently defined categorisation functions are
    # calendar operations on Time coordinates.
    #  - All these currently depend on Unit::num2date, which is deprecated (!!)
    #  - We will want to do better, when we sort out our own Calendars.
    #  - For now, just make sure these all call through this one function.
    return coord.units.num2date(time)


# --------------------------------------------
# Time categorisations : calendar date components

def add_year(cube, coord, name='year'):
    """Add a categorical calendar-year coordinate."""
    add_categorised_coord(
        cube, name, coord,
        lambda coord, x: _pt_date(coord, x).year)


def add_month_number(cube, coord, name='month_number'):
    """Add a categorical month coordinate, values 1..12."""
    add_categorised_coord(
        cube, name, coord,
        lambda coord, x: _pt_date(coord, x).month)


def add_month_fullname(cube, coord, name='month_fullname'):
    """Add a categorical month coordinate, values 'January'..'December'."""
    add_categorised_coord(
        cube, name, coord,
        lambda coord, x: calendar.month_name[_pt_date(coord, x).month],
        units='no_unit')


def add_month(cube, coord, name='month'):
    """Add a categorical month coordinate, values 'Jan'..'Dec'."""
    add_categorised_coord(
        cube, name, coord,
        lambda coord, x: calendar.month_abbr[_pt_date(coord, x).month],
        units='no_unit')


def add_day_of_month(cube, coord, name='day_of_month'):
    """Add a categorical day-of-month coordinate, values 1..31."""
    add_categorised_coord(
        cube, name, coord,
        lambda coord, x: _pt_date(coord, x).day)


def add_day_of_year(cube, coord, name='day_of_year'):
    """
    Add a categorical day-of-year coordinate, values 1..365
    (1..366 in leap years).

    """
    # Note: netcdftime.datetime objects return a normal tuple from timetuple(),
    # unlike datetime.datetime objects that return a namedtuple.
    # Index the time tuple (element 7 is day of year) instead of using named
    # element tm_yday.
    add_categorised_coord(
        cube, name, coord,
        lambda coord, x: _pt_date(coord, x).timetuple()[7])


# --------------------------------------------
# Time categorisations : days of the week

def add_weekday_number(cube, coord, name='weekday_number'):
    """Add a categorical weekday coordinate, values 0..6  [0=Monday]."""
    add_categorised_coord(
        cube, name, coord,
        lambda coord, x: _pt_date(coord, x).weekday())


def add_weekday_fullname(cube, coord, name='weekday_fullname'):
    """Add a categorical weekday coordinate, values 'Monday'..'Sunday'."""
    add_categorised_coord(
        cube, name, coord,
        lambda coord, x: calendar.day_name[_pt_date(coord, x).weekday()],
        units='no_unit')


def add_weekday(cube, coord, name='weekday'):
    """Add a categorical weekday coordinate, values 'Mon'..'Sun'."""
    add_categorised_coord(
        cube, name, coord,
        lambda coord, x: calendar.day_abbr[_pt_date(coord, x).weekday()],
        units='no_unit')


# ----------------------------------------------
# Time categorisations : meteorological seasons

def _months_in_season(season):
    """
    Returns a list of month numbers corresponding to each month in the
    given season.

    """
    cyclic_months = 'jfmamjjasondjfmamjjasond'
    m0 = cyclic_months.find(season.lower())
    if m0 < 0:
        # Can't match the season, raise an error.
        raise ValueError('unrecognised season: {!s}'.format(season))
    m1 = m0 + len(season)
    return map(lambda month: (month % 12) + 1, range(m0, m1))


def _validate_seasons(seasons):
    """Check that a set of seasons is valid.

    Validity means that all months are included in a season, and no
    month is assigned to more than one season.

    Raises ValueError if either of the conditions is not met, returns
    None otherwise.

    """
    c = collections.Counter()
    for season in seasons:
        c.update(_months_in_season(season))
    # Make a list of months that are not present...
    not_present = [calendar.month_abbr[month] for month in xrange(1, 13)
                   if month not in c.keys()]
    if not_present:
        raise ValueError('some months do not appear in any season: '
                         '{!s}'.format(', '.join(not_present)))
    # Make a list of months that appear multiple times...
    multi_present = [calendar.month_abbr[month] for month in xrange(1, 13)
                     if c[month] > 1]
    if multi_present:
        raise ValueError('some months appear in more than one season: '
                         '{!s}'.format(', '.join(multi_present)))
    return


def _month_year_adjusts(seasons):
    """Compute the year adjustments required for each month.

    These determine whether the month belongs to a season in the same
    year or is in the start of a season that counts towards the next
    year.

    """
    month_year_adjusts = [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    for season in seasons:
        months = _months_in_season(season)
        for month in filter(lambda m: m > months[-1], months):
            month_year_adjusts[month] = 1
    return month_year_adjusts


def _month_season_numbers(seasons):
    """Compute a mapping between months and season number.

    Returns a list to be indexed by month number, where the value at
    each index is the number of the season that month belongs to.

    """
    month_season_numbers = [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    for season_number, season in enumerate(seasons):
        for month in _months_in_season(season):
            month_season_numbers[month] = season_number
    return month_season_numbers


def add_season(cube, coord, name='season',
               seasons=('djf', 'mam', 'jja', 'son')):
    """
    Add a categorical season-of-year coordinate, with user specified
    seasons.

    Args:

    * cube (:class:`iris.cube.Cube`):
        The cube containing 'coord'. The new coord will be added into
        it.
    * coord (:class:`iris.coords.Coord` or string):
        Coordinate in 'cube', or its name, representing time.

    Kwargs:

    * name (string):
        Name of the created coordinate. Defaults to "season".
    * seasons (:class:`list` of strings):
        List of seasons defined by month abbreviations. Each month must
        appear once and only once. Defaults to standard meteorological
        seasons ('djf', 'mam', 'jja', 'son').

    """
    # Check that the seasons are valid.
    _validate_seasons(seasons)
    # Get a list of the season number each month is is, using month numbers
    # as the indices.
    month_season_numbers = _month_season_numbers(seasons)

    # Define a categorisation function.
    def _season(coord, value):
        dt = _pt_date(coord, value)
        return seasons[month_season_numbers[dt.month]]

    # Apply the categorisation.
    add_categorised_coord(cube, name, coord, _season, units='no_unit')


def add_season_number(cube, coord, name='season_number',
                      seasons=('djf', 'mam', 'jja', 'son')):
    """
    Add a categorical season-of-year coordinate, values 0..N-1 where
    N is the number of user specified seasons.

    Args:

    * cube (:class:`iris.cube.Cube`):
        The cube containing 'coord'. The new coord will be added into
        it.
    * coord (:class:`iris.coords.Coord` or string):
        Coordinate in 'cube', or its name, representing time.

    Kwargs:

    * name (string):
        Name of the created coordinate. Defaults to "season_number".
    * seasons (:class:`list` of strings):
        List of seasons defined by month abbreviations. Each month must
        appear once and only once. Defaults to standard meteorological
        seasons ('djf', 'mam', 'jja', 'son').

    """
    # Check that the seasons are valid.
    _validate_seasons(seasons)
    # Get a list of the season number each month is is, using month numbers
    # as the indices.
    month_season_numbers = _month_season_numbers(seasons)

    # Define a categorisation function.
    def _season_number(coord, value):
        dt = _pt_date(coord, value)
        return month_season_numbers[dt.month]

    # Apply the categorisation.
    add_categorised_coord(cube, name, coord, _season_number)


def add_season_year(cube, coord, name='season_year',
                    seasons=('djf', 'mam', 'jja', 'son')):
    """
    Add a categorical year-of-season coordinate, with user specified
    seasons.

    Args:

    * cube (:class:`iris.cube.Cube`):
        The cube containing 'coord'. The new coord will be added into
        it.
    * coord (:class:`iris.coords.Coord` or string):
        Coordinate in 'cube', or its name, representing time.

    Kwargs:

    * name (string):
        Name of the created coordinate. Defaults to "season_year".
    * seasons (:class:`list` of strings):
        List of seasons defined by month abbreviations. Each month must
        appear once and only once. Defaults to standard meteorological
        seasons ('djf', 'mam', 'jja', 'son').

    """
    # Check that the seasons are valid.
    _validate_seasons(seasons)
    # Define the adjustments to be made to the year.
    month_year_adjusts = _month_year_adjusts(seasons)

    # Define a categorisation function.
    def _season_year(coord, value):
        dt = _pt_date(coord, value)
        year = dt.year
        year += month_year_adjusts[dt.month]
        return year

    # Apply the categorisation.
    add_categorised_coord(cube, name, coord, _season_year)


def add_season_membership(cube, coord, season, name='season_membership'):
    """
    Add a categorical season membership coordinate for a user specified
    season.

    The coordinate has the value True for every time that is within the
    given season, and the value False otherwise.

    Args:

    * cube (:class:`iris.cube.Cube`):
        The cube containing 'coord'. The new coord will be added into
        it.
    * coord (:class:`iris.coords.Coord` or string):
        Coordinate in 'cube', or its name, representing time.
    * season (string):
        Season defined by month abbreviations.

    Kwargs:

    * name (string):
        Name of the created coordinate. Defaults to "season_membership".

    """
    months = _months_in_season(season)

    def _season_membership(coord, value):
        dt = _pt_date(coord, value)
        if dt.month in months:
            return True
        return False

    add_categorised_coord(cube, name, coord, _season_membership)

########NEW FILE########
__FILENAME__ = coord_systems
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Definitions of coordinate systems.

"""

from __future__ import division
from abc import ABCMeta, abstractmethod
import warnings

import cartopy.crs


class CoordSystem(object):
    """
    Abstract base class for coordinate systems.

    """
    __metaclass__ = ABCMeta

    grid_mapping_name = None

    def __eq__(self, other):
        return (self.__class__ == other.__class__ and
                self.__dict__ == other.__dict__)

    def __ne__(self, other):
        # Must supply __ne__, Python does not defer to __eq__ for
        # negative equality.
        return not (self == other)

    def xml_element(self, doc, attrs=None):
        """Default behaviour for coord systems."""
        # attrs - optional list of (k,v) items, used for alternate output

        xml_element_name = type(self).__name__
        # lower case the first char
        first_char = xml_element_name[0]
        xml_element_name = xml_element_name.replace(first_char,
                                                    first_char.lower(),
                                                    1)

        coord_system_xml_element = doc.createElement(xml_element_name)

        if attrs is None:
            attrs = self.__dict__.items()
        attrs.sort(key=lambda attr: attr[0])

        for name, value in attrs:
            coord_system_xml_element.setAttribute(name, str(value))

        return coord_system_xml_element

    @abstractmethod
    def as_cartopy_crs(self):
        """
        Return a cartopy CRS representing our native coordinate
        system.

        """
        pass

    @abstractmethod
    def as_cartopy_projection(self):
        """
        Return a cartopy projection representing our native map.

        This will be the same as the :func:`~CoordSystem.as_cartopy_crs` for
        map projections but for spherical coord systems (which are not map
        projections) we use a map projection, such as PlateCarree.

        """
        pass


class GeogCS(CoordSystem):
    """
    A geographic (ellipsoidal) coordinate system, defined by the shape of
    the Earth and a prime meridian.

    """

    grid_mapping_name = "latitude_longitude"

    def __init__(self, semi_major_axis=None, semi_minor_axis=None,
                 inverse_flattening=None, longitude_of_prime_meridian=0):
        """
        Creates a new GeogCS.

        Kwargs:

            * semi_major_axis              -  of ellipsoid in metres
            * semi_minor_axis              -  of ellipsoid in metres
            * inverse_flattening           -  of ellipsoid
            * longitude_of_prime_meridian  -  Can be used to specify the
                                              prime meridian on the ellipsoid
                                              in degrees. Default = 0.

        If just semi_major_axis is set, with no semi_minor_axis or
        inverse_flattening, then a perfect sphere is created from the given
        radius.

        If just two of semi_major_axis, semi_minor_axis, and
        inverse_flattening are given the missing element is calulated from the
        formula:
        :math:`flattening = (major - minor) / major`

        Currently, Iris will not allow over-specification (all three ellipsoid
        paramaters).
        Examples::

            cs = GeogCS(6371229)
            pp_cs = GeogCS(iris.fileformats.pp.EARTH_RADIUS)
            airy1830 = GeogCS(semi_major_axis=6377563.396,
                              semi_minor_axis=6356256.909)
            airy1830 = GeogCS(semi_major_axis=6377563.396,
                              inverse_flattening=299.3249646)
            custom_cs = GeogCS(6400000, 6300000)

        """
        # No ellipsoid specified? (0 0 0)
        if ((semi_major_axis is None) and (semi_minor_axis is None) and
                (inverse_flattening is None)):
            raise ValueError("No ellipsoid specified")

        # Ellipsoid over-specified? (1 1 1)
        if ((semi_major_axis is not None) and (semi_minor_axis is not None) and
                (inverse_flattening is not None)):
            raise ValueError("Ellipsoid is overspecified")

        # Perfect sphere (semi_major_axis only)? (1 0 0)
        elif (semi_major_axis is not None and (semi_minor_axis is None and
                                               inverse_flattening is None)):
            semi_minor_axis = semi_major_axis
            inverse_flattening = 0.0

        # Calculate semi_major_axis? (0 1 1)
        elif semi_major_axis is None and (semi_minor_axis is not None and
                                          inverse_flattening is not None):
            semi_major_axis = -semi_minor_axis / ((1.0 - inverse_flattening) /
                                                  inverse_flattening)

        # Calculate semi_minor_axis? (1 0 1)
        elif semi_minor_axis is None and (semi_major_axis is not None and
                                          inverse_flattening is not None):
            semi_minor_axis = semi_major_axis - ((1.0 / inverse_flattening) *
                                                 semi_major_axis)

        # Calculate inverse_flattening? (1 1 0)
        elif inverse_flattening is None and (semi_major_axis is not None and
                                             semi_minor_axis is not None):
            if semi_major_axis == semi_minor_axis:
                inverse_flattening = 0.0
            else:
                inverse_flattening = 1.0 / (
                    (semi_major_axis - semi_minor_axis) / semi_major_axis)

        # We didn't get enough to specify an ellipse.
        else:
            raise ValueError("Insufficient ellipsoid specification")

        #: Major radius of the ellipsoid in metres.
        self.semi_major_axis = float(semi_major_axis)

        #: Minor radius of the ellipsoid in metres.
        self.semi_minor_axis = float(semi_minor_axis)

        #: :math:`1/f` where :math:`f = (a-b)/a`
        self.inverse_flattening = float(inverse_flattening)

        #: Describes 'zero' on the ellipsoid in degrees.
        self.longitude_of_prime_meridian = float(longitude_of_prime_meridian)

    def _pretty_attrs(self):
        attrs = [("semi_major_axis", self.semi_major_axis)]
        if self.semi_major_axis != self.semi_minor_axis:
            attrs.append(("semi_minor_axis", self.semi_minor_axis))
        if self.longitude_of_prime_meridian != 0.0:
            attrs.append(("longitude_of_prime_meridian",
                          self.longitude_of_prime_meridian))
        return attrs

    def __repr__(self):
        attrs = self._pretty_attrs()
        # Special case for 1 pretty attr
        if len(attrs) == 1 and attrs[0][0] == "semi_major_axis":
            return "GeogCS(%r)" % self.semi_major_axis
        else:
            return "GeogCS(%s)" % ", ".join(
                ["%s=%r" % (k, v) for k, v in attrs])

    def __str__(self):
        attrs = self._pretty_attrs()
        # Special case for 1 pretty attr
        if len(attrs) == 1 and attrs[0][0] == "semi_major_axis":
            return "GeogCS(%s)" % self.semi_major_axis
        else:
            return "GeogCS(%s)" % ", ".join(
                ["%s=%s" % (k, v) for k, v in attrs])

    def xml_element(self, doc):
        # Special output for spheres
        attrs = self._pretty_attrs()
        if len(attrs) == 1 and attrs[0][0] == "semi_major_axis":
            attrs = [("earth_radius", self.semi_major_axis)]

        return CoordSystem.xml_element(self, doc, attrs)

    def as_cartopy_crs(self):
        return cartopy.crs.Geodetic(self.as_cartopy_globe())

    def as_cartopy_projection(self):
        return cartopy.crs.PlateCarree()

    def as_cartopy_globe(self):
        # Explicitly set `ellipse` to None as a workaround for
        # Cartopy setting WGS84 as the default.
        return cartopy.crs.Globe(semimajor_axis=self.semi_major_axis,
                                 semiminor_axis=self.semi_minor_axis,
                                 ellipse=None)


class RotatedGeogCS(CoordSystem):
    """
    A coordinate system with rotated pole, on an optional :class:`GeogCS`.

    """

    grid_mapping_name = "rotated_latitude_longitude"

    def __init__(self, grid_north_pole_latitude, grid_north_pole_longitude,
                 north_pole_grid_longitude=0, ellipsoid=None):
        """
        Constructs a coordinate system with rotated pole, on an
        optional :class:`GeogCS`.

        Args:

            * grid_north_pole_latitude  - The true latitude of the rotated
                                          pole in degrees.
            * grid_north_pole_longitude - The true longitude of the rotated
                                          pole in degrees.

        Kwargs:

            * north_pole_grid_longitude - Longitude of true north pole in
                                          rotated grid in degrees. Default = 0.
            * ellipsoid                 - Optional :class:`GeogCS` defining
                                          the ellipsoid.

        Examples::

            rotated_cs = RotatedGeogCS(30, 30)
            another_cs = RotatedGeogCS(30, 30,
                                       ellipsoid=GeogCS(6400000, 6300000))

        """
        #: The true latitude of the rotated pole in degrees.
        self.grid_north_pole_latitude = float(grid_north_pole_latitude)

        #: The true longitude of the rotated pole in degrees.
        self.grid_north_pole_longitude = float(grid_north_pole_longitude)

        #: Longitude of true north pole in rotated grid in degrees.
        self.north_pole_grid_longitude = float(north_pole_grid_longitude)

        #: Ellipsoid definition.
        self.ellipsoid = ellipsoid

    def _pretty_attrs(self):
        attrs = [("grid_north_pole_latitude", self.grid_north_pole_latitude),
                 ("grid_north_pole_longitude", self.grid_north_pole_longitude)]
        if self.north_pole_grid_longitude != 0.0:
            attrs.append(("north_pole_grid_longitude",
                          self.north_pole_grid_longitude))
        if self.ellipsoid is not None:
            attrs.append(("ellipsoid", self.ellipsoid))
        return attrs

    def __repr__(self):
        attrs = self._pretty_attrs()
        result = "RotatedGeogCS(%s)" % ", ".join(
            ["%s=%r" % (k, v) for k, v in attrs])
        # Extra prettiness
        result = result.replace("grid_north_pole_latitude=", "")
        result = result.replace("grid_north_pole_longitude=", "")
        return result

    def __str__(self):
        attrs = self._pretty_attrs()
        result = "RotatedGeogCS(%s)" % ", ".join(
            ["%s=%s" % (k, v) for k, v in attrs])
        # Extra prettiness
        result = result.replace("grid_north_pole_latitude=", "")
        result = result.replace("grid_north_pole_longitude=", "")
        return result

    def xml_element(self, doc):
        return CoordSystem.xml_element(self, doc, self._pretty_attrs())

    def as_cartopy_crs(self):
        return cartopy.crs.RotatedGeodetic(self.grid_north_pole_longitude,
                                           self.grid_north_pole_latitude)

    def as_cartopy_projection(self):
        return cartopy.crs.RotatedPole(self.grid_north_pole_longitude,
                                       self.grid_north_pole_latitude)


class TransverseMercator(CoordSystem):
    """
    A cylindrical map projection, with XY coordinates measured in metres.

    """

    grid_mapping_name = "transverse_mercator"

    def __init__(self, latitude_of_projection_origin,
                 longitude_of_central_meridian, false_easting, false_northing,
                 scale_factor_at_central_meridian, ellipsoid=None):
        """
        Constructs a TransverseMercator object.

        Args:

            * latitude_of_projection_origin
                    True latitude of planar origin in degrees.

            * longitude_of_central_meridian
                    True longitude of planar origin in degrees.

            * false_easting
                    X offset from planar origin in metres.

            * false_northing
                    Y offset from planar origin in metres.

            * scale_factor_at_central_meridian
                    Reduces the cylinder to slice through the ellipsoid
                    (secant form). Used to provide TWO longitudes of zero
                    distortion in the area of interest.

        Kwargs:

            * ellipsoid
                    Optional :class:`GeogCS` defining the ellipsoid.

        Example::

            airy1830 = GeogCS(6377563.396, 6356256.909)
            osgb = TransverseMercator(49, -2, 400000, -100000, 0.9996012717,
                                      ellipsoid=airy1830)

        """
        #: True latitude of planar origin in degrees.
        self.latitude_of_projection_origin = float(
            latitude_of_projection_origin)

        #: True longitude of planar origin in degrees.
        self.longitude_of_central_meridian = float(
            longitude_of_central_meridian)

        #: X offset from planar origin in metres.
        self.false_easting = float(false_easting)

        #: Y offset from planar origin in metres.
        self.false_northing = float(false_northing)

        #: Reduces the cylinder to slice through the ellipsoid (secant form).
        self.scale_factor_at_central_meridian = float(
            scale_factor_at_central_meridian)

        #: Ellipsoid definition.
        self.ellipsoid = ellipsoid

    def __repr__(self):
        return "TransverseMercator(latitude_of_projection_origin={!r}, "\
               "longitude_of_central_meridian={!r}, false_easting={!r}, "\
               "false_northing={!r}, scale_factor_at_central_meridian={!r}, "\
               "ellipsoid={!r})".format(self.latitude_of_projection_origin,
                                        self.longitude_of_central_meridian,
                                        self.false_easting,
                                        self.false_northing,
                                        self.scale_factor_at_central_meridian,
                                        self.ellipsoid)

    def as_cartopy_crs(self):
        if self.ellipsoid is not None:
            globe = self.ellipsoid.as_cartopy_globe()
        else:
            globe = None

        return cartopy.crs.TransverseMercator(
            central_longitude=self.longitude_of_central_meridian,
            central_latitude=self.latitude_of_projection_origin,
            false_easting=self.false_easting,
            false_northing=self.false_northing,
            scale_factor=self.scale_factor_at_central_meridian,
            globe=globe)

    def as_cartopy_projection(self):
        return self.as_cartopy_crs()


class OSGB(TransverseMercator):
    """A Specific transverse mercator projection on a specific ellipsoid."""
    def __init__(self):
        TransverseMercator.__init__(self, 49, -2, 400000, -100000,
                                    0.9996012717,
                                    GeogCS(6377563.396, 6356256.909))

    def as_cartopy_crs(self):
        return cartopy.crs.OSGB()

    def as_cartopy_projection(self):
        return cartopy.crs.OSGB()


class Orthographic(CoordSystem):
    """
    An orthographic map projection.

    """

    grid_mapping_name = 'orthographic'

    def __init__(self, latitude_of_projection_origin,
                 longitude_of_projection_origin, false_easting=0.0,
                 false_northing=0.0, ellipsoid=None):
        """
        Constructs an Orthographic coord system.

        Args:

        * latitude_of_projection_origin:
            True latitude of planar origin in degrees.

        * longitude_of_projection_origin:
            True longitude of planar origin in degrees.

        * false_easting
            X offset from planar origin in metres. Defaults to 0.

        * false_northing
            Y offset from planar origin in metres. Defaults to 0.

        Kwargs:

        * ellipsoid
            :class:`GeogCS` defining the ellipsoid.

        """
        #: True latitude of planar origin in degrees.
        self.latitude_of_projection_origin = float(
            latitude_of_projection_origin)

        #: True longitude of planar origin in degrees.
        self.longitude_of_projection_origin = float(
            longitude_of_projection_origin)

        #: X offset from planar origin in metres.
        self.false_easting = float(false_easting)

        #: Y offset from planar origin in metres.
        self.false_northing = float(false_northing)

        #: Ellipsoid definition.
        self.ellipsoid = ellipsoid

    def __repr__(self):
        return "Orthographic(latitude_of_projection_origin={!r}, "\
               "longitude_of_projection_origin={!r}, "\
               "false_easting={!r}, false_northing={!r}, "\
               "ellipsoid={!r})".format(self.latitude_of_projection_origin,
                                        self.longitude_of_projection_origin,
                                        self.false_easting,
                                        self.false_northing,
                                        self.ellipsoid)

    def as_cartopy_crs(self):
        if self.ellipsoid is not None:
            globe = self.ellipsoid.as_cartopy_globe()
        else:
            globe = cartopy.crs.Globe()

        warnings.warn('Discarding false_easting and false_northing that are '
                      'not used by Cartopy.')

        return cartopy.crs.Orthographic(
            central_longitude=self.longitude_of_projection_origin,
            central_latitude=self.latitude_of_projection_origin,
            globe=globe)

    def as_cartopy_projection(self):
        return self.as_cartopy_crs()


class VerticalPerspective(CoordSystem):
    """
    An geostationary satellite image map projection.

    """

    grid_mapping_name = 'vertical_perspective'

    def __init__(self, latitude_of_projection_origin,
                 longitude_of_projection_origin, perspective_point_height,
                 false_easting=0, false_northing=0, ellipsoid=None):
        """
        Constructs an Vertical Perspective Geostationary coord system.

        Args:

        * latitude_of_projection_origin:
            True latitude of planar origin in degrees.

        * longitude_of_projection_origin:
            True longitude of planar origin in degrees.

        * perspective_point_height:
            Altitude of satellite in metres.

        * false_easting
            X offset from planar origin in metres. Defaults to 0.

        * false_northing
            Y offset from planar origin in metres. Defaults to 0.

        Kwargs:

        * ellipsoid
            :class:`GeogCS` defining the ellipsoid.

        """
        #: True latitude of planar origin in degrees.
        self.latitude_of_projection_origin = float(
            latitude_of_projection_origin)
        if self.latitude_of_projection_origin != 0.0:
            raise ValueError('Non-zero latitude of projection currently not'
                             ' supported by Cartopy.')

        #: True longitude of planar origin in degrees.
        self.longitude_of_projection_origin = float(
            longitude_of_projection_origin)

        #: Altitude of satellite in metres.
        # test if perspective_point_height may be cast to float for proj.4
        test_pph = float(perspective_point_height)
        self.perspective_point_height = perspective_point_height

        #: X offset from planar origin in metres.
        test_fe = float(false_easting)
        self.false_easting = false_easting

        #: Y offset from planar origin in metres.
        test_fn = float(false_northing)
        self.false_northing = false_northing

        #: Ellipsoid definition.
        self.ellipsoid = ellipsoid

    def __repr__(self):
        return "Vertical Perspective(latitude_of_projection_origin={!r}, "\
               "longitude_of_projection_origin={!r}, "\
               "perspective_point_height = {!r}, "\
               "false_easting={!r}, false_northing={!r}, "\
               "ellipsoid={!r})".format(self.latitude_of_projection_origin,
                                        self.longitude_of_projection_origin,
                                        self.perspective_point_height,
                                        self.false_easting,
                                        self.false_northing,
                                        self.ellipsoid)

    def as_cartopy_crs(self):
        if self.ellipsoid is not None:
            globe = self.ellipsoid.as_cartopy_globe()
        else:
            globe = cartopy.crs.Globe()

        return cartopy.crs.Geostationary(
            central_longitude=self.longitude_of_projection_origin,
            satellite_height=self.perspective_point_height,
            false_easting=self.false_easting,
            false_northing=self.false_northing,
            globe=globe)

    def as_cartopy_projection(self):
        return self.as_cartopy_crs()


class Stereographic(CoordSystem):
    """
    A stereographic map projection.

    """

    grid_mapping_name = "stereographic"

    def __init__(self, central_lat, central_lon,
                 false_easting=0.0, false_northing=0.0,
                 true_scale_lat=None, ellipsoid=None):
        """
        Constructs a Stereographic coord system.

        Args:

            * central_lat
                    The latitude of the pole.

            * central_lon
                    The central longitude, which aligns with the y axis.

            * false_easting
                    X offset from planar origin in metres. Defaults to 0.

            * false_northing
                    Y offset from planar origin in metres. Defaults to 0.

        Kwargs:

            * true_scale_lat
                    Latitude of true scale.

            * ellipsoid
                    :class:`GeogCS` defining the ellipsoid.

        """

        #: True latitude of planar origin in degrees.
        self.central_lat = float(central_lat)

        #: True longitude of planar origin in degrees.
        self.central_lon = float(central_lon)

        #: X offset from planar origin in metres.
        self.false_easting = float(false_easting)

        #: Y offset from planar origin in metres.
        self.false_northing = float(false_northing)

        #: Latitude of true scale.
        self.true_scale_lat = float(true_scale_lat) if true_scale_lat else None

        #: Ellipsoid definition.
        self.ellipsoid = ellipsoid

    def __repr__(self):
        return "Stereographic(central_lat={!r}, central_lon={!r}, "\
               "false_easting={!r}, false_northing={!r}, "\
               "true_scale_lat={!r}, "\
               "ellipsoid={!r})".format(self.central_lat, self.central_lon,
                                        self.false_easting,
                                        self.false_northing,
                                        self.true_scale_lat,
                                        self.ellipsoid)

    def as_cartopy_crs(self):
        if self.ellipsoid is not None:
            globe = self.ellipsoid.as_cartopy_globe()
        else:
            globe = cartopy.crs.Globe()
        return cartopy.crs.Stereographic(
            self.central_lat, self.central_lon,
            self.false_easting, self.false_northing,
            self.true_scale_lat, globe)

    def as_cartopy_projection(self):
        return self.as_cartopy_crs()


class LambertConformal(CoordSystem):
    """
    A coordinate system in the Lambert Conformal conic projection.

    """

    grid_mapping_name = "lambert_conformal"

    def __init__(self, central_lat=39.0, central_lon=-96.0,
                 false_easting=0.0, false_northing=0.0,
                 secant_latitudes=(33, 45), ellipsoid=None):
        """
        Constructs a LambertConformal coord system.

        Args:

            * central_lat
                    The latitude of "unitary scale".

            * central_lon
                    The central longitude.

            * false_easting
                    X offset from planar origin in metres.

            * false_northing
                    Y offset from planar origin in metres.

        Kwargs:

            * secant_latitudes
                    Latitudes of secant intersection.

            * ellipsoid
                    :class:`GeogCS` defining the ellipsoid.

        .. note:

            Default arguments are for the familiar USA map:
            central_lon=-96.0, central_lat=39.0,
            false_easting=0.0, false_northing=0.0,
            secant_latitudes=(33, 45)

        """

        #: True latitude of planar origin in degrees.
        self.central_lat = central_lat
        #: True longitude of planar origin in degrees.
        self.central_lon = central_lon
        #: X offset from planar origin in metres.
        self.false_easting = false_easting
        #: Y offset from planar origin in metres.
        self.false_northing = false_northing
        #: Latitudes of secant intersection.
        self.secant_latitudes = secant_latitudes
        #: Ellipsoid definition.
        self.ellipsoid = ellipsoid

    def __repr__(self):
        return "LambertConformal(central_lat={!r}, central_lon={!r}, "\
               "false_easting={!r}, false_northing={!r}, "\
               "secant_latitudes={!r}, ellipsoid={!r})".format(
                   self.central_lat, self.central_lon,
                   self.false_easting, self.false_northing,
                   self.secant_latitudes, self.ellipsoid)

    def as_cartopy_crs(self):
        # We're either north or south polar. Set a cutoff accordingly.
        if self.secant_latitudes is not None:
            lats = self.secant_latitudes
            max_lat = lats[0] if abs(lats[0]) > abs(lats[1]) else lats[1]
            cutoff = -30 if max_lat > 0 else 30
        else:
            cutoff = None

        if self.ellipsoid is not None:
            globe = self.ellipsoid.as_cartopy_globe()
        else:
            globe = cartopy.crs.Globe()

        return cartopy.crs.LambertConformal(
            self.central_lon, self.central_lat,
            self.false_easting, self.false_northing,
            self.secant_latitudes, globe, cutoff)

    def as_cartopy_projection(self):
        return self.as_cartopy_crs()

########NEW FILE########
__FILENAME__ = cube
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Classes for representing multi-dimensional data with metadata.

"""

from xml.dom.minidom import Document
import collections
import copy
import datetime
import operator
import re
import UserDict
import warnings
import zlib

import biggus
import numpy as np
import numpy.ma as ma

import iris.analysis
from iris.analysis.cartography import wrap_lons
import iris.analysis.maths
import iris.analysis.interpolate
import iris.aux_factory
import iris.coord_systems
import iris.coords
import iris._concatenate
import iris._constraints
import iris._merge
import iris.exceptions
import iris.util

from iris._cube_coord_common import CFVariableMixin


__all__ = ['Cube', 'CubeList', 'CubeMetadata']


class CubeMetadata(collections.namedtuple('CubeMetadata',
                                          ['standard_name',
                                           'long_name',
                                           'var_name',
                                           'units',
                                           'attributes',
                                           'cell_methods'])):
    """
    Represents the phenomenon metadata for a single :class:`Cube`.

    """
    def name(self, default='unknown'):
        """
        Returns a human-readable name.

        First it tries self.standard_name, then it tries the 'long_name'
        attribute, then the 'var_name' attribute, before falling back to
        the value of `default` (which itself defaults to 'unknown').

        """
        return self.standard_name or self.long_name or self.var_name or default


# The XML namespace to use for CubeML documents
XML_NAMESPACE_URI = "urn:x-iris:cubeml-0.2"


class _CubeFilter(object):
    """
    A constraint, paired with a list of cubes matching that constraint.

    """
    def __init__(self, constraint, cubes=None):
        self.constraint = constraint
        if cubes is None:
            cubes = CubeList()
        self.cubes = cubes

    def __len__(self):
        return len(self.cubes)

    def add(self, cube):
        """
        Adds the appropriate (sub)cube to the list of cubes where it
        matches the constraint.

        """
        sub_cube = self.constraint.extract(cube)
        if sub_cube is not None:
            self.cubes.append(sub_cube)

    def merged(self, unique=False):
        """
        Returns a new :class:`_CubeFilter` by merging the list of
        cubes.

        Kwargs:

        * unique:
            If True, raises `iris.exceptions.DuplicateDataError` if
            duplicate cubes are detected.

        """
        return _CubeFilter(self.constraint, self.cubes.merge(unique))


class _CubeFilterCollection(object):
    """
    A list of _CubeFilter instances.

    """
    @staticmethod
    def from_cubes(cubes, constraints=None):
        """
        Creates a new collection from an iterable of cubes, and some
        optional constraints.

        """
        constraints = iris._constraints.list_of_constraints(constraints)
        pairs = [_CubeFilter(constraint) for constraint in constraints]
        collection = _CubeFilterCollection(pairs)
        for cube in cubes:
            collection.add_cube(cube)
        return collection

    def __init__(self, pairs):
        self.pairs = pairs

    def add_cube(self, cube):
        """
        Adds the given :class:`~iris.cube.Cube` to all of the relevant
        constraint pairs.

        """
        for pair in self.pairs:
            pair.add(cube)

    def cubes(self):
        """
        Returns all the cubes in this collection concatenated into a
        single :class:`CubeList`.

        """
        result = CubeList()
        for pair in self.pairs:
            result.extend(pair.cubes)
        return result

    def merged(self, unique=False):
        """
        Returns a new :class:`_CubeFilterCollection` by merging all the cube
        lists of this collection.

        Kwargs:

        * unique:
            If True, raises `iris.exceptions.DuplicateDataError` if
            duplicate cubes are detected.

        """
        return _CubeFilterCollection([pair.merged(unique) for pair in
                                      self.pairs])


class CubeList(list):
    """
    All the functionality of a standard :class:`list` with added "Cube"
    context.

    """

    def __new__(cls, list_of_cubes=None):
        """Given a :class:`list` of cubes, return a CubeList instance."""
        cube_list = list.__new__(cls, list_of_cubes)

        # Check that all items in the incoming list are cubes. Note that this
        # checking does not guarantee that a CubeList instance *always* has
        # just cubes in its list as the append & __getitem__ methods have not
        # been overridden.
        if not all([isinstance(cube, Cube) for cube in cube_list]):
            raise ValueError('All items in list_of_cubes must be Cube '
                             'instances.')
        return cube_list

    def __str__(self):
        """Runs short :meth:`Cube.summary` on every cube."""
        result = ['%s: %s' % (i, cube.summary(shorten=True)) for i, cube in
                  enumerate(self)]
        if result:
            result = '\n'.join(result)
        else:
            result = '< No cubes >'
        return result

    def __repr__(self):
        """Runs repr on every cube."""
        return '[%s]' % ',\n'.join([repr(cube) for cube in self])

    # TODO #370 Which operators need overloads?
    def __add__(self, other):
        return CubeList(list.__add__(self, other))

    def __getitem__(self, keys):
        """x.__getitem__(y) <==> x[y]"""
        result = super(CubeList, self).__getitem__(keys)
        if isinstance(result, list):
            result = CubeList(result)
        return result

    def __getslice__(self, start, stop):
        """
        x.__getslice__(i, j) <==> x[i:j]

        Use of negative indices is not supported.

        """
        result = super(CubeList, self).__getslice__(start, stop)
        result = CubeList(result)
        return result

    def xml(self, checksum=False, order=True, byteorder=True):
        """Return a string of the XML that this list of cubes represents."""
        doc = Document()
        cubes_xml_element = doc.createElement("cubes")
        cubes_xml_element.setAttribute("xmlns", XML_NAMESPACE_URI)

        for cube_obj in self:
            cubes_xml_element.appendChild(
                cube_obj._xml_element(
                    doc, checksum=checksum, order=order, byteorder=byteorder))

        doc.appendChild(cubes_xml_element)

        # return our newly created XML string
        return doc.toprettyxml(indent="  ")

    def extract(self, constraints, strict=False):
        """
        Filter each of the cubes which can be filtered by the given
        constraints.

        This method iterates over each constraint given, and subsets each of
        the cubes in this CubeList where possible. Thus, a CubeList of length
        **n** when filtered with **m** constraints can generate a maximum of
        **m * n** cubes.

        Keywords:

        * strict - boolean
            If strict is True, then there must be exactly one cube which is
            filtered per constraint.

        """
        return self._extract_and_merge(self, constraints, strict,
                                       merge_unique=None)

    @staticmethod
    def _extract_and_merge(cubes, constraints, strict, merge_unique=False):
        # * merge_unique - if None: no merging, if false: non unique merging,
        # else unique merging (see merge)

        constraints = iris._constraints.list_of_constraints(constraints)

        # group the resultant cubes by constraints in a dictionary
        constraint_groups = dict([(constraint, CubeList()) for constraint in
                                 constraints])
        for cube in cubes:
            for constraint, cube_list in constraint_groups.iteritems():
                sub_cube = constraint.extract(cube)
                if sub_cube is not None:
                    cube_list.append(sub_cube)

        if merge_unique is not None:
            for constraint, cubelist in constraint_groups.iteritems():
                constraint_groups[constraint] = cubelist.merge(merge_unique)

        result = CubeList()
        for constraint in constraints:
            constraint_cubes = constraint_groups[constraint]
            if strict and len(constraint_cubes) != 1:
                msg = 'Got %s cubes for constraint %r, ' \
                      'expecting 1.' % (len(constraint_cubes), constraint)
                raise iris.exceptions.ConstraintMismatchError(msg)
            result.extend(constraint_cubes)

        if strict and len(constraints) == 1:
            result = result[0]

        return result

    def extract_strict(self, constraints):
        """
        Calls :meth:`CubeList.extract` with the strict keyword set to True.

        """
        return self.extract(constraints, strict=True)

    def merge_cube(self):
        """
        Return the merged contents of the :class:`CubeList` as a single
        :class:`Cube`.

        If it is not possible to merge the `CubeList` into a single
        `Cube`, a :class:`~iris.exceptions.MergeError` will be raised
        describing the reason for the failure.

        For example:

            >>> cube_1 = iris.cube.Cube([1, 2])
            >>> cube_1.add_aux_coord(iris.coords.AuxCoord(0, long_name='x'))
            >>> cube_2 = iris.cube.Cube([3, 4])
            >>> cube_2.add_aux_coord(iris.coords.AuxCoord(1, long_name='x'))
            >>> cube_2.add_dim_coord(
            ...     iris.coords.DimCoord([0, 1], long_name='z'), 0)
            >>> single_cube = iris.cube.CubeList([cube_1, cube_2]).merge_cube()
            Traceback (most recent call last):
            ...
            iris.exceptions.MergeError: failed to merge into a single cube.
              Coordinates in cube.dim_coords differ: z.
              Coordinate-to-dimension mapping differs for cube.dim_coords.

        """
        if not self:
            raise ValueError("can't merge an empty CubeList")

        # Register each of our cubes with a single ProtoCube.
        proto_cube = iris._merge.ProtoCube(self[0])
        for cube in self[1:]:
            proto_cube.register(cube, error_on_mismatch=True)

        # Extract the merged cube from the ProtoCube.
        merged_cube, = proto_cube.merge()
        return merged_cube

    def merge(self, unique=True):
        """
        Returns the :class:`CubeList` resulting from merging this
        :class:`CubeList`.

        Kwargs:

        * unique:
            If True, raises `iris.exceptions.DuplicateDataError` if
            duplicate cubes are detected.

        This combines cubes with different values of an auxiliary scalar
        coordinate, by constructing a new dimension.

        .. testsetup::

            import iris
            c1 = iris.cube.Cube([0,1,2], long_name='some_parameter')
            xco = iris.coords.DimCoord([11, 12, 13], long_name='x_vals')
            c1.add_dim_coord(xco, 0)
            c1.add_aux_coord(iris.coords.AuxCoord([100], long_name='y_vals'))
            c2 = c1.copy()
            c2.coord('y_vals').points = [200]

        For example::

            >>> print c1
            some_parameter / (unknown)          (x_vals: 3)
                 Dimension coordinates:
                      x_vals                           x
                 Scalar coordinates:
                      y_vals: 100
            >>> print c2
            some_parameter / (unknown)          (x_vals: 3)
                 Dimension coordinates:
                      x_vals                           x
                 Scalar coordinates:
                      y_vals: 200
            >>> cube_list = iris.cube.CubeList([c1, c2])
            >>> new_cube = cube_list.merge()[0]
            >>> print new_cube
            some_parameter / (unknown)          (y_vals: 2; x_vals: 3)
                 Dimension coordinates:
                      y_vals                           x          -
                      x_vals                           -          x
            >>> print new_cube.coord('y_vals').points
            [100 200]
            >>>

        Contrast this with :meth:`iris.cube.CubeList.concatenate`, which joins
        cubes along an existing dimension.

        .. note::

            If time coordinates in the list of cubes have differing epochs then
            the cubes will not be able to be merged. If this occurs, use
            :func:`iris.util.unify_time_units` to normalise the epochs of the
            time coordinates so that the cubes can be merged.

        """
        # Register each of our cubes with its appropriate ProtoCube.
        proto_cubes_by_name = {}
        for cube in self:
            name = cube.standard_name
            proto_cubes = proto_cubes_by_name.setdefault(name, [])
            proto_cube = None

            for target_proto_cube in proto_cubes:
                if target_proto_cube.register(cube):
                    proto_cube = target_proto_cube
                    break

            if proto_cube is None:
                proto_cube = iris._merge.ProtoCube(cube)
                proto_cubes.append(proto_cube)

        # Extract all the merged cubes from the ProtoCubes.
        merged_cubes = CubeList()
        for name in sorted(proto_cubes_by_name):
            for proto_cube in proto_cubes_by_name[name]:
                merged_cubes.extend(proto_cube.merge(unique=unique))

        return merged_cubes

    def concatenate(self):
        """
        Concatenate the cubes over their common dimensions.

        Returns:
            A new :class:`iris.cube.CubeList` of concatenated
            :class:`iris.cube.Cube` instances.

        This combines cubes with a common dimension coordinate, but occupying
        different regions of the coordinate value.  The cubes are joined across
        that dimension.

        .. testsetup::

            import iris
            import numpy as np
            xco = iris.coords.DimCoord([11, 12, 13, 14], long_name='x_vals')
            yco1 = iris.coords.DimCoord([4, 5], long_name='y_vals')
            yco2 = iris.coords.DimCoord([7, 9, 10], long_name='y_vals')
            c1 = iris.cube.Cube(np.zeros((2,4)), long_name='some_parameter')
            c1.add_dim_coord(xco, 1)
            c1.add_dim_coord(yco1, 0)
            c2 = iris.cube.Cube(np.zeros((3,4)), long_name='some_parameter')
            c2.add_dim_coord(xco, 1)
            c2.add_dim_coord(yco2, 0)

        For example::

            >>> print c1
            some_parameter / (unknown)          (y_vals: 2; x_vals: 4)
                 Dimension coordinates:
                      y_vals                           x          -
                      x_vals                           -          x
            >>> print c1.coord('y_vals').points
            [4 5]
            >>> print c2
            some_parameter / (unknown)          (y_vals: 3; x_vals: 4)
                 Dimension coordinates:
                      y_vals                           x          -
                      x_vals                           -          x
            >>> print c2.coord('y_vals').points
            [ 7  9 10]
            >>> cube_list = iris.cube.CubeList([c1, c2])
            >>> new_cube = cube_list.concatenate()[0]
            >>> print new_cube
            some_parameter / (unknown)          (y_vals: 5; x_vals: 4)
                 Dimension coordinates:
                      y_vals                           x          -
                      x_vals                           -          x
            >>> print new_cube.coord('y_vals').points
            [ 4  5  7  9 10]
            >>>

        Contrast this with :meth:`iris.cube.CubeList.merge`, which makes a new
        dimension from values of an auxiliary scalar coordinate.

        .. note::

            If time coordinates in the list of cubes have differing epochs then
            the cubes will not be able to be concatenated. If this occurs, use
            :func:`iris.util.unify_time_units` to normalise the epochs of the
            time coordinates so that the cubes can be concatenated.

        .. warning::

            This routine will load your data payload!

        """
        return iris._concatenate.concatenate(self)


class Cube(CFVariableMixin):
    """
    A single Iris cube of data and metadata.

    Typically obtained from :func:`iris.load`, :func:`iris.load_cube`,
    :func:`iris.load_cubes`, or from the manipulation of existing cubes.

    For example:

        >>> cube = iris.load_cube(iris.sample_data_path('air_temp.pp'))
        >>> print cube
        air_temperature / (K)               (latitude: 73; longitude: 96)
             Dimension coordinates:
                  latitude                           x              -
                  longitude                          -              x
             Scalar coordinates:
                  forecast_period: 6477 hours, bound=(-28083.0, 6477.0) hours
                  forecast_reference_time: 1998-03-01 03:00:00
                  pressure: 1000.0 hPa
                  time: 1998-12-01 00:00:00, \
bound=(1994-12-01 00:00:00, 1998-12-01 00:00:00)
             Attributes:
                  STASH: m01s16i203
                  source: Data from Met Office Unified Model
             Cell methods:
                  mean: time


    See the :doc:`user guide</userguide/index>` for more information.

    """
    def __init__(self, data, standard_name=None, long_name=None,
                 var_name=None, units=None, attributes=None,
                 cell_methods=None, dim_coords_and_dims=None,
                 aux_coords_and_dims=None, aux_factories=None):
        """
        Creates a cube with data and optional metadata.

        Not typically used - normally cubes are obtained by loading data
        (e.g. :func:`iris.load`) or from manipulating existing cubes.

        Args:

        * data
            This object defines the shape of the cube and the phenomenon
            value in each cell.

            It can be a biggus array, a numpy array, a numpy array
            subclass (such as :class:`numpy.ma.MaskedArray`), or an
            *array_like* as described in :func:`numpy.asarray`.

            See :attr:`Cube.data<iris.cube.Cube.data>`.

        Kwargs:

        * standard_name
            The standard name for the Cube's data.
        * long_name
            An unconstrained description of the cube.
        * var_name
            The CF variable name for the cube.
        * units
            The unit of the cube, e.g. ``"m s-1"`` or ``"kelvin"``.
        * attributes
            A dictionary of cube attributes
        * cell_methods
            A tuple of CellMethod objects, generally set by Iris, e.g.
            ``(CellMethod("mean", coords='latitude'), )``.
        * dim_coords_and_dims
            A list of coordinates with scalar dimension mappings, e.g
            ``[(lat_coord, 0), (lon_coord, 1)]``.
        * aux_coords_and_dims
            A list of coordinates with dimension mappings,
            e.g ``[(lat_coord, 0), (lon_coord, (0, 1))]``.
            See also :meth:`Cube.add_dim_coord()<iris.cube.Cube.add_dim_coord>`
            and :meth:`Cube.add_aux_coord()<iris.cube.Cube.add_aux_coord>`.
        * aux_factories
            A list of auxiliary coordinate factories. See
            :mod:`iris.aux_factory`.

        For example::

            latitude = DimCoord(range(-85, 105, 10), standard_name='latitude',
                                units='degrees')
            longitude = DimCoord(range(0, 360, 10), standard_name='longitude',
                                 units='degrees')
            cube = Cube(np.zeros((18, 36), np.float32),
                        dim_coords_and_dims=[(latitude, 0), (longitude, 1)])

        """
        # Temporary error while we transition the API.
        if isinstance(data, basestring):
            raise TypeError('Invalid data type: {!r}.'.format(data))

        if not isinstance(data, (biggus.Array, ma.MaskedArray)):
            data = np.asarray(data)
        self._my_data = data

        #: The "standard name" for the Cube's phenomenon.
        self.standard_name = standard_name

        #: An instance of :class:`iris.unit.Unit` describing the Cube's data.
        self.units = units

        #: The "long name" for the Cube's phenomenon.
        self.long_name = long_name

        #: The CF variable name for the Cube.
        self.var_name = var_name

        self.cell_methods = cell_methods

        #: A dictionary, with a few restricted keys, for arbitrary
        #: Cube metadata.
        self.attributes = attributes

        # Coords
        self._dim_coords_and_dims = []
        self._aux_coords_and_dims = []
        self._aux_factories = []

        identities = set()
        if dim_coords_and_dims:
            dims = set()
            for coord, dim in dim_coords_and_dims:
                identity = coord.standard_name, coord.long_name
                if identity not in identities and dim not in dims:
                    self._add_unique_dim_coord(coord, dim)
                else:
                    self.add_dim_coord(coord, dim)
                identities.add(identity)
                dims.add(dim)

        if aux_coords_and_dims:
            for coord, dims in aux_coords_and_dims:
                identity = coord.standard_name, coord.long_name
                if identity not in identities:
                    self._add_unique_aux_coord(coord, dims)
                else:
                    self.add_aux_coord(coord, dims)
                identities.add(identity)

        if aux_factories:
            for factory in aux_factories:
                self.add_aux_factory(factory)

    @property
    def metadata(self):
        """
        An instance of :class:`CubeMetadata` describing the phenomenon.

        This property can be updated with any of:
         - another :class:`CubeMetadata` instance,
         - a tuple/dict which can be used to make a :class:`CubeMetadata`,
         - or any object providing the attributes exposed by
           :class:`CubeMetadata`.

        """
        return CubeMetadata(self.standard_name, self.long_name, self.var_name,
                            self.units, self.attributes, self.cell_methods)

    @metadata.setter
    def metadata(self, value):
        try:
            value = CubeMetadata(**value)
        except TypeError:
            try:
                value = CubeMetadata(*value)
            except TypeError:
                attr_check = lambda name: not hasattr(value, name)
                missing_attrs = filter(attr_check, CubeMetadata._fields)
                if missing_attrs:
                    raise TypeError('Invalid/incomplete metadata')
        for name in CubeMetadata._fields:
            setattr(self, name, getattr(value, name))

    def is_compatible(self, other, ignore=None):
        """
        Return whether the cube is compatible with another.

        Compatibility is determined by comparing :meth:`iris.cube.Cube.name()`,
        :attr:`iris.cube.Cube.units`, :attr:`iris.cube.Cube.cell_methods` and
        :attr:`iris.cube.Cube.attributes` that are present in both objects.

        Args:

        * other:
            An instance of :class:`iris.cube.Cube` or
            :class:`iris.cube.CubeMetadata`.
        * ignore:
           A single attribute key or iterable of attribute keys to ignore when
           comparing the cubes. Default is None. To ignore all attributes set
           this to other.attributes.

        Returns:
           Boolean.

        .. seealso::

            :meth:`iris.util.describe_diff()`

        .. note::

            This function does not indicate whether the two cubes can be
            merged, instead it checks only the four items quoted above for
            equality. Determining whether two cubes will merge requires
            additional logic that is beyond the scope of this method.

        """
        compatible = (self.name() == other.name() and
                      self.units == other.units and
                      self.cell_methods == other.cell_methods)

        if compatible:
            common_keys = set(self.attributes).intersection(other.attributes)
            if ignore is not None:
                if isinstance(ignore, basestring):
                    ignore = (ignore,)
                common_keys = common_keys.difference(ignore)
            for key in common_keys:
                if np.any(self.attributes[key] != other.attributes[key]):
                    compatible = False
                    break

        return compatible

    def convert_units(self, unit):
        """
        Change the cube's units, converting the values in the data array.

        For example, if a cube's :attr:`~iris.cube.Cube.units` are
        kelvin then::

            cube.convert_units('celsius')

        will change the cube's :attr:`~iris.cube.Cube.units` attribute to
        celsius and subtract 273.15 from each value in
        :attr:`~iris.cube.Cube.data`.

        .. warning::
            Calling this method will trigger any deferred loading, causing
            the cube's data array to be loaded into memory.

        """
        # If the cube has units convert the data.
        if not self.units.is_unknown():
            self.data = self.units.convert(self.data, unit)
        self.units = unit

    def add_cell_method(self, cell_method):
        """Add a CellMethod to the Cube."""
        self.cell_methods += (cell_method, )

    def add_aux_coord(self, coord, data_dims=None):
        """
        Adds a CF auxiliary coordinate to the cube.

        Args:

        * coord
            The :class:`iris.coords.DimCoord` or :class:`iris.coords.AuxCoord`
            instance to add to the cube.

        Kwargs:

        * data_dims
            Integer or iterable of integers giving the data dimensions spanned
            by the coordinate.

        Raises a ValueError if a coordinate with identical metadata already
        exists on the cube.

        See also :meth:`Cube.remove_coord()<iris.cube.Cube.remove_coord>`.

        """
        if self.coords(coord):  # TODO: just fail on duplicate object
            raise ValueError('Duplicate coordinates are not permitted.')
        self._add_unique_aux_coord(coord, data_dims)

    def _add_unique_aux_coord(self, coord, data_dims):
        # Convert to a tuple of integers
        if data_dims is None:
            data_dims = tuple()
        elif isinstance(data_dims, collections.Container):
            data_dims = tuple(int(d) for d in data_dims)
        else:
            data_dims = (int(data_dims),)

        if data_dims:
            if len(data_dims) != coord.ndim:
                msg = 'Invalid data dimensions: {} given, {} expected for ' \
                      '{!r}.'.format(len(data_dims), coord.ndim, coord.name())
                raise ValueError(msg)
            # Check compatibility with the shape of the data
            for i, dim in enumerate(data_dims):
                if coord.shape[i] != self.shape[dim]:
                    msg = 'Unequal lengths. Cube dimension {} => {};' \
                          ' coord {!r} dimension {} => {}.'
                    raise ValueError(msg.format(dim, self.shape[dim],
                                                coord.name(), i,
                                                coord.shape[i]))
        elif coord.shape != (1,):
            raise ValueError('Missing data dimensions for multi-valued'
                             ' coordinate {!r}'.format(coord.name()))

        self._aux_coords_and_dims.append([coord, data_dims])

    def add_aux_factory(self, aux_factory):
        """
        Adds an auxiliary coordinate factory to the cube.

        Args:

        * aux_factory
            The :class:`iris.aux_factory.AuxCoordFactory` instance to add.

        """
        if not isinstance(aux_factory, iris.aux_factory.AuxCoordFactory):
            raise TypeError('Factory must be a subclass of '
                            'iris.aux_factory.AuxCoordFactory.')
        self._aux_factories.append(aux_factory)

    def add_dim_coord(self, dim_coord, data_dim):
        """
        Add a CF coordinate to the cube.

        Args:

        * dim_coord
            The :class:`iris.coords.DimCoord` instance to add to the cube.
        * data_dim
            Integer giving the data dimension spanned by the coordinate.

        Raises a ValueError if a coordinate with identical metadata already
        exists on the cube or if a coord already exists for the
        given dimension.

        See also :meth:`Cube.remove_coord()<iris.cube.Cube.remove_coord>`.

        """
        if self.coords(dim_coord):
            raise ValueError('The coordinate already exists on the cube. '
                             'Duplicate coordinates are not permitted.')
        # Check dimension is available
        if self.coords(dimensions=data_dim, dim_coords=True):
            raise ValueError('A dim_coord is already associated with '
                             'dimension %d.' % data_dim)
        self._add_unique_dim_coord(dim_coord, data_dim)

    def _add_unique_dim_coord(self, dim_coord, data_dim):
        if isinstance(dim_coord, iris.coords.AuxCoord):
            raise ValueError('The dim_coord may not be an AuxCoord instance.')

        # Convert data_dim to a single integer
        if isinstance(data_dim, collections.Container):
            if len(data_dim) != 1:
                raise ValueError('The supplied data dimension must be a'
                                 ' single number.')
            data_dim = int(list(data_dim)[0])
        else:
            data_dim = int(data_dim)

        # Check data_dim value is valid
        if data_dim < 0 or data_dim >= self.ndim:
            raise ValueError('The cube does not have the specified dimension '
                             '(%d)' % data_dim)

        # Check compatibility with the shape of the data
        if dim_coord.shape[0] != self.shape[data_dim]:
            msg = 'Unequal lengths. Cube dimension {} => {}; coord {!r} => {}.'
            raise ValueError(msg.format(data_dim, self.shape[data_dim],
                                        dim_coord.name(),
                                        len(dim_coord.points)))

        self._dim_coords_and_dims.append([dim_coord, int(data_dim)])

    def remove_aux_factory(self, aux_factory):
        """Removes the given auxiliary coordinate factory from the cube."""
        self._aux_factories.remove(aux_factory)

    def _remove_coord(self, coord):
        self._dim_coords_and_dims = [(coord_, dim) for coord_, dim in
                                     self._dim_coords_and_dims if coord_
                                     is not coord]
        self._aux_coords_and_dims = [(coord_, dims) for coord_, dims in
                                     self._aux_coords_and_dims if coord_
                                     is not coord]

    def remove_coord(self, coord):
        """
        Removes a coordinate from the cube.

        Args:

        * coord (string or coord)
            The (name of the) coordinate to remove from the cube.

        See also :meth:`Cube.add_coord()<iris.cube.Cube.add_coord>`.

        """
        coord = self.coord(coord)
        self._remove_coord(coord)

        for factory in self.aux_factories:
            factory.update(coord)

    def replace_coord(self, new_coord):
        """
        Replace the coordinate whose metadata matches the given coordinate.

        """
        old_coord = self.coord(new_coord)
        dims = self.coord_dims(old_coord)
        was_dimensioned = old_coord in self.dim_coords
        self._remove_coord(old_coord)
        if was_dimensioned and isinstance(new_coord, iris.coords.DimCoord):
            self.add_dim_coord(new_coord, dims[0])
        else:
            self.add_aux_coord(new_coord, dims)

        for factory in self.aux_factories:
            factory.update(old_coord, new_coord)

    def coord_dims(self, coord):
        """
        Returns a tuple of the data dimensions relevant to the given
        coordinate.

        When searching for the given coordinate in the cube the comparison is
        made using coordinate metadata equality. Hence the given coordinate
        instance need not exist on the cube, and may contain different
        coordinate values.

        Args:

        * coord (string or coord)
            The (name of the) coord to look for.

        """

        coord = self.coord(coord)

        # Search for existing coordinate (object) on the cube, faster lookup
        # than equality - makes no functional difference.
        matches = [(dim,) for coord_, dim in self._dim_coords_and_dims if
                   coord_ is coord]
        if not matches:
            matches = [dims for coord_, dims in self._aux_coords_and_dims if
                       coord_ is coord]

        # Search derived aux coords
        target_defn = coord._as_defn()
        if not matches:
            match = lambda factory: factory._as_defn() == target_defn
            factories = filter(match, self._aux_factories)
            matches = [factory.derived_dims(self.coord_dims) for factory in
                       factories]

        # Deprecate name based searching
        # -- Search by coord name, if have no match
        # XXX Where did this come from? And why isn't it reflected in the
        # docstring?
        if not matches:
            warnings.warn('name based coord matching is deprecated and will '
                          'be removed in a future release.',
                          stacklevel=2)
            matches = [(dim,) for coord_, dim in self._dim_coords_and_dims if
                       coord_.name() == coord.name()]
        # Finish deprecate name based searching

        if not matches:
            raise iris.exceptions.CoordinateNotFoundError(coord.name())

        return matches[0]

    def aux_factory(self, name=None, standard_name=None, long_name=None,
                    var_name=None):
        """
        Returns the single coordinate factory that matches the criteria,
        or raises an error if not found.

        Kwargs:

        * name
            If not None, matches against factory.name().
        * standard_name
            The CF standard name of the desired coordinate factory.
            If None, does not check for standard name.
        * long_name
            An unconstrained description of the coordinate factory.
            If None, does not check for long_name.
        * var_name
            The CF variable name of the desired coordinate factory.
            If None, does not check for var_name.

        .. note::

            If the arguments given do not result in precisely 1 coordinate
            factory being matched, an
            :class:`iris.exceptions.CoordinateNotFoundError` is raised.

        """
        factories = self.aux_factories

        if name is not None:
            factories = [factory for factory in factories if
                         factory.name() == name]

        if standard_name is not None:
            factories = [factory for factory in factories if
                         factory.standard_name == standard_name]

        if long_name is not None:
            factories = [factory for factory in factories if
                         factory.long_name == long_name]

        if var_name is not None:
            factories = [factory for factory in factories if
                         factory.var_name == var_name]

        if len(factories) > 1:
            factory_names = (factory.name() for factory in factories)
            msg = 'Expected to find exactly one coordinate factory, but ' \
                  'found {}. They were: {}.'.format(len(factories),
                                                    ', '.join(factory_names))
            raise iris.exceptions.CoordinateNotFoundError(msg)
        elif len(factories) == 0:
            msg = 'Expected to find exactly one coordinate factory, but ' \
                  'found none.'
            raise iris.exceptions.CoordinateNotFoundError(msg)

        return factories[0]

    def coords(self, name_or_coord=None, standard_name=None,
               long_name=None, var_name=None, attributes=None, axis=None,
               contains_dimension=None, dimensions=None, coord=None,
               coord_system=None, dim_coords=None, name=None):
        """
        Return a list of coordinates in this cube fitting the given criteria.

        Kwargs:

        * name_or_coord
            Either

            (a) a :attr:`standard_name`, :attr:`long_name`, or
            :attr:`var_name`. Defaults to value of `default`
            (which itself defaults to `unknown`) as defined in
            :class:`iris._cube_coord_common.CFVariableMixin`.

            (b) a coordinate instance with metadata equal to that of
            the desired coordinates. Accepts either a
            :class:`iris.coords.DimCoord`, :class:`iris.coords.AuxCoord`,
            :class:`iris.aux_factory.AuxCoordFactory`
            or :class:`iris.coords.CoordDefn`.
        * name
            .. deprecated:: 1.6. Please use the name_or_coord kwarg.
        * standard_name
            The CF standard name of the desired coordinate. If None, does not
            check for standard name.
        * long_name
            An unconstrained description of the coordinate. If None, does not
            check for long_name.
        * var_name
            The CF variable name of the desired coordinate. If None, does not
            check for var_name.
        * attributes
            A dictionary of attributes desired on the coordinates. If None,
            does not check for attributes.
        * axis
            The desired coordinate axis, see
            :func:`iris.util.guess_coord_axis`. If None, does not check for
            axis. Accepts the values 'X', 'Y', 'Z' and 'T' (case-insensitive).
        * contains_dimension
            The desired coordinate contains the data dimension. If None, does
            not check for the dimension.
        * dimensions
            The exact data dimensions of the desired coordinate. Coordinates
            with no data dimension can be found with an empty tuple or list
            (i.e. ``()`` or ``[]``). If None, does not check for dimensions.
        * coord
            .. deprecated:: 1.6. Please use the name_or_coord kwarg.
        * coord_system
            Whether the desired coordinates have coordinate systems equal to
            the given coordinate system. If None, no check is done.
        * dim_coords
            Set to True to only return coordinates that are the cube's
            dimension coordinates. Set to False to only return coordinates
            that are the cube's auxiliary and derived coordinates. If None,
            returns all coordinates.

        See also :meth:`Cube.coord()<iris.cube.Cube.coord>`.

        """
        # Handle deprecated kwargs
        if name is not None:
            name_or_coord = name
            warnings.warn('the name kwarg is deprecated and will be removed '
                          'in a future release. Consider converting '
                          'existing code to use the name_or_coord '
                          'kwarg as a replacement.',
                          stacklevel=2)
        if coord is not None:
            name_or_coord = coord
            warnings.warn('the coord kwarg is deprecated and will be removed '
                          'in a future release. Consider converting '
                          'existing code to use the name_or_coord '
                          'kwarg as a replacement.',
                          stacklevel=2)
        # Finish handling deprecated kwargs

        name = None
        coord = None

        if isinstance(name_or_coord, basestring):
            name = name_or_coord
        else:
            coord = name_or_coord

        coords_and_factories = []

        if dim_coords in [True, None]:
            coords_and_factories += list(self.dim_coords)

        if dim_coords in [False, None]:
            coords_and_factories += list(self.aux_coords)
            coords_and_factories += list(self.aux_factories)

        if name is not None:
            coords_and_factories = [coord_ for coord_ in coords_and_factories
                                    if coord_.name() == name]

        if standard_name is not None:
            coords_and_factories = [coord_ for coord_ in coords_and_factories
                                    if coord_.standard_name == standard_name]

        if long_name is not None:
            coords_and_factories = [coord_ for coord_ in coords_and_factories
                                    if coord_.long_name == long_name]

        if var_name is not None:
            coords_and_factories = [coord_ for coord_ in coords_and_factories
                                    if coord_.var_name == var_name]

        if axis is not None:
            axis = axis.upper()
            guess_axis = iris.util.guess_coord_axis
            coords_and_factories = [coord_ for coord_ in coords_and_factories
                                    if guess_axis(coord_) == axis]

        if attributes is not None:
            if not isinstance(attributes, collections.Mapping):
                msg = 'The attributes keyword was expecting a dictionary ' \
                      'type, but got a %s instead.' % type(attributes)
                raise ValueError(msg)
            attr_filter = lambda coord_: all(k in coord_.attributes and
                                             coord_.attributes[k] == v for
                                             k, v in attributes.iteritems())
            coords_and_factories = [coord_ for coord_ in coords_and_factories
                                    if attr_filter(coord_)]

        if coord_system is not None:
            coords_and_factories = [coord_ for coord_ in coords_and_factories
                                    if coord_.coord_system == coord_system]

        if coord is not None:
            if isinstance(coord, iris.coords.CoordDefn):
                defn = coord
            else:
                defn = coord._as_defn()
            coords_and_factories = [coord_ for coord_ in coords_and_factories
                                    if coord_._as_defn() == defn]

        if contains_dimension is not None:
            coords_and_factories = [coord_ for coord_ in coords_and_factories
                                    if contains_dimension in
                                    self.coord_dims(coord_)]

        if dimensions is not None:
            if not isinstance(dimensions, collections.Container):
                dimensions = [dimensions]
            dimensions = tuple(dimensions)
            coords_and_factories = [coord_ for coord_ in coords_and_factories
                                    if self.coord_dims(coord_) == dimensions]

        # If any factories remain after the above filters we have to make the
        # coords so they can be returned
        def extract_coord(coord_or_factory):
            if isinstance(coord_or_factory, iris.aux_factory.AuxCoordFactory):
                coord = coord_or_factory.make_coord(self.coord_dims)
            elif isinstance(coord_or_factory, iris.coords.Coord):
                coord = coord_or_factory
            else:
                msg = 'Expected Coord or AuxCoordFactory, got ' \
                      '{!r}.'.format(type(coord_or_factory))
                raise ValueError(msg)
            return coord
        coords = [extract_coord(coord_or_factory) for coord_or_factory in
                  coords_and_factories]

        return coords

    def coord(self, name_or_coord=None, standard_name=None,
              long_name=None, var_name=None, attributes=None, axis=None,
              contains_dimension=None, dimensions=None, coord=None,
              coord_system=None, dim_coords=None, name=None):
        """
        Return a single coord given the same arguments as :meth:`Cube.coords`.

        .. note::

            If the arguments given do not result in precisely 1 coordinate
            being matched, an :class:`iris.exceptions.CoordinateNotFoundError`
            is raised.

        .. seealso::

            :meth:`Cube.coords()<iris.cube.Cube.coords>` for full keyword
            documentation.

        """
        # Handle deprecated kwargs
        if name is not None:
            name_or_coord = name
            warnings.warn('the name kwarg is deprecated and will be removed '
                          'in a future release. Consider converting '
                          'existing code to use the name_or_coord '
                          'kwarg as a replacement.',
                          stacklevel=2)
        if coord is not None:
            name_or_coord = coord
            warnings.warn('the coord kwarg is deprecated and will be removed '
                          'in a future release. Consider converting '
                          'existing code to use the name_or_coord '
                          'kwarg as a replacement.',
                          stacklevel=2)
        # Finish handling deprecated kwargs

        coords = self.coords(name_or_coord=name_or_coord,
                             standard_name=standard_name,
                             long_name=long_name, var_name=var_name,
                             attributes=attributes, axis=axis,
                             contains_dimension=contains_dimension,
                             dimensions=dimensions,
                             coord_system=coord_system,
                             dim_coords=dim_coords)

        if len(coords) > 1:
            msg = 'Expected to find exactly 1 coordinate, but found %s. ' \
                  'They were: %s.' % (len(coords), ', '.join(coord.name() for
                                                             coord in coords))
            raise iris.exceptions.CoordinateNotFoundError(msg)
        elif len(coords) == 0:
            bad_name = name or standard_name or long_name or \
                (coord and coord.name()) or ''
            msg = 'Expected to find exactly 1 %s coordinate, but found ' \
                  'none.' % bad_name
            raise iris.exceptions.CoordinateNotFoundError(msg)

        return coords[0]

    def coord_system(self, spec=None):
        """
        Find the coordinate system of the given type.

        If no target coordinate system is provided then find
        any available coordinate system.

        Kwargs:

        * spec:
            The the name or type of a coordinate system subclass.
            E.g. ::

                cube.coord_system("GeogCS")
                cube.coord_system(iris.coord_systems.GeogCS)

            If spec is provided as a type it can be a superclass of
            any coordinate system found.

            If spec is None, then find any available coordinate
            systems within the :class:`iris.cube.Cube`.

        Returns:
            The :class:`iris.coord_systems.CoordSystem` or None.

        """
        if isinstance(spec, basestring) or spec is None:
            spec_name = spec
        else:
            msg = "type %s is not a subclass of CoordSystem" % spec
            assert issubclass(spec, iris.coord_systems.CoordSystem), msg
            spec_name = spec.__name__

        # Gather a temporary list of our unique CoordSystems.
        coord_systems = ClassDict(iris.coord_systems.CoordSystem)
        for coord in self.coords():
            if coord.coord_system:
                coord_systems.add(coord.coord_system, replace=True)

        result = None
        if spec_name is None:
            for key in sorted(coord_systems.keys()):
                result = coord_systems[key]
                break
        else:
            result = coord_systems.get(spec_name)

        return result

    @property
    def cell_methods(self):
        """
        Tuple of :class:`iris.coords.CellMethod` representing the processing
        done on the phenomenon.

        """
        return self._cell_methods

    @cell_methods.setter
    def cell_methods(self, cell_methods):
        self._cell_methods = tuple(cell_methods) if cell_methods else tuple()

    @property
    def shape(self):
        """The shape of the data of this cube."""
        shape = self.lazy_data().shape
        return shape

    @property
    def ndim(self):
        """The number of dimensions in the data of this cube."""
        return len(self.shape)

    def lazy_data(self, array=None):
        """
        Return a :class:`biggus.Array` representing the
        multi-dimensional data of the Cube, and optionally provide a
        new array of values.

        Accessing this method will never cause the data to be loaded.
        Similarly, calling methods on, or indexing, the returned Array
        will not cause the Cube to have loaded data.

        If the data have already been loaded for the Cube, the returned
        Array will be a :class:`biggus.NumpyArrayAdapter` which wraps
        the numpy array from `self.data`.

        Kwargs:

        * array (:class:`biggus.Array` or None):
            When this is not None it sets the multi-dimensional data of
            the cube to the given value.

        Returns:
            A :class:`biggus.Array` representing the multi-dimensional
            data of the Cube.

        """
        if array is not None:
            if not isinstance(array, biggus.Array):
                raise TypeError('new values must be a biggus.Array')
            if self.shape != array.shape:
                # The _ONLY_ data reshape permitted is converting a
                # 0-dimensional array into a 1-dimensional array of
                # length one.
                # i.e. self.shape = () and array.shape == (1,)
                if self.shape or array.shape != (1,):
                    raise ValueError('Require cube data with shape %r, got '
                                     '%r.' % (self.shape, array.shape))
            self._my_data = array
        else:
            array = self._my_data
            if not isinstance(array, biggus.Array):
                array = biggus.NumpyArrayAdapter(array)
        return array

    @property
    def data(self):
        """
        The :class:`numpy.ndarray` representing the multi-dimensional data of
        the cube.

        .. note::

            Cubes obtained from netCDF, PP, and FieldsFile files will only
            populate this attribute on its first use.

            To obtain the shape of the data without causing it to be loaded,
            use the Cube.shape attribute.

        Example::
            >>> fname = iris.sample_data_path('air_temp.pp')
            >>> cube = iris.load_cube(fname, 'air_temperature')  \
# cube.data does not yet have a value.
            >>> print cube.shape                                 \
# cube.data still does not have a value.
            (73, 96)
            >>> cube = cube[:10, :20]                            \
# cube.data still does not have a value.
            >>> data = cube.data                                 \
# Only now is the data loaded.
            >>> print data.shape
            (10, 20)

        """
        data = self._my_data
        if not isinstance(data, np.ndarray):
            try:
                data = data.masked_array()
            except MemoryError:
                msg = "Failed to create the cube's data as there was not" \
                      " enough memory available.\n" \
                      "The array shape would have been {0!r} and the data" \
                      " type {1}.\n" \
                      "Consider freeing up variables or indexing the cube" \
                      " before getting its data."
                msg = msg.format(self.shape, data.dtype)
                raise MemoryError(msg)
            # Unmask the array only if it is filled.
            if ma.count_masked(data) == 0:
                data = data.data
            self._my_data = data
        return data

    @data.setter
    def data(self, value):
        data = np.asanyarray(value)

        if self.shape != data.shape:
            # The _ONLY_ data reshape permitted is converting a 0-dimensional
            # array i.e. self.shape == () into a 1-dimensional array of length
            # one i.e. data.shape == (1,)
            if self.shape or data.shape != (1,):
                raise ValueError('Require cube data with shape %r, got '
                                 '%r.' % (self.shape, data.shape))

        self._my_data = data

    def has_lazy_data(self):
        return isinstance(self._my_data, biggus.Array)

    @property
    def dim_coords(self):
        """
        Return a tuple of all the dimension coordinates, ordered by dimension.

        .. note::

            The length of the returned tuple is not necessarily the same as
            :attr:`Cube.ndim` as there may be dimensions on the cube without
            dimension coordinates. It is therefore unreliable to use the
            resulting tuple to identify the dimension coordinates for a given
            dimension - instead use the :meth:`Cube.coord` method with the
            ``dimensions`` and ``dim_coords`` keyword arguments.

        """
        return tuple((coord for coord, dim in
                      sorted(self._dim_coords_and_dims,
                             key=lambda (coord, dim): (dim, coord.name()))))

    @property
    def aux_coords(self):
        """
        Return a tuple of all the auxiliary coordinates, ordered by
        dimension(s).

        """
        return tuple((coord for coord, dims in
                      sorted(self._aux_coords_and_dims,
                             key=lambda (coord, dims): (dims, coord.name()))))

    @property
    def derived_coords(self):
        """
        Return a tuple of all the coordinates generated by the coordinate
        factories.

        """
        return tuple(factory.make_coord(self.coord_dims) for factory in
                     sorted(self.aux_factories,
                            key=lambda factory: factory.name()))

    @property
    def aux_factories(self):
        """Return a tuple of all the coordinate factories."""
        return tuple(self._aux_factories)

    def _summary_coord_extra(self, coord, indent):
        # Returns the text needed to ensure this coordinate can be
        # distinguished from all others with the same name.
        extra = ''
        similar_coords = self.coords(coord.name())
        if len(similar_coords) > 1:
            # Find all the attribute keys
            keys = set()
            for similar_coord in similar_coords:
                keys.update(similar_coord.attributes.iterkeys())
            # Look for any attributes that vary
            vary = set()
            attributes = {}
            for key in keys:
                for similar_coord in similar_coords:
                    if key not in similar_coord.attributes:
                        vary.add(key)
                        break
                    value = similar_coord.attributes[key]
                    if attributes.setdefault(key, value) != value:
                        vary.add(key)
                        break
            keys = sorted(vary & coord.attributes.viewkeys())
            bits = ['{}={!r}'.format(key, coord.attributes[key]) for key in
                    keys]
            if bits:
                extra = indent + ', '.join(bits)
        return extra

    def _summary_extra(self, coords, summary, indent):
        # Where necessary, inserts extra lines into the summary to ensure
        # coordinates can be distinguished.
        new_summary = []
        for coord, summary in zip(coords, summary):
            new_summary.append(summary)
            extra = self._summary_coord_extra(coord, indent)
            if extra:
                new_summary.append(extra)
        return new_summary

    def summary(self, shorten=False, name_padding=35):
        """
        Unicode string summary of the Cube with name, a list of dim coord names
        versus length and optionally relevant coordinate information.

        """
        # Create a set to contain the axis names for each data dimension.
        dim_names = [set() for dim in xrange(len(self.shape))]

        # Add the dim_coord names that participate in the associated data
        # dimensions.
        for dim in xrange(len(self.shape)):
            dim_coords = self.coords(contains_dimension=dim, dim_coords=True)
            if dim_coords:
                dim_names[dim].add(dim_coords[0].name())
            else:
                dim_names[dim].add('-- ')

        # Convert axes sets to lists and sort.
        dim_names = [sorted(names, key=sorted_axes) for names in dim_names]

        # Generate textual summary of the cube dimensionality.
        if self.shape == ():
            dimension_header = 'scalar cube'
        else:
            dimension_header = '; '.join(
                [', '.join(dim_names[dim]) +
                 ': %d' % dim_shape for dim, dim_shape in
                 enumerate(self.shape)])

        nameunit = '{name} / ({units})'.format(name=self.name(),
                                               units=self.units)
        cube_header = '{nameunit!s:{length}} ({dimension})'.format(
            length=name_padding,
            nameunit=nameunit,
            dimension=dimension_header)
        summary = ''

        # Generate full cube textual summary.
        if not shorten:
            indent = 10
            extra_indent = ' ' * 13

            # Cache the derived coords so we can rely on consistent
            # object IDs.
            derived_coords = self.derived_coords
            # Determine the cube coordinates that are scalar (single-valued)
            # AND non-dimensioned.
            dim_coords = self.dim_coords
            aux_coords = self.aux_coords
            all_coords = dim_coords + aux_coords + derived_coords
            scalar_coords = [coord for coord in all_coords if not
                             self.coord_dims(coord) and coord.shape == (1,)]
            # Determine the cube coordinates that are not scalar BUT
            # dimensioned.
            scalar_coord_ids = set(map(id, scalar_coords))
            vector_dim_coords = [coord for coord in dim_coords if id(coord) not
                                 in scalar_coord_ids]
            vector_aux_coords = [coord for coord in aux_coords if id(coord) not
                                 in scalar_coord_ids]
            vector_derived_coords = [coord for coord in derived_coords if
                                     id(coord) not in scalar_coord_ids]

            # Determine the cube coordinates that don't describe the cube and
            # are most likely erroneous.
            vector_coords = vector_dim_coords + vector_aux_coords + \
                vector_derived_coords
            ok_coord_ids = scalar_coord_ids.union(set(map(id, vector_coords)))
            invalid_coords = [coord for coord in all_coords if id(coord) not
                              in ok_coord_ids]

            # Sort scalar coordinates by name.
            scalar_coords.sort(key=lambda coord: coord.name())
            # Sort vector coordinates by data dimension and name.
            vector_dim_coords.sort(
                key=lambda coord: (self.coord_dims(coord), coord.name()))
            vector_aux_coords.sort(
                key=lambda coord: (self.coord_dims(coord), coord.name()))
            vector_derived_coords.sort(
                key=lambda coord: (self.coord_dims(coord), coord.name()))
            # Sort other coordinates by name.
            invalid_coords.sort(key=lambda coord: coord.name())

            #
            # Generate textual summary of cube vector coordinates.
            #
            def vector_summary(vector_coords, cube_header, max_line_offset):
                """
                Generates a list of suitably aligned strings containing coord
                names and dimensions indicated by one or more 'x' symbols.

                .. note::

                    The function may need to update the cube header so this is
                    returned with the list of strings.

                """
                vector_summary = []
                if vector_coords:
                    # Identify offsets for each dimension text marker.
                    alignment = np.array([index for index, value in
                                          enumerate(cube_header) if
                                          value == ':'])

                    # Generate basic textual summary for each vector coordinate
                    # - WITHOUT dimension markers.
                    for coord in vector_coords:
                        vector_summary.append('%*s%s' % (
                            indent, ' ', iris.util.clip_string(coord.name())))
                    min_alignment = min(alignment)

                    # Determine whether the cube header requires realignment
                    # due to one or more longer vector coordinate summaries.
                    if max_line_offset >= min_alignment:
                        delta = max_line_offset - min_alignment + 5
                        cube_header = '%-*s (%s)' % (int(name_padding + delta),
                                                     self.name() or 'unknown',
                                                     dimension_header)
                        alignment += delta

                    # Generate full textual summary for each vector coordinate
                    # - WITH dimension markers.
                    for index, coord in enumerate(vector_coords):
                        dims = self.coord_dims(coord)
                        for dim in xrange(len(self.shape)):
                            width = alignment[dim] - len(vector_summary[index])
                            char = 'x' if dim in dims else '-'
                            line = '{pad:{width}}{char}'.format(pad=' ',
                                                                width=width,
                                                                char=char)
                            vector_summary[index] += line
                    # Interleave any extra lines that are needed to distinguish
                    # the coordinates.
                    vector_summary = self._summary_extra(vector_coords,
                                                         vector_summary,
                                                         extra_indent)

                return vector_summary, cube_header

            # Calculate the maximum line offset.
            max_line_offset = 0
            for coord in all_coords:
                max_line_offset = max(max_line_offset, len('%*s%s' % (
                    indent, ' ', iris.util.clip_string(str(coord.name())))))

            if vector_dim_coords:
                dim_coord_summary, cube_header = vector_summary(
                    vector_dim_coords, cube_header, max_line_offset)
                summary += '\n     Dimension coordinates:\n' + \
                    '\n'.join(dim_coord_summary)

            if vector_aux_coords:
                aux_coord_summary, cube_header = vector_summary(
                    vector_aux_coords, cube_header, max_line_offset)
                summary += '\n     Auxiliary coordinates:\n' + \
                    '\n'.join(aux_coord_summary)

            if vector_derived_coords:
                derived_coord_summary, cube_header = vector_summary(
                    vector_derived_coords, cube_header, max_line_offset)
                summary += '\n     Derived coordinates:\n' + \
                    '\n'.join(derived_coord_summary)

            #
            # Generate textual summary of cube scalar coordinates.
            #
            scalar_summary = []

            if scalar_coords:
                for coord in scalar_coords:
                    if (coord.units in ['1', 'no_unit', 'unknown'] or
                            coord.units.is_time_reference()):
                        unit = ''
                    else:
                        unit = ' {!s}'.format(coord.units)

                    # Format cell depending on type of point and whether it
                    # has a bound
                    with iris.FUTURE.context(cell_datetime_objects=False):
                        coord_cell = coord.cell(0)
                    if isinstance(coord_cell.point, basestring):
                        # Indent string type coordinates
                        coord_cell_split = [iris.util.clip_string(str(item))
                                            for item in
                                            coord_cell.point.split('\n')]
                        line_sep = '\n{pad:{width}}'.format(
                            pad=' ', width=indent + len(coord.name()) + 2)
                        coord_cell_str = line_sep.join(coord_cell_split) + unit
                    else:
                        # Human readable times
                        if coord.units.is_time_reference():
                            coord_cell_cpoint = coord.units.num2date(
                                coord_cell.point)
                            if coord_cell.bound is not None:
                                coord_cell_cbound = coord.units.num2date(
                                    coord_cell.bound)
                        else:
                            coord_cell_cpoint = coord_cell.point
                            coord_cell_cbound = coord_cell.bound

                        coord_cell_str = '{!s}{}'.format(coord_cell_cpoint,
                                                         unit)
                        if coord_cell.bound is not None:
                            bound = '({})'.format(', '.join(str(val) for
                                                  val in coord_cell_cbound))
                            coord_cell_str += ', bound={}{}'.format(bound,
                                                                    unit)

                    scalar_summary.append('{pad:{width}}{name}: {cell}'.format(
                        pad=' ', width=indent, name=coord.name(),
                        cell=coord_cell_str))

                # Interleave any extra lines that are needed to distinguish
                # the coordinates.
                scalar_summary = self._summary_extra(scalar_coords,
                                                     scalar_summary,
                                                     extra_indent)

                summary += '\n     Scalar coordinates:\n' + '\n'.join(
                    scalar_summary)

            #
            # Generate summary of cube's invalid coordinates.
            #
            if invalid_coords:
                invalid_summary = []

                for coord in invalid_coords:
                    invalid_summary.append(
                        '%*s%s' % (indent, ' ', coord.name()))

                # Interleave any extra lines that are needed to distinguish the
                # coordinates.
                invalid_summary = self._summary_extra(
                    invalid_coords, invalid_summary, extra_indent)

                summary += '\n     Invalid coordinates:\n' + \
                    '\n'.join(invalid_summary)

            #
            # Generate summary of cube attributes.
            #
            if self.attributes:
                attribute_lines = []
                for name, value in sorted(self.attributes.iteritems()):
                    value = iris.util.clip_string(unicode(value))
                    line = u'{pad:{width}}{name}: {value}'.format(pad=' ',
                                                                  width=indent,
                                                                  name=name,
                                                                  value=value)
                    attribute_lines.append(line)
                summary += '\n     Attributes:\n' + '\n'.join(attribute_lines)

            #
            # Generate summary of cube cell methods
            #
            if self.cell_methods:
                summary += '\n     Cell methods:\n'
                cm_lines = []

                for cm in self.cell_methods:
                    cm_lines.append('%*s%s' % (indent, ' ', str(cm)))
                summary += '\n'.join(cm_lines)

        # Construct the final cube summary.
        summary = cube_header + summary

        return summary

    def assert_valid(self):
        """Raise an exception if the cube is invalid; otherwise return None."""

        warnings.warn('Cube.assert_valid() has been deprecated.')

    def __str__(self):
        return self.summary().encode(errors='replace')

    def __unicode__(self):
        return self.summary()

    def __repr__(self):
        return "<iris 'Cube' of %s>" % self.summary(shorten=True,
                                                    name_padding=1)

    def __iter__(self):
        raise TypeError('Cube is not iterable')

    def __getitem__(self, keys):
        """
        Cube indexing (through use of square bracket notation) has been
        implemented at the data level. That is, the indices provided to this
        method should be aligned to the data of the cube, and thus the indices
        requested must be applicable directly to the cube.data attribute. All
        metadata will be subsequently indexed appropriately.

        """
        # turn the keys into a full slice spec (all dims)
        full_slice = iris.util._build_full_slice_given_keys(keys,
                                                            len(self.shape))

        # make indexing on the cube column based by using the
        # column_slices_generator (potentially requires slicing the data
        # multiple times)
        dimension_mapping, slice_gen = iris.util.column_slices_generator(
            full_slice, len(self.shape))
        new_coord_dims = lambda coord_: [dimension_mapping[d] for d in
                                         self.coord_dims(coord_) if
                                         dimension_mapping[d] is not None]

        try:
            first_slice = slice_gen.next()
        except StopIteration:
            first_slice = None

        if first_slice is not None:
            data = self._my_data[first_slice]
        else:
            data = copy.deepcopy(self._my_data)

        for other_slice in slice_gen:
            data = data[other_slice]

        # We don't want a view of the data, so take a copy of it if it's
        # not already our own.
        if isinstance(data, biggus.Array) or not data.flags['OWNDATA']:
            data = copy.deepcopy(data)

        # We can turn a masked array into a normal array if it's full.
        if isinstance(data, ma.core.MaskedArray):
            if ma.count_masked(data) == 0:
                data = data.filled()

        # Make the new cube slice
        cube = Cube(data)
        cube.metadata = copy.deepcopy(self.metadata)

        # Record a mapping from old coordinate IDs to new coordinates,
        # for subsequent use in creating updated aux_factories.
        coord_mapping = {}

        # Slice the coords
        for coord in self.aux_coords:
            coord_keys = tuple([full_slice[dim] for dim in
                                self.coord_dims(coord)])
            try:
                new_coord = coord[coord_keys]
            except ValueError:
                # TODO make this except more specific to catch monotonic error
                # Attempt to slice it by converting to AuxCoord first
                new_coord = iris.coords.AuxCoord.from_coord(coord)[coord_keys]
            cube.add_aux_coord(new_coord, new_coord_dims(coord))
            coord_mapping[id(coord)] = new_coord

        for coord in self.dim_coords:
            coord_keys = tuple([full_slice[dim] for dim in
                                self.coord_dims(coord)])
            new_dims = new_coord_dims(coord)
            # Try/Catch to handle slicing that makes the points/bounds
            # non-monotonic
            try:
                new_coord = coord[coord_keys]
                if not new_dims:
                    # If the associated dimension has been sliced so the coord
                    # is a scalar move the coord to the aux_coords container
                    cube.add_aux_coord(new_coord, new_dims)
                else:
                    cube.add_dim_coord(new_coord, new_dims)
            except ValueError:
                # TODO make this except more specific to catch monotonic error
                # Attempt to slice it by converting to AuxCoord first
                new_coord = iris.coords.AuxCoord.from_coord(coord)[coord_keys]
                cube.add_aux_coord(new_coord, new_dims)
            coord_mapping[id(coord)] = new_coord

        for factory in self.aux_factories:
            cube.add_aux_factory(factory.updated(coord_mapping))

        return cube

    def subset(self, coord):
        """
        Get a subset of the cube by providing the desired resultant coordinate.

        """
        if not isinstance(coord, iris.coords.Coord):
            raise ValueError('coord_to_extract must be a valid Coord.')

        # Get the coord to extract from the cube
        coord_to_extract = self.coord(coord)
        if len(self.coord_dims(coord_to_extract)) > 1:
            msg = "Currently, only 1D coords can be used to subset a cube"
            raise iris.exceptions.CoordinateMultiDimError(msg)
        # Identify the dimension of the cube which this coordinate references
        coord_to_extract_dim = self.coord_dims(coord_to_extract)[0]

        # Identify the indices which intersect the requested coord and
        # coord_to_extract
        coordinate_indices = coord_to_extract.intersect(coord,
                                                        return_indices=True)

        # Build up a slice which spans the whole of the cube
        full_slice = [slice(None, None)] * len(self.shape)
        # Update the full slice to only extract specific indices which were
        # identified above
        full_slice[coord_to_extract_dim] = coordinate_indices
        full_slice = tuple(full_slice)
        return self[full_slice]

    def extract(self, constraint):
        """
        Filter the cube by the given constraint using
        :meth:`iris.Constraint.extract` method.

        """
        # Cast the constraint into a proper constraint if it is not so already
        constraint = iris._constraints.as_constraint(constraint)
        return constraint.extract(self)

    def intersection(self, *args, **kwargs):
        """
        Return the intersection of the cube with specified coordinate
        ranges.

        Coordinate ranges can be specified as:

        (a) instances of :class:`iris.coords.CoordExtent`.

        (b) keyword arguments, where the keyword name specifies the name
            of the coordinate (as defined in :meth:`iris.cube.Cube.coords()`)
            and the value defines the corresponding range of coordinate
            values as a tuple. The tuple must contain two, three, or four
            items corresponding to: (minimum, maximum, min_inclusive,
            max_inclusive). Where the items are defined as:

            * minimum
                The minimum value of the range to select.

            * maximum
                The maximum value of the range to select.

            * min_inclusive
                If True, coordinate values equal to `minimum` will be included
                in the selection. Default is True.

            * max_inclusive
                If True, coordinate values equal to `maximum` will be included
                in the selection. Default is True.

        .. note::

            For ranges defined over "circular" coordinates (i.e. those
            where the `units` attribute has a modulus defined) the cube
            will be "rolled" to fit where neccesary.

        .. warning::

            Currently this routine only works with "circular"
            coordinates (as defined in the previous note.)

        For example::

            >>> import iris
            >>> cube = iris.load_cube(iris.sample_data_path('air_temp.pp'))
            >>> print cube.coord('longitude').points[::10]
            [   0.           37.49999237   74.99998474  112.49996948  \
149.99996948
              187.49995422  224.99993896  262.49993896  299.99993896  \
337.49990845]
            >>> subset = cube.intersection(longitude=(30, 50))
            >>> print subset.coord('longitude').points
            [ 33.74999237  37.49999237  41.24998856  44.99998856  48.74998856]
            >>> subset = cube.intersection(longitude=(-10, 10))
            >>> print subset.coord('longitude').points
            [-7.50012207 -3.75012207  0.          3.75        7.5       ]

        Returns:
            A new :class:`~iris.cube.Cube` giving the subset of the cube
            which intersects with the requested coordinate intervals.

        """
        result = self
        for arg in args:
            result = result._intersect(*arg)
        for name, value in kwargs.iteritems():
            result = result._intersect(name, *value)
        return result

    def _intersect(self, name_or_coord, minimum, maximum,
                   min_inclusive=True, max_inclusive=True):
        coord = self.coord(name_or_coord)
        if coord.ndim != 1:
            raise iris.exceptions.CoordinateMultiDimError(coord)
        if coord.nbounds not in (0, 2):
            raise ValueError('expected 0 or 2 bound values per cell')
        if minimum > maximum:
            raise ValueError('minimum greater than maximum')
        modulus = coord.units.modulus
        if modulus is None:
            raise ValueError('coordinate units with no modulus are not yet'
                             ' supported')

        subsets, points, bounds = self._intersect_modulus(coord,
                                                          minimum, maximum,
                                                          min_inclusive,
                                                          max_inclusive)

        # By this point we have either one or two subsets along the relevant
        # dimension. If it's just one subset (which might be a slice or an
        # unordered collection of indices) we can simply index the cube
        # and we're done. If it's two subsets we need to stitch the two
        # pieces together.
        def make_chunk(key):
            chunk = self[key_tuple_prefix + (key,)]
            chunk_coord = chunk.coord(coord)
            chunk_coord.points = points[(key,)]
            if chunk_coord.has_bounds():
                chunk_coord.bounds = bounds[(key,)]
            return chunk

        dim, = self.coord_dims(coord)
        key_tuple_prefix = (slice(None),) * dim
        chunks = [make_chunk(key) for key in subsets]
        if len(chunks) == 1:
            result = chunks[0]
        else:
            if self.has_lazy_data():
                data = biggus.LinearMosaic([chunk.lazy_data()
                                            for chunk in chunks],
                                           dim)
            else:
                module = ma if ma.isMaskedArray(self.data) else np
                data = module.concatenate([chunk.data for chunk in chunks],
                                          dim)
            result = iris.cube.Cube(data)
            result.metadata = copy.deepcopy(self.metadata)

            # Record a mapping from old coordinate IDs to new coordinates,
            # for subsequent use in creating updated aux_factories.
            coord_mapping = {}

            def create_coords(src_coords, add_coord):
                # Add copies of the source coordinates, selecting
                # the appropriate subsets out of coordinates which
                # share the intersection dimension.
                for src_coord in src_coords:
                    dims = self.coord_dims(src_coord)
                    if dim in dims:
                        dim_within_coord = dims.index(dim)
                        points = np.concatenate([chunk.coord(src_coord).points
                                                 for chunk in chunks],
                                                dim_within_coord)
                        if src_coord.has_bounds():
                            bounds = np.concatenate(
                                [chunk.coord(src_coord).bounds
                                 for chunk in chunks],
                                dim_within_coord)
                        else:
                            bounds = None
                        result_coord = src_coord.copy(points=points,
                                                      bounds=bounds)
                    else:
                        result_coord = src_coord.copy()
                    add_coord(result_coord, dims)
                    coord_mapping[id(src_coord)] = result_coord

            create_coords(self.dim_coords, result.add_dim_coord)
            create_coords(self.aux_coords, result.add_aux_coord)
            for factory in self.aux_factories:
                result.add_aux_factory(factory.updated(coord_mapping))
        return result

    def _intersect_modulus(self, coord, minimum, maximum, min_inclusive,
                           max_inclusive):
        modulus = coord.units.modulus
        if maximum > minimum + modulus:
            raise ValueError("requested range greater than coordinate's"
                             " unit's modulus")
        if coord.has_bounds():
            values = coord.bounds
        else:
            values = coord.points
        if values.max() > values.min() + modulus:
            raise ValueError("coordinate's range greater than coordinate's"
                             " unit's modulus")
        min_comp = np.less_equal if min_inclusive else np.less
        max_comp = np.less_equal if max_inclusive else np.less
        if coord.has_bounds():
            bounds = wrap_lons(coord.bounds, minimum, modulus)
            inside = np.logical_and(min_comp(minimum, bounds),
                                    max_comp(bounds, maximum))
            inside_indices, = np.where(np.any(inside, axis=1))

            # To ensure that bounds (and points) of matching cells aren't
            # "scrambled" by the wrap operation we detect split cells that
            # straddle the wrap point and ensure that the lower bound of the
            # cell (and associated point) are wrapped correctly given the
            # provided wrap point.
            # For example: the cell [349.875, 350.4375] wrapped at -10 would
            # become [349.875, -9.5625] which is no longer valid. The lower
            # cell bound value (and possibly associated point) are
            # recalculated so that they are consistent with the extended
            # wapping scheme which moves the wrap point to the correct lower
            # bound value thus resulting in the cell no longer being split.

            pre_wrap_delta = np.diff(coord.bounds[inside_indices])
            post_wrap_delta = np.diff(bounds[inside_indices])
            split_cell_indices, _ = np.where(pre_wrap_delta != post_wrap_delta)
            if split_cell_indices.size:
                # Recalculate the extended minimum.
                indices = inside_indices[split_cell_indices]
                cells = bounds[indices]
                cells_delta = np.diff(coord.bounds[indices])
                cells[:, 0] = cells[:, 1] - cells_delta[:, 0]
                minimum = np.min(cells[:, 0])
                bounds = wrap_lons(coord.bounds, minimum, modulus)
            points = wrap_lons(coord.points, minimum, modulus)
        else:
            points = iris.analysis.cartography.wrap_lons(coord.points, minimum,
                                                         modulus)
            bounds = None
            inside_indices, = np.where(
                np.logical_and(min_comp(minimum, points),
                               max_comp(points, maximum)))
        if isinstance(coord, iris.coords.DimCoord):
            delta = coord.points[inside_indices] - points[inside_indices]
            tolerance = np.finfo(delta.dtype).eps * modulus
            if np.allclose(delta, delta[0], rtol=tolerance, atol=tolerance):
                # A single, contiguous block.
                subsets = [slice(inside_indices[0], inside_indices[-1] + 1)]
            else:
                # A contiguous block at the start and another at the
                # end. (NB. We can't have more than two blocks
                # because we've already restricted the coordinate's
                # range to its modulus).
                step = np.rint(np.diff(delta) / modulus)
                end_of_first_chunk = np.where(step != step[0])[0][0]
                subsets = [slice(inside_indices[end_of_first_chunk + 1], None),
                           slice(None, inside_indices[end_of_first_chunk] + 1)]
        else:
            # An AuxCoord could have its values in an arbitrary
            # order, and hence a range of values can select an
            # arbitrary subset. Also, we want to preserve the order
            # from the original AuxCoord. So we just use the indices
            # directly.
            subsets = [inside_indices]
        return subsets, points, bounds

    def _as_list_of_coords(self, names_or_coords):
        """
        Convert a name, coord, or list of names/coords to a list of coords.
        """
        # If not iterable, convert to list of a single item
        if not hasattr(names_or_coords, '__iter__'):
            names_or_coords = [names_or_coords]

        coords = []
        for name_or_coord in names_or_coords:
            if (isinstance(name_or_coord, basestring) or
                    isinstance(name_or_coord, iris.coords.Coord)):
                coords.append(self.coord(name_or_coord))
            else:
                # Don't know how to handle this type
                msg = "Don't know how to handle coordinate of type %s. " \
                      "Ensure all coordinates are of type basestring or " \
                      "iris.coords.Coord." % type(name_or_coord)
                raise TypeError(msg)
        return coords

    def slices(self, ref_to_slice, ordered=True):
        """
        Return an iterator of all subcubes given the coordinates or dimension
        indices desired to be present in each subcube.

        Args:

        * ref_to_slice (string, coord, dimension index or a list of these):
            Determines which dimensions will be returned in the subcubes (i.e.
            the dimensions that are not iterated over).
            A mix of input types can also be provided. They must all be
            orthogonal (i.e. point to different dimensions).

        Kwargs:

        * ordered: if True, the order which the coords to slice or data_dims
            are given will be the order in which they represent the data in
            the resulting cube slices.  If False, the order will follow that of
            the source cube.  Default is True.

        Returns:
            An iterator of subcubes.

        For example, to get all 2d longitude/latitude subcubes from a
        multi-dimensional cube::

            for sub_cube in cube.slices(['longitude', 'latitude']):
                print sub_cube

        """
        if not isinstance(ordered, bool):
            raise TypeError("'ordered' argument to slices must be boolean.")

        # Required to handle a mix between types
        if not hasattr(ref_to_slice, '__iter__'):
            ref_to_slice = [ref_to_slice]

        dim_to_slice = []
        for ref in ref_to_slice:
            try:
                # attempt to handle as coordinate
                coord = self._as_list_of_coords(ref)[0]
                dims = self.coord_dims(coord)
                if not dims:
                    msg = ('Requested an iterator over a coordinate ({}) '
                           'which does not describe a dimension.')
                    msg = msg.format(coord.name())
                    raise ValueError(msg)
                dim_to_slice.extend(dims)

            except TypeError:
                try:
                    # attempt to handle as dimension index
                    dim = int(ref)
                except ValueError:
                    raise ValueError('{} Incompatible type {} for '
                                     'slicing'.format(ref, type(ref)))
                if dim < 0 or dim > self.ndim:
                    msg = ('Requested an iterator over a dimension ({}) '
                           'which does not exist.'.format(dim))
                    raise ValueError(msg)
                dim_to_slice.append(dim)

        if len(set(dim_to_slice)) != len(dim_to_slice):
            msg = 'The requested coordinates are not orthogonal.'
            raise ValueError(msg)

        # Create a list with of the shape of our data
        dims_index = list(self.shape)

        # Set the dimensions which have been requested to length 1
        for d in dim_to_slice:
            dims_index[d] = 1

        return _SliceIterator(self, dims_index, dim_to_slice, ordered)

    # TODO: This is not used anywhere. Remove.
    @property
    def title(self):
        title = '%s with ' % self.name().replace('_', ' ').capitalize()
        attribute_str_list = []
        for coord in self.coords():
            if coord.shape == (1,):
                cell = coord.cell(0)
                if coord.has_points():
                    attribute_str_list.append(
                        '%s: %s' % (coord.name(), cell.point))
                elif coord.has_bounds():
                    attribute_str_list.append('%s: between %s & %s' % (
                        coord.name(), cell.bound[0], cell.bound[1]))

        current_len = len(title)
        for i, line in enumerate(attribute_str_list):
            if (current_len + len(line)) > 90:
                attribute_str_list[i] = '\n' + line
                current_len = len(line)
            else:
                current_len += len(line)

        title = title + ', '.join(attribute_str_list)
        return title

    def transpose(self, new_order=None):
        """
        Re-order the data dimensions of the cube in-place.

        new_order - list of ints, optional
                    By default, reverse the dimensions, otherwise permute the
                    axes according to the values given.

        .. note:: If defined, new_order must span all of the data dimensions.

        Example usage::

            # put the second dimension first, followed by the third dimension,
            and finally put the first dimension third cube.transpose([1, 2, 0])

        """
        if new_order is None:
            new_order = np.arange(self.data.ndim)[::-1]
        elif len(new_order) != self.data.ndim:
            raise ValueError('Incorrect number of dimensions.')

        # The data needs to be copied, otherwise this view of the transposed
        # data will not be contiguous. Ensure not to assign via the cube.data
        # setter property since we are reshaping the cube payload in-place.
        self._my_data = np.transpose(self.data, new_order).copy()

        dim_mapping = {src: dest for dest, src in enumerate(new_order)}

        def remap_dim_coord(coord_and_dim):
            coord, dim = coord_and_dim
            return coord, dim_mapping[dim]
        self._dim_coords_and_dims = map(remap_dim_coord,
                                        self._dim_coords_and_dims)

        def remap_aux_coord(coord_and_dims):
            coord, dims = coord_and_dims
            return coord, tuple(dim_mapping[dim] for dim in dims)
        self._aux_coords_and_dims = map(remap_aux_coord,
                                        self._aux_coords_and_dims)

    def xml(self, checksum=False, order=True, byteorder=True):
        """
        Returns a fully valid CubeML string representation of the Cube.

        """
        doc = Document()

        cube_xml_element = self._xml_element(doc, checksum=checksum,
                                             order=order,
                                             byteorder=byteorder)
        cube_xml_element.setAttribute("xmlns", XML_NAMESPACE_URI)
        doc.appendChild(cube_xml_element)

        # Print our newly created XML
        return doc.toprettyxml(indent="  ")

    def _xml_element(self, doc, checksum=False, order=True, byteorder=True):
        cube_xml_element = doc.createElement("cube")

        if self.standard_name:
            cube_xml_element.setAttribute('standard_name', self.standard_name)
        if self.long_name:
            cube_xml_element.setAttribute('long_name', self.long_name)
        if self.var_name:
            cube_xml_element.setAttribute('var_name', self.var_name)
        cube_xml_element.setAttribute('units', str(self.units))

        if self.attributes:
            attributes_element = doc.createElement('attributes')
            for name in sorted(self.attributes.iterkeys()):
                attribute_element = doc.createElement('attribute')
                attribute_element.setAttribute('name', name)
                value = str(self.attributes[name])
                attribute_element.setAttribute('value', value)
                attributes_element.appendChild(attribute_element)
            cube_xml_element.appendChild(attributes_element)

        coords_xml_element = doc.createElement("coords")
        for coord in sorted(self.coords(), key=lambda coord: coord.name()):
            # make a "cube coordinate" element which holds the dimensions (if
            # appropriate) which itself will have a sub-element of the
            # coordinate instance itself.
            cube_coord_xml_element = doc.createElement("coord")
            coords_xml_element.appendChild(cube_coord_xml_element)

            dims = list(self.coord_dims(coord))
            if dims:
                cube_coord_xml_element.setAttribute("datadims", repr(dims))

            coord_xml_element = coord.xml_element(doc)
            cube_coord_xml_element.appendChild(coord_xml_element)
        cube_xml_element.appendChild(coords_xml_element)

        # cell methods (no sorting!)
        cell_methods_xml_element = doc.createElement("cellMethods")
        for cm in self.cell_methods:
            cell_method_xml_element = cm.xml_element(doc)
            cell_methods_xml_element.appendChild(cell_method_xml_element)
        cube_xml_element.appendChild(cell_methods_xml_element)

        data_xml_element = doc.createElement("data")

        data_xml_element.setAttribute("shape", str(self.shape))

        # NB. Getting a checksum triggers any deferred loading,
        # in which case it also has the side-effect of forcing the
        # byte order to be native.
        if checksum:
            data = self.data

            # Ensure consistent memory layout for checksums.
            def normalise(data):
                data = np.ascontiguousarray(data)
                if data.dtype.newbyteorder('<') != data.dtype:
                    data = data.byteswap(False)
                    data.dtype = data.dtype.newbyteorder('<')
                return data

            if isinstance(data, ma.MaskedArray):
                # Fill in masked values to avoid the checksum being
                # sensitive to unused numbers. Use a fixed value so
                # a change in fill_value doesn't affect the
                # checksum.
                crc = hex(zlib.crc32(normalise(data.filled(0))))
                data_xml_element.setAttribute("checksum", crc)
                if ma.is_masked(data):
                    crc = hex(zlib.crc32(normalise(data.mask)))
                else:
                    crc = 'no-masked-elements'
                data_xml_element.setAttribute("mask_checksum", crc)
                data_xml_element.setAttribute('fill_value',
                                              str(data.fill_value))
            else:
                crc = hex(zlib.crc32(normalise(data)))
                data_xml_element.setAttribute("checksum", crc)
        elif self.has_lazy_data():
            data_xml_element.setAttribute("state", "deferred")
        else:
            data_xml_element.setAttribute("state", "loaded")

        # Add the dtype, and also the array and mask orders if the
        # data is loaded.
        if not self.has_lazy_data():
            data = self.data
            dtype = data.dtype

            def _order(array):
                order = ''
                if array.flags['C_CONTIGUOUS']:
                    order = 'C'
                elif array.flags['F_CONTIGUOUS']:
                    order = 'F'
                return order
            if order:
                data_xml_element.setAttribute('order', _order(data))

            # NB. dtype.byteorder can return '=', which is bad for
            # cross-platform consistency - so we use dtype.str
            # instead.
            if byteorder:
                array_byteorder = {'>': 'big', '<': 'little'}.get(dtype.str[0])
                if array_byteorder is not None:
                    data_xml_element.setAttribute('byteorder', array_byteorder)

            if order and isinstance(data, ma.core.MaskedArray):
                data_xml_element.setAttribute('mask_order',
                                              _order(data.mask))
        else:
            dtype = self.lazy_data().dtype
        data_xml_element.setAttribute('dtype', dtype.name)

        cube_xml_element.appendChild(data_xml_element)

        return cube_xml_element

    def copy(self, data=None):
        """
        Returns a deep copy of this cube.

        Kwargs:

        * data:
            Replace the data of the cube copy with provided data payload.

        Returns:
            A copy instance of the :class:`Cube`.

        """
        return self._deepcopy({}, data)

    def __copy__(self):
        """Shallow copying is disallowed for Cubes."""
        raise copy.Error("Cube shallow-copy not allowed. Use deepcopy() or "
                         "Cube.copy()")

    def __deepcopy__(self, memo):
        return self._deepcopy(memo)

    def _deepcopy(self, memo, data=None):
        if data is None:
            if not self.has_lazy_data() and self.ndim == 0:
                # Cope with NumPy's asymmetric (aka. "annoying!") behaviour
                # of deepcopy on 0-d arrays.
                new_cube_data = np.asanyarray(self.data)
            else:
                new_cube_data = copy.copy(self._my_data)
        else:
            if not isinstance(data, biggus.Array):
                data = np.asanyarray(data)

            if data.shape != self.shape:
                msg = 'Cannot copy cube with new data of a different shape ' \
                      '(slice or subset the cube first).'
                raise ValueError(msg)

            new_cube_data = data

        new_dim_coords_and_dims = copy.deepcopy(self._dim_coords_and_dims,
                                                memo)
        new_aux_coords_and_dims = copy.deepcopy(self._aux_coords_and_dims,
                                                memo)

        # Record a mapping from old coordinate IDs to new coordinates,
        # for subsequent use in creating updated aux_factories.
        coord_mapping = {}
        for old_pair, new_pair in zip(self._dim_coords_and_dims,
                                      new_dim_coords_and_dims):
            coord_mapping[id(old_pair[0])] = new_pair[0]
        for old_pair, new_pair in zip(self._aux_coords_and_dims,
                                      new_aux_coords_and_dims):
            coord_mapping[id(old_pair[0])] = new_pair[0]

        new_cube = Cube(new_cube_data,
                        dim_coords_and_dims=new_dim_coords_and_dims,
                        aux_coords_and_dims=new_aux_coords_and_dims)
        new_cube.metadata = copy.deepcopy(self.metadata, memo)

        for factory in self.aux_factories:
            new_cube.add_aux_factory(factory.updated(coord_mapping))

        return new_cube

    # START OPERATOR OVERLOADS
    def __eq__(self, other):
        result = NotImplemented

        if isinstance(other, Cube):
            result = self.metadata == other.metadata

            # having checked the metadata, now check the coordinates
            if result:
                coord_comparison = iris.analysis.coord_comparison(self, other)
                # if there are any coordinates which are not equal
                result = not (coord_comparison['not_equal'] or
                              coord_comparison['non_equal_data_dimension'])

            # having checked everything else, check approximate data
            # equality - loading the data if has not already been loaded.
            if result:
                result = np.all(np.abs(self.data - other.data) < 1e-8)

        return result

    # Must supply __ne__, Python does not defer to __eq__ for negative equality
    def __ne__(self, other):
        result = self.__eq__(other)
        if result is not NotImplemented:
            result = not result
        return result

    def __add__(self, other):
        return iris.analysis.maths.add(self, other, ignore=True)
    __radd__ = __add__

    def __sub__(self, other):
        return iris.analysis.maths.subtract(self, other, ignore=True)

    __mul__ = iris.analysis.maths.multiply
    __rmul__ = iris.analysis.maths.multiply
    __div__ = iris.analysis.maths.divide
    __truediv__ = iris.analysis.maths.divide
    __pow__ = iris.analysis.maths.exponentiate
    # END OPERATOR OVERLOADS

    def add_history(self, string):
        """
        Add the given string to the cube's history.
        If the history coordinate does not exist, then one will be created.

        .. deprecated:: 1.6
            Add/modify history metadata within
            attr:`~iris.cube.Cube.attributes` as needed.

        """
        warnings.warn("Cube.add_history() has been deprecated - "
                      "please modify/create cube.attributes['history'] "
                      "as needed.")

        timestamp = datetime.datetime.now().strftime("%d/%m/%y %H:%M:%S")
        string = '%s Iris: %s' % (timestamp, string)

        try:
            history = self.attributes['history']
            self.attributes['history'] = '%s\n%s' % (history, string)
        except KeyError:
            self.attributes['history'] = string

    # START ANALYSIS ROUTINES

    regridded = iris.util._wrap_function_for_method(
        iris.analysis.interpolate.regrid,
        """
        Returns a new cube with values derived from this cube on the
        horizontal grid specified by the grid_cube.

        """)

    # END ANALYSIS ROUTINES

    def collapsed(self, coords, aggregator, lazy=False, **kwargs):
        """
        Collapse one or more dimensions over the cube given the coordinate/s
        and an aggregation.

        Examples of aggregations that may be used include
        :data:`~iris.analysis.COUNT` and :data:`~iris.analysis.MAX`.

        Weighted aggregations (:class:`iris.analysis.WeightedAggregator`) may
        also be supplied. These include :data:`~iris.analysis.MEAN` and
        sum :data:`~iris.analysis.SUM`.

        Weighted aggregations support an optional *weights* keyword argument.
        If set, this should be supplied as an array of weights whose shape
        matches the cube. Values for latitude-longitude area weights may be
        calculated using :func:`iris.analysis.cartography.area_weights`.

        Args:

        * coords (string, coord or a list of strings/coords):
            Coordinate names/coordinates over which the cube should be
            collapsed.

        * aggregator (:class:`iris.analysis.Aggregator`):
            Aggregator to be applied for collapse operation.

        Kwargs:

        * lazy (bool):
            When set, the operation expects and will return a cube with a lazy
            data array.  This is only supported for certain operations using
            certain types of aggregator -- see documentation of
            :class:`iris.analysis.Aggregator`.

            .. warning::

                This keyword may in future be replaced by a different method of
                controlling the lazy/concrete operations.

        * kwargs:
            Aggregation function keyword arguments.

        Returns:
            Collapsed cube.

        For example:

            >>> import iris
            >>> import iris.analysis
            >>> path = iris.sample_data_path('ostia_monthly.nc')
            >>> cube = iris.load_cube(path)
            >>> new_cube = cube.collapsed('longitude', iris.analysis.MEAN)
            >>> print new_cube
            surface_temperature / (K)           (time: 54; latitude: 18)
                 Dimension coordinates:
                      time                           x             -
                      latitude                       -             x
                 Auxiliary coordinates:
                      forecast_reference_time        x             -
                 Scalar coordinates:
                      forecast_period: 0 hours
                      longitude: 180.0 degrees, bound=(0.0, 360.0) degrees
                 Attributes:
                      Conventions: CF-1.5
                      STASH: m01s00i024
                 Cell methods:
                      mean: month, year
                      mean: longitude


        .. note::

            Some aggregations are not commutative and hence the order of
            processing is important i.e.::

                tmp = cube.collapsed('realization', iris.analysis.VARIANCE)
                result = tmp.collapsed('height', iris.analysis.VARIANCE)

            is not necessarily the same result as::

                tmp = cube.collapsed('height', iris.analysis.VARIANCE)
                result2 = tmp.collapsed('realization', iris.analysis.VARIANCE)

            Conversely operations which operate on more than one coordinate
            at the same time are commutative as they are combined internally
            into a single operation. Hence the order of the coordinates
            supplied in the list does not matter::

                cube.collapsed(['longitude', 'latitude'],
                               iris.analysis.VARIANCE)

            is the same (apart from the logically equivalent cell methods that
            may be created etc.) as::

                cube.collapsed(['latitude', 'longitude'],
                               iris.analysis.VARIANCE)

        """
        # Convert any coordinate names to coordinates
        coords = self._as_list_of_coords(coords)

        if (isinstance(aggregator, iris.analysis.WeightedAggregator) and
                not aggregator.uses_weighting(**kwargs)):
            msg = "Collapsing spatial coordinate {!r} without weighting"
            lat_match = filter(lambda coord: 'latitude' in coord.name(),
                               coords)
            if lat_match:
                for coord in lat_match:
                    warnings.warn(msg.format(coord.name()))

        # Determine the dimensions we need to collapse (and those we don't)
        if aggregator.cell_method == 'peak':
            dims_to_collapse = [list(self.coord_dims(coord))
                                for coord in coords]

            # Remove duplicate dimensions.
            new_dims = collections.OrderedDict.fromkeys(
                d for dim in dims_to_collapse for d in dim)
            # Reverse the dimensions so the order can be maintained when
            # reshaping the data.
            dims_to_collapse = list(new_dims)[::-1]
        else:
            dims_to_collapse = set()
            for coord in coords:
                dims_to_collapse.update(self.coord_dims(coord))

        if not dims_to_collapse:
            msg = 'Cannot collapse a dimension which does not describe any ' \
                  'data.'
            raise iris.exceptions.CoordinateCollapseError(msg)

        untouched_dims = set(range(self.ndim)) - set(dims_to_collapse)

        # Remove the collapsed dimension(s) from the metadata
        indices = [slice(None, None)] * self.ndim
        for dim in dims_to_collapse:
            indices[dim] = 0
        collapsed_cube = self[tuple(indices)]

        # Collapse any coords that span the dimension(s) being collapsed
        for coord in self.dim_coords + self.aux_coords:
            coord_dims = self.coord_dims(coord)
            if set(dims_to_collapse).intersection(coord_dims):
                local_dims = [coord_dims.index(dim) for dim in
                              dims_to_collapse if dim in coord_dims]
                collapsed_cube.replace_coord(coord.collapsed(local_dims))

        untouched_dims = sorted(untouched_dims)

        # Record the axis(s) argument passed to 'aggregation', so the same is
        # passed to the 'update_metadata' function.
        collapse_axis = -1
        # Perform the actual aggregation.
        if lazy:
            # Use a lazy operation separately defined by the aggregator, based
            # on the cube lazy array.
            # NOTE: do not reform the data in this case, as 'lazy_aggregate'
            # accepts multiple axes (unlike 'aggregate').
            collapse_axis = dims_to_collapse
            data_result = aggregator.lazy_aggregate(self._my_data,
                                                    collapse_axis,
                                                    **kwargs)
        elif aggregator.cell_method == 'peak':
            # The PEAK aggregator must collapse each coordinate separately.
            untouched_shape = [self.shape[d] for d in untouched_dims]
            collapsed_shape = [self.shape[d] for d in dims_to_collapse]
            new_shape = untouched_shape + collapsed_shape

            array_dims = untouched_dims + dims_to_collapse
            unrolled_data = np.transpose(
                self.data, array_dims).reshape(new_shape)

            for dim in dims_to_collapse:
                unrolled_data = aggregator.aggregate(unrolled_data,
                                                     axis=-1,
                                                     **kwargs)
            data_result = unrolled_data
        else:
            # Perform the aggregation over the cube data
            # First reshape the data so that the dimensions being aggregated
            # over are grouped 'at the end' (i.e. axis=-1).
            dims_to_collapse = sorted(dims_to_collapse)

            end_size = reduce(operator.mul, (self.shape[dim] for dim in
                                             dims_to_collapse))
            untouched_shape = [self.shape[dim] for dim in untouched_dims]
            new_shape = untouched_shape + [end_size]
            dims = untouched_dims + dims_to_collapse
            unrolled_data = np.transpose(self.data, dims).reshape(new_shape)

            # Perform the same operation on the weights if applicable
            if kwargs.get("weights") is not None:
                weights = kwargs["weights"].view()
                kwargs["weights"] = np.transpose(weights,
                                                 dims).reshape(new_shape)

            data_result = aggregator.aggregate(unrolled_data,
                                               axis=-1,
                                               **kwargs)

        aggregator.update_metadata(collapsed_cube, coords, axis=collapse_axis,
                                   **kwargs)
        result = aggregator.post_process(collapsed_cube, data_result, **kwargs)
        return result

    def aggregated_by(self, coords, aggregator, **kwargs):
        """
        Perform aggregation over the cube given one or more "group
        coordinates".

        A "group coordinate" is a coordinate where repeating values represent a
        single group, such as a month coordinate on a daily time slice.
        TODO: It is not clear if repeating values must be consecutive to form a
        group.

        The group coordinates must all be over the same cube dimension. Each
        common value group identified over all the group-by coordinates is
        collapsed using the provided aggregator.

        Args:

        * coords (list of coord names or :class:`iris.coords.Coord` instances):
            One or more coordinates over which group aggregation is to be
            performed.
        * aggregator (:class:`iris.analysis.Aggregator`):
            Aggregator to be applied to each group.

        Kwargs:

        * kwargs:
            Aggregator and aggregation function keyword arguments.

        Returns:
            :class:`iris.cube.Cube`.

        .. note::

            This operation does not yet have support for lazy evaluation.

        For example:

            >>> import iris
            >>> import iris.analysis
            >>> import iris.coord_categorisation as cat
            >>> fname = iris.sample_data_path('ostia_monthly.nc')
            >>> cube = iris.load_cube(fname, 'surface_temperature')
            >>> cat.add_year(cube, 'time', name='year')
            >>> new_cube = cube.aggregated_by('year', iris.analysis.MEAN)
            >>> print new_cube
            surface_temperature / (K)           \
(time: 5; latitude: 18; longitude: 432)
                 Dimension coordinates:
                      time                      \
     x            -              -
                      latitude                  \
     -            x              -
                      longitude                 \
     -            -              x
                 Auxiliary coordinates:
                      forecast_reference_time   \
     x            -              -
                      year                      \
     x            -              -
                 Scalar coordinates:
                      forecast_period: 0 hours
                 Attributes:
                      Conventions: CF-1.5
                      STASH: m01s00i024
                 Cell methods:
                      mean: month, year
                      mean: year

        """
        groupby_coords = []
        dimension_to_groupby = None

        # We can't handle weights
        if isinstance(aggregator, iris.analysis.WeightedAggregator) and \
                aggregator.uses_weighting(**kwargs):
            raise ValueError('Invalid Aggregation, aggergated_by() cannot use'
                             ' weights.')

        for coord in sorted(self._as_list_of_coords(coords),
                            key=lambda coord: coord._as_defn()):
            if coord.ndim > 1:
                msg = 'Cannot aggregate_by coord %s as it is ' \
                      'multidimensional.' % coord.name()
                raise iris.exceptions.CoordinateMultiDimError(msg)
            dimension = self.coord_dims(coord)
            if not dimension:
                msg = 'Cannot group-by the coordinate "%s", as its ' \
                      'dimension does not describe any data.' % coord.name()
                raise iris.exceptions.CoordinateCollapseError(msg)
            if dimension_to_groupby is None:
                dimension_to_groupby = dimension[0]
            if dimension_to_groupby != dimension[0]:
                msg = 'Cannot group-by coordinates over different dimensions.'
                raise iris.exceptions.CoordinateCollapseError(msg)
            groupby_coords.append(coord)

        # Determine the other coordinates that share the same group-by
        # coordinate dimension.
        shared_coords = filter(lambda coord_: coord_ not in groupby_coords,
                               self.coords(dimensions=dimension_to_groupby))

        # Create the aggregation group-by instance.
        groupby = iris.analysis._Groupby(groupby_coords, shared_coords)

        # Create the resulting aggregate-by cube and remove the original
        # coordinates which are going to be groupedby.
        key = [slice(None, None)] * self.ndim
        # Generate unique index tuple key to maintain monotonicity.
        key[dimension_to_groupby] = tuple(range(len(groupby)))
        key = tuple(key)
        aggregateby_cube = self[key]
        for coord in groupby_coords + shared_coords:
            aggregateby_cube.remove_coord(coord)

        # Determine the group-by cube data shape.
        data_shape = list(self.shape)
        data_shape[dimension_to_groupby] = len(groupby)

        # Aggregate the group-by data.
        cube_slice = [slice(None, None)] * len(data_shape)

        for i, groupby_slice in enumerate(groupby.group()):
            # Slice the cube with the group-by slice to create a group-by
            # sub-cube.
            cube_slice[dimension_to_groupby] = groupby_slice
            groupby_sub_cube = self[tuple(cube_slice)]
            # Perform the aggregation over the group-by sub-cube and
            # repatriate the aggregated data into the aggregate-by cube data.
            cube_slice[dimension_to_groupby] = i
            result = aggregator.aggregate(groupby_sub_cube.data,
                                          axis=dimension_to_groupby,
                                          **kwargs)

            # Determine aggregation result data type for the aggregate-by cube
            # data on first pass.
            if i == 0:
                if isinstance(self.data, ma.MaskedArray):
                    aggregateby_data = ma.zeros(data_shape, dtype=result.dtype)
                else:
                    aggregateby_data = np.zeros(data_shape, dtype=result.dtype)

            aggregateby_data[tuple(cube_slice)] = result

        # Add the aggregation meta data to the aggregate-by cube.
        aggregator.update_metadata(aggregateby_cube,
                                   groupby_coords,
                                   aggregate=True, **kwargs)
        # Replace the appropriate coordinates within the aggregate-by cube.
        dim_coord, = self.coords(dimensions=dimension_to_groupby,
                                 dim_coords=True) or [None]
        for coord in groupby.coords:
            if dim_coord is not None and \
                    dim_coord._as_defn() == coord._as_defn() and \
                    isinstance(coord, iris.coords.DimCoord):
                aggregateby_cube.add_dim_coord(coord.copy(),
                                               dimension_to_groupby)
            else:
                aggregateby_cube.add_aux_coord(coord.copy(),
                                               dimension_to_groupby)
        # Attatch the aggregate-by data into the aggregate-by cube.
        aggregateby_cube.data = aggregateby_data

        return aggregateby_cube

    def rolling_window(self, coord, aggregator, window, **kwargs):
        """
        Perform rolling window aggregation on a cube given a coordinate, an
        aggregation method and a window size.

        Args:

        * coord (string/:class:`iris.coords.Coord`):
            The coordinate over which to perform the rolling window
            aggregation.
        * aggregator (:class:`iris.analysis.Aggregator`):
            Aggregator to be applied to the data.
        * window (int):
            Size of window to use.

        Kwargs:

        * kwargs:
            Aggregator and aggregation function keyword arguments. The weights
            argument to the aggregator, if any, should be a 1d array with the
            same length as the chosen window.

        Returns:
            :class:`iris.cube.Cube`.

        .. note::

            This operation does not yet have support for lazy evaluation.

        For example:

            >>> import iris, iris.analysis
            >>> fname = iris.sample_data_path('GloSea4', 'ensemble_010.pp')
            >>> air_press = iris.load_cube(fname, 'surface_temperature')
            >>> print air_press
            surface_temperature / (K)           \
(time: 6; latitude: 145; longitude: 192)
                 Dimension coordinates:
                      time                      \
     x            -               -
                      latitude                  \
     -            x               -
                      longitude                 \
     -            -               x
                 Auxiliary coordinates:
                      forecast_period           \
     x            -               -
                 Scalar coordinates:
                      forecast_reference_time: 2011-07-23 00:00:00
                      realization: 10
                 Attributes:
                      STASH: m01s00i024
                      source: Data from Met Office Unified Model 7.06
                 Cell methods:
                      mean: time (1 hour)


            >>> print air_press.rolling_window('time', iris.analysis.MEAN, 3)
            surface_temperature / (K)           \
(time: 4; latitude: 145; longitude: 192)
                 Dimension coordinates:
                      time                      \
     x            -               -
                      latitude                  \
     -            x               -
                      longitude                 \
     -            -               x
                 Auxiliary coordinates:
                      forecast_period           \
     x            -               -
                 Scalar coordinates:
                      forecast_reference_time: 2011-07-23 00:00:00
                      realization: 10
                 Attributes:
                      STASH: m01s00i024
                      source: Data from Met Office Unified Model 7.06
                 Cell methods:
                      mean: time (1 hour)
                      mean: time


            Notice that the forecast_period dimension now represents the 4
            possible windows of size 3 from the original cube.

        """
        coord = self._as_list_of_coords(coord)[0]

        if getattr(coord, 'circular', False):
            raise iris.exceptions.NotYetImplementedError(
                'Rolling window over a circular coordinate.')

        if window < 2:
            raise ValueError('Cannot perform rolling window '
                             'with a window size less than 2.')

        if coord.ndim > 1:
            raise iris.exceptions.CoordinateMultiDimError(coord)

        dimension = self.coord_dims(coord)
        if len(dimension) != 1:
            raise iris.exceptions.CoordinateCollapseError(
                'Cannot perform rolling window with coordinate "%s", '
                'must map to one data dimension.' % coord.name())
        dimension = dimension[0]

        # Use indexing to get a result-cube of the correct shape.
        # NB. This indexes the data array which is wasted work.
        # As index-to-get-shape-then-fiddle is a common pattern, perhaps
        # some sort of `cube.prepare()` method would be handy to allow
        # re-shaping with given data, and returning a mapping of
        # old-to-new-coords (to avoid having to use metadata identity)?
        key = [slice(None, None)] * self.ndim
        key[dimension] = slice(None, self.shape[dimension] - window + 1)
        new_cube = self[tuple(key)]

        # take a view of the original data using the rolling_window function
        # this will add an extra dimension to the data at dimension + 1 which
        # represents the rolled window (i.e. will have a length of window)
        rolling_window_data = iris.util.rolling_window(self.data,
                                                       window=window,
                                                       axis=dimension)

        # now update all of the coordinates to reflect the aggregation
        for coord_ in self.coords(dimensions=dimension):
            if coord_.has_bounds():
                warnings.warn('The bounds of coordinate %r were ignored in '
                              'the rolling window operation.' % coord_.name())

            if coord_.ndim != 1:
                raise ValueError('Cannot calculate the rolling '
                                 'window of %s as it is a multidimensional '
                                 'coordinate.' % coord_.name())

            new_bounds = iris.util.rolling_window(coord_.points, window)

            if np.issubdtype(new_bounds.dtype, np.str):
                # Handle case where the AuxCoord contains string. The points
                # are the serialized form of the points contributing to each
                # window and the bounds are the first and last points in the
                # window as with numeric coordinates.
                new_points = np.apply_along_axis(lambda x: '|'.join(x), -1,
                                                 new_bounds)
                new_bounds = new_bounds[:, (0, -1)]
            else:
                # Take the first and last element of the rolled window (i.e.
                # the bounds) and the new points are the midpoints of these
                # bounds.
                new_bounds = new_bounds[:, (0, -1)]
                new_points = np.mean(new_bounds, axis=-1)

            # wipe the coords points and set the bounds
            new_coord = new_cube.coord(coord_)
            new_coord.points = new_points
            new_coord.bounds = new_bounds

        # update the metadata of the cube itself
        aggregator.update_metadata(
            new_cube, [coord],
            action='with a rolling window of length %s over' % window,
            **kwargs)
        # and perform the data transformation, generating weights first if
        # needed
        newkwargs = {}
        if isinstance(aggregator, iris.analysis.WeightedAggregator) and \
                aggregator.uses_weighting(**kwargs):
            if 'weights' in kwargs.keys():
                weights = kwargs['weights']
                if weights.ndim > 1 or weights.shape[0] != window:
                    raise ValueError('Weights for rolling window aggregation '
                                     'must be a 1d array with the same length '
                                     'as the window.')
                newkwargs['weights'] = iris.util.broadcast_to_shape(
                    weights, rolling_window_data.shape, (dimension + 1,))
        new_cube.data = aggregator.aggregate(rolling_window_data,
                                             axis=dimension + 1,
                                             **newkwargs)

        return new_cube

    def interpolate(self, scheme, sample_points, collapse_scalar=True):
        """
        Interpolate over the :class:`~iris.cube.Cube` using the provided
        interpolation scheme and specified sample points.

        Args:

        * scheme:
            A :class:`~iris.analysis.Linear` instance, which defines the
            interpolator scheme.
        * sample_points:
            A sequence of (coordinate, points) pairs over which to interpolate.

        Kwargs:

        * collapse_scalar:
            Whether to collapse the dimension of the scalar sample points
            in the resulting cube. Default is True.

        Returns:
            A cube interpolated at the given sample points. The dimensionality
            of the cube will be the number of original cube dimensions minus
            the number of scalar coordinates, if collapse_scalar is True.

        """
        coords, points = zip(*sample_points)
        interp = scheme.interpolator(self, coords)
        return interp(points, collapse_scalar=collapse_scalar)


class ClassDict(object, UserDict.DictMixin):
    """
    A mapping that stores objects keyed on their superclasses and their names.

    The mapping has a root class, all stored objects must be a subclass of the
    root class. The superclasses used for an object include the class of the
    object, but do not include the root class. Only one object is allowed for
    any key.

    """
    def __init__(self, superclass):
        if not isinstance(superclass, type):
            raise TypeError("The superclass must be a Python type or new "
                            "style class.")
        self._superclass = superclass
        self._basic_map = {}
        self._retrieval_map = {}

    def add(self, object_, replace=False):
        '''Add an object to the dictionary.'''
        if not isinstance(object_, self._superclass):
            msg = "Only subclasses of {!r} are allowed as values.".format(
                self._superclass.__name__)
            raise TypeError(msg)
        # Find all the superclasses of the given object, starting with the
        # object's class.
        superclasses = type.mro(type(object_))
        if not replace:
            # Ensure nothing else is already registered against those
            # superclasses.
            # NB. This implies the _basic_map will also be empty for this
            # object.
            for key_class in superclasses:
                if key_class in self._retrieval_map:
                    msg = "Cannot add instance of '%s' because instance of " \
                          "'%s' already added." % (type(object_).__name__,
                                                   key_class.__name__)
                    raise ValueError(msg)
        # Register the given object against those superclasses.
        for key_class in superclasses:
            self._retrieval_map[key_class] = object_
            self._retrieval_map[key_class.__name__] = object_
        self._basic_map[type(object_)] = object_

    def __getitem__(self, class_):
        try:
            return self._retrieval_map[class_]
        except KeyError:
            raise KeyError('Coordinate system %r does not exist.' % class_)

    def __delitem__(self, class_):
        cs = self[class_]
        keys = [k for k, v in self._retrieval_map.iteritems() if v == cs]
        for key in keys:
            del self._retrieval_map[key]
        del self._basic_map[type(cs)]
        return cs

    def keys(self):
        '''Return the keys of the dictionary mapping.'''
        return self._basic_map.keys()


def sorted_axes(axes):
    """
    Returns the axis names sorted alphabetically, with the exception that
    't', 'z', 'y', and, 'x' are sorted to the end.

    """
    return sorted(axes, key=lambda name: ({'x': 4,
                                           'y': 3,
                                           'z': 2,
                                           't': 1}.get(name, 0), name))


# See Cube.slice() for the definition/context.
class _SliceIterator(collections.Iterator):
    def __init__(self, cube, dims_index, requested_dims, ordered):
        self._cube = cube

        # Let Numpy do some work in providing all of the permutations of our
        # data shape. This functionality is something like:
        # ndindex(2, 1, 3) -> [(0, 0, 0), (0, 0, 1), (0, 0, 2),
        #                      (1, 0, 0), (1, 0, 1), (1, 0, 2)]
        self._ndindex = np.ndindex(*dims_index)

        self._requested_dims = requested_dims
        # indexing relating to sliced cube
        self._mod_requested_dims = np.argsort(requested_dims)
        self._ordered = ordered

    def next(self):
        # NB. When self._ndindex runs out it will raise StopIteration for us.
        index_tuple = self._ndindex.next()

        # Turn the given tuple into a list so that we can do something with it
        index_list = list(index_tuple)

        # For each of the spanning dimensions requested, replace the 0 with a
        # spanning slice
        for d in self._requested_dims:
            index_list[d] = slice(None, None)

        # Request the slice
        cube = self._cube[tuple(index_list)]

        if self._ordered:
            if any(self._mod_requested_dims != range(len(cube.shape))):
                cube.transpose(self._mod_requested_dims)

        return cube

########NEW FILE########
__FILENAME__ = exceptions
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Exceptions specific to the Iris package.

"""
import iris.coords


class IrisError(Exception):
    """Base class for errors in the Iris package."""
    pass


class CoordinateCollapseError(IrisError):
    """Raised when a requested coordinate cannot be collapsed."""
    pass


class CoordinateNotFoundError(KeyError):
    """Raised when a search yields no coordinates."""
    pass


class CoordinateMultiDimError(ValueError):
    """Raised when a routine doesn't support multi-dimensional coordinates."""
    def __init__(self, msg):
        if isinstance(msg, iris.coords.Coord):
            fmt = "Multi-dimensional coordinate not supported: '%s'"
            msg = fmt % msg.name()
        ValueError.__init__(self, msg)


class CoordinateNotRegularError(ValueError):
    """Raised when a coordinate is unexpectedly irregular."""
    pass


class InvalidCubeError(IrisError):
    """Raised when a Cube validation check fails."""
    pass


class ConstraintMismatchError(IrisError):
    """
    Raised when a constraint operation has failed to find the correct number
    of results.

    """
    pass


class NotYetImplementedError(IrisError):
    """
    Raised by missing functionality.

    Different meaning to NotImplementedError, which is for abstract methods.

    """
    pass


class TranslationError(IrisError):
    """Raised when Iris is unable to translate format-specific codes."""
    pass


class IgnoreCubeException(IrisError):
    """
    Raised from a callback function when a cube should be ignored on load.

    """
    pass


class MergeError(IrisError):
    """
    Raised when merge is expected to produce a single cube, but fails to
    do so.

    """
    def __init__(self, differences):
        """
        Creates a MergeError with a list of textual descriptions of
        the differences which prevented a merge.

        Args:

        * differences:
            The list of strings which describe the differences.

        """
        self.differences = differences

    def __str__(self):
        return '\n  '.join(['failed to merge into a single cube.'] +
                           list(self.differences))


class DuplicateDataError(MergeError):
    """Raised when merging two or more cubes that have identical metadata."""
    def __init__(self, msg):
        self.differences = [msg]


class LazyAggregatorError(Exception):
    pass

########NEW FILE########
__FILENAME__ = animate
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Wrapper for animating iris cubes using iris or matplotlib plotting functions

"""

import warnings

import matplotlib.pyplot as plt
import matplotlib.animation as animation

import iris


def animate(cube_iterator, plot_func, fig=None, **kwargs):
    """
    Animates the given cube iterator.

    Args:

    * cube_iterator (iterable of :class:`iris.cube.Cube` objects):
        Each animation frame corresponds to each :class:`iris.cube.Cube`
        object. See :meth:`iris.cube.Cube.slices`.

    * plot_func (:mod:`~iris.plot` or :mod:`~iris.quickplot` plot):
        Plotting function used to animate.

    Kwargs:

    * fig (:class:`matplotlib.figure.Figure` instance):
        By default, the current figure will be used or a new figure instance
        created if no figure is available. See :func:`matplotlib.pyplot.gcf`.

    * coords (list of :class:`~iris.coords.Coord` objects or coordinate names):
        Use the given coordinates as the axes for the plot. The order of the
        given coordinates indicates which axis to use for each, where the first
        element is the horizontal axis of the plot and the second element is
        the vertical axis of the plot.

    * interval (int, float or long):
        Defines the time interval in milliseconds between successive frames.
        A default interval of 100ms is set.

    * vmin, vmax (int, float or long):
        Color scaling values, see :class:`matplotlib.colors.Normalize` for
        further details. Default values are determined by the min-max across
        the data set over the entire sequence.

    See :class:`matplotlib.animation.FuncAnimation` for details of other valid
    keyword arguments.

    Returns:
        :class:`~matplotlib.animation.FuncAnimation` object suitable for
        saving and or plotting.

    For example, to animate along a set of cube slices::

        cube_iter = cubes.slices(('grid_longitude', 'grid_latitude'))
        ani = animate(cube_iter, qplt.contourf)
        plt.show()

    """
    kwargs.setdefault('interval', 100)
    coords = kwargs.pop('coords', None)

    if fig is None:
        fig = plt.gcf()

    def update_animation_iris(i, cubes, vmin, vmax, coords):
        # Clearing the figure is currently necessary for compatibility with
        # the iris quickploting module - due to the colorbar.
        plt.gcf().clf()
        plot_func(cubes[i], vmin=vmin, vmax=vmax, coords=coords)

    # Turn cube iterator into a list to determine plot ranges.
    # NOTE: we check that we are not providing a cube as this has a deprecated
    # iter special method.
    if (hasattr(cube_iterator, '__iter__') and not
            isinstance(cube_iterator, iris.cube.Cube)):
        cubes = iris.cube.CubeList(cube_iterator)
    else:
        msg = 'iterable type object required for animation, {} given'.format(
            type(cube_iterator))
        raise TypeError(msg)

    supported = ['iris.plot', 'iris.quickplot']
    if plot_func.__module__ not in supported:
        msg = ('Given plotting module "{}" may not be supported, intended '
               'use: {}.')
        msg = msg.format(plot_func.__module__, supported)
        warnings.warn(msg, UserWarning)

    supported = ['contour', 'contourf', 'pcolor', 'pcolormesh']
    if plot_func.__name__ not in supported:
        msg = ('Given plotting function "{}" may not be supported, intended '
               'use: {}.')
        msg = msg.format(plot_func.__name__, supported)
        warnings.warn(msg, UserWarning)

    # Determine plot range.
    vmin = kwargs.pop('vmin', min([cc.data.min() for cc in cubes]))
    vmax = kwargs.pop('vmax', max([cc.data.max() for cc in cubes]))

    update = update_animation_iris
    frames = xrange(len(cubes))

    return animation.FuncAnimation(fig, update,
                                   frames=frames,
                                   fargs=(cubes, vmin, vmax, coords),
                                   **kwargs)

########NEW FILE########
__FILENAME__ = concatenate
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Automatic concatenation of multiple cubes over one or more existing dimensions.

.. warning::

    This functionality has now been moved to
    :meth:`iris.cube.CubeList.concatenate`.

"""


def concatenate(cubes):
    """
    Concatenate the provided cubes over common existing dimensions.

    .. warning::

        This function is now **disabled**.

        The functionality has been moved to
        :meth:`iris.cube.CubeList.concatenate`.

    """
    raise Exception(
        'The function "iris.experimental.concatenate.concatenate" has been '
        'moved, and is now a CubeList instance method.'
        '\nPlease replace '
        '"iris.experimental.concatenate.concatenate(<cubes>)" with '
        '"iris.cube.CubeList(<cubes>).concatenate()".')

########NEW FILE########
__FILENAME__ = equalise_cubes
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Experimental cube-adjusting functions to assist merge operations.

"""

import numpy as np


def equalise_attributes(cubes):
    """
    Delete cube attributes that are not identical over all cubes in a group.

    This function simply deletes any attributes which are not the same for
    all the given cubes.  The cubes will then have identical attributes.  The
    given cubes are modified in-place.

    Args:

    * cubes (iterable of :class:`iris.cube.Cube`):
        A collection of cubes to compare and adjust.

    """
    # Work out which attributes are identical across all the cubes.
    common_keys = cubes[0].attributes.keys()
    for cube in cubes[1:]:
        cube_keys = cube.attributes.keys()
        common_keys = [
            key for key in common_keys
            if key in cube_keys
            and np.all(cube.attributes[key] == cubes[0].attributes[key])]

    # Remove all the other attributes.
    for cube in cubes:
        for key in cube.attributes.keys():
            if key not in common_keys:
                del cube.attributes[key]

########NEW FILE########
__FILENAME__ = raster
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Experimental module for importing/exporting raster data from Iris cubes using
the GDAL library.

See also: `GDAL - Geospatial Data Abstraction Library <http://www.gdal.org>`_.

TODO: If this module graduates from experimental the (optional) GDAL
      dependency should be added to INSTALL

"""
import numpy as np
from osgeo import gdal, osr

import iris
import iris.coord_systems
import iris.unit


_GDAL_DATATYPES = {
    'i2': gdal.GDT_Int16,
    'i4': gdal.GDT_Int32,
    'u1': gdal.GDT_Byte,
    'u2': gdal.GDT_UInt16,
    'u4': gdal.GDT_UInt32,
    'f4': gdal.GDT_Float32,
    'f8': gdal.GDT_Float64,
}


def _gdal_write_array(x_min, x_step, y_max, y_step, coord_system, data, fname,
                      ftype):
    """
    Use GDAL WriteArray to export data as a 32-bit raster image.
    Requires the array data to be of the form: North-at-top
    and West-on-left.

    Args:
        * x_min: Minimum X coordinate bounds value.
        * x_step: Change in X coordinate per cell.
        * y_max: Maximum Y coordinate bounds value.
        * y_step: Change in Y coordinate per cell.
        * coord_system (iris.coord_systems.CoordSystem):
            Coordinate system for X and Y.
        * data (numpy.ndarray): 2d array of values to export
        * fname (string): Output file name.
        * ftype (string): Export file type.

    .. note::

        Projection information is currently not written to the output.

    """
    byte_order = data.dtype.str[0]
    format = data.dtype.str[1:]
    dtype = _GDAL_DATATYPES.get(format)
    if dtype is None:
        raise ValueError('Unsupported data type: {}'.format(data.dtype))

    driver = gdal.GetDriverByName(ftype)
    gdal_dataset = driver.Create(fname, data.shape[1], data.shape[0],
                                 1, dtype)

    # Where possible, set the projection.
    if coord_system is not None:
        srs = osr.SpatialReference()
        proj4_defn = coord_system.as_cartopy_crs().proj4_init
        # GDAL can't cope with "+proj=lonlat" which Cartopy produces.
        proj4_defn = proj4_defn.replace('lonlat', 'longlat')
        if srs.ImportFromProj4(proj4_defn):
            msg = 'Unsupported coordinate system: {}'.format(coord_system)
            raise ValueError(msg)
        gdal_dataset.SetProjection(srs.ExportToWkt())

    # Set the affine transformation coefficients.
    padf_transform = (x_min, x_step, 0.0, y_max, 0.0, y_step)
    gdal_dataset.SetGeoTransform(padf_transform)

    band = gdal_dataset.GetRasterBand(1)
    if isinstance(data, np.ma.core.MaskedArray):
        data = data.copy()
        data[data.mask] = data.fill_value
        band.SetNoDataValue(float(data.fill_value))
    # GeoTIFF always needs little-endian data.
    if byte_order == '>':
        data = data.astype(data.dtype.newbyteorder('<'))
    band.WriteArray(data)


def export_geotiff(cube, fname):
    """
    Writes cube data to raster file format as a PixelIsArea GeoTiff image.

    Args:
        * cube (Cube): The 2D regularly gridded cube slice to be exported.
                       The cube must have regular, contiguous bounds.
        * fname (string): Output file name.

    .. note::

        For more details on GeoTiff specification and PixelIsArea, see:
        http://www.remotesensing.org/geotiff/spec/geotiff2.5.html#2.5.2.2

    """
    if cube.ndim != 2:
        raise ValueError("The cube must be two dimensional.")

    coord_x = cube.coord(axis='X', dim_coords=True)
    coord_y = cube.coord(axis='Y', dim_coords=True)

    if coord_x.bounds is None or coord_y.bounds is None:
        raise ValueError('Coordinates must have bounds, consider using '
                         'guess_bounds()')

    if coord_x is None or coord_y is None or \
       coord_x.coord_system != coord_y.coord_system:
        raise ValueError('The X and Y coordinates must share a CoordSystem.')

    xy_step = []
    for coord in [coord_x, coord_y]:
        name = coord.name()
        if coord.nbounds != 2:
            msg = 'Coordinate {!r} must have two bounds ' \
                'per point.'.format(name)
            raise ValueError(msg)
        if not (coord.units == iris.unit.Unit('degrees') or
                coord.units.is_convertible('meters')):
            raise ValueError('Coordinate {!r} units must be either degrees or '
                             'convertible to meters.'.format(name))
        if not coord.is_contiguous():
            raise ValueError('Coordinate {!r} bounds must be '
                             'contiguous.'.format(name))
        xy_step.append(np.diff(coord.bounds[0]))
        if not np.allclose(np.diff(coord.bounds), xy_step[-1]):
            msg = 'Coordinate {!r} bounds must be regularly ' \
                'spaced.'.format(name)
            raise ValueError(msg)

    if coord_x.points[0] > coord_x.points[-1]:
        raise ValueError('Coordinate {!r} x-points must be monotonically'
                         'increasing.'.format(name))

    data = cube.data

    # Make sure we have a YX data layout.
    if cube.coord_dims(coord_x) == 0:
        data = data.T

    x_step, y_step = xy_step
    if y_step > 0:
        # Flip the data so North is at the top.
        data = data[::-1, :]
        y_step *= -1

    coord_system = coord_x.coord_system
    x_bounds = coord_x.bounds
    if isinstance(coord_system, iris.coord_systems.GeogCS):
        big_indices = np.where(coord_x.points > 180)[0]
        n_big = len(big_indices)
        if n_big:
            data = np.roll(data, n_big, axis=1)
            x_bounds = x_bounds.copy()
            x_bounds[big_indices] -= 360

    x_min = np.min(x_bounds)
    y_max = np.max(coord_y.bounds)
    _gdal_write_array(x_min, x_step, y_max, y_step, coord_system, data, fname,
                      'GTiff')

########NEW FILE########
__FILENAME__ = regrid
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Regridding functions.

"""
import collections
import copy
import warnings

import numpy as np
import numpy.ma as ma
from scipy.sparse import csc_matrix

import iris.analysis.cartography
from iris.analysis._interpolator import _extend_circular_coord_and_data
from iris.analysis._scipy_interpolate import _RegularGridInterpolator
import iris.coord_systems
import iris.cube
import iris.unit


def _get_xy_dim_coords(cube):
    """
    Return the x and y dimension coordinates from a cube.

    This function raises a ValueError if the cube does not contain one and
    only one set of x and y dimension coordinates. It also raises a ValueError
    if the identified x and y coordinates do not have coordinate systems that
    are equal.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.

    Returns:
        A tuple containing the cube's x and y dimension coordinates.

    """
    x_coords = cube.coords(axis='x', dim_coords=True)
    if len(x_coords) != 1:
        raise ValueError('Cube {!r} must contain a single 1D x '
                         'coordinate.'.format(cube.name()))
    x_coord = x_coords[0]

    y_coords = cube.coords(axis='y', dim_coords=True)
    if len(y_coords) != 1:
        raise ValueError('Cube {!r} must contain a single 1D y '
                         'coordinate.'.format(cube.name()))
    y_coord = y_coords[0]

    if x_coord.coord_system != y_coord.coord_system:
        raise ValueError("The cube's x ({!r}) and y ({!r}) "
                         "coordinates must have the same coordinate "
                         "system.".format(x_coord.name(), y_coord.name()))

    return x_coord, y_coord


def _get_xy_coords(cube):
    """
    Return the x and y coordinates from a cube.

    This function will preferentially return a pair of dimension
    coordinates (if there are more than one potential x or y dimension
    coordinates a ValueError will be raised). If the cube does not have
    a pair of x and y dimension coordinates it will return 1D auxiliary
    coordinates (including scalars). If there is not one and only one set
    of x and y auxiliary coordinates a ValueError will be raised.

    Having identified the x and y coordinates, the function checks that they
    have equal coordinate systems and that they do not occupy the same
    dimension on the cube.

    Args:

    * cube:
        An instance of :class:`iris.cube.Cube`.

    Returns:
        A tuple containing the cube's x and y coordinates.

    """
    # Look for a suitable dimension coords first.
    x_coords = cube.coords(axis='x', dim_coords=True)
    if not x_coords:
        # If there is no x coord in dim_coords look for scalars or
        # monotonic coords in aux_coords.
        x_coords = [coord for coord in cube.coords(axis='x', dim_coords=False)
                    if coord.ndim == 1 and coord.is_monotonic()]
    if len(x_coords) != 1:
        raise ValueError('Cube {!r} must contain a single 1D x '
                         'coordinate.'.format(cube.name()))
    x_coord = x_coords[0]

    # Look for a suitable dimension coords first.
    y_coords = cube.coords(axis='y', dim_coords=True)
    if not y_coords:
        # If there is no y coord in dim_coords look for scalars or
        # monotonic coords in aux_coords.
        y_coords = [coord for coord in cube.coords(axis='y', dim_coords=False)
                    if coord.ndim == 1 and coord.is_monotonic()]
    if len(y_coords) != 1:
        raise ValueError('Cube {!r} must contain a single 1D y '
                         'coordinate.'.format(cube.name()))
    y_coord = y_coords[0]

    if x_coord.coord_system != y_coord.coord_system:
        raise ValueError("The cube's x ({!r}) and y ({!r}) "
                         "coordinates must have the same coordinate "
                         "system.".format(x_coord.name(), y_coord.name()))

    # The x and y coordinates must describe different dimensions
    # or be scalar coords.
    x_dims = cube.coord_dims(x_coord)
    x_dim = None
    if x_dims:
        x_dim = x_dims[0]

    y_dims = cube.coord_dims(y_coord)
    y_dim = None
    if y_dims:
        y_dim = y_dims[0]

    if x_dim is not None and y_dim == x_dim:
        raise ValueError("The cube's x and y coords must not describe the "
                         "same data dimension.")

    return x_coord, y_coord


def _sample_grid(src_coord_system, grid_x_coord, grid_y_coord):
    """
    Convert the rectilinear grid coordinates to a curvilinear grid in
    the source coordinate system.

    The `grid_x_coord` and `grid_y_coord` must share a common coordinate
    system.

    Args:

    * src_coord_system:
        The :class:`iris.coord_system.CoordSystem` for the grid of the
        source Cube.
    * grid_x_coord:
        The :class:`iris.coords.DimCoord` for the X coordinate.
    * grid_y_coord:
        The :class:`iris.coords.DimCoord` for the Y coordinate.

    Returns:
        A tuple of the X and Y coordinate values as 2-dimensional
        arrays.

    """
    src_crs = src_coord_system.as_cartopy_crs()
    grid_crs = grid_x_coord.coord_system.as_cartopy_crs()
    grid_x, grid_y = np.meshgrid(grid_x_coord.points, grid_y_coord.points)
    # Skip the CRS transform if we can to avoid precision problems.
    if src_crs == grid_crs:
        sample_grid_x = grid_x
        sample_grid_y = grid_y
    else:
        sample_xyz = src_crs.transform_points(grid_crs, grid_x, grid_y)
        sample_grid_x = sample_xyz[..., 0]
        sample_grid_y = sample_xyz[..., 1]
    return sample_grid_x, sample_grid_y


def _regrid_bilinear_array(src_data, x_dim, y_dim, src_x_coord, src_y_coord,
                           sample_grid_x, sample_grid_y):
    """
    Regrid the given data from the src grid to the sample grid.

    Args:

    * src_data:
        An N-dimensional NumPy array.
    * x_dim:
        The X dimension within `src_data`.
    * y_dim:
        The Y dimension within `src_data`.
    * src_x_coord:
        The X :class:`iris.coords.DimCoord`.
    * src_y_coord:
        The Y :class:`iris.coords.DimCoord`.
    * sample_grid_x:
        A 2-dimensional array of sample X values.
    * sample_grid_y:
        A 2-dimensional array of sample Y values.

    Returns:
        The regridded data as an N-dimensional NumPy array. The lengths
        of the X and Y dimensions will now match those of the sample
        grid.

    """
    if sample_grid_x.shape != sample_grid_y.shape:
        raise ValueError('Inconsistent sample grid shapes.')
    if sample_grid_x.ndim != 2:
        raise ValueError('Sample grid must be 2-dimensional.')

    # Prepare the result data array
    shape = list(src_data.shape)
    assert shape[x_dim] == src_x_coord.shape[0]
    assert shape[y_dim] == src_y_coord.shape[0]

    shape[y_dim] = sample_grid_x.shape[0]
    shape[x_dim] = sample_grid_x.shape[1]

    # If we're given integer values, convert them to the smallest
    # possible float dtype that can accurately preserve the values.
    dtype = src_data.dtype
    if dtype.kind == 'i':
        dtype = np.promote_types(dtype, np.float16)

    if isinstance(src_data, ma.MaskedArray):
        data = ma.empty(shape, dtype=dtype)
        data.mask = np.zeros(data.shape, dtype=np.bool)
    else:
        data = np.empty(shape, dtype=dtype)

    # The interpolation class requires monotonically increasing coordinates,
    # so flip the coordinate(s) and data if the aren't.
    reverse_x = src_x_coord.points[0] > src_x_coord.points[1]
    reverse_y = src_y_coord.points[0] > src_y_coord.points[1]
    flip_index = [slice(None)] * src_data.ndim
    if reverse_x:
        src_x_coord = src_x_coord[::-1]
        flip_index[x_dim] = slice(None, None, -1)
    if reverse_y:
        src_y_coord = src_y_coord[::-1]
        flip_index[y_dim] = slice(None, None, -1)
    src_data = src_data[tuple(flip_index)]

    if src_x_coord.circular:
        x_points, src_data = _extend_circular_coord_and_data(src_x_coord,
                                                             src_data, x_dim)
    else:
        x_points = src_x_coord.points

    # Slice out the first full 2D piece of data for construction of the
    # interpolator.
    index = [0] * src_data.ndim
    index[x_dim] = index[y_dim] = slice(None)
    initial_data = src_data[tuple(index)]
    if y_dim < x_dim:
        initial_data = initial_data.T

    # Construct the interpolator, we will fill in any values out of bounds
    # manually.
    interpolator = _RegularGridInterpolator([x_points,
                                             src_y_coord.points],
                                            initial_data, fill_value=None,
                                            bounds_error=False)

    # Construct the target coordinate points array, suitable for passing to
    # the interpolator multiple times.
    interpolator_coords = [sample_grid_x.astype(np.float64)[..., np.newaxis],
                           sample_grid_y.astype(np.float64)[..., np.newaxis]]

    # Map all the requested values into the range of the source
    # data (centred over the centre of the source data to allow
    # extrapolation where required).
    min_x, max_x = x_points.min(), x_points.max()
    min_y, max_y = src_y_coord.points.min(), src_y_coord.points.max()
    if src_x_coord.units.modulus:
        modulus = src_x_coord.units.modulus
        offset = (max_x + min_x - modulus) * 0.5
        interpolator_coords[0] -= offset
        interpolator_coords[0] = (interpolator_coords[0] % modulus) + offset

    interpolator_coords = np.dstack(interpolator_coords)

    # Figure out which values are out of range, we will use this as a mask
    # once we've computed the interpolated values.
    out_of_range = ((interpolator_coords[..., 0] < min_x) |
                    (interpolator_coords[..., 0] > max_x) |
                    (interpolator_coords[..., 1] < min_y) |
                    (interpolator_coords[..., 1] > max_y))

    def interpolate(data):
        # Update the interpolator for this data slice.
        data = data.astype(interpolator.values.dtype)
        if y_dim < x_dim:
            data = data.T
        interpolator.values = data
        data = interpolator(interpolator_coords)
        data[out_of_range] = np.nan
        if y_dim > x_dim:
            data = data.T
        return data

    # Build up a shape suitable for passing to ndindex, inside the loop we
    # will insert slice(None) on the data indices.
    iter_shape = list(shape)
    iter_shape[x_dim] = iter_shape[y_dim] = 1

    # Iterate through each 2d slice of the data, updating the interpolator
    # with the new data as we go.
    for index in np.ndindex(tuple(iter_shape)):
        index = list(index)
        index[x_dim] = index[y_dim] = slice(None)

        data[tuple(index)] = interpolate(src_data[tuple(index)])

        if isinstance(data, ma.MaskedArray) and src_data.mask is not False:
            new_mask = interpolate(src_data[tuple(index)].mask.astype(float))
            data.mask[tuple(index)] = np.isnan(new_mask) | (new_mask > 0)

    return data


def _regrid_reference_surface(src_surface_coord, surface_dims, x_dim, y_dim,
                              src_x_coord, src_y_coord,
                              sample_grid_x, sample_grid_y, regrid_callback):
    """
    Return a new reference surface coordinate appropriate to the sample
    grid.

    Args:

    * src_surface_coord:
        The :class:`iris.coords.Coord` containing the source reference
        surface.
    * surface_dims:
        The tuple of the data dimensions relevant to the source
        reference surface coordinate.
    * x_dim:
        The X dimension within the source Cube.
    * y_dim:
        The Y dimension within the source Cube.
    * src_x_coord:
        The X :class:`iris.coords.DimCoord`.
    * src_y_coord:
        The Y :class:`iris.coords.DimCoord`.
    * sample_grid_x:
        A 2-dimensional array of sample X values.
    * sample_grid_y:
        A 2-dimensional array of sample Y values.
    * regrid_callback:
        The routine that will be used to calculate the interpolated
        values for the new reference surface.

    Returns:
        The new reference surface coordinate.

    """
    # Determine which of the reference surface's dimensions span the X
    # and Y dimensions of the source cube.
    surface_x_dim = surface_dims.index(x_dim)
    surface_y_dim = surface_dims.index(y_dim)
    surface = regrid_callback(src_surface_coord.points,
                              surface_x_dim, surface_y_dim,
                              src_x_coord, src_y_coord,
                              sample_grid_x, sample_grid_y)
    surface_coord = src_surface_coord.copy(surface)
    return surface_coord


def _create_cube(data, src, x_dim, y_dim, src_x_coord, src_y_coord,
                 grid_x_coord, grid_y_coord, sample_grid_x, sample_grid_y,
                 regrid_callback):
    """
    Return a new Cube for the result of regridding the source Cube onto
    the new grid.

    All the metadata and coordinates of the result Cube are copied from
    the source Cube, with two exceptions:
        - Grid dimension coordinates are copied from the grid Cube.
        - Auxiliary coordinates which span the grid dimensions are
          ignored, except where they provide a reference surface for an
          :class:`iris.aux_factory.AuxCoordFactory`.

    Args:

    * data:
        The regridded data as an N-dimensional NumPy array.
    * src:
        The source Cube.
    * x_dim:
        The X dimension within the source Cube.
    * y_dim:
        The Y dimension within the source Cube.
    * src_x_coord:
        The X :class:`iris.coords.DimCoord`.
    * src_y_coord:
        The Y :class:`iris.coords.DimCoord`.
    * grid_x_coord:
        The :class:`iris.coords.DimCoord` for the new grid's X
        coordinate.
    * grid_y_coord:
        The :class:`iris.coords.DimCoord` for the new grid's Y
        coordinate.
    * sample_grid_x:
        A 2-dimensional array of sample X values.
    * sample_grid_y:
        A 2-dimensional array of sample Y values.
    * regrid_callback:
        The routine that will be used to calculate the interpolated
        values of any reference surfaces.

    Returns:
        The new, regridded Cube.

    """
    # Create a result cube with the appropriate metadata
    result = iris.cube.Cube(data)
    result.metadata = copy.deepcopy(src.metadata)

    # Copy across all the coordinates which don't span the grid.
    # Record a mapping from old coordinate IDs to new coordinates,
    # for subsequent use in creating updated aux_factories.
    coord_mapping = {}

    def copy_coords(src_coords, add_method):
        for coord in src_coords:
            dims = src.coord_dims(coord)
            if coord is src_x_coord:
                coord = grid_x_coord
            elif coord is src_y_coord:
                coord = grid_y_coord
            elif x_dim in dims or y_dim in dims:
                continue
            result_coord = coord.copy()
            add_method(result_coord, dims)
            coord_mapping[id(coord)] = result_coord

    copy_coords(src.dim_coords, result.add_dim_coord)
    copy_coords(src.aux_coords, result.add_aux_coord)

    # Copy across any AuxFactory instances, and regrid their reference
    # surfaces where required.
    for factory in src.aux_factories:
        for coord in factory.dependencies.itervalues():
            if coord is None:
                continue
            dims = src.coord_dims(coord)
            if x_dim in dims and y_dim in dims:
                result_coord = _regrid_reference_surface(
                    coord, dims, x_dim, y_dim, src_x_coord, src_y_coord,
                    sample_grid_x, sample_grid_y, regrid_callback)
                result.add_aux_coord(result_coord, dims)
                coord_mapping[id(coord)] = result_coord
        try:
            result.add_aux_factory(factory.updated(coord_mapping))
        except KeyError:
            msg = 'Cannot update aux_factory {!r} because of dropped' \
                  ' coordinates.'.format(factory.name())
            warnings.warn(msg)
    return result


def regrid_bilinear_rectilinear_src_and_grid(src, grid):
    """
    Return a new Cube that is the result of regridding the source Cube
    onto the grid of the grid Cube using bilinear interpolation.

    Both the source and grid Cubes must be defined on rectilinear grids.

    Auxiliary coordinates which span the grid dimensions are ignored,
    except where they provide a reference surface for an
    :class:`iris.aux_factory.AuxCoordFactory`.

    Args:

    * src:
        The source :class:`iris.cube.Cube` providing the data.
    * grid:
        The :class:`iris.cube.Cube` which defines the new grid.

    Returns:
        The :class:`iris.cube.Cube` resulting from regridding the source
        data onto the grid defined by the grid Cube.

    """
    # Validity checks
    if not isinstance(src, iris.cube.Cube):
        raise TypeError("'src' must be a Cube")
    if not isinstance(grid, iris.cube.Cube):
        raise TypeError("'grid' must be a Cube")
    src_x_coord, src_y_coord = _get_xy_dim_coords(src)
    grid_x_coord, grid_y_coord = _get_xy_dim_coords(grid)
    src_cs = src_x_coord.coord_system
    grid_cs = grid_x_coord.coord_system
    if src_cs is None or grid_cs is None:
        raise ValueError("Both 'src' and 'grid' Cubes must have a"
                         " coordinate system for their rectilinear grid"
                         " coordinates.")

    def _valid_units(coord):
        if isinstance(coord.coord_system, (iris.coord_systems.GeogCS,
                                           iris.coord_systems.RotatedGeogCS)):
            valid_units = 'degrees'
        else:
            valid_units = 'm'
        return coord.units == valid_units
    if not all(_valid_units(coord) for coord in (src_x_coord, src_y_coord,
                                                 grid_x_coord, grid_y_coord)):
        raise ValueError("Unsupported units: must be 'degrees' or 'm'.")

    # Convert the grid to a 2D sample grid in the src CRS.
    sample_grid_x, sample_grid_y = _sample_grid(src_cs,
                                                grid_x_coord, grid_y_coord)

    # Compute the interpolated data values.
    x_dim = src.coord_dims(src_x_coord)[0]
    y_dim = src.coord_dims(src_y_coord)[0]
    src_data = src.data
    fill_value = None
    if np.ma.isMaskedArray(src_data):
        # Since we're using iris.analysis.interpolate.linear which
        # doesn't respect the mask, we replace masked values with NaN.
        fill_value = src_data.fill_value
        src_data = src_data.filled(np.nan)
    data = _regrid_bilinear_array(src_data, x_dim, y_dim,
                                  src_x_coord, src_y_coord,
                                  sample_grid_x, sample_grid_y)
    # Convert NaN based results to masked array where appropriate.
    mask = np.isnan(data)
    if np.any(mask):
        data = np.ma.MaskedArray(data, mask, fill_value=fill_value)

    # Wrap up the data as a Cube.
    result = _create_cube(data, src, x_dim, y_dim, src_x_coord, src_y_coord,
                          grid_x_coord, grid_y_coord,
                          sample_grid_x, sample_grid_y,
                          _regrid_bilinear_array)
    return result


def _within_bounds(bounds, lower, upper):
    """
    Return whether both lower and upper lie within the extremes
    of bounds.

    """
    min_bound = np.min(bounds)
    max_bound = np.max(bounds)

    return (min_bound <= lower <= max_bound) and \
        (min_bound <= upper <= max_bound)


def _cropped_bounds(bounds, lower, upper):
    """
    Return a new bounds array and corresponding slice object (or indices)
    that result from cropping the provided bounds between the specified lower
    and upper values. The bounds at the extremities will be truncated so that
    they start and end with lower and upper.

    This function will return an empty NumPy array and slice if there is no
    overlap between the region covered by bounds and the region from lower to
    upper.

    If lower > upper the resulting bounds may not be contiguous and the
    indices object will be a tuple of indices rather than a slice object.

    Args:

    * bounds:
        An (n, 2) shaped array of monotonic contiguous bounds.
    * lower:
        Lower bound at which to crop the bounds array.
    * upper:
        Upper bound at which to crop the bounds array.

    Returns:
        A tuple of the new bounds array and the corresponding slice object or
        indices from the zeroth axis of the original array.

    """
    reversed_flag = False
    # Ensure order is increasing.
    if bounds[0, 0] > bounds[-1, 0]:
        # Reverse bounds
        bounds = bounds[::-1, ::-1]
        reversed_flag = True

    # Number of bounds.
    n = bounds.shape[0]

    if lower <= upper:
        if lower > bounds[-1, 1] or upper < bounds[0, 0]:
            new_bounds = bounds[0:0]
            indices = slice(0, 0)
        else:
            # A single region lower->upper.
            if lower < bounds[0, 0]:
                # Region extends below bounds so use first lower bound.
                l = 0
                lower = bounds[0, 0]
            else:
                # Index of last lower bound less than or equal to lower.
                l = np.nonzero(bounds[:, 0] <= lower)[0][-1]
            if upper > bounds[-1, 1]:
                # Region extends above bounds so use last upper bound.
                u = n - 1
                upper = bounds[-1, 1]
            else:
                # Index of first upper bound greater than or equal to
                # upper.
                u = np.nonzero(bounds[:, 1] >= upper)[0][0]
            # Extract the bounds in our region defined by lower->upper.
            new_bounds = np.copy(bounds[l:(u + 1), :])
            # Replace first and last values with specified bounds.
            new_bounds[0, 0] = lower
            new_bounds[-1, 1] = upper
            if reversed_flag:
                indices = slice(n - (u + 1), n - l)
            else:
                indices = slice(l, u + 1)
    else:
        # Two regions [0]->upper, lower->[-1]
        # [0]->upper
        if upper < bounds[0, 0]:
            # Region outside src bounds.
            new_bounds_left = bounds[0:0]
            indices_left = tuple()
            slice_left = slice(0, 0)
        else:
            if upper > bounds[-1, 1]:
                # Whole of bounds.
                u = n - 1
                upper = bounds[-1, 1]
            else:
                # Index of first upper bound greater than or equal to upper.
                u = np.nonzero(bounds[:, 1] >= upper)[0][0]
            # Extract the bounds in our region defined by [0]->upper.
            new_bounds_left = np.copy(bounds[0:(u + 1), :])
            # Replace last value with specified bound.
            new_bounds_left[-1, 1] = upper
            if reversed_flag:
                indices_left = tuple(range(n - (u + 1), n))
                slice_left = slice(n - (u + 1), n)
            else:
                indices_left = tuple(range(0, u + 1))
                slice_left = slice(0, u + 1)
        # lower->[-1]
        if lower > bounds[-1, 1]:
            # Region is outside src bounds.
            new_bounds_right = bounds[0:0]
            indices_right = tuple()
            slice_right = slice(0, 0)
        else:
            if lower < bounds[0, 0]:
                # Whole of bounds.
                l = 0
                lower = bounds[0, 0]
            else:
                # Index of last lower bound less than or equal to lower.
                l = np.nonzero(bounds[:, 0] <= lower)[0][-1]
            # Extract the bounds in our region defined by lower->[-1].
            new_bounds_right = np.copy(bounds[l:, :])
            # Replace first value with specified bound.
            new_bounds_right[0, 0] = lower
            if reversed_flag:
                indices_right = tuple(range(0, n - l))
                slice_right = slice(0, n - l)
            else:
                indices_right = tuple(range(l, n))
                slice_right = slice(l, None)

        if reversed_flag:
            # Flip everything around.
            indices_left, indices_right = indices_right, indices_left
            slice_left, slice_right = slice_right, slice_left

        # Combine regions.
        new_bounds = np.concatenate((new_bounds_left, new_bounds_right))
        # Use slices if possible, but if we have two regions use indices.
        if indices_left and indices_right:
            indices = indices_left + indices_right
        elif indices_left:
            indices = slice_left
        elif indices_right:
            indices = slice_right
        else:
            indices = slice(0, 0)

    if reversed_flag:
        new_bounds = new_bounds[::-1, ::-1]

    return new_bounds, indices


def _cartesian_area(y_bounds, x_bounds):
    """
    Return an array of the areas of each cell given two arrays
    of cartesian bounds.

    Args:

    * y_bounds:
        An (n, 2) shaped NumPy array.
    * x_bounds:
        An (m, 2) shaped NumPy array.

    Returns:
        An (n, m) shaped Numpy array of areas.

    """
    heights = y_bounds[:, 1] - y_bounds[:, 0]
    widths = x_bounds[:, 1] - x_bounds[:, 0]
    return np.abs(np.outer(heights, widths))


def _spherical_area(y_bounds, x_bounds, radius=1.0):
    """
    Return an array of the areas of each cell on a sphere
    given two arrays of latitude and longitude bounds in radians.

    Args:

    * y_bounds:
        An (n, 2) shaped NumPy array of latitide bounds in radians.
    * x_bounds:
        An (m, 2) shaped NumPy array of longitude bounds in radians.
    * radius:
        Radius of the sphere. Default is 1.0.

    Returns:
        An (n, m) shaped Numpy array of areas.

    """
    return iris.analysis.cartography._quadrant_area(
        y_bounds + np.pi / 2.0, x_bounds, radius)


def _get_bounds_in_units(coord, units, dtype):
    """Return a copy of coord's bounds in the specified units and dtype."""
    # The bounds are cast to dtype before conversion to prevent issues when
    # mixing float32 and float64 types.
    return coord.units.convert(coord.bounds.astype(dtype), units).astype(dtype)


def _regrid_area_weighted_array(src_data, x_dim, y_dim,
                                src_x_bounds, src_y_bounds,
                                grid_x_bounds, grid_y_bounds,
                                grid_x_decreasing, grid_y_decreasing,
                                area_func, circular=False):
    """
    Regrid the given data from its source grid to a new grid using
    an area weighted mean to determine the resulting data values.

    Args:

    * src_data:
        An N-dimensional NumPy array.
    * x_dim:
        The X dimension within `src_data`.
    * y_dim:
        The Y dimension within `src_data`.
    * src_x_bounds:
        A NumPy array of bounds along the X axis defining the source grid.
    * src_y_bounds:
        A NumPy array of bounds along the Y axis defining the source grid.
    * grid_x_bounds:
        A NumPy array of bounds along the X axis defining the new grid.
    * grid_y_bounds:
        A NumPy array of bounds along the Y axis defining the new grid.
    * grid_x_decreasing:
        Boolean indicating whether the X coordinate of the new grid is
        in descending order.
    * grid_y_decreasing:
        Boolean indicating whether the Y coordinate of the new grid is
        in descending order.
    * area_func:
        A function that returns an (p, q) array of weights given an (p, 2)
        shaped array of Y bounds and an (q, 2) shaped array of X bounds.
    * circular:
        A boolean indicating whether the `src_x_bounds` are periodic. Default
        is False.

    Returns:
        The regridded data as an N-dimensional NumPy array. The lengths
        of the X and Y dimensions will now match those of the target
        grid.

    """
    # Create empty data array to match the new grid.
    # Note that dtype is not preserved and that the array is
    # masked to allow for regions that do not overlap.
    new_shape = list(src_data.shape)
    if x_dim is not None:
        new_shape[x_dim] = grid_x_bounds.shape[0]
    if y_dim is not None:
        new_shape[y_dim] = grid_y_bounds.shape[0]

    # Flag to indicate whether the original data was a masked array.
    src_masked = ma.isMaskedArray(src_data)
    if src_masked:
        new_data = ma.zeros(new_shape, fill_value=src_data.fill_value)
    else:
        new_data = ma.zeros(new_shape)
    # Assign to mask to explode it, allowing indexed assignment.
    new_data.mask = False

    # Simple for loop approach.
    indices = [slice(None)] * new_data.ndim
    for j, (y_0, y_1) in enumerate(grid_y_bounds):
        # Reverse lower and upper if dest grid is decreasing.
        if grid_y_decreasing:
            y_0, y_1 = y_1, y_0
        y_bounds, y_indices = _cropped_bounds(src_y_bounds, y_0, y_1)
        for i, (x_0, x_1) in enumerate(grid_x_bounds):
            # Reverse lower and upper if dest grid is decreasing.
            if grid_x_decreasing:
                x_0, x_1 = x_1, x_0
            x_bounds, x_indices = _cropped_bounds(src_x_bounds, x_0, x_1)

            # Determine whether to mask element i, j based on overlap with
            # src.
            # If x_0 > x_1 then we want [0]->x_1 and x_0->[0] + mod in the case
            # of wrapped longitudes. However if the src grid is not global
            # (i.e. circular) this new cell would include a region outside of
            # the extent of the src grid and should therefore be masked.
            outside_extent = x_0 > x_1 and not circular
            if (outside_extent or not _within_bounds(src_y_bounds, y_0, y_1) or
                    not _within_bounds(src_x_bounds, x_0, x_1)):
                # Mask out element(s) in new_data
                if x_dim is not None:
                    indices[x_dim] = i
                if y_dim is not None:
                    indices[y_dim] = j
                new_data[tuple(indices)] = ma.masked
            else:
                # Calculate weighted mean of data points.
                # Slice out relevant data (this may or may not be a view()
                # depending on x_indices being a slice or not).
                if x_dim is not None:
                    indices[x_dim] = x_indices
                if y_dim is not None:
                    indices[y_dim] = y_indices
                if isinstance(x_indices, tuple) and \
                        isinstance(y_indices, tuple):
                    raise RuntimeError('Cannot handle split bounds '
                                       'in both x and y.')
                data = src_data[tuple(indices)]

                # Calculate weights based on areas of cropped bounds.
                weights = area_func(y_bounds, x_bounds)

                # Numpy 1.7 allows the axis keyword arg to be a tuple.
                # If the version of NumPy is less than 1.7 manipulate the axes
                # of the data so the x and y dimensions can be flattened.
                Version = collections.namedtuple('Version',
                                                 ('major', 'minor', 'micro'))
                np_version = Version(*(int(val) for val in
                                       np.version.version.split('.')))
                if np_version.minor < 7:
                    if y_dim is not None and x_dim is not None:
                        flattened_shape = list(data.shape)
                        if y_dim > x_dim:
                            data = np.rollaxis(data, y_dim, data.ndim)
                            data = np.rollaxis(data, x_dim, data.ndim)
                            del flattened_shape[y_dim]
                            del flattened_shape[x_dim]
                        else:
                            data = np.rollaxis(data, x_dim, data.ndim)
                            data = np.rollaxis(data, y_dim, data.ndim)
                            del flattened_shape[x_dim]
                            del flattened_shape[y_dim]
                            weights = weights.T
                        flattened_shape.append(-1)
                        data = data.reshape(*flattened_shape)
                    elif y_dim is not None:
                        flattened_shape = list(data.shape)
                        del flattened_shape[y_dim]
                        flattened_shape.append(-1)
                        data = data.swapaxes(y_dim, -1).reshape(
                            *flattened_shape)
                    elif x_dim is not None:
                        flattened_shape = list(data.shape)
                        del flattened_shape[x_dim]
                        flattened_shape.append(-1)
                        data = data.swapaxes(x_dim, -1).reshape(
                            *flattened_shape)
                    # Axes of data over which the weighted mean is calculated.
                    axis = -1
                    new_data_pt = ma.average(data, weights=weights.ravel(),
                                             axis=axis)
                else:
                    # Transpose weights to match dim ordering in data.
                    weights_shape_y = weights.shape[0]
                    weights_shape_x = weights.shape[1]
                    if x_dim is not None and y_dim is not None and \
                            x_dim < y_dim:
                        weights = weights.T
                    # Broadcast the weights array to allow numpy's ma.average
                    # to be called.
                    weights_padded_shape = [1] * data.ndim
                    axes = []
                    if y_dim is not None:
                        weights_padded_shape[y_dim] = weights_shape_y
                        axes.append(y_dim)
                    if x_dim is not None:
                        weights_padded_shape[x_dim] = weights_shape_x
                        axes.append(x_dim)
                    # Assign new shape to raise error on copy.
                    weights.shape = weights_padded_shape
                    # Broadcast weights to match shape of data.
                    _, broadcasted_weights = np.broadcast_arrays(data, weights)
                    # Axes of data over which the weighted mean is calculated.
                    axis = tuple(axes)
                    # Calculate weighted mean taking into account missing data.
                    new_data_pt = ma.average(data, weights=broadcasted_weights,
                                             axis=axis)

                # Determine suitable mask for data associated with cell.
                # Could use all() here.
                if src_masked:
                    # data.mask may be a bool, if not collapse via any().
                    if data.mask.ndim:
                        new_data_pt_mask = data.mask.any(axis=axis)
                    else:
                        new_data_pt_mask = data.mask

                # Insert data (and mask) values into new array.
                if x_dim is not None:
                    indices[x_dim] = i
                if y_dim is not None:
                    indices[y_dim] = j
                new_data[tuple(indices)] = new_data_pt
                if src_masked:
                    new_data.mask[tuple(indices)] = new_data_pt_mask

    # Remove new mask if original data was not masked
    # and no values in the new array are masked.
    if not src_masked and not new_data.mask.any():
        new_data = new_data.data

    return new_data


def regrid_area_weighted_rectilinear_src_and_grid(src_cube, grid_cube):
    """
    Return a new cube with data values calculated using the area weighted
    mean of data values from src_grid regridded onto the horizontal grid of
    grid_cube.

    This function requires that the horizontal grids of both cubes are
    rectilinear (i.e. expressed in terms of two orthogonal 1D coordinates)
    and that these grids are in the same coordinate system. This function
    also requires that the coordinates describing the horizontal grids
    all have bounds.

    Args:

    * src_cube:
        An instance of :class:`iris.cube.Cube` that supplies the data,
        metadata and coordinates.
    * grid_cube:
        An instance of :class:`iris.cube.Cube` that supplies the desired
        horizontal grid definition.

    Returns:
        A new :class:`iris.cube.Cube` instance.

    """
    # Get the 1d monotonic (or scalar) src and grid coordinates.
    src_x, src_y = _get_xy_coords(src_cube)
    grid_x, grid_y = _get_xy_coords(grid_cube)

    # Condition 1: All x and y coordinates must have contiguous bounds to
    # define areas.
    if not src_x.is_contiguous() or not src_y.is_contiguous() or \
            not grid_x.is_contiguous() or not grid_y.is_contiguous():
        raise ValueError("The horizontal grid coordinates of both the source "
                         "and grid cubes must have contiguous bounds.")

    # Condition 2: Everything must have the same coordinate system.
    src_cs = src_x.coord_system
    grid_cs = grid_x.coord_system
    if src_cs != grid_cs:
        raise ValueError("The horizontal grid coordinates of both the source "
                         "and grid cubes must have the same coordinate "
                         "system.")

    # Condition 3: cannot create vector coords from scalars.
    src_x_dims = src_cube.coord_dims(src_x)
    src_x_dim = None
    if src_x_dims:
        src_x_dim = src_x_dims[0]
    src_y_dims = src_cube.coord_dims(src_y)
    src_y_dim = None
    if src_y_dims:
        src_y_dim = src_y_dims[0]
    if src_x_dim is None and grid_x.shape[0] != 1 or \
            src_y_dim is None and grid_y.shape[0] != 1:
        raise ValueError('The horizontal grid coordinates of source cube '
                         'includes scalar coordinates, but the new grid does '
                         'not. The new grid must not require additional data '
                         'dimensions to be created.')

    # Determine whether to calculate flat or spherical areas.
    # Don't only rely on coord system as it may be None.
    spherical = (isinstance(src_cs, (iris.coord_systems.GeogCS,
                                     iris.coord_systems.RotatedGeogCS)) or
                 src_x.units == 'degrees' or src_x.units == 'radians')

    # Get src and grid bounds in the same units.
    x_units = iris.unit.Unit('radians') if spherical else src_x.units
    y_units = iris.unit.Unit('radians') if spherical else src_y.units

    # Operate in highest precision.
    src_dtype = np.promote_types(src_x.bounds.dtype, src_y.bounds.dtype)
    grid_dtype = np.promote_types(grid_x.bounds.dtype, grid_y.bounds.dtype)
    dtype = np.promote_types(src_dtype, grid_dtype)

    src_x_bounds = _get_bounds_in_units(src_x, x_units, dtype)
    src_y_bounds = _get_bounds_in_units(src_y, y_units, dtype)
    grid_x_bounds = _get_bounds_in_units(grid_x, x_units, dtype)
    grid_y_bounds = _get_bounds_in_units(grid_y, y_units, dtype)

    # Determine whether target grid bounds are decreasing. This must
    # be determined prior to wrap_lons being called.
    grid_x_decreasing = grid_x_bounds[-1, 0] < grid_x_bounds[0, 0]
    grid_y_decreasing = grid_y_bounds[-1, 0] < grid_y_bounds[0, 0]

    # Wrapping of longitudes.
    if spherical:
        base = np.min(src_x_bounds)
        modulus = x_units.modulus
        # Only wrap if necessary to avoid introducing floating
        # point errors.
        if np.min(grid_x_bounds) < base or \
                np.max(grid_x_bounds) > (base + modulus):
            grid_x_bounds = iris.analysis.cartography.wrap_lons(grid_x_bounds,
                                                                base, modulus)

    # Determine whether the src_x coord has periodic boundary conditions.
    circular = getattr(src_x, 'circular', False)

    # Use simple cartesian area function or one that takes into
    # account the curved surface if coord system is spherical.
    if spherical:
        area_func = _spherical_area
    else:
        area_func = _cartesian_area

    # Calculate new data array for regridded cube.
    new_data = _regrid_area_weighted_array(src_cube.data, src_x_dim, src_y_dim,
                                           src_x_bounds, src_y_bounds,
                                           grid_x_bounds, grid_y_bounds,
                                           grid_x_decreasing,
                                           grid_y_decreasing,
                                           area_func, circular)

    # Wrap up the data as a Cube.
    # Create 2d meshgrids as required by _create_cube func.
    meshgrid_x, meshgrid_y = np.meshgrid(grid_x.points, grid_y.points)
    new_cube = _create_cube(new_data, src_cube, src_x_dim, src_y_dim,
                            src_x, src_y, grid_x, grid_y,
                            meshgrid_x, meshgrid_y,
                            _regrid_bilinear_array)

    # Slice out any length 1 dimensions.
    indices = [slice(None, None)] * new_data.ndim
    if src_x_dim is not None and new_cube.shape[src_x_dim] == 1:
        indices[src_x_dim] = 0
    if src_y_dim is not None and new_cube.shape[src_y_dim] == 1:
        indices[src_y_dim] = 0
    if 0 in indices:
        new_cube = new_cube[tuple(indices)]

    return new_cube


def regrid_weighted_curvilinear_to_rectilinear(src_cube, weights, grid_cube):
    """
    Return a new cube with the data values calculated using the weighted
    mean of data values from :data:`src_cube` and the weights from
    :data:`weights` regridded onto the horizontal grid of :data:`grid_cube`.

    This function requires that the :data:`src_cube` has a curvilinear
    horizontal grid and the target :data:`grid_cube` is rectilinear
    i.e. expressed in terms of two orthogonal 1D horizontal coordinates.
    Both grids must be in the same coordinate system, and the :data:`grid_cube`
    must have horizontal coordinates that are both bounded and contiguous.

    Note that, for any given target :data:`grid_cube` cell, only the points
    from the :data:`src_cube` that are bound by that cell will contribute to
    the cell result. The bounded extent of the :data:`src_cube` will not be
    considered here.

    A target :data:`grid_cube` cell result will be calculated as,
    :math:`\sum (src\_cube.data_{ij} * weights_{ij}) / \sum weights_{ij}`, for
    all :math:`ij` :data:`src_cube` points that are bound by that cell.

    .. warning::

        * Only 2D cubes are supported.
        * All coordinates that span the :data:`src_cube` that don't define
          the horizontal curvilinear grid will be ignored.
        * The :class:`iris.unit.Unit` of the horizontal grid coordinates
          must be either :data:`degrees` or :data:`radians`.

    Args:

    * src_cube:
        A :class:`iris.cube.Cube` instance that defines the source
        variable grid to be regridded.
    * weights:
        A :class:`numpy.ndarray` instance that defines the weights
        for the source variable grid cells. Must have the same shape
        as the :data:`src_cube.data`.
    * grid_cube:
        A :class:`iris.cube.Cube` instance that defines the target
        rectilinear grid.

    Returns:
        A :class:`iris.cube.Cube` instance.

    """
    if src_cube.shape != weights.shape:
        msg = 'The source cube and weights require the same data shape.'
        raise ValueError(msg)

    if src_cube.ndim != 2 or grid_cube.ndim != 2:
        msg = 'The source cube and target grid cube must reference 2D data.'
        raise ValueError(msg)

    if src_cube.aux_factories:
        msg = 'All source cube derived coordinates will be ignored.'
        warnings.warn(msg)

    # Get the source cube x and y 2D auxiliary coordinates.
    sx, sy = src_cube.coord(axis='x'), src_cube.coord(axis='y')
    # Get the target grid cube x and y dimension coordinates.
    tx, ty = _get_xy_dim_coords(grid_cube)

    if sx.units.modulus is None or sy.units.modulus is None or \
            sx.units != sy.units:
        msg = 'The source cube x ({!r}) and y ({!r}) coordinates must ' \
            'have units of degrees or radians.'
        raise ValueError(msg.format(sx.name(), sy.name()))

    if tx.units.modulus is None or ty.units.modulus is None or \
            tx.units != ty.units:
        msg = 'The target grid cube x ({!r}) and y ({!r}) coordinates must ' \
            'have units of degrees or radians.'
        raise ValueError(msg.format(tx.name(), ty.name()))

    if sx.units != tx.units:
        msg = 'The source cube and target grid cube must have x and y ' \
            'coordinates with the same units.'
        raise ValueError(msg)

    if sx.ndim != sy.ndim:
        msg = 'The source cube x ({!r}) and y ({!r}) coordinates must ' \
            'have the same dimensionality.'
        raise ValueError(msg.format(sx.name(), sy.name()))

    if sx.ndim != 2:
        msg = 'The source cube x ({!r}) and y ({!r}) coordinates must ' \
            'be 2D auxiliary coordinates.'
        raise ValueError(msg.format(sx.name(), sy.name()))

    if sx.coord_system != sy.coord_system:
        msg = 'The source cube x ({!r}) and y ({!r}) coordinates must ' \
            'have the same coordinate system.'
        raise ValueError(msg.format(sx.name(), sy.name()))

    if sx.coord_system != tx.coord_system and \
            sx.coord_system is not None and \
            tx.coord_system is not None:
        msg = 'The source cube and target grid cube must have the same ' \
            'coordinate system.'
        raise ValueError(msg)

    if not tx.has_bounds() or not tx.is_contiguous():
        msg = 'The target grid cube x ({!r})coordinate requires ' \
            'contiguous bounds.'
        raise ValueError(msg.format(tx.name()))

    if not ty.has_bounds() or not ty.is_contiguous():
        msg = 'The target grid cube y ({!r}) coordinate requires ' \
            'contiguous bounds.'
        raise ValueError(msg.format(ty.name()))

    def _src_align_and_flatten(coord):
        # Return a flattened, unmasked copy of a coordinate's points array that
        # will align with a flattened version of the source cube's data.
        points = coord.points
        if src_cube.coord_dims(coord) == (1, 0):
            points = points.T
        if points.shape != src_cube.shape:
            msg = 'The shape of the points array of !r is not compatible ' \
                'with the shape of !r.'.format(coord.name(), src_cube.name())
            raise ValueError(msg)
        return np.asarray(points.flatten())

    # Align and flatten the coordinate points of the source space.
    sx_points = _src_align_and_flatten(sx)
    sy_points = _src_align_and_flatten(sy)

    # Match the source cube x coordinate range to the target grid
    # cube x coordinate range.
    min_sx, min_tx = np.min(sx.points), np.min(tx.points)
    modulus = sx.units.modulus
    if min_sx < 0 and min_tx >= 0:
        indices = np.where(sx_points < 0)
        sx_points[indices] += modulus
    elif min_sx >= 0 and min_tx < 0:
        indices = np.where(sx_points > (modulus / 2))
        sx_points[indices] -= modulus

    # Create target grid cube x and y cell boundaries.
    tx_depth, ty_depth = tx.points.size, ty.points.size
    tx_dim, = grid_cube.coord_dims(tx)
    ty_dim, = grid_cube.coord_dims(ty)

    tx_cells = np.concatenate((tx.bounds[:, 0],
                               tx.bounds[-1, 1].reshape(1)))
    ty_cells = np.concatenate((ty.bounds[:, 0],
                               ty.bounds[-1, 1].reshape(1)))

    # Determine the target grid cube x and y cells that bound
    # the source cube x and y points.

    def _regrid_indices(cells, depth, points):
        # Calculate the minimum difference in cell extent.
        extent = np.min(np.diff(cells))
        if extent == 0:
            # Detected an dimension coordinate with an invalid
            # zero length cell extent.
            msg = 'The target grid cube {} ({!r}) coordinate contains ' \
                'a zero length cell extent.'
            axis, name = 'x', tx.name()
            if points is sy_points:
                axis, name = 'y', ty.name()
            raise ValueError(msg.format(axis, name))
        elif extent > 0:
            # The cells of the dimension coordinate are in ascending order.
            indices = np.searchsorted(cells, points, side='right') - 1
        else:
            # The cells of the dimension coordinate are in descending order.
            # np.searchsorted() requires ascending order, so we require to
            # account for this restriction.
            cells = cells[::-1]
            right = np.searchsorted(cells, points, side='right')
            left = np.searchsorted(cells, points, side='left')
            indices = depth - right
            # Only those points that exactly match the left-hand cell bound
            # will differ between 'left' and 'right'. Thus their appropriate
            # target cell location requires to be recalculated to give the
            # correct descending [upper, lower) interval cell, source to target
            # regrid behaviour.
            delta = np.where(left != right)[0]
            if delta.size:
                indices[delta] = depth - left[delta]
        return indices

    x_indices = _regrid_indices(tx_cells, tx_depth, sx_points)
    y_indices = _regrid_indices(ty_cells, ty_depth, sy_points)

    # Now construct a sparse M x N matix, where M is the flattened target
    # space, and N is the flattened source space. The sparse matrix will then
    # be populated with those source cube points that contribute to a specific
    # target cube cell.

    # Determine the valid indices and their offsets in M x N space.
    if ma.isMaskedArray(src_cube.data):
        # Calculate the valid M offsets, accounting for the source cube mask.
        mask = ~src_cube.data.mask.flatten()
        cols = np.where((y_indices >= 0) & (y_indices < ty_depth) &
                        (x_indices >= 0) & (x_indices < tx_depth) &
                        mask)[0]
    else:
        # Calculate the valid M offsets.
        cols = np.where((y_indices >= 0) & (y_indices < ty_depth) &
                        (x_indices >= 0) & (x_indices < tx_depth))[0]

    # Reduce the indices to only those that are valid.
    x_indices = x_indices[cols]
    y_indices = y_indices[cols]

    # Calculate the valid N offsets.
    if ty_dim < tx_dim:
        rows = y_indices * tx.points.size + x_indices
    else:
        rows = x_indices * ty.points.size + y_indices

    # Calculate the associated valid weights.
    weights_flat = weights.flatten()
    data = weights_flat[cols]

    # Build our sparse M x N matrix of weights.
    sparse_matrix = csc_matrix((data, (rows, cols)),
                               shape=(grid_cube.data.size, src_cube.data.size))

    # Performing a sparse sum to collapse the matrix to (M, 1).
    sum_weights = sparse_matrix.sum(axis=1).getA()

    # Determine the rows (flattened target indices) that have a
    # contribution from one or more source points.
    rows = np.nonzero(sum_weights)

    # Calculate the numerator of the weighted mean (M, 1).
    numerator = sparse_matrix * src_cube.data.reshape(-1, 1)

    # Calculate the weighted mean payload.
    weighted_mean = ma.masked_all(numerator.shape, dtype=numerator.dtype)
    weighted_mean[rows] = numerator[rows] / sum_weights[rows]

    # Construct the final regridded weighted mean cube.
    dim_coords_and_dims = zip((ty.copy(), tx.copy()), (ty_dim, tx_dim))
    cube = iris.cube.Cube(weighted_mean.reshape(grid_cube.shape),
                          dim_coords_and_dims=dim_coords_and_dims)
    cube.metadata = copy.deepcopy(src_cube.metadata)

    for coord in src_cube.coords(dimensions=()):
        cube.add_aux_coord(coord.copy())

    return cube

########NEW FILE########
__FILENAME__ = regrid_conservative
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Support for conservative regridding via ESMPy.

"""

import numpy as np
# Import ESMF via iris.proxy, just so we can build the docs with no ESMF.
import iris.proxy
iris.proxy.apply_proxy('ESMF', globals())

import cartopy.crs as ccrs
import iris
import iris.experimental.regrid as i_regrid


#: A static Cartopy Geodetic() instance for transforming to true-lat-lons.
_CRS_TRUELATLON = ccrs.Geodetic()


def _convert_latlons(crs, x_array, y_array):
    """
    Convert x+y coords in a given crs to (x,y) values in true-lat-lons.

    .. note::

        Uses a plain Cartopy Geodetic to convert to true-lat-lons.  This makes
        no allowance for a non-spherical earth.  But then, neither does ESMF.

    """
    ll_values = _CRS_TRUELATLON.transform_points(crs, x_array, y_array)
    return ll_values[..., 0], ll_values[..., 1]


def _make_esmpy_field(x_coord, y_coord, ref_name='field',
                      data=None, mask=None):
    """
    Create an ESMPy ESMF.Field on given coordinates.

    Create a ESMF.Grid from the coordinates, defining corners and centre
    positions as lats+lons.
    Add a grid mask if provided.
    Create and return a Field mapped on this Grid, setting data if provided.

    Args:

    * x_coord, y_coord (:class:`iris.coords.Coord`):
        One-dimensional coordinates of shape (nx,) and (ny,).
        Their contiguous bounds define an ESMF.Grid of shape (nx, ny).

    Kwargs:

    * data (:class:`numpy.ndarray`, shape (nx,ny)):
        Set the Field data content.
    * mask (:class:`numpy.ndarray`, boolean, shape (nx,ny)):
        Add a mask item to the grid, assigning it 0/1 where mask=False/True.

    """
    # Create a Grid object describing the coordinate cells.
    dims = [len(coord.points) for coord in (x_coord, y_coord)]
    dims = np.array(dims, dtype=np.int32)  # specific type required by ESMF.
    grid = ESMF.Grid(dims)

    # Get all cell corner coordinates as true-lat-lons
    x_bounds, y_bounds = np.meshgrid(x_coord.contiguous_bounds(),
                                     y_coord.contiguous_bounds())
    grid_crs = x_coord.coord_system.as_cartopy_crs()
    lon_bounds, lat_bounds = _convert_latlons(grid_crs, x_bounds, y_bounds)

    # Add grid 'coord' element for corners, and fill with corner values.
    grid.add_coords(staggerlocs=[ESMF.StaggerLoc.CORNER])
    grid_corners_x = grid.get_coords(0, ESMF.StaggerLoc.CORNER)
    grid_corners_x[:] = lon_bounds.T
    grid_corners_y = grid.get_coords(1, ESMF.StaggerLoc.CORNER)
    grid_corners_y[:] = lat_bounds.T

    # calculate the cell centre-points
    # NOTE: we don't care about Iris' idea of where the points 'really' are
    # *but* ESMF requires the data in the CENTER for conservative regrid,
    # according to the documentation :
    #  - http://www.earthsystemmodeling.org/
    #        esmf_releases/public/last/ESMF_refdoc.pdf
    #  - section  22.2.3 : ESMF_REGRIDMETHOD
    #
    # We are currently determining cell centres in native coords, then
    # converting these into true-lat-lons.
    # It is confirmed by experiment that moving these centre location *does*
    # changes the regrid results.
    # TODO: work out why this is needed, and whether these centres are 'right'.

    # Average cell corners in native coordinates, then translate to lats+lons
    # (more costly, but presumably 'more correct' than averaging lats+lons).
    x_centres = x_coord.contiguous_bounds()
    x_centres = 0.5 * (x_centres[:-1] + x_centres[1:])
    y_centres = y_coord.contiguous_bounds()
    y_centres = 0.5 * (y_centres[:-1] + y_centres[1:])
    x_points, y_points = np.meshgrid(x_centres, y_centres)
    lon_points, lat_points = _convert_latlons(grid_crs, x_points, y_points)

    # Add grid 'coord' element for centres + fill with centre-points values.
    grid.add_coords(staggerlocs=[ESMF.StaggerLoc.CENTER])
    grid_centers_x = grid.get_coords(0, ESMF.StaggerLoc.CENTER)
    grid_centers_x[:] = lon_points.T
    grid_centers_y = grid.get_coords(1, ESMF.StaggerLoc.CENTER)
    grid_centers_y[:] = lat_points.T

    # Add a mask item, if requested
    if mask is not None:
        grid.add_item(ESMF.GridItem.MASK,
                      [ESMF.StaggerLoc.CENTER])
        grid_mask = grid.get_item(ESMF.GridItem.MASK)
        grid_mask[:] = np.where(mask, 1, 0)

    # create a Field based on this grid
    field = ESMF.Field(grid, ref_name)

    # assign data content, if provided
    if data is not None:
        field.data[:] = data

    return field


def regrid_conservative_via_esmpy(source_cube, grid_cube):
    """
    Perform a conservative regridding with ESMPy.

    Regrids the data of a source cube onto a new grid defined by a destination
    cube.

    Args:

    * source_cube (:class:`iris.cube.Cube`):
        Source data.  Must have two identifiable horizontal dimension
        coordinates.
    * grid_cube (:class:`iris.cube.Cube`):
        Define the target horizontal grid:  Only the horizontal dimension
        coordinates are actually used.

    Returns:
        A new cube derived from source_cube, regridded onto the specified
        horizontal grid.

    Any additional coordinates which map onto the horizontal dimensions are
    removed, while all other metadata is retained.
    If there are coordinate factories with 2d horizontal reference surfaces,
    the reference surfaces are also regridded, using ordinary bilinear
    interpolation.

    .. note::

        Both source and destination cubes must have two dimension coordinates
        identified with axes 'X' and 'Y' which share a coord_system with a
        Cartopy CRS.
        The grids are defined by :meth:`iris.coords.Coord.contiguous_bounds` of
        these.

    .. note::

        Initialises the ESMF Manager, if it was not already called.
        This implements default Manager operations (e.g. logging).

        To alter this, make a prior call to ESMF.Manager().

    """
    # Get source + target XY coordinate pairs and check they are suitable.
    src_coords = i_regrid._get_xy_dim_coords(source_cube)
    dst_coords = i_regrid._get_xy_dim_coords(grid_cube)
    src_cs = src_coords[0].coord_system
    grid_cs = dst_coords[0].coord_system
    if src_cs is None or grid_cs is None:
        raise ValueError("Both 'src' and 'grid' Cubes must have a"
                         " coordinate system for their rectilinear grid"
                         " coordinates.")

    if src_cs.as_cartopy_crs() is None or grid_cs.as_cartopy_crs() is None:
        raise ValueError("Both 'src' and 'grid' Cubes coord_systems must have "
                         "a valid associated Cartopy CRS.")

    def _valid_units(coord):
        if isinstance(coord.coord_system, (iris.coord_systems.GeogCS,
                                           iris.coord_systems.RotatedGeogCS)):
            valid_units = 'degrees'
        else:
            valid_units = 'm'
        return coord.units == valid_units

    if not all(_valid_units(coord) for coord in src_coords + dst_coords):
        raise ValueError("Unsupported units: must be 'degrees' or 'm'.")

    # Initialise the ESMF manager in case it was not already done.
    ESMF.Manager()

    # Create a data array for the output cube.
    src_dims_xy = [source_cube.coord_dims(coord)[0] for coord in src_coords]
    # Size matches source, except for X+Y dimensions
    dst_shape = np.array(source_cube.shape)
    dst_shape[src_dims_xy] = [coord.shape[0] for coord in dst_coords]
    # NOTE: result array is masked -- fix this afterward if all unmasked
    fullcube_data = np.ma.zeros(dst_shape)

    # Iterate 2d slices over all possible indices of the 'other' dimensions
    all_other_dims = filter(lambda i_dim: i_dim not in src_dims_xy,
                            xrange(source_cube.ndim))
    all_combinations_of_other_inds = np.ndindex(*dst_shape[all_other_dims])
    for other_indices in all_combinations_of_other_inds:
        # Construct a tuple of slices to address the 2d xy field
        slice_indices_array = np.array([slice(None)] * source_cube.ndim)
        slice_indices_array[all_other_dims] = other_indices
        slice_indices_tuple = tuple(slice_indices_array)

        # Get the source data, reformed into the right dimension order, (x,y).
        src_data_2d = source_cube.data[slice_indices_tuple]
        if (src_dims_xy[0] > src_dims_xy[1]):
            src_data_2d = src_data_2d.transpose()

        # Work out whether we have missing data to define a source grid mask.
        if np.ma.is_masked(src_data_2d):
            srcdata_mask = np.ma.getmask(src_data_2d)
        else:
            srcdata_mask = None

        # Construct ESMF Field objects on source and destination grids.
        src_field = _make_esmpy_field(src_coords[0], src_coords[1],
                                      data=src_data_2d, mask=srcdata_mask)
        dst_field = _make_esmpy_field(dst_coords[0], dst_coords[1])

        # Make Field for destination coverage fraction (for missing data calc).
        coverage_field = ESMF.Field(dst_field.grid, 'validmask_dst')

        # Do the actual regrid with ESMF.
        mask_flag_values = np.array([1], dtype=np.int32)
        regrid_method = ESMF.Regrid(src_field, dst_field,
                                    src_mask_values=mask_flag_values,
                                    regrid_method=ESMF.RegridMethod.CONSERVE,
                                    unmapped_action=ESMF.UnmappedAction.IGNORE,
                                    dst_frac_field=coverage_field)
        regrid_method(src_field, dst_field)
        data = dst_field.data

        # Convert destination 'coverage fraction' into a missing-data mask.
        # Set = wherever part of cell goes outside source grid, or overlaps a
        # masked source cell.
        coverage_tolerance_threshold = 1.0 - 1.0e-8
        data.mask = coverage_field.data < coverage_tolerance_threshold

        # Transpose ESMF result dims (X,Y) back to the order of the source
        if (src_dims_xy[0] > src_dims_xy[1]):
            data = data.transpose()

        # Paste regridded slice back into parent array
        fullcube_data[slice_indices_tuple] = data

    # Remove the data mask if completely unused.
    if not np.ma.is_masked(fullcube_data):
        fullcube_data = np.array(fullcube_data)

    # Generate a full 2d sample grid, as required for regridding orography
    # NOTE: as seen in "regrid_bilinear_rectilinear_src_and_grid"
    # TODO: can this not also be wound into the _create_cube method ?
    src_cs = src_coords[0].coord_system
    sample_grid_x, sample_grid_y = i_regrid._sample_grid(src_cs,
                                                         dst_coords[0],
                                                         dst_coords[1])

    # Return result as a new cube based on the source.
    # TODO: please tidy this interface !!!
    return i_regrid._create_cube(
        fullcube_data,
        src=source_cube,
        x_dim=src_dims_xy[0],
        y_dim=src_dims_xy[1],
        src_x_coord=src_coords[0],
        src_y_coord=src_coords[1],
        grid_x_coord=dst_coords[0],
        grid_y_coord=dst_coords[1],
        sample_grid_x=sample_grid_x,
        sample_grid_y=sample_grid_y,
        regrid_callback=i_regrid._regrid_bilinear_array)

########NEW FILE########
__FILENAME__ = abf
# (C) British Crown Copyright 2012 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Provides ABF (and ABL) file format capabilities.

ABF and ABL files are satellite file formats defined by Boston University.
Including this module adds ABF and ABL loading to the session's capabilities.

The documentation for this file format can be found
`here <http://cliveg.bu.edu/modismisr/lai3g-fpar3g.html>`_.

"""

import calendar
import datetime
import glob
import os.path

import numpy as np
import numpy.ma as ma

import iris
from iris.coords import AuxCoord, DimCoord
from iris.coord_systems import GeogCS
import iris.fileformats
import iris.io.format_picker


X_SIZE = 4320
Y_SIZE = 2160


month_numbers = {"jan": 1, "feb": 2, "mar": 3, "apr": 4, "may": 5, "jun": 6,
                 "jul": 7, "aug": 8, "sep": 9, "oct": 10, "nov": 11, "dec": 12}


class ABFField(object):
    """
    A data field from an ABF (or ABL) file.

    Capable of creating a :class:`~iris.cube.Cube`.

    """
    def __init__(self, filename):
        """
        Create an ABFField object from the given filename.

        Args:

            * filename - An ABF filename.

        Example::

            field = ABFField("AVHRRBUVI01.1985feba.abl")

        """
        basename = os.path.basename(filename)
        if len(basename) != 24:
            raise ValueError("ABFField expects a filename of 24 characters: "
                             "{}".format(basename))
        self._filename = filename

    def __getattr__(self, key):
        # Do we need to load now?
        if key == 'data' and 'data' not in self.__dict__:
            self._read()
        try:
            return self.__dict__[key]
        except KeyError:
            raise AttributeError("ABFField has no attribute '{}'".format(key))

    def _read(self):
        """Read the field from the given filename."""
        basename = os.path.basename(self._filename)
        self.version = int(basename[9:11])
        self.year = int(basename[12:16])
        self.month = basename[16:19]
        self.period = basename[19:20]
        self.format = basename[21:24]

        self.month = month_numbers[self.month]

        # Data is 8 bit bigendian.
        data = np.fromfile(self._filename, dtype='>u1').reshape(X_SIZE, Y_SIZE)
        # Iris' preferred dimensional ordering is (y,x).
        data = data.transpose()
        # Flip, for a positive step through the Y dimension.
        data = data[::-1]
        # Any percentages greater than 100 represent missing data.
        data = ma.masked_greater(data, 100)
        # The default fill value is 999999(!), so we choose something
        # more sensible. NB. 999999 % 256 = 63 = bad.
        data.fill_value = 255
        self.data = data

    def to_cube(self):
        """Return a new :class:`~iris.cube.Cube` from this ABFField."""

        cube = iris.cube.Cube(self.data)

        # Name.
        if self.format.lower() == "abf":
            cube.rename("leaf_area_index")
        elif self.format.lower() == "abl":
            cube.rename("FAPAR")
        else:
            msg = "Unknown ABF/ABL format: {}".format(self.format)
            raise iris.exceptions.TranslationError(msg)
        cube.units = "%"

        # Grid.
        step = 1.0 / 12.0

        llcs = GeogCS(semi_major_axis=6378137.0, semi_minor_axis=6356752.31424)

        x_coord = DimCoord(np.arange(X_SIZE) * step + (step / 2) - 180,
                           standard_name="longitude", units="degrees",
                           coord_system=llcs)

        y_coord = DimCoord(np.arange(Y_SIZE) * step + (step / 2) - 90,
                           standard_name="latitude",  units="degrees",
                           coord_system=llcs)

        x_coord.guess_bounds()
        y_coord.guess_bounds()

        cube.add_dim_coord(x_coord, 1)
        cube.add_dim_coord(y_coord, 0)

        # Time.
        if self.period == "a":
            start = 1
            end = 15
        elif self.period == "b":
            start = 16
            end = calendar.monthrange(self.year, self.month)[1]
        else:
            raise iris.exceptions.TranslationError("Unknown period: "
                                                   "{}".format(self.period))

        start = datetime.date(year=self.year, month=self.month, day=start)
        end = datetime.date(year=self.year, month=self.month, day=end)

        # Convert to "days since 0001-01-01".
        # Iris will have proper datetime objects in the future.
        # This step will not be necessary.
        start = start.toordinal() - 1
        end = end.toordinal() - 1

        # TODO: Should we put the point in the middle of the period instead?
        cube.add_aux_coord(AuxCoord(start, standard_name="time",
                                    units="days since 0001-01-01",
                                    bounds=[start, end]))

        # TODO: Do they only come from Boston?
        # Attributes.
        cube.attributes["source"] = "Boston University"

        return cube


def load_cubes(filespecs, callback=None):
    """
    Loads cubes from a list of ABF filenames.

    Args:

    * filenames - list of ABF filenames to load

    Kwargs:

    * callback - a function that can be passed to :func:`iris.io.run_callback`

    .. note::

        The resultant cubes may not be in the same order as in the file.

    """
    if isinstance(filespecs, basestring):
        filespecs = [filespecs]

    for filespec in filespecs:
        for filename in glob.glob(filespec):

            field = ABFField(filename)
            cube = field.to_cube()

            # Were we given a callback?
            if callback is not None:
                cube = iris.io.run_callback(callback, cube, field, filename)
                if cube is None:
                    continue

            yield cube

########NEW FILE########
__FILENAME__ = cf
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Provides the capability to load netCDF files and interprete them
according to the 'NetCDF Climate and Forecast (CF) Metadata Conventions'.

References:

[CF]  NetCDF Climate and Forecast (CF) Metadata conventions, Version 1.5, October, 2010.
[NUG] NetCDF User's Guide, http://www.unidata.ucar.edu/software/netcdf/docs/netcdf.html

"""

from abc import ABCMeta, abstractmethod
import os
import re
import UserDict
import warnings

import netCDF4
import numpy as np
import numpy.ma as ma

import iris.util


#
# CF parse pattern common to both formula terms and measure CF variables.
#
_CF_PARSE = re.compile(r'''
                           \s*
                           (?P<lhs>[\w_]+)
                           \s*:\s*
                           (?P<rhs>[\w_]+)
                           \s*
                        ''', re.VERBOSE)

# NetCDF variable attributes handled by the netCDF4 module and
# therefore automatically classed as "used" attributes.
_CF_ATTRS_IGNORE = set(['_FillValue', 'add_offset', 'missing_value', 'scale_factor', ])


################################################################################
class CFVariable(object):
    """Abstract base class wrapper for a CF-netCDF variable."""

    __metaclass__ = ABCMeta

    #: Name of the netCDF variable attribute that identifies this
    #: CF-netCDF variable.
    cf_identity = None
    
    def __init__(self, name, data):
        # Accessing the list of netCDF attributes is surprisingly slow.
        # Since it's used repeatedly, caching the list makes things
        # quite a bit faster.
        self._nc_attrs = data.ncattrs()

        #: NetCDF variable name
        self.cf_name = name

        #: NetCDF4 Variable data instance
        self.cf_data = data

        #: Collection of CF-netCDF variables associated with this variable
        self.cf_group = None

        #: CF-netCDF formula terms that his variable participates in
        self.cf_terms_by_root = {}

        self.cf_attrs_reset()

    @staticmethod
    def _identify_common(variables, ignore, target):
        if ignore is None:
            ignore = []

        if target is None:
            target = variables
        elif isinstance(target, basestring):
            if target not in variables:
                raise ValueError('Cannot identify unknown target CF-netCDF variable %r' % target)
            target = {target: variables[target]}
        else:
            raise TypeError('Expect a target CF-netCDF variable name')

        return (ignore, target)

    @abstractmethod
    def identify(self, variables, ignore=None, target=None, warn=True):
        """
        Identify all variables that match the criterion for this CF-netCDF variable class.

        Args:

        * variables:
            Dictionary of netCDF4.Variable instance by variable name.

        Kwargs:

        * ignore:
            List of variable names to ignore.
        * target:
            Name of a single variable to check.
        * warn:
            Issue a warning if a missing variable is referenced.

        Returns:
            Dictionary of CFVariable instance by variable name.

        """
        pass

    def __eq__(self, other):
        # CF variable names are unique.
        return self.cf_name == other.cf_name

    def __ne__(self, other):
        # CF variable names are unique.
        return self.cf_name != other.cf_name

    def __getattr__(self, name):
        # Accessing netCDF attributes is surprisingly slow. Since
        # they're often read repeatedly, caching the values makes things
        # quite a bit faster.
        if name in self._nc_attrs:
            self._cf_attrs.add(name)
        value = getattr(self.cf_data, name)
        setattr(self, name, value)
        return value

    def __getitem__(self, key):
        return self.cf_data.__getitem__(key)

    def __len__(self):
        return self.cf_data.__len__()

    def __repr__(self):
        return '%s(%r, %r)' % (self.__class__.__name__, self.cf_name, self.cf_data)

    def cf_attrs(self):
        """Return a list of all attribute name and value pairs of the CF-netCDF variable."""
        return tuple((attr, self.getncattr(attr))
                        for attr in sorted(self._nc_attrs))

    def cf_attrs_ignored(self):
        """Return a list of all ignored attribute name and value pairs of the CF-netCDF variable."""
        return tuple((attr, self.getncattr(attr)) for attr in
                        sorted(set(self._nc_attrs) & _CF_ATTRS_IGNORE))

    def cf_attrs_used(self):
        """Return a list of all accessed attribute name and value pairs of the CF-netCDF variable."""
        return tuple((attr, self.getncattr(attr)) for attr in
                        sorted(self._cf_attrs))

    def cf_attrs_unused(self):
        """Return a list of all non-accessed attribute name and value pairs of the CF-netCDF variable."""
        return tuple((attr, self.getncattr(attr)) for attr in
                        sorted(set(self._nc_attrs) - self._cf_attrs))

    def cf_attrs_reset(self):
        """Reset the history of accessed attribute names of the CF-netCDF variable."""
        self._cf_attrs = set([item[0] for item in self.cf_attrs_ignored()])

    def add_formula_term(self, root, term):
        """
        Register the participation of this CF-netCDF variable in a CF-netCDF formula term.

        Args:

        * root (string):
            The name of CF-netCDF variable that defines the CF-netCDF formula_terms attribute.
        * term (string):
            The associated term name of this variable in the formula_terms definition.

        Returns:
            None.

        """
        self.cf_terms_by_root[root] = term

    def has_formula_terms(self):
        """
        Determine whether this CF-netCDF variable participates in a CF-netcdf formula term.

        Returns:
            Boolean.

        """
        return bool(self.cf_terms_by_root)


class CFAncillaryDataVariable(CFVariable):
    """
    A CF-netCDF ancillary data variable is a variable that provides metadata
    about the individual values of another data variable.

    Identified by the CF-netCDF variable attribute 'ancillary_variables'.

    Ref: [CF] Section 3.4. Ancillary Data.

    """
    cf_identity = 'ancillary_variables'

    @classmethod
    def identify(cls, variables, ignore=None, target=None, warn=True):
        result = {}
        ignore, target = cls._identify_common(variables, ignore, target)
        netcdf_variable_names = variables.keys()

        # Identify all CF ancillary data variables.
        for nc_var_name, nc_var in target.iteritems():
            # Check for ancillary data variable references.
            nc_var_att = getattr(nc_var, cls.cf_identity, None)

            if nc_var_att is not None:
                for name in nc_var_att.split():
                    if name not in ignore:
                        if name not in netcdf_variable_names:
                            if warn:
                                message = 'Missing CF-netCDF ancillary data variable %r, referenced by netCDF variable %r'
                                warnings.warn(message % (name, nc_var_name))
                        else:
                            result[name] = CFAncillaryDataVariable(name, variables[name])

        return result


class CFAuxiliaryCoordinateVariable(CFVariable):
    """
    A CF-netCDF auxiliary coordinate variable is any netCDF variable that contains
    coordinate data, but is not a CF-netCDF coordinate variable by definition.

    There is no relationship between the name of a CF-netCDF auxiliary coordinate
    variable and the name(s) of its dimension(s).

    Identified by the CF-netCDF variable attribute 'coordinates'.
    Also see :class:`iris.fileformats.cf.CFLabelVariable`.

    Ref: [CF] Chapter 5. Coordinate Systems.
         [CF] Section 6.2. Alternative Coordinates.

    """
    cf_identity = 'coordinates'

    @classmethod
    def identify(cls, variables, ignore=None, target=None, warn=True):
        result = {}
        ignore, target = cls._identify_common(variables, ignore, target)
        netcdf_variable_names = variables.keys()

        # Identify all CF auxiliary coordinate variables.
        for nc_var_name, nc_var in target.iteritems():
            # Check for auxiliary coordinate variable references.
            nc_var_att = getattr(nc_var, cls.cf_identity, None)

            if nc_var_att is not None:
                for name in nc_var_att.split():
                    if name not in ignore:
                        if name not in netcdf_variable_names:
                            if warn:
                                message = 'Missing CF-netCDF auxiliary coordinate variable %r, referenced by netCDF variable %r'
                                warnings.warn(message % (name, nc_var_name))
                        else:
                            # Restrict to non-string type i.e. not a CFLabelVariable.
                            if not np.issubdtype(variables[name].dtype, np.str):
                                result[name] = CFAuxiliaryCoordinateVariable(name, variables[name])

        return result


class CFBoundaryVariable(CFVariable):
    """
    A CF-netCDF boundary variable is associated with a CF-netCDF variable that contains
    coordinate data. When a data value provides information about conditions in a cell
    occupying a region of space/time or some other dimension, the boundary variable
    provides a description of cell extent.

    A CF-netCDF boundary variable will have one more dimension than its associated
    CF-netCDF coordinate variable or CF-netCDF auxiliary coordinate variable.

    Identified by the CF-netCDF variable attribute 'bounds'.

    Ref: [CF] Section 7.1. Cell Boundaries.

    """
    cf_identity = 'bounds'

    @classmethod
    def identify(cls, variables, ignore=None, target=None, warn=True):
        result = {}
        ignore, target = cls._identify_common(variables, ignore, target)
        netcdf_variable_names = variables.keys()

        # Identify all CF boundary variables.
        for nc_var_name, nc_var in target.iteritems():
            # Check for a boundary variable reference.
            nc_var_att = getattr(nc_var, cls.cf_identity, None)

            if nc_var_att is not None:
                name = nc_var_att.strip()

                if name not in ignore:
                    if name not in netcdf_variable_names:
                        if warn:
                            message = 'Missing CF-netCDF boundary variable %r, referenced by netCDF variable %r'
                            warnings.warn(message % (name, nc_var_name))
                    else:
                        result[name] = CFBoundaryVariable(name, variables[name])

        return result


class CFClimatologyVariable(CFVariable):
    """
    A CF-netCDF climatology variable is associated with a CF-netCDF variable that contains
    coordinate data. When a data value provides information about conditions in a cell
    occupying a region of space/time or some other dimension, the climatology variable
    provides a climatological description of cell extent.

    A CF-netCDF climatology variable will have one more dimension than its associated
    CF-netCDF coordinate variable.

    Identified by the CF-netCDF variable attribute 'climatology'.

    Ref: [CF] Section 7.4. Climatological Statistics

    """
    cf_identity = 'climatology'

    @classmethod
    def identify(cls, variables, ignore=None, target=None, warn=True):
        result = {}
        ignore, target = cls._identify_common(variables, ignore, target)
        netcdf_variable_names = variables.keys()

        # Identify all CF climatology variables.
        for nc_var_name, nc_var in target.iteritems():
            # Check for a climatology variable reference.
            nc_var_att = getattr(nc_var, cls.cf_identity, None)

            if nc_var_att is not None:
                name = nc_var_att.strip()

                if name not in ignore:
                    if name not in netcdf_variable_names:
                        if warn:
                            message = 'Missing CF-netCDF climatology variable %r, referenced by netCDF variable %r'
                            warnings.warn(message % (name, nc_var_name))
                    else:
                        result[name] = CFClimatologyVariable(name, variables[name])

        return result


class CFCoordinateVariable(CFVariable):
    """
    A CF-netCDF coordinate variable is a one-dimensional variable with the same name
    as its dimension, and it is defined as a numeric data type with values that are
    ordered monotonically. Missing values are not allowed in CF-netCDF coordinate
    variables. Also see [NUG] Section 2.3.1.

    Identified by the above criterion, there is no associated CF-netCDF variable
    attribute.

    Ref: [CF] 1.2. Terminology.

    """
    @classmethod
    def identify(cls, variables, ignore=None, target=None, warn=True, monotonic=False):
        result = {}
        ignore, target = cls._identify_common(variables, ignore, target)

        # Identify all CF coordinate variables.
        for nc_var_name, nc_var in target.iteritems():
            if nc_var_name in ignore:
                continue
            # String variables can't be coordinates
            if np.issubdtype(nc_var.dtype, np.str):
                continue
            # Restrict to one-dimensional with name as dimension OR zero-dimensional scalar
            if not ((nc_var.ndim == 1 and nc_var_name in nc_var.dimensions) or (nc_var.ndim == 0)):
                continue
            # Restrict to monotonic?
            if monotonic:
                data = nc_var[:]
                # Gracefully fill a masked coordinate.
                if ma.isMaskedArray(data):
                    data = ma.filled(data)
                if nc_var.shape == () or nc_var.shape == (1,) or iris.util.monotonic(data):
                    result[nc_var_name] = CFCoordinateVariable(nc_var_name, nc_var)
            else:
                result[nc_var_name] = CFCoordinateVariable(nc_var_name, nc_var)

        return result


class CFDataVariable(CFVariable):
    """
    A CF-netCDF variable containing data pay-load that maps to an Iris :class:`iris.cube.Cube`.

    """
    @classmethod
    def identify(cls, variables, ignore=None, target=None, warn=True):
        raise NotImplementedError


class _CFFormulaTermsVariable(CFVariable):
    """
    A CF-netCDF formula terms variable corresponds to a term in a formula that
    allows dimensional vertical coordinate values to be computed from dimensionless
    vertical coordinate values and associated variables at specific grid points.

    Identified by the CF-netCDF variable attribute 'formula_terms'.

    Ref: [CF] Section 4.3.2. Dimensional Vertical Coordinate.
         [CF] Appendix D. Dimensionless Vertical Coordinates.

    """
    cf_identity = 'formula_terms'

    def __init__(self, name, data, formula_root, formula_term):
        CFVariable.__init__(self, name, data)
        # Register the formula root and term relationship.
        self.add_formula_term(formula_root, formula_term)

    @classmethod
    def identify(cls, variables, ignore=None, target=None, warn=True):
        result = {}
        ignore, target = cls._identify_common(variables, ignore, target)
        netcdf_variable_names = variables.keys()

        # Identify all CF formula terms variables.
        for nc_var_name, nc_var in target.iteritems():
            # Check for formula terms variable references.
            nc_var_att = getattr(nc_var, cls.cf_identity, None)

            if nc_var_att is not None:
                for match_item in _CF_PARSE.finditer(nc_var_att):
                    match_group = match_item.groupdict()
                    # Ensure that term name is lower case, as expected.
                    term_name = match_group['lhs'].lower()
                    variable_name = match_group['rhs']

                    if variable_name not in ignore:
                        if variable_name not in netcdf_variable_names:
                            if warn:
                                message = 'Missing CF-netCDF formula term variable %r, referenced by netCDF variable %r'
                                warnings.warn(message % (variable_name, nc_var_name))
                        else:
                            if variable_name not in result:
                                result[variable_name] = _CFFormulaTermsVariable(variable_name,
                                                                                variables[variable_name],
                                                                                nc_var_name, term_name)
                            else:
                                result[variable_name].add_formula_term(nc_var_name, term_name)

        return result

    def __repr__(self):
        return '%s(%r, %r, %r)' % (self.__class__.__name__,
                                   self.cf_name, self.cf_data,
                                   self.cf_terms_by_root)


class CFGridMappingVariable(CFVariable):
    """
    A CF-netCDF grid mapping variable contains a list of specific attributes that
    define a particular grid mapping. A CF-netCDF grid mapping variable must contain
    the attribute 'grid_mapping_name'.

    Based on the value of the 'grid_mapping_name' attribute, there are associated
    standard names of CF-netCDF coordinate variables that contain the mapping's
    independent variables.

    Identified by the CF-netCDF variable attribute 'grid_mapping'.

    Ref: [CF] Section 5.6. Horizontal Coordinate Reference Systems, Grid Mappings, and Projections.
         [CF] Appendix F. Grid Mappings.

    """
    cf_identity = 'grid_mapping'

    @classmethod
    def identify(cls, variables, ignore=None, target=None, warn=True):
        result = {}
        ignore, target = cls._identify_common(variables, ignore, target)
        netcdf_variable_names = variables.keys()

        # Identify all grid mapping variables.
        for nc_var_name, nc_var in target.iteritems():
            # Check for a grid mapping variable reference.
            nc_var_att = getattr(nc_var, cls.cf_identity, None)

            if nc_var_att is not None:
                name = nc_var_att.strip()

                if name not in ignore:
                    if name not in netcdf_variable_names:
                        if warn:
                            message = 'Missing CF-netCDF grid mapping variable %r, referenced by netCDF variable %r'
                            warnings.warn(message % (name, nc_var_name))
                    else:
                        result[name] = CFGridMappingVariable(name, variables[name])

        return result


class CFLabelVariable(CFVariable):
    """
    A CF-netCDF CF label variable is any netCDF variable that contain string
    textual information, or labels.

    Identified by the CF-netCDF variable attribute 'coordinates'.
    Also see :class:`iris.fileformats.cf.CFAuxiliaryCoordinateVariable`.

    Ref: [CF] Section 6.1. Labels.

    """
    cf_identity = 'coordinates'

    @classmethod
    def identify(cls, variables, ignore=None, target=None, warn=True):
        result = {}
        ignore, target = cls._identify_common(variables, ignore, target)
        netcdf_variable_names = variables.keys()

        # Identify all CF label variables.
        for nc_var_name, nc_var in target.iteritems():
            # Check for label variable references.
            nc_var_att = getattr(nc_var, cls.cf_identity, None)

            if nc_var_att is not None:
                for name in nc_var_att.split():
                    if name not in ignore:
                        if name not in netcdf_variable_names:
                            if warn:
                                message = 'Missing CF-netCDF label variable %r, referenced by netCDF variable %r'
                                warnings.warn(message % (name, nc_var_name))
                        else:
                            # Restrict to only string type.
                            if np.issubdtype(variables[name].dtype, np.str):
                                result[name] = CFLabelVariable(name, variables[name])

        return result

    def cf_label_data(self, cf_data_var):
        """
        Return the associated CF-netCDF label variable strings.

        Args:

        * cf_data_var (:class:`iris.fileformats.cf.CFDataVariable`):
            The CF-netCDF data variable which the CF-netCDF label variable describes.

        Returns:
            String labels.

        """

        if not isinstance(cf_data_var, CFDataVariable):
            raise TypeError('cf_data_var argument should be of type CFDataVariable. Got %r.' % type(cf_data_var))

        # Determine the name of the label string (or length) dimension by
        # finding the dimension name that doesn't exist within the data dimensions.
        str_dim_name = list(set(self.dimensions) - set(cf_data_var.dimensions))

        if len(str_dim_name) != 1:
            raise ValueError('Invalid string dimensions for CF-netCDF label variable %r' % self.cf_name)

        str_dim_name = str_dim_name[0]
        label_data = self[:]

        if isinstance(label_data, ma.MaskedArray):
            label_data = label_data.filled()

        # Determine whether we have a string-valued scalar label
        # i.e. a character variable that only has one dimension (the length of the string).
        if self.ndim == 1:
            data = np.array([''.join(label_data).strip()])
        else:
            # Determine the index of the string dimension.
            str_dim = self.dimensions.index(str_dim_name)

            # Calculate new label data shape (without string dimension) and create payload array.
            new_shape = tuple(dim_len for i, dim_len in enumerate(self.shape) if i != str_dim)
            data = np.empty(new_shape, dtype='|S%d' % self.shape[str_dim])

            for index in np.ndindex(new_shape):
                # Create the slice for the label data.
                if str_dim == 0:
                    label_index = (slice(None, None),) + index
                else:
                    label_index = index + (slice(None, None),)

                data[index] = ''.join(label_data[label_index]).strip()

        return data

    def cf_label_dimensions(self, cf_data_var):
        """
        Return the name of the associated CF-netCDF label variable data dimensions.

        Args:

        * cf_data_var (:class:`iris.fileformats.cf.CFDataVariable`):
            The CF-netCDF data variable which the CF-netCDF label variable describes.

        Returns:
            Tuple of label data dimension names.

        """

        if not isinstance(cf_data_var, CFDataVariable):
            raise TypeError('cf_data_var argument should be of type CFDataVariable. Got %r.' % type(cf_data_var))

        return tuple([dim_name for dim_name in self.dimensions if dim_name in cf_data_var.dimensions])


class CFMeasureVariable(CFVariable):
    """
    A CF-netCDF measure variable is a variable that contains cell areas or volumes.

    Identified by the CF-netCDF variable attribute 'cell_measures'.

    Ref: [CF] Section 7.2. Cell Measures.

    """
    cf_identity = 'cell_measures'

    def __init__(self, name, data, measure):
        CFVariable.__init__(self, name, data)
        #: Associated cell measure of the cell variable
        self.cf_measure = measure
 
    @classmethod
    def identify(cls, variables, ignore=None, target=None, warn=True):
        result = {}
        ignore, target = cls._identify_common(variables, ignore, target)
        netcdf_variable_names = variables.keys()

        # Identify all CF measure variables.
        for nc_var_name, nc_var in target.iteritems():
            # Check for measure variable references.
            nc_var_att = getattr(nc_var, cls.cf_identity, None)

            if nc_var_att is not None:
                for match_item in _CF_PARSE.finditer(nc_var_att):
                    match_group = match_item.groupdict()
                    measure = match_group['lhs']
                    variable_name = match_group['rhs']

                    if variable_name not in ignore:
                        if variable_name not in netcdf_variable_names:
                            if warn:
                                message = 'Missing CF-netCDF measure variable %r, referenced by netCDF variable %r'
                                warnings.warn(message % (variable_name, nc_var_name))
                        else:
                            result[variable_name] = CFMeasureVariable(variable_name, variables[variable_name], measure)

        return result


################################################################################
class CFGroup(object, UserDict.DictMixin):
    """
    Represents a collection of 'NetCDF Climate and Forecast (CF) Metadata
    Conventions' variables and netCDF global attributes.

    """
    def __init__(self):       
        #: Collection of CF-netCDF variables
        self._cf_variables = {}
        #: Collection of netCDF global attributes
        self.global_attributes = {}

    def _cf_getter(self, cls):
        # Generate dictionary with dictionary comprehension.
        return {cf_name:cf_var for cf_name, cf_var in self._cf_variables.iteritems() if isinstance(cf_var, cls)}

    @property
    def ancillary_variables(self):
        """Collection of CF-netCDF ancillary variables."""
        return self._cf_getter(CFAncillaryDataVariable)

    @property
    def auxiliary_coordinates(self):
        """Collection of CF-netCDF auxiliary coordinate variables."""
        return self._cf_getter(CFAuxiliaryCoordinateVariable)

    @property
    def bounds(self):
        """Collection of CF-netCDF boundary variables."""
        return self._cf_getter(CFBoundaryVariable)

    @property
    def climatology(self):
        """Collection of CF-netCDF climatology variables."""
        return self._cf_getter(CFClimatologyVariable)

    @property
    def coordinates(self):
        """Collection of CF-netCDF coordinate variables."""
        return self._cf_getter(CFCoordinateVariable)

    @property
    def data_variables(self):
        """Collection of CF-netCDF data pay-load variables."""
        return self._cf_getter(CFDataVariable)

    @property
    def formula_terms(self):
        """Collection of CF-netCDF variables that participate in a CF-netCDF formula term."""
        return {cf_name:cf_var for cf_name, cf_var in self._cf_variables.iteritems() if cf_var.has_formula_terms()}

    @property
    def grid_mappings(self):
        """Collection of CF-netCDF grid mapping variables."""
        return self._cf_getter(CFGridMappingVariable)

    @property
    def labels(self):
        """Collection of CF-netCDF label variables."""
        return self._cf_getter(CFLabelVariable)

    @property
    def cell_measures(self):
        """Collection of CF-netCDF measure variables."""
        return self._cf_getter(CFMeasureVariable)

    def keys(self):
        """Return the names of all the CF-netCDF variables in the group."""
        return self._cf_variables.keys()

    def __setitem__(self, name, variable):
        if not isinstance(variable, CFVariable):
            raise TypeError('Attempted to add an invalid CF-netCDF variable to the %s' % self.__class__.__name__)

        if name != variable.cf_name:
            raise ValueError('Mismatch between key name %r and CF-netCDF variable name %r' % (str(name), variable.cf_name))

        self._cf_variables[name] = variable

    def __getitem__(self, name):
        if name not in self._cf_variables:
            raise KeyError('Cannot get unknown CF-netCDF variable name %r' % str(name))

        return self._cf_variables[name]

    def __delitem__(self, name):
        if name not in self._cf_variables:
            raise KeyError('Cannot delete unknown CF-netcdf variable name %r' % str(name))

        del self._cf_variables[name]

    def __repr__(self):
        result = []
        result.append('variables:%d' % len(self._cf_variables))
        result.append('global_attributes:%d' % len(self.global_attributes))

        return '<%s of %s>' % (self.__class__.__name__, ', '.join(result))


################################################################################
class CFReader(object):
    """
    This class allows the contents of a netCDF file to be interpreted according
    to the 'NetCDF Climate and Forecast (CF) Metadata Conventions'.

    """
    def __init__(self, filename, warn=False, monotonic=False):
        self._filename = os.path.expanduser(filename)
        # All CF variable types EXCEPT for the "special cases" of
        # CFDataVariable, CFCoordinateVariable and _CFFormulaTermsVariable.
        self._variable_types = (CFAncillaryDataVariable, CFAuxiliaryCoordinateVariable,
                                CFBoundaryVariable, CFClimatologyVariable,
                                CFGridMappingVariable, CFLabelVariable, CFMeasureVariable)
        
        #: Collection of CF-netCDF variables associated with this netCDF file
        self.cf_group = CFGroup()

        self._dataset = netCDF4.Dataset(self._filename, mode='r')

        # Issue load optimisation warning.
        if warn and self._dataset.file_format in ['NETCDF3_CLASSIC', 'NETCDF3_64BIT']:
            warnings.warn('Optimise CF-netCDF loading by converting data from NetCDF3 ' \
                          'to NetCDF4 file format using the "nccopy" command.')

        self._check_monotonic = monotonic

        self._translate()
        self._build_cf_groups()
        self._reset()

    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, self._filename)

    def _translate(self):
        """Classify the netCDF variables into CF-netCDF variables."""

        netcdf_variable_names = self._dataset.variables.keys()

        # Identify all CF coordinate variables first. This must be done
        # first as, by CF convention, the definition of a CF auxiliary
        # coordinate variable may include a scalar CF coordinate variable,
        # whereas we want these two types of variables to be mutually exclusive.
        coords = CFCoordinateVariable.identify(self._dataset.variables,
                                               monotonic=self._check_monotonic)
        self.cf_group.update(coords)
        coordinate_names = self.cf_group.coordinates.keys()

        # Identify all CF variables EXCEPT for the "special cases".
        for variable_type in self._variable_types:
            # Prevent grid mapping variables being mis-identified as CF coordinate variables.
            ignore = None if issubclass(variable_type, CFGridMappingVariable) else coordinate_names
            self.cf_group.update(variable_type.identify(self._dataset.variables, ignore=ignore))

        # Identify global netCDF attributes.
        attr_dict = {attr_name: getattr(self._dataset, attr_name, '') for
                        attr_name in self._dataset.ncattrs()}
        self.cf_group.global_attributes.update(attr_dict)

        # Identify and register all CF formula terms.
        formula_terms = _CFFormulaTermsVariable.identify(self._dataset.variables)

        for cf_var in formula_terms.itervalues():
            for cf_root, cf_term in cf_var.cf_terms_by_root.iteritems():
                # Ignore formula terms owned by a bounds variable.
                if cf_root not in self.cf_group.bounds:
                    cf_name = cf_var.cf_name
                    if cf_var.cf_name not in self.cf_group:
                        self.cf_group[cf_name] = CFAuxiliaryCoordinateVariable(cf_name, cf_var.cf_data)
                    self.cf_group[cf_name].add_formula_term(cf_root, cf_term)

        # Determine the CF data variables.
        data_variable_names = set(netcdf_variable_names) - set(self.cf_group.ancillary_variables) - \
                              set(self.cf_group.auxiliary_coordinates) - set(self.cf_group.bounds) - \
                              set(self.cf_group.climatology) - set(self.cf_group.coordinates) - \
                              set(self.cf_group.grid_mappings) - set(self.cf_group.labels) - \
                              set(self.cf_group.cell_measures)

        for name in data_variable_names:
            self.cf_group[name] = CFDataVariable(name, self._dataset.variables[name])

    def _build_cf_groups(self):
        """Build the first order relationships between CF-netCDF variables."""

        coordinate_names = self.cf_group.coordinates.keys()

        for cf_variable in self.cf_group.itervalues():
            cf_group = CFGroup()

            # Build CF variable relationships.
            for variable_type in self._variable_types:
                # Prevent grid mapping variables being mis-identified as
                # CF coordinate variables.
                ignore = None if issubclass(variable_type, CFGridMappingVariable) else coordinate_names
                match = variable_type.identify(self._dataset.variables, ignore=ignore,
                                               target=cf_variable.cf_name, warn=False)
                cf_group.update({name: self.cf_group[name] for name in match.iterkeys()})

            # Build CF data variable relationships.
            if isinstance(cf_variable, CFDataVariable):
                # Add global netCDF attributes.
                cf_group.global_attributes.update(self.cf_group.global_attributes)
                # Add appropriate "dimensioned" CF coordinate variables.
                cf_group.update({cf_name: self.cf_group[cf_name] for cf_name
                                    in cf_variable.dimensions if cf_name in
                                    self.cf_group.coordinates})
                # Add appropriate "dimensionless" CF coordinate variables.
                coordinates_attr = getattr(cf_variable, 'coordinates', '')
                cf_group.update({cf_name: self.cf_group[cf_name] for cf_name
                                    in coordinates_attr.split() if cf_name in
                                    self.cf_group.coordinates})
                # Add appropriate formula terms.
                for cf_var in self.cf_group.formula_terms.itervalues():
                    for cf_root in cf_var.cf_terms_by_root:
                        if cf_root in cf_group and cf_var.cf_name not in cf_group:
                            # Sanity check dimensionality.
                            dims = set(cf_var.dimensions)
                            if dims.issubset(cf_variable.dimensions):
                                cf_group[cf_var.cf_name] = cf_var
                            else:
                                msg = 'Ignoring formula terms variable {!r} ' \
                                    'referenced by data variable {!r}: ' \
                                    'dimension ' \
                                    'mis-match.'.format(cf_var.cf_name,
                                                        cf_variable.cf_name)
                                warnings.warn(msg)

            # Add the CF group to the variable.
            cf_variable.cf_group = cf_group

    def _reset(self):
        """Reset the attribute touch history of each variable."""

        for nc_var_name in self._dataset.variables.iterkeys():
            self.cf_group[nc_var_name].cf_attrs_reset()

    def __del__(self):
        # Explicitly close dataset to prevent file remaining open.
        self._dataset.close()

########NEW FILE########
__FILENAME__ = dot
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Provides Creation and saving of DOT graphs for a :class:`iris.cube.Cube`.

"""

import os
import subprocess

import iris
import iris.util


_GRAPH_INDENT = ' ' * 4
_SUBGRAPH_INDENT = ' ' * 8

_DOT_EXECUTABLE_PATH = iris.config.get_option('System', 'dot_path',
                                              default='dot')
if not os.path.exists(_DOT_EXECUTABLE_PATH):
    _DOT_EXECUTABLE_PATH = None
#: Whether the 'dot' program is present (required for "dotpng" output).
DOT_AVAILABLE = _DOT_EXECUTABLE_PATH is not None


def save(cube, target):
    """Save a dot representation of the cube.

    Args:

        * cube   - A :class:`iris.cube.Cube`.
        * target - A filename or open file handle.

    See also :func:`iris.io.save`.

    """
    if isinstance(target, basestring):
        dot_file = open(target, "wt")
    elif hasattr(target, "write"):
        if hasattr(target, "mode") and "b" in target.mode:
            raise ValueError("Target is binary")
        dot_file = target
    else:
        raise ValueError("Can only save dot to filename or filehandle")

    dot_file.write(cube_text(cube))

    if isinstance(target, basestring):
        dot_file.close()


def save_png(source, target, launch=False):
    """
    Produces a "dot" instance diagram by calling dot and optionally launching the resulting image.

    Args:

        * source - A :class:`iris.cube.Cube`, or dot filename.
        * target - A filename or open file handle.
                   If passing a file handle, take care to open it for binary output.

    Kwargs:

        * launch - Display the image. Default is False.

    See also :func:`iris.io.save`.

    """
    # From cube or dot file?
    if isinstance(source, iris.cube.Cube):
        # Create dot file
        dot_file_path = iris.util.create_temp_filename(".dot")
        save(source, dot_file_path)
    elif isinstance(source, basestring):
        dot_file_path = source
    else:
        raise ValueError("Can only write dot png for a Cube or DOT file")

    # Create png data
    if not _DOT_EXECUTABLE_PATH:
        raise ValueError('Executable "dot" not found: '
                         'Review dot_path setting in site.cfg.')
    # To filename or open file handle?
    if isinstance(target, basestring):
        subprocess.call([_DOT_EXECUTABLE_PATH, '-T', 'png', '-o', target,
                         dot_file_path])
    elif hasattr(target, "write"):
        if hasattr(target, "mode") and "b" not in target.mode:
            raise ValueError("Target not binary")
        subprocess.call([_DOT_EXECUTABLE_PATH, '-T', 'png', dot_file_path],
                        stdout=target)
    else:
        raise ValueError("Can only write dot png for a filename or writable")

    # Display?
    if launch:
        if os.name == 'mac':
            subprocess.call(('open', target))
        elif os.name == 'nt':
            subprocess.call(('start', target))
        elif os.name == 'posix':
            subprocess.call(('firefox', target))
        else:
            raise iris.exceptions.NotYetImplementedError('Unhandled operating system. The image has been created in %s' % target)

    # Remove the dot file if we created it
    if isinstance(source, iris.cube.Cube):
        os.remove(dot_file_path)


def cube_text(cube):
    """Return a DOT text representation a `iris.cube.Cube`.

    Args:

     * cube  -  The cube for which to create DOT text.

    """
    # We use r'' type string constructor as when we type \n in a string without the r'' constructor
    # we get back a new line character - this is not what DOT expects.
    # Therefore, newline characters should be created explicitly by having multi-lined strings.
    relationships = r''
    relationships_association = r''

    dimension_nodes = r'''
    subgraph clusterCubeDimensions {
        label="Cube data"
    '''

    # TODO: Separate dim_coords from aux_coords.
    coord_nodes = r'''
    subgraph clusterCoords {
        label = "Coords"
'''

    coord_system_nodes = r'''
    subgraph clusterCoordSystems {
        label = "CoordSystems"
'''

    for i, size in enumerate(cube.shape):
        dimension_nodes += '\n' + _dot_node(_SUBGRAPH_INDENT, 'CubeDimension_' + str(i), str(i), [('len', size)])

    # Coords and their coord_systems
    coords = sorted(cube.coords(), key=lambda c: c.name())
    written_cs = []
    for i, coord in enumerate(coords):
        coord_label = 'Coord_' + str(i)
        coord_nodes += _coord_text(coord_label, coord)
        cs = coord.coord_system
        if cs:
            # Create the cs node - or find an identical, already written cs
            if cs not in written_cs:
                written_cs.append(cs)
                uid = written_cs.index(cs)
                coord_system_nodes += _coord_system_text(cs, uid)
            else:
                uid = written_cs.index(cs)
            relationships += '\n    "%s" -> "CoordSystem_%s_%s"' % (coord_label, coord.coord_system.__class__.__name__, uid)
        relationships += '\n    ":Cube" -> "%s"' % coord_label

        # Are there any relationships to data dimensions?
        dims = cube.coord_dims(coord)
        for dim in dims:
            relationships_association += '\n    "%s" -> "CubeDimension_%s":w' % (coord_label, dim)


    dimension_nodes += '''
    }
    '''

    coord_nodes += '''
    }
    '''

    coord_system_nodes += '''
    }
    '''

    # return a string pulling everything together
    template = '''
digraph CubeGraph{

    rankdir = "LR"
    fontname = "Bitstream Vera Sans"
    fontsize = 8

    node [
        fontname = "Bitstream Vera Sans"
        fontsize = 8
        shape = "record"
    ]

#   Nodes
%(cube_node)s
    %(dimension_nodes)s
    %(coord_nodes)s
    %(coord_sys_nodes)s
    edge [
        arrowhead = "normal"
    ]

#   RELATIONSHIPS

#   Containment
    %(relationships)s
    edge [
        style="dashed"
        arrowhead = "onormal"
    ]

#   Association
    %(associations)s
}
    '''
    cube_attributes = [(name, value) for name, value in sorted(cube.attributes.iteritems(), key=lambda item: item[0])]
    cube_node = _dot_node(_GRAPH_INDENT, ':Cube', 'Cube', cube_attributes)
    res_string = template % {
                        'cube_node': cube_node,
                        'dimension_nodes': dimension_nodes,
                        'coord_nodes': coord_nodes,
                        'coord_sys_nodes': coord_system_nodes,
                        'relationships': relationships,
                        'associations': relationships_association
                    }
    return res_string


def _coord_text(label, coord):
    """
    Returns a string containing the dot representation for a single coordinate node.

    Args:

    * label
        The dot ID of the coordinate node.
    * coord
        The coordinate to convert.

    """
    # Which bits to write?
    # Note: This is is not very OO but we are achieving a separation of DOT from cdm by doing this.
    if isinstance(coord, iris.coords.DimCoord):
        _dot_attrs = ('standard_name', 'long_name', 'units', 'circular')
    elif isinstance(coord, iris.coords.AuxCoord):
        _dot_attrs = ('standard_name', 'long_name', 'units')
    else:
        raise ValueError("Unhandled coordinate type: " + str(type(coord)))
    attrs = [(name, getattr(coord, name)) for name in _dot_attrs]

    if coord.attributes:
        custom_attrs = sorted(coord.attributes.iteritems(), key=lambda item: item[0])
        attrs.extend(custom_attrs)

    node = _dot_node(_SUBGRAPH_INDENT, label, coord.__class__.__name__, attrs)
    return node


def _coord_system_text(cs, uid):
    """
    Returns a string containing the dot representation for a single coordinate system node.

    Args:

    * cs
        The coordinate system to convert.
    * uid
        The uid allows/distinguishes non-identical CoordSystems of the same type.

    """
    attrs = []
    for k, v in cs.__dict__.iteritems():
        if isinstance(v, iris.cube.Cube):
            attrs.append((k, 'defined'))
        else:
            attrs.append((k, v))

    attrs.sort(key=lambda attr: attr[0])

    label = "CoordSystem_%s_%s" % (cs.__class__.__name__, uid)
    node = _dot_node(_SUBGRAPH_INDENT, label, cs.__class__.__name__,  attrs)
    return node


def _dot_node(indent, id, name, attributes):
    """
    Returns a string containing the dot representation for a single node.

    Args:

     * id
        The ID of the node.
     * name
        The visual name of the node.
     * attributes
        An iterable of (name, value) attribute pairs.

    """
    attributes = r'\n'.join('%s: %s' % item for item in attributes)
    template = """%(indent)s"%(id)s" [
%(indent)s    label = "%(name)s|%(attributes)s"
%(indent)s]
"""
    node = template % {'id': id, 'name': name, 'attributes': attributes, 'indent': indent}
    return node

########NEW FILE########
__FILENAME__ = ff
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Provides UK Met Office Fields File (FF) format specific capabilities.

"""

import os
import warnings

import numpy as np

from iris.exceptions import NotYetImplementedError
from iris.fileformats._ff_cross_references import STASH_TRANS
import pp


IMDI = -32768

FF_HEADER_DEPTH = 256      # In words (64-bit).
DEFAULT_FF_WORD_DEPTH = 8  # In bytes.

# UM marker to signify empty lookup table entry.
_FF_LOOKUP_TABLE_TERMINATE = -99

# UM FieldsFile fixed length header names and positions.
UM_FIXED_LENGTH_HEADER = [
    ('data_set_format_version',    (1, )),
    ('sub_model',                  (2, )),
    ('vert_coord_type',            (3, )),
    ('horiz_grid_type',            (4, )),
    ('dataset_type',               (5, )),
    ('run_identifier',             (6, )),
    ('experiment_number',          (7, )),
    ('calendar',                   (8, )),
    ('grid_staggering',            (9, )),
    ('time_type',                  (10, )),
    ('projection_number',          (11, )),
    ('model_version',              (12, )),
    ('obs_file_type',              (14, )),
    ('last_fieldop_type',          (15, )),
    ('first_validity_time',        (21, 22, 23, 24, 25, 26, 27, )),
    ('last_validity_time',         (28, 29, 30, 31, 32, 33, 34, )),
    ('misc_validity_time',         (35, 36, 37, 38, 39, 40, 41, )),
    ('integer_constants',          (100, 101, )),
    ('real_constants',             (105, 106, )),
    ('level_dependent_constants',  (110, 111, 112, )),
    ('row_dependent_constants',    (115, 116, 117, )),
    ('column_dependent_constants', (120, 121, 122, )),
    ('fields_of_constants',        (125, 126, 127, )),
    ('extra_constants',            (130, 131, )),
    ('temp_historyfile',           (135, 136, )),
    ('compressed_field_index1',    (140, 141, )),
    ('compressed_field_index2',    (142, 143, )),
    ('compressed_field_index3',    (144, 145, )),
    ('lookup_table',               (150, 151, 152, )),
    ('total_prognostic_fields',    (153, )),
    ('data',                       (160, 161, 162, )), ]

# Offset value to convert from UM_FIXED_LENGTH_HEADER positions to
# FF_HEADER offsets.
UM_TO_FF_HEADER_OFFSET = 1
# Offset the UM_FIXED_LENGTH_HEADER positions to FF_HEADER offsets.
FF_HEADER = [
    (name, tuple(position - UM_TO_FF_HEADER_OFFSET for position in positions))
    for name, positions in UM_FIXED_LENGTH_HEADER]

# UM FieldsFile fixed length header pointer names.
_FF_HEADER_POINTERS = [
    'integer_constants',
    'real_constants',
    'level_dependent_constants',
    'row_dependent_constants',
    'column_dependent_constants',
    'fields_of_constants',
    'extra_constants',
    'temp_historyfile',
    'compressed_field_index1',
    'compressed_field_index2',
    'compressed_field_index3',
    'lookup_table',
    'data', ]

_LBUSER_DTYPE_LOOKUP = {1: '>f{word_depth}',
                        2: '>i{word_depth}',
                        3: '>i{word_depth}',
                        'default': '>f{word_depth}', }

#: Codes used in STASH_GRID which indicate the x coordinate is on the
#: edge of the cell.
X_COORD_U_GRID = (11, 18, 27)

#: Codes used in STASH_GRID which indicate the y coordinate is on the
#: edge of the cell.
Y_COORD_V_GRID = (11, 19, 28)

#: Grid codes found in the STASH master which are currently known to be
#: handled correctly. A warning is issued if a grid is found which is not
#: handled.
HANDLED_GRIDS = (1, 2, 3, 4, 5, 26, 29) + X_COORD_U_GRID + Y_COORD_V_GRID

# REAL constants header names as described by UM documentation paper F3.
# NB. These are zero-based indices as opposed to the one-based indices
# used in F3.
REAL_EW_SPACING = 0
REAL_NS_SPACING = 1
REAL_FIRST_LAT = 2
REAL_FIRST_LON = 3
REAL_POLE_LAT = 4
REAL_POLE_LON = 5


class Grid(object):
    """
    An abstract class representing the default/file-level grid
    definition for a FieldsFile.

    """
    def __init__(self, column_dependent_constants, row_dependent_constants,
                 real_constants, horiz_grid_type):
        """
        Create a Grid from the relevant sections of the FFHeader.

        Args:

        * column_dependent_constants (numpy.ndarray):
            The `column_dependent_constants` from a FFHeader.

        * row_dependent_constants (numpy.ndarray):
            The `row_dependent_constants` from a FFHeader.

        * real_constants (numpy.ndarray):
            The `real_constants` from a FFHeader.

        * horiz_grid_type (integer):
            `horiz_grid_type` from a FFHeader.

        """
        self.column_dependent_constants = column_dependent_constants
        self.row_dependent_constants = row_dependent_constants
        self.ew_spacing = real_constants[REAL_EW_SPACING]
        self.ns_spacing = real_constants[REAL_NS_SPACING]
        self.first_lat = real_constants[REAL_FIRST_LAT]
        self.first_lon = real_constants[REAL_FIRST_LON]
        self.pole_lat = real_constants[REAL_POLE_LAT]
        self.pole_lon = real_constants[REAL_POLE_LON]
        self.horiz_grid_type = horiz_grid_type

    def _x_vectors(self, subgrid):
        # Abstract method to return the X vector for the given sub-grid.
        raise NotImplementedError()

    def _y_vectors(self, subgrid):
        # Abstract method to return the X vector for the given sub-grid.
        raise NotImplementedError()

    def regular_x(self, subgrid):
        # Abstract method to return BZX, BDX for the given sub-grid.
        raise NotImplementedError()

    def regular_y(self, subgrid):
        # Abstract method to return BZY, BDY for the given sub-grid.
        raise NotImplementedError()

    def vectors(self, subgrid):
        """
        Return the X and Y coordinate vectors for the given sub-grid of
        this grid.

        Args:

        * subgrid (integer):
            A "grid type code" as described in UM documentation paper C4.

        Returns:
            A 2-tuple of X-vector, Y-vector.

        """
        x_p, x_u = self._x_vectors()
        y_p, y_v = self._y_vectors()
        x = x_p
        y = y_p
        if subgrid in X_COORD_U_GRID:
            x = x_u
        if subgrid in Y_COORD_V_GRID:
            y = y_v
        return x, y


class ArakawaC(Grid):
    """
    An abstract class representing an Arakawa C-grid.

    """
    def _x_vectors(self):
        x_p, x_u = None, None
        if self.column_dependent_constants is not None:
            x_p = self.column_dependent_constants[:, 0]
            if self.column_dependent_constants.shape[1] == 2:
                # Wrap around for global field
                if self.horiz_grid_type == 0:
                    x_u = self.column_dependent_constants[:-1, 1]
                else:
                    x_u = self.column_dependent_constants[:, 1]
        return x_p, x_u

    def regular_x(self, subgrid):
        """
        Return the "zeroth" value and step for the X coordinate on the
        given sub-grid of this grid.

        Args:

        * subgrid (integer):
            A "grid type code" as described in UM documentation paper C4.

        Returns:
            A 2-tuple of BZX, BDX.

        """
        bdx = self.ew_spacing
        bzx = self.first_lon - bdx
        if subgrid in X_COORD_U_GRID:
            bzx += 0.5 * bdx
        return bzx, bdx

    def regular_y(self, subgrid):
        """
        Return the "zeroth" value and step for the Y coordinate on the
        given sub-grid of this grid.

        Args:

        * subgrid (integer):
            A "grid type code" as described in UM documentation paper C4.

        Returns:
            A 2-tuple of BZY, BDY.

        """
        bdy = self.ns_spacing
        bzy = self.first_lat - bdy
        if subgrid in Y_COORD_V_GRID:
            bzy += self._v_offset * bdy
        return bzy, bdy


class NewDynamics(ArakawaC):
    """
    An Arakawa C-grid as used by UM New Dynamics.

    The theta and u points are at the poles.

    """

    _v_offset = 0.5

    def _y_vectors(self):
        y_p, y_v = None, None
        if self.row_dependent_constants is not None:
            y_p = self.row_dependent_constants[:, 0]
            if self.row_dependent_constants.shape[1] == 2:
                y_v = self.row_dependent_constants[:-1, 1]
        return y_p, y_v


class ENDGame(ArakawaC):
    """
    An Arakawa C-grid as used by UM ENDGame.

    The v points are at the poles.

    """

    _v_offset = -0.5

    def _y_vectors(self):
        y_p, y_v = None, None
        if self.row_dependent_constants is not None:
            y_p = self.row_dependent_constants[:-1, 0]
            if self.row_dependent_constants.shape[1] == 2:
                y_v = self.row_dependent_constants[:, 1]
        return y_p, y_v


class FFHeader(object):
    """A class to represent the FIXED_LENGTH_HEADER section of a FieldsFile."""

    GRID_STAGGERING_CLASS = {3: NewDynamics, 6: ENDGame}

    def __init__(self, filename, word_depth=DEFAULT_FF_WORD_DEPTH):
        """
        Create a FieldsFile header instance by reading the
        FIXED_LENGTH_HEADER section of the FieldsFile, making the names
        defined in FF_HEADER available as attributes of a FFHeader instance.

        Args:

        * filename (string):
            Specify the name of the FieldsFile.

        Returns:
            FFHeader object.

        """

        #: File name of the FieldsFile.
        self.ff_filename = filename
        self._word_depth = word_depth

        # Read the FF header data
        with open(filename, 'rb') as ff_file:
            # typically 64-bit words (aka. int64 or ">i8")
            header_data = np.fromfile(ff_file,
                                      dtype='>i{0}'.format(word_depth),
                                      count=FF_HEADER_DEPTH)
            header_data = tuple(header_data)
            # Create FF instance attributes
            for name, offsets in FF_HEADER:
                if len(offsets) == 1:
                    value = header_data[offsets[0]]
                else:
                    value = header_data[offsets[0]:offsets[-1] + 1]
                setattr(self, name, value)

            # Turn the pointer values into real arrays.
            for elem in _FF_HEADER_POINTERS:
                if elem not in ['data', 'lookup_table']:
                    if self._attribute_is_pointer_and_needs_addressing(elem):
                        addr = getattr(self, elem)
                        ff_file.seek((addr[0] - 1) * word_depth, os.SEEK_SET)
                        if len(addr) == 2:
                            res = np.fromfile(ff_file,
                                              dtype='>f{0}'.format(word_depth),
                                              count=addr[1])
                        elif len(addr) == 3:
                            res = np.fromfile(ff_file,
                                              dtype='>f{0}'.format(word_depth),
                                              count=addr[1]*addr[2])
                            res = res.reshape((addr[1], addr[2]), order='F')
                        else:
                            raise ValueError('ff header element {} is not'
                                             'handled correctly'.format(elem))
                    else:
                        res = None
                    setattr(self, elem, res)

    def __str__(self):
        attributes = []
        for name, _ in FF_HEADER:
            attributes.append('    {}: {}'.format(name, getattr(self, name)))
        return 'FF Header:\n' + '\n'.join(attributes)

    def __repr__(self):
        return '{}({!r})'.format(type(self).__name__, self.ff_filename)

    def _attribute_is_pointer_and_needs_addressing(self, name):
        if name in _FF_HEADER_POINTERS:
            attr = getattr(self, name)

            # Check that we haven't already addressed this pointer,
            # that the pointer is actually referenceable (i.e. >0)
            # and that the attribute is not marked as missing.
            is_referenceable = (isinstance(attr, tuple) and
                                attr[0] > 0 and attr[0] != IMDI)
        else:
            msg = '{!r} object does not have pointer attribute {!r}'
            raise AttributeError(msg.format(self.__class__.__name__, name))
        return is_referenceable

    def shape(self, name):
        """
        Return the dimension shape of the FieldsFile FIXED_LENGTH_HEADER
        pointer attribute.

        Args:

        * name (string):
            Specify the name of the FIXED_LENGTH_HEADER attribute.

        Returns:
            Dimension tuple.

        """

        if name in _FF_HEADER_POINTERS:
            value = getattr(self, name)[1:]
        else:
            msg = '{!r} object does not have pointer address {!r}'
            raise AttributeError(msg.format(self.__class_.__name__, name))
        return value

    def grid(self):
        """Return the Grid definition for the FieldsFile."""
        grid_class = self.GRID_STAGGERING_CLASS.get(self.grid_staggering)
        if grid_class is None:
            grid_class = NewDynamics
            warnings.warn(
                'Staggered grid type: {} not currently interpreted, assuming '
                'standard C-grid'.format(self.grid_staggering))
        grid = grid_class(self.column_dependent_constants,
                          self.row_dependent_constants,
                          self.real_constants, self.horiz_grid_type)
        return grid


class FF2PP(object):
    """A class to extract the individual PPFields from within a FieldsFile."""

    def __init__(self, filename, read_data=False,
                 word_depth=DEFAULT_FF_WORD_DEPTH):
        """
        Create a FieldsFile to Post Process instance that returns a generator
        of PPFields contained within the FieldsFile.

        Args:

        * filename (string):
            Specify the name of the FieldsFile.

        Kwargs:

        * read_data (boolean):
            Specify whether to read the associated PPField data within
            the FieldsFile.  Default value is False.

        Returns:
            PPField generator.

        For example::

            >>> for field in ff.FF2PP(filename):
            ...     print field

        """

        self._ff_header = FFHeader(filename, word_depth=word_depth)
        self._word_depth = word_depth
        self._filename = filename
        self._read_data = read_data

    def _payload(self, field):
        """Calculate the payload data depth (in bytes) and type."""
        lbpack_n1 = field.raw_lbpack % 10
        if lbpack_n1 == 0:
            # Data payload is not packed.
            data_depth = (field.lblrec - field.lbext) * self._word_depth
            # Determine PP field 64-bit payload datatype.
            lookup = _LBUSER_DTYPE_LOOKUP
            dtype_template = lookup.get(field.lbuser[0], lookup['default'])
            dtype_name = dtype_template.format(word_depth=self._word_depth)
            data_type = np.dtype(dtype_name)
        else:
            # Data payload is packed.
            if lbpack_n1 == 1:
                # Data packed using WGDOS archive method.
                data_depth = ((field.lbnrec * 2) - 1) * pp.PP_WORD_DEPTH
            elif lbpack_n1 == 2:
                # Data packed using CRAY 32-bit method.
                data_depth = (field.lblrec - field.lbext) * pp.PP_WORD_DEPTH
            else:
                msg = 'PP fields with LBPACK of {} are not supported.'
                raise NotYetImplementedError(msg.format(field.raw_lbpack))

            # Determine PP field payload datatype.
            lookup = pp.LBUSER_DTYPE_LOOKUP
            data_type = lookup.get(field.lbuser[0], lookup['default'])

        return data_depth, data_type

    def _det_border(self, field_dim, halo_dim):
        # Update field coordinates for a variable resolution LBC file where
        # the resolution of the very edge (within the rim width) is assumed to
        # be same as the halo.
        def range_order(range1, range2, resolution):
            # Handles whether increasing/decreasing ranges.
            if np.sign(resolution) > 0:
                lower = range1
                upper = range2
            else:
                upper = range1
                lower = range2
            return lower, upper

        # Ensure that the resolution is the same on both edges.
        res_low = np.array([field_dim[1] - field_dim[0]])
        res_high = np.array([field_dim[-1] - field_dim[-2]])
        if not np.allclose(res_low, res_high):
            msg = ('The x or y coordinates of your boundary condition field '
                   'may be incorrect, not having taken into account the '
                   'boundary size.')
            warnings.warn(msg)
        else:
            range2 = field_dim[0] - res_low
            range1 = field_dim[0] - halo_dim * res_low
            lower, upper = range_order(range1, range2, res_low)
            extra_before = np.linspace(lower, upper, halo_dim)

            range1 = field_dim[-1] + res_high
            range2 = field_dim[-1] + halo_dim * res_high
            lower, upper = range_order(range1, range2, res_high)
            extra_after = np.linspace(lower, upper, halo_dim)

            field_dim = np.concatenate([extra_before, field_dim, extra_after])
        return field_dim

    def _extract_field(self):
        # FF table pointer initialisation based on FF LOOKUP table
        # configuration.

        lookup_table = self._ff_header.lookup_table
        table_index, table_entry_depth, table_count = lookup_table
        table_offset = (table_index - 1) * self._word_depth       # in bytes
        table_entry_depth = table_entry_depth * self._word_depth  # in bytes
        # Open the FF for processing.
        ff_file = open(self._ff_header.ff_filename, 'rb')
        ff_file_seek = ff_file.seek

        # Check for an instantaneous dump.
        if self._ff_header.dataset_type == 1:
            table_count = self._ff_header.total_prognostic_fields

        is_boundary_packed = self._ff_header.dataset_type == 5

        grid = self._ff_header.grid()

        # Process each FF LOOKUP table entry.
        while table_count:
            table_count -= 1
            # Move file pointer to the start of the current FF LOOKUP
            # table entry.
            ff_file_seek(table_offset, os.SEEK_SET)
            # Read the current PP header entry from the FF LOOKUP table.
            header_longs = np.fromfile(
                ff_file, dtype='>i{0}'.format(self._word_depth),
                count=pp.NUM_LONG_HEADERS)
            # Check whether the current FF LOOKUP table entry is valid.
            if header_longs[0] == _FF_LOOKUP_TABLE_TERMINATE:
                # There are no more FF LOOKUP table entries to read.
                break
            header_floats = np.fromfile(
                ff_file, dtype='>f{0}'.format(self._word_depth),
                count=pp.NUM_FLOAT_HEADERS)
            header = tuple(header_longs) + tuple(header_floats)

            # Calculate next FF LOOKUP table entry.
            table_offset += table_entry_depth
            # Construct a PPField object and populate using the header_data
            # read from the current FF LOOKUP table.
            # (The PPField sub-class will depend on the header release number.)
            field = pp.make_pp_field(header)
            # Calculate start address of the associated PP header data.
            data_offset = field.lbegin * self._word_depth
            # Determine PP field payload depth and type.
            data_depth, data_type = self._payload(field)

            # Fast stash look-up.
            stash_s = field.lbuser[3] / 1000
            stash_i = field.lbuser[3] % 1000
            stash = 'm{:02}s{:02}i{:03}'.format(field.lbuser[6],
                                                stash_s, stash_i)
            stash_entry = STASH_TRANS.get(stash, None)
            if stash_entry is None:
                subgrid = None
                warnings.warn('The STASH code {0} was not found in the '
                              'STASH to grid type mapping. Picking the P '
                              'position as the cell type'.format(stash))
            else:
                subgrid = stash_entry.grid_code
                if subgrid not in HANDLED_GRIDS:
                    warnings.warn('The stash code {} is on a grid {} which '
                                  'has not been explicitly handled by the '
                                  'fieldsfile loader. Assuming the data is on '
                                  'a P grid.'.format(stash, subgrid))

            field.x, field.y = grid.vectors(subgrid)

            # Use the per-file grid if no per-field metadata is available.
            no_x = field.bzx in (0, field.bmdi) and field.x is None
            no_y = field.bzy in (0, field.bmdi) and field.y is None
            if no_x and no_y:
                field.bzx, field.bdx = grid.regular_x(subgrid)
                field.bzy, field.bdy = grid.regular_y(subgrid)
                field.bplat = grid.pole_lat
                field.bplon = grid.pole_lon
            elif no_x or no_y:
                warnings.warn('Partially missing X or Y coordinate values.')

            if is_boundary_packed:
                name_mapping = dict(rim_width=slice(4, 6), y_halo=slice(2, 4),
                                    x_halo=slice(0, 2))
                b_packing = pp.SplittableInt(field.lbuser[2], name_mapping)
                field.lbpack.boundary_packing = b_packing
                # Fix the lbrow and lbnpt to be the actual size of the data
                # array, since the field is no longer a "boundary" fields file
                # field.
                # Note: The documentation states that lbrow (y) doesn't
                # contain the halo rows, but no such comment exists at UM v8.5
                # for lbnpt (x). Experimentation has shown that lbnpt also
                # excludes the halo size.
                field.lbrow += 2 * field.lbpack.boundary_packing.y_halo
                field.lbnpt += 2 * field.lbpack.boundary_packing.x_halo
                # Update the x and y coordinates for this field. Note: it may
                # be that this needs to update x and y also, but that is yet
                # to be confirmed.
                if (field.bdx in (0, field.bmdi) or
                        field.bdy in (0, field.bmdi)):
                    field.x = self._det_border(field.x, b_packing.x_halo)
                    field.y = self._det_border(field.y, b_packing.y_halo)
                else:
                    if field.bdy < 0:
                        warnings.warn('The LBC has a bdy less than 0. No '
                                      'case has previously been seen of '
                                      'this, and the decompression may be '
                                      'erroneous.')
                    field.bzx -= field.bdx * b_packing.x_halo
                    field.bzy -= field.bdy * b_packing.y_halo

            if self._read_data:
                # Read the actual bytes. This can then be converted to a
                # numpy array at a higher level.
                ff_file_seek(data_offset, os.SEEK_SET)
                field._data = pp.LoadedArrayBytes(ff_file.read(data_depth),
                                                  data_type)
            else:
                # Provide enough context to read the data bytes later on.
                field._data = (self._filename, data_offset,
                               data_depth, data_type)
            yield field
        ff_file.close()

    def __iter__(self):
        return pp._interpret_fields(self._extract_field())


def load_cubes(filenames, callback):
    """
    Loads cubes from a list of fields files filenames.

    Args:

    * filenames - list of fields files filenames to load

    Kwargs:

    * callback - a function which can be passed on to
        :func:`iris.io.run_callback`

    .. note::

        The resultant cubes may not be in the order that they are in the
        file (order is not preserved when there is a field with
        orography references).

    """
    return pp._load_cubes_variable_loader(filenames, callback, FF2PP)


def load_cubes_32bit_ieee(filenames, callback):
    """
    Loads cubes from a list of 32bit ieee converted fieldsfiles filenames.

    .. seealso::

        :func:`load_cubes` for keyword details

    """
    return pp._load_cubes_variable_loader(filenames, callback, FF2PP,
                                          {'word_depth': 4})

########NEW FILE########
__FILENAME__ = grib_phenom_translation
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
'''
Provide grib 1 and 2 phenomenon translations to + from CF terms.

This is done by wrapping '_grib_cf_map.py',
which is in a format provided by the metadata translation project.

Currently supports only these ones:

* grib1 --> cf
* grib2 --> cf
* cf --> grib2

'''

import collections
import warnings

import numpy as np

from iris.fileformats.grib import _grib_cf_map as grcf
import iris.std_names
import iris.unit


class LookupTable(dict):
    """
    Specialised dictionary object for making lookup tables.

    Returns None for unknown keys (instead of raising exception).
    Raises exception for any attempt to change an existing entry,
    (but it is still possible to remove keys)

    """
    def __init__(self, *args, **kwargs):
        self._super = super(LookupTable, self)
        self._super.__init__(*args, **kwargs)

    def __getitem__(self, key):
        if key not in self:
            return None
        return self._super.__getitem__(key)

    def __setitem__(self, key, value):
        if key in self and self[key] is not value:
            raise KeyError('Attempted to set dict[{}] = {}, '
                           'but this is already set to {}.'.format(
                               key, value, self[key]))
        self._super.__setitem__(key, value)


# Define namedtuples for keys+values of the Grib1 lookup table.

_Grib1ToCfKeyClass = collections.namedtuple(
    'Grib1CfKey',
    ('table2_version', 'centre_number', 'param_number'))

# NOTE: this form is currently used for both Grib1 *and* Grib2
_GribToCfDataClass = collections.namedtuple(
    'Grib1CfData',
    ('standard_name', 'long_name', 'units', 'set_height'))


# Create the grib1-to-cf lookup table.

def _make_grib1_cf_table():
    """ Build the Grib1 to CF phenomenon translation table. """
    table = LookupTable()

    def _make_grib1_cf_entry(table2_version, centre_number, param_number,
                             standard_name, long_name, units, set_height=None):
        """
        Check data, convert types and create a new _GRIB1_CF_TABLE key/value.

        Note that set_height is an optional parameter.  Used to denote
        phenomena that imply a height definition (agl),
        e.g. "2-metre tempererature".

        """
        grib1_key = _Grib1ToCfKeyClass(table2_version=int(table2_version),
                                       centre_number=int(centre_number),
                                       param_number=int(param_number))
        if standard_name is not None:
            if standard_name not in iris.std_names.STD_NAMES:
                warnings.warn('{} is not a recognised CF standard name '
                              '(skipping).'.format(standard_name))
                return None
        # convert units string to iris Unit (i.e. mainly, check it is good)
        iris_units = iris.unit.Unit(units)
        cf_data = _GribToCfDataClass(standard_name=standard_name,
                                     long_name=long_name,
                                     units=iris_units,
                                     set_height=set_height)
        return (grib1_key, cf_data)

    # Interpret the imported Grib1-to-CF table.
    for (grib1data, cfdata) in grcf.GRIB1_LOCAL_TO_CF.iteritems():
        assert grib1data.edition == 1
        association_entry = _make_grib1_cf_entry(
            table2_version=grib1data.t2version,
            centre_number=grib1data.centre,
            param_number=grib1data.iParam,
            standard_name=cfdata.standard_name,
            long_name=cfdata.long_name,
            units=cfdata.units)
        if association_entry is not None:
            key, value = association_entry
            table[key] = value

    # Do the same for special Grib1 codes that include an implied height level.
    for (grib1data, (cfdata, extra_dimcoord)) \
            in grcf.GRIB1_LOCAL_TO_CF_CONSTRAINED.iteritems():
        assert grib1data.edition == 1
        if extra_dimcoord.standard_name != 'height':
            raise ValueError('Got implied dimension coord of "{}", '
                             'currently can only handle "height".'.format(
                                 extra_dimcoord.standard_name))
        if extra_dimcoord.units != 'm':
            raise ValueError('Got implied dimension units of "{}", '
                             'currently can only handle "m".'.format(
                                 extra_dimcoord.units))
        if len(extra_dimcoord.points) != 1:
            raise ValueError('Implied dimension has {} points, '
                             'currently can only handle 1.'.format(
                                 len(extra_dimcoord.points)))
        association_entry = _make_grib1_cf_entry(
            table2_version=int(grib1data.t2version),
            centre_number=int(grib1data.centre),
            param_number=int(grib1data.iParam),
            standard_name=cfdata.standard_name,
            long_name=cfdata.long_name,
            units=cfdata.units,
            set_height=extra_dimcoord.points[0])
        if association_entry is not None:
            key, value = association_entry
            table[key] = value

    return table


_GRIB1_CF_TABLE = _make_grib1_cf_table()


# Define a namedtuple for the keys of the Grib2 lookup table.

_Grib2ToCfKeyClass = collections.namedtuple(
    'Grib2CfKey',
    ('param_discipline', 'param_category', 'param_number'))


# Create the grib2-to-cf lookup table.

def _make_grib2_to_cf_table():
    """ Build the Grib2 to CF phenomenon translation table. """
    table = LookupTable()

    def _make_grib2_cf_entry(param_discipline, param_category, param_number,
                             standard_name, long_name, units):
        """
        Check data, convert types and make a _GRIB2_CF_TABLE key/value pair.

        Note that set_height is an optional parameter.  Used to denote
        phenomena that imply a height definition (agl),
        e.g. "2-metre tempererature".

        """
        grib2_key = _Grib2ToCfKeyClass(param_discipline=int(param_discipline),
                                       param_category=int(param_category),
                                       param_number=int(param_number))
        if standard_name is not None:
            if standard_name not in iris.std_names.STD_NAMES:
                warnings.warn('{} is not a recognised CF standard name '
                              '(skipping).'.format(standard_name))
                return None
        # convert units string to iris Unit (i.e. mainly, check it is good)
        iris_units = iris.unit.Unit(units)
        cf_data = _GribToCfDataClass(standard_name=standard_name,
                                     long_name=long_name,
                                     units=iris_units,
                                     set_height=None)
        return (grib2_key, cf_data)

    # Interpret the grib2 info from grib_cf_map
    for grib2data, cfdata in grcf.GRIB2_TO_CF.iteritems():
        assert grib2data.edition == 2
        association_entry = _make_grib2_cf_entry(
            param_discipline=grib2data.discipline,
            param_category=grib2data.category,
            param_number=grib2data.number,
            standard_name=cfdata.standard_name,
            long_name=cfdata.long_name,
            units=cfdata.units)
        if association_entry is not None:
            key, value = association_entry
            table[key] = value

    return table


_GRIB2_CF_TABLE = _make_grib2_to_cf_table()


# Define namedtuples for key+values of the cf-to-grib2 lookup table.

_CfToGrib2KeyClass = collections.namedtuple(
    'CfGrib2Key',
    ('standard_name', 'long_name'))

_CfToGrib2DataClass = collections.namedtuple(
    'CfGrib2Data',
    ('discipline', 'category', 'number', 'units'))


# Create the cf-to-grib2 lookup table.

def _make_cf_to_grib2_table():
    """ Build the Grib1 to CF phenomenon translation table. """
    table = LookupTable()

    def _make_cf_grib2_entry(standard_name, long_name,
                             param_discipline, param_category, param_number,
                             units):
        """
        Check data, convert types and make a new _CF_TABLE key/value pair.

        """
        assert standard_name is not None or long_name is not None
        if standard_name is not None:
            long_name = None
            if standard_name not in iris.std_names.STD_NAMES:
                warnings.warn('{} is not a recognised CF standard name '
                              '(skipping).'.format(standard_name))
                return None
        cf_key = _CfToGrib2KeyClass(standard_name, long_name)
        # convert units string to iris Unit (i.e. mainly, check it is good)
        iris_units = iris.unit.Unit(units)
        grib2_data = _CfToGrib2DataClass(discipline=int(param_discipline),
                                         category=int(param_category),
                                         number=int(param_number),
                                         units=iris_units)
        return (cf_key, grib2_data)

    # Interpret the imported CF-to-Grib2 table into a lookup table
    for cfdata, grib2data in grcf.CF_TO_GRIB2.iteritems():
        assert grib2data.edition == 2
        iris_units = iris.unit.Unit(cfdata.units)
        association_entry = _make_cf_grib2_entry(
            standard_name=cfdata.standard_name,
            long_name=cfdata.long_name,
            param_discipline=grib2data.discipline,
            param_category=grib2data.category,
            param_number=grib2data.number,
            units=iris_units)
        if association_entry is not None:
            key, value = association_entry
            table[key] = value

    return table

_CF_GRIB2_TABLE = _make_cf_to_grib2_table()


# Interface functions for translation lookup

def grib1_phenom_to_cf_info(table2_version, centre_number, param_number):
    """
    Lookup grib-1 parameter --> cf_data or None.

    Returned cf_data has attributes:

    * standard_name
    * long_name
    * units : a :class:`iris.unit.Unit`
    * set_height :  a scalar 'height' value , or None

    """
    grib1_key = _Grib1ToCfKeyClass(table2_version=table2_version,
                                   centre_number=centre_number,
                                   param_number=param_number)
    return _GRIB1_CF_TABLE[grib1_key]


def grib2_phenom_to_cf_info(param_discipline, param_category, param_number):
    """
    Lookup grib-2 parameter --> cf_data or None.

    Returned cf_data has attributes:

    * standard_name
    * long_name
    * units : a :class:`iris.unit.Unit`

    """
    grib2_key = _Grib2ToCfKeyClass(param_discipline=int(param_discipline),
                                   param_category=int(param_category),
                                   param_number=int(param_number))
    return _GRIB2_CF_TABLE[grib2_key]


def cf_phenom_to_grib2_info(standard_name, long_name=None):
    """
    Lookup CF names --> grib2_data or None.

    Returned grib2_data has attributes:

    * discipline
    * category
    * number
    * units : a :class:`iris.unit.Unit`
        The unit represents the defined reference units for the message data.

    """
    if standard_name is not None:
        long_name = None
    return _CF_GRIB2_TABLE[(standard_name, long_name)]

########NEW FILE########
__FILENAME__ = grib_save_rules
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


import warnings

import gribapi
import numpy as np
import numpy.ma as ma

import iris
import iris.exceptions
import iris.unit
from iris.fileformats.rules import is_regular, regular_step
from iris.fileformats.grib import grib_phenom_translation as gptx


def gribbability_check(cube):
    "We always need the following things for grib saving."

    # GeogCS exists?
    cs0 = cube.coord(dimensions=[0]).coord_system
    cs1 = cube.coord(dimensions=[1]).coord_system
    if cs0 is None or cs1 is None:
        raise iris.exceptions.TranslationError("CoordSystem not present")
    if cs0 != cs1:
        raise iris.exceptions.TranslationError("Inconsistent CoordSystems")

    # Regular?
    y_coord = cube.coord(dimensions=[0])
    x_coord = cube.coord(dimensions=[1])
    if not is_regular(x_coord) or not is_regular(y_coord):
        raise iris.exceptions.TranslationError(
            "Cannot save irregular grids to grib")

    # Time period exists?
    if not cube.coords("time"):
        raise iris.exceptions.TranslationError("time coord not found")


# ###########################
# ### grid template stuff ###
# ###########################


def shape_of_the_earth(cube, grib):

    # assume latlon
    cs = cube.coord(dimensions=[0]).coord_system

    # Turn them all missing to start with (255 for byte, -1 for long)
    gribapi.grib_set_long(grib, "scaleFactorOfRadiusOfSphericalEarth", 255)
    gribapi.grib_set_long(grib, "scaledValueOfRadiusOfSphericalEarth", -1)
    gribapi.grib_set_long(grib, "scaleFactorOfEarthMajorAxis", 255)
    gribapi.grib_set_long(grib, "scaledValueOfEarthMajorAxis", -1)
    gribapi.grib_set_long(grib, "scaleFactorOfEarthMinorAxis", 255)
    gribapi.grib_set_long(grib, "scaledValueOfEarthMinorAxis", -1)

    ellipsoid = cs
    if isinstance(cs, iris.coord_systems.RotatedGeogCS):
        ellipsoid = cs.ellipsoid

    if ellipsoid.inverse_flattening == 0.0:
        gribapi.grib_set_long(grib, "shapeOfTheEarth", 1)
        gribapi.grib_set_long(grib, "scaleFactorOfRadiusOfSphericalEarth", 0)
        gribapi.grib_set_long(grib, "scaledValueOfRadiusOfSphericalEarth",
                              ellipsoid.semi_major_axis)
    else:
        gribapi.grib_set_long(grib, "shapeOfTheEarth", 7)
        gribapi.grib_set_long(grib, "scaleFactorOfEarthMajorAxis", 0)
        gribapi.grib_set_long(grib, "scaledValueOfEarthMajorAxis",
                              ellipsoid.semi_major_axis)
        gribapi.grib_set_long(grib, "scaleFactorOfEarthMinorAxis", 0)
        gribapi.grib_set_long(grib, "scaledValueOfEarthMinorAxis",
                              ellipsoid.semi_minor_axis)


def grid_dims(x_coord, y_coord, grib):
    gribapi.grib_set_long(grib, "Ni", x_coord.shape[0])
    gribapi.grib_set_long(grib, "Nj", y_coord.shape[0])


def latlon_first_last(x_coord, y_coord, grib):
    if x_coord.has_bounds() or y_coord.has_bounds():
        warnings.warn("Ignoring xy bounds")

# XXX Pending #1125
#    gribapi.grib_set_double(grib, "latitudeOfFirstGridPointInDegrees",
#                            float(y_coord.points[0]))
#    gribapi.grib_set_double(grib, "latitudeOfLastGridPointInDegrees",
#                            float(y_coord.points[-1]))
#    gribapi.grib_set_double(grib, "longitudeOfFirstGridPointInDegrees",
#                            float(x_coord.points[0]))
#    gribapi.grib_set_double(grib, "longitudeOfLastGridPointInDegrees",
#                            float(x_coord.points[-1]))
# WORKAROUND
    gribapi.grib_set_long(grib, "latitudeOfFirstGridPoint",
                          int(y_coord.points[0]*1000000))
    gribapi.grib_set_long(grib, "latitudeOfLastGridPoint",
                          int(y_coord.points[-1]*1000000))
    gribapi.grib_set_long(grib, "longitudeOfFirstGridPoint",
                          int((x_coord.points[0] % 360)*1000000))
    gribapi.grib_set_long(grib, "longitudeOfLastGridPoint",
                          int((x_coord.points[-1] % 360)*1000000))


def dx_dy(x_coord, y_coord, grib):
    x_step = regular_step(x_coord)
    y_step = regular_step(y_coord)
    # TODO: THIS USED BE "Dx" and "Dy"!!! DID THE API CHANGE AGAIN???
    gribapi.grib_set_double(grib, "DxInDegrees", float(abs(x_step)))
    gribapi.grib_set_double(grib, "DyInDegrees", float(abs(y_step)))


def scanning_mode_flags(x_coord, y_coord, grib):
    gribapi.grib_set_long(grib, "iScansPositively",
                          int(x_coord.points[1] - x_coord.points[0] > 0))
    gribapi.grib_set_long(grib, "jScansPositively",
                          int(y_coord.points[1] - y_coord.points[0] > 0))


def latlon_common(cube, grib):
    y_coord = cube.coord(dimensions=[0])
    x_coord = cube.coord(dimensions=[1])
    shape_of_the_earth(cube, grib)
    grid_dims(x_coord, y_coord, grib)
    latlon_first_last(x_coord, y_coord, grib)
    dx_dy(x_coord, y_coord, grib)
    scanning_mode_flags(x_coord, y_coord, grib)


def rotated_pole(cube, grib):
    cs = cube.coord(dimensions=[0]).coord_system

# XXX Pending #1125
#    gribapi.grib_set_double(grib, "latitudeOfSouthernPoleInDegrees",
#                            float(cs.n_pole.latitude))
#    gribapi.grib_set_double(grib, "longitudeOfSouthernPoleInDegrees",
#                            float(cs.n_pole.longitude))
#    gribapi.grib_set_double(grib, "angleOfRotationInDegrees", 0)
# WORKAROUND
    latitude = -int(cs.grid_north_pole_latitude*1000000)
    longitude = int(((cs.grid_north_pole_longitude+180) % 360)*1000000)
    gribapi.grib_set_long(grib, "latitudeOfSouthernPole", latitude)
    gribapi.grib_set_long(grib, "longitudeOfSouthernPole", longitude)
    gribapi.grib_set_long(grib, "angleOfRotation", 0)


def grid_template(cube, grib):
    cs = cube.coord(dimensions=[0]).coord_system
    if isinstance(cs, iris.coord_systems.GeogCS):
        # template 3.0
        gribapi.grib_set_long(grib, "gridDefinitionTemplateNumber", 0)
        latlon_common(cube, grib)

    # rotated
    elif isinstance(cs, iris.coord_systems.RotatedGeogCS):
        # template 3.1
        gribapi.grib_set_long(grib, "gridDefinitionTemplateNumber", 1)
        latlon_common(cube, grib)
        rotated_pole(cube, grib)
    else:
        raise ValueError("Currently unhandled CoordSystem: %s" % cs)


# ##############################
# ### product template stuff ###
# ##############################


def param_code(cube, grib):
    # NOTE: for now, can match by *either* standard_name or long_name.
    # This allows workarounds for data with no identified standard_name.
    grib2_info = gptx.cf_phenom_to_grib2_info(cube.standard_name,
                                              cube.long_name)
    if grib2_info is not None:
        gribapi.grib_set_long(grib, "discipline",
                              int(grib2_info.discipline))
        gribapi.grib_set_long(grib, "parameterCategory",
                              int(grib2_info.category))
        gribapi.grib_set_long(grib, "parameterNumber",
                              int(grib2_info.number))
    else:
        gribapi.grib_set_long(grib, "discipline", 255)
        gribapi.grib_set_long(grib, "parameterCategory", 255)
        gribapi.grib_set_long(grib, "parameterNumber", 255)
        warnings.warn('Unable to determine Grib2 parameter code for cube.\n'
                      'discipline, parameterCategory and parameterNumber '
                      'have been set to "missing".')


def generating_process_type(cube, grib):
    # analysis = 0
    # initialisation = 1
    # forecast = 2
    # more...

    # missing
    gribapi.grib_set_long(grib, "typeOfGeneratingProcess", 255)


def background_process_id(cube, grib):
    # locally defined
    gribapi.grib_set_long(grib, "backgroundProcess", 255)


def generating_process_id(cube, grib):
    # locally defined
    gribapi.grib_set_long(grib, "generatingProcessIdentifier", 255)


def obs_time_after_cutoff(cube, grib):
    # nothing stored in iris for this at present
    gribapi.grib_set_long(grib, "hoursAfterDataCutoff", 0)
    gribapi.grib_set_long(grib, "minutesAfterDataCutoff", 0)


def _non_missing_forecast_period(cube):
    # Calculate "model start time" to use as the reference time.
    fp_coord = cube.coord("forecast_period")

    # Convert fp and t to hours so we can subtract to calculate R.
    cf_fp_hrs = fp_coord.units.convert(fp_coord.points[0], 'hours')
    t_coord = cube.coord("time").copy()
    hours_since = iris.unit.Unit("hours since epoch",
                                 calendar=t_coord.units.calendar)
    t_coord.convert_units(hours_since)

    rt_num = t_coord.points[0] - cf_fp_hrs
    rt = hours_since.num2date(rt_num)
    rt_meaning = 1  # "start of forecast"

    # Forecast period
    if fp_coord.units == iris.unit.Unit("hours"):
        grib_time_code = 1
    elif fp_coord.units == iris.unit.Unit("minutes"):
        grib_time_code = 0
    elif fp_coord.units == iris.unit.Unit("seconds"):
        grib_time_code = 13
    else:
        raise iris.exceptions.TranslationError(
            "Unexpected units for 'forecast_period' : %s" % fp_coord.units)

    if not t_coord.has_bounds():
        fp = fp_coord.points[0]
    else:
        if not fp_coord.has_bounds():
            raise iris.exceptions.TranslationError(
                "bounds on 'time' coordinate requires bounds on"
                " 'forecast_period'.")
        fp = fp_coord.bounds[0][0]

    if fp - int(fp):
        warnings.warn("forecast_period encoding problem: "
                      "scaling required.")
    fp = int(fp)

    # Turn negative forecast times into grib negative numbers?
    from iris.fileformats.grib import hindcast_workaround
    if hindcast_workaround and fp < 0:
        msg = "Encoding negative forecast period from {} to ".format(fp)
        fp = 2**31 + abs(fp)
        msg += "{}".format(np.int32(fp))
        warnings.warn(msg)

    return rt, rt_meaning, fp, grib_time_code


def _missing_forecast_period(cube):
    # We have no way of knowing the CF forecast reference time.
    # Set GRIB reference time to "verifying time of forecast",
    # and the forecast period to 0h.
    warnings.warn('No CF forecast_period. Setting reference time to mean '
                  '"verifying time of forecast", "forecast time" = 0h')

    t_coord = cube.coord("time")
    t = t_coord.bounds[0, 0] if t_coord.has_bounds() else t_coord.points[0]
    rt = t_coord.units.num2date(t)
    rt_meaning = 2  # "verification time of forecast"

    fp = 0
    fp_meaning = 1  # hours

    return rt, rt_meaning, fp, fp_meaning


def time_range(cube, grib):
    """Grib encoding of forecast_period."""
    try:
        fp_coord = cube.coord("forecast_period")
    except iris.exceptions.CoordinateNotFoundError:
        fp_coord = None

    if fp_coord is not None:
        _, _, fp, grib_time_code = _non_missing_forecast_period(cube)
    else:
        _, _, fp, grib_time_code = _missing_forecast_period(cube)

    gribapi.grib_set_long(grib, "indicatorOfUnitOfTimeRange", grib_time_code)
    gribapi.grib_set_long(grib, "forecastTime", fp)


def hybrid_surfaces(cube, grib):
    is_hybrid = False
# XXX Addressed in #1118 pending #1039 for hybrid levels
#
#    # hybrid height? (assume points)
#    if cube.coords("model_level") and cube.coords("level_height") and \
#       cube.coords("sigma") and \
#       isinstance(cube.coord("sigma").coord_system,
#                  iris.coord_systems.HybridHeightCS):
#        is_hybrid = True
#        gribapi.grib_set_long(grib, "typeOfFirstFixedSurface", 118)
#        gribapi.grib_set_long(grib, "scaledValueOfFirstFixedSurface",
#                              long(cube.coord("model_level").points[0]))
#        gribapi.grib_set_long(grib, "PVPresent", 1)
#        gribapi.grib_set_long(grib, "numberOfVerticalCoordinateValues", 2)
#        level_height = cube.coord("level_height").points[0]
#        sigma = cube.coord("sigma").points[0]
#        gribapi.grib_set_double_array(grib, "pv", [level_height, sigma])
#
#    # hybrid pressure?
#    if XXX:
#        pass
    return is_hybrid


def non_hybrid_surfaces(cube, grib):

    # Look for something we can export
    v_coord = grib_v_code = output_unit = None

    # pressure
    if cube.coords("air_pressure") or cube.coords("pressure"):
        grib_v_code = 100
        output_unit = iris.unit.Unit("Pa")
        v_coord = (cube.coords("air_pressure") or cube.coords("pressure"))[0]

    # altitude
    elif cube.coords("altitude"):
        grib_v_code = 102
        output_unit = iris.unit.Unit("m")
        v_coord = cube.coord("altitude")

    # height
    elif cube.coords("height"):
        grib_v_code = 103
        output_unit = iris.unit.Unit("m")
        v_coord = cube.coord("height")

    # unknown / absent
    else:
        # check for *ANY* height coords at all...
        v_coords = cube.coords(axis='z')
        if v_coords:
            # There are vertical coordinate(s), but we don't understand them...
            v_coords_str = ' ,'.join(["'{}'".format(c.name())
                                      for c in v_coords])
            raise iris.exceptions.TranslationError(
                'The vertical-axis coordinate(s) ({}) '
                'are not recognised or handled.'.format(v_coords_str))

    # What did we find?
    if v_coord is None:
        # No vertical coordinate: record as 'surface' level (levelType=1).
        # NOTE: may *not* be truly correct, but seems to be common practice.
        # Still under investigation :
        # See https://github.com/SciTools/iris/issues/519
        gribapi.grib_set_long(grib, "typeOfFirstFixedSurface", 1)
        gribapi.grib_set_long(grib, "scaleFactorOfFirstFixedSurface", 0)
        gribapi.grib_set_long(grib, "scaledValueOfFirstFixedSurface", 0)
        # Set secondary surface = 'missing'.
        gribapi.grib_set_long(grib, "typeOfSecondFixedSurface", -1)
        gribapi.grib_set_long(grib, "scaleFactorOfSecondFixedSurface", 255)
        gribapi.grib_set_long(grib, "scaledValueOfSecondFixedSurface", -1)
    elif not v_coord.has_bounds():
        # No second surface
        output_v = v_coord.units.convert(v_coord.points[0], output_unit)
        if output_v - abs(output_v):
            warnings.warn("Vertical level encoding problem: scaling required.")
        output_v = int(output_v)

        gribapi.grib_set_long(grib, "typeOfFirstFixedSurface", grib_v_code)
        gribapi.grib_set_long(grib, "scaleFactorOfFirstFixedSurface", 0)
        gribapi.grib_set_long(grib, "scaledValueOfFirstFixedSurface", output_v)
        gribapi.grib_set_long(grib, "typeOfSecondFixedSurface", -1)
        gribapi.grib_set_long(grib, "scaleFactorOfSecondFixedSurface", 255)
        gribapi.grib_set_long(grib, "scaledValueOfSecondFixedSurface", -1)
    else:
        # bounded : set lower+upper surfaces
        output_v = v_coord.units.convert(v_coord.bounds[0], output_unit)
        if output_v[0] - abs(output_v[0]) or output_v[1] - abs(output_v[1]):
            warnings.warn("Vertical level encoding problem: scaling required.")
        gribapi.grib_set_long(grib, "typeOfFirstFixedSurface", grib_v_code)
        gribapi.grib_set_long(grib, "typeOfSecondFixedSurface", grib_v_code)
        gribapi.grib_set_long(grib, "scaleFactorOfFirstFixedSurface", 0)
        gribapi.grib_set_long(grib, "scaleFactorOfSecondFixedSurface", 0)
        gribapi.grib_set_long(grib, "scaledValueOfFirstFixedSurface",
                              int(output_v[0]))
        gribapi.grib_set_long(grib, "scaledValueOfSecondFixedSurface",
                              int(output_v[1]))


def surfaces(cube, grib):
    if not hybrid_surfaces(cube, grib):
        non_hybrid_surfaces(cube, grib)


def product_common(cube, grib):
    param_code(cube, grib)
    generating_process_type(cube, grib)
    background_process_id(cube, grib)
    generating_process_id(cube, grib)
    obs_time_after_cutoff(cube, grib)
    time_range(cube, grib)
    surfaces(cube, grib)


def type_of_statistical_processing(cube, grib, coord):
    """Search for processing over the given coord."""
    stat_code = 255  # (grib code table 4.10)

    # if the last cell method applies only to the given coord...
    cell_method = cube.cell_methods[-1]
    coord_names = cell_method.coord_names
    if len(coord_names) == 1 and coord_names[0] == coord.name():
        stat_codes = {'mean': 0, 'sum': 1, 'maximum': 2, 'minimum': 3,
                      'standard_deviation': 6}
        stat_code = stat_codes[cell_method.method]
    if stat_code == 255:
        warnings.warn("Unable to determine type of statistical processing")
    gribapi.grib_set_long(grib, "typeOfStatisticalProcessing", stat_code)


def time_processing_period(cube, grib):
    """
    For template 4.8 (time mean, time max, etc).

    The time range is taken from the 'time' coordinate bounds.
    If the cell-method coordinate is not 'time' itself, the type of statistic
    will always be 'unknown'.

    """
    # We could probably split this function up a bit

    # Can safely assume bounded pt.
    pt_coord = cube.coord("time")
    end = iris.unit.num2date(pt_coord.bounds[0, 1], pt_coord.units.name,
                             pt_coord.units.calendar)

    gribapi.grib_set_long(grib, "yearOfEndOfOverallTimeInterval", end.year)
    gribapi.grib_set_long(grib, "monthOfEndOfOverallTimeInterval", end.month)
    gribapi.grib_set_long(grib, "dayOfEndOfOverallTimeInterval", end.day)
    gribapi.grib_set_long(grib, "hourOfEndOfOverallTimeInterval", end.hour)
    gribapi.grib_set_long(grib, "minuteOfEndOfOverallTimeInterval", end.minute)
    gribapi.grib_set_long(grib, "secondOfEndOfOverallTimeInterval", end.second)

    gribapi.grib_set_long(grib, "numberOfTimeRange", 1)
    gribapi.grib_set_long(grib, "numberOfMissingInStatisticalProcess", 0)

    type_of_statistical_processing(cube, grib, pt_coord)

    # Type of time increment, e.g incrementing fp, incrementing ref
    # time, etc. (code table 4.11)
    gribapi.grib_set_long(grib, "typeOfTimeIncrement", 255)
    # time unit for period over which statistical processing is done (hours)
    gribapi.grib_set_long(grib, "indicatorOfUnitForTimeRange", 1)
    # period over which statistical processing is done
    gribapi.grib_set_long(grib, "lengthOfTimeRange",
                          float(pt_coord.bounds[0, 1] - pt_coord.bounds[0, 0]))
    # time unit between successive source fields (not setting this at present)
    gribapi.grib_set_long(grib, "indicatorOfUnitForTimeIncrement", 255)
    # between successive source fields (just set to 0 for now)
    gribapi.grib_set_long(grib, "timeIncrement", 0)


def _cube_is_time_statistic(cube):
    """
    Test whether we can identify this cube as a statistic over time.

    At present, accept anything whose latest cell method operates over a single
    coordinate that "looks like" a time factor (i.e. some specific names).
    In particular, we recognise the coordinate names defined in
    :py:mod:`iris.coord_categorisation`.

    """
    # The *only* relevant information is in cell_methods, as coordinates or
    # dimensions of aggregation may no longer exist.  So it's not possible to
    # be definitive, but we handle *some* useful cases.
    # In other cases just say "no", which is safe even when not ideal.

    # Identify a single coordinate from the latest cell_method.
    if not cube.cell_methods:
        return False
    latest_coordnames = cube.cell_methods[-1].coord_names
    if len(latest_coordnames) != 1:
        return False
    coord_name = latest_coordnames[0]

    # Define accepted time names, including those from coord_categorisations.
    recognised_time_names = ['time', 'year', 'month', 'day', 'weekday',
                             'season']

    # Accept it if the name is recognised.
    # Currently does *not* recognise related names like 'month_number' or
    # 'years', as that seems potentially unsafe.
    return coord_name in recognised_time_names


def product_template(cube, grib):
    # This will become more complex if we cover more templates, such as 4.15

    # forecast (template 4.0)
    if not cube.coord("time").has_bounds():
        gribapi.grib_set_long(grib, "productDefinitionTemplateNumber", 0)
        product_common(cube, grib)
        return

    # time processed (template 4.8)
    if _cube_is_time_statistic(cube):
        gribapi.grib_set_long(grib, "productDefinitionTemplateNumber", 8)
        product_common(cube, grib)
        time_processing_period(cube, grib)
        return

    # Don't know how to handle this kind of data
    raise iris.exceptions.TranslationError(
        'A suitable product template could not be deduced')


def centre(cube, grib):
    # TODO: read centre from cube
    gribapi.grib_set_long(grib, "centre", 74)  # UKMO
    gribapi.grib_set_long(grib, "subCentre", 0)  # exeter is not in the spec


def reference_time(cube, grib):
    # Set the reference time.
    # (analysis, forecast start, verify time, obs time, etc)
    try:
        fp_coord = cube.coord("forecast_period")
    except iris.exceptions.CoordinateNotFoundError:
        fp_coord = None

    if fp_coord is not None:
        rt, rt_meaning, _, _ = _non_missing_forecast_period(cube)
    else:
        rt, rt_meaning, _, _ = _missing_forecast_period(cube)

    gribapi.grib_set_long(grib, "significanceOfReferenceTime", rt_meaning)
    gribapi.grib_set_long(
        grib, "dataDate", "%04d%02d%02d" % (rt.year, rt.month, rt.day))
    gribapi.grib_set_long(
        grib, "dataTime", "%02d%02d" % (rt.hour, rt.minute))

    # TODO: Set the calendar, when we find out what happened to the proposal!
    # http://tinyurl.com/oefqgv6
    # I was sure it was approved for pre-operational use but it's not there.


def identification(cube, grib):
    centre(cube, grib)
    reference_time(cube, grib)

    # operational product, operational test, research product, etc
    # (missing for now)
    gribapi.grib_set_long(grib, "productionStatusOfProcessedData", 255)
    # analysis, forecast, processed satellite, processed radar,
    # (analysis and forecast products for now)
    gribapi.grib_set_long(grib, "typeOfProcessedData", 2)


def data(cube, grib):
    # Masked data?
    if isinstance(cube.data, ma.core.MaskedArray):
        # What missing value shall we use?
        if not np.isnan(cube.data.fill_value):
            # Use the data's fill value.
            fill_value = float(cube.data.fill_value)
        else:
            # We can't use the data's fill value if it's NaN,
            # the GRIB API doesn't like it.
            # Calculate an MDI outside the data range.
            min, max = cube.data.min(), cube.data.max()
            fill_value = min - (max - min) * 0.1
        # Prepare the unmaksed data array, using fill_value as the MDI.
        data = cube.data.filled(fill_value)
    else:
        fill_value = None
        data = cube.data

    # units scaling
    grib2_info = gptx.cf_phenom_to_grib2_info(cube.standard_name,
                                              cube.long_name)
    if grib2_info is None:
        # for now, just allow this
        warnings.warn('Unable to determine Grib2 parameter code for cube.\n'
                      'Message data may not be correctly scaled.')
    else:
        if cube.units != grib2_info.units:
            data = cube.units.convert(data, grib2_info.units)
            if fill_value is not None:
                fill_value = cube.units.convert(fill_value, grib2_info.units)

    if fill_value is None:
        # Disable missing values in the grib message.
        gribapi.grib_set(grib, "bitmapPresent", 0)
    else:
        # Enable missing values in the grib message.
        gribapi.grib_set(grib, "bitmapPresent", 1)
        gribapi.grib_set_double(grib, "missingValue", fill_value)
    gribapi.grib_set_double_array(grib, "values", data.flatten())

    # todo: check packing accuracy?
#    print "packingError", gribapi.getb_get_double(grib, "packingError")


def run(cube, grib):
    gribbability_check(cube)
    identification(cube, grib)
    grid_template(cube, grib)
    product_template(cube, grib)
    data(cube, grib)

########NEW FILE########
__FILENAME__ = load_rules
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

# Historically this was auto-generated from
# SciTools/iris-code-generators:tools/gen_rules.py

import warnings

import numpy as np

from iris.aux_factory import HybridPressureFactory
from iris.coords import AuxCoord, CellMethod, DimCoord
from iris.fileformats.rules import Factory, Reference, ReferenceTarget
from iris.unit import CALENDAR_GREGORIAN, Unit


def convert(grib):
    factories = []
    references = []
    standard_name = None
    long_name = None
    units = None
    attributes = {}
    cell_methods = []
    dim_coords_and_dims = []
    aux_coords_and_dims = []

    if \
            (grib.gridType=="reduced_gg"):
        aux_coords_and_dims.append((AuxCoord(grib._y_points, grib._y_coord_name, units='degrees', coord_system=grib._coord_system), 0))
        aux_coords_and_dims.append((AuxCoord(grib._x_points, grib._x_coord_name, units='degrees', coord_system=grib._coord_system), 0))

    if \
            (grib.gridType=="regular_ll") and \
            (grib.jPointsAreConsecutive == 0):
        dim_coords_and_dims.append((DimCoord(grib._y_points, grib._y_coord_name, units='degrees', coord_system=grib._coord_system), 0))
        dim_coords_and_dims.append((DimCoord(grib._x_points, grib._x_coord_name, units='degrees', coord_system=grib._coord_system, circular=grib._x_circular), 1))

    if \
            (grib.gridType=="regular_ll") and \
            (grib.jPointsAreConsecutive == 1):
        dim_coords_and_dims.append((DimCoord(grib._y_points, grib._y_coord_name, units='degrees', coord_system=grib._coord_system), 1))
        dim_coords_and_dims.append((DimCoord(grib._x_points, grib._x_coord_name, units='degrees', coord_system=grib._coord_system, circular=grib._x_circular), 0))

    if \
            (grib.gridType=="regular_gg") and \
            (grib.jPointsAreConsecutive == 0):
        dim_coords_and_dims.append((DimCoord(grib._y_points, grib._y_coord_name, units='degrees', coord_system=grib._coord_system), 0))
        dim_coords_and_dims.append((DimCoord(grib._x_points, grib._x_coord_name, units='degrees', coord_system=grib._coord_system, circular=grib._x_circular), 1))

    if \
            (grib.gridType=="regular_gg") and \
            (grib.jPointsAreConsecutive == 1):
        dim_coords_and_dims.append((DimCoord(grib._y_points, grib._y_coord_name, units='degrees', coord_system=grib._coord_system), 1))
        dim_coords_and_dims.append((DimCoord(grib._x_points, grib._x_coord_name, units='degrees', coord_system=grib._coord_system, circular=grib._x_circular), 0))

    if \
            (grib.gridType=="rotated_ll") and \
            (grib.jPointsAreConsecutive == 0):
        dim_coords_and_dims.append((DimCoord(grib._y_points, grib._y_coord_name, units='degrees', coord_system=grib._coord_system), 0))
        dim_coords_and_dims.append((DimCoord(grib._x_points, grib._x_coord_name, units='degrees', coord_system=grib._coord_system, circular=grib._x_circular), 1))

    if \
            (grib.gridType=="rotated_ll") and \
            (grib.jPointsAreConsecutive == 1):
        dim_coords_and_dims.append((DimCoord(grib._y_points, grib._y_coord_name, units='degrees', coord_system=grib._coord_system), 1))
        dim_coords_and_dims.append((DimCoord(grib._x_points, grib._x_coord_name, units='degrees', coord_system=grib._coord_system, circular=grib._x_circular), 0))

    if grib.gridType in ["polar_stereographic", "lambert"]:
        dim_coords_and_dims.append((DimCoord(grib._y_points, grib._y_coord_name, units="m", coord_system=grib._coord_system), 0))
        dim_coords_and_dims.append((DimCoord(grib._x_points, grib._x_coord_name, units="m", coord_system=grib._coord_system), 1))

    if \
            (grib.edition == 1) and \
            (grib.table2Version < 128) and \
            (grib.indicatorOfParameter == 11) and \
            (grib._cf_data is None):
        standard_name = "air_temperature"
        units = "kelvin"

    if \
            (grib.edition == 1) and \
            (grib.table2Version < 128) and \
            (grib.indicatorOfParameter == 33) and \
            (grib._cf_data is None):
        standard_name = "x_wind"
        units = "m s-1"

    if \
            (grib.edition == 1) and \
            (grib.table2Version < 128) and \
            (grib.indicatorOfParameter == 34) and \
            (grib._cf_data is None):
        standard_name = "y_wind"
        units = "m s-1"

    if \
            (grib.edition == 1) and \
            (grib._cf_data is not None):
        standard_name = grib._cf_data.standard_name
        long_name = grib._cf_data.standard_name or grib._cf_data.long_name
        units = grib._cf_data.units

    if \
            (grib.edition == 1) and \
            (grib.table2Version >= 128) and \
            (grib._cf_data is None):
        long_name = "UNKNOWN LOCAL PARAM " + str(grib.indicatorOfParameter) + "." + str(grib.table2Version)
        units = "???"

    if \
            (grib.edition == 1) and \
            (grib.table2Version == 1) and \
            (grib.indicatorOfParameter >= 128):
        long_name = "UNKNOWN LOCAL PARAM " + str(grib.indicatorOfParameter) + "." + str(grib.table2Version)
        units = "???"

    if \
            (grib.edition == 2) and \
            (grib._cf_data is not None):
        standard_name = grib._cf_data.standard_name
        long_name = grib._cf_data.long_name
        units = grib._cf_data.units

    if \
            (grib.edition == 1) and \
            (grib._phenomenonDateTime != -1.0):
        aux_coords_and_dims.append((DimCoord(points=grib.startStep, standard_name='forecast_period', units=grib._forecastTimeUnit), None))
        aux_coords_and_dims.append((DimCoord(points=grib.phenomenon_points('hours'), standard_name='time', units=Unit('hours since epoch', CALENDAR_GREGORIAN)), None))

    def add_bounded_time_coords(aux_coords_and_dims, grib):
        t_bounds = grib.phenomenon_bounds('hours')
        period = Unit('hours').convert(t_bounds[1] - t_bounds[0],
                                       grib._forecastTimeUnit)
        aux_coords_and_dims.append((
            DimCoord(standard_name='forecast_period',
                     units=grib._forecastTimeUnit,
                     points=grib._forecastTime + 0.5 * period,
                     bounds=[grib._forecastTime, grib._forecastTime + period]),
            None))
        aux_coords_and_dims.append((
            DimCoord(standard_name='time',
                     units=Unit('hours since epoch', CALENDAR_GREGORIAN),
                     points=0.5 * (t_bounds[0] + t_bounds[1]),
                     bounds=t_bounds),
            None))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 3):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("mean", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 4):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("sum", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 5):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("_difference", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 51):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("mean", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 113):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("mean", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 114):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("sum", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 115):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("mean", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 116):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("sum", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 117):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("mean", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 118):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("_covariance", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 123):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("mean", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 124):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("sum", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.timeRangeIndicator == 125):
        add_bounded_time_coords(aux_coords_and_dims, grib)
        cell_methods.append(CellMethod("standard_deviation", coords="time"))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 0):
        aux_coords_and_dims.append((DimCoord(points=Unit(grib._forecastTimeUnit).convert(np.int32(grib._forecastTime), "hours"), standard_name='forecast_period', units="hours"), None))
        aux_coords_and_dims.append((DimCoord(points=grib.phenomenon_points('hours'), standard_name='time', units=Unit('hours since epoch', CALENDAR_GREGORIAN)), None))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber in (8, 9)):
        add_bounded_time_coords(aux_coords_and_dims, grib)

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 8) and \
            (grib.typeOfStatisticalProcessing == 0):
        cell_methods.append(CellMethod("mean", coords="time"))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 8) and \
            (grib.typeOfStatisticalProcessing == 1):
        cell_methods.append(CellMethod("sum", coords="time"))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 8) and \
            (grib.typeOfStatisticalProcessing == 2):
        cell_methods.append(CellMethod("maximum", coords="time"))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 8) and \
            (grib.typeOfStatisticalProcessing == 3):
        cell_methods.append(CellMethod("minimum", coords="time"))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 8) and \
            (grib.typeOfStatisticalProcessing == 4):
        cell_methods.append(CellMethod("_difference", coords="time"))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 8) and \
            (grib.typeOfStatisticalProcessing == 5):
        cell_methods.append(CellMethod("_root_mean_square", coords="time"))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 8) and \
            (grib.typeOfStatisticalProcessing == 6):
        cell_methods.append(CellMethod("standard_deviation", coords="time"))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 8) and \
            (grib.typeOfStatisticalProcessing == 7):
        cell_methods.append(CellMethod("_convariance", coords="time"))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 8) and \
            (grib.typeOfStatisticalProcessing == 8):
        cell_methods.append(CellMethod("_difference", coords="time"))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 8) and \
            (grib.typeOfStatisticalProcessing == 9):
        cell_methods.append(CellMethod("_ratio", coords="time"))

    if \
            (grib.edition == 1) and \
            (grib.levelType == 'pl'):
        aux_coords_and_dims.append((DimCoord(points=grib.level,  long_name="pressure", units="hPa"), None))

    if \
            (grib.edition == 1) and \
            (grib.levelType == 'sfc') and \
            (grib._cf_data is not None) and \
            (grib._cf_data.set_height is not None):
        aux_coords_and_dims.append((DimCoord(points=grib._cf_data.set_height,  long_name="height", units="m", attributes={'positive':'up'}), None))

    if \
            (grib.edition == 1) and \
            (grib.levelType == 'ml') and \
            (hasattr(grib, 'pv')):
        aux_coords_and_dims.append((AuxCoord(grib.level, standard_name='model_level_number', attributes={'positive': 'up'}), None))
        aux_coords_and_dims.append((DimCoord(grib.pv[grib.level], long_name='level_pressure', units='Pa'), None))
        aux_coords_and_dims.append((AuxCoord(grib.pv[grib.numberOfCoordinatesValues/2 + grib.level], long_name='sigma'), None))
        factories.append(Factory(HybridPressureFactory, [{'long_name': 'level_pressure'}, {'long_name': 'sigma'}, Reference('surface_pressure')]))

    if \
            (grib.edition == 2) and \
            (grib.typeOfFirstFixedSurface != grib.typeOfSecondFixedSurface):
        warnings.warn("Different vertical bound types not yet handled.")

    if \
            (grib.edition == 2) and \
            (grib.typeOfFirstFixedSurface == 103) and \
            (grib.typeOfSecondFixedSurface == 255):
        aux_coords_and_dims.append((DimCoord(points=grib.scaledValueOfFirstFixedSurface/(10.0**grib.scaleFactorOfFirstFixedSurface), standard_name="height", units="m"), None))

    if \
            (grib.edition == 2) and \
            (grib.typeOfFirstFixedSurface == 103) and \
            (grib.typeOfSecondFixedSurface != 255):
        aux_coords_and_dims.append((DimCoord(points=0.5*(grib.scaledValueOfFirstFixedSurface/(10.0**grib.scaleFactorOfFirstFixedSurface) + grib.scaledValueOfSecondFixedSurface/(10.0**grib.scaleFactorOfSecondFixedSurface)), standard_name="height", units="m", bounds=[grib.scaledValueOfFirstFixedSurface/(10.0**grib.scaleFactorOfFirstFixedSurface) , grib.scaledValueOfSecondFixedSurface/(10.0**grib.scaleFactorOfSecondFixedSurface)]), None))

    if \
            (grib.edition == 2) and \
            (grib.typeOfFirstFixedSurface == 100) and \
            (grib.typeOfSecondFixedSurface == 255):
        aux_coords_and_dims.append((DimCoord(points=grib.scaledValueOfFirstFixedSurface/(10.0**grib.scaleFactorOfFirstFixedSurface), long_name="pressure", units="Pa"), None))

    if \
            (grib.edition == 2) and \
            (grib.typeOfFirstFixedSurface == 100) and \
            (grib.typeOfSecondFixedSurface != 255):
        aux_coords_and_dims.append((DimCoord(points=0.5*(grib.scaledValueOfFirstFixedSurface/(10.0**grib.scaleFactorOfFirstFixedSurface) + grib.scaledValueOfSecondFixedSurface/(10.0**grib.scaleFactorOfSecondFixedSurface)), long_name="pressure", units="Pa", bounds=[grib.scaledValueOfFirstFixedSurface/(10.0**grib.scaleFactorOfFirstFixedSurface) , grib.scaledValueOfSecondFixedSurface/(10.0**grib.scaleFactorOfSecondFixedSurface)]), None))

    if \
            (grib.edition == 2) and \
            (grib.typeOfFirstFixedSurface in [105, 119]) and \
            (grib.numberOfCoordinatesValues > 0):
        aux_coords_and_dims.append((AuxCoord(grib.scaledValueOfFirstFixedSurface, standard_name='model_level_number', attributes={'positive': 'up'}), None))
        aux_coords_and_dims.append((DimCoord(grib.pv[grib.scaledValueOfFirstFixedSurface], long_name='level_pressure', units='Pa'), None))
        aux_coords_and_dims.append((AuxCoord(grib.pv[grib.numberOfCoordinatesValues/2 + grib.scaledValueOfFirstFixedSurface], long_name='sigma'), None))
        factories.append(Factory(HybridPressureFactory, [{'long_name': 'level_pressure'}, {'long_name': 'sigma'}, Reference('surface_air_pressure')]))

    if grib._originatingCentre != 'unknown':
        aux_coords_and_dims.append((AuxCoord(points=grib._originatingCentre, long_name='originating_centre', units='no_unit'), None))

    if \
            (grib.edition == 2) and \
            (grib.productDefinitionTemplateNumber == 1):
        aux_coords_and_dims.append((DimCoord(points=grib.perturbationNumber, long_name='ensemble_member', units='no_unit'), None))

    if grib.productDefinitionTemplateNumber not in (0, 8):
        attributes["GRIB_LOAD_WARNING"] = ("unsupported GRIB%d ProductDefinitionTemplate: #4.%d" % (grib.edition, grib.productDefinitionTemplateNumber))

    if \
            (grib.edition == 2) and \
            (grib.centre == 'ecmf') and \
            (grib.discipline == 0) and \
            (grib.parameterCategory == 3) and \
            (grib.parameterNumber == 25) and \
            (grib.typeOfFirstFixedSurface == 105):
        references.append(ReferenceTarget('surface_air_pressure', lambda cube: {'standard_name': 'surface_air_pressure', 'units': 'Pa', 'data': np.exp(cube.data)}))

    return (factories, references, standard_name, long_name, units, attributes,
            cell_methods, dim_coords_and_dims, aux_coords_and_dims)

########NEW FILE########
__FILENAME__ = _grib_cf_map
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
#
# DO NOT EDIT: AUTO-GENERATED
"""
Provides GRIB/CF phenomenon translations.

"""
from collections import namedtuple


CFName = namedtuple('CFName', 'standard_name long_name units')

DimensionCoordinate = namedtuple('DimensionCoordinate',
                                 'standard_name units points')

G1LocalParam = namedtuple('G1LocalParam', 'edition t2version centre iParam')
G2Param = namedtuple('G2Param', 'edition discipline category number')


GRIB1_LOCAL_TO_CF_CONSTRAINED = {
    G1LocalParam(1, 128, 98, 165): (CFName('x_wind', None, 'm s-1'), DimensionCoordinate('height', 'm', (10,))),
    G1LocalParam(1, 128, 98, 166): (CFName('y_wind', None, 'm s-1'), DimensionCoordinate('height', 'm', (10,))),
    G1LocalParam(1, 128, 98, 167): (CFName('air_temperature', None, 'K'), DimensionCoordinate('height', 'm', (2,))),
    G1LocalParam(1, 128, 98, 168): (CFName('dew_point_temperature', None, 'K'), DimensionCoordinate('height', 'm', (2,))),
    }

GRIB1_LOCAL_TO_CF = {
    G1LocalParam(1, 128, 98, 31): CFName('sea_ice_area_fraction', None, '1'),
    G1LocalParam(1, 128, 98, 34): CFName('sea_surface_temperature', None, 'K'),
    G1LocalParam(1, 128, 98, 59): CFName('atmosphere_specific_convective_available_potential_energy', None, 'J kg-1'),
    G1LocalParam(1, 128, 98, 129): CFName('geopotential', None, 'm2 s-2'),
    G1LocalParam(1, 128, 98, 130): CFName('air_temperature', None, 'K'),
    G1LocalParam(1, 128, 98, 131): CFName('x_wind', None, 'm s-1'),
    G1LocalParam(1, 128, 98, 132): CFName('y_wind', None, 'm s-1'),
    G1LocalParam(1, 128, 98, 135): CFName('lagrangian_tendency_of_air_pressure', None, 'Pa s-1'),
    G1LocalParam(1, 128, 98, 141): CFName('thickness_of_snowfall_amount', None, 'm'),
    G1LocalParam(1, 128, 98, 151): CFName('air_pressure_at_sea_level', None, 'Pa'),
    G1LocalParam(1, 128, 98, 157): CFName('relative_humidity', None, '%'),
    G1LocalParam(1, 128, 98, 164): CFName('cloud_area_fraction', None, '1'),
    G1LocalParam(1, 128, 98, 173): CFName('surface_roughness_length', None, 'm'),
    G1LocalParam(1, 128, 98, 174): CFName(None, 'grib_physical_atmosphere_albedo', '1'),
    G1LocalParam(1, 128, 98, 186): CFName('low_type_cloud_area_fraction', None, '1'),
    G1LocalParam(1, 128, 98, 187): CFName('medium_type_cloud_area_fraction', None, '1'),
    G1LocalParam(1, 128, 98, 188): CFName('high_type_cloud_area_fraction', None, '1'),
    G1LocalParam(1, 128, 98, 235): CFName(None, 'grib_skin_temperature', 'K'),
    }

GRIB2_TO_CF = {
    G2Param(2, 0, 0, 0): CFName('air_temperature', None, 'K'),
    G2Param(2, 0, 0, 2): CFName('air_potential_temperature', None, 'K'),
    G2Param(2, 0, 0, 6): CFName('dew_point_temperature', None, 'K'),
    G2Param(2, 0, 0, 17): CFName(None, 'grib_skin_temperature', 'K'),
    G2Param(2, 0, 1, 0): CFName('specific_humidity', None, 'kg kg-1'),
    G2Param(2, 0, 1, 1): CFName('relative_humidity', None, '%'),
    G2Param(2, 0, 1, 3): CFName(None, 'precipitable_water', 'kg m-2'),
    G2Param(2, 0, 1, 11): CFName('thickness_of_snowfall_amount', None, 'm'),
    G2Param(2, 0, 1, 13): CFName('liquid_water_content_of_surface_snow', None, 'kg m-2'),
    G2Param(2, 0, 1, 22): CFName(None, 'cloud_mixing_ratio', 'kg kg-1'),
    G2Param(2, 0, 1, 64): CFName('atmosphere_mass_content_of_water_vapor', None, 'kg m-2'),
    G2Param(2, 0, 2, 0): CFName('wind_from_direction', None, 'degrees'),
    G2Param(2, 0, 2, 1): CFName('wind_speed', None, 'm s-1'),
    G2Param(2, 0, 2, 2): CFName('x_wind', None, 'm s-1'),
    G2Param(2, 0, 2, 3): CFName('y_wind', None, 'm s-1'),
    G2Param(2, 0, 2, 8): CFName('lagrangian_tendency_of_air_pressure', None, 'Pa s-1'),
    G2Param(2, 0, 2, 10): CFName('atmosphere_absolute_vorticity', None, 's-1'),
    G2Param(2, 0, 3, 0): CFName('air_pressure', None, 'Pa'),
    G2Param(2, 0, 3, 1): CFName('air_pressure_at_sea_level', None, 'Pa'),
    G2Param(2, 0, 3, 3): CFName(None, 'icao_standard_atmosphere_reference_height', 'm'),
    G2Param(2, 0, 3, 4): CFName('geopotential', None, 'm2 s-2'),
    G2Param(2, 0, 3, 5): CFName('geopotential_height', None, 'm'),
    G2Param(2, 0, 3, 9): CFName('geopotential_height_anomaly', None, 'm'),
    G2Param(2, 0, 6, 1): CFName('cloud_area_fraction', None, '%'),
    G2Param(2, 0, 6, 3): CFName('low_type_cloud_area_fraction', None, '%'),
    G2Param(2, 0, 6, 4): CFName('medium_type_cloud_area_fraction', None, '%'),
    G2Param(2, 0, 6, 5): CFName('high_type_cloud_area_fraction', None, '%'),
    G2Param(2, 0, 6, 6): CFName('atmosphere_mass_content_of_cloud_liquid_water', None, 'kg m-2'),
    G2Param(2, 0, 6, 7): CFName('cloud_area_fraction_in_atmosphere_layer', None, '%'),
    G2Param(2, 0, 7, 6): CFName('atmosphere_specific_convective_available_potential_energy', None, 'J kg-1'),
    G2Param(2, 0, 7, 7): CFName(None, 'convective_inhibition', 'J kg-1'),
    G2Param(2, 0, 7, 8): CFName(None, 'storm_relative_helicity', 'J kg-1'),
    G2Param(2, 0, 14, 0): CFName('atmosphere_mole_content_of_ozone', None, 'Dobson'),
    G2Param(2, 0, 19, 1): CFName(None, 'grib_physical_atmosphere_albedo', '%'),
    G2Param(2, 2, 0, 0): CFName('land_area_fraction', None, '1'),
    G2Param(2, 2, 0, 1): CFName('surface_roughness_length', None, 'm'),
    G2Param(2, 2, 0, 2): CFName('soil_temperature', None, 'K'),
    G2Param(2, 2, 0, 7): CFName('surface_altitude', None, 'm'),
    G2Param(2, 10, 1, 2): CFName('sea_water_x_velocity', None, 'm s-1'),
    G2Param(2, 10, 1, 3): CFName('sea_water_y_velocity', None, 'm s-1'),
    G2Param(2, 10, 2, 0): CFName('sea_ice_area_fraction', None, '1'),
    G2Param(2, 10, 3, 0): CFName('sea_surface_temperature', None, 'K'),
    }

CF_CONSTRAINED_TO_GRIB1_LOCAL = {
    (CFName('air_temperature', None, 'K'), DimensionCoordinate('height', 'm', (2,))): G1LocalParam(1, 128, 98, 167),
    (CFName('dew_point_temperature', None, 'K'), DimensionCoordinate('height', 'm', (2,))): G1LocalParam(1, 128, 98, 168),
    (CFName('x_wind', None, 'm s-1'), DimensionCoordinate('height', 'm', (10,))): G1LocalParam(1, 128, 98, 165),
    (CFName('y_wind', None, 'm s-1'), DimensionCoordinate('height', 'm', (10,))): G1LocalParam(1, 128, 98, 166),
    }

CF_TO_GRIB1_LOCAL = {
    CFName(None, 'grib_physical_atmosphere_albedo', '1'): G1LocalParam(1, 128, 98, 174),
    CFName(None, 'grib_skin_temperature', 'K'): G1LocalParam(1, 128, 98, 235),
    CFName('air_pressure_at_sea_level', None, 'Pa'): G1LocalParam(1, 128, 98, 151),
    CFName('air_temperature', None, 'K'): G1LocalParam(1, 128, 98, 130),
    CFName('atmosphere_specific_convective_available_potential_energy', None, 'J kg-1'): G1LocalParam(1, 128, 98, 59),
    CFName('cloud_area_fraction', None, '1'): G1LocalParam(1, 128, 98, 164),
    CFName('geopotential', None, 'm2 s-2'): G1LocalParam(1, 128, 98, 129),
    CFName('high_type_cloud_area_fraction', None, '1'): G1LocalParam(1, 128, 98, 188),
    CFName('lagrangian_tendency_of_air_pressure', None, 'Pa s-1'): G1LocalParam(1, 128, 98, 135),
    CFName('low_type_cloud_area_fraction', None, '1'): G1LocalParam(1, 128, 98, 186),
    CFName('medium_type_cloud_area_fraction', None, '1'): G1LocalParam(1, 128, 98, 187),
    CFName('relative_humidity', None, '%'): G1LocalParam(1, 128, 98, 157),
    CFName('sea_ice_area_fraction', None, '1'): G1LocalParam(1, 128, 98, 31),
    CFName('sea_surface_temperature', None, 'K'): G1LocalParam(1, 128, 98, 34),
    CFName('surface_roughness_length', None, 'm'): G1LocalParam(1, 128, 98, 173),
    CFName('thickness_of_snowfall_amount', None, 'm'): G1LocalParam(1, 128, 98, 141),
    CFName('x_wind', None, 'm s-1'): G1LocalParam(1, 128, 98, 131),
    CFName('y_wind', None, 'm s-1'): G1LocalParam(1, 128, 98, 132),
    }

CF_TO_GRIB2 = {
    CFName(None, 'cloud_mixing_ratio', 'kg kg-1'): G2Param(2, 0, 1, 22),
    CFName(None, 'convective_inhibition', 'J kg-1'): G2Param(2, 0, 7, 7),
    CFName(None, 'grib_physical_atmosphere_albedo', '%'): G2Param(2, 0, 19, 1),
    CFName(None, 'grib_skin_temperature', 'K'): G2Param(2, 0, 0, 17),
    CFName(None, 'icao_standard_atmosphere_reference_height', 'm'): G2Param(2, 0, 3, 3),
    CFName(None, 'precipitable_water', 'kg m-2'): G2Param(2, 0, 1, 3),
    CFName(None, 'storm_relative_helicity', 'J kg-1'): G2Param(2, 0, 7, 8),
    CFName('air_potential_temperature', None, 'K'): G2Param(2, 0, 0, 2),
    CFName('air_pressure', None, 'Pa'): G2Param(2, 0, 3, 0),
    CFName('air_pressure_at_sea_level', None, 'Pa'): G2Param(2, 0, 3, 1),
    CFName('air_temperature', None, 'K'): G2Param(2, 0, 0, 0),
    CFName('atmosphere_absolute_vorticity', None, 's-1'): G2Param(2, 0, 2, 10),
    CFName('atmosphere_mass_content_of_cloud_liquid_water', None, 'kg m-2'): G2Param(2, 0, 6, 6),
    CFName('atmosphere_mass_content_of_water_vapor', None, 'kg m-2'): G2Param(2, 0, 1, 64),
    CFName('atmosphere_mole_content_of_ozone', None, 'Dobson'): G2Param(2, 0, 14, 0),
    CFName('atmosphere_specific_convective_available_potential_energy', None, 'J kg-1'): G2Param(2, 0, 7, 6),
    CFName('cloud_area_fraction', None, '%'): G2Param(2, 0, 6, 1),
    CFName('cloud_area_fraction_in_atmosphere_layer', None, '%'): G2Param(2, 0, 6, 7),
    CFName('dew_point_temperature', None, 'K'): G2Param(2, 0, 0, 6),
    CFName('geopotential', None, 'm2 s-2'): G2Param(2, 0, 3, 4),
    CFName('geopotential_height', None, 'm'): G2Param(2, 0, 3, 5),
    CFName('geopotential_height_anomaly', None, 'm'): G2Param(2, 0, 3, 9),
    CFName('high_type_cloud_area_fraction', None, '%'): G2Param(2, 0, 6, 5),
    CFName('lagrangian_tendency_of_air_pressure', None, 'Pa s-1'): G2Param(2, 0, 2, 8),
    CFName('land_area_fraction', None, '1'): G2Param(2, 2, 0, 0),
    CFName('land_binary_mask', None, '1'): G2Param(2, 2, 0, 0),
    CFName('liquid_water_content_of_surface_snow', None, 'kg m-2'): G2Param(2, 0, 1, 13),
    CFName('low_type_cloud_area_fraction', None, '%'): G2Param(2, 0, 6, 3),
    CFName('medium_type_cloud_area_fraction', None, '%'): G2Param(2, 0, 6, 4),
    CFName('relative_humidity', None, '%'): G2Param(2, 0, 1, 1),
    CFName('sea_ice_area_fraction', None, '1'): G2Param(2, 10, 2, 0),
    CFName('sea_surface_temperature', None, 'K'): G2Param(2, 10, 3, 0),
    CFName('sea_water_x_velocity', None, 'm s-1'): G2Param(2, 10, 1, 2),
    CFName('sea_water_y_velocity', None, 'm s-1'): G2Param(2, 10, 1, 3),
    CFName('soil_temperature', None, 'K'): G2Param(2, 2, 0, 2),
    CFName('specific_humidity', None, 'kg kg-1'): G2Param(2, 0, 1, 0),
    CFName('surface_altitude', None, 'm'): G2Param(2, 2, 0, 7),
    CFName('surface_roughness_length', None, 'm'): G2Param(2, 2, 0, 1),
    CFName('thickness_of_snowfall_amount', None, 'm'): G2Param(2, 0, 1, 11),
    CFName('wind_from_direction', None, 'degrees'): G2Param(2, 0, 2, 0),
    CFName('wind_speed', None, 'm s-1'): G2Param(2, 0, 2, 1),
    CFName('x_wind', None, 'm s-1'): G2Param(2, 0, 2, 2),
    CFName('y_wind', None, 'm s-1'): G2Param(2, 0, 2, 3),
    }

########NEW FILE########
__FILENAME__ = name
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Provides NAME file format loading capabilities."""

import iris.io


def _get_NAME_loader(filename):
    """
    Return the approriate load function for a NAME file based
    on the contents of its header.

    """
    # Lazy import to avoid importing name_loaders until
    # attempting to load a NAME file.
    import iris.fileformats.name_loaders as name_loaders

    load = None
    with open(filename, 'r') as file_handle:
        header = name_loaders.read_header(file_handle)

    # Infer file type based on contents of header.
    if 'Run name' in header:
        if 'X grid origin' not in header:
            load = name_loaders.load_NAMEIII_trajectory
        elif header.get('X grid origin') is not None:
            load = name_loaders.load_NAMEIII_field
        else:
            load = name_loaders.load_NAMEIII_timeseries
    elif 'Title' in header:
        if 'Number of series' in header:
            load = name_loaders.load_NAMEII_timeseries
        else:
            load = name_loaders.load_NAMEII_field

    if load is None:
        raise ValueError('Unable to determine NAME file type '
                         'of {!r}.'.format(filename))

    return load


def load_cubes(filenames, callback):
    """
    Return a generator of cubes given one or more filenames and an
    optional callback.

    Args:

    * filenames (string/list):
        One or more NAME filenames to load.

    Kwargs:

    * callback (callable function):
        A function which can be passed on to :func:`iris.io.run_callback`.

    Returns:
         A generator of :class:`iris.cubes.Cube` instances.

    """
    if isinstance(filenames, basestring):
        filenames = [filenames]

    for filename in filenames:
        load = _get_NAME_loader(filename)
        for cube in load(filename):
            if callback is not None:
                cube = iris.io.run_callback(callback, cube,
                                            None, filename)
            if cube is not None:
                yield cube

########NEW FILE########
__FILENAME__ = name_loaders
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""NAME file format loading functions."""

import collections
import datetime
from itertools import izip
import re
import warnings

import numpy as np

from iris.coords import AuxCoord, DimCoord, CellMethod
import iris.coord_systems
import iris.cube
from iris.exceptions import TranslationError
import iris.unit


EARTH_RADIUS = 6371229.0
NAMEIII_DATETIME_FORMAT = '%d/%m/%Y  %H:%M %Z'
NAMEII_FIELD_DATETIME_FORMAT = '%H%M%Z %d/%m/%Y'
NAMEII_TIMESERIES_DATETIME_FORMAT = '%d/%m/%Y  %H:%M:%S'


NAMECoord = collections.namedtuple('NAMECoord', ['name',
                                                 'dimension',
                                                 'values'])


def _split_name_and_units(name):
    units = None
    if "(" in name and ")" in name:
        split = name.rsplit("(", 1)
        try_units = split[1].replace(")", "").strip()
        try:
            try_units = iris.unit.Unit(try_units)
        except ValueError:
            pass
        else:
            name = split[0].strip()
            units = try_units
    return name, units


def read_header(file_handle):
    """
    Return a dictionary containing the header information extracted
    from the the provided NAME file object.

    Args:

    * file_handle (file-like object):
        A file-like object from which to read the header information.

    Returns:
        A dictionary containing the extracted header information.

    """
    header = {}
    header['NAME Version'] = file_handle.next().strip()
    for line in file_handle:
        words = line.split(':', 1)
        if len(words) != 2:
            break
        key, value = [word.strip() for word in words]
        header[key] = value

    # Cast some values into floats or integers if they match a
    # given name. Set any empty string values to None.
    for key, value in header.items():
        if value:
            if key in ['X grid origin', 'Y grid origin',
                       'X grid resolution', 'Y grid resolution']:
                header[key] = float(value)
            elif key in ['X grid size', 'Y grid size',
                         'Number of preliminary cols',
                         'Number of field cols',
                         'Number of fields',
                         'Number of series']:
                header[key] = int(value)
        else:
            header[key] = None

    return header


def _read_data_arrays(file_handle, n_arrays, shape):
    """
    Return a list of NumPy arrays containing the data extracted from
    the provided file object. The number and shape of the arrays
    must be specified.

    """
    data_arrays = [np.zeros(shape, dtype=np.float32) for
                   i in range(n_arrays)]

    # Iterate over the remaining lines which represent the data in
    # a column form.
    for line in file_handle:
        # Split the line by comma, removing the last empty column
        # caused by the trailing comma
        vals = line.split(',')[:-1]

        # Cast the x and y grid positions to integers and convert
        # them to zero based indices
        x = int(float(vals[0])) - 1
        y = int(float(vals[1])) - 1

        # Populate the data arrays (i.e. all columns but the leading 4).
        for i, data_array in enumerate(data_arrays):
            data_array[y, x] = float(vals[i + 4])

    return data_arrays


def _build_lat_lon_for_NAME_field(header):
    """
    Return regular latitude and longitude coordinates extracted from
    the provided header dictionary.

    """
    start = header['X grid origin']
    step = header['X grid resolution']
    count = header['X grid size']
    pts = start + np.arange(count, dtype=np.float64) * step
    lon = NAMECoord(name='longitude', dimension=1, values=pts)

    start = header['Y grid origin']
    step = header['Y grid resolution']
    count = header['Y grid size']
    pts = start + np.arange(count, dtype=np.float64) * step
    lat = NAMECoord(name='latitude', dimension=0, values=pts)

    return lat, lon


def _build_lat_lon_for_NAME_timeseries(column_headings):
    """
    Return regular latitude and longitude coordinates extracted from
    the provided column_headings dictionary.

    """
    pattern = re.compile(r'\-?[0-9]*\.[0-9]*')
    new_Xlocation_column_header = []
    for t in column_headings['X']:
        if 'Lat-Long' in t:
            matches = pattern.search(t)
            new_Xlocation_column_header.append(float(matches.group(0)))
        else:
            new_Xlocation_column_header.append(t)
    column_headings['X'] = new_Xlocation_column_header
    lon = NAMECoord(name='longitude', dimension=None,
                    values=column_headings['X'])

    new_Ylocation_column_header = []
    for t in column_headings['Y']:
        if 'Lat-Long' in t:
            matches = pattern.search(t)
            new_Ylocation_column_header.append(float(matches.group(0)))
        else:
            new_Ylocation_column_header.append(t)
    column_headings['Y'] = new_Ylocation_column_header
    lat = NAMECoord(name='latitude', dimension=None,
                    values=column_headings['Y'])

    return lat, lon


def _calc_integration_period(time_avgs):
    """
    Return a list of datetime.timedelta objects determined from the provided
    list of averaging/integration period column headings.

    """
    integration_periods = []
    pattern = re.compile(
        r'(\d{0,2})(day)?\s*(\d{1,2})(hr)?\s*(\d{1,2})(min)?\s*(\w*)')
    for time_str in time_avgs:
        days = 0
        hours = 0
        minutes = 0
        matches = pattern.search(time_str)
        if matches:
            if len(matches.group(1)) > 0:
                days = float(matches.group(1))
            if len(matches.group(3)) > 0:
                hours = float(matches.group(3))
            if len(matches.group(1)) > 0:
                minutes = float(matches.group(5))
        total_hours = days * 24.0 + hours + minutes / 60.0
        integration_periods.append(datetime.timedelta(hours=total_hours))
    return integration_periods


def _parse_units(units):
    """
    Return a known :class:`iris.unit.Unit` given a NAME unit

    .. note::

        * Some NAME units are not currently handled.
        * Units which are in the wrong case (case is ignored in NAME)
        * Units where the space between SI units is missing
        * Units where the characters used are non-standard (i.e. 'mc' for
          micro instead of 'u')

    Args:

    * units (string):
        NAME units.

    Returns:
        An instance of :class:`iris.unit.Unit`.

    """

    unit_mapper = {'Risks/m3': '1',    # Used for Bluetongue
                   'TCID50s/m3': '1',  # Used for Foot and Mouth
                   'TCID50/m3': '1',   # Used for Foot and Mouth
                   'N/A': '1',         # Used for CHEMET area at risk
                   'lb': 'pounds',     # pounds
                   'oz': '1',          # ounces
                   'deg': 'degree',    # angular degree
                   'oktas': '1',       # oktas
                   'deg C': 'deg_C',   # degrees Celsius
                   'FL': 'unknown'     # flight level
                   }

    units = unit_mapper.get(units, units)

    units = units.replace('Kg', 'kg')
    units = units.replace('gs', 'g s')
    units = units.replace('Bqs', 'Bq s')
    units = units.replace('mcBq', 'uBq')
    units = units.replace('mcg', 'ug')
    try:
        units = iris.unit.Unit(units)
    except ValueError:
        warnings.warn('Unknown units: {!r}'.format(units))
        units = iris.unit.Unit(None)

    return units


def _cf_height_from_name(z_coord):
    """
    Parser for the z component of field headings.

    This parse is specifically for handling the z component of NAME field
    headings, which include height above ground level, height above sea level
    and flight level etc.  This function returns an iris coordinate
    representing this field heading.

    Args:

    * z_coord (list):
        A field heading, specifically the z component.

    Returns:
        An instance of :class:`iris.coords.AuxCoord` representing the
        interpretation of the supplied field heading.

    """

    # NAMEII - integer/float support.
    # Match against height agl, asl and Pa.
    pattern = re.compile(r'^From\s*'
                         '(?P<lower_bound>[0-9]+(\.[0-9]+)?)'
                         '\s*-\s*'
                         '(?P<upper_bound>[0-9]+(\.[0-9]+)?)'
                         '\s*(?P<type>m\s*asl|m\s*agl|Pa)'
                         '(?P<extra>.*)')

    # Match against flight level.
    pattern_fl = re.compile(r'^From\s*'
                            '(?P<type>FL)'
                            '(?P<lower_bound>[0-9]+(\.[0-9]+)?)'
                            '\s*-\s*FL'
                            '(?P<upper_bound>[0-9]+(\.[0-9]+)?)'
                            '(?P<extra>.*)')

    # NAMEIII - integer/float support.
    # Match scalar against height agl, asl, Pa, FL
    pattern_scalar = re.compile(r'Z\s*=\s*'
                                '(?P<point>[0-9]+(\.[0-9]+)?)'
                                '\s*(?P<type>m\s*agl|m\s*asl|FL|Pa)'
                                '(?P<extra>.*)')

    type_name = {'magl': 'height', 'masl': 'altitude', 'FL': 'flight_level',
                 'Pa': 'air_pressure'}
    patterns = [pattern, pattern_fl, pattern_scalar]

    units = 'no-unit'
    points = z_coord
    bounds = None
    standard_name = None
    long_name = 'z'
    for pattern in patterns:
        match = pattern.match(z_coord)
        if match:
            match = match.groupdict()
            # Do not interpret if there is additional information to the match
            if match['extra']:
                break
            units = match['type'].replace(' ', '')
            name = type_name[units]

            # Interpret points if present.
            if 'point' in match:
                points = float(match['point'])
            # Interpret points from bounds.
            else:
                bounds = np.array([float(match['lower_bound']),
                                   float(match['upper_bound'])])
                points = bounds.sum() / 2.

            long_name = None
            if name == 'altitude':
                units = units[0]
                standard_name = name
                long_name = 'altitude above sea level'
            elif name == 'height':
                units = units[0]
                standard_name = name
                long_name = 'height above ground level'
            elif name == 'air_pressure':
                standard_name = name
            elif name == 'flight_level':
                long_name = name
            units = _parse_units(units)

            break

    coord = AuxCoord(points, units=units, standard_name=standard_name,
                     long_name=long_name, bounds=bounds)

    return coord


def _generate_cubes(header, column_headings, coords, data_arrays,
                    cell_methods=None):
    """
    Yield :class:`iris.cube.Cube` instances given
    the headers, column headings, coords and data_arrays extracted
    from a NAME file.

    """
    for i, data_array in enumerate(data_arrays):
        # Turn the dictionary of column headings with a list of header
        # information for each field into a dictionary of headings for
        # just this field.
        field_headings = {k: v[i] for k, v in
                          column_headings.iteritems()}

        # Make a cube.
        cube = iris.cube.Cube(data_array)

        # Determine the name and units.
        name = '{} {}'.format(field_headings['Species'],
                              field_headings['Quantity'])
        name = name.upper().replace(' ', '_')
        cube.rename(name)

        # Some units are not in SI units, are missing spaces or typed
        # in the wrong case. _parse_units returns units that are
        # recognised by Iris.
        cube.units = _parse_units(field_headings['Unit'])

        # Define and add the singular coordinates of the field (flight
        # level, time etc.)
        z_coord = _cf_height_from_name(field_headings['Z'])
        cube.add_aux_coord(z_coord)

        # Define the time unit and use it to serialise the datetime for
        # the time coordinate.
        time_unit = iris.unit.Unit(
            'hours since epoch', calendar=iris.unit.CALENDAR_GREGORIAN)

        # Build time, latitude and longitude coordinates.
        for coord in coords:
            pts = coord.values
            coord_sys = None
            if coord.name == 'latitude' or coord.name == 'longitude':
                coord_units = 'degrees'
                coord_sys = iris.coord_systems.GeogCS(EARTH_RADIUS)
            if coord.name == 'time':
                coord_units = time_unit
                pts = time_unit.date2num(coord.values)

            if coord.dimension is not None:
                icoord = DimCoord(points=pts,
                                  standard_name=coord.name,
                                  units=coord_units,
                                  coord_system=coord_sys)
                if coord.name == 'time' and 'Av or Int period' in \
                        field_headings:
                    dt = coord.values - \
                        field_headings['Av or Int period']
                    bnds = time_unit.date2num(
                        np.vstack((dt, coord.values)).T)
                    icoord.bounds = bnds
                else:
                    icoord.guess_bounds()
                cube.add_dim_coord(icoord, coord.dimension)
            else:
                icoord = AuxCoord(points=pts[i],
                                  standard_name=coord.name,
                                  coord_system=coord_sys,
                                  units=coord_units)
                if coord.name == 'time' and 'Av or Int period' in \
                        field_headings:
                    dt = coord.values - \
                        field_headings['Av or Int period']
                    bnds = time_unit.date2num(
                        np.vstack((dt, coord.values)).T)
                    icoord.bounds = bnds[i, :]
                cube.add_aux_coord(icoord)

        # Headings/column headings which are encoded elsewhere.
        headings = ['X', 'Y', 'Z', 'Time', 'Unit', 'Av or Int period',
                    'X grid origin', 'Y grid origin',
                    'X grid size', 'Y grid size',
                    'X grid resolution', 'Y grid resolution', ]

        # Add the Main Headings as attributes.
        for key, value in header.iteritems():
            if value is not None and value != '' and \
                    key not in headings:
                cube.attributes[key] = value

        # Add the Column Headings as attributes
        for key, value in field_headings.iteritems():
            if value is not None and value != '' and \
                    key not in headings:
                cube.attributes[key] = value

        if cell_methods is not None:
            cube.add_cell_method(cell_methods[i])

        yield cube


def _build_cell_methods(av_or_ints, coord):
    """
    Return a list of :class:`iris.coords.CellMethod` instances
    based on the provided list of column heading entries and the
    associated coordinate. If a given entry does not correspond to a cell
    method (e.g. "No time averaging"), a value of None is inserted.

    Args:

    * av_or_ints (iterable of strings):
        An iterable of strings containing the colummn heading entries
        to be parsed.
    * coord (string or :class:`iris.coords.Coord`):
        The coordinate name (or :class:`iris.coords.Coord` instance)
        to which the column heading entries refer.

    Returns:
        A list that is the same length as `av_or_ints` containing
        :class:`iris.coords.CellMethod` instances or values of None.

    """
    cell_methods = []
    no_avg_pattern = re.compile(r'^(no( (.* )?averaging)?)?$', re.IGNORECASE)
    for av_or_int in av_or_ints:
        if no_avg_pattern.search(av_or_int) is not None:
            cell_method = None
        elif 'average' in av_or_int or 'averaged' in av_or_int:
            cell_method = CellMethod('mean', coord)
        elif 'integral' in av_or_int or 'integrated' in av_or_int:
            cell_method = CellMethod('sum', coord)
        else:
            cell_method = None
            msg = 'Unknown {} statistic: {!r}. Unable to create cell method.'
            warnings.warn(msg.format(coord, av_or_int))
        cell_methods.append(cell_method)
    return cell_methods


def load_NAMEIII_field(filename):
    """
    Load a NAME III grid output file returning a
    generator of :class:`iris.cube.Cube` instances.

    Args:

    * filename (string):
        Name of file to load.

    Returns:
        A generator :class:`iris.cube.Cube` instances.

    """
    # Loading a file gives a generator of lines which can be progressed using
    # the next() method. This will come in handy as we wish to progress
    # through the file line by line.
    with open(filename, 'r') as file_handle:
        # Create a dictionary which can hold the header metadata about this
        # file.
        header = read_header(file_handle)

        # Skip the next line (contains the word Fields:) in the file.
        file_handle.next()

        # Read the lines of column definitions.
        # In this version a fixed order of column headings is assumed (and
        # first 4 columns are ignored).
        column_headings = {}
        for column_header_name in ['Species Category', 'Name', 'Quantity',
                                   'Species', 'Unit', 'Sources', 'Ensemble Av',
                                   'Time Av or Int', 'Horizontal Av or Int',
                                   'Vertical Av or Int', 'Prob Perc',
                                   'Prob Perc Ens', 'Prob Perc Time',
                                   'Time', 'Z', 'D']:
            cols = [col.strip() for col in file_handle.next().split(',')]
            column_headings[column_header_name] = cols[4:-1]

        # Convert the time to python datetimes.
        new_time_column_header = []
        for i, t in enumerate(column_headings['Time']):
            dt = datetime.datetime.strptime(t, NAMEIII_DATETIME_FORMAT)
            new_time_column_header.append(dt)
        column_headings['Time'] = new_time_column_header

        # Convert averaging/integrating period to timedeltas.
        column_headings['Av or Int period'] = _calc_integration_period(
            column_headings['Time Av or Int'])

        # Build a time coordinate.
        tdim = NAMECoord(name='time', dimension=None,
                         values=np.array(column_headings['Time']))

        cell_methods = _build_cell_methods(column_headings['Time Av or Int'],
                                           tdim.name)

        # Build regular latitude and longitude coordinates.
        lat, lon = _build_lat_lon_for_NAME_field(header)

        coords = [lon, lat, tdim]

        # Skip the line after the column headings.
        file_handle.next()

        # Create data arrays to hold the data for each column.
        n_arrays = header['Number of field cols']
        shape = (header['Y grid size'], header['X grid size'])
        data_arrays = _read_data_arrays(file_handle, n_arrays, shape)

    return _generate_cubes(header, column_headings, coords, data_arrays,
                           cell_methods)


def load_NAMEII_field(filename):
    """
    Load a NAME II grid output file returning a
    generator of :class:`iris.cube.Cube` instances.

    Args:

    * filename (string):
        Name of file to load.

    Returns:
        A generator :class:`iris.cube.Cube` instances.

    """
    with open(filename, 'r') as file_handle:
        # Create a dictionary which can hold the header metadata about this
        # file.
        header = read_header(file_handle)

        # Origin in namever=2 format is bottom-left hand corner so alter this
        # to centre of a grid box
        header['X grid origin'] = header['X grid origin'] + \
            header['X grid resolution'] / 2
        header['Y grid origin'] = header['Y grid origin'] + \
            header['Y grid resolution'] / 2

        # Read the lines of column definitions.
        # In this version a fixed order of column headings is assumed (and
        # first 4 columns are ignored).
        column_headings = {}
        for column_header_name in ['Species Category', 'Species',
                                   'Time Av or Int', 'Quantity',
                                   'Unit', 'Z', 'Time']:
            cols = [col.strip() for col in file_handle.next().split(',')]
            column_headings[column_header_name] = cols[4:-1]

        # Convert the time to python datetimes
        new_time_column_header = []
        for i, t in enumerate(column_headings['Time']):
            dt = datetime.datetime.strptime(t, NAMEII_FIELD_DATETIME_FORMAT)
            new_time_column_header.append(dt)
        column_headings['Time'] = new_time_column_header

        # Convert averaging/integrating period to timedeltas.
        pattern = re.compile(r'\s*(\d{3})\s*(hr)?\s*(time)\s*(\w*)')
        column_headings['Av or Int period'] = []
        for i, t in enumerate(column_headings['Time Av or Int']):
            matches = pattern.search(t)
            hours = 0
            if matches:
                if len(matches.group(1)) > 0:
                    hours = float(matches.group(1))
            column_headings['Av or Int period'].append(
                datetime.timedelta(hours=hours))

        # Build a time coordinate.
        tdim = NAMECoord(name='time', dimension=None,
                         values=np.array(column_headings['Time']))

        cell_methods = _build_cell_methods(column_headings['Time Av or Int'],
                                           tdim.name)

        # Build regular latitude and longitude coordinates.
        lat, lon = _build_lat_lon_for_NAME_field(header)

        coords = [lon, lat, tdim]

        # Skip the blank line after the column headings.
        file_handle.next()

        # Create data arrays to hold the data for each column.
        n_arrays = header['Number of fields']
        shape = (header['Y grid size'], header['X grid size'])
        data_arrays = _read_data_arrays(file_handle, n_arrays, shape)

    return _generate_cubes(header, column_headings, coords, data_arrays,
                           cell_methods)


def load_NAMEIII_timeseries(filename):
    """
    Load a NAME III time series file returning a
    generator of :class:`iris.cube.Cube` instances.

    Args:

    * filename (string):
        Name of file to load.

    Returns:
        A generator :class:`iris.cube.Cube` instances.

    """
    with open(filename, 'r') as file_handle:
        # Create a dictionary which can hold the header metadata about this
        # file.
        header = read_header(file_handle)

        # skip the next line (contains the word Fields:) in the file.
        file_handle.next()

        # Read the lines of column definitions - currently hardwired
        column_headings = {}
        for column_header_name in ['Species Category', 'Name', 'Quantity',
                                   'Species', 'Unit', 'Sources', 'Ens Av',
                                   'Time Av or Int', 'Horizontal Av or Int',
                                   'Vertical Av or Int', 'Prob Perc',
                                   'Prob Perc Ens', 'Prob Perc Time',
                                   'Location', 'X', 'Y', 'Z', 'D']:
            cols = [col.strip() for col in file_handle.next().split(',')]
            column_headings[column_header_name] = cols[1:-1]

        # Determine the coordinates of the data and store in namedtuples.
        # Extract latitude and longitude information from X, Y location
        # headings.
        lat, lon = _build_lat_lon_for_NAME_timeseries(column_headings)

        # Convert averaging/integrating period to timedeltas.
        column_headings['Av or Int period'] = _calc_integration_period(
            column_headings['Time Av or Int'])

        # Skip the line after the column headings.
        file_handle.next()

        # Make a list of data lists to hold the data for each column.
        data_lists = [[] for i in range(header['Number of field cols'])]
        time_list = []

        # Iterate over the remaining lines which represent the data in a
        # column form.
        for line in file_handle:
            # Split the line by comma, removing the last empty column caused
            # by the trailing comma.
            vals = line.split(',')[:-1]

            # Time is stored in the first column.
            t = vals[0].strip()
            dt = datetime.datetime.strptime(t, NAMEIII_DATETIME_FORMAT)
            time_list.append(dt)

            # Populate the data arrays.
            for i, data_list in enumerate(data_lists):
                data_list.append(float(vals[i + 1]))

        data_arrays = [np.array(l) for l in data_lists]
        time_array = np.array(time_list)
        tdim = NAMECoord(name='time', dimension=0, values=time_array)

        coords = [lon, lat, tdim]

    return _generate_cubes(header, column_headings, coords, data_arrays)


def load_NAMEII_timeseries(filename):
    """
    Load a NAME II Time Series file returning a
    generator of :class:`iris.cube.Cube` instances.

    Args:

    * filename (string):
        Name of file to load.

    Returns:
        A generator :class:`iris.cube.Cube` instances.

    """
    with open(filename, 'r') as file_handle:
        # Create a dictionary which can hold the header metadata about this
        # file.
        header = read_header(file_handle)

        # Read the lines of column definitions.
        column_headings = {}
        for column_header_name in ['Y', 'X', 'Location',
                                   'Species Category', 'Species',
                                   'Quantity', 'Z', 'Unit']:
            cols = [col.strip() for col in file_handle.next().split(',')]
            column_headings[column_header_name] = cols[1:-1]

        # Determine the coordinates of the data and store in namedtuples.
        # Extract latitude and longitude information from X, Y location
        # headings.
        lat, lon = _build_lat_lon_for_NAME_timeseries(column_headings)

        # Skip the blank line after the column headings.
        file_handle.next()

        # Make a list of data arrays to hold the data for each column.
        data_lists = [[] for i in range(header['Number of series'])]
        time_list = []

        # Iterate over the remaining lines which represent the data in a
        # column form.
        for line in file_handle:
            # Split the line by comma, removing the last empty column caused
            # by the trailing comma.
            vals = line.split(',')[:-1]

            # Time is stored in the first two columns.
            t = (vals[0].strip() + ' ' + vals[1].strip())
            dt = datetime.datetime.strptime(
                t, NAMEII_TIMESERIES_DATETIME_FORMAT)
            time_list.append(dt)

            # Populate the data arrays.
            for i, data_list in enumerate(data_lists):
                data_list.append(float(vals[i + 2]))

        data_arrays = [np.array(l) for l in data_lists]
        time_array = np.array(time_list)
        tdim = NAMECoord(name='time', dimension=0, values=time_array)

        coords = [lon, lat, tdim]

    return _generate_cubes(header, column_headings, coords, data_arrays)


def load_NAMEIII_trajectory(filename):
    """
    Load a NAME III trajectory file returning a
    generator of :class:`iris.cube.Cube` instances.

    Args:

    * filename (string):
        Name of file to load.

    Returns:
        A generator :class:`iris.cube.Cube` instances.

    """
    time_unit = iris.unit.Unit('hours since epoch',
                               calendar=iris.unit.CALENDAR_GREGORIAN)

    with open(filename, 'r') as infile:
        header = read_header(infile)

        # read the column headings
        for line in infile:
            if line.startswith("    "):
                break
        headings = [heading.strip() for heading in line.split(",")]

        # read the columns
        columns = [[] for i in range(len(headings))]
        for line in infile:
            values = [v.strip() for v in line.split(",")]
            for c, v in enumerate(values):
                if "UTC" in v:
                    v = v.replace(":00 ", " ")  # Strip out milliseconds.
                    v = datetime.datetime.strptime(v, NAMEIII_DATETIME_FORMAT)
                else:
                    try:
                        v = float(v)
                    except ValueError:
                        pass
                columns[c].append(v)

    # Where's the Z column?
    z_column = None
    for i, heading in enumerate(headings):
        if heading.startswith("Z "):
            z_column = i
            break
    if z_column is None:
        raise iris.exceptions.TranslationError("Expected a Z column")

    # Every column up to Z becomes a coordinate.
    coords = []
    for name, values in izip(headings[:z_column+1], columns[:z_column+1]):
        values = np.array(values)
        if np.all(np.array(values) == values[0]):
            values = [values[0]]

        standard_name = long_name = units = None
        if isinstance(values[0], datetime.datetime):
            values = time_unit.date2num(values)
            units = time_unit
            if name == "Time":
                name = "time"
        elif " (Lat-Long)" in name:
            if name.startswith("X"):
                name = "longitude"
            elif name.startswith("Y"):
                name = "latitude"
            units = "degrees"
        elif name == "Z (m asl)":
            name = "height"
            units = "m"
            long_name = "height above sea level"

        try:
            coord = DimCoord(values, units=units)
        except ValueError:
            coord = AuxCoord(values, units=units)
        coord.rename(name)
        if coord.long_name is None and long_name is not None:
            coord.long_name = long_name
        coords.append(coord)

    # Every numerical column after the Z becomes a cube.
    for name, values in izip(headings[z_column+1:], columns[z_column+1:]):
        try:
            float(values[0])
        except ValueError:
            continue
        # units embedded in column heading?
        name, units = _split_name_and_units(name)
        cube = iris.cube.Cube(values, units=units)
        cube.rename(name)
        for coord in coords:
            dim = 0 if len(coord.points) > 1 else None
            if isinstance(coord, DimCoord) and coord.name() == "time":
                cube.add_dim_coord(coord.copy(), dim)
            else:
                cube.add_aux_coord(coord.copy(), dim)
        yield cube

########NEW FILE########
__FILENAME__ = netcdf
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Module to support the loading of a NetCDF file into an Iris cube.

See also: `netCDF4 python <http://code.google.com/p/netcdf4-python/>`_.

Also refer to document 'NetCDF Climate and Forecast (CF) Metadata Conventions',
Version 1.4, 27 February 2009.

"""

import collections
import itertools
import os
import os.path
import string
import warnings

import biggus
import iris.proxy
iris.proxy.apply_proxy('netCDF4', globals())
import numpy as np
import numpy.ma as ma
from pyke import knowledge_engine

import iris.analysis
from iris.aux_factory import HybridHeightFactory, HybridPressureFactory, \
    OceanSigmaZFactory
import iris.coord_systems
import iris.coords
import iris.cube
import iris.exceptions
import iris.fileformats.cf
import iris.fileformats._pyke_rules
import iris.io
import iris.unit
import iris.util


# Show Pyke inference engine statistics.
DEBUG = False

# Pyke CF related file names.
_PYKE_RULE_BASE = 'fc_rules_cf'
_PYKE_FACT_BASE = 'facts_cf'

# Standard CML spatio-temporal axis names.
SPATIO_TEMPORAL_AXES = ['t', 'z', 'y', 'x']

# Pass through CF attributes:
#  - comment
#  - Conventions
#  - history
#  - institution
#  - reference
#  - source
#  - title
#  - positive
#
_CF_ATTRS = ['add_offset', 'ancillary_variables', 'axis', 'bounds', 'calendar',
             'cell_measures', 'cell_methods', 'climatology', 'compress',
             'coordinates', '_FillValue', 'flag_masks', 'flag_meanings',
             'flag_values', 'formula_terms', 'grid_mapping', 'leap_month',
             'leap_year', 'long_name', 'missing_value', 'month_lengths',
             'scale_factor', 'standard_error_multiplier',
             'standard_name', 'units', 'valid_max', 'valid_min', 'valid_range']

# CF attributes that should not be global.
_CF_DATA_ATTRS = ['flag_masks', 'flag_meanings', 'flag_values',
                  'instance_dimension', 'sample_dimension',
                  'standard_error_multiplier']

# CF attributes that should only be global.
_CF_GLOBAL_ATTRS = ['conventions', 'featureType', 'history', 'title']

# UKMO specific attributes that should not be global.
_UKMO_DATA_ATTRS = ['STASH', 'um_stash_source', 'ukmo__process_flags']

_CF_CONVENTIONS_VERSION = 'CF-1.5'

_FactoryDefn = collections.namedtuple('_FactoryDefn', ('primary', 'std_name',
                                                       'formula_terms_format'))
_FACTORY_DEFNS = {
    HybridHeightFactory: _FactoryDefn(
        primary='delta',
        std_name='atmosphere_hybrid_height_coordinate',
        formula_terms_format='a: {delta} b: {sigma} orog: {orography}'),
    HybridPressureFactory: _FactoryDefn(
        primary='delta',
        std_name='atmosphere_hybrid_sigma_pressure_coordinate',
        formula_terms_format='ap: {delta} b: {sigma} '
        'ps: {surface_air_pressure}'),
    OceanSigmaZFactory: _FactoryDefn(
        primary='zlev',
        std_name='ocean_sigma_z_coordinate',
        formula_terms_format='sigma: {sigma} eta: {eta} depth: {depth} '
        'depth_c: {depth_c} nsigma: {nsigma} zlev: {zlev}')
}


class CFNameCoordMap(object):
    """Provide a simple CF name to CF coordinate mapping."""

    _Map = collections.namedtuple('_Map', ['name', 'coord'])

    def __init__(self):
        self._map = []

    def append(self, name, coord):
        """
        Append the given name and coordinate pair to the mapping.

        Args:

        * name:
            CF name of the associated coordinate.

        * coord:
            The coordinate of the associated CF name.

        Returns:
            None.

        """
        self._map.append(CFNameCoordMap._Map(name, coord))

    @property
    def names(self):
        """Return all the CF names."""

        return [pair.name for pair in self._map]

    @property
    def coords(self):
        """Return all the coordinates."""

        return [pair.coord for pair in self._map]

    def name(self, coord):
        """
        Return the CF name, given a coordinate

        Args:

        * coord:
            The coordinate of the associated CF name.

        Returns:
            Coordinate.

        """
        result = None
        for pair in self._map:
            if coord == pair.coord:
                result = pair.name
                break
        if result is None:
            msg = 'Coordinate is not mapped, {!r}'.format(coord)
            raise KeyError(msg)
        return result

    def coord(self, name):
        """
        Return the coordinate, given a CF name.

        Args:

        * name:
            CF name of the associated coordinate.

        Returns:
            CF name.

        """
        result = None
        for pair in self._map:
            if name == pair.name:
                result = pair.coord
                break
        if result is None:
            msg = 'Name is not mapped, {!r}'.format(name)
            raise KeyError(msg)
        return result


def _pyke_kb_engine():
    """Return the PyKE knowledge engine for CF->cube conversion."""

    pyke_dir = os.path.join(os.path.dirname(__file__), '_pyke_rules')
    compile_dir = os.path.join(pyke_dir, 'compiled_krb')
    engine = None

    if os.path.exists(compile_dir):
        tmpvar = [os.path.getmtime(os.path.join(compile_dir, fname)) for
                  fname in os.listdir(compile_dir) if not
                  fname.startswith('_')]
        if tmpvar:
            oldest_pyke_compile_file = min(tmpvar)
            rule_age = os.path.getmtime(
                os.path.join(pyke_dir, _PYKE_RULE_BASE + '.krb'))

            if oldest_pyke_compile_file >= rule_age:
                # Initialise the pyke inference engine.
                engine = knowledge_engine.engine(
                    (None, 'iris.fileformats._pyke_rules.compiled_krb'))

    if engine is None:
        engine = knowledge_engine.engine(iris.fileformats._pyke_rules)

    return engine


class NetCDFDataProxy(object):
    """A reference to the data payload of a single NetCDF file variable."""

    __slots__ = ('shape', 'dtype', 'path', 'variable_name', 'fill_value')

    def __init__(self, shape, dtype, path, variable_name, fill_value):
        self.shape = shape
        self.dtype = dtype
        self.path = path
        self.variable_name = variable_name
        self.fill_value = fill_value

    @property
    def ndim(self):
        return len(self.shape)

    def __getitem__(self, keys):
        dataset = netCDF4.Dataset(self.path)
        try:
            variable = dataset.variables[self.variable_name]
            # Get the NetCDF variable data and slice.
            data = variable[keys]
        finally:
            dataset.close()
        return data

    def __repr__(self):
        fmt = '<{self.__class__.__name__} shape={self.shape}' \
              ' dtype={self.dtype!r} path={self.path!r}' \
              ' variable_name={self.variable_name!r}>'
        return fmt.format(self=self)

    def __getstate__(self):
        return {attr: getattr(self, attr) for attr in self.__slots__}

    def __setstate__(self, state):
        for key, value in state.iteritems():
            setattr(self, key, value)


def _assert_case_specific_facts(engine, cf, cf_group):
    # Initialise pyke engine "provides" hooks.
    engine.provides['coordinates'] = []

    # Assert facts for CF coordinates.
    for cf_name in cf_group.coordinates.iterkeys():
        engine.add_case_specific_fact(_PYKE_FACT_BASE, 'coordinate',
                                      (cf_name,))

    # Assert facts for CF auxiliary coordinates.
    for cf_name in cf_group.auxiliary_coordinates.iterkeys():
        engine.add_case_specific_fact(_PYKE_FACT_BASE, 'auxiliary_coordinate',
                                      (cf_name,))

    # Assert facts for CF grid_mappings.
    for cf_name in cf_group.grid_mappings.iterkeys():
        engine.add_case_specific_fact(_PYKE_FACT_BASE, 'grid_mapping',
                                      (cf_name,))

    # Assert facts for CF labels.
    for cf_name in cf_group.labels.iterkeys():
        engine.add_case_specific_fact(_PYKE_FACT_BASE, 'label',
                                      (cf_name,))

    # Assert facts for CF formula terms associated with the cf_group
    # of the CF data variable.
    formula_root = set()
    for cf_var in cf.cf_group.formula_terms.itervalues():
        for cf_root, cf_term in cf_var.cf_terms_by_root.iteritems():
            # Only assert this fact if the formula root variable is
            # defined in the CF group of the CF data variable.
            if cf_root in cf_group:
                formula_root.add(cf_root)
                engine.add_case_specific_fact(_PYKE_FACT_BASE, 'formula_term',
                                              (cf_var.cf_name, cf_root,
                                               cf_term))

    for cf_root in formula_root:
        engine.add_case_specific_fact(_PYKE_FACT_BASE, 'formula_root',
                                      (cf_root,))


def _pyke_stats(engine, cf_name):
    if DEBUG:
        print '-' * 80
        print 'CF Data Variable: %r' % cf_name

        engine.print_stats()

        print 'Rules Triggered:'

        for rule in sorted(list(engine.rule_triggered)):
            print '\t%s' % rule

        print 'Case Specific Facts:'
        kb_facts = engine.get_kb(_PYKE_FACT_BASE)

        for key in kb_facts.entity_lists.iterkeys():
            for arg in kb_facts.entity_lists[key].case_specific_facts:
                print '\t%s%s' % (key, arg)


def _set_attributes(attributes, key, value):
    """Set attributes dictionary, converting unicode strings appropriately."""

    if isinstance(value, unicode):
        try:
            attributes[str(key)] = str(value)
        except UnicodeEncodeError:
            attributes[str(key)] = value
    else:
        attributes[str(key)] = value


def _load_cube(engine, cf, cf_var, filename):
    """Create the cube associated with the CF-netCDF data variable."""

    # Figure out what the eventual data type will be after any scale/offset
    # transforms.
    dummy_data = np.zeros(1, dtype=cf_var.dtype)
    if hasattr(cf_var, 'scale_factor'):
        dummy_data = cf_var.scale_factor * dummy_data
    if hasattr(cf_var, 'add_offset'):
        dummy_data = cf_var.add_offset + dummy_data

    # Create cube with deferred data, but no metadata
    fill_value = getattr(cf_var.cf_data, '_FillValue',
                         netCDF4.default_fillvals[cf_var.dtype.str[1:]])
    proxy = NetCDFDataProxy(cf_var.shape, dummy_data.dtype,
                            filename, cf_var.cf_name, fill_value)
    data = biggus.OrthoArrayAdapter(proxy)
    cube = iris.cube.Cube(data)

    # Reset the pyke inference engine.
    engine.reset()

    # Initialise pyke engine rule processing hooks.
    engine.cf_var = cf_var
    engine.cube = cube
    engine.provides = {}
    engine.requires = {}
    engine.rule_triggered = set()
    engine.filename = filename

    # Assert any case-specific facts.
    _assert_case_specific_facts(engine, cf, cf_var.cf_group)

    # Run pyke inference engine with forward chaining rules.
    engine.activate(_PYKE_RULE_BASE)

    # Populate coordinate attributes with the untouched attributes from the
    # associated CF-netCDF variable.
    coordinates = engine.provides.get('coordinates', [])
    attribute_predicate = lambda item: item[0] not in _CF_ATTRS

    for coord, cf_var_name in coordinates:
        tmpvar = itertools.ifilter(attribute_predicate,
                                   cf.cf_group[cf_var_name].cf_attrs_unused())
        for attr_name, attr_value in tmpvar:
            _set_attributes(coord.attributes, attr_name, attr_value)

    tmpvar = itertools.ifilter(attribute_predicate, cf_var.cf_attrs_unused())
    # Attach untouched attributes of the associated CF-netCDF data variable to
    # the cube.
    for attr_name, attr_value in tmpvar:
        _set_attributes(cube.attributes, attr_name, attr_value)

    # Show pyke session statistics.
    _pyke_stats(engine, cf_var.cf_name)

    return cube


def _load_aux_factory(engine, cube):
    """
    Convert any CF-netCDF dimensionless coordinate to an AuxCoordFactory.

    """
    formula_type = engine.requires.get('formula_type')
    if formula_type in ['atmosphere_hybrid_height_coordinate',
                        'atmosphere_hybrid_sigma_pressure_coordinate',
                        'ocean_sigma_z_coordinate']:
        def coord_from_term(term):
            # Convert term names to coordinates (via netCDF variable names).
            name = engine.requires['formula_terms'][term]
            for coord, cf_var_name in engine.provides['coordinates']:
                if cf_var_name == name:
                    return coord
            raise ValueError('Unable to find coordinate for variable '
                             '{!r}'.format(name))

        if formula_type == 'atmosphere_hybrid_height_coordinate':
            delta = coord_from_term('a')
            sigma = coord_from_term('b')
            orography = coord_from_term('orog')
            factory = HybridHeightFactory(delta, sigma, orography)
        elif formula_type == 'atmosphere_hybrid_sigma_pressure_coordinate':
            # Hybrid pressure has two valid versions of its formula terms:
            # "p0: var1 a: var2 b: var3 ps: var4" or
            # "ap: var1 b: var2 ps: var3" where "ap = p0 * a"
            try:
                # Attempt to get the "ap" term.
                delta = coord_from_term('ap')
            except (KeyError, ValueError):
                # The "ap" term is unavailable, so try getting terms "p0"
                # and "a" terms in order to derive an "ap" equivalent term.
                coord_p0 = coord_from_term('p0')
                if coord_p0.shape != (1,):
                    msg = 'Expecting {!r} to be a scalar reference pressure ' \
                        'coordinate, got shape {!r}'.format(coord_p0.var_name,
                                                            coord_p0.shape)
                    raise ValueError(msg)
                if coord_p0.has_bounds():
                    msg = 'Ignoring atmosphere hybrid sigma pressure scalar ' \
                        'coordinate {!r} bounds.'.format(coord_p0.name())
                    warnings.warn(msg)
                coord_a = coord_from_term('a')
                delta = coord_a * coord_p0.points[0]
                delta.units = coord_a.units * coord_p0.units
                delta.rename('vertical pressure')
                delta.var_name = 'ap'
                cube.add_aux_coord(delta, cube.coord_dims(coord_a))

            sigma = coord_from_term('b')
            surface_air_pressure = coord_from_term('ps')
            factory = HybridPressureFactory(delta, sigma, surface_air_pressure)
        elif formula_type == 'ocean_sigma_z_coordinate':
            sigma = coord_from_term('sigma')
            eta = coord_from_term('eta')
            depth = coord_from_term('depth')
            depth_c = coord_from_term('depth_c')
            nsigma = coord_from_term('nsigma')
            zlev = coord_from_term('zlev')
            factory = OceanSigmaZFactory(sigma, eta, depth,
                                         depth_c, nsigma, zlev)
        cube.add_aux_factory(factory)


def load_cubes(filenames, callback=None):
    """
    Loads cubes from a list of NetCDF filenames/URLs.

    Args:

    * filenames (string/list):
        One or more NetCDF filenames/DAP URLs to load from.

    Kwargs:

    * callback (callable function):
        Function which can be passed on to :func:`iris.io.run_callback`.

    Returns:
        Generator of loaded NetCDF :class:`iris.cubes.Cube`.

    """
    # Initialise the pyke inference engine.
    engine = _pyke_kb_engine()

    if isinstance(filenames, basestring):
        filenames = [filenames]

    for filename in filenames:
        # Ingest the netCDF file.
        cf = iris.fileformats.cf.CFReader(filename)

        # Process each CF data variable.
        for cf_var in cf.cf_group.data_variables.itervalues():
            cube = _load_cube(engine, cf, cf_var, filename)

            # Process any associated formula terms and attach
            # the corresponding AuxCoordFactory.
            _load_aux_factory(engine, cube)

            # Perform any user registered callback function.
            cube = iris.io.run_callback(callback, cube, engine.cf_var,
                                        filename)

            # Callback mechanism may return None, which must not be yielded
            if cube is None:
                continue

            yield cube


class Saver(object):
    """A manager for saving netcdf files."""

    def __init__(self, filename, netcdf_format):
        """
        A manager for saving netcdf files.

        Args:

        * filename (string):
            Name of the netCDF file to save the cube.

        * netcdf_format (string):
            Underlying netCDF file format, one of 'NETCDF4', 'NETCDF4_CLASSIC',
            'NETCDF3_CLASSIC' or 'NETCDF3_64BIT'. Default is 'NETCDF4' format.

        Returns:
            None.

        For example::

            # Initialise Manager for saving
            with Saver(filename, netcdf_format) as sman:
                # Iterate through the cubelist.
                for cube in cubes:
                    sman.write(cube)

        """
        if netcdf_format not in ['NETCDF4', 'NETCDF4_CLASSIC',
                                 'NETCDF3_CLASSIC', 'NETCDF3_64BIT']:
            raise ValueError('Unknown netCDF file format, got %r' %
                             netcdf_format)

        # All persistent variables
        #: CF name mapping with iris coordinates
        self._name_coord_map = CFNameCoordMap()
        #: List of dimension coordinates added to the file
        self._dim_coords = []
        #: List of grid mappings added to the file
        self._coord_systems = []
        #: A dictionary, listing dimension names and corresponding length
        self._existing_dim = {}
        #: NetCDF dataset
        try:
            self._dataset = netCDF4.Dataset(filename, mode='w',
                                            format=netcdf_format)
        except RuntimeError:
            dir_name = os.path.dirname(filename)
            if not os.path.isdir(dir_name):
                msg = 'No such file or directory: {}'.format(dir_name)
                raise IOError(msg)
            if not os.access(dir_name, os.R_OK | os.W_OK):
                msg = 'Permission denied: {}'.format(filename)
                raise IOError(msg)
            else:
                raise

    def __enter__(self):
        return self

    def __exit__(self, type, value, traceback):
        """Flush any buffered data to the CF-netCDF file before closing."""

        self._dataset.sync()
        self._dataset.close()

    def write(self, cube, local_keys=None, unlimited_dimensions=None,
              zlib=False, complevel=4, shuffle=True, fletcher32=False,
              contiguous=False, chunksizes=None, endian='native',
              least_significant_digit=None):
        """
        Wrapper for saving cubes to a NetCDF file.

        Args:

        * cube (:class:`iris.cube.Cube`):
            A :class:`iris.cube.Cube` to be saved to a netCDF file.

        Kwargs:

        * local_keys (iterable of strings):
            An interable of cube attribute keys. Any cube attributes with
            matching keys will become attributes on the data variable rather
            than global attributes.

        * unlimited_dimensions (iterable of strings and/or
          :class:`iris.coords.Coord` objects):
            Explicit list of coordinate names (or coordinate objects)
            corresponding to coordinate dimensions of `cube` to save with the
            NetCDF dimension variable length 'UNLIMITED'. By default, the
            outermost (first) dimension for each cube is used. Only the
            'NETCDF4' format supports multiple 'UNLIMITED' dimensions. To save
            no unlimited dimensions, use `unlimited_dimensions=[]` (an empty
            list).

        * zlib (bool):
            If `True`, the data will be compressed in the netCDF file using
            gzip compression (default `False`).

        * complevel (int):
            An integer between 1 and 9 describing the level of compression
            desired (default 4). Ignored if `zlib=False`.

        * shuffle (bool):
            If `True`, the HDF5 shuffle filter will be applied before
            compressing the data (default `True`). This significantly improves
            compression. Ignored if `zlib=False`.

        * fletcher32 (bool):
            If `True`, the Fletcher32 HDF5 checksum algorithm is activated to
            detect errors. Default `False`.

        * contiguous (bool):
            If `True`, the variable data is stored contiguously on disk.
            Default `False`. Setting to `True` for a variable with an unlimited
            dimension will trigger an error.

        * chunksizes (tuple of int):
            Used to manually specify the HDF5 chunksizes for each dimension of
            the variable. A detailed discussion of HDF chunking and I/O
            performance is available here:
            http://www.hdfgroup.org/HDF5/doc/H5.user/Chunking.html. Basically,
            you want the chunk size for each dimension to match as closely as
            possible the size of the data block that users will read from the
            file. `chunksizes` cannot be set if `contiguous=True`.

        * endian (string):
            Used to control whether the data is stored in little or big endian
            format on disk. Possible values are 'little', 'big' or 'native'
            (default). The library will automatically handle endian conversions
            when the data is read, but if the data is always going to be read
            on a computer with the opposite format as the one used to create
            the file, there may be some performance advantage to be gained by
            setting the endian-ness.

        * least_significant_digit (int):
            If `least_significant_digit` is specified, variable data will be
            truncated (quantized). In conjunction with `zlib=True` this
            produces 'lossy', but significantly more efficient compression. For
            example, if `least_significant_digit=1`, data will be quantized
            using `numpy.around(scale*data)/scale`, where `scale = 2**bits`,
            and `bits` is determined so that a precision of 0.1 is retained (in
            this case `bits=4`). From
            http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml:
            "least_significant_digit -- power of ten of the smallest decimal
            place in unpacked data that is a reliable value". Default is
            `None`, or no quantization, or 'lossless' compression.

        Returns:
            None.

        .. note::

            The `zlib`, `complevel`, `shuffle`, `fletcher32`, `contiguous`,
            `chunksizes` and `endian` keywords are silently ignored for netCDF
            3 files that do not use HDF5.

        """
        cf_profile_available = (
            'cf_profile' in iris.site_configuration and
            iris.site_configuration['cf_profile'] not in [None, False])

        if cf_profile_available:
            # Perform a CF profile of the cube. This may result in an exception
            # being raised if mandatory requirements are not satisfied.
            profile = iris.site_configuration['cf_profile'](cube)

        # Get suitable dimension names.
        dimension_names = self._get_dim_names(cube)

        # Create the CF-netCDF data dimensions.
        self._create_cf_dimensions(cube, dimension_names, unlimited_dimensions)

        # Create the associated cube CF-netCDF data variable.
        cf_var_cube = self._create_cf_data_variable(
            cube, dimension_names, local_keys, zlib=zlib, complevel=complevel,
            shuffle=shuffle, fletcher32=fletcher32, contiguous=contiguous,
            chunksizes=chunksizes, endian=endian,
            least_significant_digit=least_significant_digit)

        # Add coordinate variables.
        self._add_dim_coords(cube, dimension_names)

        # Add the auxiliary coordinate variable names and associate the data
        # variable to them
        self._add_aux_coords(cube, cf_var_cube, dimension_names)

        # Add the formula terms to the appropriate cf variables for each
        # aux factory in the cube.
        self._add_aux_factories(cube)

        # Add data variable-only attribute names to local_keys.
        if local_keys is None:
            local_keys = set()
        else:
            local_keys = set(local_keys)
        local_keys.update(_CF_DATA_ATTRS, _UKMO_DATA_ATTRS)

        # Add global attributes taking into account local_keys.
        global_attributes = {k: v for k, v in cube.attributes.iteritems() if k
                             not in local_keys and k.lower() != 'conventions'}
        self.update_global_attributes(global_attributes)

        if cf_profile_available:
            # Perform a CF patch of the dataset.
            iris.site_configuration['cf_patch'](profile, self._dataset,
                                                cf_var_cube)

    def update_global_attributes(self, attributes=None, **kwargs):
        """
        Update the CF global attributes based on the provided
        iterable/dictionary and/or keyword arguments.

        Args:

        * attributes (dict or iterable of key, value pairs):
            CF global attributes to be updated.

        """
        if attributes is not None:
            # Handle sequence e.g. [('fruit', 'apple'), ...].
            if not hasattr(attributes, 'keys'):
                attributes = dict(attributes)

            for attr_name in sorted(attributes):
                setattr(self._dataset, attr_name, attributes[attr_name])

        for attr_name in sorted(kwargs):
            setattr(self._dataset, attr_name, kwargs[attr_name])

    def _create_cf_dimensions(self, cube, dimension_names,
                              unlimited_dimensions=None):
        """
        Create the CF-netCDF data dimensions.

        Args:

        * cube (:class:`iris.cube.Cube`):
            A :class:`iris.cube.Cube` in which to lookup coordinates.

        Kwargs:

        * unlimited_dimensions (iterable of strings and/or
          :class:`iris.coords.Coord` objects):
            List of coordinates to make unlimited. By default, the
            outermost dimension is made unlimited.

        Returns:
            None.

        """
        unlimited_dim_names = []
        if unlimited_dimensions is None:
            if dimension_names:
                unlimited_dim_names.append(dimension_names[0])
        else:
            for coord in unlimited_dimensions:
                try:
                    coord = cube.coord(name_or_coord=coord, dim_coords=True)
                except iris.exceptions.CoordinateNotFoundError:
                    # coordinate isn't used for this cube, but it might be
                    # used for a different one
                    pass
                else:
                    dim_name = self._get_coord_variable_name(cube, coord)
                    unlimited_dim_names.append(dim_name)

        for dim_name in dimension_names:
            if dim_name not in self._dataset.dimensions:
                if dim_name in unlimited_dim_names:
                    size = None
                else:
                    size = self._existing_dim[dim_name]
                self._dataset.createDimension(dim_name, size)

    def _add_aux_coords(self, cube, cf_var_cube, dimension_names):
        """
        Add aux. coordinate to the dataset and associate with the data variable

        Args:

        * cube (:class:`iris.cube.Cube`):
            A :class:`iris.cube.Cube` to be saved to a netCDF file.
        * cf_var_cube (:class:`netcdf.netcdf_variable`):
            cf variable cube representation.
        * dimension_names (list):
            Names associated with the dimensions of the cube.

        """
        auxiliary_coordinate_names = []
        # Add CF-netCDF variables for the associated auxiliary coordinates.
        for coord in sorted(cube.aux_coords, key=lambda coord: coord.name()):
            # Create the associated coordinate CF-netCDF variable.
            if coord not in self._name_coord_map.coords:
                cf_name = self._create_cf_variable(cube, dimension_names,
                                                   coord)
                self._name_coord_map.append(cf_name, coord)
            else:
                cf_name = self._name_coord_map.name(coord)

            if cf_name is not None:
                auxiliary_coordinate_names.append(cf_name)

        # Add CF-netCDF auxiliary coordinate variable references to the
        # CF-netCDF data variable.
        if auxiliary_coordinate_names:
            cf_var_cube.coordinates = ' '.join(
                sorted(auxiliary_coordinate_names))

    def _add_dim_coords(self, cube, dimension_names):
        """
        Add coordinate variables to NetCDF dataset.

        Args:

        * cube (:class:`iris.cube.Cube`):
            A :class:`iris.cube.Cube` to be saved to a netCDF file.
        * dimension_names (list):
            Names associated with the dimensions of the cube.

        """
        # Ensure we create the netCDF coordinate variables first.
        for coord in cube.dim_coords:
            # Create the associated coordinate CF-netCDF variable.
            if coord not in self._name_coord_map.coords:
                cf_name = self._create_cf_variable(cube, dimension_names,
                                                   coord)
                self._name_coord_map.append(cf_name, coord)

    def _add_aux_factories(self, cube):
        """
        Modifies the variables of the NetCDF dataset to represent
        the presence of dimensionless vertical coordinates based on
        the aux factories of the cube (if any).

        Args:

        * cube (:class:`iris.cube.Cube`):
            A :class:`iris.cube.Cube` to be saved to a netCDF file.

        """
        primaries = []
        for factory in cube.aux_factories:
            factory_defn = _FACTORY_DEFNS.get(type(factory), None)
            if factory_defn is None:
                msg = 'Unable to determine formula terms ' \
                      'for AuxFactory: {!r}'.format(factory)
                warnings.warn(msg)
            else:
                # Override `standard_name`, `long_name`, and `axis` of the
                # primary coord that signals the presense of a dimensionless
                # vertical coord, then set the `formula_terms` attribute.
                primary_coord = factory.dependencies[factory_defn.primary]
                if primary_coord in primaries:
                    msg = 'Cube {!r} has multiple aux factories that share ' \
                          'a common primary coordinate {!r}. Unable to save ' \
                          'to netCDF as having multiple formula terms on a ' \
                          'single coordinate is not supported.'
                    raise ValueError(msg.format(cube, primary_coord.name()))
                primaries.append(primary_coord)
                cf_name = self._name_coord_map.name(primary_coord)
                cf_var = self._dataset.variables[cf_name]
                cf_var.standard_name = factory_defn.std_name
                cf_var.axis = 'Z'
                names = {key: self._name_coord_map.name(coord) for
                         key, coord in factory.dependencies.iteritems()}
                formula_terms = factory_defn.formula_terms_format.format(
                    **names)
                cf_var.formula_terms = formula_terms

    def _get_dim_names(self, cube):
        """
        Determine suitable CF-netCDF data dimension names.

        Args:

        * cube (:class:`iris.cube.Cube`):
            A :class:`iris.cube.Cube` to be saved to a netCDF file.

        Returns:
            List of dimension names with length equal the number of dimensions
            in the cube.

        """
        dimension_names = []
        for dim in xrange(cube.ndim):
            coords = cube.coords(dimensions=dim, dim_coords=True)
            if coords:
                coord = coords[0]

                dim_name = self._get_coord_variable_name(cube, coord)
                # Add only dimensions that have not already been added.
                if coord not in self._dim_coords:
                    # Determine unique dimension name
                    while (dim_name in self._existing_dim or
                           dim_name in self._name_coord_map.names):
                        dim_name = self._increment_name(dim_name)

                    # Update names added, current cube dim names used and
                    # unique coordinates added.
                    self._existing_dim[dim_name] = coord.shape[0]
                    dimension_names.append(dim_name)
                    self._dim_coords.append(coord)
                else:
                    # Return the dim_name associated with the existing
                    # coordinate.
                    dim_name = self._name_coord_map.name(coord)
                    dimension_names.append(dim_name)

            else:
                # No CF-netCDF coordinates describe this data dimension.
                dim_name = 'dim%d' % dim
                if dim_name in self._existing_dim:
                    # Increment name if conflicted with one already existing.
                    if self._existing_dim[dim_name] != cube.shape[dim]:
                        while (dim_name in self._existing_dim and
                               self._existing_dim[dim_name] !=
                               cube.shape[dim] or
                               dim_name in self._name_coord_map.names):
                            dim_name = self._increment_name(dim_name)
                        # Update dictionary with new entry
                        self._existing_dim[dim_name] = cube.shape[dim]
                else:
                    # Update dictionary with new entry
                    self._existing_dim[dim_name] = cube.shape[dim]

                dimension_names.append(dim_name)
        return dimension_names

    def _cf_coord_identity(self, coord):
        """
        Determine a suitable units from a given coordinate.

        Args:

        * coord (:class:`iris.coords.Coord`):
            A coordinate of a cube.

        Returns:
            The (standard_name, long_name, unit) of the given
            :class:`iris.coords.Coord` instance.

        """

        units = str(coord.units)

        # TODO: Use #61 to get the units.
        if isinstance(coord.coord_system, iris.coord_systems.GeogCS):
            if "latitude" in coord.standard_name:
                units = 'degrees_north'
            elif "longitude" in coord.standard_name:
                units = 'degrees_east'

        elif isinstance(coord.coord_system, iris.coord_systems.RotatedGeogCS):
            units = 'degrees'

        elif isinstance(coord.coord_system,
                        iris.coord_systems.TransverseMercator):
            units = 'm'

        return coord.standard_name, coord.long_name, units

    def _ensure_valid_dtype(self, values, src_name, src_object):
        # NetCDF3 does not support int64 or unsigned ints, so we check
        # if we can store them as int32 instead.
        if ((np.issubdtype(values.dtype, np.int64) or
                np.issubdtype(values.dtype, np.unsignedinteger)) and
                self._dataset.file_format in ('NETCDF3_CLASSIC',
                                              'NETCDF3_64BIT')):
            # Cast to an integer type supported by netCDF3.
            if not np.can_cast(values.max(), np.int32) or \
                    not np.can_cast(values.min(), np.int32):
                msg = 'The data type of {} {!r} is not supported by {} and' \
                      ' its values cannot be safely cast to a supported' \
                      ' integer type.'
                msg = msg.format(src_name, src_object,
                                 self._dataset.file_format)
                raise ValueError(msg)
            values = values.astype(np.int32)
        return values

    def _create_cf_bounds(self, coord, cf_var, cf_name):
        """
        Create the associated CF-netCDF bounds variable.

        Args:

        * coord (:class:`iris.coords.Coord`):
            A coordinate of a cube.
        * cf_var:
            CF-netCDF variable
        * cf_name (string):
            name of the CF-NetCDF variable.

        Returns:
            None

        """
        if coord.has_bounds():
            # Get the values in a form which is valid for the file format.
            bounds = self._ensure_valid_dtype(coord.bounds,
                                              'the bounds of coordinate',
                                              coord)
            n_bounds = bounds.shape[-1]

            if n_bounds == 2:
                bounds_dimension_name = 'bnds'
            else:
                bounds_dimension_name = 'bnds_%s' % n_bounds

            if bounds_dimension_name not in self._dataset.dimensions:
                # Create the bounds dimension with the appropriate extent.
                self._dataset.createDimension(bounds_dimension_name, n_bounds)

            cf_var.bounds = cf_name + '_bnds'
            cf_var_bounds = self._dataset.createVariable(
                cf_var.bounds, bounds.dtype.newbyteorder('='),
                cf_var.dimensions + (bounds_dimension_name,))
            cf_var_bounds[:] = bounds

    def _get_cube_variable_name(self, cube):
        """
        Returns a CF-netCDF variable name for the given cube.

        Args:

        * cube (class:`iris.cube.Cube`):
            An instance of a cube for which a CF-netCDF variable
            name is required.

        Returns:
            A CF-netCDF variable name as a string.

        """
        if cube.var_name is not None:
            cf_name = cube.var_name
        else:
            # Convert to lower case and replace whitespace by underscores.
            cf_name = '_'.join(cube.name().lower().split())

        return cf_name

    def _get_coord_variable_name(self, cube, coord):
        """
        Returns a CF-netCDF variable name for the given coordinate.

        Args:

        * cube (:class:`iris.cube.Cube`):
            The cube that contains the given coordinate.
        * coord (:class:`iris.coords.Coord`):
            An instance of a coordinate for which a CF-netCDF variable
            name is required.

        Returns:
            A CF-netCDF variable name as a string.

        """
        if coord.var_name is not None:
            cf_name = coord.var_name
        else:
            name = coord.standard_name or coord.long_name
            if not name or set(name).intersection(string.whitespace):
                # Auto-generate name based on associated dimensions.
                name = ''
                for dim in cube.coord_dims(coord):
                    name += 'dim{}'.format(dim)
                # Handle scalar coordinate (dims == ()).
                if not name:
                    name = 'unknown_scalar'
            # Convert to lower case and replace whitespace by underscores.
            cf_name = '_'.join(name.lower().split())

        return cf_name

    def _create_cf_variable(self, cube, dimension_names, coord):
        """
        Create the associated CF-netCDF variable in the netCDF dataset for the
        given coordinate. If required, also create the CF-netCDF bounds
        variable and associated dimension.

        Args:

        * cube (:class:`iris.cube.Cube`):
            The associated cube being saved to CF-netCDF file.
        * dimension_names (list):
            Names for each dimension of the cube.
        * coord (:class:`iris.coords.Coord`):
            The coordinate to be saved to CF-netCDF file.

        Returns:
            The string name of the associated CF-netCDF variable saved.

        """
        cf_name = self._get_coord_variable_name(cube, coord)
        while cf_name in self._dataset.variables:
            cf_name = self._increment_name(cf_name)

        # Derive the data dimension names for the coordinate.
        cf_dimensions = [dimension_names[dim] for dim in
                         cube.coord_dims(coord)]

        if np.issubdtype(coord.points.dtype, np.str):
            string_dimension_depth = coord.points.dtype.itemsize
            string_dimension_name = 'string%d' % string_dimension_depth

            # Determine whether to create the string length dimension.
            if string_dimension_name not in self._dataset.dimensions:
                self._dataset.createDimension(string_dimension_name,
                                              string_dimension_depth)

            # Add the string length dimension to dimension names.
            cf_dimensions.append(string_dimension_name)

            # Create the label coordinate variable.
            cf_var = self._dataset.createVariable(cf_name, '|S1',
                                                  cf_dimensions)

            # Add the payload to the label coordinate variable.
            if len(cf_dimensions) == 1:
                cf_var[:] = list('%- *s' % (string_dimension_depth,
                                            coord.points[0]))
            else:
                for index in np.ndindex(coord.points.shape):
                    index_slice = tuple(list(index) + [slice(None, None)])
                    cf_var[index_slice] = list('%- *s' %
                                               (string_dimension_depth,
                                                coord.points[index]))
        else:
            # Identify the collection of coordinates that represent CF-netCDF
            # coordinate variables.
            cf_coordinates = cube.dim_coords

            if coord in cf_coordinates:
                # By definition of a CF-netCDF coordinate variable this
                # coordinate must be 1-D and the name of the CF-netCDF variable
                # must be the same as its dimension name.
                cf_name = cf_dimensions[0]

            # Get the values in a form which is valid for the file format.
            points = self._ensure_valid_dtype(coord.points, 'coordinate',
                                              coord)

            # Create the CF-netCDF variable.
            cf_var = self._dataset.createVariable(
                cf_name, points.dtype.newbyteorder('='), cf_dimensions)

            # Add the axis attribute for spatio-temporal CF-netCDF coordinates.
            if coord in cf_coordinates:
                axis = iris.util.guess_coord_axis(coord)
                if axis is not None and axis.lower() in SPATIO_TEMPORAL_AXES:
                    cf_var.axis = axis.upper()

            # Add the data to the CF-netCDF variable.
            cf_var[:] = points

            # Create the associated CF-netCDF bounds variable.
            self._create_cf_bounds(coord, cf_var, cf_name)

        # Deal with CF-netCDF units and standard name.
        standard_name, long_name, units = self._cf_coord_identity(coord)

        if units != 'unknown':
            cf_var.units = units

        if standard_name is not None:
            cf_var.standard_name = standard_name

        if long_name is not None:
            cf_var.long_name = long_name

        # Add the CF-netCDF calendar attribute.
        if coord.units.calendar:
            cf_var.calendar = coord.units.calendar

        # Add any other custom coordinate attributes.
        for name in sorted(coord.attributes):
            value = coord.attributes[name]

            if name == 'STASH':
                # Adopting provisional Metadata Conventions for representing MO
                # Scientific Data encoded in NetCDF Format.
                name = 'um_stash_source'
                value = str(value)

            # Don't clobber existing attributes.
            if not hasattr(cf_var, name):
                setattr(cf_var, name, value)

        return cf_name

    def _create_cf_cell_methods(self, cube, dimension_names):
        """
        Create CF-netCDF string representation of a cube cell methods.

        Args:

        * cube (:class:`iris.cube.Cube`) or cubelist
          (:class:`iris.cube.CubeList`):
            A :class:`iris.cube.Cube`, :class:`iris.cube.CubeList` or list of
            cubes to be saved to a netCDF file.
        * dimension_names (list):
            Names associated with the dimensions of the cube.

        Returns:
            CF-netCDF string representation of a cube cell methods.

        """
        cell_methods = []

        # Identify the collection of coordinates that represent CF-netCDF
        # coordinate variables.
        cf_coordinates = cube.dim_coords

        for cm in cube.cell_methods:
            names = ''

            for name in cm.coord_names:
                coord = cube.coords(name)

                if coord:
                    coord = coord[0]
                    if coord in cf_coordinates:
                        name = dimension_names[cube.coord_dims(coord)[0]]

                names += '%s: ' % name

            interval = ' '.join(['interval: %s' % interval for interval in
                                 cm.intervals or []])
            comment = ' '.join(['comment: %s' % comment for comment in
                                cm.comments or []])
            extra = ' '.join([interval, comment]).strip()

            if extra:
                extra = ' (%s)' % extra

            cell_methods.append(names + cm.method + extra)

        return ' '.join(cell_methods)

    def _create_cf_grid_mapping(self, cube, cf_var_cube):
        """
        Create CF-netCDF grid mapping variable and associated CF-netCDF
        data variable grid mapping attribute.

        Args:

        * cube (:class:`iris.cube.Cube`) or cubelist
          (:class:`iris.cube.CubeList`):
            A :class:`iris.cube.Cube`, :class:`iris.cube.CubeList` or list of
            cubes to be saved to a netCDF file.
        * cf_var_cube (:class:`netcdf.netcdf_variable`):
            cf variable cube representation.

        Returns:
            None

        """
        cs = cube.coord_system('CoordSystem')
        if cs is not None:
            # Grid var not yet created?
            if cs not in self._coord_systems:
                while cs.grid_mapping_name in self._dataset.variables:
                    cs.grid_mapping_name = (
                        self._increment_name(cs.grid_mapping_name))

                cf_var_grid = self._dataset.createVariable(
                    cs.grid_mapping_name, np.int32)
                cf_var_grid.grid_mapping_name = cs.grid_mapping_name

                def add_ellipsoid(ellipsoid):
                    cf_var_grid.longitude_of_prime_meridian = (
                        ellipsoid.longitude_of_prime_meridian)
                    semi_major = ellipsoid.semi_major_axis
                    semi_minor = ellipsoid.semi_minor_axis
                    if semi_minor == semi_major:
                        cf_var_grid.earth_radius = semi_major
                    else:
                        cf_var_grid.semi_major_axis = semi_major
                        cf_var_grid.semi_minor_axis = semi_minor

                # latlon
                if isinstance(cs, iris.coord_systems.GeogCS):
                    add_ellipsoid(cs)

                # rotated latlon
                elif isinstance(cs, iris.coord_systems.RotatedGeogCS):
                    if cs.ellipsoid:
                        add_ellipsoid(cs.ellipsoid)
                    cf_var_grid.grid_north_pole_latitude = (
                        cs.grid_north_pole_latitude)
                    cf_var_grid.grid_north_pole_longitude = (
                        cs.grid_north_pole_longitude)
                    cf_var_grid.north_pole_grid_longitude = (
                        cs.north_pole_grid_longitude)

                # tmerc
                elif isinstance(cs, iris.coord_systems.TransverseMercator):
                    if cs.ellipsoid:
                        add_ellipsoid(cs.ellipsoid)
                    cf_var_grid.longitude_of_central_meridian = (
                        cs.longitude_of_central_meridian)
                    cf_var_grid.latitude_of_projection_origin = (
                        cs.latitude_of_projection_origin)
                    cf_var_grid.false_easting = cs.false_easting
                    cf_var_grid.false_northing = cs.false_northing
                    cf_var_grid.scale_factor_at_central_meridian = (
                        cs.scale_factor_at_central_meridian)

                # osgb (a specific tmerc)
                elif isinstance(cs, iris.coord_systems.OSGB):
                    warnings.warn('OSGB coordinate system not yet handled')

                # other
                else:
                    warnings.warn('Unable to represent the horizontal '
                                  'coordinate system. The coordinate system '
                                  'type %r is not yet implemented.' % type(cs))

                self._coord_systems.append(cs)

            # Refer to grid var
            cf_var_cube.grid_mapping = cs.grid_mapping_name

    def _create_cf_data_variable(self, cube, dimension_names, local_keys=None,
                                 **kwargs):
        """
        Create CF-netCDF data variable for the cube and any associated grid
        mapping.

        Args:

        * cube (:class:`iris.cube.Cube`):
            The associated cube being saved to CF-netCDF file.
        * dimension_names (list):
            String names for each dimension of the cube.

        Kwargs:

        * local_keys (iterable of strings):
            An interable of cube attribute keys. Any cube attributes
            with matching keys will become attributes on the data variable.

        All other keywords are passed through to the dataset's `createVariable`
        method.

        Returns:
            The newly created CF-netCDF data variable.

        """
        cf_name = self._get_cube_variable_name(cube)
        while cf_name in self._dataset.variables:
            cf_name = self._increment_name(cf_name)

        # Determine whether there is a cube MDI value.
        fill_value = None
        if isinstance(cube.data, ma.core.MaskedArray):
            fill_value = cube.data.fill_value

        # Get the values in a form which is valid for the file format.
        data = self._ensure_valid_dtype(cube.data, 'cube', cube)

        # Create the cube CF-netCDF data variable with data payload.
        cf_var = self._dataset.createVariable(
            cf_name, data.dtype.newbyteorder('='), dimension_names,
            fill_value=fill_value, **kwargs)
        cf_var[:] = data

        if cube.standard_name:
            cf_var.standard_name = cube.standard_name

        if cube.long_name:
            cf_var.long_name = cube.long_name

        if cube.units != 'unknown':
            cf_var.units = str(cube.units)

        # Add data variable-only attribute names to local_keys.
        if local_keys is None:
            local_keys = set()
        else:
            local_keys = set(local_keys)
        local_keys.update(_CF_DATA_ATTRS, _UKMO_DATA_ATTRS)

        # Add any cube attributes whose keys are in local_keys as
        # CF-netCDF data variable attributes.
        attr_names = set(cube.attributes).intersection(local_keys)
        for attr_name in sorted(attr_names):
            # Do not output 'conventions' attribute.
            if attr_name.lower() == 'conventions':
                continue

            value = cube.attributes[attr_name]

            if attr_name == 'STASH':
                # Adopting provisional Metadata Conventions for representing MO
                # Scientific Data encoded in NetCDF Format.
                attr_name = 'um_stash_source'
                value = str(value)

            if attr_name == "ukmo__process_flags":
                value = " ".join([x.replace(" ", "_") for x in value])

            if attr_name in _CF_GLOBAL_ATTRS:
                msg = '{attr_name!r} is being added as CF data variable ' \
                      'attribute, but {attr_name!r} should only be a CF ' \
                      'global attribute.'.format(attr_name=attr_name)
                warnings.warn(msg)

            setattr(cf_var, attr_name, value)

        # Create the CF-netCDF data variable cell method attribute.
        cell_methods = self._create_cf_cell_methods(cube, dimension_names)

        if cell_methods:
            cf_var.cell_methods = cell_methods

        # Create the CF-netCDF grid mapping.
        self._create_cf_grid_mapping(cube, cf_var)

        return cf_var

    def _increment_name(self, varname):
        """
        Increment string name or begin increment.

        Avoidance of conflicts between variable names, where the name is
        incremented to distinguish it from others.

        Args:

        * varname (string):
            Variable name to increment.

        Returns:
            Incremented varname.

        """
        num = 0
        try:
            name, endnum = varname.rsplit('_', 1)
            if endnum.isdigit():
                num = int(endnum) + 1
                varname = name
        except ValueError:
            pass

        return '{}_{}'.format(varname, num)


def save(cube, filename, netcdf_format='NETCDF4', local_keys=None,
         unlimited_dimensions=None, zlib=False, complevel=4, shuffle=True,
         fletcher32=False, contiguous=False, chunksizes=None, endian='native',
         least_significant_digit=None):
    """
    Save cube(s) to a netCDF file, given the cube and the filename.

    * Iris will write CF 1.5 compliant NetCDF files.
    * The attributes dictionaries on each cube in the saved cube list
      will be compared and common attributes saved as NetCDF global
      attributes where appropriate.
    * Keyword arguments specifying how to save the data are applied
      to each cube. To use different settings for different cubes, use
      the NetCDF Context manager (:class:`~Saver`) directly.

    Args:

    * cube (:class:`iris.cube.Cube` or :class:`iris.cube.CubeList`):
        A :class:`iris.cube.Cube`, :class:`iris.cube.CubeList` or other
        iterable of cubes to be saved to a netCDF file.

    * filename (string):
        Name of the netCDF file to save the cube(s).

    Kwargs:

    * netcdf_format (string):
        Underlying netCDF file format, one of 'NETCDF4', 'NETCDF4_CLASSIC',
        'NETCDF3_CLASSIC' or 'NETCDF3_64BIT'. Default is 'NETCDF4' format.

    * local_keys (iterable of strings):
        An interable of cube attribute keys. Any cube attributes with
        matching keys will become attributes on the data variable rather
        than global attributes.

    * unlimited_dimensions (iterable of strings and/or
      :class:`iris.coords.Coord` objects):
        Explicit list of coordinate names (or coordinate objects) corresponding
        to coordinate dimensions of `cube` to save with the NetCDF dimension
        variable length 'UNLIMITED'. By default, the outermost (first)
        dimension for each cube is used. Only the 'NETCDF4' format supports
        multiple 'UNLIMITED' dimensions. To save no unlimited dimensions, use
        `unlimited_dimensions=[]` (an empty list).

    * zlib (bool):
        If `True`, the data will be compressed in the netCDF file using gzip
        compression (default `False`).

    * complevel (int):
        An integer between 1 and 9 describing the level of compression desired
        (default 4). Ignored if `zlib=False`.

    * shuffle (bool):
        If `True`, the HDF5 shuffle filter will be applied before compressing
        the data (default `True`). This significantly improves compression.
        Ignored if `zlib=False`.

    * fletcher32 (bool):
        If `True`, the Fletcher32 HDF5 checksum algorithm is activated to
        detect errors. Default `False`.

    * contiguous (bool):
        If `True`, the variable data is stored contiguously on disk. Default
        `False`. Setting to `True` for a variable with an unlimited dimension
        will trigger an error.

    * chunksizes (tuple of int):
        Used to manually specify the HDF5 chunksizes for each dimension of the
        variable. A detailed discussion of HDF chunking and I/O performance is
        available here: http://www.hdfgroup.org/HDF5/doc/H5.user/Chunking.html.
        Basically, you want the chunk size for each dimension to match as
        closely as possible the size of the data block that users will read
        from the file. `chunksizes` cannot be set if `contiguous=True`.

    * endian (string):
        Used to control whether the data is stored in little or big endian
        format on disk. Possible values are 'little', 'big' or 'native'
        (default). The library will automatically handle endian conversions
        when the data is read, but if the data is always going to be read on a
        computer with the opposite format as the one used to create the file,
        there may be some performance advantage to be gained by setting the
        endian-ness.

    * least_significant_digit (int):
        If `least_significant_digit` is specified, variable data will be
        truncated (quantized). In conjunction with `zlib=True` this produces
        'lossy', but significantly more efficient compression. For example, if
        `least_significant_digit=1`, data will be quantized using
        `numpy.around(scale*data)/scale`, where `scale = 2**bits`, and `bits`
        is determined so that a precision of 0.1 is retained (in this case
        `bits=4`). From
        http://www.cdc.noaa.gov/cdc/conventions/cdc_netcdf_standard.shtml:
        "least_significant_digit -- power of ten of the smallest decimal place
        in unpacked data that is a reliable value". Default is `None`, or no
        quantization, or 'lossless' compression.

    Returns:
        None.

    .. note::

        The `zlib`, `complevel`, `shuffle`, `fletcher32`, `contiguous`,
        `chunksizes` and `endian` keywords are silently ignored for netCDF 3
        files that do not use HDF5.

    .. seealso::

        NetCDF Context manager (:class:`~Saver`).

    """
    if isinstance(cube, iris.cube.Cube):
        cubes = iris.cube.CubeList()
        cubes.append(cube)
    else:
        cubes = cube

    if local_keys is None:
        local_keys = set()
    else:
        local_keys = set(local_keys)

    # Determine the attribute keys that are common across all cubes and
    # thereby extend the collection of local_keys for attributes
    # that should be attributes on data variables.
    attributes = cubes[0].attributes
    common_keys = set(attributes)
    for cube in cubes[1:]:
        keys = set(cube.attributes)
        local_keys.update(keys.symmetric_difference(common_keys))
        common_keys.intersection_update(keys)
        different_value_keys = []
        for key in common_keys:
            if attributes[key] != cube.attributes[key]:
                different_value_keys.append(key)
        common_keys.difference_update(different_value_keys)
        local_keys.update(different_value_keys)

    # Initialise Manager for saving
    with Saver(filename, netcdf_format) as sman:
        # Iterate through the cubelist.
        for cube in cubes:
            sman.write(cube, local_keys, unlimited_dimensions, zlib, complevel,
                       shuffle, fletcher32, contiguous, chunksizes, endian,
                       least_significant_digit)

        # Add conventions attribute.
        sman.update_global_attributes(Conventions=_CF_CONVENTIONS_VERSION)

########NEW FILE########
__FILENAME__ = nimrod
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Provides NIMROD file format capabilities."""

import glob
import numpy as np
import os
import struct
import sys

import iris
from iris.exceptions import TranslationError
import iris.fileformats.nimrod_load_rules


# general header (int16) elements
general_header_int16s = ("vt_year", "vt_month", "vt_day", "vt_hour",
                         "vt_minute", "vt_second", "dt_year", "dt_month",
                         "dt_day", "dt_hour", "dt_minute", "datum_type",
                         "datum_len", "experiment_num", "horizontal_grid_type",
                         "num_rows", "num_cols", "nimrod_version",
                         "field_code", "vertical_coord_type",
                         "reference_vertical_coord_type",
                         "data_specific_float32_len",
                         "data_specific_int16_len",
                         "origin_corner", "int_mdi", "period_minutes",
                         "num_model_levels", "proj_biaxial_ellipsoid",
                         "ensemble_member", "spare1", "spare2")


# general header (float32) elements
general_header_float32s = ("vertical_coord", "reference_vertical_coord",
                           "y_origin", "row_step", "x_origin", "column_step",
                           "float32_mdi", "MKS_data_scaling", "data_offset",
                           "x_offset", "y_offset", "true_origin_latitude",
                           "true_origin_longitude", "true_origin_easting",
                           "true_origin_northing", "tm_meridian_scaling")


# data specific header (float32) elements
data_header_float32s = ("tl_y", "tl_x", "tr_y", "ty_x", "br_y", "br_x", "bl_y",
                        "bl_x", "sat_calib", "sat_space_count",
                        "ducting_index", "elevation_angle")


# data specific header (int16) elements
data_header_int16s = ("radar_num", "radars_bitmask", "more_radars_bitmask",
                      "clutter_map_num", "calibration_type",
                      "bright_band_height", "bright_band_intensity",
                      "bright_band_test1", "bright_band_test2", "infill_flag",
                      "stop_elevation", "int16_vertical_coord",
                      "int16_reference_vertical_coord", "int16_y_origin",
                      "int16_row_step", "int16_x_origin", "int16_column_step",
                      "int16_float32_mdi", "int16_data_scaling",
                      "int16_data_offset", "int16_x_offset", "int16_y_offset",
                      "int16_true_origin_latitude",
                      "int16_true_origin_longitude", "int16_tl_y",
                      "int16_tl_x", "int16_tr_y", "int16_ty_x", "int16_br_y",
                      "int16_br_x", "int16_bl_y", "int16_bl_x", "sensor_id",
                      "meteosat_id", "alphas_available")


def _read_chars(infile, num):
    """Read characters from the (big-endian) file."""
    instr = infile.read(num)
    return struct.unpack(">%ds" % num, instr)[0]


class NimrodField(object):
    """
    A data field from a NIMROD file.

    Capable of converting itself into a :class:`~iris.cube.Cube`

    """
    def __init__(self, from_file=None):
        """
        Create a NimrodField object and optionally read from an open file.

        Example::

            with open("nimrod_file", "rb") as infile:
                field = NimrodField(infile)

        """
        if from_file is not None:
            self.read(from_file)

    def read(self, infile):
        """Read the next field from the given file object."""
        self._read_header(infile)
        self._read_data(infile)

    def _read_header_subset(self, infile, names, dtype):
        # Read contiguous header items of the same data type.
        values = np.fromfile(infile, dtype=dtype, count=len(names))
        if sys.byteorder == "little":
            values.byteswap(True)
        for i, name in enumerate(names):
            setattr(self, name, values[i])

    def _read_header(self, infile):
        """Load the 512 byte header (surrounded by 4-byte length)."""

        leading_length = struct.unpack(">L", infile.read(4))[0]
        if leading_length != 512:
            raise TranslationError("Expected header leading_length of 512")

        # general header (int16) elements 1-31 (bytes 1-62)
        self._read_header_subset(infile, general_header_int16s, np.int16)

        # general header (float32) elements 32-59 (bytes 63-174)
        self._read_header_subset(infile, general_header_float32s, np.float32)
        # skip unnamed floats
        infile.seek(4 * (28 - len(general_header_float32s)), os.SEEK_CUR)

        # data specific header (float32) elements 60-104 (bytes 175-354)
        self._read_header_subset(infile, data_header_float32s, np.float32)
        # skip unnamed floats
        infile.seek(4 * (45 - len(data_header_float32s)), os.SEEK_CUR)

        # data specific header (char) elements 105-107 (bytes 355-410)
        self.units = _read_chars(infile, 8)
        self.source = _read_chars(infile, 24)
        self.title = _read_chars(infile, 24)

        # data specific header (int16) elements 108- (bytes 411-512)
        self._read_header_subset(infile, data_header_int16s, np.int16)
        # skip unnamed int16s
        infile.seek(2 * (51 - len(data_header_int16s)), os.SEEK_CUR)

        trailing_length = struct.unpack(">L", infile.read(4))[0]
        if trailing_length != leading_length:
            raise TranslationError('Expected header trailing_length of {}, '
                                   'got {}.'.format(leading_length,
                                                    trailing_length))

    def _read_data(self, infile):
        """
        Read the data array: int8, int16, int32 or float32

        (surrounded by 4-byte length, at start and end)

        """
        # what are we expecting?
        num_data = int(self.num_rows) * int(self.num_cols)
        num_data_bytes = int(num_data) * int(self.datum_len)

        # format string for unpacking the file.read()
        # 0:real
        if self.datum_type == 0:
            numpy_dtype = np.float32
        # 1:int
        elif self.datum_type == 1:
            if self.datum_len == 1:
                numpy_dtype = np.int8
            elif self.datum_len == 2:
                numpy_dtype = np.int16
            elif self.datum_len == 4:
                numpy_dtype = np.int32
            else:
                raise TranslationError("Undefined datum length "
                                       "%d" % self.datum_type)
        # 2:byte
        elif self.datum_type == 2:
            numpy_dtype = np.byte
        else:
            raise TranslationError("Undefined data type")
        leading_length = struct.unpack(">L", infile.read(4))[0]
        if leading_length != num_data_bytes:
            raise TranslationError("Expected data leading_length of %d" %
                                   num_data_bytes)

        # TODO: Deal appropriately with MDI. Can't just create masked arrays
        #       as cube merge converts masked arrays with no masks to ndarrays,
        #       thus mergable cube can split one mergable cube into two.
        self.data = np.fromfile(infile, dtype=numpy_dtype, count=num_data)

        if sys.byteorder == "little":
            self.data.byteswap(True)

        trailing_length = struct.unpack(">L", infile.read(4))[0]
        if trailing_length != leading_length:
            raise TranslationError("Expected data trailing_length of %d" %
                                   num_data_bytes)

        # Form the correct shape.
        self.data = self.data.reshape(self.num_rows, self.num_cols)


def load_cubes(filenames, callback=None):
    """
    Loads cubes from a list of NIMROD filenames.

    Args:

    * filenames - list of NIMROD filenames to load

    Kwargs:

    * callback - a function which can be passed on to
                 :func:`iris.io.run_callback`

    .. note::

        The resultant cubes may not be in the same order as in the files.

    """
    if isinstance(filenames, basestring):
        filenames = [filenames]

    for filename in filenames:
        for path in glob.glob(filename):
            with open(path, "rb") as infile:
                while True:
                    try:
                        field = NimrodField(infile)
                    except struct.error:
                        # End of file. Move on to the next file.
                        break

                    cube = iris.fileformats.nimrod_load_rules.run(field)

                    # Were we given a callback?
                    if callback is not None:
                        cube = iris.io.run_callback(callback,
                                                    cube,
                                                    field,
                                                    filename)
                    if cube is None:
                        continue

                    yield cube

########NEW FILE########
__FILENAME__ = nimrod_load_rules
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Rules for converting NIMROD fields into cubes."""


import warnings

import netcdftime
import numpy as np

import iris
from iris.coords import DimCoord
from iris.exceptions import TranslationError


__all__ = ['run']


# Meridian scaling for British National grid.
MERIDIAN_SCALING_BNG = 0.9996012717

NIMROD_DEFAULT = -32767.0

TIME_UNIT = iris.unit.Unit('hours since 1970-01-01 00:00:00',
                           calendar=iris.unit.CALENDAR_STANDARD)


FIELD_CODES = {73: "orography"}
VERTICAL_CODES = {0: "height", 1: "altitude", 12: "levels_below_ground"}


class TranslationWarning(Warning):
    pass


def name(cube, field):
    """Set the cube's name from the field."""

    cube.rename(field.title.strip())


def units(cube, field):
    """
    Set the cube's units from the field.

    Unhandled units are stored in an "invalid_units" attribute instead.

    """
    units = field.units.strip()
    try:
        cube.units = units
    except ValueError:
        # Just add it as an attribute.
        warnings.warn("Unhandled units '{0}' recorded in cube attributes.".
                      format(units))
        cube.attributes["invalid_units"] = units


def time(cube, field):
    """Add a time coord to the cube."""
    valid_date = netcdftime.datetime(field.vt_year, field.vt_month,
                                     field.vt_day, field.vt_hour,
                                     field.vt_minute, field.vt_second)
    point = TIME_UNIT.date2num(valid_date)

    bounds = None
    if field.period_minutes != field.int_mdi and field.period_minutes != 0:
        # Create a bound array to handle the Period of Interest if set.
        bounds = (point - (field.period_minutes / 60.0), point)

    time_coord = DimCoord(points=point,
                          bounds=bounds,
                          standard_name='time',
                          units=TIME_UNIT)

    cube.add_aux_coord(time_coord)


def reference_time(cube, field):
    """Add a 'reference time' to the cube, if present in the field."""
    if field.dt_year != field.int_mdi:
        data_date = netcdftime.datetime(field.dt_year, field.dt_month,
                                        field.dt_day, field.dt_hour,
                                        field.dt_minute)

        ref_time_coord = DimCoord(TIME_UNIT.date2num(data_date),
                                  standard_name='forecast_reference_time',
                                  units=TIME_UNIT)

        cube.add_aux_coord(ref_time_coord)


def experiment(cube, field):
    """Add an 'experiment number' to the cube, if present in the field."""
    if field.experiment_num != field.int_mdi:
        cube.add_aux_coord(DimCoord(field.experiment_num,
                                    long_name="experiment_number"))


def proj_biaxial_ellipsoid(cube, field):
    """
    Ellipsoid definition is currently ignored.

    """
    pass


def tm_meridian_scaling(cube, field):
    """
    Deal with the scale factor on the central meridian for transverse mercator
    projections if present in the field.

    Currently only caters for British National Grid.

    """
    if field.tm_meridian_scaling not in [field.float32_mdi, NIMROD_DEFAULT]:
        if abs(field.tm_meridian_scaling - MERIDIAN_SCALING_BNG) < 1e-6:
            pass  # This is the expected value for British National Grid
        else:
            warnings.warn("tm_meridian_scaling not yet handled: {}"
                          "".format(field.tm_meridian_scaling),
                          TranslationWarning)


def british_national_grid_x(cube, field):
    """Add a British National Grid X coord to the cube."""
    x_coord = DimCoord(np.arange(field.num_cols) * field.column_step +
                       field.x_origin, standard_name="projection_x_coordinate",
                       units="m", coord_system=iris.coord_systems.OSGB())
    cube.add_dim_coord(x_coord, 1)


def british_national_grid_y(cube, field):
    """
    Add a British National Grid Y coord to the cube.

    Currently only handles origin in the top left corner.

    """
    if field.origin_corner == 0:  # top left
        y_coord = DimCoord(np.arange(field.num_rows)[::-1] *
                           -field.row_step + field.y_origin,
                           standard_name="projection_y_coordinate", units="m",
                           coord_system=iris.coord_systems.OSGB())
        cube.add_dim_coord(y_coord, 0)
    else:
        raise TranslationError("Corner {0} not yet implemented".
                               format(field.origin_corner))


def horizontal_grid(cube, field):
    """Add X and Y coords to the cube.

    Currently only handles British National Grid.

    """
    # "NG" (British National Grid)
    if field.horizontal_grid_type == 0:
        british_national_grid_x(cube, field)
        british_national_grid_y(cube, field)
    else:
        raise TranslationError("Grid type %d not yet implemented" %
                               field.horizontal_grid_type)


def orography_vertical_coord(cube, field):
    """Special handling of vertical coords for orography fields: Do nothing."""
    # We can find values in the vertical coord, such as 9999,
    # for orography fields. Don't make a vertical coord from these.
    pass


def height_vertical_coord(cube, field):
    """Add a height coord to the cube, if present in the field."""
    if (field.reference_vertical_coord_type == field.int_mdi or
            field.reference_vertical_coord == field.float32_mdi):
        height_coord = DimCoord(field.vertical_coord,
                                standard_name="height", units="m",
                                attributes={"positive": "up"})
        cube.add_aux_coord(height_coord)
    else:
        raise TranslationError("Bounded vertical not yet implemented")


def altitude_vertical_coord(cube, field):
    """Add an altitude coord to the cube, if present in the field."""
    if (field.reference_vertical_coord_type == field.int_mdi or
            field.reference_vertical_coord == field.float32_mdi):
                alti_coord = DimCoord(field.vertical_coord,
                                      standard_name="altitude",
                                      units="m",
                                      attributes={"positive": "up"})
                cube.add_aux_coord(alti_coord)
    else:
        raise TranslationError("Bounded vertical not yet implemented")


def levels_below_ground_vertical_coord(cube, field):
    """Add a levels_below_ground coord to the cube, if present in the field."""
    if (field.reference_vertical_coord_type == field.int_mdi or
            field.reference_vertical_coord == field.float32_mdi):
                lev_coord = DimCoord(field.vertical_coord,
                                     long_name="levels_below_ground",
                                     units="1",
                                     attributes={"positive": "down"})
                cube.add_aux_coord(lev_coord)
    else:
        raise TranslationError("Bounded vertical not yet implemented")


def vertical_coord(cube, field):
    """Add a vertical coord to the cube."""
    v_type = field.vertical_coord_type

    if v_type not in [field.int_mdi, NIMROD_DEFAULT]:
        if FIELD_CODES.get(field.field_code, None) == "orography":
            orography_vertical_coord(cube, field)
        else:
            vertical_code_name = VERTICAL_CODES.get(v_type, None)
            if vertical_code_name == "height":
                height_vertical_coord(cube, field)
            elif vertical_code_name == "altitude":
                altitude_vertical_coord(cube, field)
            elif vertical_code_name == "levels_below_ground":
                levels_below_ground_vertical_coord(cube, field)
            else:
                warnings.warn("Vertical coord {!r} not yet handled"
                              "".format(v_type), TranslationWarning)


def ensemble_member(cube, field):
    """Add an 'ensemble member' coord to the cube, if present in the field."""
    ensemble_member = getattr(field, "ensemble_member")
    if ensemble_member != field.int_mdi:
        cube.add_aux_coord(DimCoord(ensemble_member, "realization"))


def origin_corner(cube, field):
    """Ensure the data matches the order of the coords we've made."""
    if field.origin_corner == 0:  # top left
        cube.data = cube.data[::-1, :].copy()
    else:
        raise TranslationError("Corner {0} not yet implemented".
                               format(field.origin_corner))
    return cube


def attributes(cube, field):
    """Add attributes to the cube."""
    def add_attr(name):
        """Add an attribute to the cube."""
        if hasattr(field, name):
            value = getattr(field, name)
            if value not in [field.int_mdi, field.float32_mdi]:
                cube.attributes[name] = value

    add_attr("nimrod_version")
    add_attr("field_code")
    add_attr("num_model_levels")
    add_attr("sat_calib")
    add_attr("sat_space_count")
    add_attr("ducting_index")
    add_attr("elevation_angle")
    add_attr("radar_num")
    add_attr("radars_bitmask")
    add_attr("more_radars_bitmask")
    add_attr("clutter_map_num")
    add_attr("calibration_type")
    add_attr("bright_band_height")
    add_attr("bright_band_intensity")
    add_attr("bright_band_test1")
    add_attr("bright_band_test2")
    add_attr("infill_flag")
    add_attr("stop_elevation")
    add_attr("sensor_id")
    add_attr("meteosat_id")
    add_attr("alphas_available")

    cube.attributes["source"] = field.source.strip()


def run(field):
    """
    Convert a NIMROD field to an Iris cube.

    Args:

        * field - a :class:`~iris.fileformats.nimrod.NimrodField`

    Returns:

        * A new :class:`~iris.cube.Cube`, created from the NimrodField.

    """
    cube = iris.cube.Cube(field.data)

    name(cube, field)
    units(cube, field)

    # time
    time(cube, field)
    reference_time(cube, field)

    experiment(cube, field)

    # horizontal grid
    proj_biaxial_ellipsoid(cube, field)
    tm_meridian_scaling(cube, field)
    horizontal_grid(cube, field)

    # vertical
    vertical_coord(cube, field)

    # add other stuff, if present
    ensemble_member(cube, field)
    attributes(cube, field)

    origin_corner(cube, field)

    return cube

########NEW FILE########
__FILENAME__ = pp
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Provides UK Met Office Post Process (PP) format specific capabilities.

"""

import abc
import collections
from copy import deepcopy
import itertools
import operator
import os
import re
import struct
import warnings

import biggus
import numpy as np
import numpy.ma as ma
import netcdftime

import iris.config
import iris.fileformats.rules
import iris.unit
import iris.fileformats.pp_rules
import iris.coord_systems
import iris.proxy
iris.proxy.apply_proxy('iris.fileformats.pp_packing', globals())


__all__ = ['load', 'save', 'load_cubes', 'PPField',
           'add_load_rules', 'reset_load_rules',
           'add_save_rules', 'reset_save_rules', 'STASH', 'EARTH_RADIUS']


EARTH_RADIUS = 6371229.0


# PP->Cube and Cube->PP rules are loaded on first use
_load_rules = None
_save_rules = None


PP_HEADER_DEPTH = 256
PP_WORD_DEPTH = 4
NUM_LONG_HEADERS = 45
NUM_FLOAT_HEADERS = 19

# The header definition for header release 2.
#: A list of (header_name, position_in_header(tuple of)) pairs for
#: header release 2 - using the one-based UM/FORTRAN indexing convention.
UM_HEADER_2 = [
        ('lbyr',   (1, )),
        ('lbmon',  (2, )),
        ('lbdat',  (3, )),
        ('lbhr',   (4, )),
        ('lbmin',  (5, )),
        ('lbday',  (6, )),
        ('lbyrd',  (7, )),
        ('lbmond', (8, )),
        ('lbdatd', (9, )),
        ('lbhrd',  (10, )),
        ('lbmind', (11, )),
        ('lbdayd', (12, )),
        ('lbtim',  (13, )),
        ('lbft',   (14, )),
        ('lblrec', (15, )),
        ('lbcode', (16, )),
        ('lbhem',  (17, )),
        ('lbrow',  (18, )),
        ('lbnpt',  (19, )),
        ('lbext',  (20, )),
        ('lbpack', (21, )),
        ('lbrel',  (22, )),
        ('lbfc',   (23, )),
        ('lbcfc',  (24, )),
        ('lbproc', (25, )),
        ('lbvc',   (26, )),
        ('lbrvc',  (27, )),
        ('lbexp',  (28, )),
        ('lbegin', (29, )),
        ('lbnrec', (30, )),
        ('lbproj', (31, )),
        ('lbtyp',  (32, )),
        ('lblev',  (33, )),
        ('lbrsvd', (34, 35, 36, 37, )),
        ('lbsrce', (38, )),
        ('lbuser', (39, 40, 41, 42, 43, 44, 45, )),
        ('brsvd',  (46, 47, 48, 49, )),
        ('bdatum', (50, )),
        ('bacc',   (51, )),
        ('blev',   (52, )),
        ('brlev',  (53, )),
        ('bhlev',  (54, )),
        ('bhrlev', (55, )),
        ('bplat',  (56, )),
        ('bplon',  (57, )),
        ('bgor',   (58, )),
        ('bzy',    (59, )),
        ('bdy',    (60, )),
        ('bzx',    (61, )),
        ('bdx',    (62, )),
        ('bmdi',   (63, )),
        ('bmks',   (64, )),
    ]

# The header definition for header release 3.
#: A list of (header_name, position_in_header(tuple of)) pairs for
#: header release 3 - using the one-based UM/FORTRAN indexing convention.
UM_HEADER_3 = [
        ('lbyr',   (1, )),
        ('lbmon',  (2, )),
        ('lbdat',  (3, )),
        ('lbhr',   (4, )),
        ('lbmin',  (5, )),
        ('lbsec',  (6, )),
        ('lbyrd',  (7, )),
        ('lbmond', (8, )),
        ('lbdatd', (9, )),
        ('lbhrd',  (10, )),
        ('lbmind', (11, )),
        ('lbsecd', (12, )),
        ('lbtim',  (13, )),
        ('lbft',   (14, )),
        ('lblrec', (15, )),
        ('lbcode', (16, )),
        ('lbhem',  (17, )),
        ('lbrow',  (18, )),
        ('lbnpt',  (19, )),
        ('lbext',  (20, )),
        ('lbpack', (21, )),
        ('lbrel',  (22, )),
        ('lbfc',   (23, )),
        ('lbcfc',  (24, )),
        ('lbproc', (25, )),
        ('lbvc',   (26, )),
        ('lbrvc',  (27, )),
        ('lbexp',  (28, )),
        ('lbegin', (29, )),
        ('lbnrec', (30, )),
        ('lbproj', (31, )),
        ('lbtyp',  (32, )),
        ('lblev',  (33, )),
        ('lbrsvd', (34, 35, 36, 37, )),
        ('lbsrce', (38, )),
        ('lbuser', (39, 40, 41, 42, 43, 44, 45, )),
        ('brsvd',  (46, 47, 48, 49, )),
        ('bdatum', (50, )),
        ('bacc',   (51, )),
        ('blev',   (52, )),
        ('brlev',  (53, )),
        ('bhlev',  (54, )),
        ('bhrlev', (55, )),
        ('bplat',  (56, )),
        ('bplon',  (57, )),
        ('bgor',   (58, )),
        ('bzy',    (59, )),
        ('bdy',    (60, )),
        ('bzx',    (61, )),
        ('bdx',    (62, )),
        ('bmdi',   (63, )),
        ('bmks',   (64, )),
    ]

# A map from header-release-number to header definition
UM_HEADERS = {2: UM_HEADER_2, 3: UM_HEADER_3}

# Offset value to convert from UM_HEADER positions to PP_HEADER offsets.
UM_TO_PP_HEADER_OFFSET = 1

#: A dictionary mapping IB values to their names.
EXTRA_DATA = {
                 1: 'x',
                 2: 'y',
                 3: 'lower_y_domain',
                 4: 'lower_x_domain',
                 5: 'upper_y_domain',
                 6: 'upper_x_domain',
                 7: 'lower_z_domain',
                 8: 'upper_z_domain',
                 10: 'field_title',
                 11: 'domain_title',
                 12: 'x_lower_bound',
                 13: 'x_upper_bound',
                 14: 'y_lower_bound',
                 15: 'y_upper_bound',
             }


#: Maps lbuser[0] to numpy data type. "default" will be interpreted if
#: no match is found, providing a warning in such a case.
LBUSER_DTYPE_LOOKUP = {1 :np.dtype('>f4'), 
                       2 :np.dtype('>i4'), 
                       3 :np.dtype('>i4'),
                       -1:np.dtype('>f4'),
                       -2:np.dtype('>i4'),
                       -3:np.dtype('>i4'),
                       'default': np.dtype('>f4'),
                       }

# LBPROC codes and their English equivalents
LBPROC_PAIRS = ((1, "Difference from another experiment"),
                (2, "Difference from zonal (or other spatial) mean"),
                (4, "Difference from time mean"),
                (8, "X-derivative (d/dx)"),
                (16, "Y-derivative (d/dy)"),
                (32, "Time derivative (d/dt)"),
                (64, "Zonal mean field"),
                (128, "Time mean field"),
                (256, "Product of two fields"),
                (512, "Square root of a field"),
                (1024, "Difference between fields at levels BLEV and BRLEV"),
                (2048, "Mean over layer between levels BLEV and BRLEV"),
                (4096, "Minimum value of field during time period"),
                (8192, "Maximum value of field during time period"),
                (16384, "Magnitude of a vector, not specifically wind speed"),
                (32768, "Log10 of a field"),
                (65536, "Variance of a field"),
                (131072, "Mean over an ensemble of parallel runs"))

# lbproc_map is dict mapping lbproc->English and English->lbproc essentially a one to one mapping
lbproc_map = {x : y for x,y in itertools.chain(LBPROC_PAIRS, ((y,x) for x,y in LBPROC_PAIRS))}


class STASH(collections.namedtuple('STASH', 'model section item')):
    """
    A class to hold a single STASH code.

    Create instances using:
        >>> model = 1
        >>> section = 2
        >>> item = 3
        >>> my_stash = iris.fileformats.pp.STASH(model, section, item)

    Access the sub-components via:
        >>> my_stash.model
        1
        >>> my_stash.section
        2
        >>> my_stash.item
        3

    String conversion results in the MSI format:
        >>> print iris.fileformats.pp.STASH(1, 16, 203)
        m01s16i203

    """

    __slots__ = ()

    def __new__(cls, model, section, item):
        """

        Args:

        * model
            A positive integer less than 100, or None.
        * section
            A non-negative integer less than 100, or None.
        * item
            A positive integer less than 1000, or None.

        """
        model = cls._validate_member('model', model, 1, 99)
        section = cls._validate_member('section', section, 0, 99)
        item = cls._validate_member('item', item, 1, 999)
        return super(STASH, cls).__new__(cls, model, section, item)

    @staticmethod
    def from_msi(msi):
        """Convert a STASH code MSI string to a STASH instance."""
        if not isinstance(msi, basestring):
            raise TypeError('Expected STASH code MSI string, got %r' % msi)

        msi_match = re.match('^\s*m(.*)s(.*)i(.*)\s*$', msi, re.IGNORECASE)

        if msi_match is None:
            raise ValueError('Expected STASH code MSI string "mXXsXXiXXX", got %r' % msi)

        return STASH(*msi_match.groups())

    @staticmethod
    def _validate_member(name, value, lower_limit, upper_limit):
        # Returns a valid integer or None.
        try:
            value = int(value)
            if not lower_limit <= value <= upper_limit:
                value = None
        except (TypeError, ValueError):
            value = None
        return value

    def __str__(self):
        model = self._format_member(self.model, 2)
        section = self._format_member(self.section, 2)
        item = self._format_member(self.item, 3)
        return 'm{}s{}i{}'.format(model, section, item)

    def _format_member(self, value, num_digits):
        if value is None:
            result = '?' * num_digits
        else:
            format_spec = '0' + str(num_digits)
            result = format(value, format_spec)
        return result

    def lbuser3(self):
        """Return the lbuser[3] value that this stash represents."""
        return (self.section or 0) * 1000 + (self.item or 0)
    
    def lbuser6(self):
        """Return the lbuser[6] value that this stash represents."""
        return self.model or 0

    @property
    def is_valid(self):
        return '?' not in str(self)

    def __eq__(self, other):
        if isinstance(other, basestring):
            return super(STASH, self).__eq__(STASH.from_msi(other))
        else:
            return super(STASH, self).__eq__(other)

    def __ne__(self, other):
        return not self.__eq__(other)


class SplittableInt(object):
    """
    A class to hold integers which can easily get each decimal digit individually.

    >>> three_six_two = SplittableInt(362)
    >>> print three_six_two
    362
    >>> print three_six_two[0]
    2
    >>> print three_six_two[2]
    3

    .. note:: No support for negative numbers

    """
    def __init__(self, value, name_mapping_dict=None):
        """
        Build a SplittableInt given the positive integer value provided.

        Kwargs:

        * name_mapping_dict - (dict)
            A special mapping to provide name based access to specific integer positions:

                >>> a = SplittableInt(1234, {'hundreds': 2})
                >>> print a.hundreds
                2
                >>> a.hundreds = 9
                >>> print a.hundreds
                9
                >>> print a
                1934

        """
        if value < 0:
            raise ValueError('Negative numbers not supported with splittable integers object')

        # define the name lookup first (as this is the way __setattr__ is plumbed)
        #: A dictionary mapping special attribute names on this object
        #: to the slices/indices required to access them.
        self._name_lookup = name_mapping_dict or {}
        self._value = value

        self._calculate_str_value_from_value()

    def __int__(self):
        return int(self._value)

    def _calculate_str_value_from_value(self):
        # Reverse the string to get the appropriate index when getting the sliced value
        self._strvalue = [int(c) for c in str(self._value)[::-1]]

        # Associate the names in the lookup table to attributes
        for name, index in self._name_lookup.items():
            object.__setattr__(self, name, self[index])

    def _calculate_value_from_str_value(self):
        self._value = np.sum([ 10**i * val for i, val in enumerate(self._strvalue)])

    def __len__(self):
        return len(self._strvalue)

    def __getitem__(self, key):
        try:
            val = self._strvalue[key]
        except IndexError:
            val = 0

        # if the key returns a list of values, then combine them together to an integer
        if isinstance(val, list):
            val = sum([10**i * val for i, val in enumerate(val)])

        return val

    def __setitem__(self, key, value):
        # The setitem method has been overridden so that assignment using ``val[0] = 1`` style syntax updates
        # the entire object appropriately.

        if (not isinstance(value, int) or value < 0):
            raise ValueError('Can only set %s as a positive integer value.' % key)

        if isinstance(key, slice):
            if ((key.start is not None and key.start < 0) or
                (key.step is not None and key.step < 0) or
                (key.stop is not None and key.stop < 0)):
                raise ValueError('Cannot assign a value with slice objects containing negative indices.')

            # calculate the current length of the value of this string
            current_length = len(range(*key.indices(len(self))))

            # get indices for as many digits as have been requested. Putting the upper limit on the number of digits at 100.
            indices = range(*key.indices(100))
            if len(indices) < len(str(value)):
                raise ValueError('Cannot put %s into %s as it has too many digits.' % (value, key))

            # Iterate over each of the indices in the slice, zipping them together with the associated digit
            for index, digit in zip(indices, str(value).zfill(current_length)[::-1]):
                # assign each digit to the associated index
                self.__setitem__(index, int(digit))

        else:
            # If we are trying to set to an index which does not currently exist in _strvalue then extend it to the
            # appropriate length
            if (key + 1) > len(self):
                new_str_value = [0] * (key + 1)
                new_str_value[:len(self)] = self._strvalue
                self._strvalue = new_str_value

            self._strvalue[key] = value

            for name, index in self._name_lookup.items():
                if index == key:
                    object.__setattr__(self, name, value)

            self._calculate_value_from_str_value()

    def __setattr__(self, name, value):
        # if the attribute is a special value, update the index value which will in turn update the attribute value
        if (name != '_name_lookup' and name in self._name_lookup.keys()):
            self[self._name_lookup[name]] = value
        else:
            object.__setattr__(self, name, value)

    def __str__(self):
        return str(self._value)

    def __repr__(self):
        return 'SplittableInt(%r, name_mapping_dict=%r)' % (self._value, self._name_lookup)

    def __eq__(self, other):
        result = NotImplemented
        if isinstance(other, SplittableInt):
            result = self._value == other._value
        elif isinstance(other, int):
            result = self._value == other
        return result

    def __ne__(self, other):
        result = self.__eq__(other)
        if result is not NotImplemented:
            result = not result
        return result

    def _compare(self, other, op):
        result = NotImplemented
        if isinstance(other, SplittableInt):
            result = op(self._value, other._value)
        elif isinstance(other, int):
            result = op(self._value, other)
        return result

    def __lt__(self, other):
        return self._compare(other, operator.lt)

    def __le__(self, other):
        return self._compare(other, operator.le)

    def __gt__(self, other):
        return self._compare(other, operator.gt)

    def __ge__(self, other):
        return self._compare(other, operator.ge)


class BitwiseInt(SplittableInt):
    """
    A class to hold an integer, of fixed bit-length, which can easily get/set each bit individually.

    .. note::

        Uses a fixed number of bits.
        Will raise an Error when attempting to access an out-of-range flag.

    >>> a = BitwiseInt(511)
    >>> a.flag1
    1
    >>> a.flag8
    1
    >>> a.flag128
    1
    >>> a.flag256
    1
    >>> a.flag512
    AttributeError: 'BitwiseInt' object has no attribute 'flag512'
    >>> a.flag512 = 1
    AttributeError: Cannot set a flag that does not exist: flag512

    """

    def __init__(self, value, num_bits=None):
        """ """ # intentionally empty docstring as all covered in the class docstring.

        SplittableInt.__init__(self, value)
        self.flags = ()

        #do we need to calculate the number of bits based on the given value?
        self._num_bits = num_bits
        if self._num_bits is None:
            self._num_bits = 0
            while((value >> self._num_bits) > 0):
                self._num_bits += 1
        else:
            #make sure the number of bits is enough to store the given value.
            if (value >> self._num_bits) > 0:
                raise ValueError("Not enough bits to store value")

        self._set_flags_from_value()

    def _set_flags_from_value(self):
        all_flags = []

        # Set attributes "flag[n]" to 0 or 1
        for i in range(self._num_bits):
            flag_name = 1 << i
            flag_value = ((self._value >> i) & 1)
            object.__setattr__(self, 'flag%d' % flag_name, flag_value)

            # Add to list off all flags
            if flag_value:
                all_flags.append(flag_name)

        self.flags = tuple(all_flags)

    def _set_value_from_flags(self):
        self._value = 0
        for i in range(self._num_bits):
            bit_value = pow(2, i)
            flag_name = "flag%i" % bit_value
            flag_value = object.__getattribute__(self, flag_name)
            self._value += flag_value * bit_value

    def __iand__(self, value):
        """Perform an &= operation."""
        self._value &= value
        self._set_flags_from_value()
        return self

    def __ior__(self, value):
        """Perform an |= operation."""
        self._value |= value
        self._set_flags_from_value()
        return self

    def __iadd__(self, value):
        """Perform an inplace add operation"""
        self._value += value
        self._set_flags_from_value()
        return self

    def __setattr__(self, name, value):
        # Allow setting of the attribute flags
        # Are we setting a flag?
        if name.startswith("flag") and name != "flags":
            #true and false become 1 and 0
            if not isinstance(value, bool):
                raise TypeError("Can only set bits to True or False")

            # Setting an existing flag?
            if hasattr(self, name):
                #which flag?
                flag_value = int(name[4:])
                #on or off?
                if value:
                    self |= flag_value
                else:
                    self &= ~flag_value

            # Fail if an attempt has been made to set a flag that does not exist
            else:
                raise AttributeError("Cannot set a flag that does not exist: %s" % name)

        # If we're not setting a flag, then continue as normal
        else:
            SplittableInt.__setattr__(self, name, value)


class PPDataProxy(object):
    """A reference to the data payload of a single PP field."""

    __slots__ = ('shape', 'src_dtype', 'path', 'offset', 'data_len', '_lbpack',
                 'mdi', 'mask')
    
    def __init__(self, shape, src_dtype, path, offset, data_len, lbpack, mdi,
                 mask):
        self.shape = shape
        self.src_dtype = src_dtype
        self.path = path
        self.offset = offset
        self.data_len = data_len
        self.lbpack = lbpack
        self.mdi = mdi
        self.mask = mask

    # lbpack
    def _lbpack_setter(self, value):
        self._lbpack = value

    def _lbpack_getter(self):
        value = self._lbpack
        if not isinstance(self._lbpack, SplittableInt):
            mapping = dict(n5=slice(4, None), n4=3, n3=2, n2=1, n1=0)
            value = SplittableInt(self._lbpack, mapping)
        return value

    lbpack = property(_lbpack_getter, _lbpack_setter)

    @property
    def dtype(self):
        return self.src_dtype.newbyteorder('=')

    @property
    def fill_value(self):
        return self.mdi

    @property
    def ndim(self):
        return len(self.shape)

    def __getitem__(self, keys):
        with open(self.path, 'rb') as pp_file:
            pp_file.seek(self.offset, os.SEEK_SET)
            data_bytes = pp_file.read(self.data_len)
            data = _data_bytes_to_shaped_array(data_bytes, self.lbpack,
                                               self.shape, self.src_dtype,
                                               self.mdi, self.mask)
        return data.__getitem__(keys)

    def __repr__(self):
        fmt = '<{self.__class__.__name__} shape={self.shape}' \
              ' src_dtype={self.dtype!r} path={self.path!r}' \
              ' offset={self.offset} mask={self.mask!r}>'
        return fmt.format(self=self)

    def __getstate__(self):
        # Because we have __slots__, this is needed to support Pickle.dump()
        return [(name, getattr(self, name)) for name in self.__slots__]

    def __setstate__(self, state):
        # Because we have __slots__, this is needed to support Pickle.load()
        # (Use setattr, as there is no object dictionary.)
        for (key, value) in state:
            setattr(self, key, value)

    def __eq__(self, other):
        result = NotImplemented
        if isinstance(other, PPDataProxy):
            result = True
            for attr in self.__slots__:
                if getattr(self, attr) != getattr(other, attr):
                    result = False
                    break
        return result

    def __ne__(self, other):
        result = self.__eq__(other)
        if result is not NotImplemented:
            result = not result
        return result


def _data_bytes_to_shaped_array(data_bytes, lbpack, data_shape, data_type, mdi,
                                mask=None):
    """
    Convert the already read binary data payload into a numpy array, unpacking
    and decompressing as per the F3 specification.

    """
    if lbpack.n1 in (0, 2):
        data = np.frombuffer(data_bytes, dtype=data_type)
    elif lbpack.n1 == 1:
        data = pp_packing.wgdos_unpack(data_bytes, data_shape[0],
                                       data_shape[1], mdi)
    elif lbpack.n1 == 4:
        data = pp_packing.rle_decode(data_bytes, data_shape[0], data_shape[1], mdi)
    else:
        raise iris.exceptions.NotYetImplementedError(
                'PP fields with LBPACK of %s are not yet supported.' % lbpack)

    # Ensure we have write permission on the data buffer.
    data.setflags(write=True)

    # Ensure the data is in the native byte order
    if not data.dtype.isnative:
        data.byteswap(True)
        data.dtype = data.dtype.newbyteorder('=')

    if hasattr(lbpack, 'boundary_packing'):
        # Convert a long string of numbers into a "lateral boundary
        # condition" array, which is split into 4 quartiles, North
        # East, South, West and where North and South contain the corners.
        
        boundary_packing = lbpack.boundary_packing
        compressed_data = data
        data = np.ma.masked_all(data_shape)

        boundary_height = boundary_packing.y_halo + boundary_packing.rim_width
        boundary_width = boundary_packing.x_halo + boundary_packing.rim_width
        y_height, x_width = data_shape
        # The height of the east and west components.
        mid_height = y_height - 2 * boundary_height
        
        n_s_shape = boundary_height, x_width
        e_w_shape = mid_height, boundary_width
        
        # Keep track of our current position in the array.
        current_posn = 0
        
        north = compressed_data[:boundary_height*x_width]
        current_posn += len(north)
        data[-boundary_height:, :] = north.reshape(*n_s_shape)

        east = compressed_data[current_posn:
                               current_posn + boundary_width * mid_height]
        current_posn += len(east)
        data[boundary_height:-boundary_height,
             -boundary_width:] = east.reshape(*e_w_shape)

        south = compressed_data[current_posn:
                                current_posn + boundary_height * x_width]
        current_posn += len(south)
        data[:boundary_height, :] = south.reshape(*n_s_shape)
        
        west = compressed_data[current_posn:
                               current_posn + boundary_width * mid_height]
        current_posn += len(west)
        data[boundary_height:-boundary_height,
             :boundary_width] = west.reshape(*e_w_shape)

    elif lbpack.n2 == 2:
        if mask is None:
            raise ValueError('No mask was found to unpack the data. '
                             'Could not load.')
        land_mask = mask.data.astype(np.bool)
        sea_mask = ~land_mask
        new_data = np.ma.masked_all(land_mask.shape)
        if lbpack.n3 == 1:
            # Land mask packed data.
            new_data.mask = sea_mask
            # Sometimes the data comes in longer than it should be (i.e. it
            # looks like the compressed data is compressed, but the trailing
            # data hasn't been clipped off!).
            new_data[land_mask] = data[:land_mask.sum()]
        elif lbpack.n3 == 2:
            # Sea mask packed data.
            new_data.mask = land_mask
            new_data[sea_mask] = data[:sea_mask.sum()]
        else:
            raise ValueError('Unsupported mask compression.')
        data = new_data

    else:
        # Reform in row-column order
        data.shape = data_shape

    # Mask the array?
    if mdi in data:
        data = ma.masked_values(data, mdi, copy=False)

    return data


# The special headers of the PPField classes which get some improved functionality
_SPECIAL_HEADERS = ('lbtim', 'lbcode', 'lbpack', 'lbproc', 'data', 'stash',
                    't1', 't2')


def _header_defn(release_number):
    """
    Returns the zero-indexed header definition for a particular release of a PPField.

    """
    um_header = UM_HEADERS[release_number]
    offset = UM_TO_PP_HEADER_OFFSET
    return [(name, tuple(position - offset for position in positions)) for name, positions in um_header]


def _pp_attribute_names(header_defn):
    """
    Returns the allowed attributes of a PPField:
        all of the normal headers (i.e. not the _SPECIAL_HEADERS),
        the _SPECIAL_HEADERS with '_' prefixed,
        the possible extra data headers.

    """
    normal_headers = list(name for name, positions in header_defn if name not in _SPECIAL_HEADERS)
    special_headers = list('_' + name for name in _SPECIAL_HEADERS)
    extra_data = EXTRA_DATA.values()
    raw_attributes = ['_raw_header', 'raw_lbtim', 'raw_lbpack']
    return normal_headers + special_headers + extra_data + raw_attributes


class PPField(object):
    """
    A generic class for PP fields - not specific to a particular header release number.

    A PPField instance can easily access the PP header "words" as attributes with some added useful capabilities::

        for field in iris.fileformats.pp.load(filename):
            print field.lbyr
            print field.lbuser
            print field.lbuser[0]
            print field.lbtim
            print field.lbtim.ia
            print field.t1

    """

    # NB. Subclasses must define the attribute HEADER_DEFN to be their
    # zero-based header definition. See PPField2 and PPField3 for examples.

    __metaclass__ = abc.ABCMeta

    __slots__ = ()

    def __init__(self, header=None):
        # Combined header longs and floats data cache.
        self._raw_header = header
        self.raw_lbtim = None
        self.raw_lbpack = None
        if header is not None:
            self.raw_lbtim = header[self.HEADER_DICT['lbtim'][0]]
            self.raw_lbpack = header[self.HEADER_DICT['lbpack'][0]]

    def __getattr__(self, key):
        """
        This method supports deferred attribute creation, which offers a
        significant loading optimisation, particularly when not all attributes
        are referenced and therefore created on the instance.

        When an 'ordinary' HEADER_DICT attribute is required, its associated
        header offset is used to lookup the data value/s from the combined
        header longs and floats data cache. The attribute is then set with this
        value/s on the instance. Thus future lookups for this attribute will be
        optimised, avoiding the __getattr__ lookup mechanism again.

        When a 'special' HEADER_DICT attribute (leading underscore) is
        required, its associated 'ordinary' (no leading underscore) header
        offset is used to lookup the data value/s from the combined header
        longs and floats data cache. The 'ordinary' attribute is then set
        with this value/s on the instance. This is required as 'special'
        attributes have supporting property convenience functionality base on
        the attribute value e.g. see 'lbpack' and 'lbtim'. Note that, for
        'special' attributes the interface is via the 'ordinary' attribute but
        the underlying attribute value is stored within the 'special'
        attribute.

        """
        try:
            loc = self.HEADER_DICT[key]
        except KeyError:
            if key[0] == '_' and key[1:] in self.HEADER_DICT:
                # Must be a special attribute.
                loc = self.HEADER_DICT[key[1:]]
            else:
                cls = self.__class__.__name__
                msg = '{!r} object has no attribute {!r}'.format(cls, key)
                raise AttributeError(msg)

        if len(loc) == 1:
            value = self._raw_header[loc[0]]
        else:
            start = loc[0]
            stop = loc[-1] + 1
            value = tuple(self._raw_header[start:stop])

        # Now cache the attribute value on the instance.
        if key[0] == '_':
            # First we need to assign to the attribute so that the
            # special attribute is calculated, then we retrieve it.
            setattr(self, key[1:], value)
            value = getattr(self, key)
        else:
            setattr(self, key, value)
        return value

    @abc.abstractproperty
    def t1(self):
        pass

    @abc.abstractproperty
    def t2(self):
        pass

    def __repr__(self):
        """Return a string representation of the PP field."""

        # Define an ordering on the basic header names
        attribute_priority_lookup = {name: loc[0] for name, loc in self.HEADER_DEFN}

        # With the attributes sorted the order will remain stable if extra attributes are added.
        public_attribute_names =  attribute_priority_lookup.keys() + EXTRA_DATA.values()
        self_attrs = [(name, getattr(self, name, None)) for name in public_attribute_names]
        self_attrs = filter(lambda pair: pair[1] is not None, self_attrs)

        # Output any masked data as separate `data` and `mask`
        # components, to avoid the standard MaskedArray output
        # which causes irrelevant discrepancies between NumPy
        # v1.6 and v1.7.
        if ma.isMaskedArray(self._data):
            # Force the fill value to zero to have the minimum
            # impact on the output style.
            self_attrs.append(('data.data', self._data.filled(0)))
            self_attrs.append(('data.mask', self._data.mask))
        else:
            self_attrs.append(('data', self._data))

        # sort the attributes by position in the pp header followed, then by alphabetical order.
        attributes = sorted(self_attrs, key=lambda pair: (attribute_priority_lookup.get(pair[0], 999), pair[0]) )

        return 'PP Field' + ''.join(['\n   %s: %s' % (k, v) for k, v in attributes]) + '\n'

    @property
    def stash(self):
        """A stash property giving access to the associated STASH object, now supporting __eq__"""
        if (not hasattr(self, '_stash') or
                self.lbuser[6] != self._stash.lbuser6() or
                self.lbuser[3] != self._stash.lbuser3()):
            self._stash = STASH(self.lbuser[6], self.lbuser[3] / 1000, self.lbuser[3] % 1000)
        return self._stash
    
    @stash.setter
    def stash(self, stash):
        if isinstance(stash, basestring):
            self._stash = STASH.from_msi(stash)
        elif isinstance(stash, STASH):
            self._stash = stash
        else:
            raise ValueError('Cannot set stash to {!r}'.format(stash))
        
        # Keep the lbuser up to date.
        self.lbuser = list(self.lbuser)
        self.lbuser[6] = self._stash.lbuser6()
        self.lbuser[3] = self._stash.lbuser3()

    # lbtim
    def _lbtim_setter(self, new_value):
        if not isinstance(new_value, SplittableInt):
            self.raw_lbtim = new_value
            # add the ia/ib/ic values for lbtim
            new_value = SplittableInt(new_value, {'ia':slice(2, None), 'ib':1, 'ic':0})
        else:
            self.raw_lbtim = new_value._value
        self._lbtim = new_value

    lbtim = property(lambda self: self._lbtim, _lbtim_setter)

    # lbcode
    def _lbcode_setter(self, new_value):
        if not isinstance(new_value, SplittableInt):
            # add the ix/iy values for lbcode
            new_value = SplittableInt(new_value, {'iy':slice(0, 2), 'ix':slice(2, 4)})
        self._lbcode = new_value

    lbcode = property(lambda self: self._lbcode, _lbcode_setter)

    # lbpack
    def _lbpack_setter(self, new_value):
        if not isinstance(new_value, SplittableInt):
            self.raw_lbpack = new_value
            # add the n1/n2/n3/n4/n5 values for lbpack
            name_mapping = dict(n5=slice(4, None), n4=3, n3=2, n2=1, n1=0)
            new_value = SplittableInt(new_value, name_mapping)
        else:
            self.raw_lbpack = new_value._value
        self._lbpack = new_value

    lbpack = property(lambda self: self._lbpack, _lbpack_setter)

    # lbproc
    def _lbproc_setter(self, new_value):
        if not isinstance(new_value, BitwiseInt):
            new_value = BitwiseInt(new_value, num_bits=18)
        self._lbproc = new_value

    lbproc = property(lambda self: self._lbproc, _lbproc_setter)

    @property
    def data(self):
        """The :class:`numpy.ndarray` representing the multidimensional data of the pp file"""
        # Cache the real data on first use
        if isinstance(self._data, biggus.Array):
            data = self._data.masked_array()
            if ma.count_masked(data) == 0:
                data = data.data
            self._data = data
        return self._data

    @data.setter
    def data(self, value):
        self._data = value

    @property
    def calendar(self):
        """Return the calendar of the field."""
        # TODO #577 What calendar to return when ibtim.ic in [0, 3]
        calendar = iris.unit.CALENDAR_GREGORIAN
        if self.lbtim.ic == 2:
            calendar = iris.unit.CALENDAR_360_DAY
        elif self.lbtim.ic == 4:
            calendar = iris.unit.CALENDAR_365_DAY
        return calendar

    def _read_extra_data(self, pp_file, file_reader, extra_len):
        """Read the extra data section and update the self appropriately."""

        # While there is still extra data to decode run this loop
        while extra_len > 0:
            extra_int_code = struct.unpack_from('>L', file_reader(PP_WORD_DEPTH))[0]
            extra_len -= PP_WORD_DEPTH

            ib = extra_int_code % 1000
            ia = extra_int_code // 1000

            data_len = ia * PP_WORD_DEPTH

            if ib == 10:
                self.field_title = ''.join(struct.unpack_from('>%dc' % data_len, file_reader(data_len))).rstrip('\00')
            elif ib == 11:
                self.domain_title = ''.join(struct.unpack_from('>%dc' % data_len, file_reader(data_len))).rstrip('\00')
            elif ib in EXTRA_DATA:
                attr_name = EXTRA_DATA[ib]
                values = np.fromfile(pp_file, dtype=np.dtype('>f%d' % PP_WORD_DEPTH), count=ia)
                # Ensure the values are in the native byte order
                if not values.dtype.isnative:
                    values.byteswap(True)
                    values.dtype = values.dtype.newbyteorder('=')
                setattr(self, attr_name, values)
            else:
                raise ValueError('Unknown IB value for extra data: %s' % ib)

            extra_len -= data_len

    @property
    def x_bounds(self):
        if hasattr(self, "x_lower_bound") and hasattr(self, "x_upper_bound"):
            return np.column_stack((self.x_lower_bound, self.x_upper_bound))

    @property
    def y_bounds(self):
        if hasattr(self, "y_lower_bound") and hasattr(self, "y_upper_bound"):
            return np.column_stack((self.y_lower_bound, self.y_upper_bound))

    def save(self, file_handle):
        """
        Save the PPField to the given file object (typically created with :func:`open`).

        ::

            # to append the field to a file
            a_pp_field.save(open(filename, 'ab'))

            # to overwrite/create a file
            a_pp_field.save(open(filename, 'wb'))


        .. note::

            The fields which are automatically calculated are: 'lbext',
            'lblrec' and 'lbuser[0]'. Some fields are not currently
            populated, these are: 'lbegin', 'lbnrec', 'lbuser[1]'.

        """

        # Before we can actually write to file, we need to calculate the header elements.
        # First things first, make sure the data is big-endian
        data = self.data
        if isinstance(data, ma.core.MaskedArray):
            data = data.filled(fill_value=self.bmdi)

        if data.dtype.newbyteorder('>') != data.dtype:
            # take a copy of the data when byteswapping
            data = data.byteswap(False)
            data.dtype = data.dtype.newbyteorder('>')

        # Create the arrays which will hold the header information
        lb = np.empty(shape=NUM_LONG_HEADERS, dtype=np.dtype(">u%d" % PP_WORD_DEPTH))
        b = np.empty(shape=NUM_FLOAT_HEADERS, dtype=np.dtype(">f%d" % PP_WORD_DEPTH))

        # Populate the arrays from the PPField
        for name, pos in self.HEADER_DEFN:
            try:
                header_elem = getattr(self, name)
            except AttributeError:
                raise AttributeError("PPField.save() could not find %s" % name)
            if pos[0] <= NUM_LONG_HEADERS - UM_TO_PP_HEADER_OFFSET:
                index = slice(pos[0], pos[-1] + 1)
                if isinstance(header_elem, SplittableInt):
                    header_elem = int(header_elem)
                lb[index] = header_elem
            else:
                index = slice(pos[0] - NUM_LONG_HEADERS, pos[-1] - NUM_LONG_HEADERS + 1)
                b[index] = header_elem

        # Although all of the elements are now populated, we still need to update some of the elements in case
        # things have changed (for example, the data length etc.)

        # Set up a variable to represent the datalength of this PPField in WORDS.
        len_of_data_payload = 0

        # set up a list to hold the extra data which will need to be encoded at the end of the data
        extra_items = []
        # iterate through all of the possible extra data fields
        for ib, extra_data_attr_name in EXTRA_DATA.iteritems():
            # try to get the extra data field, returning None if it doesn't exist
            extra_elem = getattr(self, extra_data_attr_name, None)
            if extra_elem is not None:
                # The special case of character extra data must be caught
                if isinstance(extra_elem, basestring):
                    ia = len(extra_elem)
                    # pad any strings up to a multiple of PP_WORD_DEPTH (this length is # of bytes)
                    ia = (PP_WORD_DEPTH - (ia-1) % PP_WORD_DEPTH) + (ia-1)
                    extra_elem = extra_elem.ljust(ia, '\00')

                    # ia is now the datalength in WORDS of the string
                    ia /= PP_WORD_DEPTH
                else:
                    # ia is the datalength in WORDS
                    ia = np.product(extra_elem.shape)
                    # flip the byteorder if the data is not big-endian
                    if extra_elem.dtype.newbyteorder('>') != extra_elem.dtype:
                        # take a copy of the extra data when byte swapping
                        extra_elem = extra_elem.byteswap(False)
                        extra_elem.dtype = extra_elem.dtype.newbyteorder('>')

                # add the number of bytes to the len_of_data_payload variable + the extra integer which will encode ia/ib
                len_of_data_payload += PP_WORD_DEPTH * ia + PP_WORD_DEPTH
                integer_code = 1000 * ia + ib
                extra_items.append( [integer_code, extra_elem] )

                if ia >= 1000:
                    raise IOError('PP files cannot write extra data with more than '
                                  '1000 elements. Tried to write "%s" which has %s '
                                  'elements.' % (extra_data_attr_name, ib)
                                  )

        # populate lbext in WORDS
        lb[self.HEADER_DICT['lbext'][0]] = len_of_data_payload / PP_WORD_DEPTH

        # Put the data length of pp.data into len_of_data_payload (in BYTES)
        len_of_data_payload += data.size * PP_WORD_DEPTH

        # populate lbrec in WORDS
        lb[self.HEADER_DICT['lblrec'][0]] = len_of_data_payload / PP_WORD_DEPTH

        # populate lbuser[0] to have the data's datatype
        if data.dtype == np.dtype('>f4'):
            lb[self.HEADER_DICT['lbuser'][0]] = 1
        elif data.dtype == np.dtype('>f8'):
            warnings.warn("Downcasting array precision from float64 to float32 for save."
                          "If float64 precision is required then please save in a different format")
            data = data.astype('>f4')
            lb[self.HEADER_DICT['lbuser'][0]] = 1
        elif data.dtype == np.dtype('>i4'):
            # NB: there is no physical difference between lbuser[0] of 2 or 3 so we encode just 2
            lb[self.HEADER_DICT['lbuser'][0]] = 2
        else:
            raise IOError('Unable to write data array to a PP file. The datatype was %s.' % data.dtype)

        # NB: lbegin, lbnrec, lbuser[1] not set up

        # Now that we have done the manouvering required, write to the file...
        if not isinstance(file_handle, file):
            raise TypeError('The file_handle argument must be an instance of a Python file object, but got %r. \n'
                             'e.g. open(filename, "wb") to open a binary file with write permission.' % type(file_handle))

        pp_file = file_handle

        # header length
        pp_file.write(struct.pack(">L", PP_HEADER_DEPTH))

        # 49 integers
        lb.tofile(pp_file)
        # 16 floats
        b.tofile(pp_file)

        #Header length (again)
        pp_file.write(struct.pack(">L", PP_HEADER_DEPTH))

        # Data length (including extra data length)
        pp_file.write(struct.pack(">L", int(len_of_data_payload)))

        # the data itself
        if lb[self.HEADER_DICT['lbpack'][0]] == 0:
            data.tofile(pp_file)
        else:
            msg = 'Writing packed pp data with lbpack of {} ' \
                'is not supported.'.format(lb[self.HEADER_DICT['lbpack'][0]])
            raise NotImplementedError(msg)

        # extra data elements
        for int_code, extra_data in extra_items:
            pp_file.write(struct.pack(">L", int(int_code)))
            if isinstance(extra_data, basestring):
                pp_file.write(struct.pack(">%sc" % len(extra_data), *extra_data))
            else:
                extra_data = extra_data.astype(np.dtype('>f4'))
                extra_data.tofile(pp_file)

        # Data length (again)
        pp_file.write(struct.pack(">L", int(len_of_data_payload)))

    ##############################################################
    #
    # From here on define helper methods for PP -> Cube conversion.
    #

    def regular_points(self, xy):
        """Return regular points from the PPField, or fail if not regular.

        Args:

            * xy - a string, "x" or "y" to specify the dimension for which to return points.

        .. deprecated:: 1.5

        """ 
        msg = "The 'regular_points' method is deprecated."
        warnings.warn(msg, UserWarning, stacklevel=2)

        if xy.lower() == "x":
            bz = self.bzx
            bd = self.bdx
            count = self.lbnpt
        elif xy.lower() == "y":
            bz = self.bzy
            bd = self.bdy
            count = self.lbrow
        else:
            raise ValueError("'x' or 'y' not supplied")

        return (bz + bd) + bd * np.arange(count, dtype=np.float32)

    def regular_bounds(self, xy):
        """Return regular bounds from the PPField, or fail if not regular.

        Args:

            * xy - a string, "x" or "y" to specify the dimension for which to return points.

        .. deprecated:: 1.5

        """ 
        msg = "The 'regular_bounds' method is deprecated."
        warnings.warn(msg, UserWarning, stacklevel=2)

        if xy.lower() == "x":
            delta = 0.5 * self.bdx
        elif xy.lower() == "y":
            delta = 0.5 * self.bdy
        else:
            raise ValueError("'x' or 'y' not supplied")

        points = self.regular_points(xy)
        return np.concatenate([[points - delta], [points + delta]]).T

    def time_unit(self, time_unit, epoch='epoch'):
        return iris.unit.Unit('%s since %s' % (time_unit, epoch), calendar=self.calendar)

    def coord_system(self):
        """Return a CoordSystem for this PPField.

        Returns:
            Currently, a :class:`~iris.coord_systems.GeogCS` or :class:`~iris.coord_systems.RotatedGeogCS`.

        """
        geog_cs =  iris.coord_systems.GeogCS(EARTH_RADIUS)
        if self.bplat != 90.0 or self.bplon != 0.0:
            geog_cs = iris.coord_systems.RotatedGeogCS(self.bplat, self.bplon, ellipsoid=geog_cs)

        return geog_cs

    def _x_coord_name(self):
        # TODO: Remove once we have the ability to derive this in the rules.
        x_name = "longitude"
        if isinstance(self.coord_system(), iris.coord_systems.RotatedGeogCS):
            x_name = "grid_longitude"
        return x_name

    def _y_coord_name(self):
        # TODO: Remove once we have the ability to derive this in the rules.
        y_name = "latitude"
        if isinstance(self.coord_system(), iris.coord_systems.RotatedGeogCS):
            y_name = "grid_latitude"
        return y_name

    def copy(self):
        """
        Returns a deep copy of this PPField.

        Returns:
            A copy instance of the :class:`PPField`.

        """
        return self._deepcopy({})

    def __deepcopy__(self, memo):
        return self._deepcopy(memo)

    def _deepcopy(self, memo):
        field = self.__class__()
        for attr in self.__slots__:
            if hasattr(self, attr):
                value = getattr(self, attr)
                # Cope with inability to deepcopy a 0-d NumPy array.
                if attr == '_data' and value is not None and value.ndim == 0:
                    setattr(field, attr, np.array(deepcopy(value[()], memo)))
                else:
                    setattr(field, attr, deepcopy(value, memo))
        return field

    def __eq__(self, other):
        result = NotImplemented
        if isinstance(other, PPField):
            result = True
            for attr in self.__slots__:
                attrs = [hasattr(self, attr), hasattr(other, attr)]
                if all(attrs):
                    self_attr = getattr(self, attr)
                    other_attr = getattr(other, attr)
                    if isinstance(self_attr, biggus.NumpyArrayAdapter):
                        self_attr = self_attr.concrete
                    if isinstance(other_attr, biggus.NumpyArrayAdapter):
                        other_attr = other_attr.concrete
                    if not np.all(self_attr == other_attr):
                        result = False
                        break
                elif any(attrs):
                    result = False
                    break
        return result

    def __ne__(self, other):
        result = self.__eq__(other)
        if result is not NotImplemented:
            result = not result
        return result


class PPField2(PPField):
    """
    A class to hold a single field from a PP file, with a header release number of 2.

    """
    HEADER_DEFN = _header_defn(2)
    HEADER_DICT = dict(HEADER_DEFN)

    __slots__ = _pp_attribute_names(HEADER_DEFN)

    def _get_t1(self):
        if not hasattr(self, '_t1'):
            self._t1 = netcdftime.datetime(self.lbyr, self.lbmon, self.lbdat, self.lbhr, self.lbmin)
        return self._t1

    def _set_t1(self, dt):
        self.lbyr = dt.year
        self.lbmon = dt.month
        self.lbdat = dt.day
        self.lbhr = dt.hour
        self.lbmin = dt.minute
        self.lbday = int(dt.strftime('%j'))
        if hasattr(self, '_t1'):
            delattr(self, '_t1')

    t1 = property(_get_t1, _set_t1, None,
        "A netcdftime.datetime object consisting of the lbyr, lbmon, lbdat, lbhr, and lbmin attributes.")

    def _get_t2(self):
        if not hasattr(self, '_t2'):
            self._t2 = netcdftime.datetime(self.lbyrd, self.lbmond, self.lbdatd, self.lbhrd, self.lbmind)
        return self._t2

    def _set_t2(self, dt):
        self.lbyrd = dt.year
        self.lbmond = dt.month
        self.lbdatd = dt.day
        self.lbhrd = dt.hour
        self.lbmind = dt.minute
        self.lbdayd = int(dt.strftime('%j'))
        if hasattr(self, '_t2'):
            delattr(self, '_t2')

    t2 = property(_get_t2, _set_t2, None,
        "A netcdftime.datetime object consisting of the lbyrd, lbmond, lbdatd, lbhrd, and lbmind attributes.")


class PPField3(PPField):
    """
    A class to hold a single field from a PP file, with a header release number of 3.

    """
    HEADER_DEFN = _header_defn(3)
    HEADER_DICT = dict(HEADER_DEFN)

    __slots__ = _pp_attribute_names(HEADER_DEFN)

    def _get_t1(self):
        if not hasattr(self, '_t1'):
            self._t1 = netcdftime.datetime(self.lbyr, self.lbmon, self.lbdat, self.lbhr, self.lbmin, self.lbsec)
        return self._t1

    def _set_t1(self, dt):
        self.lbyr = dt.year
        self.lbmon = dt.month
        self.lbdat = dt.day
        self.lbhr = dt.hour
        self.lbmin = dt.minute
        self.lbsec = dt.second
        if hasattr(self, '_t1'):
            delattr(self, '_t1')

    t1 = property(_get_t1, _set_t1, None,
        "A netcdftime.datetime object consisting of the lbyr, lbmon, lbdat, lbhr, lbmin, and lbsec attributes.")

    def _get_t2(self):
        if not hasattr(self, '_t2'):
            self._t2 = netcdftime.datetime(self.lbyrd, self.lbmond, self.lbdatd, self.lbhrd, self.lbmind, self.lbsecd)
        return self._t2

    def _set_t2(self, dt):
        self.lbyrd = dt.year
        self.lbmond = dt.month
        self.lbdatd = dt.day
        self.lbhrd = dt.hour
        self.lbmind = dt.minute
        self.lbsecd = dt.second
        if hasattr(self, '_t2'):
            delattr(self, '_t2')

    t2 = property(_get_t2, _set_t2, None,
        "A netcdftime.datetime object consisting of the lbyrd, lbmond, lbdatd, lbhrd, lbmind, and lbsecd attributes.")


PP_CLASSES = {
    2: PPField2,
    3: PPField3
}


def make_pp_field(header):
    # Choose a PP field class from the value of LBREL
    lbrel = header[21]
    if lbrel not in PP_CLASSES:
        raise ValueError('Unsupported header release number: {}'.format(lbrel))
    pp_field = PP_CLASSES[lbrel](header)
    return pp_field


LoadedArrayBytes = collections.namedtuple('LoadedArrayBytes', 'bytes, dtype')


def load(filename, read_data=False):
    """
    Return an iterator of PPFields given a filename.

    Args:

    * filename - string of the filename to load.

    Kwargs:

    * read_data - boolean
        Flag whether or not the data should be read, if False an empty data manager
        will be provided which can subsequently load the data on demand. Default False.

    To iterate through all of the fields in a pp file::

        for field in iris.fileformats.pp.load(filename):
            print field

    """
    return _interpret_fields(_field_gen(filename, read_data_bytes=read_data))


def _interpret_fields(fields):
    """
    Turn the fields read with load and FF2PP._extract_field into useable
    fields. One of the primary purposes of this function is to either convert
    "deferred bytes" into "deferred arrays" or "loaded bytes" into actual
    numpy arrays (via the _create_field_data) function.

    """
    land_mask = None
    landmask_compressed_fields = []
    for field in fields:
        # Store the first reference to a land mask, and use this as the
        # definitive mask for future fields in this generator.
        if land_mask is None and field.lbuser[6] == 1 and \
                (field.lbuser[3] / 1000) == 0 and \
                (field.lbuser[3] % 1000) == 30:
            land_mask = field

        # Handle land compressed data payloads,
        # when lbpack.n2 is 2.
        if (field.raw_lbpack / 10 % 10) == 2:
            if land_mask is None:
                landmask_compressed_fields.append(field)
                continue

            # Land compressed fields don't have a lbrow and lbnpt.
            field.lbrow, field.lbnpt = land_mask.lbrow, land_mask.lbnpt

        data_shape = (field.lbrow, field.lbnpt)
        _create_field_data(field, data_shape, land_mask)
        yield field

    if landmask_compressed_fields:
        if land_mask is None:
            warnings.warn('Landmask compressed fields existed without a '
                          'landmask to decompress with. The data will have '
                          'a shape of (0, 0) and will not read.')
            mask_shape = (0, 0)
        else:
            mask_shape = (land_mask.lbrow, land_mask.lbnpt)

        for field in landmask_compressed_fields:
            field.lbrow, field.lbnpt = mask_shape
            _create_field_data(field, (field.lbrow, field.lbnpt), land_mask)
            yield field


def _create_field_data(field, data_shape, land_mask):
    """
    Modifies a field's ``_data`` attribute either by:
     * converting DeferredArrayBytes into a biggus array,
     * converting LoadedArrayBytes into an actual numpy array.

    """
    if isinstance(field._data, LoadedArrayBytes):
        loaded_bytes = field._data
        field._data = _data_bytes_to_shaped_array(loaded_bytes.bytes,
                                                  field.lbpack, data_shape,
                                                  loaded_bytes.dtype,
                                                  field.bmdi, land_mask)
    else:
        # Wrap the reference to the data payload within a data proxy
        # in order to support deferred data loading.
        fname, position, n_bytes, dtype = field._data
        proxy = PPDataProxy(data_shape, dtype,
                            fname, position,
                            n_bytes, field.raw_lbpack,
                            field.bmdi, land_mask)
        field._data = biggus.NumpyArrayAdapter(proxy)


def _field_gen(filename, read_data_bytes):
    """
    Returns a generator of "half-formed" PPField instances derived from
    the given filename.

    A field returned by the generator is only "half-formed" because its
    `_data` attribute represents a simple one-dimensional stream of
    bytes. (Encoded as an instance of either LoadedArrayBytes or
    DeferredArrayBytes, depending on the value of `read_data_bytes`.)
    This is because fields encoded with a land/sea mask do not contain
    sufficient information within the field to determine the final
    two-dimensional shape of the data.

    """
    pp_file = open(filename, 'rb')

    # Get a reference to the seek method on the file
    # (this is accessed 3* #number of headers so can provide a small performance boost)
    pp_file_seek = pp_file.seek
    pp_file_read = pp_file.read

    # Keep reading until we reach the end of file
    while True:
        # Move past the leading header length word
        pp_file_seek(PP_WORD_DEPTH, os.SEEK_CUR)
        # Get the LONG header entries
        header_longs = np.fromfile(pp_file, dtype='>i%d' % PP_WORD_DEPTH, count=NUM_LONG_HEADERS)
        # Nothing returned => EOF
        if len(header_longs) == 0:
            break
        # Get the FLOAT header entries
        header_floats = np.fromfile(pp_file, dtype='>f%d' % PP_WORD_DEPTH, count=NUM_FLOAT_HEADERS)
        header = tuple(header_longs) + tuple(header_floats)

        # Make a PPField of the appropriate sub-class (depends on header release number)
        pp_field = make_pp_field(header)

        # Skip the trailing 4-byte word containing the header length
        pp_file_seek(PP_WORD_DEPTH, os.SEEK_CUR)

        # Read the word telling me how long the data + extra data is
        # This value is # of bytes
        len_of_data_plus_extra = struct.unpack_from('>L', pp_file_read(PP_WORD_DEPTH))[0]
        if len_of_data_plus_extra != pp_field.lblrec * PP_WORD_DEPTH:
            raise ValueError('LBLREC has a different value to the integer recorded after the '
                             'header in the file (%s and %s).' % (pp_field.lblrec * PP_WORD_DEPTH,
                                                                  len_of_data_plus_extra))

        # calculate the extra length in bytes
        extra_len = pp_field.lbext * PP_WORD_DEPTH

        # Derive size and datatype of payload
        data_len = len_of_data_plus_extra - extra_len
        dtype = LBUSER_DTYPE_LOOKUP.get(pp_field.lbuser[0],
                                        LBUSER_DTYPE_LOOKUP['default'])

        if read_data_bytes:
            # Read the actual bytes. This can then be converted to a numpy array
            # at a higher level.
            pp_field._data = LoadedArrayBytes(pp_file.read(data_len), dtype)
        else:
            # Provide enough context to read the data bytes later on.
            pp_field._data = (filename, pp_file.tell(), data_len, dtype)
            # Seek over the actual data payload.
            pp_file_seek(data_len, os.SEEK_CUR)

        # Do we have any extra data to deal with?
        if extra_len:
            pp_field._read_extra_data(pp_file, pp_file_read, extra_len)

        # Skip that last 4 byte record telling me the length of the field I have already read
        pp_file_seek(PP_WORD_DEPTH, os.SEEK_CUR)
        yield pp_field
    pp_file.close()


def _ensure_load_rules_loaded():
    """Makes sure the standard conversion and verification rules are loaded."""

    # Uses these module-level variables
    global _load_rules, _cross_reference_rules

    rules = iris.fileformats.rules

    if _load_rules is None:
        basepath = iris.config.CONFIG_PATH
        _load_rules = rules.RulesContainer(os.path.join(basepath, 'pp_rules.txt'))

    if _cross_reference_rules is None:
        basepath = iris.config.CONFIG_PATH
        _cross_reference_rules = rules.RulesContainer(os.path.join(basepath, 'pp_cross_reference_rules.txt'),
                                                           rule_type=rules.ObjectReturningRule)


def add_load_rules(filename):
    """
    Registers a rules file for use during the PP load process.

    Registered files are processed after the standard conversion rules, and in
    the order they were registered.
    
        .. deprecated:: 1.5

    """
    msg = "The 'add_load_rules' function is deprecated."
    warnings.warn(msg, UserWarning, stacklevel=2)

    # Uses this module-level variable
    global _load_rules

    if _load_rules is None:
        _load_rules = iris.fileformats.rules.RulesContainer(filename)
    else:
        _load_rules.import_rules(filename)


def reset_load_rules():
    """Resets the PP load process to use only the standard conversion rules."""

    # Uses this module-level variable
    global _load_rules

    _load_rules = None


def _ensure_save_rules_loaded():
    """Makes sure the standard save rules are loaded."""

    # Uses these module-level variables
    global _save_rules

    if _save_rules is None:
        # Load the pp save rules
        rules_filename = os.path.join(iris.config.CONFIG_PATH, 'pp_save_rules.txt')
        _save_rules = iris.fileformats.rules.RulesContainer(rules_filename, iris.fileformats.rules.ProcedureRule)


def add_save_rules(filename):
    """
    Registers a rules file for use during the PP save process.

    Registered files are processed after the standard conversion rules, and in
    the order they were registered.

    """
    _ensure_save_rules_loaded()
    _save_rules.import_rules(filename)


def reset_save_rules():
    """Resets the PP save process to use only the standard conversion rules."""

    # Uses this module-level variable
    global _save_rules

    _save_rules = None


def load_cubes(filenames, callback=None):
    """
    Loads cubes from a list of pp filenames.

    Args:

    * filenames - list of pp filenames to load

    Kwargs:

    * callback - a function which can be passed on to :func:`iris.io.run_callback`

    .. note::

        The resultant cubes may not be in the order that they are in the file (order
        is not preserved when there is a field with orography references)

    """
    return _load_cubes_variable_loader(filenames, callback, load)


def _load_cubes_variable_loader(filenames, callback, loading_function,
                                loading_function_kwargs=None):
    pp_loader = iris.fileformats.rules.Loader(
        loading_function, loading_function_kwargs or {},
        iris.fileformats.pp_rules.convert, _load_rules)
    return iris.fileformats.rules.load_cubes(filenames, callback, pp_loader)


def save(cube, target, append=False, field_coords=None):
    """
    Use the PP saving rules (and any user rules) to save a cube to a PP file.

    Args:

        * cube         - A :class:`iris.cube.Cube`, :class:`iris.cube.CubeList` or list of cubes.
        * target       - A filename or open file handle.

    Kwargs:

        * append       - Whether to start a new file afresh or add the cube(s) to the end of the file.
                         Only applicable when target is a filename, not a file handle.
                         Default is False.

        * field_coords - list of 2 coords or coord names which are to be used for
                         reducing the given cube into 2d slices, which will ultimately
                         determine the x and y coordinates of the resulting fields.
                         If None, the final two  dimensions are chosen for slicing.

    See also :func:`iris.io.save`.

    """

    # Open issues
    # Could use rules in "sections" ... e.g. to process the extensive dimensions; ...?
    # Could pre-process the cube to add extra convenient terms?
    #    e.g. x-coord, y-coord ... but what about multiple coordinates on the dimension?

    # How to perform the slicing?
    #   Do we always slice in the last two dimensions?
    #   Not all source data will contain lat-lon slices.
    # What do we do about dimensions with multiple coordinates?

    # Deal with:
    #   LBLREC - Length of data record in words (incl. extra data)
    #       Done on save(*)
    #   LBUSER[0] - Data type
    #       Done on save(*)
    #   LBUSER[1] - Start address in DATA (?! or just set to "null"?)
    #   BLEV - Level - the value of the coordinate for LBVC

    # *) With the current on-save way of handling LBLREC and LBUSER[0] we can't
    # check if they've been set correctly without *actually* saving as a binary
    # PP file. That also means you can't use the same reference.txt file for
    # loaded vs saved fields (unless you re-load the saved field!).

    # Set to (or leave as) "null":
    #   LBEGIN - Address of start of field in direct access dataset
    #   LBEXP - Experiment identification
    #   LBPROJ - Fields file projection number
    #   LBTYP - Fields file field type code
    #   LBLEV - Fields file level code / hybrid height model level

    # Build confidence by having a PP object that records which header items
    # have been set, and only saves if they've all been set?
    #   Watch out for extra-data.

    # On the flip side, record which Cube metadata has been "used" and flag up
    # unused?

    _ensure_save_rules_loaded()

    # pp file
    if isinstance(target, basestring):
        pp_file = open(target, "ab" if append else "wb")
    elif hasattr(target, "write"):
        if hasattr(target, "mode") and "b" not in target.mode:
            raise ValueError("Target not binary")
        pp_file = target
    else:
        raise ValueError("Can only save pp to filename or writable")

    n_dims = len(cube.shape)
    if n_dims < 2:
        raise ValueError('Unable to save a cube of fewer than 2 dimensions.')

    if field_coords is not None:
        # cast the given coord/coord names into cube coords
        field_coords = cube._as_list_of_coords(field_coords)
        if len(field_coords) != 2:
            raise ValueError('Got %s coordinates in field_coords, expecting exactly 2.' % len(field_coords))
    else:
        # default to the last two dimensions (if result of coords is an empty list, will
        # raise an IndexError)
        # NB watch out for the ordering of the dimensions
        field_coords = (cube.coords(dimensions=n_dims-2)[0], cube.coords(dimensions=n_dims-1)[0])

    # Save each named or latlon slice2D in the cube
    for slice2D in cube.slices(field_coords):
        # Start with a blank PPField
        pp_field = PPField3()

        # Set all items to 0 because we need lbuser, lbtim
        # and some others to be present before running the rules.
        for name, positions in pp_field.HEADER_DEFN:
            # Establish whether field name is integer or real
            default = 0 if positions[0] <= NUM_LONG_HEADERS - UM_TO_PP_HEADER_OFFSET else 0.0
            # Establish whether field position is scalar or composite
            if len(positions) > 1:
                default = [default] * len(positions)
            setattr(pp_field, name, default)

        # Some defaults should not be 0
        pp_field.lbrel = 3      # Header release 3.
        pp_field.lbcode = 1     # Grid code.
        pp_field.bmks = 1.0     # Some scaley thing.
        pp_field.lbproc = 0
        # From UM doc F3: "Set to -99 if LBEGIN not known"
        pp_field.lbuser[1] = -99

        # Set the data
        pp_field.data = slice2D.data

        # Run the PP save rules on the slice2D, to fill the PPField,
        # recording the rules that were used
        rules_result = _save_rules.verify(slice2D, pp_field)
        verify_rules_ran = rules_result.matching_rules

        # Log the rules used
        iris.fileformats.rules.log('PP_SAVE', target if isinstance(target, basestring) else target.name, verify_rules_ran)

        # Write to file
        pp_field.save(pp_file)

    if isinstance(target, basestring):
        pp_file.close()

########NEW FILE########
__FILENAME__ = pp_rules
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

# Historically this was auto-generated from
# SciTools/iris-code-generators:tools/gen_rules.py

import warnings

import numpy as np

from iris.aux_factory import HybridHeightFactory, HybridPressureFactory
from iris.coords import AuxCoord, CellMethod, DimCoord
from iris.fileformats.rules import Factory, Reference, ReferenceTarget
from iris.fileformats.um_cf_map import LBFC_TO_CF, STASH_TO_CF
from iris.unit import Unit
import iris.fileformats.pp
import iris.unit


def _model_level_number(field):
    """
    Return the model level number of a field.

    Args:

    * field (:class:`iris.fileformats.pp.PPField`)
        PP field to inspect.

    Returns:
        Model level number (integer).

    """
    # See Word no. 33 (LBLEV) in section 4 of UM Model Docs (F3).
    SURFACE_AND_ZEROTH_RHO_LEVEL_LBLEV = 9999

    if field.lblev == SURFACE_AND_ZEROTH_RHO_LEVEL_LBLEV:
        model_level_number = 0
    else:
        model_level_number = field.lblev

    return model_level_number


def convert(f):
    factories = []
    references = []
    standard_name = None
    long_name = None
    units = None
    attributes = {}
    cell_methods = []
    dim_coords_and_dims = []
    aux_coords_and_dims = []

    if \
            (f.lbtim.ia == 0) and \
            (f.lbtim.ib == 0) and \
            (f.lbtim.ic in [1, 2, 3, 4]) and \
            (len(f.lbcode) != 5 or (len(f.lbcode) == 5 and f.lbcode.ix not in [20, 21, 22, 23] and f.lbcode.iy not in [20, 21, 22, 23])):
        aux_coords_and_dims.append((DimCoord(f.time_unit('hours').date2num(f.t1), standard_name='time', units=f.time_unit('hours')), None))

    if \
            (f.lbtim.ia == 0) and \
            (f.lbtim.ib == 1) and \
            (f.lbtim.ic in [1, 2, 3, 4]) and \
            (len(f.lbcode) != 5 or (len(f.lbcode) == 5 and f.lbcode.ix not in [20, 21, 22, 23] and f.lbcode.iy not in [20, 21, 22, 23])):
        aux_coords_and_dims.append((DimCoord(f.time_unit('hours', f.t2).date2num(f.t1), standard_name='forecast_period', units='hours'), None))
        aux_coords_and_dims.append((DimCoord(f.time_unit('hours').date2num(f.t1), standard_name='time', units=f.time_unit('hours')), None))
        aux_coords_and_dims.append((DimCoord(f.time_unit('hours').date2num(f.t2), standard_name='forecast_reference_time', units=f.time_unit('hours')), None))

    if \
            (f.lbtim.ib == 2) and \
            (f.lbtim.ic in [1, 2, 4]) and \
            ((len(f.lbcode) != 5) or (len(f.lbcode) == 5 and f.lbcode.ix not in [20, 21, 22, 23] and f.lbcode.iy not in [20, 21, 22, 23])):
        t_unit = f.time_unit('hours')
        t1_hours = t_unit.date2num(f.t1)
        t2_hours = t_unit.date2num(f.t2)
        period = t2_hours - t1_hours
        aux_coords_and_dims.append((
            DimCoord(standard_name='forecast_period', units='hours',
                     points=f.lbft - 0.5 * period,
                     bounds=[f.lbft - period, f.lbft]),
            None))
        aux_coords_and_dims.append((
            DimCoord(standard_name='time', units=t_unit,
                     points=0.5 * (t1_hours + t2_hours),
                     bounds=[t1_hours, t2_hours]),
            None))
        aux_coords_and_dims.append((DimCoord(f.time_unit('hours').date2num(f.t2) - f.lbft, standard_name='forecast_reference_time', units=f.time_unit('hours')), None))

    if \
            (f.lbtim.ib == 3) and \
            (f.lbtim.ic in [1, 2, 4]) and \
            ((len(f.lbcode) != 5) or (len(f.lbcode) == 5 and f.lbcode.ix not in [20, 21, 22, 23] and f.lbcode.iy not in [20, 21, 22, 23])):
        t_unit = f.time_unit('hours')
        t1_hours = t_unit.date2num(f.t1)
        t2_hours = t_unit.date2num(f.t2)
        period = t2_hours - t1_hours
        aux_coords_and_dims.append((
            DimCoord(standard_name='forecast_period', units='hours',
                     points=f.lbft, bounds=[f.lbft - period, f.lbft]),
            None))
        aux_coords_and_dims.append((
            DimCoord(standard_name='time', units=t_unit,
                     points=t2_hours, bounds=[t1_hours, t2_hours]),
            None))
        aux_coords_and_dims.append((DimCoord(f.time_unit('hours').date2num(f.t2) - f.lbft, standard_name='forecast_reference_time', units=f.time_unit('hours')), None))

    if \
            (f.lbtim.ib == 3) and \
            (f.lbtim.ic in [1, 2, 4]) and \
            ((len(f.lbcode) != 5) or (len(f.lbcode) == 5 and f.lbcode.ix not in [20, 21, 22, 23] and f.lbcode.iy not in [20, 21, 22, 23])) and \
            (f.lbmon == 12 and f.lbdat == 1 and f.lbhr == 0 and f.lbmin == 0) and \
            (f.lbmond == 3 and f.lbdatd == 1 and f.lbhrd == 0 and f.lbmind == 0):
        aux_coords_and_dims.append((AuxCoord('djf', long_name='season', units='no_unit'), None))

    if \
            (f.lbtim.ib == 3) and \
            (f.lbtim.ic in [1, 2, 4]) and \
            ((len(f.lbcode) != 5) or (len(f.lbcode) == 5 and f.lbcode.ix not in [20, 21, 22, 23] and f.lbcode.iy not in [20, 21, 22, 23])) and \
            (f.lbmon == 3 and f.lbdat == 1 and f.lbhr == 0 and f.lbmin == 0) and \
            (f.lbmond == 6 and f.lbdatd == 1 and f.lbhrd == 0 and f.lbmind == 0):
        aux_coords_and_dims.append((AuxCoord('mam', long_name='season', units='no_unit'), None))

    if \
            (f.lbtim.ib == 3) and \
            (f.lbtim.ic in [1, 2, 4]) and \
            ((len(f.lbcode) != 5) or (len(f.lbcode) == 5 and f.lbcode.ix not in [20, 21, 22, 23] and f.lbcode.iy not in [20, 21, 22, 23])) and \
            (f.lbmon == 6 and f.lbdat == 1 and f.lbhr == 0 and f.lbmin == 0) and \
            (f.lbmond == 9 and f.lbdatd == 1 and f.lbhrd == 0 and f.lbmind == 0):
        aux_coords_and_dims.append((AuxCoord('jja', long_name='season', units='no_unit'), None))

    if \
            (f.lbtim.ib == 3) and \
            (f.lbtim.ic in [1, 2, 4]) and \
            ((len(f.lbcode) != 5) or (len(f.lbcode) == 5 and f.lbcode.ix not in [20, 21, 22, 23] and f.lbcode.iy not in [20, 21, 22, 23])) and \
            (f.lbmon == 9 and f.lbdat == 1 and f.lbhr == 0 and f.lbmin == 0) and \
            (f.lbmond == 12 and f.lbdatd == 1 and f.lbhrd == 0 and f.lbmind == 0):
        aux_coords_and_dims.append((AuxCoord('son', long_name='season', units='no_unit'), None))

    if \
            (f.bdx != 0.0) and \
            (f.bdx != f.bmdi) and \
            (len(f.lbcode) != 5) and \
            (f.lbcode[0] == 1):
        dim_coords_and_dims.append((DimCoord.from_regular(f.bzx, f.bdx, f.lbnpt, standard_name=f._x_coord_name(), units='degrees', circular=(f.lbhem in [0, 4]), coord_system=f.coord_system()), 1))

    if \
            (f.bdx != 0.0) and \
            (f.bdx != f.bmdi) and \
            (len(f.lbcode) != 5) and \
            (f.lbcode[0] == 2):
        dim_coords_and_dims.append((DimCoord.from_regular(f.bzx, f.bdx, f.lbnpt, standard_name=f._x_coord_name(), units='degrees', circular=(f.lbhem in [0, 4]), coord_system=f.coord_system(), with_bounds=True), 1))

    if \
            (f.bdy != 0.0) and \
            (f.bdy != f.bmdi) and \
            (len(f.lbcode) != 5) and \
            (f.lbcode[0] == 1):
        dim_coords_and_dims.append((DimCoord.from_regular(f.bzy, f.bdy, f.lbrow, standard_name=f._y_coord_name(), units='degrees', coord_system=f.coord_system()), 0))

    if \
            (f.bdy != 0.0) and \
            (f.bdy != f.bmdi) and \
            (len(f.lbcode) != 5) and \
            (f.lbcode[0] == 2):
        dim_coords_and_dims.append((DimCoord.from_regular(f.bzy, f.bdy, f.lbrow, standard_name=f._y_coord_name(), units='degrees', coord_system=f.coord_system(), with_bounds=True), 0))

    if \
            (f.bdy == 0.0 or f.bdy == f.bmdi) and \
            (len(f.lbcode) != 5 or (len(f.lbcode) == 5 and f.lbcode.iy == 10)):
        dim_coords_and_dims.append((DimCoord(f.y, standard_name=f._y_coord_name(), units='degrees', bounds=f.y_bounds, coord_system=f.coord_system()), 0))

    if \
            (f.bdx == 0.0 or f.bdx == f.bmdi) and \
            (len(f.lbcode) != 5 or (len(f.lbcode) == 5 and f.lbcode.ix == 11)):
        dim_coords_and_dims.append((DimCoord(f.x, standard_name=f._x_coord_name(),  units='degrees', bounds=f.x_bounds, circular=(f.lbhem in [0, 4]), coord_system=f.coord_system()), 1))

    if \
            (len(f.lbcode) == 5) and \
            (f.lbcode.iy == 2) and \
            (f.bdy == 0 or f.bdy == f.bmdi):
        dim_coords_and_dims.append((DimCoord(f.y, standard_name='height', units='km', bounds=f.y_bounds, attributes={'positive': 'up'}), 0))

    if \
            (len(f.lbcode) == 5) and \
            (f.lbcode[-1] == 1) and \
            (f.lbcode.iy == 4):
        dim_coords_and_dims.append((DimCoord(f.y, standard_name='depth', units='m', bounds=f.y_bounds, attributes={'positive': 'down'}), 0))

    if \
            (len(f.lbcode) == 5) and \
            (f.lbcode.ix == 10) and \
            (f.bdx != 0) and \
            (f.bdx != f.bmdi):
        dim_coords_and_dims.append((DimCoord.from_regular(f.bzx, f.bdx, f.lbnpt, standard_name=f._y_coord_name(), units='degrees', coord_system=f.coord_system()), 1))

    if \
            (len(f.lbcode) == 5) and \
            (f.lbcode.iy == 1) and \
            (f.bdy == 0 or f.bdy == f.bmdi):
        dim_coords_and_dims.append((DimCoord(f.y, long_name='pressure', units='hPa', bounds=f.y_bounds), 0))

    if \
            (len(f.lbcode) == 5) and \
            (f.lbcode.ix == 1) and \
            (f.bdx == 0 or f.bdx == f.bmdi):
        dim_coords_and_dims.append((DimCoord(f.x, long_name='pressure', units='hPa', bounds=f.x_bounds), 1))

    if \
            (len(f.lbcode) == 5) and \
            (f.lbcode[-1] == 1) and \
            (f.lbcode.iy == 23):
        dim_coords_and_dims.append((DimCoord(f.y, standard_name='time', units=iris.unit.Unit('days since 0000-01-01 00:00:00', calendar=iris.unit.CALENDAR_360_DAY), bounds=f.y_bounds), 0))

    if \
            (len(f.lbcode) == 5) and \
            (f.lbcode[-1] == 1) and \
            (f.lbcode.ix == 23):
        dim_coords_and_dims.append((DimCoord(f.x, standard_name='time', units=iris.unit.Unit('days since 0000-01-01 00:00:00', calendar=iris.unit.CALENDAR_360_DAY), bounds=f.x_bounds), 1))

    if \
            (len(f.lbcode) == 5) and \
            (f.lbcode[-1] == 1) and \
            (f.lbcode.ix == 13) and \
            (f.bdx != 0):
        dim_coords_and_dims.append((DimCoord.from_regular(f.bzx, f.bdx, f.lbnpt, long_name='site_number', units='1'), 1))

    if \
            (len(f.lbcode) == 5) and \
            (13 in [f.lbcode.ix, f.lbcode.iy]) and \
            (11 not in [f.lbcode.ix, f.lbcode.iy]) and \
            (hasattr(f, 'lower_x_domain')) and \
            (hasattr(f, 'upper_x_domain')) and \
            (all(f.lower_x_domain != -1.e+30)) and \
            (all(f.upper_x_domain != -1.e+30)):
        aux_coords_and_dims.append((AuxCoord((f.lower_x_domain + f.upper_x_domain) / 2.0, standard_name=f._x_coord_name(), units='degrees', bounds=np.array([f.lower_x_domain, f.upper_x_domain]).T, coord_system=f.coord_system()), 1 if f.lbcode.ix == 13 else 0))

    if \
            (len(f.lbcode) == 5) and \
            (13 in [f.lbcode.ix, f.lbcode.iy]) and \
            (10 not in [f.lbcode.ix, f.lbcode.iy]) and \
            (hasattr(f, 'lower_y_domain')) and \
            (hasattr(f, 'upper_y_domain')) and \
            (all(f.lower_y_domain != -1.e+30)) and \
            (all(f.upper_y_domain != -1.e+30)):
        aux_coords_and_dims.append((AuxCoord((f.lower_y_domain + f.upper_y_domain) / 2.0, standard_name=f._y_coord_name(), units='degrees', bounds=np.array([f.lower_y_domain, f.upper_y_domain]).T, coord_system=f.coord_system()), 1 if f.lbcode.ix == 13 else 0))

    if \
            (f.lbproc == 128) and \
            (f.lbtim.ib == 2) and \
            (f.lbtim.ia == 0):
        cell_methods.append(CellMethod("mean", coords="time"))

    if \
            (f.lbproc == 128) and \
            (f.lbtim.ib == 2) and \
            (f.lbtim.ia != 0):
        cell_methods.append(CellMethod("mean", coords="time", intervals="%d hour" % f.lbtim.ia))

    if \
            (f.lbproc == 128) and \
            (f.lbtim.ib == 3):
        cell_methods.append(CellMethod("mean", coords="time"))

    if \
            (f.lbproc == 128) and \
            (f.lbtim.ib not in [2, 3]):
        cell_methods.append(CellMethod("mean", coords="time"))

    if \
            (f.lbproc == 4096) and \
            (f.lbtim.ib == 2) and \
            (f.lbtim.ia == 0):
        cell_methods.append(CellMethod("minimum", coords="time"))

    if \
            (f.lbproc == 4096) and \
            (f.lbtim.ib == 2) and \
            (f.lbtim.ia != 0):
        cell_methods.append(CellMethod("minimum", coords="time", intervals="%d hour" % f.lbtim.ia))

    if \
            (f.lbproc == 4096) and \
            (f.lbtim.ib != 2):
        cell_methods.append(CellMethod("minimum", coords="time"))

    if \
            (f.lbproc == 8192) and \
            (f.lbtim.ib == 2) and \
            (f.lbtim.ia == 0):
        cell_methods.append(CellMethod("maximum", coords="time"))

    if \
            (f.lbproc == 8192) and \
            (f.lbtim.ib == 2) and \
            (f.lbtim.ia != 0):
        cell_methods.append(CellMethod("maximum", coords="time", intervals="%d hour" % f.lbtim.ia))

    if \
            (f.lbproc == 8192) and \
            (f.lbtim.ib != 2):
        cell_methods.append(CellMethod("maximum", coords="time"))

    if f.lbproc not in [0, 128, 4096, 8192]:
        attributes["ukmo__process_flags"] = tuple(sorted([iris.fileformats.pp.lbproc_map[flag] for flag in f.lbproc.flags]))

    if \
            (f.lbvc == 1) and \
            (not (str(f.stash) in ['m01s03i236', 'm01s03i237', 'm01s03i245', 'm01s03i247', 'm01s03i250'])) and \
            (f.blev != -1):
        aux_coords_and_dims.append((DimCoord(f.blev, standard_name='height', units='m', attributes={'positive': 'up'}), None))

    if str(f.stash) in ['m01s03i236', 'm01s03i237', 'm01s03i245', 'm01s03i247', 'm01s03i250']:
        aux_coords_and_dims.append((DimCoord(1.5, standard_name='height', units='m', attributes={'positive': 'up'}), None))

    if \
            (len(f.lbcode) != 5) and \
            (f.lbvc == 2):
        aux_coords_and_dims.append((DimCoord(_model_level_number(f), standard_name='model_level_number', attributes={'positive': 'down'}), None))

    if \
            (len(f.lbcode) != 5) and \
            (f.lbvc == 2) and \
            (f.brsvd[0] == f.brlev):
        aux_coords_and_dims.append((DimCoord(f.blev, standard_name='depth', units='m', attributes={'positive': 'down'}), None))

    if \
            (len(f.lbcode) != 5) and \
            (f.lbvc == 2) and \
            (f.brsvd[0] != f.brlev):
        aux_coords_and_dims.append((DimCoord(f.blev, standard_name='depth', units='m', bounds=[f.brsvd[0], f.brlev], attributes={'positive': 'down'}), None))

    # soil level
    if len(f.lbcode) != 5 and f.lbvc == 6:
        aux_coords_and_dims.append((DimCoord(_model_level_number(f), long_name='soil_model_level_number', attributes={'positive': 'down'}), None))

    if \
            (f.lbvc == 8) and \
            (len(f.lbcode) != 5 or (len(f.lbcode) == 5 and 1 not in [f.lbcode.ix, f.lbcode.iy])):
        aux_coords_and_dims.append((DimCoord(f.blev, long_name='pressure', units='hPa'), None))

    if \
            (len(f.lbcode) != 5) and \
            (f.lbvc == 19):
        aux_coords_and_dims.append((DimCoord(f.blev, standard_name='air_potential_temperature', units='K', attributes={'positive': 'up'}), None))

    # Hybrid pressure coordinate
    if f.lbvc == 9:
        model_level_number = DimCoord(_model_level_number(f),
                                      standard_name='model_level_number',
                                      attributes={'positive': 'up'})
        # The following match the hybrid height scheme, but data has the
        # blev and bhlev values the other way around.
        #level_pressure = DimCoord(f.blev,
        #                          long_name='level_pressure',
        #                          units='Pa',
        #                          bounds=[f.brlev, f.brsvd[0]])
        #sigma = AuxCoord(f.bhlev,
        #                 long_name='sigma',
        #                 bounds=[f.bhrlev, f.brsvd[1]])
        level_pressure = DimCoord(f.bhlev,
                                  long_name='level_pressure',
                                  units='Pa',
                                  bounds=[f.bhrlev, f.brsvd[1]])
        sigma = AuxCoord(f.blev,
                         long_name='sigma',
                         bounds=[f.brlev, f.brsvd[0]])
        aux_coords_and_dims.extend([(model_level_number, None),
                                    (level_pressure, None),
                                    (sigma, None)])
        factories.append(Factory(HybridPressureFactory,
                                 [{'long_name': 'level_pressure'},
                                  {'long_name': 'sigma'},
                                  Reference('surface_air_pressure')]))

    if f.lbvc == 65:
        aux_coords_and_dims.append((DimCoord(_model_level_number(f), standard_name='model_level_number', attributes={'positive': 'up'}), None))
        aux_coords_and_dims.append((DimCoord(f.blev, long_name='level_height', units='m', bounds=[f.brlev, f.brsvd[0]], attributes={'positive': 'up'}), None))
        aux_coords_and_dims.append((AuxCoord(f.bhlev, long_name='sigma', bounds=[f.bhrlev, f.brsvd[1]]), None))
        factories.append(Factory(HybridHeightFactory, [{'long_name': 'level_height'}, {'long_name': 'sigma'}, Reference('orography')]))

    if f.lbrsvd[3] != 0:
        aux_coords_and_dims.append((DimCoord(f.lbrsvd[3], standard_name='realization'), None))

    if f.lbuser[4] != 0:
        aux_coords_and_dims.append((DimCoord(f.lbuser[4], long_name='pseudo_level', units='1'), None))

    if f.lbuser[6] == 1 and f.lbuser[3] == 5226:
        standard_name = "precipitation_amount"
        units = "kg m-2"

    if \
            (f.lbuser[6] == 2) and \
            (f.lbuser[3] == 101):
        standard_name = "sea_water_potential_temperature"
        units = "Celsius"

    if \
            ((f.lbsrce % 10000) == 1111) and \
            ((f.lbsrce / 10000) / 100.0 > 0):
        attributes['source'] = 'Data from Met Office Unified Model %4.2f' % ((f.lbsrce / 10000) / 100.0)

    if \
            ((f.lbsrce % 10000) == 1111) and \
            ((f.lbsrce / 10000) / 100.0 == 0):
        attributes['source'] = 'Data from Met Office Unified Model'

    if f.lbuser[6] != 0 or (f.lbuser[3] / 1000) != 0 or (f.lbuser[3] % 1000) != 0:
        attributes['STASH'] = f.stash

    if \
            (f.lbuser[6] == 1) and \
            (f.lbuser[3] == 4205):
        standard_name = "mass_fraction_of_cloud_ice_in_air"
        units = "1"

    if \
            (f.lbuser[6] == 1) and \
            (f.lbuser[3] == 4206):
        standard_name = "mass_fraction_of_cloud_liquid_water_in_air"
        units = "1"

    if \
            (f.lbuser[6] == 1) and \
            (f.lbuser[3] == 30204):
        standard_name = "air_temperature"
        units = "K"

    if \
            (f.lbuser[6] == 4) and \
            (f.lbuser[3] == 6001):
        standard_name = "sea_surface_wave_significant_height"
        units = "m"

    if str(f.stash) in STASH_TO_CF:
        standard_name = STASH_TO_CF[str(f.stash)].standard_name
        units = STASH_TO_CF[str(f.stash)].units
        long_name = STASH_TO_CF[str(f.stash)].long_name

    if \
            (not f.stash.is_valid) and \
            (f.lbfc in LBFC_TO_CF):
        standard_name = LBFC_TO_CF[f.lbfc].standard_name
        units = LBFC_TO_CF[f.lbfc].units
        long_name = LBFC_TO_CF[f.lbfc].long_name

    if f.lbuser[3] == 33:
        references.append(ReferenceTarget('orography', None))

    if f.lbuser[3] == 409 or f.lbuser[3] == 1:
        references.append(ReferenceTarget('surface_air_pressure', None))

    return (factories, references, standard_name, long_name, units, attributes,
            cell_methods, dim_coords_and_dims, aux_coords_and_dims)

########NEW FILE########
__FILENAME__ = rules
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Processing of simple IF-THEN rules.

"""

import abc
import collections
import getpass
import logging
import logging.handlers as handlers
import operator
import os
import os.path
import platform
import sys
import types
import warnings

import numpy as np
import numpy.ma as ma

import iris.config as config
import iris.cube
import iris.exceptions
import iris.fileformats.um_cf_map
import iris.unit
from iris.util import is_regular, regular_step

RuleResult = collections.namedtuple('RuleResult', ['cube', 'matching_rules', 'factories'])
Factory = collections.namedtuple('Factory', ['factory_class', 'args'])
ReferenceTarget = collections.namedtuple('ReferenceTarget',
                                         ('name', 'transform'))


class ConcreteReferenceTarget(object):
    """Everything you need to make a real Cube for a named reference."""

    def __init__(self, name, transform=None):
        #: The name used to connect references with referencees.
        self.name = name
        #: An optional transformation to apply to the cubes.
        self.transform = transform
        self._src_cubes = iris.cube.CubeList()
        self._final_cube = None

    def add_cube(self, cube):
        self._src_cubes.append(cube)

    def as_cube(self):
        if self._final_cube is None:
            src_cubes = self._src_cubes
            if len(src_cubes) > 1:
                # Merge the reference cubes to allow for
                # time-varying surface pressure in hybrid-presure.
                src_cubes = src_cubes.merge(unique=False)
                if len(src_cubes) > 1:
                    warnings.warn('Multiple reference cubes for {}'
                                  .format(self.name))
            src_cube = src_cubes[-1]

            if self.transform is None:
                self._final_cube = src_cube
            else:
                final_cube = src_cube.copy()
                attributes = self.transform(final_cube)
                for name, value in attributes.iteritems():
                    setattr(final_cube, name, value)
                self._final_cube = final_cube

        return self._final_cube


# Controls the deferred import of all the symbols from iris.coords.
# This import all is used as the rules file does not use fully qualified class names.
_import_pending = True


# Dummy logging routine for when we don't want to do any logging.
def _dummy_log(format, filename, rules):
    pass


# Genuine logging routine
def _real_log(format, filename, rules):
    # Replace "\" with "\\", and "," with "\,"
    filename = filename.replace('\\', '\\\\').replace(',', '\\,')
    _rule_logger.info("%s,%s,%s" % (format, filename, ','.join([rule.id for rule in rules])))


# Debug logging routine (more informative that just object ids)
def _verbose_log(format, filename, rules):
    # Replace "\" with "\\", and "," with "\,"
    filename = filename.replace('\\', '\\\\').replace(',', '\\,')
    _rule_logger.info("\n\n-----\n\n%s,%s,%s" % (format, filename, '\n\n'.join([str(rule) for rule in rules])))


# Prepares a logger for file-based logging of rule usage
def _prepare_rule_logger(verbose=False):
    # Default to the dummy logger that does nothing
    logger = _dummy_log

    # Only do real logging if we've been told the directory to use ...
    log_dir = config.RULE_LOG_DIR
    if log_dir is not None:
        user = getpass.getuser()

        # .. and if we haven't been told to ignore the current invocation.
        ignore = False
        ignore_users = config.RULE_LOG_IGNORE
        if ignore_users is not None:
            ignore_users = ignore_users.split(',')
            ignore = user in ignore_users

        if not ignore:
            try:
                hostname = platform.node() or 'UNKNOWN'
                log_path = os.path.join(log_dir, '_'.join([hostname, user]))
                file_handler = handlers.RotatingFileHandler(log_path, maxBytes=1e7, backupCount=5)
                format = '%%(asctime)s,%s,%%(message)s' % getpass.getuser()
                file_handler.setFormatter(logging.Formatter(format, '%Y-%m-%d %H:%M:%S'))

                global _rule_logger
                _rule_logger = logging.getLogger('iris.fileformats.rules')
                _rule_logger.setLevel(logging.INFO)
                _rule_logger.addHandler(file_handler)
                _rule_logger.propagate = False

                if verbose:
                    logger = _verbose_log
                else:
                    logger = _real_log

            except IOError:
                # If we can't create the log file for some reason then it's fine to just silently
                # ignore the error and fallback to using the dummy logging routine.
                pass

    return logger


# Defines the "log" function for this module
log = _prepare_rule_logger()


class DebugString(str):
    """
    Used by the rules for debug purposes

    """


class CMAttribute(object):
    """
    Used by the rules for defining attributes on the Cube in a consistent manner.

    """
    __slots__ = ('name', 'value')
    def __init__(self, name, value):
        self.name = name
        self.value = value


class CMCustomAttribute(object):
    """
    Used by the rules for defining custom attributes on the Cube in a consistent manner.

    """
    __slots__ = ('name', 'value')
    def __init__(self, name, value):
        self.name = name
        self.value = value


class CoordAndDims(object):
    """
    Used within rules to represent a mapping of coordinate to data dimensions.

    """
    def __init__(self, coord, dims=None):
        self.coord = coord
        if dims is None:
            dims = []
        if not isinstance(dims, list):
            dims = [dims]
        self.dims = dims

    def add_coord(self, cube):
        added = False

        # Try to add to dim_coords?
        if isinstance(self.coord, iris.coords.DimCoord) and self.dims:
            if len(self.dims) > 1:
                raise Exception("Only 1 dim allowed for a DimCoord")

            # Does the cube already have a coord for this dim?
            already_taken = False
            for coord, coord_dim in cube._dim_coords_and_dims:
                if coord_dim == self.dims[0]:
                    already_taken = True
                    break

            if not already_taken:
                cube.add_dim_coord(self.coord, self.dims[0])
                added = True

        # If we didn't add it to dim_coords, add it to aux_coords.
        if not added:
            cube.add_aux_coord(self.coord, self.dims)

    def __repr__(self):
        return "<CoordAndDims: %r, %r>" % (self.coord.name, self.dims)


class Reference(iris.util._OrderedHashable):
    _names = ('name',)
    """
    A named placeholder for inter-field references.

    """


def calculate_forecast_period(time, forecast_reference_time):
    """
    Return the forecast period in hours derived from time and
    forecast_reference_time scalar coordinates.

    """
    if time.points.size != 1:
        raise ValueError('Expected a time coordinate with a single '
                         'point. {!r} has {} points.'.format(time.name(),
                                                             time.points.size))

    if not time.has_bounds():
        raise ValueError('Expected a time coordinate with bounds.')

    if forecast_reference_time.points.size != 1:
        raise ValueError('Expected a forecast_reference_time coordinate '
                         'with a single point. {!r} has {} '
                         'points.'.format(forecast_reference_time.name(),
                                          forecast_reference_time.points.size))

    origin = time.units.origin.replace(time.units.origin.split()[0], 'hours')
    units = iris.unit.Unit(origin, calendar=time.units.calendar)

    # Determine start and eof of period in hours since a common epoch.
    end = time.units.convert(time.bounds[0, 1], units)
    start = forecast_reference_time.units.convert(
        forecast_reference_time.points[0], units)
    forecast_period = end - start

    return forecast_period


class Rule(object):
    """
    A collection of condition expressions and their associated action expressions.

    Example rule::

        IF
            f.lbuser[6] == 2
            f.lbuser[3] == 101
        THEN
            CMAttribute('standard_name', 'sea_water_potential_temperature')
            CMAttribute('units', 'Celsius')

    """
    def __init__(self, conditions, actions):
        """Create instance methods from our conditions and actions."""
        if not hasattr(conditions, '__iter__'):
            raise TypeError('Variable conditions should be iterable, got: '+ type(conditions))
        if not hasattr(actions, '__iter__'):
            raise TypeError('Variable actions should be iterable, got: '+ type(actions))

        self._conditions = conditions
        self._actions = actions
        self._exec_actions = []

        self.id = str(hash((tuple(self._conditions), tuple(self._actions))))

        for i, condition in enumerate(conditions):
            self._conditions[i] = condition

        # Create the conditions method.
        self._create_conditions_method()

        # Create the action methods.
        for i, action in enumerate(self._actions):
            if not action:
                action = 'None'
            self._create_action_method(i, action)

    def _create_conditions_method(self):
        # Bundle all the conditions into one big string.
        conditions = '(%s)' % ') and ('.join(self._conditions)
        if not conditions:
            conditions = 'None'
        # Create a method to evaluate the conditions.
        # NB. This creates the name '_exec_conditions' in the local
        # namespace, which is then used below.
        code = 'def _exec_conditions(self, field, f, pp, grib, cm): return %s'
        exec compile(code % conditions, '<string>', 'exec')
        # Make it a method of ours.
        self._exec_conditions = types.MethodType(_exec_conditions, self, type(self))

    @abc.abstractmethod
    def _create_action_method(self, i, action):
        pass

    @abc.abstractmethod
    def _process_action_result(self, obj, cube):
        pass

    def __repr__(self):
        string = "IF\n"
        string += '\n'.join(self._conditions)
        string += "\nTHEN\n"
        string += '\n'.join(self._actions)
        return string

    def evaluates_true(self, cube, field):
        """Returns True if and only if all the conditions evaluate to True for the given field."""
        field = field
        f = field
        pp = field
        grib = field
        cm = cube

        try:
            result = self._exec_conditions(field, f, pp, grib, cm)
        except Exception, err:
            print >> sys.stderr, 'Condition failed to run conditions: %s : %s' % (self._conditions, err)
            raise err

        return result

    def _matches_field(self, field):
        """Simple wrapper onto evaluates_true in the case where cube is None."""
        return self.evaluates_true(None, field)

    def run_actions(self, cube, field):
        """
        Adds to the given cube based on the return values of all the actions.

        """
        # Deferred import of all the symbols from iris.coords.
        # This import all is used as the rules file does not use fully qualified class names.
        global _import_pending
        if _import_pending:
            globals().update(iris.aux_factory.__dict__)
            globals().update(iris.coords.__dict__)
            globals().update(iris.coord_systems.__dict__)
            globals().update(iris.fileformats.um_cf_map.__dict__)
            globals().update(iris.unit.__dict__)
            _import_pending = False

        # Define the variables which the eval command should be able to see
        f = field
        pp = field
        grib = field
        cm = cube

        factories = []
        for i, action in enumerate(self._actions):
            try:
                # Run this action.
                obj = self._exec_actions[i](field, f, pp, grib, cm)
                # Process the return value (if any), e.g a CM object or None.
                action_factory = self._process_action_result(obj, cube)
                if action_factory:
                    factories.append(action_factory)

            except iris.exceptions.CoordinateNotFoundError, err:
                print >> sys.stderr, 'Failed (msg:%(error)s) to find coordinate, perhaps consider running last: %(command)s' % {'command':action, 'error': err}
            except AttributeError, err:
                print >> sys.stderr, 'Failed to get value (%(error)s) to execute: %(command)s' % {'command':action, 'error': err}
            except Exception, err:
                print >> sys.stderr, 'Failed (msg:%(error)s) to run:\n    %(command)s\nFrom the rule:\n%(me)r' % {'me':self, 'command':action, 'error': err}
                raise err

        return factories


class FunctionRule(Rule):
    """A Rule with values returned by its actions."""
    def _create_action_method(self, i, action):
        # CM loading style action. Returns an object, such as a coord.
        exec compile('def _exec_action_%d(self, field, f, pp, grib, cm): return %s' % (i, action), '<string>', 'exec')
        # Make it a method of ours.
        exec 'self._exec_action_%d = types.MethodType(_exec_action_%d, self, type(self))' % (i, i)
        # Add to our list of actions.
        exec 'self._exec_actions.append(self._exec_action_%d)' % i

    def _process_action_result(self, obj, cube):
        """Process the result of an action."""

        factory = None

        # NB. The names such as 'CoordAndDims' and 'CellMethod' are defined by
        # the "deferred import" performed by Rule.run_actions() above.
        if isinstance(obj, CoordAndDims):
            obj.add_coord(cube)

        #cell methods - not yet implemented
        elif isinstance(obj, CellMethod):
            cube.add_cell_method(obj)

        elif isinstance(obj, CMAttribute):
            # Temporary code to deal with invalid standard names from the translation table.
            # TODO: when name is "standard_name" force the value to be a real standard name
            if obj.name == 'standard_name' and obj.value is not None:
                cube.rename(obj.value)
            elif obj.name == 'units':
                # Graceful loading of units.
                try:
                    setattr(cube, obj.name, obj.value)
                except ValueError:
                    msg = 'Ignoring PP invalid units {!r}'.format(obj.value)
                    warnings.warn(msg)
                    cube.attributes['invalid_units'] = obj.value
                    cube.units = iris.unit._UNKNOWN_UNIT_STRING
            else:
                setattr(cube, obj.name, obj.value)

        elif isinstance(obj, CMCustomAttribute):
            cube.attributes[obj.name] = obj.value

        elif isinstance(obj, Factory):
            factory = obj

        elif isinstance(obj, DebugString):
            print obj

        # The function returned nothing, like the pp save actions, "lbft = 3"
        elif obj is None:
            pass

        else:
            raise Exception("Object could not be added to cube. Unknown type: " + obj.__class__.__name__)

        return factory


class ProcedureRule(Rule):
    """A Rule with nothing returned by its actions."""
    def _create_action_method(self, i, action):
        # PP saving style action. No return value, e.g. "pp.lbft = 3".
        exec compile('def _exec_action_%d(self, field, f, pp, grib, cm): %s' % (i, action), '<string>', 'exec')
        # Make it a method of ours.
        exec 'self._exec_action_%d = types.MethodType(_exec_action_%d, self, type(self))' % (i, i)
        # Add to our list of actions.
        exec 'self._exec_actions.append(self._exec_action_%d)' % i

    def _process_action_result(self, obj, cube):
        # This should always be None, as our rules won't create anything.
        pass

    def conditional_warning(self, condition, warning):
        pass  # without this pass statement it alsp print, "  Args:" on a new line.
        if condition:
            warnings.warn(warning)


class RulesContainer(object):
    """
    A collection of :class:`Rule` instances, with the ability to read rule
    definitions from files and run the rules against given fields.

    """
    def __init__(self, filepath=None, rule_type=FunctionRule):
        """Create a new rule set, optionally adding rules from the specified file.

        The rule_type defaults to :class:`FunctionRule`,
        e.g for CM loading actions that return objects, such as *AuxCoord(...)*

        rule_type can also be set to :class:`ProcedureRule`
        e.g for PP saving actions that do not return anything, such as *pp.lbuser[3] = 16203*
        """
        self._rules = []
        self.rule_type = rule_type
        if filepath is not None:
            self.import_rules(filepath)

    def import_rules(self, filepath):
        """Extend the rule collection with the rules defined in the specified file."""
        # Define state constants
        IN_CONDITION = 1
        IN_ACTION = 2

        rule_file = os.path.expanduser(filepath)
        file = open(rule_file, 'r')

        conditions = []
        actions = []
        state = None
        for line in file:
            line = line.rstrip()
            if line == "IF":
                if conditions and actions:
                    self._rules.append(self.rule_type(conditions, actions))
                conditions = []
                actions = []
                state = IN_CONDITION
            elif line == "THEN":
                state = IN_ACTION
            elif len(line) == 0:
                pass
            elif line.strip().startswith('#'):
                pass
            elif state == IN_CONDITION:
                conditions.append(line)
            elif state == IN_ACTION:
                actions.append(line)
            else:
                raise Exception('Rule file not read correctly at line: ' + line)
        if conditions and actions:
            self._rules.append(self.rule_type(conditions, actions))
        file.close()

    def verify(self, cube, field):
        """
        Add to the given :class:`iris.cube.Cube` by running this set of
        rules with the given field.

        Args:

        * cube:
            An instance of :class:`iris.cube.Cube`.
        * field:
            A field object relevant to the rule set.

        Returns: (cube, matching_rules)

        * cube - the resultant cube
        * matching_rules - a list of rules which matched

        """
        matching_rules = []
        factories = []
        for rule in self._rules:
            if rule.evaluates_true(cube, field):
                matching_rules.append(rule)
                rule_factories = rule.run_actions(cube, field)
                if rule_factories:
                    factories.extend(rule_factories)
        return RuleResult(cube, matching_rules, factories)


def scalar_coord(cube, coord_name):
    """Try to find a single-valued coord with the given name."""
    found_coord = None
    for coord in cube.coords(coord_name):
        if coord.shape == (1,):
            found_coord = coord
            break
    return found_coord


def vector_coord(cube, coord_name):
    """Try to find a one-dimensional, multi-valued coord with the given name."""
    found_coord = None
    for coord in cube.coords(coord_name):
        if len(coord.shape) == 1 and coord.shape[0] > 1:
            found_coord = coord
            break
    return found_coord


def scalar_cell_method(cube, method, coord_name):
    """Try to find the given type of cell method over a single coord with the given name."""
    found_cell_method = None
    for cell_method in cube.cell_methods:
        if cell_method.method == method and len(cell_method.coord_names) == 1:
            name = cell_method.coord_names[0]
            coords = cube.coords(name)
            if len(coords) == 1:
                found_cell_method = cell_method
    return found_cell_method


def has_aux_factory(cube, aux_factory_class):
    """
    Try to find an class:`~iris.aux_factory.AuxCoordFactory` instance of the
    specified type on the cube.

    """
    for factory in cube.aux_factories:
        if isinstance(factory, aux_factory_class):
            return True
    return False


def aux_factory(cube, aux_factory_class):
    """
    Return the class:`~iris.aux_factory.AuxCoordFactory` instance of the
    specified type from a cube.

    """
    aux_factories = [aux_factory for aux_factory in cube.aux_factories if
                     isinstance(aux_factory, aux_factory_class)]
    if not aux_factories:
        raise ValueError('Cube does not have an aux factory of '
                         'type {!r}.'.format(aux_factory_class))
    elif len(aux_factories) > 1:
        raise ValueError('Cube has more than one aux factory of '
                         'type {!r}.'.format(aux_factory_class))
    return aux_factories[0]


class _ReferenceError(Exception):
    """Signals an invalid/missing reference field."""
    pass


def _dereference_args(factory, reference_targets, regrid_cache, cube):
    """Converts all the arguments for a factory into concrete coordinates."""
    args = []
    for arg in factory.args:
        if isinstance(arg, Reference):
            if arg.name in reference_targets:
                src = reference_targets[arg.name].as_cube()
                # If necessary, regrid the reference cube to
                # match the grid of this cube.
                src = _ensure_aligned(regrid_cache, src, cube)
                if src is not None:
                    new_coord = iris.coords.AuxCoord(src.data,
                                                     src.standard_name,
                                                     src.long_name,
                                                     src.var_name,
                                                     src.units,
                                                     attributes=src.attributes)
                    dims = [cube.coord_dims(src_coord)[0]
                                for src_coord in src.dim_coords]
                    cube.add_aux_coord(new_coord, dims)
                    args.append(new_coord)
                else:
                    raise _ReferenceError('Unable to regrid reference for'
                                          ' {!r}'.format(arg.name))
            else:
                raise _ReferenceError("The file(s) {{filenames}} don't contain"
                                      " field(s) for {!r}.".format(arg.name))
        else:
            # If it wasn't a Reference, then arg is a dictionary
            # of keyword arguments for cube.coord(...).
            args.append(cube.coord(**arg))
    return args


def _regrid_to_target(src_cube, target_coords, target_cube):
    # Interpolate onto the target grid.
    sample_points = [(coord, coord.points) for coord in target_coords]
    result_cube = iris.analysis.interpolate.linear(src_cube, sample_points)

    # Any scalar coords on the target_cube will have become vector
    # coords on the resample src_cube (i.e. result_cube).
    # These unwanted vector coords need to be pushed back to scalars.
    index = [slice(None, None)] * result_cube.ndim
    for target_coord in target_coords:
        if not target_cube.coord_dims(target_coord):
            result_dim = result_cube.coord_dims(target_coord)[0]
            index[result_dim] = 0
    if not all(key == slice(None, None) for key in index):
        result_cube = result_cube[tuple(index)]
    return result_cube


def _ensure_aligned(regrid_cache, src_cube, target_cube):
    """
    Returns a version of `src_cube` suitable for use as an AuxCoord
    on `target_cube`, or None if no version can be made.

    """
    result_cube = None

    # Check that each of src_cube's dim_coords matches up with a single
    # coord on target_cube.
    try:
        target_coords = []
        for dim_coord in src_cube.dim_coords:
            target_coords.append(target_cube.coord(dim_coord))
    except iris.exceptions.CoordinateNotFoundError:
        # One of the src_cube's dim_coords didn't exist on the
        # target_cube... so we can't regrid (i.e. just return None).
        pass
    else:
        # So we can use `iris.analysis.interpolate.linear()` later,
        # ensure each target coord is either a scalar or maps to a
        # single, distinct dimension.
        target_dims = [target_cube.coord_dims(coord) for coord in target_coords]
        target_dims = filter(None, target_dims)
        unique_dims = set()
        for dims in target_dims:
            unique_dims.update(dims)
        compatible = len(target_dims) == len(unique_dims)

        if compatible:
            cache_key = id(src_cube)
            if cache_key not in regrid_cache:
                regrid_cache[cache_key] = ([src_cube.dim_coords], [src_cube])
            grids, cubes = regrid_cache[cache_key]
            # 'grids' is a list of tuples of coordinates, so convert
            # the 'target_coords' list into a tuple to be consistent.
            target_coords = tuple(target_coords)
            try:
                # Look for this set of target coordinates in the cache.
                i = grids.index(target_coords)
                result_cube = cubes[i]
            except ValueError:
                # Not already cached, so do the hard work of interpolating.
                result_cube = _regrid_to_target(src_cube, target_coords,
                                                target_cube)
                # Add it to the cache.
                grids.append(target_coords)
                cubes.append(result_cube)

    return result_cube


Loader = collections.namedtuple('Loader',
                                ('field_generator', 'field_generator_kwargs',
                                 'converter', 'legacy_custom_rules'))


def _make_cube(field, converter):
    # Convert the field to a Cube.
    (factories, references, standard_name, long_name, units, attributes,
     cell_methods, dim_coords_and_dims, aux_coords_and_dims) = converter(field)

    try:
        data = field._data
    except AttributeError:
        data = field.data

    cube = iris.cube.Cube(data,
                          attributes=attributes,
                          cell_methods=cell_methods,
                          dim_coords_and_dims=dim_coords_and_dims,
                          aux_coords_and_dims=aux_coords_and_dims)

    # Temporary code to deal with invalid standard names in the
    # translation table.
    if standard_name is not None:
        cube.rename(standard_name)
    if long_name is not None:
        cube.long_name = long_name
    if units is not None:
        # Temporary code to deal with invalid units in the translation
        # table.
        try:
            cube.units = units
        except ValueError:
            msg = 'Ignoring PP invalid units {!r}'.format(units)
            warnings.warn(msg)
            cube.attributes['invalid_units'] = units
            cube.units = iris.unit._UNKNOWN_UNIT_STRING

    return cube, factories, references


def load_cubes(filenames, user_callback, loader):
    concrete_reference_targets = {}
    results_needing_reference = []

    if isinstance(filenames, basestring):
        filenames = [filenames]

    for filename in filenames:
        for field in loader.field_generator(filename, **loader.field_generator_kwargs):
            # Convert the field to a Cube.
            cube, factories, references = _make_cube(field, loader.converter)

            # Run any custom user-provided rules.
            if loader.legacy_custom_rules:
                loader.legacy_custom_rules.verify(cube, field)

            cube = iris.io.run_callback(user_callback, cube, field, filename)

            if cube is None:
                continue
            # Cross referencing
            for reference in references:
                name = reference.name
                # Register this cube as a source cube for the named
                # reference.
                target = concrete_reference_targets.get(name)
                if target is None:
                    target = ConcreteReferenceTarget(name, reference.transform)
                    concrete_reference_targets[name] = target
                target.add_cube(cube)

            if factories:
                results_needing_reference.append((cube, factories))
            else:
                yield cube

    regrid_cache = {}
    for cube, factories in results_needing_reference:
        for factory in factories:
            try:
                args = _dereference_args(factory, concrete_reference_targets,
                                         regrid_cache, cube)
            except _ReferenceError as e:
                msg = 'Unable to create instance of {factory}. ' + e.message
                factory_name = factory.factory_class.__name__
                warnings.warn(msg.format(filenames=filenames,
                                         factory=factory_name))
            else:
                aux_factory = factory.factory_class(*args)
                cube.add_aux_factory(aux_factory)
        yield cube

########NEW FILE########
__FILENAME__ = um_cf_map
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
#
# DO NOT EDIT: AUTO-GENERATED

from collections import namedtuple


CFName = namedtuple('CFName', 'standard_name long_name units')


LBFC_TO_CF = {
    5: CFName('atmosphere_boundary_layer_thickness', None, 'm'),
    23: CFName('soil_temperature', None, 'K'),
    27: CFName('air_density', None, 'kg m-3'),
    36: CFName('land_area_fraction', None, '1'),
    37: CFName('sea_ice_area_fraction', None, '1'),
    50: CFName('wind_speed', None, 'm s-1'),
    73: CFName('atmosphere_relative_vorticity', None, 's-1'),
    74: CFName('divergence_of_wind', None, 's-1'),
    83: CFName('potential_vorticity_of_atmosphere_layer', None, 'Pa-1 s-1'),
    94: CFName('convective_rainfall_amount', None, 'kg m-2'),
    97: CFName('rainfall_flux', None, 'kg m-2 s-1'),
    102: CFName('stratiform_rainfall_amount', None, 'kg m-2'),
    108: CFName('snowfall_flux', None, 'kg m-2 s-1'),
    111: CFName('surface_runoff_amount', None, 'kg m-2'),
    116: CFName('stratiform_snowfall_amount', None, 'kg m-2'),
    117: CFName('convective_snowfall_amount', None, 'kg m-2'),
    122: CFName('moisture_content_of_soil_layer', None, 'kg m-2'),
    183: CFName('wind_speed', None, 'm s-1'),
    200: CFName('toa_incoming_shortwave_flux', None, 'W m-2'),
    203: CFName('surface_downwelling_shortwave_flux_in_air', None, 'W m-2'),
    206: CFName('toa_outgoing_longwave_flux', None, 'W m-2'),
    208: CFName('surface_downwelling_shortwave_flux_in_air_assuming_clear_sky', None, 'W m-2'),
    209: CFName('sea_ice_temperature', None, 'K'),
    253: CFName('tendency_of_air_temperature_due_to_longwave_heating', None, 'K s-1'),
    261: CFName('downward_heat_flux_in_sea_ice', None, 'W m-2'),
    321: CFName('root_depth', None, 'm'),
    326: CFName('vegetation_area_fraction', None, '1'),
    328: CFName('surface_albedo_assuming_deep_snow', None, '1'),
    329: CFName('volume_fraction_of_condensed_water_in_soil_at_wilting_point', None, '1'),
    330: CFName('volume_fraction_of_condensed_water_in_soil_at_critical_point', None, '1'),
    332: CFName('soil_porosity', None, '1'),
    333: CFName('soil_hydraulic_conductivity_at_saturation', None, 'm s-1'),
    335: CFName('soil_thermal_capacity', None, 'J kg-1 K-1'),
    336: CFName('soil_thermal_conductivity', None, 'W m-1 K-1'),
    342: CFName('soil_suction_at_saturation', None, 'Pa'),
    687: CFName('sea_ice_thickness', None, 'm'),
    701: CFName('surface_eastward_sea_water_velocity', None, 'm s-1'),
    702: CFName('surface_northward_sea_water_velocity', None, 'm s-1'),
    1025: CFName('surface_downward_eastward_stress', None, 'Pa'),
    1026: CFName('surface_downward_northward_stress', None, 'Pa'),
    1373: CFName('mass_fraction_of_dimethyl_sulfide_in_air', None, '1'),
    1374: CFName('mass_fraction_of_sulfur_dioxide_in_air', None, '1'),
    1382: CFName('leaf_area_index', None, '1'),
    1383: CFName('canopy_height', None, 'm'),
    1385: CFName('mass_fraction_of_unfrozen_water_in_soil_moisture', None, '1'),
    1386: CFName('mass_fraction_of_frozen_water_in_soil_moisture', None, '1'),
    1392: CFName('leaf_area_index', None, '1'),
    1393: CFName('canopy_height', None, 'm'),
    1395: CFName('soil_albedo', None, '1'),
    1507: CFName('snow_grain_size', None, '1e-6 m'),
    1559: CFName('soil_moisture_content_at_field_capacity', None, 'kg m-2'),
    1720: CFName('cloud_area_fraction_in_atmosphere_layer', None, '1'),
    }

STASH_TO_CF = {
    'm01s00i001': CFName('surface_air_pressure', None, 'Pa'),
    'm01s00i002': CFName('x_wind', None, 'm s-1'),
    'm01s00i003': CFName('y_wind', None, 'm s-1'),
    'm01s00i004': CFName('air_potential_temperature', None, 'K'),
    'm01s00i009': CFName('moisture_content_of_soil_layer', None, 'kg m-2'),
    'm01s00i010': CFName('specific_humidity', None, '1'),
    'm01s00i012': CFName('mass_fraction_of_cloud_ice_in_air', None, '1'),
    'm01s00i013': CFName('convective_cloud_area_fraction', None, '1'),
    'm01s00i020': CFName('soil_temperature', None, 'K'),
    'm01s00i023': CFName('snowfall_amount', None, 'kg m-2'),
    'm01s00i024': CFName('surface_temperature', None, 'K'),
    'm01s00i025': CFName('atmosphere_boundary_layer_thickness', None, 'm'),
    'm01s00i026': CFName('surface_roughness_length', None, 'm'),
    'm01s00i028': CFName('surface_eastward_sea_water_velocity', None, 'm s-1'),
    'm01s00i029': CFName('surface_northward_sea_water_velocity', None, 'm s-1'),
    'm01s00i030': CFName('land_binary_mask', None, '1'),
    'm01s00i031': CFName('sea_ice_area_fraction', None, '1'),
    'm01s00i032': CFName('sea_ice_thickness', None, 'm'),
    'm01s00i033': CFName('surface_altitude', None, 'm'),
    'm01s00i040': CFName('volume_fraction_of_condensed_water_in_soil_at_wilting_point', None, '1'),
    'm01s00i041': CFName('volume_fraction_of_condensed_water_in_soil_at_critical_point', None, '1'),
    'm01s00i043': CFName('soil_porosity', None, '1'),
    'm01s00i044': CFName('soil_hydraulic_conductivity_at_saturation', None, 'm s-1'),
    'm01s00i046': CFName('soil_thermal_capacity', None, 'J kg-1 K-1'),
    'm01s00i047': CFName('soil_thermal_conductivity', None, 'W m-1 K-1'),
    'm01s00i048': CFName('soil_suction_at_saturation', None, 'Pa'),
    'm01s00i049': CFName('sea_ice_temperature', None, 'K'),
    'm01s00i050': CFName('vegetation_area_fraction', None, '1'),
    'm01s00i051': CFName('root_depth', None, 'm'),
    'm01s00i052': CFName('surface_albedo_assuming_no_snow', None, '1'),
    'm01s00i053': CFName('surface_albedo_assuming_deep_snow', None, '1'),
    'm01s00i060': CFName('mass_fraction_of_ozone_in_air', None, '1'),
    'm01s00i101': CFName('mass_fraction_of_sulfur_dioxide_in_air', None, '1'),
    'm01s00i102': CFName('mass_fraction_of_dimethyl_sulfide_in_air', None, '1'),
    'm01s00i150': CFName('upward_air_velocity', None, 'm s-1'),
    'm01s00i205': CFName('land_area_fraction', None, '1'),
    'm01s00i208': CFName('leaf_area_index', None, '1'),
    'm01s00i209': CFName('canopy_height', None, 'm'),
    'm01s00i214': CFName('mass_fraction_of_unfrozen_water_in_soil_moisture', None, '1'),
    'm01s00i215': CFName('mass_fraction_of_frozen_water_in_soil_moisture', None, '1'),
    'm01s00i217': CFName('leaf_area_index', None, '1'),
    'm01s00i218': CFName('canopy_height', None, 'm'),
    'm01s00i220': CFName('soil_albedo', None, '1'),
    'm01s00i223': CFName('soil_carbon_content', None, 'kg m-2'),
    'm01s00i231': CFName('snow_grain_size', None, '1e-6 m'),
    'm01s00i252': CFName('mass_fraction_of_carbon_dioxide_in_air', None, '1'),
    'm01s00i254': CFName('mass_fraction_of_cloud_liquid_water_in_air', None, '1'),
    'm01s00i255': CFName('dimensionless_exner_function', None, '1'),
    'm01s00i269': CFName('surface_eastward_sea_water_velocity', None, 'm s-1'),
    'm01s00i270': CFName('surface_northward_sea_water_velocity', None, 'm s-1'),
    'm01s00i406': CFName('dimensionless_exner_function', None, '1'),
    'm01s00i407': CFName('air_pressure', None, 'Pa'),
    'm01s00i408': CFName('air_pressure', None, 'Pa'),
    'm01s00i409': CFName('surface_air_pressure', None, 'Pa'),
    'm01s00i505': CFName('land_area_fraction', None, '1'),
    'm01s00i506': CFName('surface_temperature', None, 'K'),
    'm01s00i507': CFName('surface_temperature', None, 'K'),
    'm01s00i508': CFName('surface_temperature', None, 'K'),
    'm01s00i509': CFName('sea_ice_albedo', None, '1'),
    'm01s01i004': CFName('air_temperature', None, 'K'),
    'm01s01i201': CFName('surface_net_downward_shortwave_flux', None, 'W m-2'),
    'm01s01i203': CFName('surface_net_downward_shortwave_flux', None, 'W m-2'),
    'm01s01i205': CFName('toa_outgoing_shortwave_flux', None, 'W m-2'),
    'm01s01i207': CFName('toa_incoming_shortwave_flux', None, 'W m-2'),
    'm01s01i208': CFName('toa_outgoing_shortwave_flux', None, 'W m-2'),
    'm01s01i209': CFName('toa_outgoing_shortwave_flux_assuming_clear_sky', None, 'W m-2'),
    'm01s01i210': CFName('surface_downwelling_shortwave_flux_in_air_assuming_clear_sky', None, 'W m-2'),
    'm01s01i211': CFName('surface_upwelling_shortwave_flux_in_air_assuming_clear_sky', None, 'W m-2'),
    'm01s01i232': CFName('tendency_of_air_temperature_due_to_shortwave_heating', None, 'unknown'),
    'm01s01i233': CFName('tendency_of_air_temperature_due_to_shortwave_heating_assuming_clear_sky', None, 'unknown'),
    'm01s01i235': CFName('surface_downwelling_shortwave_flux_in_air', None, 'W m-2'),
    'm01s01i237': CFName('net_downward_shortwave_flux_in_air', None, 'W m-2'),
    'm01s01i238': CFName('tropopause_upwelling_shortwave_flux', None, 'unknown'),
    'm01s01i242': CFName(None, 'large_scale_cloud_liquid_water_content_of_atmosphere_layer', 'unknown'),
    'm01s01i410': CFName('surface_downwelling_shortwave_flux_in_air_assuming_clear_sky', None, 'W m-2'),
    'm01s01i435': CFName('surface_downwelling_shortwave_flux_in_air', None, 'W m-2'),
    'm01s02i004': CFName('air_temperature', None, 'K'),
    'm01s02i201': CFName('surface_net_downward_longwave_flux', None, 'W m-2'),
    'm01s02i203': CFName('surface_net_downward_longwave_flux', None, 'W m-2'),
    'm01s02i204': CFName('cloud_area_fraction', None, '1'),
    'm01s02i205': CFName('toa_outgoing_longwave_flux', None, 'W m-2'),
    'm01s02i206': CFName('toa_outgoing_longwave_flux_assuming_clear_sky', None, 'W m-2'),
    'm01s02i207': CFName('surface_downwelling_longwave_flux_in_air', None, 'W m-2'),
    'm01s02i208': CFName('surface_downwelling_longwave_flux_in_air_assuming_clear_sky', None, 'W m-2'),
    'm01s02i232': CFName('tendency_of_air_temperature_due_to_longwave_heating', None, 'K s-1'),
    'm01s02i233': CFName('tendency_of_air_temperature_due_to_longwave_heating_assuming_clear_sky', None, 'unknown'),
    'm01s02i237': CFName('tropopause_net_downward_longwave_flux', None, 'unknown'),
    'm01s02i238': CFName('tropopause_downwelling_longwave_flux', None, 'unknown'),
    'm01s02i260': CFName('mass_fraction_of_ozone_in_air', None, '1'),
    'm01s02i261': CFName('cloud_area_fraction_in_atmosphere_layer', None, '1'),
    'm01s03i004': CFName('air_temperature', None, 'm s-1'),
    'm01s03i010': CFName('specific_humidity', None, '1'),
    'm01s03i025': CFName('atmosphere_boundary_layer_thickness', None, 'm'),
    'm01s03i201': CFName('downward_heat_flux_in_sea_ice', None, 'W m-2'),
    'm01s03i202': CFName('downward_heat_flux_in_soil', None, 'unknown'),
    'm01s03i217': CFName('surface_upward_sensible_heat_flux', None, 'W m-2'),
    'm01s03i219': CFName('surface_downward_eastward_stress', None, 'Pa'),
    'm01s03i220': CFName('surface_downward_northward_stress', None, 'Pa'),
    'm01s03i223': CFName('surface_upward_water_flux', None, 'kg m-2 s-1'),
    'm01s03i224': CFName('wind_mixing_energy_flux_into_sea_water', None, 'W m-2'),
    'm01s03i225': CFName('x_wind', None, 'm s-1'),
    'm01s03i226': CFName('y_wind', None, 'm s-1'),
    'm01s03i227': CFName('wind_speed', None, 'm s-1'),
    'm01s03i228': CFName('surface_upward_sensible_heat_flux', None, 'W m-2'),
    'm01s03i230': CFName('wind_speed', None, 'm s-1'),
    'm01s03i234': CFName('surface_upward_latent_heat_flux', None, 'W m-2'),
    'm01s03i236': CFName('air_temperature', None, 'K'),
    'm01s03i237': CFName('specific_humidity', None, '1'),
    'm01s03i238': CFName('soil_temperature', None, 'K'),
    'm01s03i245': CFName('relative_humidity', None, '%'),
    'm01s03i249': CFName('wind_speed', None, 'm s-1'),
    'm01s03i250': CFName('dew_point_temperature', None, 'K'),
    'm01s03i258': CFName('surface_snow_melt_heat_flux', None, 'W m-2'),
    'm01s03i261': CFName('gross_primary_productivity_of_carbon', None, 'kg m-2 s-1'),
    'm01s03i262': CFName('net_primary_productivity_of_carbon', None, 'kg m-2 s-1'),
    'm01s03i295': CFName(None, 'surface_snow_area_fraction_where_land', '%'),
    'm01s03i298': CFName('water_sublimation_flux', None, 'kg m-2 s-1'),
    'm01s03i313': CFName('soil_moisture_content_at_field_capacity', None, 'kg m-2'),
    'm01s03i332': CFName('toa_outgoing_longwave_flux', None, 'W m-2'),
    'm01s03i334': CFName('water_potential_evaporation_flux', None, 'kg m-2 s-1'),
    'm01s03i337': CFName('downward_heat_flux_in_soil', None, 'W m-2'),
    'm01s03i391': CFName('surface_downward_eastward_stress', None, 'Pa'),
    'm01s03i392': CFName('surface_downward_eastward_stress', None, 'Pa'),
    'm01s03i393': CFName('surface_downward_northward_stress', None, 'Pa'),
    'm01s03i394': CFName('surface_downward_northward_stress', None, 'Pa'),
    'm01s03i460': CFName('surface_downward_eastward_stress', None, 'Pa'),
    'm01s03i461': CFName('surface_downward_northward_stress', None, 'Pa'),
    'm01s04i004': CFName('air_temperature', None, 'K'),
    'm01s04i010': CFName('specific_humidity', None, '1'),
    'm01s04i201': CFName('stratiform_rainfall_amount', None, 'kg m-2'),
    'm01s04i202': CFName('stratiform_snowfall_amount', None, 'kg m-2'),
    'm01s04i203': CFName('stratiform_rainfall_rate', None, 'kg m-2 s-1'),
    'm01s04i204': CFName(None, 'stratiform_snowfall_rate', 'kg m-2 s-1'),
    'm01s05i010': CFName('specific_humidity', None, '1'),
    'm01s05i181': CFName('tendency_of_air_temperature_due_to_convection', None, 'K s-1'),
    'm01s05i182': CFName('tendency_of_specific_humidity_due_to_convection', None, 's-1'),
    'm01s05i185': CFName('tendency_of_eastward_wind_due_to_convection', None, 'm s-2'),
    'm01s05i186': CFName('tendency_of_northward_wind_due_to_convection', None, 'm s-2'),
    'm01s05i201': CFName('convective_rainfall_amount', None, 'kg m-2'),
    'm01s05i202': CFName('convective_snowfall_amount', None, 'kg m-2'),
    'm01s05i205': CFName('convective_rainfall_rate', None, 'kg m-2 s-1'),
    'm01s05i206': CFName('convective_snowfall_flux', None, 'kg m-2 s-1'),
    'm01s05i209': CFName('air_temperature', None, 'K'),
    'm01s05i212': CFName('convective_cloud_area_fraction_in_atmosphere_layer', None, 'unknown'),
    'm01s05i213': CFName('mass_fraction_of_convective_cloud_liquid_water_in_air', None, '1'),
    'm01s05i214': CFName('rainfall_flux', None, 'kg m-2 s-1'),
    'm01s05i215': CFName('snowfall_flux', None, 'kg m-2 s-1'),
    'm01s05i216': CFName('precipitation_flux', None, 'kg m-2 s-1'),
    'm01s05i233': CFName('mass_fraction_of_convective_cloud_liquid_water_in_air', None, '1'),
    'm01s06i185': CFName('tendency_of_eastward_wind_due_to_gravity_wave_drag', None, 'm s-2'),
    'm01s06i186': CFName('tendency_of_northward_wind_due_to_gravity_wave_drag', None, 'm s-2'),
    'm01s06i201': CFName('atmosphere_eastward_stress_due_to_gravity_wave_drag', None, 'Pa'),
    'm01s06i202': CFName('atmosphere_northward_stress_due_to_gravity_wave_drag', None, 'Pa'),
    'm01s08i023': CFName('surface_snow_amount', None, 'kg m-2'),
    'm01s08i202': CFName(None, 'surface_snow_melt_flux_where_land', 'kg m-2 s-1'),
    'm01s08i204': CFName('surface_runoff_amount', None, 'kg m-2'),
    'm01s08i205': CFName('subsurface_runoff_amount', None, 'kg m-2'),
    'm01s08i208': CFName('soil_moisture_content', None, 'unknown'),
    'm01s08i209': CFName('canopy_water_amount', None, 'unknown'),
    'm01s08i223': CFName('moisture_content_of_soil_layer', None, 'kg m-2'),
    'm01s08i225': CFName('soil_temperature', None, 'K'),
    'm01s08i229': CFName('mass_fraction_of_unfrozen_water_in_soil_moisture', None, 'unknown'),
    'm01s08i230': CFName('mass_fraction_of_frozen_water_in_soil_moisture', None, 'unknown'),
    'm01s08i231': CFName(None, 'surface_snow_melt_flux_where_land', 'kg m-2 s-1'),
    'm01s08i234': CFName('surface_runoff_flux', None, 'kg m-2 s-1'),
    'm01s08i235': CFName('subsurface_runoff_flux', None, 'kg m-2 s-1'),
    'm01s09i004': CFName('air_temperature', None, 'K'),
    'm01s09i010': CFName('specific_humidity', None, '1'),
    'm01s09i201': CFName('stratiform_cloud_area_fraction_in_atmosphere_layer', None, '1'),
    'm01s12i004': CFName('air_temperature', None, 'K'),
    'm01s12i010': CFName('specific_humidity', None, '1'),
    'm01s12i012': CFName('mass_fraction_of_cloud_ice_in_air', None, '1'),
    'm01s12i181': CFName('tendency_of_air_temperature_due_to_advection', None, 'K s-1'),
    'm01s12i182': CFName('tendency_of_specific_humidity_due_to_advection', None, 's-1'),
    'm01s12i183': CFName('tendency_of_mass_fraction_of_cloud_liquid_water_in_air_due_to_advection', None, 's-1'),
    'm01s12i184': CFName('tendency_of_mass_fraction_of_cloud_ice_in_air_due_to_advection', None, 's-1'),
    'm01s12i185': CFName('tendency_of_eastward_wind_due_to_advection', None, 'm s-1'),
    'm01s12i186': CFName('tendency_of_northward_wind_due_to_advection', None, 'm s-1'),
    'm01s12i187': CFName('tendency_of_upward_air_velocity_due_to_advection', None, 'm s-1'),
    'm01s12i201': CFName('lagrangian_tendency_of_air_pressure', None, 'Pa s-1'),
    'm01s13i181': CFName('tendency_of_air_temperature_due_to_diffusion', None, 'K s-1'),
    'm01s13i182': CFName('tendency_of_specific_humidity_due_to_diffusion', None, 's-1'),
    'm01s13i185': CFName('tendency_of_eastward_wind_due_to_diffusion', None, 'm s-1'),
    'm01s13i186': CFName('tendency_of_northward_wind_due_to_diffusion', None, 'm s-1'),
    'm01s15i108': CFName('air_pressure', None, 'Pa'),
    'm01s15i119': CFName('air_potential_temperature', None, 'K'),
    'm01s15i127': CFName('air_density', None, 'kg m-3'),
    'm01s15i142': CFName('upward_air_velocity', None, 'm s-1'),
    'm01s15i143': CFName('x_wind', None, 'm s-1'),
    'm01s15i144': CFName('y_wind', None, 'm s-1'),
    'm01s15i201': CFName('x_wind', None, 'm s-1'),
    'm01s15i202': CFName('y_wind', None, 'm s-1'),
    'm01s15i214': CFName('ertel_potential_vorticity', None, 'K m2 kg-1 s-1'),
    'm01s15i215': CFName('air_potential_temperature', None, 'K'),
    'm01s15i216': CFName('air_potential_temperature', None, 'K'),
    'm01s15i217': CFName('potential_vorticity_of_atmosphere_layer', None, 'Pa-1 s-1'),
    'm01s15i218': CFName('potential_vorticity_of_atmosphere_layer', None, 'Pa-1 s-1'),
    'm01s15i219': CFName('square_of_air_temperature', None, 'unknown'),
    'm01s15i220': CFName('square_of_eastward_wind', None, 'unknown'),
    'm01s15i221': CFName('square_of_northward_wind', None, 'unknown'),
    'm01s15i222': CFName('lagrangian_tendency_of_air_pressure', None, 'Pa s-1'),
    'm01s15i223': CFName('product_of_omega_and_air_temperature', None, 'unknown'),
    'm01s15i224': CFName('product_of_eastward_wind_and_omega', None, 'unknown'),
    'm01s15i225': CFName('product_of_northward_wind_and_omega', None, 'unknown'),
    'm01s15i226': CFName('specific_humidity', None, 'unknown'),
    'm01s15i227': CFName('product_of_eastward_wind_and_specific_humidity', None, 'unknown'),
    'm01s15i228': CFName('product_of_northward_wind_and_specific_humidity', None, 'unknown'),
    'm01s15i235': CFName('product_of_omega_and_specific_humidity', None, 'unknown'),
    'm01s15i238': CFName('geopotential_height', None, 'unknown'),
    'm01s15i239': CFName('product_of_eastward_wind_and_geopotential_height', None, 'unknown'),
    'm01s15i240': CFName('product_of_northward_wind_and_geopotential_height', None, 'unknown'),
    'm01s15i242': CFName('upward_air_velocity', None, 'm s-1'),
    'm01s15i243': CFName('x_wind', None, 'm s-1'),
    'm01s15i244': CFName('y_wind', None, 'm s-1'),
    'm01s16i004': CFName('air_temperature', None, 'K'),
    'm01s16i201': CFName('geopotential_height', None, 'm'),
    'm01s16i202': CFName('geopotential_height', None, 'm'),
    'm01s16i203': CFName('air_temperature', None, 'K'),
    'm01s16i204': CFName('relative_humidity', None, '%'),
    'm01s16i222': CFName('air_pressure_at_sea_level', None, 'Pa'),
    'm01s16i224': CFName(None, 'square_of_height', 'm2'),
    'm01s16i255': CFName('geopotential_height', None, 'm'),
    'm01s16i256': CFName('relative_humidity', None, '%'),
    'm01s20i003': CFName('wind_speed', None, 'm s-1'),
    'm01s20i004': CFName('wind_speed', None, 'm s-1'),
    'm01s20i005': CFName('divergence_of_wind', None, 's-1'),
    'm01s20i006': CFName('atmosphere_relative_vorticity', None, 's-1'),
    'm01s20i024': CFName('tropopause_air_pressure', None, 'Pa'),
    'm01s20i025': CFName('tropopause_air_temperature', None, 'K'),
    'm01s20i026': CFName('tropopause_altitude', None, 'm'),
    'm01s20i034': CFName('air_pressure_at_freezing_level', None, 'Pa'),
    'm01s20i064': CFName('tropopause_air_pressure', None, 'Pa'),
    'm01s20i065': CFName('tropopause_air_temperature', None, 'K'),
    'm01s20i066': CFName('tropopause_altitude', None, 'm'),
    'm01s30i003': CFName('upward_air_velocity', None, 'm s-1'),
    'm01s30i004': CFName('air_temperature', None, 'K'),
    'm01s30i005': CFName('specific_humidity', None, '1'),
    'm01s30i007': CFName('specific_kinetic_energy_of_air', None, 'm2 s-2'),
    'm01s30i111': CFName('air_temperature', None, 'K'),
    'm01s30i113': CFName('relative_humidity', None, '%'),
    'm01s30i181': CFName('tendency_of_air_temperature', None, 'K s-1'),
    'm01s30i182': CFName('tendency_of_specific_humidity', None, 's-1'),
    'm01s30i183': CFName('tendency_of_mass_fraction_of_cloud_liquid_water_in_air', None, 's-1'),
    'm01s30i184': CFName('tendency_of_mass_fraction_of_cloud_ice_in_air', None, 's-1'),
    'm01s30i185': CFName('tendency_of_eastward_wind', None, 'm s-1'),
    'm01s30i186': CFName('tendency_of_northward_wind', None, 'm s-1'),
    'm01s30i187': CFName('tendency_of_upward_air_velocity', None, 'm s-1'),
    'm01s30i188': CFName('tendency_of_air_density', None, 'kg m-3 s-1'),
    'm01s30i201': CFName('x_wind', None, 'm s-1'),
    'm01s30i202': CFName('y_wind', None, 'm s-1'),
    'm01s30i203': CFName('upward_air_velocity', None, 'm s-1'),
    'm01s30i204': CFName('air_temperature', None, 'K'),
    'm01s30i205': CFName('specific_humidity', None, '1'),
    'm01s30i207': CFName('geopotential_height', None, 'm'),
    'm01s30i208': CFName('lagrangian_tendency_of_air_pressure', None, 'Pa s-1'),
    'm01s30i211': CFName('square_of_eastward_wind', None, 'm2 s-2'),
    'm01s30i212': CFName('product_of_eastward_wind_and_northward_wind', None, 'm2 s-2'),
    'm01s30i213': CFName('product_of_eastward_wind_and_upward_air_velocity', None, 'm2 s-2'),
    'm01s30i214': CFName('product_of_eastward_wind_and_air_temperature', None, 'K m s-1'),
    'm01s30i215': CFName('product_of_eastward_wind_and_specific_humidity', None, 'm s-1'),
    'm01s30i217': CFName('product_of_eastward_wind_and_geopotential_height', None, 'm2 s-1'),
    'm01s30i218': CFName('product_of_eastward_wind_and_omega', None, 'Pa m s-1'),
    'm01s30i222': CFName('square_of_northward_wind', None, 'm2 s-2'),
    'm01s30i223': CFName('product_of_northward_wind_and_upward_air_velocity', None, 'm2 s-2'),
    'm01s30i224': CFName('product_of_northward_wind_and_air_temperature', None, 'K m s-1'),
    'm01s30i225': CFName('product_of_northward_wind_and_specific_humidity', None, 'm s-1'),
    'm01s30i227': CFName('product_of_northward_wind_and_geopotential_height', None, 'm2 s-1'),
    'm01s30i228': CFName('product_of_northward_wind_and_omega', None, 'Pa m s-1'),
    'm01s30i233': CFName('square_of_upward_air_velocity', None, 'm2 s-2'),
    'm01s30i234': CFName('product_of_upward_air_velocity_and_air_temperature', None, 'K m s-1'),
    'm01s30i235': CFName('product_of_upward_air_velocity_and_specific_humidity', None, 'm s-1'),
    'm01s30i244': CFName('square_of_air_temperature', None, 'K2'),
    'm01s30i245': CFName('product_of_air_temperature_and_specific_humidity', None, 'K'),
    'm01s30i248': CFName('product_of_air_temperature_and_omega', None, 'K Pa s-1'),
    'm01s30i258': CFName('product_of_specific_humidity_and_omega', None, 'Pa s-1'),
    'm01s30i277': CFName('square_of_geopotential_height', None, 'm2'),
    'm01s30i278': CFName('product_of_geopotential_height_and_omega', None, 'Pa m s-1'),
    'm01s30i288': CFName('square_of_lagrangian_tendency_of_air_pressure', None, 'Pa2 s-2'),
    'm01s30i302': CFName('virtual_temperature', None, 'K'),
    'm01s30i310': CFName('northward_transformed_eulerian_mean_air_velocity', None, 'MKS'),
    'm01s30i311': CFName('northward_transformed_eulerian_mean_air_velocity', None, 'MKS'),
    'm01s30i312': CFName('northward_eliassen_palm_flux_in_air', None, 'MKS'),
    'm01s30i313': CFName('upward_eliassen_palm_flux_in_air', None, 'MKS'),
    'm01s30i314': CFName('tendency_of_eastward_wind_due_to_eliassen_palm_flux_divergence', None, 'MKS'),
    'm01s30i401': CFName('atmosphere_kinetic_energy_content', None, 'J m-2'),
    'm01s30i405': CFName('atmosphere_cloud_liquid_water_content', None, 'kg m-2'),
    'm01s30i406': CFName('atmosphere_cloud_ice_content', None, 'kg m-2'),
    'm01s30i417': CFName('surface_air_pressure', None, 'Pa'),
    'm01s30i418': CFName('surface_air_pressure', None, 'Pa'),
    'm01s30i451': CFName('tropopause_air_pressure', None, 'Pa'),
    'm01s30i452': CFName('tropopause_air_temperature', None, 'K'),
    'm01s30i453': CFName('tropopause_altitude', None, 'm'),
    'm01s33i001': CFName('mole_fraction_of_ozone_in_air', None, 'mole mole-1'),
    'm01s33i004': CFName(None, 'mole_fraction_of_nitrogen_trioxide_in_air', 'mole mole-1'),
    'm01s33i005': CFName('mole_fraction_of_dinitrogen_pentoxide_in_air', None, 'mole mole-1'),
    'm01s33i006': CFName('mole_fraction_of_peroxynitric_acid_in_air', None, 'mole mole-1'),
    'm01s33i007': CFName('mole_fraction_of_chlorine_nitrate_in_air', None, 'mole mole-1'),
    'm01s33i009': CFName('mole_fraction_of_methane_in_air', None, 'mole mole-1'),
    'm01s33i041': CFName('mole_fraction_of_atomic_chlorine_in_air', None, '1'),
    'm01s33i042': CFName('mole_fraction_of_chlorine_monoxide_in_air', None, '1'),
    'm01s33i043': CFName('mole_fraction_of_dichlorine_peroxide_in_air', None, '1'),
    'm01s33i044': CFName('mole_fraction_of_chlorine_dioxide_in_air', None, '1'),
    'm01s33i047': CFName('mole_fraction_of_bromine_chloride_in_air', None, '1'),
    'm01s33i048': CFName('mole_fraction_of_bromine_nitrate_in_air', None, '1'),
    'm01s33i049': CFName('mole_fraction_of_nitrous_oxide_in_air', None, '1'),
    'm01s33i051': CFName('mole_fraction_of_hypochlorous_acid_in_air', None, '1'),
    'm01s33i054': CFName('mole_fraction_of_chlorine_nitrate_in_air', None, '1'),
    'm01s33i055': CFName('mole_fraction_of_cfc11_in_air', None, '1'),
    'm01s33i056': CFName('mole_fraction_of_cfc12_in_air', None, '1'),
    'm01s33i058': CFName('mole_fraction_of_atomic_nitrogen_in_air', None, '1'),
    'm01s33i150': CFName('age_of_stratospheric_air', None, '1'),
    'm01s34i001': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i002': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i003': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i004': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i005': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i006': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i007': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i008': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i009': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i010': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i011': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i012': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i013': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i014': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i015': CFName(None, 'mass_fraction_of_ethyl_hydroperoxide_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i016': CFName(None, 'mass_fraction_of_acetaldehyde_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i017': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i018': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i019': CFName(None, 'mass_fraction_of_n-propyl_hydroperoxide_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i020': CFName(None, 'mass_fraction_of_i-propyl_hydroperoxide_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i021': CFName(None, 'mass_fraction_of_propanal_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i022': CFName(None, 'mass_fraction_of_acetone_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i023': CFName(None, 'mass_fraction_of_acetonylhydroperoxide_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i024': CFName(None, 'mass_fraction_of_peroxypropionyl_nitrate_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i025': CFName(None, 'mass_fraction_of_methyl_nitrate_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i026': CFName(None, 'mass_fraction_of_stratospheric_ozone_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i027': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i028': CFName(None, 'mass_fraction_of_isoprene_hydroperoxide_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i030': CFName(None, 'mass_fraction_of_methacrolein_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i031': CFName(None, 'mass_fraction_of_methacroyl_hydroperoxide_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i032': CFName(None, 'mass_fraction_of_methacryloylperoxy_nitrate_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i033': CFName(None, 'mass_fraction_of_hydroxyacetone_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i034': CFName(None, 'mass_fraction_of_methlyglyoxal_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i035': CFName(None, 'mass_fraction_of_second_generation_isoprene_nitrate_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i036': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i037': CFName(None, 'mass_fraction_of_peracetic_acid_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i038': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i041': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i042': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i043': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i044': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i045': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i047': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i048': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i049': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i051': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i052': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i053': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i054': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i055': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i056': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i057': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i058': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i059': CFName(None, 'mass_fraction_of_ground_state_atomic_oxygen_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i070': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i071': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i072': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i073': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i074': CFName(None, 'mass_fraction_of_methanesulfonic_acid_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i075': CFName(None, 'mass_fraction_of_dimethyl_sulfoxide', 'Mass mixing ratio (kg/kg)'),
    'm01s34i076': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i077': CFName(None, 'mass_fraction_of_carbon_disulfide_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i078': CFName(None, 'mass_fraction_of_carbonyl_sulfide_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i079': CFName(None, 'mass_fraction_of_hydrogen_sulfide_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i080': CFName(None, 'mass_fraction_of_atomic_hydrogen_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i081': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i082': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i083': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i084': CFName(None, 'mass_fraction_of_ethyl_peroxy_radical_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i085': CFName(None, 'mass_fraction_of_peroxyacetyl_radical_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i086': CFName(None, 'mass_fraction_of_n-propylperoxy_radical_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i087': CFName(None, 'mass_fraction_of_isopropylperoxy_radical_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i088': CFName(None, 'mass_fraction_of_peroxypropanoyl_radical_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i089': CFName(None, 'mass_fraction_of_acetonyl_peroxy_radical_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i093': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i096': CFName(None, 'mass_fraction_of_methyl_ethyl_ketone_in_air', 'Mass mixing ratio (kg/kg)'),
    'm01s34i097': CFName(None, None, 'Mass mixing ratio (kg/kg)'),
    'm01s34i100': CFName(None, 'mass_fraction_of_lumped_chlorine_expressed_as_hydrogen_chloride', 'Mass mixing ratio (kg/kg)'),
    'm02s00i101': CFName('sea_water_potential_temperature', None, 'degC'),
    'm02s00i102': CFName('sea_water_salinity', None, '1e3 psu @0.035'),
    'm02s00i121': CFName('baroclinic_eastward_sea_water_velocity', None, 'cm s-1'),
    'm02s00i122': CFName('baroclinic_northward_sea_water_velocity', None, 'cm s-1'),
    'm02s00i130': CFName('ocean_barotropic_streamfunction', None, 'cm3 s-1'),
    'm02s00i131': CFName('ocean_barotropic_streamfunction', None, 'cm3 s-1'),
    'm02s00i132': CFName('tendency_of_ocean_barotropic_streamfunction', None, 'cm3 s-2'),
    'm02s00i133': CFName('tendency_of_ocean_barotropic_streamfunction', None, 'cm3 s-2'),
    'm02s00i134': CFName('surface_air_pressure', None, 'g cm-1 s-2'),
    'm02s00i135': CFName('barotropic_eastward_sea_water_velocity', None, 'cm s-1'),
    'm02s00i136': CFName('barotropic_northward_sea_water_velocity', None, 'cm s-1'),
    'm02s00i137': CFName('ocean_mixed_layer_thickness', None, 'm'),
    'm02s00i139': CFName('downward_eastward_stress_at_sea_ice_base', None, 'Pa'),
    'm02s00i140': CFName('downward_northward_stress_at_sea_ice_base', None, 'Pa'),
    'm02s00i141': CFName('surface_snow_thickness', None, 'm'),
    'm02s00i143': CFName('upward_sea_ice_basal_heat_flux', None, 'W m-2'),
    'm02s00i146': CFName('sea_ice_area_fraction', None, '1'),
    'm02s00i147': CFName('sea_ice_thickness', None, 'm'),
    'm02s00i148': CFName('eastward_sea_ice_velocity', None, 'm s-1'),
    'm02s00i149': CFName('northward_sea_ice_velocity', None, 'm s-1'),
    'm02s00i150': CFName('surface_downward_eastward_stress', None, 'Pa'),
    'm02s00i151': CFName('surface_downward_northward_stress', None, 'Pa'),
    'm02s00i152': CFName('wind_mixing_energy_flux_into_sea_water', None, 'W m-2'),
    'm02s00i166': CFName('water_flux_into_sea_water_from_rivers', None, 'kg m-2 s-1'),
    'm02s00i171': CFName('snowfall_flux', None, 'kg m-2 s-1'),
    'm02s00i180': CFName('sea_surface_temperature', None, 'K'),
    'm02s00i181': CFName('sea_surface_salinity', None, '1e3 psu @0.035'),
    'm02s00i182': CFName('air_temperature', None, 'K'),
    'm02s00i183': CFName('sea_ice_thickness', None, 'm'),
    'm02s00i185': CFName('heat_flux_correction', None, 'W m-2'),
    'm02s00i186': CFName('water_flux_correction', None, 'Kg m-2 s-1'),
    'm02s00i190': CFName('surface_snow_and_ice_melt_heat_flux', None, 'W m-2'),
    'm02s00i191': CFName('downward_heat_flux_in_sea_ice', None, 'W m-2'),
    'm02s30i201': CFName('upward_sea_water_velocity', None, 'cm s-1'),
    'm02s30i202': CFName('ocean_mixed_layer_thickness', None, 'm'),
    'm02s30i211': CFName('northward_ocean_heat_transport', None, 'PW'),
    'm02s30i212': CFName('northward_ocean_salt_transport', None, '1e7kg s-1'),
    'm02s30i320': CFName('eastward_sea_water_velocity', None, 'cm s-1'),
    'm02s30i321': CFName('northward_sea_water_velocity', None, 'cm s-1'),
    'm02s30i324': CFName('ocean_mixed_layer_thickness', None, 'm'),
    'm02s32i201': CFName('tendency_of_sea_ice_area_fraction_due_to_dynamics', None, 's-1'),
    'm02s32i202': CFName('tendency_of_sea_ice_thickness_due_to_dynamics', None, 'm s-1'),
    'm02s32i209': CFName('eastward_sea_ice_velocity', None, 'm s-1'),
    'm02s32i210': CFName('northward_sea_ice_velocity', None, 'm s-1'),
    'm02s32i211': CFName('tendency_of_sea_ice_area_fraction_due_to_thermodynamics', None, 's-1'),
    'm02s32i212': CFName('tendency_of_sea_ice_thickness_due_to_thermodynamics', None, 'm s-1'),
    'm02s32i219': CFName('downward_eastward_stress_at_sea_ice_base', None, 'Pa'),
    'm02s32i220': CFName('downward_northward_stress_at_sea_ice_base', None, 'Pa'),
    'm03s00i177': CFName(None, 'prescribed_heat_flux_into_slab_ocean', 'W m-2'),
    }

CF_TO_LBFC = {
    CFName(None, 'stratiform_snowfall_rate', 'kg m-2 s-1'): 118,
    CFName('age_of_stratospheric_air', None, '1'): 501,
    CFName('air_density', None, 'kg m-3'): 27,
    CFName('air_potential_temperature', None, 'K'): 19,
    CFName('air_pressure', None, 'Pa'): 8,
    CFName('air_pressure_at_freezing_level', None, 'Pa'): 8,
    CFName('air_pressure_at_sea_level', None, 'Pa'): 8,
    CFName('atmosphere_boundary_layer_thickness', None, 'm'): 5,
    CFName('atmosphere_eastward_stress_due_to_gravity_wave_drag', None, 'Pa'): 61,
    CFName('atmosphere_kinetic_energy_content', None, 'J m-2'): 63,
    CFName('atmosphere_northward_stress_due_to_gravity_wave_drag', None, 'Pa'): 62,
    CFName('atmosphere_relative_vorticity', None, 's-1'): 73,
    CFName('cloud_area_fraction', None, '1'): 30,
    CFName('cloud_area_fraction_in_atmosphere_layer', None, '1'): 1720,
    CFName('convective_cloud_area_fraction', None, '1'): 34,
    CFName('convective_rainfall_amount', None, 'kg m-2'): 94,
    CFName('convective_snowfall_amount', None, 'kg m-2'): 117,
    CFName('dimensionless_exner_function', None, '1'): 7,
    CFName('divergence_of_wind', None, 's-1'): 74,
    CFName('downward_heat_flux_in_sea_ice', None, 'W m-2'): 261,
    CFName('downward_heat_flux_in_soil', None, 'W m-2'): 1564,
    CFName('ertel_potential_vorticity', None, 'K m2 kg-1 s-1'): 82,
    CFName('geopotential_height', None, 'm'): 1,
    CFName('lagrangian_tendency_of_air_pressure', None, 'Pa s-1'): 40,
    CFName('land_binary_mask', None, '1'): 395,
    CFName('mass_fraction_of_carbon_dioxide_in_air', None, '1'): 1564,
    CFName('mass_fraction_of_cloud_liquid_water_in_air', None, '1'): 79,
    CFName('mass_fraction_of_dimethyl_sulfide_in_air', None, '1'): 1373,
    CFName('mass_fraction_of_frozen_water_in_soil_moisture', None, '1'): 1386,
    CFName('mass_fraction_of_ozone_in_air', None, '1'): 453,
    CFName('mass_fraction_of_sulfur_dioxide_in_air', None, '1'): 1374,
    CFName('mass_fraction_of_unfrozen_water_in_soil_moisture', None, '1'): 1385,
    CFName('moisture_content_of_soil_layer', None, 'kg m-2'): 122,
    CFName('mole_fraction_of_atomic_chlorine_in_air', None, '1'): 501,
    CFName('mole_fraction_of_atomic_nitrogen_in_air', None, '1'): 501,
    CFName('mole_fraction_of_bromine_chloride_in_air', None, '1'): 501,
    CFName('mole_fraction_of_bromine_nitrate_in_air', None, '1'): 501,
    CFName('mole_fraction_of_cfc11_in_air', None, '1'): 501,
    CFName('mole_fraction_of_cfc12_in_air', None, '1'): 501,
    CFName('mole_fraction_of_chlorine_dioxide_in_air', None, '1'): 501,
    CFName('mole_fraction_of_chlorine_monoxide_in_air', None, '1'): 501,
    CFName('mole_fraction_of_chlorine_nitrate_in_air', None, '1'): 501,
    CFName('mole_fraction_of_dichlorine_peroxide_in_air', None, '1'): 501,
    CFName('mole_fraction_of_hypochlorous_acid_in_air', None, '1'): 501,
    CFName('mole_fraction_of_nitrous_oxide_in_air', None, '1'): 501,
    CFName('rainfall_flux', None, 'kg m-2 s-1'): 97,
    CFName('relative_humidity', None, '%'): 88,
    CFName('root_depth', None, 'm'): 321,
    CFName('sea_ice_albedo', None, '1'): 322,
    CFName('sea_ice_area_fraction', None, '1'): 37,
    CFName('sea_ice_temperature', None, 'K'): 209,
    CFName('sea_ice_thickness', None, 'm'): 687,
    CFName('snow_grain_size', None, '1e-6 m'): 1507,
    CFName('snowfall_amount', None, 'kg m-2'): 93,
    CFName('snowfall_flux', None, 'kg m-2 s-1'): 108,
    CFName('soil_albedo', None, '1'): 1395,
    CFName('soil_carbon_content', None, 'kg m-2'): 1397,
    CFName('soil_hydraulic_conductivity_at_saturation', None, 'm s-1'): 333,
    CFName('soil_moisture_content_at_field_capacity', None, 'kg m-2'): 1559,
    CFName('soil_porosity', None, '1'): 332,
    CFName('soil_suction_at_saturation', None, 'Pa'): 342,
    CFName('soil_temperature', None, 'K'): 23,
    CFName('soil_thermal_capacity', None, 'J kg-1 K-1'): 335,
    CFName('soil_thermal_conductivity', None, 'W m-1 K-1'): 336,
    CFName('specific_kinetic_energy_of_air', None, 'm2 s-2'): 60,
    CFName('stratiform_cloud_area_fraction_in_atmosphere_layer', None, '1'): 220,
    CFName('stratiform_rainfall_amount', None, 'kg m-2'): 102,
    CFName('stratiform_rainfall_rate', None, 'kg m-2 s-1'): 99,
    CFName('stratiform_snowfall_amount', None, 'kg m-2'): 116,
    CFName('subsurface_runoff_amount', None, 'kg m-2'): 112,
    CFName('subsurface_runoff_flux', None, 'kg m-2 s-1'): 1533,
    CFName('surface_albedo_assuming_deep_snow', None, '1'): 328,
    CFName('surface_albedo_assuming_no_snow', None, '1'): 322,
    CFName('surface_altitude', None, 'm'): 1,
    CFName('surface_downwelling_shortwave_flux_in_air', None, 'W m-2'): 203,
    CFName('surface_downwelling_shortwave_flux_in_air_assuming_clear_sky', None, 'W m-2'): 208,
    CFName('surface_eastward_sea_water_velocity', None, 'm s-1'): 701,
    CFName('surface_net_downward_longwave_flux', None, 'W m-2'): 187,
    CFName('surface_net_downward_shortwave_flux', None, 'W m-2'): 186,
    CFName('surface_northward_sea_water_velocity', None, 'm s-1'): 702,
    CFName('surface_roughness_length', None, 'm'): 324,
    CFName('surface_runoff_amount', None, 'kg m-2'): 111,
    CFName('surface_runoff_flux', None, 'kg m-2 s-1'): 1532,
    CFName('surface_snow_amount', None, 'kg m-2'): 93,
    CFName('surface_temperature', None, 'K'): 16,
    CFName('surface_upward_sensible_heat_flux', None, 'W m-2'): 178,
    CFName('surface_upward_water_flux', None, 'kg m-2 s-1'): 184,
    CFName('surface_upwelling_shortwave_flux_in_air_assuming_clear_sky', None, 'W m-2'): 207,
    CFName('tendency_of_air_density', None, 'kg m-3 s-1'): 7,
    CFName('tendency_of_air_temperature', None, 'K s-1'): 16,
    CFName('tendency_of_air_temperature_due_to_diffusion', None, 'K s-1'): 16,
    CFName('tendency_of_air_temperature_due_to_longwave_heating', None, 'K s-1'): 253,
    CFName('tendency_of_eastward_wind', None, 'm s-1'): 56,
    CFName('tendency_of_eastward_wind_due_to_diffusion', None, 'm s-1'): 56,
    CFName('tendency_of_mass_fraction_of_cloud_ice_in_air', None, 's-1'): 78,
    CFName('tendency_of_mass_fraction_of_cloud_liquid_water_in_air', None, 's-1'): 79,
    CFName('tendency_of_northward_wind', None, 'm s-1'): 57,
    CFName('tendency_of_northward_wind_due_to_diffusion', None, 'm s-1'): 57,
    CFName('tendency_of_specific_humidity', None, 's-1'): 95,
    CFName('tendency_of_specific_humidity_due_to_diffusion', None, 's-1'): 95,
    CFName('tendency_of_upward_air_velocity', None, 'm s-1'): 42,
    CFName('toa_incoming_shortwave_flux', None, 'W m-2'): 200,
    CFName('toa_outgoing_longwave_flux', None, 'W m-2'): 206,
    CFName('toa_outgoing_longwave_flux_assuming_clear_sky', None, 'W m-2'): 210,
    CFName('toa_outgoing_shortwave_flux', None, 'W m-2'): 201,
    CFName('toa_outgoing_shortwave_flux_assuming_clear_sky', None, 'W m-2'): 207,
    CFName('tropopause_air_pressure', None, 'Pa'): 8,
    CFName('tropopause_air_temperature', None, 'K'): 16,
    CFName('tropopause_altitude', None, 'm'): 1,
    CFName('upward_air_velocity', None, 'm s-1'): 42,
    CFName('vegetation_area_fraction', None, '1'): 326,
    CFName('virtual_temperature', None, 'K'): 16,
    CFName('volume_fraction_of_condensed_water_in_soil_at_critical_point', None, '1'): 330,
    CFName('volume_fraction_of_condensed_water_in_soil_at_wilting_point', None, '1'): 329,
    CFName('water_potential_evaporation_flux', None, 'kg m-2 s-1'): 115,
    CFName('wind_mixing_energy_flux_into_sea_water', None, 'W m-2'): 182,
    }

########NEW FILE########
__FILENAME__ = _ff_cross_references
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
#
# DO NOT EDIT: AUTO-GENERATED
"""
Auto-generated from iris/tools/gen_stash_refs.py
Relates grid code and field code to the stash code.

"""
from collections import namedtuple


Stash = namedtuple('Stash', 'grid_code field_code')


STASH_TRANS = {
    "m01s00i001": Stash(1, 8),
    "m01s00i002": Stash(18, 56),
    "m01s00i003": Stash(19, 57),
    "m01s00i004": Stash(1, 19),
    "m01s00i005": Stash(21, 0),
    "m01s00i006": Stash(21, 0),
    "m01s00i007": Stash(1, 1),
    "m01s00i008": Stash(21, 2011),
    "m01s00i009": Stash(21, 122),
    "m01s00i010": Stash(1, 95),
    "m01s00i011": Stash(1, 95),
    "m01s00i012": Stash(1, 78),
    "m01s00i013": Stash(1, 34),
    "m01s00i014": Stash(1, 222),
    "m01s00i015": Stash(1, 223),
    "m01s00i016": Stash(1, 219),
    "m01s00i017": Stash(21, 174),
    "m01s00i018": Stash(21, 175),
    "m01s00i020": Stash(21, 23),
    "m01s00i021": Stash(1, 1100),
    "m01s00i022": Stash(21, 271),
    "m01s00i023": Stash(1, 93),
    "m01s00i024": Stash(1, 16),
    "m01s00i025": Stash(1, 5),
    "m01s00i026": Stash(1, 324),
    "m01s00i027": Stash(3, 0),
    "m01s00i028": Stash(18, 701),
    "m01s00i029": Stash(19, 702),
    "m01s00i030": Stash(1, 395),
    "m01s00i031": Stash(3, 37),
    "m01s00i032": Stash(3, 687),
    "m01s00i033": Stash(1, 1),
    "m01s00i034": Stash(21, 150),
    "m01s00i035": Stash(21, 152),
    "m01s00i036": Stash(21, 153),
    "m01s00i037": Stash(21, 154),
    "m01s00i038": Stash(3, 0),
    "m01s00i039": Stash(1, 0),
    "m01s00i040": Stash(21, 329),
    "m01s00i041": Stash(21, 330),
    "m01s00i042": Stash(21, 331),
    "m01s00i043": Stash(21, 332),
    "m01s00i044": Stash(21, 333),
    "m01s00i045": Stash(21, 334),
    "m01s00i046": Stash(21, 335),
    "m01s00i047": Stash(21, 336),
    "m01s00i048": Stash(21, 342),
    "m01s00i049": Stash(3, 209),
    "m01s00i050": Stash(21, 326),
    "m01s00i051": Stash(21, 321),
    "m01s00i052": Stash(21, 322),
    "m01s00i053": Stash(21, 328),
    "m01s00i054": Stash(21, 323),
    "m01s00i055": Stash(21, 325),
    "m01s00i056": Stash(21, 327),
    "m01s00i057": Stash(1, 287),
    "m01s00i058": Stash(1, 569),
    "m01s00i059": Stash(1, 570),
    "m01s00i060": Stash(22, 453),
    "m01s00i061": Stash(1, 501),
    "m01s00i062": Stash(1, 502),
    "m01s00i063": Stash(1, 503),
    "m01s00i064": Stash(1, 504),
    "m01s00i065": Stash(1, 505),
    "m01s00i066": Stash(1, 506),
    "m01s00i067": Stash(1, 507),
    "m01s00i068": Stash(1, 508),
    "m01s00i069": Stash(1, 509),
    "m01s00i070": Stash(1, 2038),
    "m01s00i071": Stash(1, 2039),
    "m01s00i072": Stash(1, 2040),
    "m01s00i073": Stash(1, 2041),
    "m01s00i074": Stash(1, 2056),
    "m01s00i075": Stash(1, 515),
    "m01s00i076": Stash(1, 516),
    "m01s00i077": Stash(1, 517),
    "m01s00i078": Stash(1, 518),
    "m01s00i079": Stash(1, 519),
    "m01s00i080": Stash(1, 520),
    "m01s00i081": Stash(1, 521),
    "m01s00i082": Stash(1, 522),
    "m01s00i084": Stash(1, 524),
    "m01s00i085": Stash(1, 525),
    "m01s00i086": Stash(1, 526),
    "m01s00i087": Stash(1, 527),
    "m01s00i088": Stash(1, 528),
    "m01s00i089": Stash(1, 529),
    "m01s00i090": Stash(1, 286),
    "m01s00i091": Stash(1, 0),
    "m01s00i093": Stash(21, 0),
    "m01s00i095": Stash(3, 93),
    "m01s00i096": Stash(1, 921),
    "m01s00i097": Stash(0, 0),
    "m01s00i098": Stash(1, 0),
    "m01s00i099": Stash(1, 1709),
    "m01s00i100": Stash(1, 1711),
    "m01s00i101": Stash(1, 1374),
    "m01s00i102": Stash(1, 1373),
    "m01s00i103": Stash(1, 1370),
    "m01s00i104": Stash(1, 1371),
    "m01s00i105": Stash(1, 1372),
    "m01s00i106": Stash(1, 581),
    "m01s00i107": Stash(1, 1379),
    "m01s00i108": Stash(1, 1491),
    "m01s00i109": Stash(1, 1492),
    "m01s00i110": Stash(1, 1493),
    "m01s00i111": Stash(1, 1683),
    "m01s00i112": Stash(1, 1684),
    "m01s00i113": Stash(1, 1685),
    "m01s00i114": Stash(1, 1491),
    "m01s00i115": Stash(1, 1491),
    "m01s00i116": Stash(1, 1491),
    "m01s00i117": Stash(1, 1491),
    "m01s00i118": Stash(1, 1491),
    "m01s00i121": Stash(1, 569),
    "m01s00i122": Stash(1, 580),
    "m01s00i123": Stash(1, 600),
    "m01s00i124": Stash(1, 581),
    "m01s00i125": Stash(1, 60),
    "m01s00i126": Stash(1, 569),
    "m01s00i127": Stash(1, 572),
    "m01s00i128": Stash(1, 573),
    "m01s00i129": Stash(1, 573),
    "m01s00i130": Stash(1, 574),
    "m01s00i131": Stash(1, 574),
    "m01s00i132": Stash(1, 575),
    "m01s00i133": Stash(1, 1558),
    "m01s00i134": Stash(1, 573),
    "m01s00i135": Stash(1, 573),
    "m01s00i150": Stash(1, 42),
    "m01s00i151": Stash(23, 1905),
    "m01s00i152": Stash(23, 1906),
    "m01s00i153": Stash(23, 1902),
    "m01s00i154": Stash(19, 57),
    "m01s00i155": Stash(21, 1907),
    "m01s00i156": Stash(21, 1908),
    "m01s00i157": Stash(1, 1909),
    "m01s00i160": Stash(1, 1910),
    "m01s00i161": Stash(1, 1911),
    "m01s00i162": Stash(1, 1912),
    "m01s00i163": Stash(1, 1913),
    "m01s00i164": Stash(1, 1914),
    "m01s00i165": Stash(1, 1915),
    "m01s00i166": Stash(1, 1916),
    "m01s00i167": Stash(1, 1917),
    "m01s00i168": Stash(1, 1918),
    "m01s00i169": Stash(1, 1919),
    "m01s00i171": Stash(1, 186),
    "m01s00i172": Stash(1, 186),
    "m01s00i173": Stash(1, 187),
    "m01s00i174": Stash(1, 187),
    "m01s00i176": Stash(18, 61),
    "m01s00i177": Stash(19, 62),
    "m01s00i178": Stash(1, 182),
    "m01s00i179": Stash(1, 178),
    "m01s00i180": Stash(1, 107),
    "m01s00i181": Stash(1, 285),
    "m01s00i184": Stash(1, 1414),
    "m01s00i185": Stash(1, 1415),
    "m01s00i186": Stash(1, 99),
    "m01s00i187": Stash(1, 118),
    "m01s00i188": Stash(1, 98),
    "m01s00i189": Stash(1, 119),
    "m01s00i190": Stash(1, 922),
    "m01s00i191": Stash(1, 183),
    "m01s00i192": Stash(1, 112),
    "m01s00i201": Stash(1, 8),
    "m01s00i202": Stash(18, 56),
    "m01s00i203": Stash(19, 57),
    "m01s00i204": Stash(1, 19),
    "m01s00i205": Stash(1, 36),
    "m01s00i208": Stash(21, 1382),
    "m01s00i209": Stash(21, 1383),
    "m01s00i211": Stash(1, 218),
    "m01s00i212": Stash(1, 1101),
    "m01s00i213": Stash(21, 1384),
    "m01s00i214": Stash(21, 1385),
    "m01s00i215": Stash(21, 1386),
    "m01s00i216": Stash(21, 1391),
    "m01s00i217": Stash(21, 1392),
    "m01s00i218": Stash(21, 1393),
    "m01s00i219": Stash(21, 1394),
    "m01s00i220": Stash(21, 1395),
    "m01s00i221": Stash(1, 1396),
    "m01s00i222": Stash(1, 259),
    "m01s00i223": Stash(21, 1397),
    "m01s00i224": Stash(21, 1398),
    "m01s00i225": Stash(21, 1500),
    "m01s00i226": Stash(21, 1501),
    "m01s00i227": Stash(21, 1502),
    "m01s00i228": Stash(21, 1503),
    "m01s00i229": Stash(21, 1504),
    "m01s00i230": Stash(21, 1505),
    "m01s00i231": Stash(21, 1507),
    "m01s00i232": Stash(21, 1508),
    "m01s00i233": Stash(21, 1510),
    "m01s00i234": Stash(21, 1511),
    "m01s00i235": Stash(1, 113),
    "m01s00i236": Stash(21, 1579),
    "m01s00i237": Stash(21, 1580),
    "m01s00i238": Stash(1, 1581),
    "m01s00i239": Stash(1, 1582),
    "m01s00i240": Stash(21, 1583),
    "m01s00i241": Stash(21, 93),
    "m01s00i242": Stash(21, 93),
    "m01s00i243": Stash(21, 322),
    "m01s00i244": Stash(21, 322),
    "m01s00i245": Stash(21, 322),
    "m01s00i246": Stash(21, 1879),
    "m01s00i250": Stash(1, 1560),
    "m01s00i251": Stash(1, 1561),
    "m01s00i252": Stash(1, 1564),
    "m01s00i253": Stash(1, 0),
    "m01s00i254": Stash(1, 79),
    "m01s00i255": Stash(1, 7),
    "m01s00i256": Stash(18, 56),
    "m01s00i257": Stash(19, 57),
    "m01s00i258": Stash(1, 42),
    "m01s00i259": Stash(1, 0),
    "m01s00i260": Stash(1, 0),
    "m01s00i261": Stash(1, 0),
    "m01s00i262": Stash(1, 0),
    "m01s00i263": Stash(1, 0),
    "m01s00i264": Stash(1, 0),
    "m01s00i265": Stash(1, 1729),
    "m01s00i266": Stash(1, 220),
    "m01s00i267": Stash(1, 1425),
    "m01s00i268": Stash(1, 1426),
    "m01s00i269": Stash(1, 701),
    "m01s00i270": Stash(1, 702),
    "m01s00i271": Stash(1, 1689),
    "m01s00i272": Stash(1, 1681),
    "m01s00i273": Stash(1, 1921),
    "m01s00i274": Stash(21, 900),
    "m01s00i275": Stash(21, 900),
    "m01s00i276": Stash(21, 900),
    "m01s00i277": Stash(21, 900),
    "m01s00i278": Stash(21, 900),
    "m01s00i279": Stash(21, 900),
    "m01s00i280": Stash(21, 900),
    "m01s00i281": Stash(21, 900),
    "m01s00i282": Stash(21, 900),
    "m01s00i283": Stash(21, 900),
    "m01s00i284": Stash(21, 900),
    "m01s00i285": Stash(21, 900),
    "m01s00i286": Stash(21, 1394),
    "m01s00i287": Stash(21, 1513),
    "m01s00i288": Stash(21, 1513),
    "m01s00i289": Stash(21, 1513),
    "m01s00i290": Stash(1, 1907),
    "m01s00i291": Stash(21, 0),
    "m01s00i292": Stash(21, 0),
    "m01s00i293": Stash(21, 0),
    "m01s00i294": Stash(21, 0),
    "m01s00i295": Stash(21, 0),
    "m01s00i296": Stash(21, 0),
    "m01s00i297": Stash(21, 0),
    "m01s00i298": Stash(21, 0),
    "m01s00i299": Stash(21, 0),
    "m01s00i301": Stash(1, 0),
    "m01s00i302": Stash(1, 0),
    "m01s00i303": Stash(1, 0),
    "m01s00i304": Stash(1, 0),
    "m01s00i305": Stash(1, 0),
    "m01s00i306": Stash(1, 0),
    "m01s00i307": Stash(1, 0),
    "m01s00i308": Stash(1, 0),
    "m01s00i321": Stash(1, 573),
    "m01s00i340": Stash(1, 520),
    "m01s00i341": Stash(22, 453),
    "m01s00i342": Stash(1, 0),
    "m01s00i343": Stash(1, 2001),
    "m01s00i344": Stash(1, 2002),
    "m01s00i345": Stash(1, 1801),
    "m01s00i346": Stash(1, 1802),
    "m01s00i347": Stash(1, 1803),
    "m01s00i351": Stash(1, 385),
    "m01s00i352": Stash(1, 386),
    "m01s00i353": Stash(1, 387),
    "m01s00i354": Stash(1, 388),
    "m01s00i355": Stash(1, 389),
    "m01s00i356": Stash(1, 390),
    "m01s00i357": Stash(1, 391),
    "m01s00i358": Stash(1, 392),
    "m01s00i359": Stash(1, 393),
    "m01s00i360": Stash(1, 394),
    "m01s00i361": Stash(1, 395),
    "m01s00i362": Stash(1, 396),
    "m01s00i363": Stash(1, 397),
    "m01s00i364": Stash(1, 398),
    "m01s00i365": Stash(1, 399),
    "m01s00i366": Stash(1, 400),
    "m01s00i367": Stash(1, 401),
    "m01s00i368": Stash(1, 402),
    "m01s00i369": Stash(1, 403),
    "m01s00i370": Stash(1, 404),
    "m01s00i371": Stash(1, 405),
    "m01s00i376": Stash(21, 0),
    "m01s00i377": Stash(21, 0),
    "m01s00i378": Stash(21, 0),
    "m01s00i379": Stash(21, 0),
    "m01s00i380": Stash(21, 0),
    "m01s00i381": Stash(21, 0),
    "m01s00i382": Stash(21, 0),
    "m01s00i383": Stash(21, 0),
    "m01s00i384": Stash(21, 0),
    "m01s00i385": Stash(21, 0),
    "m01s00i386": Stash(21, 0),
    "m01s00i387": Stash(1, 43),
    "m01s00i388": Stash(1, 19),
    "m01s00i389": Stash(1, 27),
    "m01s00i390": Stash(1, 0),
    "m01s00i391": Stash(1, 101),
    "m01s00i392": Stash(1, 101),
    "m01s00i393": Stash(1, 101),
    "m01s00i394": Stash(1, 101),
    "m01s00i395": Stash(1, 101),
    "m01s00i396": Stash(1, 101),
    "m01s00i397": Stash(1, 0),
    "m01s00i398": Stash(1, 7),
    "m01s00i401": Stash(1, 7),
    "m01s00i402": Stash(1, 79),
    "m01s00i403": Stash(1, 78),
    "m01s00i404": Stash(1, 1731),
    "m01s00i405": Stash(21, 106),
    "m01s00i406": Stash(1, 7),
    "m01s00i407": Stash(1, 8),
    "m01s00i408": Stash(1, 8),
    "m01s00i409": Stash(1, 8),
    "m01s00i410": Stash(1, 0),
    "m01s00i411": Stash(1, 0),
    "m01s00i412": Stash(1, 0),
    "m01s00i413": Stash(1, 1411),
    "m01s00i414": Stash(1, 1410),
    "m01s00i415": Stash(1, 1413),
    "m01s00i416": Stash(1, 1412),
    "m01s00i418": Stash(1, 1630),
    "m01s00i419": Stash(1, 1631),
    "m01s00i420": Stash(1, 1632),
    "m01s00i421": Stash(1, 1633),
    "m01s00i422": Stash(1, 1633),
    "m01s00i423": Stash(1, 1633),
    "m01s00i424": Stash(1, 1633),
    "m01s00i425": Stash(1, 1633),
    "m01s00i426": Stash(1, 1633),
    "m01s00i431": Stash(1, 1634),
    "m01s00i432": Stash(1, 1634),
    "m01s00i433": Stash(1, 1634),
    "m01s00i434": Stash(1, 1634),
    "m01s00i435": Stash(1, 1634),
    "m01s00i436": Stash(1, 1634),
    "m01s00i460": Stash(1, 902),
    "m01s00i466": Stash(21, 1397),
    "m01s00i467": Stash(21, 1397),
    "m01s00i468": Stash(21, 1397),
    "m01s00i469": Stash(21, 1397),
    "m01s00i470": Stash(21, 1503),
    "m01s00i471": Stash(21, 1503),
    "m01s00i472": Stash(21, 1503),
    "m01s00i473": Stash(21, 1503),
    "m01s00i480": Stash(1, 1080),
    "m01s00i481": Stash(22, 1081),
    "m01s00i482": Stash(22, 1082),
    "m01s00i483": Stash(22, 1083),
    "m01s00i484": Stash(22, 1084),
    "m01s00i485": Stash(22, 1085),
    "m01s00i486": Stash(22, 1086),
    "m01s00i487": Stash(22, 1087),
    "m01s00i490": Stash(21, 1137),
    "m01s00i491": Stash(1, 1138),
    "m01s00i492": Stash(1, 1139),
    "m01s00i493": Stash(1, 40),
    "m01s00i494": Stash(21, 0),
    "m01s00i495": Stash(21, 0),
    "m01s00i496": Stash(21, 0),
    "m01s00i497": Stash(21, 0),
    "m01s00i498": Stash(21, 0),
    "m01s00i499": Stash(21, 0),
    "m01s00i500": Stash(21, 0),
    "m01s00i501": Stash(21, 0),
    "m01s00i502": Stash(21, 0),
    "m01s00i505": Stash(21, 395),
    "m01s00i506": Stash(1, 16),
    "m01s00i507": Stash(1, 16),
    "m01s00i508": Stash(1, 16),
    "m01s00i509": Stash(1, 322),
    "m01s00i510": Stash(1, 322),
    "m01s00i511": Stash(21, 1907),
    "m01s01i004": Stash(1, 16),
    "m01s01i101": Stash(1, 0),
    "m01s01i102": Stash(1, 0),
    "m01s01i103": Stash(1, 0),
    "m01s01i104": Stash(1, 0),
    "m01s01i105": Stash(1, 0),
    "m01s01i106": Stash(1, 0),
    "m01s01i107": Stash(1, 0),
    "m01s01i108": Stash(1, 0),
    "m01s01i109": Stash(1, 0),
    "m01s01i110": Stash(1, 0),
    "m01s01i111": Stash(1, 0),
    "m01s01i112": Stash(1, 0),
    "m01s01i113": Stash(1, 0),
    "m01s01i114": Stash(1, 0),
    "m01s01i115": Stash(1, 0),
    "m01s01i116": Stash(1, 0),
    "m01s01i161": Stash(1, 0),
    "m01s01i181": Stash(1, 0),
    "m01s01i182": Stash(1, 0),
    "m01s01i183": Stash(1, 0),
    "m01s01i192": Stash(1, 0),
    "m01s01i193": Stash(1, 0),
    "m01s01i194": Stash(1, 0),
    "m01s01i195": Stash(1, 0),
    "m01s01i198": Stash(1, 0),
    "m01s01i199": Stash(1, 0),
    "m01s01i201": Stash(1, 186),
    "m01s01i202": Stash(1, 186),
    "m01s01i203": Stash(1, 186),
    "m01s01i204": Stash(1, 186),
    "m01s01i205": Stash(1, 201),
    "m01s01i206": Stash(1, 328),
    "m01s01i207": Stash(1, 200),
    "m01s01i208": Stash(1, 201),
    "m01s01i209": Stash(1, 207),
    "m01s01i210": Stash(1, 208),
    "m01s01i211": Stash(1, 207),
    "m01s01i212": Stash(1, 1027),
    "m01s01i213": Stash(1, 1028),
    "m01s01i214": Stash(1, 1029),
    "m01s01i215": Stash(1, 186),
    "m01s01i216": Stash(1, 186),
    "m01s01i217": Stash(1, 0),
    "m01s01i218": Stash(1, 0),
    "m01s01i219": Stash(1, 0),
    "m01s01i220": Stash(1, 0),
    "m01s01i221": Stash(1, 164),
    "m01s01i222": Stash(1, 165),
    "m01s01i223": Stash(1, 166),
    "m01s01i224": Stash(1, 167),
    "m01s01i225": Stash(1, 163),
    "m01s01i226": Stash(1, 165),
    "m01s01i230": Stash(1, 0),
    "m01s01i231": Stash(1, 0),
    "m01s01i232": Stash(1, 251),
    "m01s01i233": Stash(1, 252),
    "m01s01i234": Stash(1, 1376),
    "m01s01i235": Stash(1, 203),
    "m01s01i236": Stash(1, 1377),
    "m01s01i237": Stash(1, 186),
    "m01s01i238": Stash(1, 201),
    "m01s01i241": Stash(1, 1464),
    "m01s01i242": Stash(1, 1465),
    "m01s01i243": Stash(1, 1466),
    "m01s01i244": Stash(1, 1467),
    "m01s01i245": Stash(1, 1468),
    "m01s01i246": Stash(1, 1469),
    "m01s01i247": Stash(1, 1470),
    "m01s01i248": Stash(1, 1471),
    "m01s01i254": Stash(1, 1805),
    "m01s01i255": Stash(1, 1806),
    "m01s01i257": Stash(1, 186),
    "m01s01i258": Stash(1, 186),
    "m01s01i259": Stash(1, 186),
    "m01s01i260": Stash(1, 186),
    "m01s01i262": Stash(1, 1714),
    "m01s01i263": Stash(1, 1715),
    "m01s01i264": Stash(1, 1721),
    "m01s01i265": Stash(1, 1722),
    "m01s01i266": Stash(1, 1723),
    "m01s01i267": Stash(1, 1724),
    "m01s01i268": Stash(1, 322),
    "m01s01i269": Stash(1, 322),
    "m01s01i270": Stash(1, 322),
    "m01s01i271": Stash(1, 322),
    "m01s01i280": Stash(1, 1807),
    "m01s01i281": Stash(1, 1808),
    "m01s01i288": Stash(1, 203),
    "m01s01i289": Stash(1, 208),
    "m01s01i290": Stash(1, 1891),
    "m01s01i291": Stash(1, 1892),
    "m01s01i292": Stash(1, 1030),
    "m01s01i293": Stash(1, 1031),
    "m01s01i294": Stash(1, 1032),
    "m01s01i295": Stash(1, 1033),
    "m01s01i296": Stash(1, 1034),
    "m01s01i408": Stash(1, 201),
    "m01s01i409": Stash(1, 207),
    "m01s01i410": Stash(1, 208),
    "m01s01i411": Stash(1, 207),
    "m01s01i417": Stash(1, 201),
    "m01s01i418": Stash(1, 203),
    "m01s01i419": Stash(1, 207),
    "m01s01i420": Stash(1, 208),
    "m01s01i433": Stash(1, 252),
    "m01s01i435": Stash(1, 203),
    "m01s01i437": Stash(1, 186),
    "m01s01i438": Stash(1, 201),
    "m01s02i004": Stash(1, 16),
    "m01s02i101": Stash(1, 0),
    "m01s02i161": Stash(1, 0),
    "m01s02i181": Stash(1, 0),
    "m01s02i182": Stash(1, 0),
    "m01s02i183": Stash(1, 0),
    "m01s02i192": Stash(1, 0),
    "m01s02i193": Stash(1, 0),
    "m01s02i194": Stash(1, 0),
    "m01s02i195": Stash(1, 0),
    "m01s02i198": Stash(1, 0),
    "m01s02i199": Stash(1, 0),
    "m01s02i201": Stash(1, 187),
    "m01s02i202": Stash(1, 187),
    "m01s02i203": Stash(1, 187),
    "m01s02i204": Stash(1, 30),
    "m01s02i205": Stash(1, 206),
    "m01s02i206": Stash(1, 210),
    "m01s02i207": Stash(1, 205),
    "m01s02i208": Stash(1, 211),
    "m01s02i217": Stash(1, 0),
    "m01s02i218": Stash(1, 0),
    "m01s02i219": Stash(1, 0),
    "m01s02i220": Stash(1, 0),
    "m01s02i232": Stash(1, 253),
    "m01s02i233": Stash(1, 254),
    "m01s02i237": Stash(1, 187),
    "m01s02i238": Stash(1, 205),
    "m01s02i251": Stash(1, 0),
    "m01s02i252": Stash(1, 0),
    "m01s02i253": Stash(1, 0),
    "m01s02i254": Stash(1, 0),
    "m01s02i255": Stash(1, 0),
    "m01s02i256": Stash(1, 0),
    "m01s02i260": Stash(1, 453),
    "m01s02i261": Stash(1, 1720),
    "m01s02i262": Stash(1, 1718),
    "m01s02i263": Stash(1, 1719),
    "m01s02i264": Stash(1, 1725),
    "m01s02i265": Stash(1, 1726),
    "m01s02i266": Stash(1, 1727),
    "m01s02i267": Stash(1, 1728),
    "m01s02i269": Stash(1, 30),
    "m01s02i270": Stash(1, 30),
    "m01s02i271": Stash(1, 30),
    "m01s02i272": Stash(1, 30),
    "m01s02i273": Stash(1, 30),
    "m01s02i274": Stash(1, 30),
    "m01s02i275": Stash(1, 30),
    "m01s02i276": Stash(1, 30),
    "m01s02i277": Stash(1, 30),
    "m01s02i280": Stash(1, 1930),
    "m01s02i281": Stash(1, 1931),
    "m01s02i282": Stash(1, 1932),
    "m01s02i283": Stash(1, 1932),
    "m01s02i284": Stash(1, 0),
    "m01s02i285": Stash(1, 0),
    "m01s02i286": Stash(1, 0),
    "m01s02i287": Stash(1, 0),
    "m01s02i288": Stash(1, 0),
    "m01s02i289": Stash(1, 0),
    "m01s02i290": Stash(1, 30),
    "m01s02i291": Stash(1, 30),
    "m01s02i292": Stash(1, 30),
    "m01s02i293": Stash(1, 30),
    "m01s02i295": Stash(1, 0),
    "m01s02i296": Stash(1, 0),
    "m01s02i297": Stash(1, 0),
    "m01s02i298": Stash(1, 0),
    "m01s02i299": Stash(1, 0),
    "m01s02i300": Stash(1, 0),
    "m01s02i301": Stash(1, 0),
    "m01s02i302": Stash(1, 0),
    "m01s02i303": Stash(1, 0),
    "m01s02i304": Stash(1, 0),
    "m01s02i305": Stash(1, 0),
    "m01s02i308": Stash(1, 79),
    "m01s02i309": Stash(1, 78),
    "m01s02i310": Stash(1, 79),
    "m01s02i311": Stash(1, 78),
    "m01s02i312": Stash(1, 1425),
    "m01s02i313": Stash(1, 1426),
    "m01s02i314": Stash(1, 1425),
    "m01s02i315": Stash(1, 1426),
    "m01s02i320": Stash(1, 0),
    "m01s02i321": Stash(1, 0),
    "m01s02i322": Stash(1, 0),
    "m01s02i323": Stash(1, 0),
    "m01s02i324": Stash(1, 0),
    "m01s02i325": Stash(1, 0),
    "m01s02i330": Stash(1, 0),
    "m01s02i331": Stash(1, 0),
    "m01s02i332": Stash(1, 0),
    "m01s02i333": Stash(1, 0),
    "m01s02i334": Stash(1, 0),
    "m01s02i335": Stash(1, 0),
    "m01s02i336": Stash(1, 0),
    "m01s02i337": Stash(1, 30),
    "m01s02i340": Stash(1, 0),
    "m01s02i341": Stash(1, 0),
    "m01s02i342": Stash(1, 0),
    "m01s02i343": Stash(1, 30),
    "m01s02i344": Stash(1, 30),
    "m01s02i345": Stash(1, 30),
    "m01s02i346": Stash(1, 30),
    "m01s02i347": Stash(1, 30),
    "m01s02i348": Stash(1, 0),
    "m01s02i349": Stash(1, 30),
    "m01s02i350": Stash(1, 0),
    "m01s02i351": Stash(1, 0),
    "m01s02i352": Stash(1, 0),
    "m01s02i353": Stash(1, 0),
    "m01s02i354": Stash(1, 0),
    "m01s02i355": Stash(1, 0),
    "m01s02i356": Stash(1, 0),
    "m01s02i360": Stash(1, 30),
    "m01s02i370": Stash(1, 0),
    "m01s02i371": Stash(1, 30),
    "m01s02i372": Stash(1, 0),
    "m01s02i373": Stash(1, 0),
    "m01s02i374": Stash(1, 30),
    "m01s02i375": Stash(1, 0),
    "m01s02i376": Stash(1, 0),
    "m01s02i377": Stash(1, 0),
    "m01s02i378": Stash(1, 0),
    "m01s02i380": Stash(1, 0),
    "m01s02i381": Stash(1, 0),
    "m01s02i382": Stash(1, 0),
    "m01s02i383": Stash(1, 0),
    "m01s02i384": Stash(1, 0),
    "m01s02i385": Stash(1, 0),
    "m01s02i386": Stash(1, 0),
    "m01s02i387": Stash(1, 0),
    "m01s02i388": Stash(1, 0),
    "m01s02i389": Stash(1, 1697),
    "m01s02i390": Stash(1, 1698),
    "m01s02i406": Stash(1, 210),
    "m01s02i407": Stash(1, 205),
    "m01s02i408": Stash(1, 211),
    "m01s02i417": Stash(1, 206),
    "m01s02i418": Stash(1, 205),
    "m01s02i419": Stash(1, 210),
    "m01s02i420": Stash(1, 211),
    "m01s02i421": Stash(1, 0),
    "m01s02i422": Stash(1, 0),
    "m01s02i423": Stash(1, 0),
    "m01s02i424": Stash(1, 0),
    "m01s02i425": Stash(1, 0),
    "m01s02i426": Stash(1, 0),
    "m01s02i427": Stash(1, 0),
    "m01s02i433": Stash(1, 254),
    "m01s02i437": Stash(1, 187),
    "m01s02i438": Stash(1, 205),
    "m01s03i002": Stash(18, 56),
    "m01s03i003": Stash(19, 57),
    "m01s03i004": Stash(1, 16),
    "m01s03i010": Stash(1, 95),
    "m01s03i024": Stash(1, 16),
    "m01s03i025": Stash(1, 5),
    "m01s03i026": Stash(1, 324),
    "m01s03i027": Stash(1, 324),
    "m01s03i028": Stash(1, 1075),
    "m01s03i049": Stash(3, 209),
    "m01s03i050": Stash(1, 0),
    "m01s03i051": Stash(2, 0),
    "m01s03i052": Stash(2, 0),
    "m01s03i053": Stash(2, 0),
    "m01s03i054": Stash(21, 1416),
    "m01s03i055": Stash(2, 0),
    "m01s03i056": Stash(2, 0),
    "m01s03i057": Stash(2, 0),
    "m01s03i058": Stash(2, 0),
    "m01s03i060": Stash(1, 0),
    "m01s03i061": Stash(1, 0),
    "m01s03i062": Stash(1, 0),
    "m01s03i063": Stash(1, 0),
    "m01s03i064": Stash(1, 0),
    "m01s03i065": Stash(1, 0),
    "m01s03i066": Stash(1, 0),
    "m01s03i067": Stash(1, 0),
    "m01s03i068": Stash(1, 0),
    "m01s03i069": Stash(1, 0),
    "m01s03i070": Stash(1, 0),
    "m01s03i071": Stash(1, 0),
    "m01s03i072": Stash(1, 0),
    "m01s03i073": Stash(1, 0),
    "m01s03i074": Stash(1, 0),
    "m01s03i075": Stash(1, 0),
    "m01s03i076": Stash(1, 0),
    "m01s03i077": Stash(1, 0),
    "m01s03i078": Stash(1, 0),
    "m01s03i079": Stash(1, 0),
    "m01s03i100": Stash(1, 1301),
    "m01s03i101": Stash(1, 1302),
    "m01s03i102": Stash(1, 1303),
    "m01s03i103": Stash(1, 1304),
    "m01s03i104": Stash(1, 1305),
    "m01s03i105": Stash(1, 1306),
    "m01s03i106": Stash(1, 1307),
    "m01s03i107": Stash(1, 1308),
    "m01s03i108": Stash(1, 1309),
    "m01s03i109": Stash(1, 1310),
    "m01s03i110": Stash(1, 1311),
    "m01s03i111": Stash(1, 1312),
    "m01s03i112": Stash(1, 1313),
    "m01s03i113": Stash(1, 1314),
    "m01s03i114": Stash(1, 1315),
    "m01s03i115": Stash(1, 1316),
    "m01s03i116": Stash(1, 1317),
    "m01s03i117": Stash(1, 1318),
    "m01s03i118": Stash(1, 1319),
    "m01s03i119": Stash(1, 1320),
    "m01s03i120": Stash(1, 1321),
    "m01s03i121": Stash(1, 1322),
    "m01s03i122": Stash(1, 1323),
    "m01s03i123": Stash(1, 1324),
    "m01s03i124": Stash(1, 1325),
    "m01s03i125": Stash(1, 1326),
    "m01s03i126": Stash(1, 1327),
    "m01s03i127": Stash(1, 1328),
    "m01s03i128": Stash(1, 1329),
    "m01s03i129": Stash(1, 1330),
    "m01s03i130": Stash(1, 2042),
    "m01s03i131": Stash(1, 2043),
    "m01s03i132": Stash(1, 2044),
    "m01s03i133": Stash(1, 2045),
    "m01s03i134": Stash(1, 2046),
    "m01s03i135": Stash(1, 2047),
    "m01s03i136": Stash(1, 2048),
    "m01s03i137": Stash(1, 2049),
    "m01s03i138": Stash(1, 2050),
    "m01s03i139": Stash(1, 2051),
    "m01s03i140": Stash(1, 2052),
    "m01s03i141": Stash(1, 2053),
    "m01s03i142": Stash(1, 2054),
    "m01s03i143": Stash(1, 2055),
    "m01s03i170": Stash(1, 0),
    "m01s03i171": Stash(1, 0),
    "m01s03i172": Stash(1, 0),
    "m01s03i173": Stash(1, 0),
    "m01s03i176": Stash(1, 0),
    "m01s03i177": Stash(1, 0),
    "m01s03i178": Stash(1, 0),
    "m01s03i179": Stash(1, 0),
    "m01s03i181": Stash(1, 0),
    "m01s03i182": Stash(1, 0),
    "m01s03i183": Stash(1, 0),
    "m01s03i184": Stash(1, 0),
    "m01s03i185": Stash(18, 0),
    "m01s03i186": Stash(19, 0),
    "m01s03i188": Stash(1, 0),
    "m01s03i189": Stash(1, 0),
    "m01s03i190": Stash(1, 0),
    "m01s03i192": Stash(1, 0),
    "m01s03i193": Stash(1, 0),
    "m01s03i194": Stash(1, 0),
    "m01s03i201": Stash(1, 261),
    "m01s03i202": Stash(2, 179),
    "m01s03i203": Stash(1, 160),
    "m01s03i204": Stash(1, 183),
    "m01s03i205": Stash(1, 52),
    "m01s03i206": Stash(11, 172),
    "m01s03i207": Stash(1, 171),
    "m01s03i208": Stash(1, 181),
    "m01s03i209": Stash(18, 56),
    "m01s03i210": Stash(19, 57),
    "m01s03i216": Stash(1, 178),
    "m01s03i217": Stash(1, 178),
    "m01s03i219": Stash(18, 61),
    "m01s03i220": Stash(19, 62),
    "m01s03i221": Stash(11, 1900),
    "m01s03i222": Stash(1, 184),
    "m01s03i223": Stash(1, 184),
    "m01s03i224": Stash(1, 182),
    "m01s03i225": Stash(11, 56),
    "m01s03i226": Stash(11, 57),
    "m01s03i227": Stash(11, 50),
    "m01s03i228": Stash(1, 178),
    "m01s03i229": Stash(2, 115),
    "m01s03i230": Stash(1, 183),
    "m01s03i231": Stash(1, 107),
    "m01s03i232": Stash(1, 285),
    "m01s03i234": Stash(1, 180),
    "m01s03i235": Stash(1, 260),
    "m01s03i236": Stash(1, 16),
    "m01s03i237": Stash(1, 95),
    "m01s03i238": Stash(2, 23),
    "m01s03i239": Stash(1, 79),
    "m01s03i240": Stash(1, 78),
    "m01s03i241": Stash(1, 184),
    "m01s03i242": Stash(1, 220),
    "m01s03i243": Stash(1, 79),
    "m01s03i244": Stash(1, 78),
    "m01s03i245": Stash(1, 88),
    "m01s03i247": Stash(1, 25),
    "m01s03i248": Stash(1, 220),
    "m01s03i249": Stash(11, 50),
    "m01s03i250": Stash(1, 17),
    "m01s03i251": Stash(1, 174),
    "m01s03i252": Stash(1, 175),
    "m01s03i253": Stash(1, 173),
    "m01s03i254": Stash(1, 19),
    "m01s03i255": Stash(1, 95),
    "m01s03i256": Stash(1, 1414),
    "m01s03i257": Stash(1, 1415),
    "m01s03i258": Stash(2, 141),
    "m01s03i259": Stash(2, 1384),
    "m01s03i260": Stash(2, 1387),
    "m01s03i261": Stash(2, 1388),
    "m01s03i262": Stash(2, 1389),
    "m01s03i263": Stash(2, 1390),
    "m01s03i264": Stash(2, 1382),
    "m01s03i265": Stash(2, 1383),
    "m01s03i270": Stash(1, 1400),
    "m01s03i271": Stash(1, 1401),
    "m01s03i272": Stash(1, 1402),
    "m01s03i273": Stash(1, 1403),
    "m01s03i274": Stash(1, 1542),
    "m01s03i275": Stash(1, 1543),
    "m01s03i276": Stash(1, 1406),
    "m01s03i277": Stash(1, 1407),
    "m01s03i278": Stash(1, 1408),
    "m01s03i279": Stash(1, 1409),
    "m01s03i280": Stash(1, 1410),
    "m01s03i281": Stash(1, 25),
    "m01s03i282": Stash(1, 220),
    "m01s03i283": Stash(1, 173),
    "m01s03i284": Stash(1, 25),
    "m01s03i285": Stash(1, 25),
    "m01s03i286": Stash(1, 1416),
    "m01s03i287": Stash(2, 1517),
    "m01s03i288": Stash(2, 1518),
    "m01s03i289": Stash(2, 1519),
    "m01s03i290": Stash(2, 1520),
    "m01s03i291": Stash(2, 1521),
    "m01s03i292": Stash(2, 1522),
    "m01s03i293": Stash(2, 1523),
    "m01s03i294": Stash(1, 1524),
    "m01s03i295": Stash(2, 1525),
    "m01s03i296": Stash(2, 1526),
    "m01s03i297": Stash(2, 1527),
    "m01s03i298": Stash(1, 1528),
    "m01s03i299": Stash(2, 1529),
    "m01s03i300": Stash(1, 1541),
    "m01s03i301": Stash(1, 1542),
    "m01s03i302": Stash(1, 1543),
    "m01s03i303": Stash(1, 1544),
    "m01s03i304": Stash(1, 1534),
    "m01s03i305": Stash(1, 1535),
    "m01s03i306": Stash(1, 1536),
    "m01s03i307": Stash(1, 1537),
    "m01s03i308": Stash(1, 1538),
    "m01s03i309": Stash(1, 1539),
    "m01s03i310": Stash(1, 1540),
    "m01s03i311": Stash(2, 1557),
    "m01s03i312": Stash(2, 1558),
    "m01s03i313": Stash(2, 1559),
    "m01s03i314": Stash(2, 202),
    "m01s03i315": Stash(2, 1391),
    "m01s03i316": Stash(2, 1510),
    "m01s03i317": Stash(2, 1893),
    "m01s03i318": Stash(2, 1894),
    "m01s03i319": Stash(2, 1895),
    "m01s03i320": Stash(2, 1397),
    "m01s03i321": Stash(2, 1504),
    "m01s03i322": Stash(2, 1505),
    "m01s03i323": Stash(2, 1508),
    "m01s03i324": Stash(2, 1511),
    "m01s03i325": Stash(2, 1499),
    "m01s03i326": Stash(2, 1562),
    "m01s03i327": Stash(1, 1563),
    "m01s03i328": Stash(2, 16),
    "m01s03i329": Stash(2, 95),
    "m01s03i330": Stash(2, 180),
    "m01s03i331": Stash(2, 1564),
    "m01s03i332": Stash(1, 206),
    "m01s03i333": Stash(1, 202),
    "m01s03i334": Stash(2, 115),
    "m01s03i335": Stash(2, 1517),
    "m01s03i337": Stash(2, 1564),
    "m01s03i338": Stash(1, 1565),
    "m01s03i339": Stash(1, 181),
    "m01s03i340": Stash(1, 1628),
    "m01s03i341": Stash(2, 115),
    "m01s03i342": Stash(2, 115),
    "m01s03i343": Stash(1, 285),
    "m01s03i347": Stash(1, 285),
    "m01s03i353": Stash(1, 1528),
    "m01s03i355": Stash(1, 1699),
    "m01s03i356": Stash(1, 1800),
    "m01s03i357": Stash(1, 1801),
    "m01s03i358": Stash(1, 1802),
    "m01s03i359": Stash(1, 1803),
    "m01s03i360": Stash(1, 1091),
    "m01s03i361": Stash(1, 1092),
    "m01s03i362": Stash(1, 1093),
    "m01s03i363": Stash(1, 1094),
    "m01s03i381": Stash(1, 202),
    "m01s03i389": Stash(1, 52),
    "m01s03i390": Stash(1, 52),
    "m01s03i391": Stash(18, 61),
    "m01s03i392": Stash(18, 61),
    "m01s03i393": Stash(19, 62),
    "m01s03i394": Stash(19, 62),
    "m01s03i395": Stash(1, 202),
    "m01s03i396": Stash(1, 1686),
    "m01s03i397": Stash(1, 1687),
    "m01s03i398": Stash(1, 1688),
    "m01s03i400": Stash(1, 1646),
    "m01s03i401": Stash(1, 1635),
    "m01s03i402": Stash(1, 1635),
    "m01s03i403": Stash(1, 1635),
    "m01s03i404": Stash(1, 1635),
    "m01s03i405": Stash(1, 1635),
    "m01s03i406": Stash(1, 1635),
    "m01s03i407": Stash(1, 1542),
    "m01s03i408": Stash(1, 1543),
    "m01s03i409": Stash(1, 1544),
    "m01s03i411": Stash(1, 1636),
    "m01s03i412": Stash(1, 1636),
    "m01s03i413": Stash(1, 1636),
    "m01s03i414": Stash(1, 1636),
    "m01s03i415": Stash(1, 1636),
    "m01s03i416": Stash(1, 1636),
    "m01s03i417": Stash(1, 1636),
    "m01s03i418": Stash(1, 1636),
    "m01s03i419": Stash(1, 1636),
    "m01s03i421": Stash(1, 1637),
    "m01s03i422": Stash(1, 1637),
    "m01s03i423": Stash(1, 1637),
    "m01s03i424": Stash(1, 1637),
    "m01s03i425": Stash(1, 1637),
    "m01s03i426": Stash(1, 1637),
    "m01s03i427": Stash(1, 1637),
    "m01s03i428": Stash(1, 1637),
    "m01s03i429": Stash(1, 1637),
    "m01s03i430": Stash(1, 1638),
    "m01s03i441": Stash(1, 1639),
    "m01s03i442": Stash(1, 1639),
    "m01s03i443": Stash(1, 1639),
    "m01s03i444": Stash(1, 1639),
    "m01s03i445": Stash(1, 1639),
    "m01s03i446": Stash(1, 1639),
    "m01s03i451": Stash(1, 1640),
    "m01s03i452": Stash(1, 1640),
    "m01s03i453": Stash(1, 1640),
    "m01s03i454": Stash(1, 1640),
    "m01s03i455": Stash(1, 1640),
    "m01s03i456": Stash(1, 1640),
    "m01s03i460": Stash(18, 1025),
    "m01s03i461": Stash(19, 1026),
    "m01s03i462": Stash(1, 1896),
    "m01s03i463": Stash(1, 1694),
    "m01s03i464": Stash(1, 1695),
    "m01s03i465": Stash(1, 1696),
    "m01s03i466": Stash(1, 1015),
    "m01s03i467": Stash(1, 1016),
    "m01s03i468": Stash(1, 1017),
    "m01s03i469": Stash(1, 1018),
    "m01s03i470": Stash(1, 1019),
    "m01s03i471": Stash(1, 1020),
    "m01s03i472": Stash(1, 1021),
    "m01s03i473": Stash(1, 1022),
    "m01s03i474": Stash(18, 1023),
    "m01s03i475": Stash(19, 1024),
    "m01s03i476": Stash(1, 1036),
    "m01s03i477": Stash(2, 1397),
    "m01s03i478": Stash(2, 1397),
    "m01s03i479": Stash(2, 1397),
    "m01s03i480": Stash(2, 1397),
    "m01s03i481": Stash(2, 1503),
    "m01s03i482": Stash(2, 1503),
    "m01s03i483": Stash(2, 1503),
    "m01s03i484": Stash(2, 1503),
    "m01s03i485": Stash(2, 1503),
    "m01s03i486": Stash(2, 1503),
    "m01s03i490": Stash(2, 2040),
    "m01s03i491": Stash(2, 2041),
    "m01s03i492": Stash(2, 2042),
    "m01s03i493": Stash(2, 2043),
    "m01s03i494": Stash(2, 2044),
    "m01s03i501": Stash(1, 1400),
    "m01s03i502": Stash(1, 1401),
    "m01s03i503": Stash(1, 1402),
    "m01s03i504": Stash(1, 1403),
    "m01s03i505": Stash(1, 1404),
    "m01s03i506": Stash(1, 1405),
    "m01s03i507": Stash(1, 1406),
    "m01s03i508": Stash(1, 1407),
    "m01s03i509": Stash(1, 0),
    "m01s03i510": Stash(1, 0),
    "m01s03i511": Stash(1, 0),
    "m01s04i004": Stash(1, 16),
    "m01s04i010": Stash(1, 95),
    "m01s04i100": Stash(1, 0),
    "m01s04i101": Stash(1, 0),
    "m01s04i102": Stash(1, 0),
    "m01s04i103": Stash(1, 0),
    "m01s04i104": Stash(1, 0),
    "m01s04i105": Stash(1, 0),
    "m01s04i130": Stash(1, 0),
    "m01s04i131": Stash(1, 0),
    "m01s04i132": Stash(1, 0),
    "m01s04i133": Stash(1, 0),
    "m01s04i136": Stash(1, 0),
    "m01s04i137": Stash(1, 0),
    "m01s04i138": Stash(1, 0),
    "m01s04i139": Stash(1, 0),
    "m01s04i141": Stash(1, 0),
    "m01s04i142": Stash(1, 0),
    "m01s04i143": Stash(1, 0),
    "m01s04i144": Stash(1, 0),
    "m01s04i152": Stash(1, 0),
    "m01s04i153": Stash(1, 0),
    "m01s04i154": Stash(1, 0),
    "m01s04i170": Stash(1, 0),
    "m01s04i171": Stash(1, 0),
    "m01s04i172": Stash(1, 0),
    "m01s04i173": Stash(1, 0),
    "m01s04i176": Stash(1, 0),
    "m01s04i177": Stash(1, 0),
    "m01s04i178": Stash(1, 0),
    "m01s04i179": Stash(1, 0),
    "m01s04i181": Stash(1, 0),
    "m01s04i182": Stash(1, 0),
    "m01s04i183": Stash(1, 0),
    "m01s04i184": Stash(1, 0),
    "m01s04i189": Stash(1, 1681),
    "m01s04i190": Stash(1, 1921),
    "m01s04i191": Stash(1, 1689),
    "m01s04i192": Stash(1, 0),
    "m01s04i193": Stash(1, 0),
    "m01s04i194": Stash(1, 0),
    "m01s04i201": Stash(1, 102),
    "m01s04i202": Stash(1, 116),
    "m01s04i203": Stash(1, 99),
    "m01s04i204": Stash(1, 118),
    "m01s04i205": Stash(1, 79),
    "m01s04i206": Stash(1, 78),
    "m01s04i207": Stash(1, 88),
    "m01s04i208": Stash(1, 25),
    "m01s04i210": Stash(1, 0),
    "m01s04i211": Stash(1, 0),
    "m01s04i212": Stash(1, 1422),
    "m01s04i213": Stash(1, 1546),
    "m01s04i214": Stash(1, 1546),
    "m01s04i215": Stash(1, 1545),
    "m01s04i216": Stash(1, 1547),
    "m01s04i217": Stash(1, 1548),
    "m01s04i218": Stash(1, 1549),
    "m01s04i219": Stash(1, 1550),
    "m01s04i220": Stash(1, 1546),
    "m01s04i221": Stash(1, 1546),
    "m01s04i222": Stash(1, 99),
    "m01s04i223": Stash(1, 118),
    "m01s04i224": Stash(1, 1573),
    "m01s04i225": Stash(1, 1574),
    "m01s04i227": Stash(1, 1575),
    "m01s04i228": Stash(1, 1546),
    "m01s04i229": Stash(1, 1546),
    "m01s04i231": Stash(1, 1641),
    "m01s04i232": Stash(1, 1641),
    "m01s04i233": Stash(1, 1641),
    "m01s04i234": Stash(1, 1641),
    "m01s04i235": Stash(1, 1641),
    "m01s04i236": Stash(1, 1641),
    "m01s04i237": Stash(1, 1691),
    "m01s04i238": Stash(1, 1692),
    "m01s04i240": Stash(1, 0),
    "m01s04i241": Stash(1, 0),
    "m01s04i242": Stash(1, 0),
    "m01s04i243": Stash(1, 0),
    "m01s04i244": Stash(1, 0),
    "m01s04i245": Stash(1, 0),
    "m01s04i246": Stash(1, 0),
    "m01s04i247": Stash(1, 0),
    "m01s04i248": Stash(1, 0),
    "m01s04i249": Stash(1, 0),
    "m01s04i250": Stash(1, 0),
    "m01s04i251": Stash(1, 0),
    "m01s04i252": Stash(1, 0),
    "m01s04i253": Stash(1, 0),
    "m01s04i254": Stash(1, 0),
    "m01s04i255": Stash(1, 0),
    "m01s04i256": Stash(1, 0),
    "m01s04i257": Stash(1, 0),
    "m01s04i258": Stash(1, 0),
    "m01s04i259": Stash(1, 0),
    "m01s04i260": Stash(1, 0),
    "m01s04i261": Stash(1, 0),
    "m01s04i262": Stash(1, 0),
    "m01s04i263": Stash(1, 0),
    "m01s04i264": Stash(1, 0),
    "m01s04i265": Stash(1, 0),
    "m01s04i266": Stash(1, 0),
    "m01s04i267": Stash(1, 0),
    "m01s04i268": Stash(1, 0),
    "m01s04i269": Stash(1, 0),
    "m01s04i270": Stash(1, 0),
    "m01s04i271": Stash(1, 0),
    "m01s04i281": Stash(1, 0),
    "m01s04i282": Stash(1, 0),
    "m01s04i283": Stash(1, 0),
    "m01s04i292": Stash(1, 0),
    "m01s04i293": Stash(1, 0),
    "m01s05i004": Stash(1, 19),
    "m01s05i010": Stash(1, 95),
    "m01s05i013": Stash(1, 34),
    "m01s05i140": Stash(1, 0),
    "m01s05i141": Stash(1, 0),
    "m01s05i142": Stash(1, 0),
    "m01s05i143": Stash(1, 0),
    "m01s05i146": Stash(1, 0),
    "m01s05i147": Stash(1, 0),
    "m01s05i148": Stash(1, 0),
    "m01s05i149": Stash(1, 0),
    "m01s05i150": Stash(1, 0),
    "m01s05i151": Stash(1, 0),
    "m01s05i152": Stash(1, 0),
    "m01s05i156": Stash(1, 0),
    "m01s05i157": Stash(1, 0),
    "m01s05i158": Stash(1, 0),
    "m01s05i161": Stash(1, 0),
    "m01s05i162": Stash(1, 0),
    "m01s05i163": Stash(1, 0),
    "m01s05i164": Stash(1, 0),
    "m01s05i172": Stash(1, 0),
    "m01s05i173": Stash(1, 0),
    "m01s05i174": Stash(1, 0),
    "m01s05i181": Stash(1, 0),
    "m01s05i182": Stash(1, 0),
    "m01s05i183": Stash(1, 0),
    "m01s05i184": Stash(1, 0),
    "m01s05i185": Stash(18, 0),
    "m01s05i186": Stash(19, 0),
    "m01s05i187": Stash(1, 0),
    "m01s05i188": Stash(1, 0),
    "m01s05i192": Stash(1, 0),
    "m01s05i193": Stash(1, 0),
    "m01s05i194": Stash(1, 0),
    "m01s05i196": Stash(1, 0),
    "m01s05i197": Stash(1, 0),
    "m01s05i201": Stash(1, 94),
    "m01s05i202": Stash(1, 117),
    "m01s05i203": Stash(1, 24),
    "m01s05i204": Stash(1, 95),
    "m01s05i205": Stash(1, 98),
    "m01s05i206": Stash(1, 119),
    "m01s05i207": Stash(1, 34),
    "m01s05i208": Stash(1, 34),
    "m01s05i209": Stash(1, 16),
    "m01s05i210": Stash(1, 34),
    "m01s05i211": Stash(1, 34),
    "m01s05i212": Stash(1, 34),
    "m01s05i213": Stash(1, 218),
    "m01s05i214": Stash(1, 97),
    "m01s05i215": Stash(1, 108),
    "m01s05i216": Stash(1, 90),
    "m01s05i217": Stash(1, 213),
    "m01s05i218": Stash(1, 222),
    "m01s05i219": Stash(1, 223),
    "m01s05i220": Stash(1, 343),
    "m01s05i221": Stash(1, 219),
    "m01s05i222": Stash(1, 344),
    "m01s05i223": Stash(1, 345),
    "m01s05i224": Stash(1, 346),
    "m01s05i225": Stash(1, 347),
    "m01s05i226": Stash(1, 77),
    "m01s05i227": Stash(1, 1697),
    "m01s05i228": Stash(1, 1698),
    "m01s05i229": Stash(1, 1419),
    "m01s05i230": Stash(1, 1420),
    "m01s05i231": Stash(1, 1134),
    "m01s05i232": Stash(1, 1135),
    "m01s05i233": Stash(1, 1482),
    "m01s05i234": Stash(1, 1629),
    "m01s05i235": Stash(18, 56),
    "m01s05i236": Stash(19, 57),
    "m01s05i237": Stash(1, 1551),
    "m01s05i238": Stash(1, 1553),
    "m01s05i239": Stash(1, 1554),
    "m01s05i240": Stash(1, 1555),
    "m01s05i241": Stash(1, 1556),
    "m01s05i242": Stash(1, 1552),
    "m01s05i243": Stash(1, 1693),
    "m01s05i244": Stash(1, 1552),
    "m01s05i246": Stash(1, 40),
    "m01s05i247": Stash(1, 1552),
    "m01s05i248": Stash(1, 1552),
    "m01s05i249": Stash(1, 40),
    "m01s05i250": Stash(1, 40),
    "m01s05i251": Stash(1, 40),
    "m01s05i252": Stash(1, 74),
    "m01s05i253": Stash(1, 74),
    "m01s05i254": Stash(1, 74),
    "m01s05i255": Stash(1, 74),
    "m01s05i256": Stash(1, 318),
    "m01s05i257": Stash(1, 319),
    "m01s05i258": Stash(1, 61),
    "m01s05i259": Stash(1, 62),
    "m01s05i260": Stash(1, 61),
    "m01s05i261": Stash(1, 62),
    "m01s05i262": Stash(1, 34),
    "m01s05i263": Stash(1, 61),
    "m01s05i264": Stash(1, 62),
    "m01s05i267": Stash(1, 0),
    "m01s05i268": Stash(1, 0),
    "m01s05i269": Stash(1, 0),
    "m01s05i270": Stash(1, 1598),
    "m01s05i271": Stash(1, 1599),
    "m01s05i272": Stash(1, 1934),
    "m01s05i273": Stash(1, 1935),
    "m01s05i274": Stash(1, 1936),
    "m01s05i275": Stash(1, 132),
    "m01s05i276": Stash(1, 1937),
    "m01s05i277": Stash(1, 90),
    "m01s05i278": Stash(1, 90),
    "m01s05i279": Stash(1, 90),
    "m01s05i280": Stash(1, 90),
    "m01s05i281": Stash(1, 1642),
    "m01s05i282": Stash(1, 1642),
    "m01s05i283": Stash(1, 1642),
    "m01s05i284": Stash(1, 1642),
    "m01s05i285": Stash(1, 1642),
    "m01s05i286": Stash(1, 1642),
    "m01s05i290": Stash(1, 1001),
    "m01s05i291": Stash(1, 1002),
    "m01s05i292": Stash(1, 1003),
    "m01s05i293": Stash(1, 1004),
    "m01s05i300": Stash(1, 1005),
    "m01s05i301": Stash(1, 1006),
    "m01s05i302": Stash(1, 1007),
    "m01s05i303": Stash(1, 1007),
    "m01s05i304": Stash(1, 1001),
    "m01s05i305": Stash(1, 1003),
    "m01s05i306": Stash(1, 1001),
    "m01s05i307": Stash(1, 1003),
    "m01s05i308": Stash(1, 1011),
    "m01s05i309": Stash(1, 1012),
    "m01s05i310": Stash(1, 1008),
    "m01s05i311": Stash(1, 1009),
    "m01s05i312": Stash(1, 1010),
    "m01s05i313": Stash(1, 1013),
    "m01s05i314": Stash(1, 1014),
    "m01s05i319": Stash(1, 0),
    "m01s05i320": Stash(1, 40),
    "m01s05i321": Stash(1, 40),
    "m01s05i322": Stash(1, 40),
    "m01s05i323": Stash(1, 40),
    "m01s05i324": Stash(1, 0),
    "m01s05i325": Stash(1, 0),
    "m01s05i326": Stash(1, 0),
    "m01s05i327": Stash(1, 0),
    "m01s05i328": Stash(1, 0),
    "m01s05i329": Stash(1, 0),
    "m01s05i330": Stash(1, 0),
    "m01s05i331": Stash(1, 0),
    "m01s05i332": Stash(1, 318),
    "m01s05i333": Stash(1, 318),
    "m01s05i334": Stash(1, 318),
    "m01s05i335": Stash(1, 318),
    "m01s05i336": Stash(1, 319),
    "m01s05i337": Stash(1, 319),
    "m01s05i338": Stash(1, 319),
    "m01s05i339": Stash(1, 319),
    "m01s05i400": Stash(1, 0),
    "m01s05i401": Stash(1, 0),
    "m01s05i402": Stash(1, 0),
    "m01s05i403": Stash(1, 0),
    "m01s05i404": Stash(1, 0),
    "m01s05i405": Stash(1, 0),
    "m01s05i406": Stash(1, 0),
    "m01s05i407": Stash(1, 0),
    "m01s05i408": Stash(1, 0),
    "m01s05i409": Stash(1, 0),
    "m01s05i410": Stash(1, 0),
    "m01s05i411": Stash(1, 0),
    "m01s05i412": Stash(1, 0),
    "m01s05i413": Stash(1, 0),
    "m01s05i414": Stash(1, 0),
    "m01s05i415": Stash(1, 0),
    "m01s05i416": Stash(1, 0),
    "m01s05i417": Stash(1, 40),
    "m01s05i418": Stash(1, 40),
    "m01s05i419": Stash(1, 40),
    "m01s05i421": Stash(1, 0),
    "m01s05i422": Stash(1, 0),
    "m01s05i423": Stash(1, 0),
    "m01s05i424": Stash(1, 0),
    "m01s05i425": Stash(1, 0),
    "m01s05i426": Stash(1, 0),
    "m01s05i427": Stash(1, 0),
    "m01s05i428": Stash(1, 0),
    "m01s05i429": Stash(1, 0),
    "m01s05i430": Stash(1, 0),
    "m01s05i431": Stash(1, 0),
    "m01s05i432": Stash(1, 0),
    "m01s06i002": Stash(18, 56),
    "m01s06i003": Stash(19, 57),
    "m01s06i101": Stash(18, 420),
    "m01s06i102": Stash(19, 421),
    "m01s06i103": Stash(18, 422),
    "m01s06i104": Stash(19, 423),
    "m01s06i105": Stash(18, 424),
    "m01s06i106": Stash(19, 425),
    "m01s06i111": Stash(11, 1095),
    "m01s06i113": Stash(11, 1096),
    "m01s06i115": Stash(11, 1097),
    "m01s06i181": Stash(1, 0),
    "m01s06i185": Stash(18, 0),
    "m01s06i186": Stash(19, 0),
    "m01s06i201": Stash(18, 61),
    "m01s06i202": Stash(19, 62),
    "m01s06i203": Stash(1, 150),
    "m01s06i204": Stash(1, 152),
    "m01s06i205": Stash(1, 153),
    "m01s06i206": Stash(1, 154),
    "m01s06i207": Stash(18, 68),
    "m01s06i208": Stash(19, 69),
    "m01s06i209": Stash(18, 70),
    "m01s06i210": Stash(19, 71),
    "m01s06i211": Stash(18, 124),
    "m01s06i212": Stash(19, 124),
    "m01s06i213": Stash(11, 125),
    "m01s06i214": Stash(1, 1579),
    "m01s06i215": Stash(1, 1580),
    "m01s06i216": Stash(1, 1581),
    "m01s06i217": Stash(1, 1582),
    "m01s06i218": Stash(1, 1583),
    "m01s06i219": Stash(1, 1584),
    "m01s06i220": Stash(1, 1585),
    "m01s06i221": Stash(1, 1586),
    "m01s06i222": Stash(1, 1587),
    "m01s06i223": Stash(18, 1588),
    "m01s06i224": Stash(19, 1589),
    "m01s06i225": Stash(18, 1590),
    "m01s06i226": Stash(19, 1591),
    "m01s06i227": Stash(18, 1592),
    "m01s06i228": Stash(19, 1593),
    "m01s06i229": Stash(18, 1594),
    "m01s06i230": Stash(19, 1595),
    "m01s06i231": Stash(18, 1596),
    "m01s06i232": Stash(19, 1597),
    "m01s06i233": Stash(1, 0),
    "m01s06i234": Stash(1, 0),
    "m01s06i235": Stash(18, 61),
    "m01s06i236": Stash(19, 62),
    "m01s06i237": Stash(1, 0),
    "m01s06i241": Stash(11, 1098),
    "m01s06i247": Stash(11, 1099),
    "m01s06i248": Stash(1, 0),
    "m01s06i249": Stash(1, 0),
    "m01s06i250": Stash(1, 0),
    "m01s07i002": Stash(11, 56),
    "m01s07i003": Stash(11, 57),
    "m01s07i201": Stash(11, 195),
    "m01s07i202": Stash(11, 196),
    "m01s08i023": Stash(1, 93),
    "m01s08i024": Stash(1, 16),
    "m01s08i201": Stash(2, 110),
    "m01s08i202": Stash(2, 197),
    "m01s08i203": Stash(2, 274),
    "m01s08i204": Stash(2, 111),
    "m01s08i205": Stash(2, 112),
    "m01s08i207": Stash(2, 338),
    "m01s08i208": Stash(2, 106),
    "m01s08i209": Stash(2, 271),
    "m01s08i210": Stash(2, 329),
    "m01s08i211": Stash(2, 330),
    "m01s08i212": Stash(2, 331),
    "m01s08i213": Stash(2, 332),
    "m01s08i214": Stash(2, 333),
    "m01s08i215": Stash(2, 334),
    "m01s08i216": Stash(2, 335),
    "m01s08i217": Stash(2, 336),
    "m01s08i218": Stash(2, 326),
    "m01s08i219": Stash(2, 321),
    "m01s08i220": Stash(1, 323),
    "m01s08i221": Stash(1, 325),
    "m01s08i222": Stash(1, 327),
    "m01s08i223": Stash(2, 122),
    "m01s08i224": Stash(1, 342),
    "m01s08i225": Stash(2, 23),
    "m01s08i226": Stash(2, 141),
    "m01s08i229": Stash(2, 1385),
    "m01s08i230": Stash(2, 1386),
    "m01s08i231": Stash(2, 1530),
    "m01s08i233": Stash(2, 1531),
    "m01s08i234": Stash(2, 1532),
    "m01s08i235": Stash(2, 1533),
    "m01s08i236": Stash(2, 1566),
    "m01s08i237": Stash(2, 1567),
    "m01s08i238": Stash(2, 1568),
    "m01s08i239": Stash(2, 1534),
    "m01s08i240": Stash(2, 1535),
    "m01s08i241": Stash(2, 1536),
    "m01s08i242": Stash(2, 1537),
    "m01s08i243": Stash(2, 1537),
    "m01s08i244": Stash(2, 1537),
    "m01s08i245": Stash(2, 1907),
    "m01s08i246": Stash(2, 1537),
    "m01s08i247": Stash(2, 1537),
    "m01s08i248": Stash(2, 1537),
    "m01s08i249": Stash(2, 1537),
    "m01s08i250": Stash(2, 1537),
    "m01s08i251": Stash(2, 1537),
    "m01s08i252": Stash(2, 1534),
    "m01s08i254": Stash(2, 1532),
    "m01s08i255": Stash(2, 1533),
    "m01s08i258": Stash(2, 1532),
    "m01s08i259": Stash(2, 1533),
    "m01s09i004": Stash(1, 16),
    "m01s09i010": Stash(1, 95),
    "m01s09i181": Stash(1, 0),
    "m01s09i182": Stash(1, 0),
    "m01s09i183": Stash(1, 0),
    "m01s09i201": Stash(1, 220),
    "m01s09i202": Stash(1, 1090),
    "m01s09i203": Stash(1, 33),
    "m01s09i204": Stash(1, 32),
    "m01s09i205": Stash(1, 31),
    "m01s09i206": Stash(1, 79),
    "m01s09i207": Stash(1, 78),
    "m01s09i208": Stash(1, 136),
    "m01s09i209": Stash(1, 136),
    "m01s09i210": Stash(1, 136),
    "m01s09i211": Stash(1, 136),
    "m01s09i212": Stash(1, 136),
    "m01s09i213": Stash(1, 136),
    "m01s09i214": Stash(1, 136),
    "m01s09i215": Stash(1, 136),
    "m01s09i216": Stash(1, 30),
    "m01s09i217": Stash(1, 30),
    "m01s09i218": Stash(1, 29),
    "m01s09i219": Stash(1, 75),
    "m01s09i220": Stash(1, 76),
    "m01s09i221": Stash(1, 139),
    "m01s09i222": Stash(1, 41),
    "m01s09i223": Stash(1, 216),
    "m01s09i224": Stash(1, 1425),
    "m01s09i225": Stash(1, 1426),
    "m01s09i226": Stash(1, 1729),
    "m01s09i227": Stash(1, 1729),
    "m01s09i228": Stash(1, 1730),
    "m01s09i229": Stash(1, 88),
    "m01s09i230": Stash(1, 25),
    "m01s09i231": Stash(1, 0),
    "m01s09i232": Stash(1, 30),
    "m01s09i233": Stash(1, 30),
    "m01s09i234": Stash(1, 0),
    "m01s09i310": Stash(1, 1730),
    "m01s09i311": Stash(1, 1730),
    "m01s09i312": Stash(1, 1730),
    "m01s09i313": Stash(1, 1730),
    "m01s09i314": Stash(1, 1730),
    "m01s09i315": Stash(1, 1730),
    "m01s09i316": Stash(1, 1730),
    "m01s09i317": Stash(1, 1730),
    "m01s09i318": Stash(1, 1730),
    "m01s09i319": Stash(1, 1730),
    "m01s09i320": Stash(1, 1730),
    "m01s09i321": Stash(1, 1730),
    "m01s09i322": Stash(1, 1730),
    "m01s09i323": Stash(1, 1730),
    "m01s09i324": Stash(1, 1730),
    "m01s09i325": Stash(1, 1730),
    "m01s09i326": Stash(1, 1730),
    "m01s09i327": Stash(1, 1730),
    "m01s09i328": Stash(1, 1730),
    "m01s09i329": Stash(1, 1730),
    "m01s09i330": Stash(1, 1730),
    "m01s09i331": Stash(1, 1730),
    "m01s09i332": Stash(1, 1730),
    "m01s09i333": Stash(1, 1730),
    "m01s09i334": Stash(1, 1730),
    "m01s09i335": Stash(1, 1730),
    "m01s09i336": Stash(1, 1730),
    "m01s09i337": Stash(1, 1730),
    "m01s09i338": Stash(1, 1730),
    "m01s09i339": Stash(1, 1730),
    "m01s09i340": Stash(1, 1730),
    "m01s09i341": Stash(1, 1730),
    "m01s09i342": Stash(1, 1730),
    "m01s09i343": Stash(1, 1730),
    "m01s09i344": Stash(1, 1730),
    "m01s09i345": Stash(1, 1730),
    "m01s09i346": Stash(1, 1730),
    "m01s09i347": Stash(1, 1730),
    "m01s09i348": Stash(1, 1730),
    "m01s09i349": Stash(1, 1730),
    "m01s09i350": Stash(1, 1730),
    "m01s09i351": Stash(1, 1730),
    "m01s09i352": Stash(1, 1730),
    "m01s09i353": Stash(1, 1730),
    "m01s09i354": Stash(1, 1730),
    "m01s09i355": Stash(1, 1730),
    "m01s09i356": Stash(1, 1730),
    "m01s09i357": Stash(1, 1730),
    "m01s09i358": Stash(1, 1730),
    "m01s09i359": Stash(1, 1730),
    "m01s09i360": Stash(1, 1730),
    "m01s09i361": Stash(1, 1730),
    "m01s09i362": Stash(1, 1730),
    "m01s09i363": Stash(1, 1730),
    "m01s09i364": Stash(1, 1730),
    "m01s09i365": Stash(1, 1730),
    "m01s09i366": Stash(1, 1730),
    "m01s09i367": Stash(1, 1730),
    "m01s09i368": Stash(1, 1730),
    "m01s09i369": Stash(1, 1730),
    "m01s09i370": Stash(1, 1730),
    "m01s09i371": Stash(1, 1730),
    "m01s09i372": Stash(1, 1730),
    "m01s09i373": Stash(1, 1730),
    "m01s09i374": Stash(1, 1730),
    "m01s09i375": Stash(1, 1730),
    "m01s09i376": Stash(1, 1730),
    "m01s09i377": Stash(1, 1730),
    "m01s09i378": Stash(1, 1730),
    "m01s09i379": Stash(1, 1730),
    "m01s09i380": Stash(1, 1730),
    "m01s09i381": Stash(1, 1730),
    "m01s09i382": Stash(1, 1730),
    "m01s09i383": Stash(1, 1730),
    "m01s09i384": Stash(1, 1730),
    "m01s09i385": Stash(1, 1730),
    "m01s09i386": Stash(1, 1730),
    "m01s09i387": Stash(1, 1730),
    "m01s09i388": Stash(1, 1730),
    "m01s09i389": Stash(1, 1730),
    "m01s09i391": Stash(1, 30),
    "m01s09i392": Stash(1, 30),
    "m01s09i393": Stash(1, 30),
    "m01s09i394": Stash(1, 30),
    "m01s09i395": Stash(1, 30),
    "m01s09i396": Stash(1, 30),
    "m01s09i397": Stash(1, 30),
    "m01s09i398": Stash(1, 30),
    "m01s10i001": Stash(1, 8),
    "m01s10i002": Stash(11, 56),
    "m01s10i003": Stash(11, 57),
    "m01s10i004": Stash(1, 19),
    "m01s10i010": Stash(1, 95),
    "m01s10i185": Stash(18, 56),
    "m01s10i186": Stash(19, 57),
    "m01s10i187": Stash(1, 42),
    "m01s10i201": Stash(11, 224),
    "m01s10i202": Stash(11, 225),
    "m01s10i203": Stash(1, 151),
    "m01s10i204": Stash(1, 43),
    "m01s10i205": Stash(1, 28),
    "m01s10i206": Stash(1, 3),
    "m01s10i207": Stash(18, 229),
    "m01s10i208": Stash(19, 230),
    "m01s10i209": Stash(18, 231),
    "m01s10i210": Stash(19, 232),
    "m01s10i211": Stash(18, 233),
    "m01s10i212": Stash(19, 234),
    "m01s10i213": Stash(18, 235),
    "m01s10i214": Stash(19, 236),
    "m01s10i215": Stash(11, 263),
    "m01s10i216": Stash(11, 264),
    "m01s10i217": Stash(11, 265),
    "m01s10i218": Stash(11, 266),
    "m01s10i219": Stash(18, 267),
    "m01s10i220": Stash(19, 268),
    "m01s10i221": Stash(18, 269),
    "m01s10i222": Stash(19, 270),
    "m01s10i223": Stash(11, 56),
    "m01s10i224": Stash(11, 57),
    "m01s10i225": Stash(11, 88),
    "m01s10i226": Stash(11, 56),
    "m01s10i227": Stash(11, 57),
    "m01s10i228": Stash(11, 88),
    "m01s10i229": Stash(1, 79),
    "m01s10i230": Stash(1, 78),
    "m01s12i002": Stash(18, 0),
    "m01s12i003": Stash(19, 0),
    "m01s12i004": Stash(1, 0),
    "m01s12i010": Stash(1, 0),
    "m01s12i012": Stash(1, 0),
    "m01s12i170": Stash(1, 0),
    "m01s12i171": Stash(1, 0),
    "m01s12i172": Stash(1, 0),
    "m01s12i173": Stash(1, 0),
    "m01s12i176": Stash(1, 0),
    "m01s12i177": Stash(1, 0),
    "m01s12i178": Stash(1, 0),
    "m01s12i179": Stash(1, 0),
    "m01s12i181": Stash(1, 0),
    "m01s12i182": Stash(1, 0),
    "m01s12i183": Stash(1, 0),
    "m01s12i184": Stash(1, 0),
    "m01s12i185": Stash(18, 0),
    "m01s12i186": Stash(19, 0),
    "m01s12i187": Stash(1, 0),
    "m01s12i189": Stash(1, 1681),
    "m01s12i190": Stash(1, 1921),
    "m01s12i191": Stash(1, 1689),
    "m01s12i192": Stash(1, 0),
    "m01s12i193": Stash(1, 0),
    "m01s12i194": Stash(1, 0),
    "m01s12i201": Stash(11, 40),
    "m01s12i202": Stash(11, 40),
    "m01s12i203": Stash(1, 0),
    "m01s12i204": Stash(1, 0),
    "m01s12i205": Stash(1, 0),
    "m01s12i254": Stash(1, 0),
    "m01s13i002": Stash(11, 56),
    "m01s13i003": Stash(11, 57),
    "m01s13i004": Stash(1, 19),
    "m01s13i010": Stash(1, 95),
    "m01s13i181": Stash(1, 16),
    "m01s13i182": Stash(1, 95),
    "m01s13i185": Stash(18, 56),
    "m01s13i186": Stash(19, 57),
    "m01s13i190": Stash(1, 0),
    "m01s13i191": Stash(1, 0),
    "m01s13i192": Stash(1, 0),
    "m01s13i193": Stash(1, 0),
    "m01s13i194": Stash(1, 0),
    "m01s13i195": Stash(1, 0),
    "m01s13i196": Stash(1, 0),
    "m01s13i197": Stash(1, 0),
    "m01s13i201": Stash(1, 0),
    "m01s13i381": Stash(1, 0),
    "m01s13i385": Stash(18, 0),
    "m01s13i386": Stash(19, 0),
    "m01s13i387": Stash(1, 0),
    "m01s13i388": Stash(1, 0),
    "m01s13i481": Stash(1, 0),
    "m01s13i485": Stash(18, 0),
    "m01s13i486": Stash(19, 0),
    "m01s13i487": Stash(1, 0),
    "m01s14i181": Stash(1, 0),
    "m01s14i201": Stash(1, 259),
    "m01s15i002": Stash(11, 56),
    "m01s15i003": Stash(11, 57),
    "m01s15i101": Stash(1, 1),
    "m01s15i102": Stash(1, 1),
    "m01s15i108": Stash(1, 8),
    "m01s15i119": Stash(1, 19),
    "m01s15i127": Stash(1, 27),
    "m01s15i142": Stash(1, 42),
    "m01s15i143": Stash(18, 56),
    "m01s15i144": Stash(19, 57),
    "m01s15i181": Stash(1, 16),
    "m01s15i182": Stash(1, 95),
    "m01s15i183": Stash(1, 0),
    "m01s15i201": Stash(11, 56),
    "m01s15i202": Stash(11, 57),
    "m01s15i203": Stash(11, 190),
    "m01s15i204": Stash(11, 8),
    "m01s15i205": Stash(11, 190),
    "m01s15i206": Stash(11, 1),
    "m01s15i207": Stash(11, 4),
    "m01s15i208": Stash(11, 8),
    "m01s15i209": Stash(11, 56),
    "m01s15i210": Stash(11, 57),
    "m01s15i211": Stash(11, 190),
    "m01s15i212": Stash(11, 56),
    "m01s15i213": Stash(11, 57),
    "m01s15i214": Stash(11, 82),
    "m01s15i215": Stash(11, 19),
    "m01s15i216": Stash(11, 19),
    "m01s15i217": Stash(11, 82),
    "m01s15i218": Stash(1, 83),
    "m01s15i219": Stash(11, 13),
    "m01s15i220": Stash(11, 58),
    "m01s15i221": Stash(11, 59),
    "m01s15i222": Stash(11, 40),
    "m01s15i223": Stash(11, 14),
    "m01s15i224": Stash(11, 53),
    "m01s15i225": Stash(11, 54),
    "m01s15i226": Stash(11, 95),
    "m01s15i227": Stash(11, 46),
    "m01s15i228": Stash(11, 47),
    "m01s15i229": Stash(11, 82),
    "m01s15i230": Stash(1, 19),
    "m01s15i231": Stash(19, 144),
    "m01s15i232": Stash(1, 145),
    "m01s15i233": Stash(1, 146),
    "m01s15i234": Stash(1, 147),
    "m01s15i235": Stash(11, 1334),
    "m01s15i236": Stash(11, 1335),
    "m01s15i237": Stash(11, 63),
    "m01s15i238": Stash(11, 1),
    "m01s15i239": Stash(11, 1334),
    "m01s15i240": Stash(11, 1334),
    "m01s15i241": Stash(11, 1399),
    "m01s15i242": Stash(1, 42),
    "m01s15i243": Stash(18, 56),
    "m01s15i244": Stash(19, 57),
    "m01s15i245": Stash(18, 56),
    "m01s15i246": Stash(19, 57),
    "m01s15i260": Stash(1, 0),
    "m01s15i261": Stash(1, 0),
    "m01s15i262": Stash(1, 0),
    "m01s15i263": Stash(1, 0),
    "m01s15i264": Stash(1, 0),
    "m01s15i265": Stash(1, 0),
    "m01s15i266": Stash(1, 0),
    "m01s15i270": Stash(1, 0),
    "m01s15i271": Stash(1, 0),
    "m01s16i004": Stash(1, 16),
    "m01s16i140": Stash(1, 0),
    "m01s16i141": Stash(1, 0),
    "m01s16i142": Stash(1, 0),
    "m01s16i143": Stash(1, 0),
    "m01s16i146": Stash(1, 0),
    "m01s16i147": Stash(1, 0),
    "m01s16i148": Stash(1, 0),
    "m01s16i149": Stash(1, 0),
    "m01s16i150": Stash(1, 0),
    "m01s16i151": Stash(1, 0),
    "m01s16i156": Stash(1, 0),
    "m01s16i157": Stash(1, 0),
    "m01s16i161": Stash(1, 0),
    "m01s16i162": Stash(1, 0),
    "m01s16i163": Stash(1, 0),
    "m01s16i164": Stash(1, 0),
    "m01s16i172": Stash(1, 0),
    "m01s16i173": Stash(1, 0),
    "m01s16i174": Stash(1, 0),
    "m01s16i181": Stash(1, 0),
    "m01s16i182": Stash(1, 0),
    "m01s16i183": Stash(1, 0),
    "m01s16i184": Stash(1, 0),
    "m01s16i192": Stash(1, 0),
    "m01s16i193": Stash(1, 0),
    "m01s16i194": Stash(1, 0),
    "m01s16i201": Stash(1, 1),
    "m01s16i202": Stash(1, 1),
    "m01s16i203": Stash(1, 16),
    "m01s16i204": Stash(1, 88),
    "m01s16i205": Stash(1, 22),
    "m01s16i206": Stash(1, 299),
    "m01s16i207": Stash(1, 1929),
    "m01s16i208": Stash(1, 8),
    "m01s16i209": Stash(1, 4),
    "m01s16i210": Stash(1, 1),
    "m01s16i211": Stash(1, 8),
    "m01s16i212": Stash(1, 35),
    "m01s16i213": Stash(1, 35),
    "m01s16i214": Stash(1, 8),
    "m01s16i215": Stash(1, 16),
    "m01s16i216": Stash(1, 1),
    "m01s16i217": Stash(1, 4),
    "m01s16i218": Stash(1, 1),
    "m01s16i219": Stash(1, 189),
    "m01s16i220": Stash(1, 1),
    "m01s16i221": Stash(1, 199),
    "m01s16i222": Stash(1, 8),
    "m01s16i223": Stash(1, 189),
    "m01s16i224": Stash(1, 15),
    "m01s16i225": Stash(1, 1),
    "m01s16i226": Stash(1, 501),
    "m01s16i227": Stash(1, 502),
    "m01s16i228": Stash(1, 503),
    "m01s16i229": Stash(1, 504),
    "m01s16i230": Stash(1, 505),
    "m01s16i231": Stash(1, 506),
    "m01s16i232": Stash(1, 507),
    "m01s16i233": Stash(1, 508),
    "m01s16i234": Stash(1, 509),
    "m01s16i235": Stash(1, 510),
    "m01s16i236": Stash(1, 511),
    "m01s16i237": Stash(1, 512),
    "m01s16i238": Stash(1, 513),
    "m01s16i239": Stash(1, 514),
    "m01s16i240": Stash(1, 515),
    "m01s16i241": Stash(1, 516),
    "m01s16i242": Stash(1, 517),
    "m01s16i243": Stash(1, 518),
    "m01s16i244": Stash(1, 519),
    "m01s16i245": Stash(1, 520),
    "m01s16i246": Stash(1, 521),
    "m01s16i247": Stash(1, 522),
    "m01s16i249": Stash(1, 524),
    "m01s16i250": Stash(1, 525),
    "m01s16i251": Stash(1, 526),
    "m01s16i252": Stash(1, 527),
    "m01s16i253": Stash(1, 528),
    "m01s16i254": Stash(1, 529),
    "m01s16i255": Stash(1, 1),
    "m01s16i256": Stash(1, 88),
    "m01s17i201": Stash(1, 1494),
    "m01s17i202": Stash(1, 1495),
    "m01s17i203": Stash(1, 1496),
    "m01s17i204": Stash(1, 1497),
    "m01s17i205": Stash(1, 576),
    "m01s17i206": Stash(1, 0),
    "m01s17i207": Stash(1, 0),
    "m01s17i208": Stash(1, 0),
    "m01s17i209": Stash(1, 0),
    "m01s17i210": Stash(1, 0),
    "m01s17i211": Stash(1, 0),
    "m01s17i212": Stash(1, 0),
    "m01s17i213": Stash(1, 0),
    "m01s17i214": Stash(1, 0),
    "m01s17i215": Stash(1, 0),
    "m01s17i216": Stash(1, 0),
    "m01s17i217": Stash(1, 0),
    "m01s17i218": Stash(1, 0),
    "m01s17i219": Stash(1, 0),
    "m01s17i220": Stash(1, 2020),
    "m01s17i221": Stash(1, 2021),
    "m01s17i222": Stash(1, 2022),
    "m01s17i223": Stash(1, 2023),
    "m01s17i224": Stash(1, 2024),
    "m01s17i225": Stash(1, 2025),
    "m01s17i226": Stash(1, 2026),
    "m01s17i227": Stash(1, 2027),
    "m01s17i228": Stash(1, 2028),
    "m01s17i229": Stash(1, 2029),
    "m01s17i230": Stash(1, 2030),
    "m01s17i231": Stash(1, 2031),
    "m01s17i232": Stash(1, 2032),
    "m01s17i233": Stash(1, 2033),
    "m01s17i234": Stash(1, 2034),
    "m01s17i235": Stash(1, 2035),
    "m01s17i236": Stash(1, 2036),
    "m01s17i237": Stash(1, 2037),
    "m01s17i240": Stash(1, 0),
    "m01s17i241": Stash(1, 0),
    "m01s17i242": Stash(1, 0),
    "m01s17i257": Stash(1, 1645),
    "m01s18i001": Stash(1, 1929),
    "m01s18i002": Stash(11, 56),
    "m01s18i003": Stash(11, 57),
    "m01s18i004": Stash(1, 19),
    "m01s18i010": Stash(1, 95),
    "m01s18i201": Stash(1, 290),
    "m01s18i202": Stash(1, 291),
    "m01s18i203": Stash(11, 292),
    "m01s18i204": Stash(1, 294),
    "m01s18i205": Stash(1, 295),
    "m01s18i209": Stash(1, 288),
    "m01s18i211": Stash(1, 303),
    "m01s18i212": Stash(1, 304),
    "m01s18i213": Stash(11, 305),
    "m01s18i214": Stash(1, 309),
    "m01s18i215": Stash(1, 310),
    "m01s18i219": Stash(1, 289),
    "m01s18i223": Stash(11, 306),
    "m01s18i231": Stash(1, 304),
    "m01s18i241": Stash(11, 305),
    "m01s18i242": Stash(11, 305),
    "m01s18i251": Stash(11, 306),
    "m01s18i252": Stash(11, 306),
    "m01s18i261": Stash(1, 303),
    "m01s18i262": Stash(1, 304),
    "m01s18i271": Stash(1, 24),
    "m01s18i272": Stash(1, 304),
    "m01s19i001": Stash(2, 1512),
    "m01s19i002": Stash(2, 1513),
    "m01s19i003": Stash(2, 1514),
    "m01s19i004": Stash(2, 1515),
    "m01s19i005": Stash(2, 1516),
    "m01s19i006": Stash(2, 1500),
    "m01s19i007": Stash(2, 1392),
    "m01s19i008": Stash(2, 1500),
    "m01s19i009": Stash(2, 1398),
    "m01s19i010": Stash(2, 1502),
    "m01s19i011": Stash(2, 1503),
    "m01s19i012": Stash(2, 1394),
    "m01s19i013": Stash(2, 1391),
    "m01s19i014": Stash(2, 1392),
    "m01s19i015": Stash(2, 1393),
    "m01s19i016": Stash(2, 1397),
    "m01s19i017": Stash(2, 1503),
    "m01s19i018": Stash(2, 1503),
    "m01s19i019": Stash(2, 1503),
    "m01s19i020": Stash(2, 1503),
    "m01s19i021": Stash(2, 1397),
    "m01s19i022": Stash(2, 1397),
    "m01s19i023": Stash(2, 1397),
    "m01s19i024": Stash(2, 1397),
    "m01s19i025": Stash(2, 1512),
    "m01s19i026": Stash(2, 1513),
    "m01s19i027": Stash(2, 1512),
    "m01s19i028": Stash(2, 1513),
    "m01s19i029": Stash(2, 1512),
    "m01s19i030": Stash(2, 1513),
    "m01s19i031": Stash(2, 1394),
    "m01s19i032": Stash(2, 1397),
    "m01s19i033": Stash(2, 1397),
    "m01s19i034": Stash(2, 1397),
    "m01s19i035": Stash(2, 1515),
    "m01s19i036": Stash(2, 1516),
    "m01s19i037": Stash(2, 1516),
    "m01s19i038": Stash(2, 1516),
    "m01s19i039": Stash(2, 1516),
    "m01s19i040": Stash(2, 1516),
    "m01s19i041": Stash(2, 1516),
    "m01s19i042": Stash(2, 1516),
    "m01s20i001": Stash(1, 2),
    "m01s20i002": Stash(1, 2),
    "m01s20i003": Stash(1, 50),
    "m01s20i004": Stash(1, 50),
    "m01s20i005": Stash(1, 74),
    "m01s20i006": Stash(1, 73),
    "m01s20i007": Stash(1, 1578),
    "m01s20i012": Stash(1, 34),
    "m01s20i013": Stash(1, 1577),
    "m01s20i014": Stash(1, 1564),
    "m01s20i015": Stash(1, 1565),
    "m01s20i016": Stash(1, 1690),
    "m01s20i017": Stash(1, 1690),
    "m01s20i018": Stash(1, 8),
    "m01s20i020": Stash(11, 56),
    "m01s20i021": Stash(11, 57),
    "m01s20i022": Stash(1, 8),
    "m01s20i023": Stash(1, 4),
    "m01s20i024": Stash(1, 8),
    "m01s20i025": Stash(1, 16),
    "m01s20i026": Stash(1, 1),
    "m01s20i027": Stash(1, 4),
    "m01s20i028": Stash(1, 191),
    "m01s20i029": Stash(1, 35),
    "m01s20i030": Stash(1, 35),
    "m01s20i031": Stash(1, 189),
    "m01s20i032": Stash(1, 89),
    "m01s20i033": Stash(1, 1),
    "m01s20i034": Stash(1, 8),
    "m01s20i035": Stash(1, 4),
    "m01s20i036": Stash(1, 1),
    "m01s20i037": Stash(1, 8),
    "m01s20i038": Stash(1, 4),
    "m01s20i039": Stash(1, 4),
    "m01s20i040": Stash(1, 4),
    "m01s20i041": Stash(11, 4),
    "m01s20i042": Stash(11, 4),
    "m01s20i043": Stash(11, 1981),
    "m01s20i044": Stash(11, 1982),
    "m01s20i045": Stash(11, 1983),
    "m01s20i046": Stash(11, 1984),
    "m01s20i047": Stash(11, 1985),
    "m01s20i048": Stash(11, 1986),
    "m01s20i049": Stash(11, 1987),
    "m01s20i050": Stash(11, 1988),
    "m01s20i051": Stash(11, 1989),
    "m01s20i052": Stash(11, 1990),
    "m01s20i053": Stash(11, 1991),
    "m01s20i054": Stash(11, 1992),
    "m01s20i055": Stash(11, 1993),
    "m01s20i056": Stash(11, 1994),
    "m01s20i057": Stash(11, 1995),
    "m01s20i058": Stash(11, 1643),
    "m01s20i059": Stash(11, 1644),
    "m01s20i060": Stash(11, 1114),
    "m01s20i061": Stash(1, 1),
    "m01s20i062": Stash(1, 8),
    "m01s20i063": Stash(1, 4),
    "m01s20i064": Stash(1, 8),
    "m01s20i065": Stash(1, 16),
    "m01s20i066": Stash(1, 1),
    "m01s20i067": Stash(1, 4),
    "m01s20i068": Stash(1, 99),
    "m01s20i069": Stash(1, 90),
    "m01s20i070": Stash(11, 1649),
    "m01s20i071": Stash(11, 1650),
    "m01s21i001": Stash(1, 8),
    "m01s21i002": Stash(11, 56),
    "m01s21i003": Stash(11, 57),
    "m01s21i004": Stash(1, 19),
    "m01s21i010": Stash(1, 95),
    "m01s21i013": Stash(1, 34),
    "m01s21i014": Stash(1, 222),
    "m01s21i015": Stash(1, 223),
    "m01s21i016": Stash(1, 219),
    "m01s21i023": Stash(1, 93),
    "m01s21i024": Stash(1, 16),
    "m01s21i025": Stash(1, 5),
    "m01s21i026": Stash(1, 324),
    "m01s21i028": Stash(3, 701),
    "m01s21i029": Stash(3, 702),
    "m01s21i031": Stash(3, 37),
    "m01s21i032": Stash(3, 687),
    "m01s21i061": Stash(1, 501),
    "m01s21i062": Stash(1, 502),
    "m01s21i063": Stash(1, 503),
    "m01s21i064": Stash(1, 504),
    "m01s21i065": Stash(1, 505),
    "m01s21i066": Stash(1, 506),
    "m01s21i067": Stash(1, 507),
    "m01s21i068": Stash(1, 508),
    "m01s21i069": Stash(1, 509),
    "m01s21i070": Stash(1, 510),
    "m01s21i071": Stash(1, 511),
    "m01s21i072": Stash(1, 512),
    "m01s21i073": Stash(1, 513),
    "m01s21i074": Stash(1, 514),
    "m01s21i075": Stash(1, 515),
    "m01s21i076": Stash(1, 516),
    "m01s21i077": Stash(1, 517),
    "m01s21i078": Stash(1, 518),
    "m01s21i079": Stash(1, 519),
    "m01s21i080": Stash(1, 520),
    "m01s21i081": Stash(1, 521),
    "m01s21i082": Stash(1, 522),
    "m01s21i083": Stash(1, 523),
    "m01s21i084": Stash(1, 524),
    "m01s21i085": Stash(1, 525),
    "m01s21i086": Stash(1, 526),
    "m01s21i087": Stash(1, 527),
    "m01s21i088": Stash(1, 528),
    "m01s21i089": Stash(1, 529),
    "m01s21i100": Stash(1, 0),
    "m01s21i101": Stash(1, 0),
    "m01s21i102": Stash(1, 0),
    "m01s21i103": Stash(1, 0),
    "m01s21i104": Stash(1, 0),
    "m01s21i105": Stash(1, 0),
    "m01s21i106": Stash(1, 0),
    "m01s21i201": Stash(11, 56),
    "m01s21i202": Stash(11, 57),
    "m01s21i203": Stash(11, 40),
    "m01s21i205": Stash(11, 190),
    "m01s21i206": Stash(11, 1),
    "m01s21i207": Stash(11, 8),
    "m01s21i208": Stash(11, 56),
    "m01s21i209": Stash(11, 57),
    "m01s21i210": Stash(1, 1),
    "m01s21i211": Stash(1, 1),
    "m01s21i212": Stash(1, 16),
    "m01s21i213": Stash(1, 88),
    "m01s21i214": Stash(1, 22),
    "m01s21i215": Stash(1, 191),
    "m01s21i216": Stash(1, 1),
    "m01s21i217": Stash(1, 8),
    "m01s21i218": Stash(1, 8),
    "m01s21i219": Stash(1, 16),
    "m01s21i220": Stash(1, 1),
    "m01s21i221": Stash(1, 31),
    "m01s21i222": Stash(1, 32),
    "m01s21i223": Stash(1, 33),
    "m01s21i224": Stash(1, 8),
    "m01s22i001": Stash(1, 8),
    "m01s22i002": Stash(11, 56),
    "m01s22i003": Stash(11, 57),
    "m01s22i004": Stash(1, 19),
    "m01s22i010": Stash(1, 95),
    "m01s22i013": Stash(1, 34),
    "m01s22i014": Stash(1, 222),
    "m01s22i015": Stash(1, 223),
    "m01s22i016": Stash(1, 219),
    "m01s22i023": Stash(1, 93),
    "m01s22i024": Stash(1, 16),
    "m01s22i025": Stash(1, 5),
    "m01s22i026": Stash(1, 324),
    "m01s22i028": Stash(3, 701),
    "m01s22i029": Stash(3, 702),
    "m01s22i031": Stash(3, 37),
    "m01s22i032": Stash(3, 687),
    "m01s22i061": Stash(1, 501),
    "m01s22i062": Stash(1, 502),
    "m01s22i063": Stash(1, 503),
    "m01s22i064": Stash(1, 504),
    "m01s22i065": Stash(1, 505),
    "m01s22i066": Stash(1, 506),
    "m01s22i067": Stash(1, 507),
    "m01s22i068": Stash(1, 508),
    "m01s22i069": Stash(1, 509),
    "m01s22i070": Stash(1, 510),
    "m01s22i071": Stash(1, 511),
    "m01s22i072": Stash(1, 512),
    "m01s22i073": Stash(1, 513),
    "m01s22i074": Stash(1, 514),
    "m01s22i075": Stash(1, 515),
    "m01s22i076": Stash(1, 516),
    "m01s22i077": Stash(1, 517),
    "m01s22i078": Stash(1, 518),
    "m01s22i079": Stash(1, 519),
    "m01s22i080": Stash(1, 520),
    "m01s22i081": Stash(1, 521),
    "m01s22i082": Stash(1, 522),
    "m01s22i083": Stash(1, 523),
    "m01s22i084": Stash(1, 524),
    "m01s22i085": Stash(1, 525),
    "m01s22i086": Stash(1, 526),
    "m01s22i087": Stash(1, 527),
    "m01s22i088": Stash(1, 528),
    "m01s22i089": Stash(1, 529),
    "m01s22i201": Stash(11, 56),
    "m01s22i202": Stash(11, 57),
    "m01s22i203": Stash(11, 40),
    "m01s22i205": Stash(11, 190),
    "m01s22i206": Stash(11, 1),
    "m01s22i207": Stash(11, 8),
    "m01s22i208": Stash(11, 56),
    "m01s22i209": Stash(11, 57),
    "m01s22i210": Stash(1, 1),
    "m01s22i211": Stash(1, 1),
    "m01s22i212": Stash(1, 16),
    "m01s22i213": Stash(1, 88),
    "m01s22i214": Stash(1, 22),
    "m01s22i215": Stash(1, 191),
    "m01s22i216": Stash(1, 1),
    "m01s22i217": Stash(1, 8),
    "m01s22i218": Stash(1, 8),
    "m01s22i219": Stash(1, 16),
    "m01s22i220": Stash(1, 1),
    "m01s22i221": Stash(1, 31),
    "m01s22i222": Stash(1, 32),
    "m01s22i223": Stash(1, 33),
    "m01s22i224": Stash(1, 8),
    "m01s23i001": Stash(1, 8),
    "m01s23i002": Stash(11, 56),
    "m01s23i003": Stash(11, 57),
    "m01s23i004": Stash(1, 19),
    "m01s23i010": Stash(1, 95),
    "m01s23i013": Stash(1, 34),
    "m01s23i014": Stash(1, 222),
    "m01s23i015": Stash(1, 223),
    "m01s23i016": Stash(1, 219),
    "m01s23i023": Stash(1, 93),
    "m01s23i024": Stash(1, 16),
    "m01s23i025": Stash(1, 5),
    "m01s23i026": Stash(1, 324),
    "m01s23i028": Stash(3, 701),
    "m01s23i029": Stash(3, 702),
    "m01s23i031": Stash(3, 37),
    "m01s23i032": Stash(3, 687),
    "m01s23i061": Stash(1, 501),
    "m01s23i062": Stash(1, 502),
    "m01s23i063": Stash(1, 503),
    "m01s23i064": Stash(1, 504),
    "m01s23i065": Stash(1, 505),
    "m01s23i066": Stash(1, 506),
    "m01s23i067": Stash(1, 507),
    "m01s23i068": Stash(1, 508),
    "m01s23i069": Stash(1, 509),
    "m01s23i070": Stash(1, 510),
    "m01s23i071": Stash(1, 511),
    "m01s23i072": Stash(1, 512),
    "m01s23i073": Stash(1, 513),
    "m01s23i074": Stash(1, 514),
    "m01s23i075": Stash(1, 515),
    "m01s23i076": Stash(1, 516),
    "m01s23i077": Stash(1, 517),
    "m01s23i078": Stash(1, 518),
    "m01s23i079": Stash(1, 519),
    "m01s23i080": Stash(1, 520),
    "m01s23i081": Stash(1, 521),
    "m01s23i082": Stash(1, 522),
    "m01s23i083": Stash(1, 523),
    "m01s23i084": Stash(1, 524),
    "m01s23i085": Stash(1, 525),
    "m01s23i086": Stash(1, 526),
    "m01s23i087": Stash(1, 527),
    "m01s23i088": Stash(1, 528),
    "m01s23i089": Stash(1, 529),
    "m01s23i201": Stash(11, 56),
    "m01s23i202": Stash(11, 57),
    "m01s23i203": Stash(11, 40),
    "m01s23i205": Stash(11, 190),
    "m01s23i206": Stash(11, 1),
    "m01s23i207": Stash(11, 8),
    "m01s23i208": Stash(11, 56),
    "m01s23i209": Stash(11, 57),
    "m01s23i210": Stash(1, 1),
    "m01s23i211": Stash(1, 1),
    "m01s23i212": Stash(1, 16),
    "m01s23i213": Stash(1, 88),
    "m01s23i214": Stash(1, 22),
    "m01s23i215": Stash(1, 191),
    "m01s23i216": Stash(1, 1),
    "m01s23i217": Stash(1, 8),
    "m01s23i218": Stash(1, 8),
    "m01s23i219": Stash(1, 16),
    "m01s23i220": Stash(1, 1),
    "m01s23i221": Stash(1, 31),
    "m01s23i222": Stash(1, 32),
    "m01s23i223": Stash(1, 33),
    "m01s23i224": Stash(1, 8),
    "m01s24i001": Stash(1, 8),
    "m01s24i002": Stash(11, 56),
    "m01s24i003": Stash(11, 57),
    "m01s24i004": Stash(1, 19),
    "m01s24i010": Stash(1, 95),
    "m01s24i013": Stash(1, 34),
    "m01s24i014": Stash(1, 222),
    "m01s24i015": Stash(1, 223),
    "m01s24i016": Stash(1, 219),
    "m01s24i023": Stash(1, 93),
    "m01s24i024": Stash(1, 16),
    "m01s24i025": Stash(1, 5),
    "m01s24i026": Stash(1, 324),
    "m01s24i028": Stash(3, 701),
    "m01s24i029": Stash(3, 702),
    "m01s24i031": Stash(3, 37),
    "m01s24i032": Stash(3, 687),
    "m01s24i061": Stash(1, 501),
    "m01s24i062": Stash(1, 502),
    "m01s24i063": Stash(1, 503),
    "m01s24i064": Stash(1, 504),
    "m01s24i065": Stash(1, 505),
    "m01s24i066": Stash(1, 506),
    "m01s24i067": Stash(1, 507),
    "m01s24i068": Stash(1, 508),
    "m01s24i069": Stash(1, 509),
    "m01s24i070": Stash(1, 510),
    "m01s24i071": Stash(1, 511),
    "m01s24i072": Stash(1, 512),
    "m01s24i073": Stash(1, 513),
    "m01s24i074": Stash(1, 514),
    "m01s24i075": Stash(1, 515),
    "m01s24i076": Stash(1, 516),
    "m01s24i077": Stash(1, 517),
    "m01s24i078": Stash(1, 518),
    "m01s24i079": Stash(1, 519),
    "m01s24i080": Stash(1, 520),
    "m01s24i081": Stash(1, 521),
    "m01s24i082": Stash(1, 522),
    "m01s24i083": Stash(1, 523),
    "m01s24i084": Stash(1, 524),
    "m01s24i085": Stash(1, 525),
    "m01s24i086": Stash(1, 526),
    "m01s24i087": Stash(1, 527),
    "m01s24i088": Stash(1, 528),
    "m01s24i089": Stash(1, 529),
    "m01s24i201": Stash(11, 56),
    "m01s24i202": Stash(11, 57),
    "m01s24i203": Stash(11, 40),
    "m01s24i205": Stash(11, 190),
    "m01s24i206": Stash(11, 1),
    "m01s24i207": Stash(11, 8),
    "m01s24i208": Stash(11, 56),
    "m01s24i209": Stash(11, 57),
    "m01s24i210": Stash(1, 1),
    "m01s24i211": Stash(1, 1),
    "m01s24i212": Stash(1, 16),
    "m01s24i213": Stash(1, 88),
    "m01s24i214": Stash(1, 22),
    "m01s24i215": Stash(1, 191),
    "m01s24i216": Stash(1, 1),
    "m01s24i217": Stash(1, 8),
    "m01s24i218": Stash(1, 8),
    "m01s24i219": Stash(1, 16),
    "m01s24i220": Stash(1, 1),
    "m01s24i221": Stash(1, 31),
    "m01s24i222": Stash(1, 32),
    "m01s24i223": Stash(1, 33),
    "m01s24i224": Stash(1, 8),
    "m01s26i001": Stash(23, 1902),
    "m01s26i002": Stash(23, 1904),
    "m01s26i003": Stash(23, 1903),
    "m01s26i004": Stash(1, 1901),
    "m01s26i006": Stash(23, 1906),
    "m01s30i001": Stash(1, 56),
    "m01s30i002": Stash(1, 57),
    "m01s30i003": Stash(1, 42),
    "m01s30i004": Stash(1, 16),
    "m01s30i005": Stash(1, 95),
    "m01s30i006": Stash(1, 1),
    "m01s30i007": Stash(1, 60),
    "m01s30i008": Stash(1, 40),
    "m01s30i011": Stash(1, 0),
    "m01s30i012": Stash(1, 0),
    "m01s30i013": Stash(1, 0),
    "m01s30i014": Stash(1, 0),
    "m01s30i015": Stash(1, 0),
    "m01s30i016": Stash(1, 0),
    "m01s30i017": Stash(1, 0),
    "m01s30i018": Stash(1, 0),
    "m01s30i022": Stash(1, 0),
    "m01s30i023": Stash(1, 0),
    "m01s30i024": Stash(1, 0),
    "m01s30i025": Stash(1, 0),
    "m01s30i026": Stash(1, 0),
    "m01s30i027": Stash(1, 0),
    "m01s30i028": Stash(1, 0),
    "m01s30i033": Stash(1, 0),
    "m01s30i034": Stash(1, 0),
    "m01s30i035": Stash(1, 0),
    "m01s30i036": Stash(1, 0),
    "m01s30i037": Stash(1, 0),
    "m01s30i038": Stash(1, 0),
    "m01s30i044": Stash(1, 0),
    "m01s30i045": Stash(1, 0),
    "m01s30i046": Stash(1, 0),
    "m01s30i047": Stash(1, 0),
    "m01s30i048": Stash(1, 0),
    "m01s30i055": Stash(1, 0),
    "m01s30i056": Stash(1, 0),
    "m01s30i057": Stash(1, 0),
    "m01s30i058": Stash(1, 0),
    "m01s30i066": Stash(1, 0),
    "m01s30i067": Stash(1, 0),
    "m01s30i077": Stash(1, 0),
    "m01s30i078": Stash(1, 0),
    "m01s30i088": Stash(1, 0),
    "m01s30i101": Stash(1, 56),
    "m01s30i102": Stash(1, 57),
    "m01s30i103": Stash(1, 42),
    "m01s30i104": Stash(1, 16),
    "m01s30i105": Stash(1, 95),
    "m01s30i106": Stash(1, 1),
    "m01s30i107": Stash(1, 95),
    "m01s30i111": Stash(1, 16),
    "m01s30i112": Stash(1, 42),
    "m01s30i113": Stash(1, 88),
    "m01s30i114": Stash(1, 42),
    "m01s30i115": Stash(1, 0),
    "m01s30i171": Stash(1, 16),
    "m01s30i172": Stash(1, 95),
    "m01s30i173": Stash(1, 79),
    "m01s30i174": Stash(1, 78),
    "m01s30i175": Stash(18, 56),
    "m01s30i176": Stash(19, 57),
    "m01s30i177": Stash(1, 42),
    "m01s30i178": Stash(1, 7),
    "m01s30i181": Stash(1, 16),
    "m01s30i182": Stash(1, 95),
    "m01s30i183": Stash(1, 79),
    "m01s30i184": Stash(1, 78),
    "m01s30i185": Stash(18, 56),
    "m01s30i186": Stash(19, 57),
    "m01s30i187": Stash(1, 42),
    "m01s30i188": Stash(1, 7),
    "m01s30i189": Stash(1, 1681),
    "m01s30i190": Stash(1, 1921),
    "m01s30i191": Stash(1, 1689),
    "m01s30i201": Stash(11, 56),
    "m01s30i202": Stash(11, 57),
    "m01s30i203": Stash(11, 0),
    "m01s30i204": Stash(11, 16),
    "m01s30i205": Stash(11, 95),
    "m01s30i206": Stash(11, 88),
    "m01s30i207": Stash(11, 1),
    "m01s30i208": Stash(11, 40),
    "m01s30i211": Stash(11, 0),
    "m01s30i212": Stash(11, 0),
    "m01s30i213": Stash(11, 0),
    "m01s30i214": Stash(11, 0),
    "m01s30i215": Stash(11, 0),
    "m01s30i217": Stash(11, 0),
    "m01s30i218": Stash(11, 0),
    "m01s30i222": Stash(11, 0),
    "m01s30i223": Stash(11, 0),
    "m01s30i224": Stash(11, 0),
    "m01s30i225": Stash(11, 0),
    "m01s30i227": Stash(11, 0),
    "m01s30i228": Stash(11, 0),
    "m01s30i233": Stash(11, 0),
    "m01s30i234": Stash(11, 0),
    "m01s30i235": Stash(11, 0),
    "m01s30i237": Stash(11, 0),
    "m01s30i238": Stash(11, 0),
    "m01s30i244": Stash(11, 0),
    "m01s30i245": Stash(11, 0),
    "m01s30i247": Stash(11, 0),
    "m01s30i248": Stash(11, 0),
    "m01s30i255": Stash(11, 0),
    "m01s30i257": Stash(11, 0),
    "m01s30i258": Stash(11, 0),
    "m01s30i277": Stash(11, 0),
    "m01s30i278": Stash(11, 0),
    "m01s30i288": Stash(11, 0),
    "m01s30i301": Stash(11, 1335),
    "m01s30i302": Stash(11, 16),
    "m01s30i303": Stash(11, 16),
    "m01s30i310": Stash(14, 1075),
    "m01s30i311": Stash(14, 1076),
    "m01s30i312": Stash(14, 1077),
    "m01s30i313": Stash(14, 1078),
    "m01s30i314": Stash(14, 1079),
    "m01s30i315": Stash(14, 1080),
    "m01s30i316": Stash(14, 1081),
    "m01s30i401": Stash(1, 63),
    "m01s30i402": Stash(1, 63),
    "m01s30i403": Stash(1, 0),
    "m01s30i404": Stash(1, 0),
    "m01s30i405": Stash(1, 0),
    "m01s30i406": Stash(1, 0),
    "m01s30i407": Stash(1, 0),
    "m01s30i408": Stash(1, 0),
    "m01s30i409": Stash(1, 0),
    "m01s30i410": Stash(18, 1399),
    "m01s30i411": Stash(1, 56),
    "m01s30i412": Stash(1, 57),
    "m01s30i413": Stash(1, 0),
    "m01s30i414": Stash(1, 56),
    "m01s30i415": Stash(1, 57),
    "m01s30i416": Stash(1, 0),
    "m01s30i417": Stash(1, 8),
    "m01s30i418": Stash(11, 8),
    "m01s30i419": Stash(1, 259),
    "m01s30i420": Stash(1, 0),
    "m01s30i421": Stash(1, 0),
    "m01s30i422": Stash(1, 0),
    "m01s30i423": Stash(1, 0),
    "m01s30i424": Stash(1, 0),
    "m01s30i425": Stash(1, 0),
    "m01s30i426": Stash(1, 0),
    "m01s30i427": Stash(1, 0),
    "m01s30i428": Stash(1, 0),
    "m01s30i429": Stash(1, 0),
    "m01s30i430": Stash(1, 0),
    "m01s30i431": Stash(1, 0),
    "m01s30i432": Stash(1, 0),
    "m01s30i433": Stash(1, 0),
    "m01s30i434": Stash(1, 0),
    "m01s30i435": Stash(1, 0),
    "m01s30i436": Stash(1, 0),
    "m01s30i437": Stash(1, 0),
    "m01s30i438": Stash(1, 0),
    "m01s30i439": Stash(1, 0),
    "m01s30i440": Stash(18, 0),
    "m01s30i441": Stash(19, 0),
    "m01s30i442": Stash(1, 0),
    "m01s30i451": Stash(1, 8),
    "m01s30i452": Stash(1, 16),
    "m01s30i453": Stash(1, 1),
    "m01s30i454": Stash(1, 4),
    "m01s31i001": Stash(29, 0),
    "m01s31i002": Stash(27, 0),
    "m01s31i003": Stash(28, 0),
    "m01s31i004": Stash(26, 0),
    "m01s31i005": Stash(26, 0),
    "m01s31i006": Stash(26, 0),
    "m01s31i007": Stash(26, 0),
    "m01s31i008": Stash(26, 0),
    "m01s31i009": Stash(26, 0),
    "m01s31i010": Stash(26, 0),
    "m01s31i011": Stash(27, 0),
    "m01s31i012": Stash(28, 0),
    "m01s31i013": Stash(26, 0),
    "m01s31i014": Stash(26, 0),
    "m01s31i015": Stash(26, 0),
    "m01s31i016": Stash(26, 0),
    "m01s31i017": Stash(26, 0),
    "m01s31i018": Stash(26, 0),
    "m01s31i019": Stash(26, 0),
    "m01s31i020": Stash(26, 0),
    "m01s31i023": Stash(26, 0),
    "m01s31i024": Stash(26, 0),
    "m01s31i025": Stash(26, 0),
    "m01s31i026": Stash(26, 0),
    "m01s31i027": Stash(26, 0),
    "m01s31i028": Stash(26, 0),
    "m01s31i029": Stash(26, 0),
    "m01s31i030": Stash(26, 0),
    "m01s31i031": Stash(26, 0),
    "m01s31i032": Stash(26, 0),
    "m01s31i033": Stash(26, 0),
    "m01s31i035": Stash(26, 0),
    "m01s31i036": Stash(26, 0),
    "m01s31i037": Stash(26, 0),
    "m01s31i038": Stash(26, 0),
    "m01s31i039": Stash(26, 0),
    "m01s31i040": Stash(26, 0),
    "m01s31i041": Stash(26, 0),
    "m01s31i042": Stash(26, 0),
    "m01s31i043": Stash(26, 0),
    "m01s31i044": Stash(26, 0),
    "m01s31i045": Stash(26, 0),
    "m01s31i046": Stash(26, 0),
    "m01s31i257": Stash(27, 0),
    "m01s31i258": Stash(28, 0),
    "m01s31i259": Stash(26, 0),
    "m01s31i260": Stash(26, 0),
    "m01s31i261": Stash(26, 0),
    "m01s31i262": Stash(26, 0),
    "m01s31i263": Stash(26, 0),
    "m01s31i264": Stash(26, 0),
    "m01s31i265": Stash(26, 0),
    "m01s31i266": Stash(27, 0),
    "m01s31i267": Stash(28, 0),
    "m01s31i268": Stash(26, 0),
    "m01s31i269": Stash(26, 0),
    "m01s31i270": Stash(26, 0),
    "m01s31i271": Stash(26, 0),
    "m01s31i272": Stash(26, 0),
    "m01s31i273": Stash(26, 0),
    "m01s31i274": Stash(26, 0),
    "m01s31i275": Stash(26, 0),
    "m01s31i276": Stash(26, 0),
    "m01s31i277": Stash(26, 0),
    "m01s31i278": Stash(26, 0),
    "m01s31i279": Stash(26, 0),
    "m01s31i280": Stash(26, 0),
    "m01s31i281": Stash(26, 0),
    "m01s31i282": Stash(26, 0),
    "m01s31i283": Stash(26, 0),
    "m01s31i284": Stash(26, 0),
    "m01s31i285": Stash(26, 0),
    "m01s31i286": Stash(26, 0),
    "m01s31i288": Stash(26, 0),
    "m01s31i289": Stash(26, 0),
    "m01s31i290": Stash(26, 0),
    "m01s31i291": Stash(26, 0),
    "m01s31i292": Stash(26, 0),
    "m01s31i293": Stash(26, 0),
    "m01s31i294": Stash(26, 0),
    "m01s31i295": Stash(26, 0),
    "m01s31i296": Stash(26, 0),
    "m01s31i297": Stash(26, 0),
    "m01s31i298": Stash(26, 0),
    "m01s31i299": Stash(26, 0),
    "m01s32i001": Stash(29, 0),
    "m01s32i002": Stash(27, 0),
    "m01s32i003": Stash(28, 0),
    "m01s32i004": Stash(26, 0),
    "m01s32i005": Stash(26, 0),
    "m01s32i006": Stash(26, 0),
    "m01s32i007": Stash(26, 0),
    "m01s32i008": Stash(26, 0),
    "m01s32i009": Stash(26, 0),
    "m01s32i010": Stash(26, 0),
    "m01s32i011": Stash(27, 0),
    "m01s32i012": Stash(28, 0),
    "m01s32i013": Stash(26, 0),
    "m01s32i014": Stash(26, 0),
    "m01s32i015": Stash(26, 0),
    "m01s32i016": Stash(26, 0),
    "m01s32i017": Stash(26, 0),
    "m01s32i018": Stash(26, 0),
    "m01s32i019": Stash(26, 0),
    "m01s32i020": Stash(26, 0),
    "m01s32i021": Stash(26, 0),
    "m01s32i022": Stash(26, 0),
    "m01s32i023": Stash(26, 0),
    "m01s32i024": Stash(26, 0),
    "m01s32i025": Stash(26, 0),
    "m01s32i026": Stash(26, 0),
    "m01s32i027": Stash(26, 0),
    "m01s32i028": Stash(26, 0),
    "m01s32i029": Stash(26, 0),
    "m01s32i030": Stash(26, 0),
    "m01s32i031": Stash(26, 0),
    "m01s32i032": Stash(26, 0),
    "m01s32i033": Stash(26, 0),
    "m01s32i035": Stash(26, 0),
    "m01s32i036": Stash(26, 0),
    "m01s32i037": Stash(26, 0),
    "m01s32i038": Stash(26, 0),
    "m01s32i039": Stash(26, 0),
    "m01s32i040": Stash(26, 0),
    "m01s32i041": Stash(26, 0),
    "m01s32i042": Stash(26, 0),
    "m01s32i043": Stash(26, 0),
    "m01s32i044": Stash(26, 0),
    "m01s32i045": Stash(26, 0),
    "m01s32i046": Stash(26, 0),
    "m01s33i001": Stash(1, 501),
    "m01s33i002": Stash(1, 502),
    "m01s33i003": Stash(1, 503),
    "m01s33i004": Stash(1, 504),
    "m01s33i005": Stash(1, 505),
    "m01s33i006": Stash(1, 506),
    "m01s33i007": Stash(1, 507),
    "m01s33i008": Stash(1, 508),
    "m01s33i009": Stash(1, 509),
    "m01s33i010": Stash(1, 510),
    "m01s33i011": Stash(1, 511),
    "m01s33i012": Stash(1, 512),
    "m01s33i013": Stash(1, 513),
    "m01s33i014": Stash(1, 514),
    "m01s33i015": Stash(1, 515),
    "m01s33i016": Stash(1, 516),
    "m01s33i017": Stash(1, 517),
    "m01s33i018": Stash(1, 518),
    "m01s33i019": Stash(1, 519),
    "m01s33i020": Stash(1, 520),
    "m01s33i021": Stash(1, 521),
    "m01s33i022": Stash(1, 522),
    "m01s33i023": Stash(1, 523),
    "m01s33i024": Stash(1, 524),
    "m01s33i025": Stash(1, 525),
    "m01s33i026": Stash(1, 526),
    "m01s33i027": Stash(1, 527),
    "m01s33i028": Stash(1, 528),
    "m01s33i029": Stash(1, 529),
    "m01s33i030": Stash(1, 530),
    "m01s33i031": Stash(1, 0),
    "m01s33i032": Stash(1, 0),
    "m01s33i033": Stash(1, 0),
    "m01s33i034": Stash(1, 0),
    "m01s33i035": Stash(1, 0),
    "m01s33i036": Stash(1, 0),
    "m01s33i037": Stash(1, 501),
    "m01s33i038": Stash(1, 501),
    "m01s33i039": Stash(1, 501),
    "m01s33i040": Stash(1, 501),
    "m01s33i041": Stash(1, 501),
    "m01s33i042": Stash(1, 501),
    "m01s33i043": Stash(1, 501),
    "m01s33i044": Stash(1, 501),
    "m01s33i045": Stash(1, 501),
    "m01s33i046": Stash(1, 501),
    "m01s33i047": Stash(1, 501),
    "m01s33i048": Stash(1, 501),
    "m01s33i049": Stash(1, 501),
    "m01s33i050": Stash(1, 501),
    "m01s33i051": Stash(1, 501),
    "m01s33i052": Stash(1, 501),
    "m01s33i053": Stash(1, 501),
    "m01s33i054": Stash(1, 501),
    "m01s33i055": Stash(1, 501),
    "m01s33i056": Stash(1, 501),
    "m01s33i057": Stash(1, 501),
    "m01s33i058": Stash(1, 501),
    "m01s33i059": Stash(1, 501),
    "m01s33i060": Stash(1, 501),
    "m01s33i061": Stash(1, 501),
    "m01s33i062": Stash(1, 501),
    "m01s33i063": Stash(1, 501),
    "m01s33i064": Stash(1, 501),
    "m01s33i065": Stash(1, 501),
    "m01s33i066": Stash(1, 501),
    "m01s33i067": Stash(1, 501),
    "m01s33i068": Stash(1, 501),
    "m01s33i069": Stash(1, 501),
    "m01s33i070": Stash(1, 501),
    "m01s33i071": Stash(1, 501),
    "m01s33i072": Stash(1, 501),
    "m01s33i073": Stash(1, 501),
    "m01s33i074": Stash(1, 501),
    "m01s33i075": Stash(1, 501),
    "m01s33i076": Stash(1, 501),
    "m01s33i077": Stash(1, 501),
    "m01s33i078": Stash(1, 501),
    "m01s33i079": Stash(1, 501),
    "m01s33i080": Stash(1, 501),
    "m01s33i081": Stash(1, 501),
    "m01s33i082": Stash(1, 501),
    "m01s33i083": Stash(1, 501),
    "m01s33i084": Stash(1, 501),
    "m01s33i085": Stash(1, 501),
    "m01s33i086": Stash(1, 501),
    "m01s33i087": Stash(1, 501),
    "m01s33i088": Stash(1, 501),
    "m01s33i089": Stash(1, 501),
    "m01s33i090": Stash(1, 501),
    "m01s33i091": Stash(1, 501),
    "m01s33i092": Stash(1, 501),
    "m01s33i093": Stash(1, 501),
    "m01s33i094": Stash(1, 501),
    "m01s33i095": Stash(1, 501),
    "m01s33i096": Stash(1, 501),
    "m01s33i097": Stash(1, 501),
    "m01s33i098": Stash(1, 501),
    "m01s33i099": Stash(1, 501),
    "m01s33i100": Stash(1, 501),
    "m01s33i101": Stash(1, 501),
    "m01s33i102": Stash(1, 501),
    "m01s33i103": Stash(1, 501),
    "m01s33i104": Stash(1, 501),
    "m01s33i105": Stash(1, 501),
    "m01s33i106": Stash(1, 501),
    "m01s33i107": Stash(1, 501),
    "m01s33i108": Stash(1, 501),
    "m01s33i109": Stash(1, 501),
    "m01s33i110": Stash(1, 501),
    "m01s33i111": Stash(1, 501),
    "m01s33i112": Stash(1, 501),
    "m01s33i113": Stash(1, 501),
    "m01s33i114": Stash(1, 501),
    "m01s33i115": Stash(1, 501),
    "m01s33i116": Stash(1, 501),
    "m01s33i117": Stash(1, 501),
    "m01s33i118": Stash(1, 501),
    "m01s33i119": Stash(1, 501),
    "m01s33i120": Stash(1, 501),
    "m01s33i121": Stash(1, 501),
    "m01s33i122": Stash(1, 501),
    "m01s33i123": Stash(1, 501),
    "m01s33i124": Stash(1, 501),
    "m01s33i125": Stash(1, 501),
    "m01s33i126": Stash(1, 501),
    "m01s33i127": Stash(1, 501),
    "m01s33i128": Stash(1, 501),
    "m01s33i129": Stash(1, 501),
    "m01s33i130": Stash(1, 501),
    "m01s33i131": Stash(1, 501),
    "m01s33i132": Stash(1, 501),
    "m01s33i133": Stash(1, 501),
    "m01s33i134": Stash(1, 501),
    "m01s33i135": Stash(1, 501),
    "m01s33i136": Stash(1, 501),
    "m01s33i137": Stash(1, 501),
    "m01s33i138": Stash(1, 501),
    "m01s33i139": Stash(1, 501),
    "m01s33i140": Stash(1, 501),
    "m01s33i141": Stash(1, 501),
    "m01s33i142": Stash(1, 501),
    "m01s33i143": Stash(1, 501),
    "m01s33i144": Stash(1, 501),
    "m01s33i145": Stash(1, 501),
    "m01s33i146": Stash(1, 501),
    "m01s33i147": Stash(1, 501),
    "m01s33i148": Stash(1, 501),
    "m01s33i149": Stash(1, 501),
    "m01s33i150": Stash(1, 501),
    "m01s33i151": Stash(1, 581),
    "m01s33i152": Stash(1, 0),
    "m01s33i153": Stash(1, 0),
    "m01s33i154": Stash(1, 0),
    "m01s33i155": Stash(1, 0),
    "m01s33i156": Stash(1, 0),
    "m01s33i157": Stash(1, 0),
    "m01s33i158": Stash(1, 0),
    "m01s33i159": Stash(1, 0),
    "m01s33i160": Stash(1, 0),
    "m01s33i161": Stash(1, 0),
    "m01s33i162": Stash(1, 0),
    "m01s33i163": Stash(1, 0),
    "m01s33i164": Stash(1, 0),
    "m01s33i165": Stash(1, 0),
    "m01s33i166": Stash(1, 0),
    "m01s33i167": Stash(1, 667),
    "m01s33i168": Stash(1, 668),
    "m01s33i169": Stash(1, 669),
    "m01s33i170": Stash(1, 670),
    "m01s33i171": Stash(1, 671),
    "m01s33i172": Stash(1, 672),
    "m01s33i173": Stash(1, 673),
    "m01s33i301": Stash(1, 1878),
    "m01s33i302": Stash(1, 502),
    "m01s33i303": Stash(1, 503),
    "m01s33i304": Stash(1, 504),
    "m01s33i305": Stash(1, 505),
    "m01s33i306": Stash(1, 506),
    "m01s33i307": Stash(1, 507),
    "m01s33i308": Stash(1, 508),
    "m01s33i309": Stash(1, 509),
    "m01s33i310": Stash(1, 510),
    "m01s33i311": Stash(1, 511),
    "m01s33i312": Stash(1, 512),
    "m01s33i313": Stash(1, 513),
    "m01s33i314": Stash(1, 514),
    "m01s33i315": Stash(1, 515),
    "m01s33i316": Stash(1, 516),
    "m01s33i317": Stash(1, 517),
    "m01s33i318": Stash(1, 518),
    "m01s33i319": Stash(1, 519),
    "m01s33i320": Stash(1, 520),
    "m01s33i321": Stash(1, 521),
    "m01s33i322": Stash(1, 522),
    "m01s33i323": Stash(1, 523),
    "m01s33i324": Stash(1, 524),
    "m01s33i325": Stash(1, 525),
    "m01s33i326": Stash(1, 526),
    "m01s33i327": Stash(1, 527),
    "m01s33i328": Stash(1, 528),
    "m01s33i329": Stash(1, 529),
    "m01s33i330": Stash(1, 530),
    "m01s33i331": Stash(1, 0),
    "m01s33i332": Stash(1, 0),
    "m01s33i333": Stash(1, 0),
    "m01s33i334": Stash(1, 0),
    "m01s33i335": Stash(1, 0),
    "m01s33i336": Stash(1, 0),
    "m01s33i337": Stash(1, 0),
    "m01s33i338": Stash(1, 0),
    "m01s33i339": Stash(1, 0),
    "m01s33i340": Stash(1, 0),
    "m01s33i341": Stash(1, 0),
    "m01s33i342": Stash(1, 0),
    "m01s33i343": Stash(1, 0),
    "m01s33i344": Stash(1, 0),
    "m01s33i345": Stash(1, 0),
    "m01s33i346": Stash(1, 0),
    "m01s33i347": Stash(1, 0),
    "m01s33i348": Stash(1, 0),
    "m01s33i349": Stash(1, 0),
    "m01s33i350": Stash(1, 0),
    "m01s33i351": Stash(1, 0),
    "m01s33i352": Stash(1, 0),
    "m01s33i353": Stash(1, 0),
    "m01s33i354": Stash(1, 0),
    "m01s33i355": Stash(1, 0),
    "m01s33i356": Stash(1, 0),
    "m01s33i357": Stash(1, 0),
    "m01s33i358": Stash(1, 0),
    "m01s33i359": Stash(1, 0),
    "m01s33i360": Stash(1, 0),
    "m01s33i361": Stash(1, 0),
    "m01s33i362": Stash(1, 0),
    "m01s33i363": Stash(1, 0),
    "m01s33i364": Stash(1, 0),
    "m01s33i365": Stash(1, 0),
    "m01s33i366": Stash(1, 0),
    "m01s33i367": Stash(1, 0),
    "m01s33i368": Stash(1, 568),
    "m01s33i369": Stash(1, 569),
    "m01s33i370": Stash(1, 570),
    "m01s33i371": Stash(1, 571),
    "m01s33i372": Stash(1, 572),
    "m01s33i373": Stash(1, 573),
    "m01s33i374": Stash(1, 574),
    "m01s33i375": Stash(1, 575),
    "m01s33i376": Stash(1, 576),
    "m01s33i377": Stash(1, 0),
    "m01s33i378": Stash(1, 0),
    "m01s33i379": Stash(1, 0),
    "m01s33i380": Stash(1, 580),
    "m01s33i381": Stash(1, 581),
    "m01s33i382": Stash(1, 0),
    "m01s33i383": Stash(1, 0),
    "m01s33i384": Stash(1, 0),
    "m01s33i385": Stash(1, 0),
    "m01s33i386": Stash(1, 0),
    "m01s33i387": Stash(1, 0),
    "m01s33i388": Stash(1, 0),
    "m01s33i389": Stash(1, 0),
    "m01s33i390": Stash(1, 0),
    "m01s33i391": Stash(1, 0),
    "m01s33i392": Stash(1, 0),
    "m01s33i393": Stash(1, 0),
    "m01s33i394": Stash(1, 0),
    "m01s33i395": Stash(1, 0),
    "m01s33i396": Stash(1, 0),
    "m01s33i397": Stash(1, 0),
    "m01s33i398": Stash(1, 0),
    "m01s33i399": Stash(1, 0),
    "m01s33i400": Stash(1, 600),
    "m01s33i401": Stash(1, 601),
    "m01s33i402": Stash(1, 602),
    "m01s33i403": Stash(1, 603),
    "m01s33i404": Stash(1, 604),
    "m01s33i405": Stash(1, 605),
    "m01s33i406": Stash(1, 606),
    "m01s33i407": Stash(1, 607),
    "m01s33i408": Stash(1, 608),
    "m01s33i409": Stash(1, 609),
    "m01s33i410": Stash(1, 610),
    "m01s33i411": Stash(1, 611),
    "m01s33i412": Stash(1, 612),
    "m01s33i413": Stash(1, 613),
    "m01s33i414": Stash(1, 614),
    "m01s33i415": Stash(1, 615),
    "m01s33i416": Stash(1, 616),
    "m01s33i417": Stash(1, 617),
    "m01s33i418": Stash(1, 618),
    "m01s33i419": Stash(1, 619),
    "m01s33i420": Stash(1, 620),
    "m01s33i421": Stash(1, 621),
    "m01s33i422": Stash(1, 622),
    "m01s33i423": Stash(1, 623),
    "m01s33i424": Stash(1, 624),
    "m01s33i425": Stash(1, 625),
    "m01s33i426": Stash(1, 626),
    "m01s33i427": Stash(1, 627),
    "m01s33i428": Stash(1, 628),
    "m01s33i429": Stash(1, 629),
    "m01s33i430": Stash(1, 630),
    "m01s33i431": Stash(1, 631),
    "m01s33i432": Stash(1, 632),
    "m01s33i433": Stash(1, 633),
    "m01s33i434": Stash(1, 634),
    "m01s33i435": Stash(1, 635),
    "m01s33i436": Stash(1, 636),
    "m01s33i437": Stash(1, 637),
    "m01s33i438": Stash(1, 638),
    "m01s33i439": Stash(1, 639),
    "m01s33i440": Stash(1, 640),
    "m01s33i441": Stash(1, 641),
    "m01s33i442": Stash(1, 642),
    "m01s33i443": Stash(1, 643),
    "m01s33i444": Stash(1, 644),
    "m01s33i445": Stash(1, 645),
    "m01s33i446": Stash(1, 646),
    "m01s33i447": Stash(1, 647),
    "m01s33i448": Stash(1, 648),
    "m01s33i449": Stash(1, 649),
    "m01s33i450": Stash(1, 650),
    "m01s33i451": Stash(1, 651),
    "m01s33i452": Stash(1, 652),
    "m01s33i453": Stash(1, 653),
    "m01s33i454": Stash(1, 654),
    "m01s33i455": Stash(1, 655),
    "m01s33i456": Stash(1, 656),
    "m01s33i457": Stash(1, 657),
    "m01s33i458": Stash(1, 658),
    "m01s33i459": Stash(1, 659),
    "m01s33i460": Stash(1, 660),
    "m01s33i461": Stash(1, 661),
    "m01s33i462": Stash(1, 662),
    "m01s33i463": Stash(1, 653),
    "m01s33i464": Stash(1, 664),
    "m01s33i465": Stash(1, 665),
    "m01s33i466": Stash(1, 666),
    "m01s33i467": Stash(1, 667),
    "m01s33i468": Stash(1, 668),
    "m01s33i469": Stash(1, 669),
    "m01s33i470": Stash(1, 670),
    "m01s33i481": Stash(1, 671),
    "m01s33i482": Stash(1, 672),
    "m01s33i483": Stash(1, 673),
    "m01s33i484": Stash(1, 674),
    "m01s33i485": Stash(1, 675),
    "m01s33i486": Stash(1, 676),
    "m01s33i487": Stash(1, 677),
    "m01s33i488": Stash(1, 678),
    "m01s33i489": Stash(1, 679),
    "m01s33i490": Stash(1, 680),
    "m01s33i491": Stash(1, 681),
    "m01s33i492": Stash(1, 682),
    "m01s33i493": Stash(1, 683),
    "m01s33i494": Stash(1, 684),
    "m01s33i495": Stash(1, 685),
    "m01s33i496": Stash(1, 686),
    "m01s33i497": Stash(1, 687),
    "m01s33i498": Stash(1, 688),
    "m01s33i499": Stash(1, 689),
    "m01s33i500": Stash(1, 690),
    "m01s33i501": Stash(1, 691),
    "m01s33i502": Stash(1, 692),
    "m01s33i503": Stash(1, 693),
    "m01s33i504": Stash(1, 694),
    "m01s33i505": Stash(1, 695),
    "m01s33i506": Stash(1, 696),
    "m01s34i001": Stash(1, 0),
    "m01s34i002": Stash(1, 0),
    "m01s34i003": Stash(1, 0),
    "m01s34i004": Stash(1, 0),
    "m01s34i005": Stash(1, 0),
    "m01s34i006": Stash(1, 0),
    "m01s34i007": Stash(1, 0),
    "m01s34i008": Stash(1, 0),
    "m01s34i009": Stash(1, 0),
    "m01s34i010": Stash(1, 0),
    "m01s34i011": Stash(1, 0),
    "m01s34i012": Stash(1, 0),
    "m01s34i013": Stash(1, 0),
    "m01s34i014": Stash(1, 0),
    "m01s34i015": Stash(1, 0),
    "m01s34i016": Stash(1, 0),
    "m01s34i017": Stash(1, 0),
    "m01s34i018": Stash(1, 0),
    "m01s34i019": Stash(1, 0),
    "m01s34i020": Stash(1, 0),
    "m01s34i021": Stash(1, 0),
    "m01s34i022": Stash(1, 0),
    "m01s34i023": Stash(1, 0),
    "m01s34i024": Stash(1, 0),
    "m01s34i025": Stash(1, 0),
    "m01s34i026": Stash(1, 0),
    "m01s34i027": Stash(1, 0),
    "m01s34i028": Stash(1, 0),
    "m01s34i029": Stash(1, 0),
    "m01s34i030": Stash(1, 0),
    "m01s34i031": Stash(1, 0),
    "m01s34i032": Stash(1, 0),
    "m01s34i033": Stash(1, 0),
    "m01s34i034": Stash(1, 0),
    "m01s34i035": Stash(1, 0),
    "m01s34i036": Stash(1, 0),
    "m01s34i037": Stash(1, 0),
    "m01s34i038": Stash(1, 0),
    "m01s34i039": Stash(1, 1861),
    "m01s34i040": Stash(1, 0),
    "m01s34i041": Stash(1, 0),
    "m01s34i042": Stash(1, 0),
    "m01s34i043": Stash(1, 0),
    "m01s34i044": Stash(1, 0),
    "m01s34i045": Stash(1, 0),
    "m01s34i046": Stash(1, 1861),
    "m01s34i047": Stash(1, 0),
    "m01s34i048": Stash(1, 0),
    "m01s34i049": Stash(1, 0),
    "m01s34i050": Stash(1, 1861),
    "m01s34i051": Stash(1, 0),
    "m01s34i052": Stash(1, 0),
    "m01s34i053": Stash(1, 0),
    "m01s34i054": Stash(1, 0),
    "m01s34i055": Stash(1, 0),
    "m01s34i056": Stash(1, 0),
    "m01s34i057": Stash(1, 0),
    "m01s34i058": Stash(1, 0),
    "m01s34i059": Stash(1, 0),
    "m01s34i060": Stash(1, 0),
    "m01s34i061": Stash(1, 1861),
    "m01s34i062": Stash(1, 1861),
    "m01s34i063": Stash(1, 1861),
    "m01s34i064": Stash(1, 1861),
    "m01s34i065": Stash(1, 1861),
    "m01s34i066": Stash(1, 1861),
    "m01s34i067": Stash(1, 1861),
    "m01s34i068": Stash(1, 1861),
    "m01s34i069": Stash(1, 1861),
    "m01s34i070": Stash(1, 0),
    "m01s34i071": Stash(1, 0),
    "m01s34i072": Stash(1, 0),
    "m01s34i073": Stash(1, 0),
    "m01s34i074": Stash(1, 0),
    "m01s34i075": Stash(1, 0),
    "m01s34i076": Stash(1, 0),
    "m01s34i077": Stash(1, 0),
    "m01s34i078": Stash(1, 0),
    "m01s34i079": Stash(1, 0),
    "m01s34i080": Stash(1, 0),
    "m01s34i081": Stash(1, 0),
    "m01s34i082": Stash(1, 0),
    "m01s34i083": Stash(1, 0),
    "m01s34i084": Stash(1, 0),
    "m01s34i085": Stash(1, 0),
    "m01s34i086": Stash(1, 0),
    "m01s34i087": Stash(1, 0),
    "m01s34i088": Stash(1, 0),
    "m01s34i089": Stash(1, 0),
    "m01s34i090": Stash(1, 0),
    "m01s34i091": Stash(1, 0),
    "m01s34i092": Stash(1, 0),
    "m01s34i093": Stash(1, 0),
    "m01s34i094": Stash(1, 0),
    "m01s34i095": Stash(1, 0),
    "m01s34i096": Stash(1, 0),
    "m01s34i097": Stash(1, 0),
    "m01s34i098": Stash(1, 0),
    "m01s34i099": Stash(1, 0),
    "m01s34i100": Stash(1, 0),
    "m01s34i101": Stash(1, 0),
    "m01s34i102": Stash(1, 0),
    "m01s34i103": Stash(1, 0),
    "m01s34i104": Stash(1, 0),
    "m01s34i105": Stash(1, 0),
    "m01s34i106": Stash(1, 0),
    "m01s34i107": Stash(1, 0),
    "m01s34i108": Stash(1, 0),
    "m01s34i109": Stash(1, 0),
    "m01s34i110": Stash(1, 0),
    "m01s34i111": Stash(1, 0),
    "m01s34i112": Stash(1, 0),
    "m01s34i113": Stash(1, 0),
    "m01s34i114": Stash(1, 0),
    "m01s34i115": Stash(1, 0),
    "m01s34i116": Stash(1, 0),
    "m01s34i117": Stash(1, 0),
    "m01s34i118": Stash(1, 0),
    "m01s34i119": Stash(1, 0),
    "m01s34i120": Stash(1, 0),
    "m01s34i121": Stash(1, 0),
    "m01s34i122": Stash(1, 0),
    "m01s34i123": Stash(1, 0),
    "m01s34i124": Stash(1, 0),
    "m01s34i125": Stash(1, 0),
    "m01s34i126": Stash(1, 0),
    "m01s34i127": Stash(1, 0),
    "m01s34i128": Stash(1, 0),
    "m01s34i129": Stash(1, 0),
    "m01s34i130": Stash(1, 0),
    "m01s34i131": Stash(1, 0),
    "m01s34i132": Stash(1, 0),
    "m01s34i133": Stash(1, 0),
    "m01s34i134": Stash(1, 0),
    "m01s34i135": Stash(1, 0),
    "m01s34i136": Stash(1, 0),
    "m01s34i137": Stash(1, 0),
    "m01s34i138": Stash(1, 0),
    "m01s34i139": Stash(1, 0),
    "m01s34i140": Stash(1, 1861),
    "m01s34i141": Stash(1, 1861),
    "m01s34i142": Stash(1, 1861),
    "m01s34i143": Stash(1, 1861),
    "m01s34i144": Stash(1, 1861),
    "m01s34i145": Stash(1, 1861),
    "m01s34i146": Stash(1, 1861),
    "m01s34i147": Stash(1, 1861),
    "m01s34i148": Stash(1, 1861),
    "m01s34i149": Stash(1, 0),
    "m01s34i150": Stash(1, 0),
    "m01s34i151": Stash(1, 1861),
    "m01s34i152": Stash(1, 1861),
    "m01s34i153": Stash(1, 1861),
    "m01s34i154": Stash(1, 1861),
    "m01s34i155": Stash(1, 1861),
    "m01s34i156": Stash(1, 1861),
    "m01s34i157": Stash(1, 1864),
    "m01s34i158": Stash(1, 1861),
    "m01s34i159": Stash(1, 158),
    "m01s34i160": Stash(1, 1861),
    "m01s34i161": Stash(1, 1861),
    "m01s34i162": Stash(1, 1862),
    "m01s34i163": Stash(1, 1862),
    "m01s34i164": Stash(1, 1861),
    "m01s34i165": Stash(1, 1861),
    "m01s34i166": Stash(1, 1861),
    "m01s34i167": Stash(1, 1861),
    "m01s34i168": Stash(1, 1861),
    "m01s34i169": Stash(1, 1861),
    "m01s34i170": Stash(1, 1861),
    "m01s34i171": Stash(1, 1861),
    "m01s34i172": Stash(1, 1861),
    "m01s34i173": Stash(1, 1861),
    "m01s34i174": Stash(1, 1861),
    "m01s34i175": Stash(1, 1861),
    "m01s34i176": Stash(1, 1861),
    "m01s34i177": Stash(1, 1861),
    "m01s34i178": Stash(1, 1861),
    "m01s34i179": Stash(1, 1861),
    "m01s34i180": Stash(1, 1881),
    "m01s34i181": Stash(1, 1881),
    "m01s34i182": Stash(1, 1881),
    "m01s34i183": Stash(1, 1881),
    "m01s34i184": Stash(1, 1881),
    "m01s34i185": Stash(1, 1881),
    "m01s34i186": Stash(1, 1881),
    "m01s34i187": Stash(1, 1881),
    "m01s34i188": Stash(1, 1881),
    "m01s34i189": Stash(1, 1881),
    "m01s34i190": Stash(1, 1881),
    "m01s34i191": Stash(1, 1881),
    "m01s34i192": Stash(1, 1881),
    "m01s34i193": Stash(1, 1881),
    "m01s34i194": Stash(1, 1881),
    "m01s34i195": Stash(1, 1881),
    "m01s34i196": Stash(1, 1881),
    "m01s34i197": Stash(1, 1881),
    "m01s34i198": Stash(1, 1881),
    "m01s34i199": Stash(1, 1881),
    "m01s34i200": Stash(1, 1881),
    "m01s34i201": Stash(1, 1881),
    "m01s34i202": Stash(1, 1881),
    "m01s34i203": Stash(1, 1881),
    "m01s34i204": Stash(1, 1881),
    "m01s34i205": Stash(1, 1881),
    "m01s34i206": Stash(1, 1881),
    "m01s34i207": Stash(1, 1881),
    "m01s34i208": Stash(1, 1881),
    "m01s34i209": Stash(1, 1881),
    "m01s34i210": Stash(1, 1881),
    "m01s34i211": Stash(1, 1881),
    "m01s34i212": Stash(1, 1881),
    "m01s34i213": Stash(1, 1881),
    "m01s34i214": Stash(1, 1881),
    "m01s34i215": Stash(1, 1881),
    "m01s34i216": Stash(1, 1881),
    "m01s34i217": Stash(1, 1881),
    "m01s34i218": Stash(1, 1881),
    "m01s34i219": Stash(1, 1881),
    "m01s34i220": Stash(1, 1881),
    "m01s34i221": Stash(1, 1881),
    "m01s34i222": Stash(1, 1881),
    "m01s34i223": Stash(1, 1881),
    "m01s34i224": Stash(1, 1881),
    "m01s34i225": Stash(1, 1873),
    "m01s34i226": Stash(1, 1878),
    "m01s34i227": Stash(1, 1873),
    "m01s34i228": Stash(1, 1878),
    "m01s34i231": Stash(1, 1883),
    "m01s34i232": Stash(1, 1883),
    "m01s34i233": Stash(1, 8),
    "m01s34i234": Stash(1, 8),
    "m01s34i235": Stash(1, 8),
    "m01s34i236": Stash(1, 1875),
    "m01s34i237": Stash(1, 1875),
    "m01s34i238": Stash(1, 1875),
    "m01s34i239": Stash(1, 1875),
    "m01s34i240": Stash(1, 1875),
    "m01s34i241": Stash(1, 1875),
    "m01s34i242": Stash(1, 1875),
    "m01s34i243": Stash(1, 1875),
    "m01s34i244": Stash(1, 1875),
    "m01s34i245": Stash(1, 1875),
    "m01s34i246": Stash(1, 1875),
    "m01s34i247": Stash(1, 1875),
    "m01s34i248": Stash(1, 1875),
    "m01s34i249": Stash(1, 1875),
    "m01s34i250": Stash(1, 1875),
    "m01s34i251": Stash(1, 1875),
    "m01s34i252": Stash(1, 1875),
    "m01s34i253": Stash(1, 1875),
    "m01s34i254": Stash(1, 1875),
    "m01s34i255": Stash(1, 1875),
    "m01s34i256": Stash(1, 1875),
    "m01s34i257": Stash(1, 1875),
    "m01s34i258": Stash(1, 1875),
    "m01s34i259": Stash(1, 1875),
    "m01s34i260": Stash(1, 1875),
    "m01s34i261": Stash(1, 1875),
    "m01s34i262": Stash(1, 1875),
    "m01s34i263": Stash(1, 1875),
    "m01s34i264": Stash(1, 1875),
    "m01s34i265": Stash(1, 1875),
    "m01s34i266": Stash(1, 1875),
    "m01s34i267": Stash(1, 1875),
    "m01s34i268": Stash(1, 1875),
    "m01s34i269": Stash(1, 1875),
    "m01s34i270": Stash(1, 1875),
    "m01s34i271": Stash(1, 1875),
    "m01s34i272": Stash(1, 1875),
    "m01s34i273": Stash(1, 1875),
    "m01s34i274": Stash(1, 1875),
    "m01s34i275": Stash(1, 1875),
    "m01s34i276": Stash(1, 1875),
    "m01s34i277": Stash(1, 1875),
    "m01s34i278": Stash(1, 1875),
    "m01s34i279": Stash(1, 1875),
    "m01s34i280": Stash(1, 1875),
    "m01s34i281": Stash(1, 1875),
    "m01s34i282": Stash(1, 1875),
    "m01s34i283": Stash(1, 1875),
    "m01s34i284": Stash(1, 1875),
    "m01s34i285": Stash(1, 1875),
    "m01s34i286": Stash(1, 1875),
    "m01s34i287": Stash(1, 1875),
    "m01s34i288": Stash(1, 1875),
    "m01s34i289": Stash(1, 1875),
    "m01s34i290": Stash(1, 1875),
    "m01s34i291": Stash(1, 1875),
    "m01s34i292": Stash(1, 1875),
    "m01s34i293": Stash(1, 1875),
    "m01s34i294": Stash(1, 1875),
    "m01s34i295": Stash(1, 1875),
    "m01s34i296": Stash(1, 1875),
    "m01s34i297": Stash(1, 1875),
    "m01s34i298": Stash(1, 1875),
    "m01s34i299": Stash(1, 1875),
    "m01s34i300": Stash(1, 1875),
    "m01s34i301": Stash(1, 1871),
    "m01s34i302": Stash(1, 1871),
    "m01s34i303": Stash(1, 1871),
    "m01s34i304": Stash(1, 1871),
    "m01s34i305": Stash(1, 1871),
    "m01s34i306": Stash(1, 1871),
    "m01s34i307": Stash(1, 1871),
    "m01s34i308": Stash(1, 1875),
    "m01s34i309": Stash(1, 1875),
    "m01s34i310": Stash(1, 1875),
    "m01s34i311": Stash(1, 1871),
    "m01s34i312": Stash(1, 1871),
    "m01s34i313": Stash(1, 1871),
    "m01s34i314": Stash(1, 1871),
    "m01s34i315": Stash(1, 1871),
    "m01s34i316": Stash(1, 1871),
    "m01s34i317": Stash(1, 1871),
    "m01s34i318": Stash(1, 1875),
    "m01s34i319": Stash(1, 1875),
    "m01s34i320": Stash(1, 1875),
    "m01s34i321": Stash(1, 1871),
    "m01s34i322": Stash(1, 1871),
    "m01s34i323": Stash(1, 1875),
    "m01s34i324": Stash(1, 1875),
    "m01s34i325": Stash(1, 1875),
    "m01s34i326": Stash(1, 1875),
    "m01s34i327": Stash(1, 1875),
    "m01s34i328": Stash(1, 1875),
    "m01s34i329": Stash(1, 1875),
    "m01s34i330": Stash(1, 1875),
    "m01s34i331": Stash(1, 1871),
    "m01s34i332": Stash(1, 1875),
    "m01s34i333": Stash(1, 1875),
    "m01s34i334": Stash(1, 1875),
    "m01s34i335": Stash(1, 1875),
    "m01s34i336": Stash(1, 1875),
    "m01s34i337": Stash(1, 1875),
    "m01s34i338": Stash(1, 1875),
    "m01s34i341": Stash(1, 1871),
    "m01s34i342": Stash(1, 1871),
    "m01s34i343": Stash(1, 1871),
    "m01s34i344": Stash(1, 1871),
    "m01s34i345": Stash(1, 1871),
    "m01s34i346": Stash(1, 1871),
    "m01s34i351": Stash(1, 1871),
    "m01s34i352": Stash(1, 1871),
    "m01s34i353": Stash(1, 1861),
    "m01s34i354": Stash(1, 1871),
    "m01s34i361": Stash(1, 1902),
    "m01s34i362": Stash(1, 0),
    "m01s34i363": Stash(1, 1902),
    "m01s34i371": Stash(1, 1871),
    "m01s34i372": Stash(1, 1871),
    "m01s34i373": Stash(1, 1871),
    "m01s34i374": Stash(1, 1871),
    "m01s34i375": Stash(1, 1871),
    "m01s34i376": Stash(1, 1871),
    "m01s34i377": Stash(1, 1871),
    "m01s34i378": Stash(1, 1871),
    "m01s34i379": Stash(1, 1871),
    "m01s34i380": Stash(1, 1875),
    "m01s34i381": Stash(1, 1871),
    "m01s34i382": Stash(1, 0),
    "m01s34i383": Stash(1, 0),
    "m01s34i384": Stash(1, 0),
    "m01s34i385": Stash(1, 1875),
    "m01s34i386": Stash(1, 1876),
    "m01s34i391": Stash(1, 1871),
    "m01s34i392": Stash(1, 1871),
    "m01s34i401": Stash(1, 1871),
    "m01s34i402": Stash(1, 1871),
    "m01s34i403": Stash(1, 1871),
    "m01s34i404": Stash(1, 1871),
    "m01s34i405": Stash(1, 1875),
    "m01s34i406": Stash(1, 1875),
    "m01s34i407": Stash(1, 1875),
    "m01s34i408": Stash(1, 1875),
    "m01s34i409": Stash(1, 1875),
    "m01s34i410": Stash(1, 1875),
    "m01s34i411": Stash(1, 1871),
    "m01s34i412": Stash(1, 1871),
    "m01s34i413": Stash(1, 1871),
    "m01s34i414": Stash(1, 1871),
    "m01s34i415": Stash(1, 1871),
    "m01s34i416": Stash(1, 1871),
    "m01s34i417": Stash(1, 1871),
    "m01s34i418": Stash(1, 1871),
    "m01s34i419": Stash(1, 1871),
    "m01s34i420": Stash(1, 1871),
    "m01s34i421": Stash(1, 1871),
    "m01s34i422": Stash(1, 1871),
    "m01s34i423": Stash(1, 1871),
    "m01s34i424": Stash(1, 1871),
    "m01s34i425": Stash(1, 1871),
    "m01s34i426": Stash(1, 1875),
    "m01s34i427": Stash(1, 1875),
    "m01s34i428": Stash(1, 1875),
    "m01s34i429": Stash(1, 1875),
    "m01s34i431": Stash(1, 1871),
    "m01s34i432": Stash(1, 1871),
    "m01s34i433": Stash(1, 1871),
    "m01s34i438": Stash(1, 1875),
    "m01s34i439": Stash(1, 1875),
    "m01s34i440": Stash(1, 1871),
    "m01s34i441": Stash(1, 1871),
    "m01s34i442": Stash(1, 1871),
    "m01s34i443": Stash(1, 1871),
    "m01s34i444": Stash(1, 1871),
    "m01s34i445": Stash(1, 1871),
    "m01s34i446": Stash(1, 1871),
    "m01s34i447": Stash(1, 1871),
    "m01s34i448": Stash(1, 1871),
    "m01s34i449": Stash(1, 1871),
    "m01s34i450": Stash(1, 1871),
    "m01s34i451": Stash(1, 1871),
    "m01s34i452": Stash(1, 1871),
    "m01s34i453": Stash(1, 1871),
    "m01s34i454": Stash(1, 1871),
    "m01s34i455": Stash(1, 1871),
    "m01s34i456": Stash(1, 1875),
    "m01s34i457": Stash(1, 1875),
    "m01s34i458": Stash(1, 1875),
    "m01s34i459": Stash(1, 1875),
    "m01s34i460": Stash(1, 1875),
    "m01s34i461": Stash(1, 1875),
    "m01s34i462": Stash(1, 1875),
    "m01s34i483": Stash(1, 1875),
    "m01s34i484": Stash(1, 1875),
    "m01s34i485": Stash(1, 1875),
    "m01s34i486": Stash(1, 1875),
    "m01s34i487": Stash(1, 1875),
    "m01s34i488": Stash(1, 1875),
    "m01s34i489": Stash(1, 1875),
    "m01s34i490": Stash(1, 1875),
    "m01s34i491": Stash(1, 1875),
    "m01s34i492": Stash(1, 1875),
    "m01s34i493": Stash(1, 1875),
    "m01s34i494": Stash(1, 1875),
    "m01s34i495": Stash(1, 1875),
    "m01s34i496": Stash(1, 1875),
    "m01s34i497": Stash(1, 1875),
    "m01s34i498": Stash(1, 1875),
    "m01s34i499": Stash(1, 1875),
    "m01s34i500": Stash(1, 1875),
    "m01s35i001": Stash(18, 56),
    "m01s35i002": Stash(19, 57),
    "m01s35i003": Stash(18, 318),
    "m01s35i004": Stash(19, 319),
    "m01s35i005": Stash(18, 318),
    "m01s35i006": Stash(19, 319),
    "m01s35i007": Stash(18, 318),
    "m01s35i008": Stash(19, 319),
    "m01s35i009": Stash(1, 63),
    "m01s35i010": Stash(1, 63),
    "m01s35i011": Stash(1, 2049),
    "m01s35i012": Stash(1, 0),
    "m01s35i013": Stash(1, 0),
    "m01s35i014": Stash(1, 0),
    "m01s35i015": Stash(1, 0),
    "m01s35i016": Stash(1, 63),
    "m01s35i017": Stash(1, 63),
    "m01s35i018": Stash(1, 259),
    "m01s35i019": Stash(1, 0),
    "m01s35i020": Stash(1, 63),
    "m01s35i021": Stash(1, 63),
    "m01s35i022": Stash(1, 63),
    "m01s36i001": Stash(26, 0),
    "m01s36i002": Stash(26, 0),
    "m01s36i003": Stash(26, 0),
    "m01s36i004": Stash(26, 0),
    "m01s36i005": Stash(26, 0),
    "m01s36i006": Stash(26, 0),
    "m01s36i007": Stash(26, 0),
    "m01s36i008": Stash(26, 0),
    "m01s36i009": Stash(26, 0),
    "m01s36i010": Stash(26, 0),
    "m01s36i011": Stash(26, 0),
    "m01s36i012": Stash(26, 0),
    "m01s36i013": Stash(26, 0),
    "m01s36i014": Stash(26, 0),
    "m01s36i015": Stash(26, 0),
    "m01s36i016": Stash(26, 0),
    "m01s36i017": Stash(26, 0),
    "m01s36i018": Stash(26, 0),
    "m01s36i019": Stash(26, 0),
    "m01s36i020": Stash(26, 0),
    "m01s36i021": Stash(26, 0),
    "m01s36i022": Stash(26, 0),
    "m01s36i023": Stash(26, 0),
    "m01s36i024": Stash(26, 0),
    "m01s36i025": Stash(26, 0),
    "m01s36i026": Stash(26, 0),
    "m01s36i027": Stash(26, 0),
    "m01s36i028": Stash(26, 0),
    "m01s36i029": Stash(26, 0),
    "m01s36i030": Stash(26, 0),
    "m01s36i031": Stash(26, 0),
    "m01s36i032": Stash(26, 0),
    "m01s36i033": Stash(26, 0),
    "m01s36i034": Stash(26, 0),
    "m01s36i035": Stash(26, 0),
    "m01s36i036": Stash(26, 0),
    "m01s36i037": Stash(26, 0),
    "m01s36i038": Stash(26, 0),
    "m01s36i039": Stash(26, 0),
    "m01s36i040": Stash(26, 0),
    "m01s36i041": Stash(26, 0),
    "m01s36i042": Stash(26, 0),
    "m01s36i043": Stash(26, 0),
    "m01s36i044": Stash(26, 0),
    "m01s36i045": Stash(26, 0),
    "m01s36i046": Stash(26, 0),
    "m01s36i047": Stash(26, 0),
    "m01s36i048": Stash(26, 0),
    "m01s36i049": Stash(26, 0),
    "m01s36i050": Stash(26, 0),
    "m01s36i051": Stash(26, 0),
    "m01s36i052": Stash(26, 0),
    "m01s36i053": Stash(26, 0),
    "m01s36i054": Stash(26, 0),
    "m01s36i055": Stash(26, 0),
    "m01s36i056": Stash(26, 0),
    "m01s36i057": Stash(26, 0),
    "m01s36i058": Stash(26, 0),
    "m01s36i059": Stash(26, 0),
    "m01s36i060": Stash(26, 0),
    "m01s36i061": Stash(26, 0),
    "m01s36i062": Stash(26, 0),
    "m01s36i063": Stash(26, 0),
    "m01s36i064": Stash(26, 0),
    "m01s36i065": Stash(26, 0),
    "m01s36i066": Stash(26, 0),
    "m01s36i067": Stash(26, 0),
    "m01s36i068": Stash(26, 0),
    "m01s36i069": Stash(26, 0),
    "m01s36i070": Stash(26, 0),
    "m01s36i071": Stash(26, 0),
    "m01s36i072": Stash(26, 0),
    "m01s36i073": Stash(26, 0),
    "m01s36i074": Stash(26, 0),
    "m01s36i075": Stash(26, 0),
    "m01s36i076": Stash(26, 0),
    "m01s36i077": Stash(26, 0),
    "m01s36i078": Stash(26, 0),
    "m01s36i079": Stash(26, 0),
    "m01s36i080": Stash(26, 0),
    "m01s36i081": Stash(26, 0),
    "m01s36i082": Stash(26, 0),
    "m01s36i083": Stash(26, 0),
    "m01s36i084": Stash(26, 0),
    "m01s36i085": Stash(26, 0),
    "m01s36i086": Stash(26, 0),
    "m01s36i087": Stash(26, 0),
    "m01s36i088": Stash(26, 0),
    "m01s36i089": Stash(26, 0),
    "m01s36i090": Stash(26, 0),
    "m01s36i091": Stash(26, 0),
    "m01s36i092": Stash(26, 0),
    "m01s36i093": Stash(26, 0),
    "m01s36i094": Stash(26, 0),
    "m01s36i095": Stash(26, 0),
    "m01s36i096": Stash(26, 0),
    "m01s36i097": Stash(26, 0),
    "m01s36i098": Stash(26, 0),
    "m01s36i099": Stash(26, 0),
    "m01s36i100": Stash(26, 0),
    "m01s36i101": Stash(26, 0),
    "m01s36i102": Stash(26, 0),
    "m01s36i103": Stash(26, 0),
    "m01s36i104": Stash(26, 0),
    "m01s36i105": Stash(26, 0),
    "m01s36i106": Stash(26, 0),
    "m01s36i107": Stash(26, 0),
    "m01s36i108": Stash(26, 0),
    "m01s36i109": Stash(26, 0),
    "m01s36i110": Stash(26, 0),
    "m01s36i111": Stash(26, 0),
    "m01s36i112": Stash(26, 0),
    "m01s36i113": Stash(26, 0),
    "m01s36i114": Stash(26, 0),
    "m01s36i115": Stash(26, 0),
    "m01s36i116": Stash(26, 0),
    "m01s36i117": Stash(26, 0),
    "m01s36i118": Stash(26, 0),
    "m01s36i119": Stash(26, 0),
    "m01s36i120": Stash(26, 0),
    "m01s36i121": Stash(26, 0),
    "m01s36i122": Stash(26, 0),
    "m01s36i123": Stash(26, 0),
    "m01s36i124": Stash(26, 0),
    "m01s36i125": Stash(26, 0),
    "m01s36i126": Stash(26, 0),
    "m01s36i127": Stash(26, 0),
    "m01s36i128": Stash(26, 0),
    "m01s36i129": Stash(26, 0),
    "m01s36i130": Stash(26, 0),
    "m01s36i131": Stash(26, 0),
    "m01s36i132": Stash(26, 0),
    "m01s36i133": Stash(26, 0),
    "m01s36i134": Stash(26, 0),
    "m01s36i135": Stash(26, 0),
    "m01s36i136": Stash(26, 0),
    "m01s36i137": Stash(26, 0),
    "m01s36i138": Stash(26, 0),
    "m01s36i139": Stash(26, 0),
    "m01s36i140": Stash(26, 0),
    "m01s36i141": Stash(26, 0),
    "m01s36i142": Stash(26, 0),
    "m01s36i143": Stash(26, 0),
    "m01s36i144": Stash(26, 0),
    "m01s36i145": Stash(26, 0),
    "m01s36i146": Stash(26, 0),
    "m01s36i147": Stash(26, 0),
    "m01s36i148": Stash(26, 0),
    "m01s36i149": Stash(26, 0),
    "m01s36i150": Stash(26, 0),
    "m01s36i257": Stash(26, 0),
    "m01s36i258": Stash(26, 0),
    "m01s36i259": Stash(26, 0),
    "m01s36i260": Stash(26, 0),
    "m01s36i261": Stash(26, 0),
    "m01s36i262": Stash(26, 0),
    "m01s36i263": Stash(26, 0),
    "m01s36i264": Stash(26, 0),
    "m01s36i265": Stash(26, 0),
    "m01s36i266": Stash(26, 0),
    "m01s36i267": Stash(26, 0),
    "m01s36i268": Stash(26, 0),
    "m01s36i269": Stash(26, 0),
    "m01s36i270": Stash(26, 0),
    "m01s36i271": Stash(26, 0),
    "m01s36i272": Stash(26, 0),
    "m01s36i273": Stash(26, 0),
    "m01s36i274": Stash(26, 0),
    "m01s36i275": Stash(26, 0),
    "m01s36i276": Stash(26, 0),
    "m01s36i277": Stash(26, 0),
    "m01s36i278": Stash(26, 0),
    "m01s36i279": Stash(26, 0),
    "m01s36i280": Stash(26, 0),
    "m01s36i281": Stash(26, 0),
    "m01s36i282": Stash(26, 0),
    "m01s36i283": Stash(26, 0),
    "m01s36i284": Stash(26, 0),
    "m01s36i285": Stash(26, 0),
    "m01s36i286": Stash(26, 0),
    "m01s36i287": Stash(26, 0),
    "m01s36i288": Stash(26, 0),
    "m01s36i289": Stash(26, 0),
    "m01s36i290": Stash(26, 0),
    "m01s36i291": Stash(26, 0),
    "m01s36i292": Stash(26, 0),
    "m01s36i293": Stash(26, 0),
    "m01s36i294": Stash(26, 0),
    "m01s36i295": Stash(26, 0),
    "m01s36i296": Stash(26, 0),
    "m01s36i297": Stash(26, 0),
    "m01s36i298": Stash(26, 0),
    "m01s36i299": Stash(26, 0),
    "m01s36i300": Stash(26, 0),
    "m01s36i301": Stash(26, 0),
    "m01s36i302": Stash(26, 0),
    "m01s36i303": Stash(26, 0),
    "m01s36i304": Stash(26, 0),
    "m01s36i305": Stash(26, 0),
    "m01s36i306": Stash(26, 0),
    "m01s36i307": Stash(26, 0),
    "m01s36i308": Stash(26, 0),
    "m01s36i309": Stash(26, 0),
    "m01s36i310": Stash(26, 0),
    "m01s36i311": Stash(26, 0),
    "m01s36i312": Stash(26, 0),
    "m01s36i313": Stash(26, 0),
    "m01s36i314": Stash(26, 0),
    "m01s36i315": Stash(26, 0),
    "m01s36i316": Stash(26, 0),
    "m01s36i317": Stash(26, 0),
    "m01s36i318": Stash(26, 0),
    "m01s36i319": Stash(26, 0),
    "m01s36i320": Stash(26, 0),
    "m01s36i321": Stash(26, 0),
    "m01s36i322": Stash(26, 0),
    "m01s36i323": Stash(26, 0),
    "m01s36i324": Stash(26, 0),
    "m01s36i325": Stash(26, 0),
    "m01s36i326": Stash(26, 0),
    "m01s36i327": Stash(26, 0),
    "m01s36i328": Stash(26, 0),
    "m01s36i329": Stash(26, 0),
    "m01s36i330": Stash(26, 0),
    "m01s36i331": Stash(26, 0),
    "m01s36i332": Stash(26, 0),
    "m01s36i333": Stash(26, 0),
    "m01s36i334": Stash(26, 0),
    "m01s36i335": Stash(26, 0),
    "m01s36i336": Stash(26, 0),
    "m01s36i337": Stash(26, 0),
    "m01s36i338": Stash(26, 0),
    "m01s36i339": Stash(26, 0),
    "m01s36i340": Stash(26, 0),
    "m01s36i341": Stash(26, 0),
    "m01s36i342": Stash(26, 0),
    "m01s36i343": Stash(26, 0),
    "m01s36i344": Stash(26, 0),
    "m01s36i345": Stash(26, 0),
    "m01s36i346": Stash(26, 0),
    "m01s36i347": Stash(26, 0),
    "m01s36i348": Stash(26, 0),
    "m01s36i349": Stash(26, 0),
    "m01s36i350": Stash(26, 0),
    "m01s36i351": Stash(26, 0),
    "m01s36i352": Stash(26, 0),
    "m01s36i353": Stash(26, 0),
    "m01s36i354": Stash(26, 0),
    "m01s36i355": Stash(26, 0),
    "m01s36i356": Stash(26, 0),
    "m01s36i357": Stash(26, 0),
    "m01s36i358": Stash(26, 0),
    "m01s36i359": Stash(26, 0),
    "m01s36i360": Stash(26, 0),
    "m01s36i361": Stash(26, 0),
    "m01s36i362": Stash(26, 0),
    "m01s36i363": Stash(26, 0),
    "m01s36i364": Stash(26, 0),
    "m01s36i365": Stash(26, 0),
    "m01s36i366": Stash(26, 0),
    "m01s36i367": Stash(26, 0),
    "m01s36i368": Stash(26, 0),
    "m01s36i369": Stash(26, 0),
    "m01s36i370": Stash(26, 0),
    "m01s36i371": Stash(26, 0),
    "m01s36i372": Stash(26, 0),
    "m01s36i373": Stash(26, 0),
    "m01s36i374": Stash(26, 0),
    "m01s36i375": Stash(26, 0),
    "m01s36i376": Stash(26, 0),
    "m01s36i377": Stash(26, 0),
    "m01s36i378": Stash(26, 0),
    "m01s36i379": Stash(26, 0),
    "m01s36i380": Stash(26, 0),
    "m01s36i381": Stash(26, 0),
    "m01s36i382": Stash(26, 0),
    "m01s36i383": Stash(26, 0),
    "m01s36i384": Stash(26, 0),
    "m01s36i385": Stash(26, 0),
    "m01s36i386": Stash(26, 0),
    "m01s36i387": Stash(26, 0),
    "m01s36i388": Stash(26, 0),
    "m01s36i389": Stash(26, 0),
    "m01s36i390": Stash(26, 0),
    "m01s36i391": Stash(26, 0),
    "m01s36i392": Stash(26, 0),
    "m01s36i393": Stash(26, 0),
    "m01s36i394": Stash(26, 0),
    "m01s36i395": Stash(26, 0),
    "m01s36i396": Stash(26, 0),
    "m01s36i397": Stash(26, 0),
    "m01s36i398": Stash(26, 0),
    "m01s36i399": Stash(26, 0),
    "m01s36i400": Stash(26, 0),
    "m01s36i401": Stash(26, 0),
    "m01s36i402": Stash(26, 0),
    "m01s36i403": Stash(26, 0),
    "m01s36i404": Stash(26, 0),
    "m01s36i405": Stash(26, 0),
    "m01s36i406": Stash(26, 0),
    "m01s37i001": Stash(26, 0),
    "m01s37i002": Stash(26, 0),
    "m01s37i003": Stash(26, 0),
    "m01s37i004": Stash(26, 0),
    "m01s37i005": Stash(26, 0),
    "m01s37i006": Stash(26, 0),
    "m01s37i007": Stash(26, 0),
    "m01s37i008": Stash(26, 0),
    "m01s37i009": Stash(26, 0),
    "m01s37i010": Stash(26, 0),
    "m01s37i011": Stash(26, 0),
    "m01s37i012": Stash(26, 0),
    "m01s37i013": Stash(26, 0),
    "m01s37i014": Stash(26, 0),
    "m01s37i015": Stash(26, 0),
    "m01s37i016": Stash(26, 0),
    "m01s37i017": Stash(26, 0),
    "m01s37i018": Stash(26, 0),
    "m01s37i019": Stash(26, 0),
    "m01s37i020": Stash(26, 0),
    "m01s37i021": Stash(26, 0),
    "m01s37i022": Stash(26, 0),
    "m01s37i023": Stash(26, 0),
    "m01s37i024": Stash(26, 0),
    "m01s37i025": Stash(26, 0),
    "m01s37i026": Stash(26, 0),
    "m01s37i027": Stash(26, 0),
    "m01s37i028": Stash(26, 0),
    "m01s37i029": Stash(26, 0),
    "m01s37i030": Stash(26, 0),
    "m01s37i031": Stash(26, 0),
    "m01s37i032": Stash(26, 0),
    "m01s37i033": Stash(26, 0),
    "m01s37i034": Stash(26, 0),
    "m01s37i035": Stash(26, 0),
    "m01s37i036": Stash(26, 0),
    "m01s37i037": Stash(26, 0),
    "m01s37i038": Stash(26, 0),
    "m01s37i039": Stash(26, 0),
    "m01s37i040": Stash(26, 0),
    "m01s37i041": Stash(26, 0),
    "m01s37i042": Stash(26, 0),
    "m01s37i043": Stash(26, 0),
    "m01s37i044": Stash(26, 0),
    "m01s37i045": Stash(26, 0),
    "m01s37i046": Stash(26, 0),
    "m01s37i047": Stash(26, 0),
    "m01s37i048": Stash(26, 0),
    "m01s37i049": Stash(26, 0),
    "m01s37i050": Stash(26, 0),
    "m01s37i051": Stash(26, 0),
    "m01s37i052": Stash(26, 0),
    "m01s37i053": Stash(26, 0),
    "m01s37i054": Stash(26, 0),
    "m01s37i055": Stash(26, 0),
    "m01s37i056": Stash(26, 0),
    "m01s37i057": Stash(26, 0),
    "m01s37i058": Stash(26, 0),
    "m01s37i059": Stash(26, 0),
    "m01s37i060": Stash(26, 0),
    "m01s37i061": Stash(26, 0),
    "m01s37i062": Stash(26, 0),
    "m01s37i063": Stash(26, 0),
    "m01s37i064": Stash(26, 0),
    "m01s37i065": Stash(26, 0),
    "m01s37i066": Stash(26, 0),
    "m01s37i067": Stash(26, 0),
    "m01s37i068": Stash(26, 0),
    "m01s37i069": Stash(26, 0),
    "m01s37i070": Stash(26, 0),
    "m01s37i071": Stash(26, 0),
    "m01s37i072": Stash(26, 0),
    "m01s37i073": Stash(26, 0),
    "m01s37i074": Stash(26, 0),
    "m01s37i075": Stash(26, 0),
    "m01s37i076": Stash(26, 0),
    "m01s37i077": Stash(26, 0),
    "m01s37i078": Stash(26, 0),
    "m01s37i079": Stash(26, 0),
    "m01s37i080": Stash(26, 0),
    "m01s37i081": Stash(26, 0),
    "m01s37i082": Stash(26, 0),
    "m01s37i083": Stash(26, 0),
    "m01s37i084": Stash(26, 0),
    "m01s37i085": Stash(26, 0),
    "m01s37i086": Stash(26, 0),
    "m01s37i087": Stash(26, 0),
    "m01s37i088": Stash(26, 0),
    "m01s37i089": Stash(26, 0),
    "m01s37i090": Stash(26, 0),
    "m01s37i091": Stash(26, 0),
    "m01s37i092": Stash(26, 0),
    "m01s37i093": Stash(26, 0),
    "m01s37i094": Stash(26, 0),
    "m01s37i095": Stash(26, 0),
    "m01s37i096": Stash(26, 0),
    "m01s37i097": Stash(26, 0),
    "m01s37i098": Stash(26, 0),
    "m01s37i099": Stash(26, 0),
    "m01s37i100": Stash(26, 0),
    "m01s37i101": Stash(26, 0),
    "m01s37i102": Stash(26, 0),
    "m01s37i103": Stash(26, 0),
    "m01s37i104": Stash(26, 0),
    "m01s37i105": Stash(26, 0),
    "m01s37i106": Stash(26, 0),
    "m01s37i107": Stash(26, 0),
    "m01s37i108": Stash(26, 0),
    "m01s37i109": Stash(26, 0),
    "m01s37i110": Stash(26, 0),
    "m01s37i111": Stash(26, 0),
    "m01s37i112": Stash(26, 0),
    "m01s37i113": Stash(26, 0),
    "m01s37i114": Stash(26, 0),
    "m01s37i115": Stash(26, 0),
    "m01s37i116": Stash(26, 0),
    "m01s37i117": Stash(26, 0),
    "m01s37i118": Stash(26, 0),
    "m01s37i119": Stash(26, 0),
    "m01s37i120": Stash(26, 0),
    "m01s37i121": Stash(26, 0),
    "m01s37i122": Stash(26, 0),
    "m01s37i123": Stash(26, 0),
    "m01s37i124": Stash(26, 0),
    "m01s37i125": Stash(26, 0),
    "m01s37i126": Stash(26, 0),
    "m01s37i127": Stash(26, 0),
    "m01s37i128": Stash(26, 0),
    "m01s37i129": Stash(26, 0),
    "m01s37i130": Stash(26, 0),
    "m01s37i131": Stash(26, 0),
    "m01s37i132": Stash(26, 0),
    "m01s37i133": Stash(26, 0),
    "m01s37i134": Stash(26, 0),
    "m01s37i135": Stash(26, 0),
    "m01s37i136": Stash(26, 0),
    "m01s37i137": Stash(26, 0),
    "m01s37i138": Stash(26, 0),
    "m01s37i139": Stash(26, 0),
    "m01s37i140": Stash(26, 0),
    "m01s37i141": Stash(26, 0),
    "m01s37i142": Stash(26, 0),
    "m01s37i143": Stash(26, 0),
    "m01s37i144": Stash(26, 0),
    "m01s37i145": Stash(26, 0),
    "m01s37i146": Stash(26, 0),
    "m01s37i147": Stash(26, 0),
    "m01s37i148": Stash(26, 0),
    "m01s37i149": Stash(26, 0),
    "m01s37i150": Stash(26, 0),
    "m01s37i257": Stash(26, 0),
    "m01s37i258": Stash(26, 0),
    "m01s37i259": Stash(26, 0),
    "m01s37i260": Stash(26, 0),
    "m01s37i261": Stash(26, 0),
    "m01s37i262": Stash(26, 0),
    "m01s37i263": Stash(26, 0),
    "m01s37i264": Stash(26, 0),
    "m01s37i265": Stash(26, 0),
    "m01s37i266": Stash(26, 0),
    "m01s37i267": Stash(26, 0),
    "m01s37i268": Stash(26, 0),
    "m01s37i269": Stash(26, 0),
    "m01s37i270": Stash(26, 0),
    "m01s37i271": Stash(26, 0),
    "m01s37i272": Stash(26, 0),
    "m01s37i273": Stash(26, 0),
    "m01s37i274": Stash(26, 0),
    "m01s37i275": Stash(26, 0),
    "m01s37i276": Stash(26, 0),
    "m01s37i277": Stash(26, 0),
    "m01s37i278": Stash(26, 0),
    "m01s37i279": Stash(26, 0),
    "m01s37i280": Stash(26, 0),
    "m01s37i281": Stash(26, 0),
    "m01s37i282": Stash(26, 0),
    "m01s37i283": Stash(26, 0),
    "m01s37i284": Stash(26, 0),
    "m01s37i285": Stash(26, 0),
    "m01s37i286": Stash(26, 0),
    "m01s37i287": Stash(26, 0),
    "m01s37i288": Stash(26, 0),
    "m01s37i289": Stash(26, 0),
    "m01s37i290": Stash(26, 0),
    "m01s37i291": Stash(26, 0),
    "m01s37i292": Stash(26, 0),
    "m01s37i293": Stash(26, 0),
    "m01s37i294": Stash(26, 0),
    "m01s37i295": Stash(26, 0),
    "m01s37i296": Stash(26, 0),
    "m01s37i297": Stash(26, 0),
    "m01s37i298": Stash(26, 0),
    "m01s37i299": Stash(26, 0),
    "m01s37i300": Stash(26, 0),
    "m01s37i301": Stash(26, 0),
    "m01s37i302": Stash(26, 0),
    "m01s37i303": Stash(26, 0),
    "m01s37i304": Stash(26, 0),
    "m01s37i305": Stash(26, 0),
    "m01s37i306": Stash(26, 0),
    "m01s37i307": Stash(26, 0),
    "m01s37i308": Stash(26, 0),
    "m01s37i309": Stash(26, 0),
    "m01s37i310": Stash(26, 0),
    "m01s37i311": Stash(26, 0),
    "m01s37i312": Stash(26, 0),
    "m01s37i313": Stash(26, 0),
    "m01s37i314": Stash(26, 0),
    "m01s37i315": Stash(26, 0),
    "m01s37i316": Stash(26, 0),
    "m01s37i317": Stash(26, 0),
    "m01s37i318": Stash(26, 0),
    "m01s37i319": Stash(26, 0),
    "m01s37i320": Stash(26, 0),
    "m01s37i321": Stash(26, 0),
    "m01s37i322": Stash(26, 0),
    "m01s37i323": Stash(26, 0),
    "m01s37i324": Stash(26, 0),
    "m01s37i325": Stash(26, 0),
    "m01s37i326": Stash(26, 0),
    "m01s37i327": Stash(26, 0),
    "m01s37i328": Stash(26, 0),
    "m01s37i329": Stash(26, 0),
    "m01s37i330": Stash(26, 0),
    "m01s37i331": Stash(26, 0),
    "m01s37i332": Stash(26, 0),
    "m01s37i333": Stash(26, 0),
    "m01s37i334": Stash(26, 0),
    "m01s37i335": Stash(26, 0),
    "m01s37i336": Stash(26, 0),
    "m01s37i337": Stash(26, 0),
    "m01s37i338": Stash(26, 0),
    "m01s37i339": Stash(26, 0),
    "m01s37i340": Stash(26, 0),
    "m01s37i341": Stash(26, 0),
    "m01s37i342": Stash(26, 0),
    "m01s37i343": Stash(26, 0),
    "m01s37i344": Stash(26, 0),
    "m01s37i345": Stash(26, 0),
    "m01s37i346": Stash(26, 0),
    "m01s37i347": Stash(26, 0),
    "m01s37i348": Stash(26, 0),
    "m01s37i349": Stash(26, 0),
    "m01s37i350": Stash(26, 0),
    "m01s37i351": Stash(26, 0),
    "m01s37i352": Stash(26, 0),
    "m01s37i353": Stash(26, 0),
    "m01s37i354": Stash(26, 0),
    "m01s37i355": Stash(26, 0),
    "m01s37i356": Stash(26, 0),
    "m01s37i357": Stash(26, 0),
    "m01s37i358": Stash(26, 0),
    "m01s37i359": Stash(26, 0),
    "m01s37i360": Stash(26, 0),
    "m01s37i361": Stash(26, 0),
    "m01s37i362": Stash(26, 0),
    "m01s37i363": Stash(26, 0),
    "m01s37i364": Stash(26, 0),
    "m01s37i365": Stash(26, 0),
    "m01s37i366": Stash(26, 0),
    "m01s37i367": Stash(26, 0),
    "m01s37i368": Stash(26, 0),
    "m01s37i369": Stash(26, 0),
    "m01s37i370": Stash(26, 0),
    "m01s37i371": Stash(26, 0),
    "m01s37i372": Stash(26, 0),
    "m01s37i373": Stash(26, 0),
    "m01s37i374": Stash(26, 0),
    "m01s37i375": Stash(26, 0),
    "m01s37i376": Stash(26, 0),
    "m01s37i377": Stash(26, 0),
    "m01s37i378": Stash(26, 0),
    "m01s37i379": Stash(26, 0),
    "m01s37i380": Stash(26, 0),
    "m01s37i381": Stash(26, 0),
    "m01s37i382": Stash(26, 0),
    "m01s37i383": Stash(26, 0),
    "m01s37i384": Stash(26, 0),
    "m01s37i385": Stash(26, 0),
    "m01s37i386": Stash(26, 0),
    "m01s37i387": Stash(26, 0),
    "m01s37i388": Stash(26, 0),
    "m01s37i389": Stash(26, 0),
    "m01s37i390": Stash(26, 0),
    "m01s37i391": Stash(26, 0),
    "m01s37i392": Stash(26, 0),
    "m01s37i393": Stash(26, 0),
    "m01s37i394": Stash(26, 0),
    "m01s37i395": Stash(26, 0),
    "m01s37i396": Stash(26, 0),
    "m01s37i397": Stash(26, 0),
    "m01s37i398": Stash(26, 0),
    "m01s37i399": Stash(26, 0),
    "m01s37i400": Stash(26, 0),
    "m01s37i401": Stash(26, 0),
    "m01s37i402": Stash(26, 0),
    "m01s37i403": Stash(26, 0),
    "m01s37i404": Stash(26, 0),
    "m01s37i405": Stash(26, 0),
    "m01s37i406": Stash(26, 0),
    "m01s38i001": Stash(1, 1875),
    "m01s38i002": Stash(1, 1875),
    "m01s38i003": Stash(1, 1875),
    "m01s38i004": Stash(1, 1875),
    "m01s38i005": Stash(1, 1875),
    "m01s38i006": Stash(1, 1875),
    "m01s38i007": Stash(1, 1875),
    "m01s38i008": Stash(1, 1875),
    "m01s38i009": Stash(1, 1875),
    "m01s38i010": Stash(1, 1875),
    "m01s38i011": Stash(1, 1875),
    "m01s38i012": Stash(1, 1875),
    "m01s38i013": Stash(1, 1875),
    "m01s38i014": Stash(1, 1875),
    "m01s38i015": Stash(1, 1875),
    "m01s38i016": Stash(1, 1875),
    "m01s38i017": Stash(1, 1875),
    "m01s38i018": Stash(1, 1875),
    "m01s38i019": Stash(1, 1875),
    "m01s38i020": Stash(1, 1875),
    "m01s38i021": Stash(1, 1875),
    "m01s38i022": Stash(1, 1875),
    "m01s38i023": Stash(1, 1875),
    "m01s38i024": Stash(1, 1875),
    "m01s38i025": Stash(1, 1875),
    "m01s38i026": Stash(1, 1875),
    "m01s38i027": Stash(1, 1875),
    "m01s38i028": Stash(1, 1875),
    "m01s38i029": Stash(1, 1875),
    "m01s38i030": Stash(1, 1875),
    "m01s38i031": Stash(1, 1875),
    "m01s38i032": Stash(1, 1875),
    "m01s38i033": Stash(1, 1875),
    "m01s38i034": Stash(1, 1875),
    "m01s38i035": Stash(1, 1875),
    "m01s38i036": Stash(1, 1875),
    "m01s38i037": Stash(1, 1875),
    "m01s38i038": Stash(1, 1875),
    "m01s38i039": Stash(1, 1875),
    "m01s38i040": Stash(1, 1875),
    "m01s38i041": Stash(1, 1875),
    "m01s38i042": Stash(1, 1875),
    "m01s38i043": Stash(1, 1875),
    "m01s38i044": Stash(1, 1875),
    "m01s38i045": Stash(1, 1875),
    "m01s38i046": Stash(1, 1875),
    "m01s38i047": Stash(1, 1875),
    "m01s38i048": Stash(1, 1875),
    "m01s38i049": Stash(1, 1875),
    "m01s38i050": Stash(1, 1875),
    "m01s38i051": Stash(1, 1875),
    "m01s38i052": Stash(1, 1875),
    "m01s38i053": Stash(1, 1875),
    "m01s38i054": Stash(1, 1875),
    "m01s38i055": Stash(1, 1875),
    "m01s38i056": Stash(1, 1875),
    "m01s38i057": Stash(1, 1875),
    "m01s38i058": Stash(1, 1875),
    "m01s38i059": Stash(1, 1875),
    "m01s38i060": Stash(1, 1875),
    "m01s38i061": Stash(1, 1875),
    "m01s38i062": Stash(1, 1875),
    "m01s38i063": Stash(1, 1875),
    "m01s38i064": Stash(1, 1875),
    "m01s38i065": Stash(1, 1875),
    "m01s38i066": Stash(1, 1875),
    "m01s38i067": Stash(1, 1875),
    "m01s38i068": Stash(1, 1875),
    "m01s38i069": Stash(1, 1875),
    "m01s38i070": Stash(1, 1875),
    "m01s38i071": Stash(1, 1875),
    "m01s38i072": Stash(1, 1875),
    "m01s38i073": Stash(1, 1875),
    "m01s38i074": Stash(1, 1875),
    "m01s38i075": Stash(1, 1875),
    "m01s38i076": Stash(1, 1875),
    "m01s38i077": Stash(1, 1875),
    "m01s38i078": Stash(1, 1875),
    "m01s38i079": Stash(1, 1875),
    "m01s38i080": Stash(1, 1875),
    "m01s38i081": Stash(1, 1875),
    "m01s38i082": Stash(1, 1875),
    "m01s38i083": Stash(1, 1875),
    "m01s38i084": Stash(1, 1875),
    "m01s38i085": Stash(1, 1875),
    "m01s38i086": Stash(1, 1875),
    "m01s38i087": Stash(1, 1875),
    "m01s38i088": Stash(1, 1875),
    "m01s38i089": Stash(1, 1875),
    "m01s38i090": Stash(1, 1875),
    "m01s38i091": Stash(1, 1875),
    "m01s38i092": Stash(1, 1875),
    "m01s38i093": Stash(1, 1875),
    "m01s38i094": Stash(1, 1875),
    "m01s38i095": Stash(1, 1875),
    "m01s38i096": Stash(1, 1875),
    "m01s38i097": Stash(1, 1875),
    "m01s38i098": Stash(1, 1875),
    "m01s38i099": Stash(1, 1875),
    "m01s38i100": Stash(1, 1875),
    "m01s38i101": Stash(1, 1875),
    "m01s38i102": Stash(1, 1875),
    "m01s38i103": Stash(1, 1875),
    "m01s38i104": Stash(1, 1875),
    "m01s38i105": Stash(1, 1875),
    "m01s38i106": Stash(1, 1875),
    "m01s38i107": Stash(1, 1875),
    "m01s38i108": Stash(1, 1875),
    "m01s38i109": Stash(1, 1875),
    "m01s38i110": Stash(1, 1875),
    "m01s38i111": Stash(1, 1875),
    "m01s38i112": Stash(1, 1875),
    "m01s38i113": Stash(1, 1875),
    "m01s38i114": Stash(1, 1875),
    "m01s38i115": Stash(1, 1875),
    "m01s38i116": Stash(1, 1875),
    "m01s38i117": Stash(1, 1875),
    "m01s38i118": Stash(1, 1875),
    "m01s38i119": Stash(1, 1875),
    "m01s38i120": Stash(1, 1875),
    "m01s38i121": Stash(1, 1875),
    "m01s38i122": Stash(1, 1875),
    "m01s38i123": Stash(1, 1875),
    "m01s38i124": Stash(1, 1875),
    "m01s38i125": Stash(1, 1875),
    "m01s38i126": Stash(1, 1875),
    "m01s38i127": Stash(1, 1875),
    "m01s38i128": Stash(1, 1875),
    "m01s38i129": Stash(1, 1875),
    "m01s38i130": Stash(1, 1875),
    "m01s38i131": Stash(1, 1875),
    "m01s38i132": Stash(1, 1875),
    "m01s38i133": Stash(1, 1875),
    "m01s38i134": Stash(1, 1875),
    "m01s38i135": Stash(1, 1875),
    "m01s38i136": Stash(1, 1875),
    "m01s38i137": Stash(1, 1875),
    "m01s38i138": Stash(1, 1875),
    "m01s38i139": Stash(1, 1875),
    "m01s38i140": Stash(1, 1875),
    "m01s38i141": Stash(1, 1875),
    "m01s38i142": Stash(1, 1875),
    "m01s38i143": Stash(1, 1875),
    "m01s38i144": Stash(1, 1875),
    "m01s38i145": Stash(1, 1875),
    "m01s38i146": Stash(1, 1875),
    "m01s38i147": Stash(1, 1875),
    "m01s38i148": Stash(1, 1875),
    "m01s38i149": Stash(1, 1875),
    "m01s38i150": Stash(1, 1875),
    "m01s38i151": Stash(1, 0),
    "m01s38i152": Stash(1, 0),
    "m01s38i153": Stash(1, 0),
    "m01s38i154": Stash(1, 0),
    "m01s38i155": Stash(1, 0),
    "m01s38i156": Stash(1, 0),
    "m01s38i157": Stash(1, 0),
    "m01s38i158": Stash(1, 0),
    "m01s38i159": Stash(1, 0),
    "m01s38i160": Stash(1, 0),
    "m01s38i161": Stash(1, 0),
    "m01s38i162": Stash(1, 571),
    "m01s38i163": Stash(1, 571),
    "m01s38i164": Stash(1, 571),
    "m01s38i165": Stash(1, 571),
    "m01s38i166": Stash(1, 575),
    "m01s38i167": Stash(1, 575),
    "m01s38i168": Stash(1, 575),
    "m01s38i169": Stash(1, 0),
    "m01s38i170": Stash(1, 575),
    "m01s38i171": Stash(1, 575),
    "m01s38i172": Stash(1, 575),
    "m01s38i173": Stash(1, 575),
    "m01s38i174": Stash(1, 575),
    "m01s38i186": Stash(1, 575),
    "m01s38i187": Stash(1, 575),
    "m01s38i188": Stash(1, 575),
    "m01s38i189": Stash(1, 575),
    "m01s38i201": Stash(1, 1870),
    "m01s38i202": Stash(1, 1870),
    "m01s38i203": Stash(1, 1870),
    "m01s38i204": Stash(1, 1870),
    "m01s38i205": Stash(1, 1870),
    "m01s38i206": Stash(1, 1870),
    "m01s38i207": Stash(1, 1870),
    "m01s38i208": Stash(1, 1870),
    "m01s38i209": Stash(1, 1870),
    "m01s38i210": Stash(1, 1870),
    "m01s38i211": Stash(1, 1870),
    "m01s38i212": Stash(1, 1870),
    "m01s38i213": Stash(1, 1870),
    "m01s38i214": Stash(1, 1870),
    "m01s38i215": Stash(1, 1870),
    "m01s38i216": Stash(1, 1870),
    "m01s38i217": Stash(1, 1870),
    "m01s38i218": Stash(1, 1870),
    "m01s38i219": Stash(1, 1870),
    "m01s38i220": Stash(1, 1870),
    "m01s38i221": Stash(1, 1870),
    "m01s38i222": Stash(1, 1870),
    "m01s38i223": Stash(1, 1870),
    "m01s38i224": Stash(1, 1870),
    "m01s38i225": Stash(1, 1870),
    "m01s38i226": Stash(1, 1870),
    "m01s38i227": Stash(1, 1870),
    "m01s38i228": Stash(1, 1870),
    "m01s38i229": Stash(1, 1870),
    "m01s38i230": Stash(1, 1870),
    "m01s38i231": Stash(1, 1870),
    "m01s38i232": Stash(1, 1870),
    "m01s38i233": Stash(1, 1870),
    "m01s38i234": Stash(1, 1870),
    "m01s38i235": Stash(1, 1870),
    "m01s38i236": Stash(1, 1870),
    "m01s38i237": Stash(1, 1870),
    "m01s38i238": Stash(1, 1870),
    "m01s38i239": Stash(1, 1870),
    "m01s38i240": Stash(1, 1870),
    "m01s38i241": Stash(1, 1870),
    "m01s38i242": Stash(1, 1870),
    "m01s38i243": Stash(1, 1870),
    "m01s38i244": Stash(1, 1870),
    "m01s38i245": Stash(1, 1870),
    "m01s38i246": Stash(1, 1870),
    "m01s38i247": Stash(1, 1870),
    "m01s38i248": Stash(1, 1870),
    "m01s38i249": Stash(1, 1870),
    "m01s38i250": Stash(1, 1870),
    "m01s38i251": Stash(1, 1870),
    "m01s38i252": Stash(1, 1870),
    "m01s38i253": Stash(1, 1870),
    "m01s38i254": Stash(1, 1870),
    "m01s38i255": Stash(1, 1870),
    "m01s38i256": Stash(1, 1870),
    "m01s38i257": Stash(1, 1870),
    "m01s38i258": Stash(1, 1870),
    "m01s38i259": Stash(1, 1870),
    "m01s38i260": Stash(1, 1870),
    "m01s38i261": Stash(1, 1870),
    "m01s38i262": Stash(1, 1870),
    "m01s38i263": Stash(1, 1870),
    "m01s38i264": Stash(1, 1870),
    "m01s38i265": Stash(1, 1870),
    "m01s38i266": Stash(1, 1870),
    "m01s38i267": Stash(1, 1870),
    "m01s38i268": Stash(1, 1870),
    "m01s38i269": Stash(1, 1870),
    "m01s38i270": Stash(1, 1870),
    "m01s38i271": Stash(1, 1870),
    "m01s38i272": Stash(1, 1870),
    "m01s38i273": Stash(1, 1870),
    "m01s38i274": Stash(1, 1870),
    "m01s38i275": Stash(1, 1870),
    "m01s38i276": Stash(1, 1870),
    "m01s38i277": Stash(1, 1870),
    "m01s38i278": Stash(1, 1870),
    "m01s38i279": Stash(1, 1870),
    "m01s38i280": Stash(1, 1870),
    "m01s38i281": Stash(1, 1870),
    "m01s38i282": Stash(1, 1870),
    "m01s38i283": Stash(1, 1870),
    "m01s38i284": Stash(1, 1870),
    "m01s38i285": Stash(1, 1870),
    "m01s38i286": Stash(1, 1870),
    "m01s38i287": Stash(1, 1870),
    "m01s38i288": Stash(1, 1870),
    "m01s38i289": Stash(1, 1870),
    "m01s38i290": Stash(1, 1870),
    "m01s38i291": Stash(1, 1870),
    "m01s38i292": Stash(1, 1870),
    "m01s38i293": Stash(1, 1870),
    "m01s38i294": Stash(1, 1870),
    "m01s38i295": Stash(1, 1870),
    "m01s38i296": Stash(1, 1870),
    "m01s38i297": Stash(1, 1870),
    "m01s38i298": Stash(1, 1870),
    "m01s38i299": Stash(1, 1870),
    "m01s38i300": Stash(1, 1870),
    "m01s38i301": Stash(1, 1870),
    "m01s38i302": Stash(1, 1870),
    "m01s38i303": Stash(1, 1870),
    "m01s38i304": Stash(1, 1870),
    "m01s38i305": Stash(1, 1870),
    "m01s38i306": Stash(1, 1870),
    "m01s38i307": Stash(1, 1870),
    "m01s38i308": Stash(1, 1870),
    "m01s38i309": Stash(1, 1870),
    "m01s38i310": Stash(1, 1870),
    "m01s38i311": Stash(1, 1870),
    "m01s38i312": Stash(1, 1870),
    "m01s38i313": Stash(1, 1870),
    "m01s38i314": Stash(1, 1870),
    "m01s38i315": Stash(1, 1870),
    "m01s38i316": Stash(1, 1870),
    "m01s38i317": Stash(1, 1870),
    "m01s38i318": Stash(1, 1870),
    "m01s38i319": Stash(1, 1870),
    "m01s38i320": Stash(1, 1870),
    "m01s38i321": Stash(1, 1870),
    "m01s38i322": Stash(1, 1870),
    "m01s38i323": Stash(1, 1870),
    "m01s38i324": Stash(1, 1870),
    "m01s38i325": Stash(1, 1870),
    "m01s38i326": Stash(1, 1870),
    "m01s38i327": Stash(1, 1870),
    "m01s38i328": Stash(1, 1870),
    "m01s38i329": Stash(1, 1870),
    "m01s38i330": Stash(1, 1870),
    "m01s38i331": Stash(1, 1870),
    "m01s38i332": Stash(1, 1870),
    "m01s38i333": Stash(1, 1870),
    "m01s38i334": Stash(1, 1870),
    "m01s38i335": Stash(1, 1870),
    "m01s38i336": Stash(1, 1870),
    "m01s38i337": Stash(1, 1870),
    "m01s38i338": Stash(1, 1870),
    "m01s38i339": Stash(1, 1870),
    "m01s38i340": Stash(1, 1870),
    "m01s38i341": Stash(1, 1870),
    "m01s38i342": Stash(1, 1870),
    "m01s38i343": Stash(1, 1870),
    "m01s38i344": Stash(1, 1870),
    "m01s38i345": Stash(1, 1870),
    "m01s38i346": Stash(1, 1870),
    "m01s38i347": Stash(1, 1870),
    "m01s38i348": Stash(1, 1870),
    "m01s38i349": Stash(1, 1870),
    "m01s38i350": Stash(1, 1870),
    "m01s38i351": Stash(1, 1870),
    "m01s38i352": Stash(1, 1870),
    "m01s38i353": Stash(1, 1870),
    "m01s38i354": Stash(1, 1870),
    "m01s38i355": Stash(1, 1870),
    "m01s38i356": Stash(1, 1870),
    "m01s38i357": Stash(1, 1870),
    "m01s38i358": Stash(1, 1870),
    "m01s38i359": Stash(1, 1870),
    "m01s38i360": Stash(1, 1870),
    "m01s38i361": Stash(1, 1870),
    "m01s38i362": Stash(1, 1870),
    "m01s38i363": Stash(1, 1870),
    "m01s38i364": Stash(1, 1870),
    "m01s38i365": Stash(1, 1870),
    "m01s38i366": Stash(1, 1870),
    "m01s38i367": Stash(1, 1870),
    "m01s38i368": Stash(1, 1870),
    "m01s38i369": Stash(1, 1870),
    "m01s38i370": Stash(1, 1870),
    "m01s38i371": Stash(1, 1870),
    "m01s38i372": Stash(1, 1870),
    "m01s38i373": Stash(1, 1870),
    "m01s38i374": Stash(1, 1870),
    "m01s38i375": Stash(1, 1870),
    "m01s38i376": Stash(1, 1870),
    "m01s38i377": Stash(1, 1870),
    "m01s38i378": Stash(1, 1870),
    "m01s38i379": Stash(1, 1870),
    "m01s38i380": Stash(1, 1870),
    "m01s38i381": Stash(1, 1870),
    "m01s38i382": Stash(1, 1870),
    "m01s38i383": Stash(1, 1870),
    "m01s38i384": Stash(1, 1870),
    "m01s38i385": Stash(1, 1870),
    "m01s38i386": Stash(1, 1870),
    "m01s38i387": Stash(1, 1870),
    "m01s38i401": Stash(1, 0),
    "m01s38i402": Stash(1, 0),
    "m01s38i403": Stash(1, 0),
    "m01s38i404": Stash(1, 0),
    "m01s38i405": Stash(1, 0),
    "m01s38i406": Stash(1, 0),
    "m01s38i407": Stash(1, 0),
    "m01s38i408": Stash(1, 0),
    "m01s38i409": Stash(1, 0),
    "m01s38i410": Stash(1, 0),
    "m01s38i411": Stash(1, 0),
    "m01s38i412": Stash(1, 0),
    "m01s38i413": Stash(1, 0),
    "m01s38i414": Stash(1, 0),
    "m01s38i415": Stash(1, 0),
    "m01s38i416": Stash(1, 1864),
    "m01s38i417": Stash(1, 1864),
    "m01s38i418": Stash(1, 1864),
    "m01s38i419": Stash(1, 1864),
    "m01s38i420": Stash(1, 1864),
    "m01s38i421": Stash(1, 1864),
    "m01s38i422": Stash(1, 1864),
    "m01s38i423": Stash(1, 0),
    "m01s38i424": Stash(1, 0),
    "m01s38i425": Stash(1, 0),
    "m01s38i426": Stash(1, 0),
    "m01s38i427": Stash(1, 0),
    "m01s38i428": Stash(1, 0),
    "m01s38i429": Stash(1, 0),
    "m01s38i430": Stash(1, 1867),
    "m01s38i431": Stash(1, 1867),
    "m01s38i432": Stash(1, 1867),
    "m01s38i433": Stash(1, 1867),
    "m01s38i434": Stash(1, 1867),
    "m01s38i435": Stash(1, 1867),
    "m01s38i436": Stash(1, 1867),
    "m01s38i437": Stash(1, 1862),
    "m01s38i438": Stash(1, 1862),
    "m01s38i439": Stash(1, 1862),
    "m01s38i440": Stash(1, 1862),
    "m01s38i441": Stash(1, 1862),
    "m01s38i442": Stash(1, 0),
    "m01s38i443": Stash(1, 0),
    "m01s38i444": Stash(1, 0),
    "m01s38i445": Stash(1, 0),
    "m01s38i446": Stash(1, 0),
    "m01s38i447": Stash(1, 0),
    "m01s38i448": Stash(1, 0),
    "m01s38i449": Stash(1, 0),
    "m01s38i450": Stash(1, 0),
    "m01s38i451": Stash(1, 0),
    "m01s38i452": Stash(1, 0),
    "m01s38i453": Stash(1, 0),
    "m01s38i454": Stash(1, 0),
    "m01s38i455": Stash(1, 0),
    "m01s38i456": Stash(1, 0),
    "m01s38i457": Stash(1, 0),
    "m01s38i458": Stash(1, 0),
    "m01s38i459": Stash(1, 0),
    "m01s38i460": Stash(1, 0),
    "m01s38i461": Stash(1, 0),
    "m01s38i462": Stash(1, 0),
    "m01s38i463": Stash(1, 0),
    "m01s38i464": Stash(1, 0),
    "m01s38i465": Stash(1, 0),
    "m01s38i466": Stash(1, 0),
    "m01s38i467": Stash(1, 0),
    "m01s38i468": Stash(1, 0),
    "m01s38i469": Stash(1, 1862),
    "m01s38i470": Stash(1, 1862),
    "m01s38i471": Stash(1, 1862),
    "m01s38i472": Stash(1, 1862),
    "m01s38i473": Stash(1, 0),
    "m01s38i474": Stash(1, 0),
    "m01s38i475": Stash(1, 42),
    "m01s38i476": Stash(1, 1425),
    "m01s38i477": Stash(1, 1862),
    "m01s38i478": Stash(1, 0),
    "m01s38i479": Stash(1, 1862),
    "m01s38i480": Stash(1, 0),
    "m01s38i481": Stash(1, 0),
    "m01s38i482": Stash(1, 0),
    "m01s38i483": Stash(1, 0),
    "m01s38i484": Stash(1, 1862),
    "m01s39i001": Stash(1, 16),
    "m01s39i002": Stash(1, 16),
    "m01s39i003": Stash(1, 317),
    "m01s39i004": Stash(1, 317),
    "m01s39i005": Stash(1, 16),
    "m01s39i006": Stash(18, 56),
    "m01s39i007": Stash(18, 56),
    "m01s39i008": Stash(18, 318),
    "m01s39i009": Stash(18, 318),
    "m01s39i010": Stash(18, 56),
    "m01s39i011": Stash(19, 57),
    "m01s39i012": Stash(19, 57),
    "m01s39i013": Stash(19, 319),
    "m01s39i014": Stash(19, 319),
    "m01s39i015": Stash(19, 57),
    "m01s44i001": Stash(1, 1102),
    "m01s44i002": Stash(1, 1103),
    "m01s44i003": Stash(1, 1104),
    "m01s44i004": Stash(1, 1105),
    "m01s44i005": Stash(1, 1106),
    "m01s44i006": Stash(1, 1107),
    "m01s44i007": Stash(1, 1108),
    "m01s44i008": Stash(1, 1109),
    "m01s44i009": Stash(1, 1110),
    "m01s44i010": Stash(1, 1111),
    "m01s44i011": Stash(1, 1136),
    "m01s44i012": Stash(1, 1651),
    "m01s44i013": Stash(1, 1652),
    "m01s50i001": Stash(1, 1871),
    "m01s50i002": Stash(1, 1871),
    "m01s50i003": Stash(1, 1871),
    "m01s50i004": Stash(1, 1871),
    "m01s50i005": Stash(1, 1871),
    "m01s50i006": Stash(1, 1871),
    "m01s50i007": Stash(1, 1871),
    "m01s50i011": Stash(1, 1871),
    "m01s50i012": Stash(1, 1871),
    "m01s50i013": Stash(1, 1871),
    "m01s50i014": Stash(1, 1871),
    "m01s50i015": Stash(1, 1871),
    "m01s50i016": Stash(1, 1871),
    "m01s50i017": Stash(1, 1871),
    "m01s50i021": Stash(1, 1871),
    "m01s50i022": Stash(1, 1871),
    "m01s50i031": Stash(1, 1871),
    "m01s50i041": Stash(1, 1871),
    "m01s50i042": Stash(1, 1871),
    "m01s50i043": Stash(1, 1871),
    "m01s50i044": Stash(1, 1871),
    "m01s50i045": Stash(1, 1871),
    "m01s50i046": Stash(1, 1871),
    "m01s50i051": Stash(1, 1871),
    "m01s50i052": Stash(1, 1871),
    "m01s50i053": Stash(1, 1861),
    "m01s50i054": Stash(1, 1871),
    "m01s50i061": Stash(1, 1902),
    "m01s50i062": Stash(1, 0),
    "m01s50i063": Stash(1, 1902),
    "m01s50i071": Stash(1, 1871),
    "m01s50i072": Stash(1, 1871),
    "m01s50i073": Stash(1, 1871),
    "m01s50i074": Stash(1, 1871),
    "m01s50i075": Stash(1, 1871),
    "m01s50i076": Stash(1, 1871),
    "m01s50i077": Stash(1, 1871),
    "m01s50i078": Stash(1, 1871),
    "m01s50i079": Stash(1, 1871),
    "m01s50i081": Stash(1, 1871),
    "m01s50i082": Stash(1, 0),
    "m01s50i083": Stash(1, 0),
    "m01s50i084": Stash(1, 0),
    "m01s50i085": Stash(1, 1875),
    "m01s50i086": Stash(1, 1876),
    "m01s50i091": Stash(1, 1871),
    "m01s50i092": Stash(1, 1871),
    "m01s50i101": Stash(1, 1871),
    "m01s50i102": Stash(1, 1871),
    "m01s50i103": Stash(1, 1871),
    "m01s50i104": Stash(1, 1871),
    "m01s50i111": Stash(1, 1871),
    "m01s50i112": Stash(1, 1871),
    "m01s50i113": Stash(1, 1871),
    "m01s50i114": Stash(1, 1871),
    "m01s50i115": Stash(1, 1871),
    "m01s50i116": Stash(1, 1871),
    "m01s50i117": Stash(1, 1871),
    "m01s50i118": Stash(1, 1871),
    "m01s50i119": Stash(1, 1871),
    "m01s50i120": Stash(1, 1871),
    "m01s50i121": Stash(1, 1871),
    "m01s50i122": Stash(1, 1871),
    "m01s50i123": Stash(1, 1871),
    "m01s50i124": Stash(1, 1871),
    "m01s50i125": Stash(1, 1871),
    "m01s50i131": Stash(1, 1871),
    "m01s50i132": Stash(1, 1871),
    "m01s50i133": Stash(1, 1871),
    "m01s50i140": Stash(1, 1871),
    "m01s50i141": Stash(1, 1871),
    "m01s50i142": Stash(1, 1871),
    "m01s50i143": Stash(1, 1871),
    "m01s50i144": Stash(1, 1871),
    "m01s50i145": Stash(1, 1871),
    "m01s50i146": Stash(1, 1871),
    "m01s50i147": Stash(1, 1871),
    "m01s50i148": Stash(1, 1871),
    "m01s50i149": Stash(1, 1871),
    "m01s50i150": Stash(1, 1871),
    "m01s50i151": Stash(1, 1871),
    "m01s50i152": Stash(1, 1871),
    "m01s50i153": Stash(1, 1871),
    "m01s50i154": Stash(1, 1871),
    "m01s50i155": Stash(1, 1871),
    "m01s50i156": Stash(1, 1876),
    "m01s50i157": Stash(1, 1876),
    "m01s50i158": Stash(1, 1876),
    "m01s50i159": Stash(1, 1876),
    "m01s50i160": Stash(1, 1876),
    "m01s50i161": Stash(1, 1876),
    "m01s50i162": Stash(1, 1876),
    "m01s50i163": Stash(1, 1876),
    "m01s50i164": Stash(1, 1876),
    "m01s50i165": Stash(1, 1876),
    "m01s50i166": Stash(1, 1876),
    "m01s50i167": Stash(1, 1876),
    "m01s50i168": Stash(1, 1876),
    "m01s50i169": Stash(1, 1876),
    "m01s50i170": Stash(1, 1876),
    "m01s50i171": Stash(1, 1876),
    "m01s50i172": Stash(1, 1878),
    "m02s00i101": Stash(0, 601),
    "m02s00i102": Stash(0, 602),
    "m02s00i103": Stash(0, 801),
    "m02s00i104": Stash(0, 802),
    "m02s00i105": Stash(0, 803),
    "m02s00i106": Stash(0, 804),
    "m02s00i107": Stash(0, 805),
    "m02s00i108": Stash(0, 806),
    "m02s00i109": Stash(0, 807),
    "m02s00i110": Stash(0, 808),
    "m02s00i111": Stash(0, 809),
    "m02s00i112": Stash(0, 810),
    "m02s00i113": Stash(0, 811),
    "m02s00i114": Stash(0, 812),
    "m02s00i115": Stash(0, 813),
    "m02s00i116": Stash(0, 814),
    "m02s00i117": Stash(0, 815),
    "m02s00i118": Stash(0, 816),
    "m02s00i119": Stash(0, 817),
    "m02s00i120": Stash(0, 818),
    "m02s00i121": Stash(0, 701),
    "m02s00i122": Stash(0, 702),
    "m02s00i128": Stash(0, 943),
    "m02s00i129": Stash(0, 944),
    "m02s00i130": Stash(0, 611),
    "m02s00i131": Stash(0, 612),
    "m02s00i132": Stash(0, 613),
    "m02s00i133": Stash(0, 614),
    "m02s00i134": Stash(0, 608),
    "m02s00i135": Stash(0, 711),
    "m02s00i136": Stash(0, 712),
    "m02s00i137": Stash(0, 653),
    "m02s00i139": Stash(0, 733),
    "m02s00i140": Stash(0, 734),
    "m02s00i141": Stash(0, 688),
    "m02s00i142": Stash(0, 685),
    "m02s00i143": Stash(0, 684),
    "m02s00i144": Stash(0, 686),
    "m02s00i145": Stash(0, 0),
    "m02s00i146": Stash(0, 683),
    "m02s00i147": Stash(0, 687),
    "m02s00i148": Stash(0, 728),
    "m02s00i149": Stash(0, 729),
    "m02s00i150": Stash(0, 721),
    "m02s00i151": Stash(0, 722),
    "m02s00i152": Stash(0, 627),
    "m02s00i153": Stash(0, 0),
    "m02s00i154": Stash(0, 866),
    "m02s00i155": Stash(0, 867),
    "m02s00i160": Stash(0, 659),
    "m02s00i161": Stash(0, 625),
    "m02s00i162": Stash(0, 626),
    "m02s00i165": Stash(0, 629),
    "m02s00i166": Stash(0, 631),
    "m02s00i167": Stash(0, 0),
    "m02s00i170": Stash(0, 698),
    "m02s00i171": Stash(0, 623),
    "m02s00i172": Stash(0, 624),
    "m02s00i175": Stash(0, 670),
    "m02s00i176": Stash(0, 670),
    "m02s00i177": Stash(0, 677),
    "m02s00i178": Stash(0, 678),
    "m02s00i179": Stash(0, 679),
    "m02s00i180": Stash(0, 650),
    "m02s00i181": Stash(0, 649),
    "m02s00i182": Stash(0, 0),
    "m02s00i183": Stash(0, 675),
    "m02s00i185": Stash(0, 671),
    "m02s00i186": Stash(0, 672),
    "m02s00i187": Stash(0, 687),
    "m02s00i188": Stash(0, 688),
    "m02s00i189": Stash(0, 689),
    "m02s00i190": Stash(0, 681),
    "m02s00i191": Stash(0, 682),
    "m02s00i192": Stash(0, 615),
    "m02s00i193": Stash(0, 616),
    "m02s00i194": Stash(0, 658),
    "m02s00i195": Stash(0, 609),
    "m02s00i196": Stash(0, 713),
    "m02s00i197": Stash(0, 714),
    "m02s00i198": Stash(0, 715),
    "m02s00i199": Stash(0, 716),
    "m02s00i200": Stash(0, 838),
    "m02s00i201": Stash(0, 839),
    "m02s00i202": Stash(0, 840),
    "m02s00i203": Stash(0, 841),
    "m02s00i204": Stash(0, 747),
    "m02s00i205": Stash(0, 748),
    "m02s00i206": Stash(0, 749),
    "m02s00i207": Stash(0, 0),
    "m02s00i208": Stash(0, 0),
    "m02s00i210": Stash(0, 800),
    "m02s00i211": Stash(0, 800),
    "m02s00i212": Stash(0, 800),
    "m02s00i213": Stash(0, 800),
    "m02s00i214": Stash(0, 800),
    "m02s00i215": Stash(0, 800),
    "m02s00i216": Stash(0, 800),
    "m02s00i217": Stash(0, 800),
    "m02s00i218": Stash(0, 800),
    "m02s00i219": Stash(0, 800),
    "m02s00i220": Stash(0, 800),
    "m02s00i221": Stash(0, 800),
    "m02s00i222": Stash(0, 1955),
    "m02s00i223": Stash(0, 1953),
    "m02s00i224": Stash(0, 1954),
    "m02s00i225": Stash(0, 1951),
    "m02s00i226": Stash(0, 1952),
    "m02s00i231": Stash(0, 1037),
    "m02s00i232": Stash(0, 1038),
    "m02s00i233": Stash(0, 1039),
    "m02s00i234": Stash(0, 1040),
    "m02s00i235": Stash(0, 1041),
    "m02s00i236": Stash(0, 1042),
    "m02s00i237": Stash(0, 1043),
    "m02s00i238": Stash(0, 1044),
    "m02s00i239": Stash(0, 1045),
    "m02s00i240": Stash(0, 1046),
    "m02s00i285": Stash(0, 617),
    "m02s00i291": Stash(0, 691),
    "m02s00i292": Stash(0, 692),
    "m02s00i293": Stash(0, 625),
    "m02s00i294": Stash(0, 626),
    "m02s00i295": Stash(0, 624),
    "m02s00i296": Stash(0, 681),
    "m02s00i297": Stash(0, 682),
    "m02s00i298": Stash(0, 681),
    "m02s00i299": Stash(0, 682),
    "m02s00i331": Stash(0, 749),
    "m02s00i332": Stash(0, 750),
    "m02s00i333": Stash(0, 751),
    "m02s30i201": Stash(0, 680),
    "m02s30i202": Stash(0, 653),
    "m02s30i203": Stash(0, 671),
    "m02s30i204": Stash(0, 672),
    "m02s30i205": Stash(0, 678),
    "m02s30i206": Stash(0, 626),
    "m02s30i207": Stash(0, 623),
    "m02s30i208": Stash(0, 685),
    "m02s30i210": Stash(0, 647),
    "m02s30i211": Stash(0, 740),
    "m02s30i212": Stash(0, 740),
    "m02s30i213": Stash(0, 740),
    "m02s30i214": Stash(0, 740),
    "m02s30i215": Stash(0, 740),
    "m02s30i216": Stash(0, 740),
    "m02s30i217": Stash(0, 740),
    "m02s30i218": Stash(0, 740),
    "m02s30i219": Stash(0, 740),
    "m02s30i220": Stash(0, 740),
    "m02s30i221": Stash(0, 740),
    "m02s30i222": Stash(0, 740),
    "m02s30i223": Stash(0, 740),
    "m02s30i224": Stash(0, 740),
    "m02s30i225": Stash(0, 740),
    "m02s30i226": Stash(0, 740),
    "m02s30i227": Stash(0, 740),
    "m02s30i228": Stash(0, 740),
    "m02s30i229": Stash(0, 740),
    "m02s30i230": Stash(0, 740),
    "m02s30i231": Stash(0, 801),
    "m02s30i232": Stash(0, 802),
    "m02s30i233": Stash(0, 803),
    "m02s30i234": Stash(0, 804),
    "m02s30i235": Stash(0, 805),
    "m02s30i236": Stash(0, 806),
    "m02s30i237": Stash(0, 807),
    "m02s30i238": Stash(0, 808),
    "m02s30i239": Stash(0, 809),
    "m02s30i240": Stash(0, 810),
    "m02s30i241": Stash(0, 811),
    "m02s30i242": Stash(0, 642),
    "m02s30i243": Stash(0, 813),
    "m02s30i244": Stash(0, 814),
    "m02s30i245": Stash(0, 815),
    "m02s30i246": Stash(0, 713),
    "m02s30i247": Stash(0, 714),
    "m02s30i248": Stash(0, 645),
    "m02s30i249": Stash(0, 646),
    "m02s30i250": Stash(0, 647),
    "m02s30i251": Stash(0, 648),
    "m02s30i252": Stash(0, 891),
    "m02s30i253": Stash(0, 892),
    "m02s30i254": Stash(0, 893),
    "m02s30i255": Stash(0, 894),
    "m02s30i256": Stash(0, 895),
    "m02s30i257": Stash(0, 896),
    "m02s30i258": Stash(0, 897),
    "m02s30i259": Stash(0, 898),
    "m02s30i260": Stash(0, 899),
    "m02s30i261": Stash(0, 900),
    "m02s30i262": Stash(0, 901),
    "m02s30i263": Stash(0, 902),
    "m02s30i264": Stash(0, 903),
    "m02s30i265": Stash(0, 904),
    "m02s30i266": Stash(0, 905),
    "m02s30i267": Stash(0, 906),
    "m02s30i268": Stash(0, 907),
    "m02s30i269": Stash(0, 908),
    "m02s30i270": Stash(0, 909),
    "m02s30i271": Stash(0, 910),
    "m02s30i272": Stash(0, 911),
    "m02s30i273": Stash(0, 912),
    "m02s30i274": Stash(0, 913),
    "m02s30i275": Stash(0, 914),
    "m02s30i276": Stash(0, 915),
    "m02s30i277": Stash(0, 916),
    "m02s30i278": Stash(0, 917),
    "m02s30i280": Stash(0, 632),
    "m02s30i281": Stash(0, 831),
    "m02s30i282": Stash(0, 832),
    "m02s30i283": Stash(0, 833),
    "m02s30i284": Stash(0, 834),
    "m02s30i285": Stash(0, 617),
    "m02s30i286": Stash(0, 795),
    "m02s30i287": Stash(0, 695),
    "m02s30i288": Stash(0, 796),
    "m02s30i289": Stash(0, 696),
    "m02s30i290": Stash(0, 797),
    "m02s30i291": Stash(0, 697),
    "m02s30i292": Stash(0, 673),
    "m02s30i293": Stash(0, 674),
    "m02s30i294": Stash(0, 867),
    "m02s30i296": Stash(0, 795),
    "m02s30i297": Stash(0, 695),
    "m02s30i298": Stash(0, 796),
    "m02s30i299": Stash(0, 696),
    "m02s30i301": Stash(0, 637),
    "m02s30i302": Stash(0, 865),
    "m02s30i303": Stash(0, 866),
    "m02s30i306": Stash(0, 648),
    "m02s30i307": Stash(0, 648),
    "m02s30i308": Stash(0, 648),
    "m02s30i309": Stash(0, 648),
    "m02s30i310": Stash(0, 648),
    "m02s30i311": Stash(0, 648),
    "m02s30i312": Stash(0, 648),
    "m02s30i313": Stash(0, 648),
    "m02s30i314": Stash(0, 648),
    "m02s30i315": Stash(0, 648),
    "m02s30i316": Stash(0, 648),
    "m02s30i317": Stash(0, 648),
    "m02s30i318": Stash(0, 648),
    "m02s30i319": Stash(0, 648),
    "m02s30i320": Stash(0, 703),
    "m02s30i321": Stash(0, 704),
    "m02s30i322": Stash(0, 648),
    "m02s30i323": Stash(0, 657),
    "m02s30i324": Stash(0, 653),
    "m02s30i325": Stash(0, 653),
    "m02s30i330": Stash(0, 860),
    "m02s30i331": Stash(0, 860),
    "m02s30i400": Stash(0, 891),
    "m02s30i401": Stash(0, 892),
    "m02s30i402": Stash(0, 893),
    "m02s30i403": Stash(0, 894),
    "m02s30i404": Stash(0, 895),
    "m02s30i405": Stash(0, 896),
    "m02s30i406": Stash(0, 897),
    "m02s30i410": Stash(0, 901),
    "m02s30i411": Stash(0, 902),
    "m02s30i412": Stash(0, 903),
    "m02s30i413": Stash(0, 904),
    "m02s30i414": Stash(0, 905),
    "m02s30i415": Stash(0, 906),
    "m02s30i416": Stash(0, 907),
    "m02s30i417": Stash(0, 908),
    "m02s30i418": Stash(0, 909),
    "m02s30i419": Stash(0, 910),
    "m02s30i420": Stash(0, 911),
    "m02s30i421": Stash(0, 912),
    "m02s30i422": Stash(0, 913),
    "m02s30i423": Stash(0, 914),
    "m02s30i424": Stash(0, 915),
    "m02s30i425": Stash(0, 916),
    "m02s30i426": Stash(0, 917),
    "m02s30i427": Stash(0, 918),
    "m02s30i428": Stash(0, 919),
    "m02s30i429": Stash(0, 920),
    "m02s30i430": Stash(0, 921),
    "m02s30i431": Stash(0, 922),
    "m02s30i432": Stash(0, 0),
    "m02s30i433": Stash(0, 0),
    "m02s30i434": Stash(0, 0),
    "m02s30i435": Stash(0, 0),
    "m02s30i436": Stash(0, 0),
    "m02s30i437": Stash(0, 0),
    "m02s30i438": Stash(0, 0),
    "m02s30i439": Stash(0, 0),
    "m02s30i440": Stash(0, 0),
    "m02s30i441": Stash(0, 0),
    "m02s30i442": Stash(0, 0),
    "m02s30i443": Stash(0, 0),
    "m02s30i444": Stash(0, 0),
    "m02s30i445": Stash(0, 0),
    "m02s30i446": Stash(0, 0),
    "m02s30i447": Stash(0, 0),
    "m02s30i448": Stash(0, 0),
    "m02s30i449": Stash(0, 940),
    "m02s30i450": Stash(0, 0),
    "m02s30i451": Stash(0, 0),
    "m02s30i452": Stash(0, 0),
    "m02s30i453": Stash(0, 0),
    "m02s30i454": Stash(0, 0),
    "m02s30i455": Stash(0, 0),
    "m02s30i456": Stash(0, 0),
    "m02s30i457": Stash(0, 0),
    "m02s30i458": Stash(0, 0),
    "m02s30i459": Stash(0, 940),
    "m02s30i460": Stash(0, 0),
    "m02s30i461": Stash(0, 940),
    "m02s30i462": Stash(0, 941),
    "m02s30i463": Stash(0, 942),
    "m02s30i464": Stash(0, 943),
    "m02s30i465": Stash(0, 944),
    "m02s30i466": Stash(0, 945),
    "m02s30i467": Stash(0, 946),
    "m02s30i468": Stash(0, 947),
    "m02s30i469": Stash(0, 0),
    "m02s30i470": Stash(0, 0),
    "m02s30i471": Stash(0, 0),
    "m02s31i201": Stash(0, 715),
    "m02s31i202": Stash(0, 618),
    "m02s31i211": Stash(0, 660),
    "m02s31i212": Stash(0, 661),
    "m02s31i213": Stash(0, 662),
    "m02s31i214": Stash(0, 663),
    "m02s31i215": Stash(0, 664),
    "m02s31i216": Stash(0, 665),
    "m02s31i217": Stash(0, 666),
    "m02s31i218": Stash(0, 667),
    "m02s31i219": Stash(0, 668),
    "m02s31i220": Stash(0, 669),
    "m02s31i285": Stash(0, 617),
    "m02s31i290": Stash(0, 669),
    "m02s32i201": Stash(0, 683),
    "m02s32i202": Stash(0, 687),
    "m02s32i203": Stash(0, 688),
    "m02s32i204": Stash(0, 687),
    "m02s32i205": Stash(0, 689),
    "m02s32i206": Stash(0, 690),
    "m02s32i207": Stash(0, 728),
    "m02s32i208": Stash(0, 729),
    "m02s32i209": Stash(0, 731),
    "m02s32i210": Stash(0, 732),
    "m02s32i211": Stash(0, 683),
    "m02s32i212": Stash(0, 687),
    "m02s32i213": Stash(0, 688),
    "m02s32i214": Stash(0, 626),
    "m02s32i215": Stash(0, 623),
    "m02s32i216": Stash(0, 601),
    "m02s32i217": Stash(0, 626),
    "m02s32i218": Stash(0, 626),
    "m02s32i219": Stash(0, 733),
    "m02s32i220": Stash(0, 734),
    "m02s32i221": Stash(0, 735),
    "m02s32i222": Stash(0, 736),
    "m02s32i223": Stash(0, 918),
    "m02s32i224": Stash(0, 919),
    "m02s32i225": Stash(0, 920),
    "m02s32i226": Stash(0, 919),
    "m02s32i227": Stash(0, 918),
    "m02s32i228": Stash(0, 919),
    "m02s32i229": Stash(0, 920),
    "m02s32i230": Stash(0, 737),
    "m02s32i231": Stash(0, 738),
    "m02s32i232": Stash(0, 918),
    "m02s32i233": Stash(0, 919),
    "m02s32i234": Stash(0, 920),
    "m02s32i235": Stash(0, 1959),
    "m02s32i236": Stash(0, 1956),
    "m02s32i237": Stash(0, 1957),
    "m02s32i238": Stash(0, 1958),
    "m02s32i239": Stash(0, 1956),
    "m02s32i240": Stash(0, 1957),
    "m02s32i241": Stash(0, 1958),
    "m02s32i242": Stash(0, 1956),
    "m02s32i243": Stash(0, 1957),
    "m02s32i244": Stash(0, 1958),
    "m02s32i245": Stash(0, 801),
    "m02s32i246": Stash(0, 801),
    "m02s32i247": Stash(0, 919),
    "m02s32i248": Stash(0, 920),
    "m02s32i249": Stash(0, 1957),
    "m02s32i250": Stash(0, 1958),
    "m02s32i300": Stash(0, 801),
    "m02s32i301": Stash(0, 801),
    "m02s32i302": Stash(0, 801),
    "m02s32i303": Stash(0, 801),
    "m02s32i304": Stash(0, 801),
    "m02s32i305": Stash(0, 801),
    "m02s32i306": Stash(0, 801),
    "m02s32i307": Stash(0, 801),
    "m02s32i308": Stash(0, 801),
    "m02s32i309": Stash(0, 801),
    "m02s35i101": Stash(0, 601),
    "m02s35i102": Stash(0, 602),
    "m02s35i121": Stash(0, 701),
    "m02s35i122": Stash(0, 702),
    "m02s35i130": Stash(0, 611),
    "m02s35i134": Stash(0, 608),
    "m02s35i135": Stash(0, 711),
    "m02s35i136": Stash(0, 712),
    "m02s35i137": Stash(0, 653),
    "m02s35i201": Stash(0, 850),
    "m02s35i202": Stash(0, 851),
    "m02s35i203": Stash(0, 852),
    "m02s35i204": Stash(0, 853),
    "m02s35i205": Stash(0, 854),
    "m02s35i206": Stash(0, 855),
    "m02s35i211": Stash(0, 860),
    "m02s35i213": Stash(0, 862),
    "m02s35i214": Stash(0, 863),
    "m02s35i215": Stash(0, 864),
    "m02s35i221": Stash(0, 871),
    "m02s35i224": Stash(0, 872),
    "m02s35i225": Stash(0, 873),
    "m02s35i231": Stash(0, 876),
    "m02s35i234": Stash(0, 877),
    "m02s35i235": Stash(0, 878),
    "m02s35i241": Stash(0, 880),
    "m02s35i244": Stash(0, 881),
    "m02s35i245": Stash(0, 882),
    "m02s35i251": Stash(0, 885),
    "m02s35i254": Stash(0, 888),
    "m02s35i255": Stash(0, 884),
    "m02s35i281": Stash(0, 683),
    "m02s35i282": Stash(0, 687),
    "m02s35i283": Stash(0, 688),
    "m02s35i284": Stash(0, 685),
    "m02s35i285": Stash(0, 686),
    "m02s35i301": Stash(0, 857),
    "m02s35i302": Stash(0, 858),
    "m02s35i310": Stash(0, 859),
    "m02s35i323": Stash(0, 699),
    "m02s41i101": Stash(0, 601),
    "m02s41i102": Stash(0, 602),
    "m02s41i103": Stash(0, 801),
    "m02s41i104": Stash(0, 802),
    "m02s41i105": Stash(0, 803),
    "m02s41i106": Stash(0, 804),
    "m02s41i107": Stash(0, 805),
    "m02s41i108": Stash(0, 806),
    "m02s41i109": Stash(0, 807),
    "m02s41i110": Stash(0, 808),
    "m02s41i111": Stash(0, 809),
    "m02s41i112": Stash(0, 810),
    "m02s41i113": Stash(0, 811),
    "m02s41i114": Stash(0, 812),
    "m02s41i115": Stash(0, 813),
    "m02s41i116": Stash(0, 814),
    "m02s41i117": Stash(0, 815),
    "m02s41i118": Stash(0, 816),
    "m02s41i119": Stash(0, 817),
    "m02s41i120": Stash(0, 818),
    "m02s41i121": Stash(0, 701),
    "m02s41i122": Stash(0, 702),
    "m02s41i130": Stash(0, 611),
    "m02s41i131": Stash(0, 612),
    "m02s41i132": Stash(0, 613),
    "m02s41i133": Stash(0, 614),
    "m02s41i134": Stash(0, 608),
    "m02s41i135": Stash(0, 711),
    "m02s41i136": Stash(0, 712),
    "m02s41i137": Stash(0, 653),
    "m02s41i139": Stash(0, 633),
    "m02s41i140": Stash(0, 634),
    "m02s41i141": Stash(0, 688),
    "m02s41i142": Stash(0, 685),
    "m02s41i143": Stash(0, 684),
    "m02s41i144": Stash(0, 686),
    "m02s41i145": Stash(0, 0),
    "m02s41i146": Stash(0, 683),
    "m02s41i147": Stash(0, 687),
    "m02s41i148": Stash(0, 728),
    "m02s41i149": Stash(0, 729),
    "m02s41i150": Stash(0, 721),
    "m02s41i151": Stash(0, 722),
    "m02s41i152": Stash(0, 627),
    "m02s41i153": Stash(0, 0),
    "m02s41i161": Stash(0, 625),
    "m02s41i162": Stash(0, 626),
    "m02s41i165": Stash(0, 629),
    "m02s41i166": Stash(0, 631),
    "m02s41i167": Stash(0, 0),
    "m02s41i170": Stash(0, 698),
    "m02s41i171": Stash(0, 623),
    "m02s41i172": Stash(0, 624),
    "m02s41i180": Stash(0, 650),
    "m02s41i181": Stash(0, 649),
    "m02s41i182": Stash(0, 0),
    "m02s41i183": Stash(0, 675),
    "m02s41i185": Stash(0, 671),
    "m02s41i186": Stash(0, 672),
    "m02s41i187": Stash(0, 678),
    "m02s41i190": Stash(0, 681),
    "m02s41i191": Stash(0, 682),
    "m02s42i101": Stash(0, 601),
    "m02s42i102": Stash(0, 602),
    "m02s42i103": Stash(0, 801),
    "m02s42i104": Stash(0, 802),
    "m02s42i105": Stash(0, 803),
    "m02s42i106": Stash(0, 804),
    "m02s42i107": Stash(0, 805),
    "m02s42i108": Stash(0, 806),
    "m02s42i109": Stash(0, 807),
    "m02s42i110": Stash(0, 808),
    "m02s42i111": Stash(0, 809),
    "m02s42i112": Stash(0, 810),
    "m02s42i113": Stash(0, 811),
    "m02s42i114": Stash(0, 812),
    "m02s42i115": Stash(0, 813),
    "m02s42i116": Stash(0, 814),
    "m02s42i117": Stash(0, 815),
    "m02s42i118": Stash(0, 816),
    "m02s42i119": Stash(0, 817),
    "m02s42i120": Stash(0, 818),
    "m02s42i121": Stash(0, 701),
    "m02s42i122": Stash(0, 702),
    "m02s42i130": Stash(0, 611),
    "m02s42i131": Stash(0, 612),
    "m02s42i132": Stash(0, 613),
    "m02s42i133": Stash(0, 614),
    "m02s42i134": Stash(0, 608),
    "m02s42i135": Stash(0, 711),
    "m02s42i136": Stash(0, 712),
    "m02s42i137": Stash(0, 653),
    "m02s42i139": Stash(0, 633),
    "m02s42i140": Stash(0, 634),
    "m02s42i141": Stash(0, 688),
    "m02s42i142": Stash(0, 685),
    "m02s42i143": Stash(0, 684),
    "m02s42i144": Stash(0, 686),
    "m02s42i145": Stash(0, 0),
    "m02s42i146": Stash(0, 683),
    "m02s42i147": Stash(0, 687),
    "m02s42i148": Stash(0, 728),
    "m02s42i149": Stash(0, 729),
    "m02s42i150": Stash(0, 721),
    "m02s42i151": Stash(0, 722),
    "m02s42i152": Stash(0, 627),
    "m02s42i153": Stash(0, 0),
    "m02s42i161": Stash(0, 625),
    "m02s42i162": Stash(0, 626),
    "m02s42i165": Stash(0, 629),
    "m02s42i166": Stash(0, 631),
    "m02s42i167": Stash(0, 0),
    "m02s42i170": Stash(0, 698),
    "m02s42i171": Stash(0, 623),
    "m02s42i172": Stash(0, 624),
    "m02s42i180": Stash(0, 650),
    "m02s42i181": Stash(0, 649),
    "m02s42i182": Stash(0, 0),
    "m02s42i183": Stash(0, 675),
    "m02s42i185": Stash(0, 671),
    "m02s42i186": Stash(0, 672),
    "m02s42i187": Stash(0, 678),
    "m02s42i190": Stash(0, 681),
    "m02s42i191": Stash(0, 682),
    "m02s43i101": Stash(0, 601),
    "m02s43i102": Stash(0, 602),
    "m02s43i103": Stash(0, 801),
    "m02s43i104": Stash(0, 802),
    "m02s43i105": Stash(0, 803),
    "m02s43i106": Stash(0, 804),
    "m02s43i107": Stash(0, 805),
    "m02s43i108": Stash(0, 806),
    "m02s43i109": Stash(0, 807),
    "m02s43i110": Stash(0, 808),
    "m02s43i111": Stash(0, 809),
    "m02s43i112": Stash(0, 810),
    "m02s43i113": Stash(0, 811),
    "m02s43i114": Stash(0, 812),
    "m02s43i115": Stash(0, 813),
    "m02s43i116": Stash(0, 814),
    "m02s43i117": Stash(0, 815),
    "m02s43i118": Stash(0, 816),
    "m02s43i119": Stash(0, 817),
    "m02s43i120": Stash(0, 818),
    "m02s43i121": Stash(0, 701),
    "m02s43i122": Stash(0, 702),
    "m02s43i130": Stash(0, 611),
    "m02s43i131": Stash(0, 612),
    "m02s43i132": Stash(0, 613),
    "m02s43i133": Stash(0, 614),
    "m02s43i134": Stash(0, 608),
    "m02s43i135": Stash(0, 711),
    "m02s43i136": Stash(0, 712),
    "m02s43i137": Stash(0, 653),
    "m02s43i139": Stash(0, 633),
    "m02s43i140": Stash(0, 634),
    "m02s43i141": Stash(0, 688),
    "m02s43i142": Stash(0, 685),
    "m02s43i143": Stash(0, 684),
    "m02s43i144": Stash(0, 686),
    "m02s43i145": Stash(0, 0),
    "m02s43i146": Stash(0, 683),
    "m02s43i147": Stash(0, 687),
    "m02s43i148": Stash(0, 728),
    "m02s43i149": Stash(0, 729),
    "m02s43i150": Stash(0, 721),
    "m02s43i151": Stash(0, 722),
    "m02s43i152": Stash(0, 627),
    "m02s43i153": Stash(0, 0),
    "m02s43i161": Stash(0, 625),
    "m02s43i162": Stash(0, 626),
    "m02s43i165": Stash(0, 629),
    "m02s43i166": Stash(0, 631),
    "m02s43i167": Stash(0, 0),
    "m02s43i170": Stash(0, 698),
    "m02s43i171": Stash(0, 623),
    "m02s43i172": Stash(0, 624),
    "m02s43i180": Stash(0, 650),
    "m02s43i181": Stash(0, 649),
    "m02s43i182": Stash(0, 0),
    "m02s43i183": Stash(0, 675),
    "m02s43i185": Stash(0, 671),
    "m02s43i186": Stash(0, 672),
    "m02s43i187": Stash(0, 678),
    "m02s43i190": Stash(0, 681),
    "m02s43i191": Stash(0, 682),
    "m02s44i101": Stash(0, 601),
    "m02s44i102": Stash(0, 602),
    "m02s44i103": Stash(0, 801),
    "m02s44i104": Stash(0, 802),
    "m02s44i105": Stash(0, 803),
    "m02s44i106": Stash(0, 804),
    "m02s44i107": Stash(0, 805),
    "m02s44i108": Stash(0, 806),
    "m02s44i109": Stash(0, 807),
    "m02s44i110": Stash(0, 808),
    "m02s44i111": Stash(0, 809),
    "m02s44i112": Stash(0, 810),
    "m02s44i113": Stash(0, 811),
    "m02s44i114": Stash(0, 812),
    "m02s44i115": Stash(0, 813),
    "m02s44i116": Stash(0, 814),
    "m02s44i117": Stash(0, 815),
    "m02s44i118": Stash(0, 816),
    "m02s44i119": Stash(0, 817),
    "m02s44i120": Stash(0, 818),
    "m02s44i121": Stash(0, 701),
    "m02s44i122": Stash(0, 702),
    "m02s44i130": Stash(0, 611),
    "m02s44i131": Stash(0, 612),
    "m02s44i132": Stash(0, 613),
    "m02s44i133": Stash(0, 614),
    "m02s44i134": Stash(0, 608),
    "m02s44i135": Stash(0, 711),
    "m02s44i136": Stash(0, 712),
    "m02s44i137": Stash(0, 653),
    "m02s44i139": Stash(0, 633),
    "m02s44i140": Stash(0, 634),
    "m02s44i141": Stash(0, 688),
    "m02s44i142": Stash(0, 685),
    "m02s44i143": Stash(0, 684),
    "m02s44i144": Stash(0, 686),
    "m02s44i145": Stash(0, 0),
    "m02s44i146": Stash(0, 683),
    "m02s44i147": Stash(0, 687),
    "m02s44i148": Stash(0, 728),
    "m02s44i149": Stash(0, 729),
    "m02s44i150": Stash(0, 721),
    "m02s44i151": Stash(0, 722),
    "m02s44i152": Stash(0, 627),
    "m02s44i153": Stash(0, 0),
    "m02s44i161": Stash(0, 625),
    "m02s44i162": Stash(0, 626),
    "m02s44i165": Stash(0, 629),
    "m02s44i166": Stash(0, 631),
    "m02s44i167": Stash(0, 0),
    "m02s44i170": Stash(0, 698),
    "m02s44i171": Stash(0, 623),
    "m02s44i172": Stash(0, 624),
    "m02s44i180": Stash(0, 650),
    "m02s44i181": Stash(0, 649),
    "m02s44i182": Stash(0, 0),
    "m02s44i183": Stash(0, 675),
    "m02s44i185": Stash(0, 671),
    "m02s44i186": Stash(0, 672),
    "m02s44i187": Stash(0, 678),
    "m02s44i190": Stash(0, 681),
    "m02s44i191": Stash(0, 682),
    "m03s00i142": Stash(3, 650),
    "m03s00i143": Stash(3, 675),
    "m03s00i177": Stash(1, 620),
    "m03s00i178": Stash(1, 650),
    "m03s00i179": Stash(1, 675),
    "m03s00i210": Stash(3, 16),
    "m03s00i211": Stash(11, 728),
    "m03s00i212": Stash(11, 729),
    "m03s00i280": Stash(3, 800),
    "m03s00i281": Stash(3, 800),
    "m03s00i282": Stash(3, 800),
    "m03s00i283": Stash(3, 800),
    "m03s00i284": Stash(3, 800),
    "m03s00i285": Stash(3, 800),
    "m03s00i286": Stash(3, 800),
    "m03s00i287": Stash(3, 800),
    "m03s00i288": Stash(3, 800),
    "m03s00i289": Stash(3, 800),
    "m03s00i290": Stash(3, 800),
    "m03s00i291": Stash(3, 800),
    "m03s00i292": Stash(18, 728),
    "m03s00i293": Stash(19, 729),
    "m03s21i177": Stash(1, 620),
    "m03s21i178": Stash(1, 650),
    "m03s21i179": Stash(1, 675),
    "m03s21i225": Stash(3, 16),
    "m03s21i226": Stash(11, 728),
    "m03s21i227": Stash(11, 729),
    "m03s22i177": Stash(1, 620),
    "m03s22i178": Stash(1, 650),
    "m03s22i179": Stash(1, 675),
    "m03s22i225": Stash(3, 16),
    "m03s22i226": Stash(11, 728),
    "m03s22i227": Stash(11, 729),
    "m03s23i177": Stash(1, 620),
    "m03s23i178": Stash(1, 650),
    "m03s23i179": Stash(1, 675),
    "m03s23i225": Stash(3, 16),
    "m03s23i226": Stash(11, 728),
    "m03s23i227": Stash(11, 729),
    "m03s24i177": Stash(1, 620),
    "m03s24i178": Stash(1, 650),
    "m03s24i179": Stash(1, 675),
    "m03s24i225": Stash(3, 16),
    "m03s24i226": Stash(11, 728),
    "m03s24i227": Stash(11, 729),
    "m03s40i023": Stash(1, 93),
    "m03s40i024": Stash(1, 16),
    "m03s40i031": Stash(3, 37),
    "m03s40i032": Stash(3, 687),
    "m03s40i177": Stash(1, 620),
    "m03s40i178": Stash(1, 650),
    "m03s40i179": Stash(1, 675),
    "m03s40i201": Stash(1, 620),
    "m03s40i202": Stash(3, 620),
    "m03s40i203": Stash(3, 687),
    "m03s40i204": Stash(3, 683),
    "m03s40i205": Stash(3, 687),
    "m03s40i206": Stash(3, 687),
    "m03s40i207": Stash(3, 688),
    "m03s40i208": Stash(3, 683),
    "m03s40i209": Stash(3, 687),
    "m03s40i210": Stash(3, 688),
    "m03s40i211": Stash(3, 684),
    "m03s40i212": Stash(3, 689),
    "m03s40i213": Stash(3, 690),
    "m03s40i214": Stash(3, 188),
    "m03s40i215": Stash(3, 188),
    "m03s40i216": Stash(3, 14),
    "m03s40i217": Stash(3, 14),
    "m03s40i218": Stash(3, 910),
    "m03s40i219": Stash(3, 911),
    "m03s40i220": Stash(3, 685),
    "m03s40i221": Stash(3, 912),
    "m03s40i222": Stash(3, 108),
    "m03s40i223": Stash(3, 108),
    "m03s40i224": Stash(3, 687),
    "m03s40i225": Stash(3, 688),
    "m03s40i226": Stash(3, 15),
    "m03s40i230": Stash(3, 801),
    "m03s40i231": Stash(3, 801),
    "m03s40i232": Stash(19, 801),
    "m03s40i233": Stash(19, 801),
    "m03s40i234": Stash(19, 801),
    "m03s40i235": Stash(19, 801),
    "m03s40i240": Stash(1, 93),
    "m03s40i241": Stash(3, 37),
    "m03s40i242": Stash(3, 687),
    "m03s40i243": Stash(3, 687),
    "m03s40i244": Stash(3, 683),
    "m03s40i245": Stash(3, 687),
    "m03s40i246": Stash(3, 688),
    "m03s40i247": Stash(3, 683),
    "m03s40i248": Stash(3, 687),
    "m03s40i249": Stash(3, 688),
    "m03s40i250": Stash(3, 683),
    "m03s40i251": Stash(3, 687),
    "m03s40i252": Stash(3, 688),
    "m03s40i253": Stash(3, 683),
    "m03s40i254": Stash(3, 687),
    "m03s40i255": Stash(3, 688),
    "m04s00i001": Stash(0, 351),
    "m04s00i002": Stash(0, 38),
    "m04s00i003": Stash(0, 2),
    "m04s00i004": Stash(0, 56),
    "m04s00i005": Stash(0, 57),
    "m04s00i006": Stash(0, 61),
    "m04s00i007": Stash(0, 62),
    "m04s00i008": Stash(0, 37),
    "m04s00i009": Stash(0, 364),
    "m04s00i010": Stash(0, 365),
    "m04s01i001": Stash(0, 353),
    "m04s02i001": Stash(0, 367),
    "m04s02i002": Stash(0, 61),
    "m04s02i003": Stash(0, 62),
    "m04s02i004": Stash(0, 364),
    "m04s02i005": Stash(0, 365),
    "m04s02i006": Stash(0, 366),
    "m04s02i007": Stash(0, 354),
    "m04s03i001": Stash(0, 355),
    "m04s04i001": Stash(0, 356),
    "m04s05i001": Stash(0, 357),
    "m04s06i001": Stash(0, 387),
    "m04s06i002": Stash(0, 393),
    "m04s06i003": Stash(0, 394),
    "m04s06i004": Stash(0, 392),
    "m04s06i005": Stash(0, 385),
    "m04s06i006": Stash(0, 388),
    "m04s06i007": Stash(0, 389),
    "m04s06i008": Stash(0, 386),
    "m04s06i009": Stash(0, 390),
    "m04s06i010": Stash(0, 391),
    "m04s06i011": Stash(0, 398),
    "m04s06i012": Stash(0, 381),
    "m04s06i013": Stash(0, 382),
    "m04s06i014": Stash(0, 383),
    "m04s06i015": Stash(0, 399),
    "m04s06i016": Stash(0, 400),
    "m04s06i017": Stash(0, 50),
    "m04s06i018": Stash(0, 55),
    "m04s06i019": Stash(0, 395),
    "m04s06i020": Stash(0, 2),
    "m04s06i021": Stash(0, 366),
    "m04s06i022": Stash(0, 368),
    "m04s06i023": Stash(0, 369),
    "m04s06i024": Stash(0, 392),
    "m04s06i025": Stash(0, 352),
    "m04s06i026": Stash(0, 353),
    "m04s06i027": Stash(0, 354),
    "m04s06i028": Stash(0, 355),
    "m04s06i029": Stash(0, 356),
    "m04s06i030": Stash(0, 357),
    "m04s06i031": Stash(0, 358),
    "m04s06i032": Stash(0, 396),
    "m04s06i033": Stash(0, 397),
    "m04s06i034": Stash(0, 398),
    "m04s06i035": Stash(0, 401),
    "m04s07i001": Stash(0, 358),
    "m04s07i002": Stash(0, 359),
    "m04s07i003": Stash(0, 360),
}

########NEW FILE########
__FILENAME__ = format_picker
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
A module to provide convenient file format identification through a combination of filename extension
and file based *magic* numbers.


To manage a collection of FormatSpecifications for loading::

    import iris.io.format_picker as fp
    import matplotlib.pyplot as plt
    fagent = fp.FormatAgent()
    png_spec = fp.FormatSpecification('PNG image', fp.MagicNumber(8),
                                      0x89504E470D0A1A0A, 
                                      handler=lambda filename: plt.imread(filename),
                                      priority=5
                                      )
    fagent.add_spec(png_spec)

To identify a specific format from a file::

    handling_spec = fagent.get_spec(png_filename, open(png_filename, 'rb'))

In the example, handling_spec will now be the png_spec previously added to the agent.

Now that a specification has been found, if a handler has been given with the specification, then the file can be handled::

    handler = handling_spec.handler
    if handler is None:
       raise ValueError('File cannot be handled.')
    else:
       result = handler(filename)

The calling sequence of handler is dependent on the function given in the original specification and can be customised to your project's needs.


"""
import collections
import os
import struct


import iris.io


class FormatAgent(object):
    """
    The FormatAgent class is the containing object which is responsible for identifying the format of a given file
    by interrogating its children FormatSpecification instances.

    Typically a FormatAgent will be created empty and then extended with the :meth:`FormatAgent.add_spec` method::

        agent = FormatAgent()
        agent.add_spec(NetCDF_specification)

    Less commonly, this can also be written::

        agent = FormatAgent(NetCDF_specification)

    """
    def __init__(self, format_specs=None):
        """ """
        self._format_specs = list(format_specs or [])
        self._format_specs.sort()

    def add_spec(self, format_spec):
        """Add a FormatSpecification instance to this agent for format consideration."""
        self._format_specs.append(format_spec)
        self._format_specs.sort()

    def __repr__(self):
        return 'FormatAgent(%r)' % self._format_specs

    def __str__(self):
        prefix = ' * ' if len(self._format_specs) > 1 else ''
        return prefix + '\n * '.join(['%s' % format_spec for format_spec in self._format_specs])

    def get_spec(self, basename, buffer_obj):
        """
        Pick the first FormatSpecification which can handle the given
        filename and file/buffer object.

        .. note::

            ``buffer_obj`` may be ``None`` when a seekable file handle is not
            feasible (such as over the http protocol). In these cases only the
            format specifications which do not require a file handle are
            tested.

        """
        element_cache = {}
        for format_spec in self._format_specs:
            # For the case where a buffer_obj is None (such as for the
            # http protocol) skip any specs which require a fh - they
            # don't match.
            if buffer_obj is None and format_spec.file_element.requires_fh:
                continue

            fmt_elem = format_spec.file_element
            fmt_elem_value = format_spec.file_element_value

            # cache the results for each file element
            if repr(fmt_elem) not in element_cache:
                # N.B. File oriented as this is assuming seekable stream.
                if buffer_obj is not None and buffer_obj.tell() != 0:
                    # reset the buffer if tell != 0
                    buffer_obj.seek(0)
 
                element_cache[repr(fmt_elem)] = \
                    fmt_elem.get_element(basename, buffer_obj)

            # If we have a callable object, then call it and tests its result, otherwise test using basic equality
            if isinstance(fmt_elem_value, collections.Callable):
                matches = fmt_elem_value(element_cache[repr(fmt_elem)])
            elif element_cache[repr(fmt_elem)] == fmt_elem_value:
                matches = True
            else:
                matches = False

            if matches:
                return format_spec

        printable_values = {}
        for key, value in element_cache.iteritems():
            value = str(value)
            if len(value) > 50:
                value = value[:50] + '...'
            printable_values[key] = value
        msg = ('No format specification could be found for the given buffer.'
               ' File element cache:\n {}'.format(printable_values))
        raise ValueError(msg)


class FormatSpecification(object):
    """
    Provides the base class for file type definition.

    Every FormatSpecification instance has a name which can be accessed with the :attr:`FormatSpecification.name` property and
    a FileElement, such as filename extension or 32-bit magic number, with an associated value for format identification.

    """
    def __init__(self, format_name, file_element, file_element_value, handler=None, priority=0):
        """
        Constructs a new FormatSpecification given the format_name and particular FileElements

        Args:

        * format_name - string name of fileformat being described
        * file_element - FileElement instance of the element which identifies this FormatSpecification
        * file_element_value - The value that the file_element should take if a file matches this FormatSpecification

        Kwargs:

        * handler - function which will be called when the specification has been identified and is required to handler a format.
                            If None, then the file can still be identified but no handling can be done.
        * priority - Integer giving a priority for considering this specification where higher priority means sooner consideration.

        """
        if not isinstance(file_element, FileElement):
            raise ValueError('file_element must be an instance of FileElement, got %r' % file_element)


        self._file_element = file_element
        self._file_element_value = file_element_value
        self._format_name = format_name
        self._handler = handler
        self.priority = priority

    def __hash__(self):
        # Hashed by specification for consistent ordering in FormatAgent (including self._handler in this hash
        # for example would order randomly according to object id)
        return hash(self._file_element)

    @property
    def file_element(self):
        return self._file_element

    @property
    def file_element_value(self):
        return self._file_element_value

    @property
    def name(self):
        """The name of this FileFormat. (Read only)"""
        return self._format_name

    @property
    def handler(self):
        """The handler function of this FileFormat. (Read only)"""
        return self._handler

    def __cmp__(self, other):
        if not isinstance(other, FormatSpecification):
            return NotImplemented

        return cmp( (-self.priority, hash(self)), (-other.priority, hash(other)) )

    def __repr__(self):
        # N.B. loader is not always going to provide a nice repr if it is a lambda function, hence a prettier version is available in __str__
        return 'FormatSpecification(%r, %r, %r, handler=%r, priority=%s)' % (self._format_name, self._file_element,
                                                                            self._file_element_value, self.handler, self.priority)

    def __str__(self):
        return '%s%s (priority %s)' % (self.name, ' (no handler available)' if self.handler is None else '',  self.priority)


class FileElement(object):
    """
    Represents a specific aspect of a FileFormat which can be identified using the given element getter function.

    """
    def __init__(self, requires_fh=True):
        """
        Constructs a new file element, which may require a file buffer.

        Kwargs:

        * requires_fh - Whether this FileElement needs a file buffer.

        """
        self.requires_fh = requires_fh
    
    def get_element(self, basename, file_handle):
        """Called when identifying the element of a file that this FileElement is representing."""
        raise NotImplementedError("get_element must be defined in a subclass")
        
    def __hash__(self):
        return hash(repr(self))
    
    def __repr__(self):
        return '{}()'.format(self.__class__.__name__)


class MagicNumber(FileElement):
    """A :class:`FileElement` that returns a byte sequence in the file."""
    len_formats = {4: ">L", 8: ">Q"}

    def __init__(self, num_bytes, offset=None):
        FileElement.__init__(self)
        self._num_bytes = num_bytes
        self._offset = offset

    def get_element(self, basename, file_handle):
        if self._offset is not None:
            file_handle.seek(self._offset)
        bytes = file_handle.read(self._num_bytes)
        fmt = self.len_formats.get(self._num_bytes)
        if len(bytes) != self._num_bytes:
            raise EOFError(file_handle.name)
        if fmt is None:
            result = bytes
        else:
            result = struct.unpack(fmt, bytes)[0]
        return result

    def __repr__(self):
        return 'MagicNumber({}, {})'.format(self._num_bytes, self._offset)


class FileExtension(FileElement):
    """A :class:`FileElement` that returns the extension from the filename."""
    def get_element(self, basename, file_handle):
        return os.path.splitext(basename)[1]


class LeadingLine(FileElement):
    """A :class:`FileElement` that returns the first line from the file."""
    def get_element(self, basename, file_handle):
        return file_handle.readline()


class UriProtocol(FileElement):
    """
    A :class:`FileElement` that returns the "scheme" and "part" from a URI,
    using :func:`~iris.io.decode_uri`.

    """
    def __init__(self):
        FileElement.__init__(self, requires_fh=False)

    def get_element(self, basename, file_handle):
        return iris.io.decode_uri(basename)[0]


########NEW FILE########
__FILENAME__ = iterate
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Cube functions for iteration in step.

"""

import collections
import itertools
import warnings

import numpy as np

import iris.exceptions

__all__ = ['izip']


def izip(*cubes, **kwargs):
    """
    Return an iterator for iterating over a collection of cubes in step.

    If the input cubes have dimensions for which there are no common
    coordinates, those dimensions will be treated as orthogonal. The
    resulting iterator will step through combinations of the associated
    coordinates.

    Args:

    * cubes (:class:`iris.cube.Cube`):
        One or more :class:`iris.cube.Cube` instances over which to iterate in
        step. Each cube should be provided as a separate argument e.g.
        ``iris.iterate.izip(cube_a, cube_b, cube_c, ...)``.

    Kwargs:

    * coords (string, coord or a list of strings/coords):
        Coordinate names/coordinates of the desired subcubes (i.e. those
        that are not iterated over). They must all be orthogonal (i.e. point
        to different dimensions).
    * ordered (Boolean):
        If True (default), the order of the coordinates in the resulting
        subcubes will match the order of the coordinates in the coords
        keyword argument. If False, the order of the coordinates will
        be preserved and will match that of the input cubes.

    Returns:
        An iterator over a collection of tuples that contain the resulting
        subcubes.

    For example:
        >>> e_content, e_density = iris.load_cubes(
        ...     iris.sample_data_path('space_weather.nc'),
        ...     ['total electron content', 'electron density'])
        >>> for tslice, hslice in iris.iterate.izip(e_content, e_density,
        ...                                         coords=['grid_latitude',
        ...                                                 'grid_longitude']):
        ...    pass

    """
    if not cubes:
        raise TypeError('Expected one or more cubes.')

    ordered = kwargs.get('ordered', True)
    if not isinstance(ordered, bool):
        raise TypeError('Expected bool ordered parameter, got %r' % ordered)

    # Convert any coordinate names to coordinates (and ensure each cube has
    # requested slice coords).
    coords_to_slice = kwargs.get('coords')
    coords_by_cube = []

    for cube in cubes:
        if coords_to_slice is None or not coords_to_slice:
            coords_by_cube.append([])
        else:
            coords_by_cube.append(cube._as_list_of_coords(coords_to_slice))

    # For each input cube, generate the union of all describing dimensions for
    # the resulting subcube.
    requested_dims_by_cube = []
    for coords, cube in itertools.izip(coords_by_cube, cubes):
        requested_dims = set()
        for coord in coords:
            requested_dims.update(cube.coord_dims(coord))

        # Make sure this cube has no shared dimensions between the requested
        # coords.
        if len(requested_dims) != sum((len(cube.coord_dims(coord)) for coord in
                                       coords)):
            msg = 'The requested coordinates (%r) of cube (%r) are not ' \
                  'orthogonal.' % ([coord.name() for coord in coords], cube)
            raise ValueError(msg)

        requested_dims_by_cube.append(requested_dims)

    # Checks on coordinates you are going to iterate over.
    # Create a list of sets (one set per cube), with each set containing the
    # dimensioned coordinates that will be iterated over (i.e exclude slice
    # coords).
    dimensioned_iter_coords_by_cube = []
    for requested_dims, cube in itertools.izip(requested_dims_by_cube, cubes):
        dimensioned_iter_coords = set()
        # Loop over dimensioned coords in each cube.
        for dim in xrange(len(cube.shape)):
            if dim not in requested_dims:
                dimensioned_iter_coords.update(
                    cube.coords(contains_dimension=dim))
        dimensioned_iter_coords_by_cube.append(dimensioned_iter_coords)

    # Check for multidimensional coords - current implementation cannot
    # iterate over multidimensional coords.
    # for dimensioned_iter_coords in dimensioned_iter_coords_by_cube:
    #    for coord in dimensioned_iter_coords:
    #        if coord.ndim > 1:
    #            raise iris.exceptions.CoordinateMultiDimError(coord)

    # Iterate through all the possible pairs of cubes to compare dimensioned
    # coordinates.
    pairs_iter = itertools.combinations(dimensioned_iter_coords_by_cube, 2)
    for dimensioned_iter_coords_a, dimensioned_iter_coords_b in pairs_iter:
        coords_by_def_a = set(_CoordWrapper(coord) for coord in
                              dimensioned_iter_coords_a)
        coords_by_def_b = set(_CoordWrapper(coord) for coord in
                              dimensioned_iter_coords_b)

        # Check that the dimensioned coords that are common across the cubes
        # (i.e. have same definition/metadata) have the same shape. If this is
        # not the case it makes no sense to iterate through the coordinate in
        # step and an exception is raised.
        common = coords_by_def_a & coords_by_def_b
        for definition_coord in common:
            # Extract matching coord from dimensioned_iter_coords_a and
            # dimensioned_iter_coords_b to access shape.
            coord_a = (coord for coord in dimensioned_iter_coords_a if
                       definition_coord == coord).next()
            coord_b = (coord for coord in dimensioned_iter_coords_b if
                       definition_coord == coord).next()
            if coord_a.shape != coord_b.shape:
                raise ValueError("Shape of common dimensioned coordinate '%s' "
                                 "does not match across all cubes. Unable "
                                 "to iterate over this coordinate in "
                                 "step." % coord_a.name())
            if coord_a != coord_b:
                warnings.warn("Iterating over coordinate '%s' in step whose "
                              "definitions match but whose values "
                              "differ." % coord_a.name())

    return _ZipSlicesIterator(cubes, requested_dims_by_cube, ordered,
                              coords_by_cube)


class _ZipSlicesIterator(collections.Iterator):
    """
    Extension to _SlicesIterator (see cube.py) to support iteration over a
    collection of cubes in step.

    """
    def __init__(self, cubes, requested_dims_by_cube, ordered, coords_by_cube):
        self._cubes = cubes
        self._requested_dims_by_cube = requested_dims_by_cube
        self._ordered = ordered
        self._coords_by_cube = coords_by_cube

        # Check that the requested_dims_by_cube and coords_by_cube lists are
        # the same length as cubes so it is feasible that there is a 1-1
        # mapping of values (itertool.izip won't catch this).
        if len(requested_dims_by_cube) != len(cubes):
            raise ValueError('requested_dims_by_cube parameter is not the same'
                             ' length as cubes.')
        if len(coords_by_cube) != len(cubes):
            raise ValueError('coords_by_cube parameter is not the same length '
                             'as cubes.')

        # Create an all encompassing dims_index called master_dims_index that
        # is iterated over (using np.ndindex) and from which the indices of the
        # subcubes can be extracted using offsets i.e. position of the
        # associated coord in the master_dims_index.
        master_dimensioned_coord_list = []
        master_dims_index = []
        self._offsets_by_cube = []
        for requested_dims, cube in itertools.izip(requested_dims_by_cube,
                                                   cubes):
            # Create a list of the shape of each cube, and set the dimensions
            # which have been requested to length 1.
            dims_index = list(cube.shape)
            for dim in requested_dims:
                dims_index[dim] = 1
            offsets = []
            # Loop over dimensions in each cube.
            for i in xrange(len(cube.shape)):
                # Obtain the coordinates for this dimension.
                cube_coords = cube.coords(dimensions=i)
                found = False
                # Loop over coords in this dimension (could be just one).
                for coord in cube_coords:
                    # Search for coord in master_dimensioned_coord_list.
                    for j, master_coords in enumerate(
                            master_dimensioned_coord_list):
                        # Use coord wrapper with desired equality
                        # functionality.
                        if _CoordWrapper(coord) in master_coords:
                            offsets.append(j)
                            found = True
                            break
                    if found:
                        break
                # If a coordinate with an equivalent definition (i.e. same
                # metadata) is not found in the master_dimensioned_coord_list,
                # add the coords assocaited with the dimension to the list,
                # add the size of the dimension to the master_dims_index and
                # store the offset.
                if not found:
                    master_dimensioned_coord_list.append(
                        set((_CoordWrapper(coord) for coord in cube_coords)))
                    master_dims_index.append(dims_index[i])
                    offsets.append(len(master_dims_index)-1)
            # Store the offsets for each cube so they can be used in
            # _ZipSlicesIterator.next().
            self._offsets_by_cube.append(offsets)

        # Let Numpy do some work in providing all of the permutations of our
        # data shape based on the combination of dimension sizes called
        # master_dims_index. This functionality is something like:
        # ndindex(2, 1, 3) -> [(0, 0, 0), (0, 0, 1), (0, 0, 2), (1, 0, 0),
        # (1, 0, 1), (1, 0, 2)]
        self._ndindex = np.ndindex(*master_dims_index)

    def next(self):
        # When self._ndindex runs out it will raise StopIteration for us.
        master_index_tuple = self._ndindex.next()

        subcubes = []
        for offsets, requested_dims, coords, cube in itertools.izip(
                self._offsets_by_cube, self._requested_dims_by_cube,
                self._coords_by_cube, self._cubes):
            # Extract the index_list for each cube from the master index using
            # the offsets and for each of the spanning dimensions requested,
            # replace the index_list value (will be a zero from np.ndindex())
            # with a spanning slice.
            index_list = [master_index_tuple[x] for x in offsets]
            for dim in requested_dims:
                index_list[dim] = slice(None, None)
            # Extract slices from the cube
            subcube = cube[tuple(index_list)]
            # Call transpose if necessary (taken from _SlicesIterator in
            # cube.py).
            if self._ordered is True:
                transpose_order = []
                for coord in coords:
                    transpose_order += sorted(subcube.coord_dims(coord))
                if transpose_order != range(subcube.ndim):
                    subcube.transpose(transpose_order)
            subcubes.append(subcube)

        return tuple(subcubes)


class _CoordWrapper:
    """
    Class for creating a coordinate wrapper that allows the use of an
    alternative equality function based on metadata rather than
    metadata + points/bounds.

    .. note::

        Uses a lightweight/incomplete implementation of the Decorator
        pattern.

    """
    def __init__(self, coord):
        self._coord = coord

    # Methods of contained class we need to expose/use.
    def _as_defn(self):
        return self._coord._as_defn()

    # Methods of contained class we want to overide/customise.
    def __eq__(self, other):
        return self._coord._as_defn() == other._as_defn()

    # Force use of __eq__ for set operations.
    def __hash__(self):
        return 1

########NEW FILE########
__FILENAME__ = palette
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Load, configure and register color map palettes and initialise
color map meta-data mappings.

"""

from __future__ import with_statement

from functools import wraps
import os
import os.path
import re

import matplotlib.cm as mpl_cm
import matplotlib.colors as mpl_colors
import numpy as np

import iris.cube
import iris.config
import iris.unit


# Symmetric normalization function pivot points by SI unit.
PIVOT_BY_UNIT = {iris.unit.Unit('K'): 273.15}

# Color map names by palette file metadata field value.
CMAP_BREWER = set()
_CMAP_BY_SCHEME = None
_CMAP_BY_KEYWORD = None
_CMAP_BY_STD_NAME = None

_MISSING_KWARG_CMAP = 'missing kwarg cmap'
_MISSING_KWARG_NORM = 'missing kwarg norm'


def is_brewer(cmap):
    """
    Determine whether the color map is a Cynthia Brewer color map.

    Args:

    * cmap:
        The color map instance.

    Returns:
        Boolean.

    """
    result = False
    if cmap is not None:
        result = cmap.name in CMAP_BREWER
    return result


def _default_cmap_norm(args, kwargs):
    """
    This function injects default cmap and norm behavour into the keyword
    arguments, based on the cube referenced within the positional arguments.
    """
    cube = None

    # Find the single cube reference within the positional arguments.
    for arg in args:
        if isinstance(arg, iris.cube.Cube):
            cube = arg
            break

    # Find the keyword arguments of interest.
    colors = kwargs.get('colors', None)
    # cmap = None to disable default behaviour.
    cmap = kwargs.get('cmap', _MISSING_KWARG_CMAP)
    # norm = None to disable default behaviour.
    norm = kwargs.get('norm', _MISSING_KWARG_NORM)

    # Note that "colors" and "cmap" keywords are mutually exclusive.
    if colors is None and cube is not None:
        std_name = cube.standard_name.lower() if cube.standard_name else ""

        # Perform default "cmap" keyword behaviour.
        if cmap == _MISSING_KWARG_CMAP:
            # Check for an exact match against standard name.
            cmaps = _CMAP_BY_STD_NAME.get(std_name, set())

            if len(cmaps) == 0:
                # Check for a fuzzy match against a keyword.
                for keyword in _CMAP_BY_KEYWORD.iterkeys():
                    if keyword in std_name:
                        cmaps.update(_CMAP_BY_KEYWORD[keyword])

            # Add default color map to keyword arguments.
            if len(cmaps):
                cmap = sorted(cmaps, reverse=True)[0]
                kwargs['cmap'] = mpl_cm.get_cmap(cmap)

        # Perform default "norm" keyword behaviour.
        if norm == _MISSING_KWARG_NORM:
            if 'anomaly' in std_name:
                # Determine the pivot point.
                pivot = PIVOT_BY_UNIT.get(cube.units, 0)
                norm = SymmetricNormalize(pivot)
                kwargs['norm'] = norm

    return args, kwargs


def cmap_norm(cube):
    """
    Determine the default :class:`matplotlib.colors.LinearSegmentedColormap`
    and :class:`iris.palette.SymmetricNormalize` instances associated with
    the cube.

    Args:

    * cube (:class:`iris.cube.Cube`):
        Source cube to generate default palette from.

    Returns:
        Tuple of :class:`matplotlib.colors.LinearSegmentedColormap` and
        :class:`iris.palette.SymmetricNormalize`

    """
    args, kwargs = _default_cmap_norm((cube,), {})
    return kwargs.get('cmap'), kwargs.get('norm')


def auto_palette(func):
    """
    Decorator wrapper function to control the default behaviour of the
    matplotlib cmap and norm keyword arguments.

    Args:

    * func (callable):
        Callable function to be wrapped by the decorator.

    Returns:
        Closure wrapper function.

    """
    @wraps(func)
    def wrapper_func(*args, **kwargs):
        """
        Closure wrapper function to provide default keyword argument
        behaviour.

        """
        # Update the keyword arguments with defaults.
        args, kwargs = _default_cmap_norm(args, kwargs)
        # Call the wrapped function and return its result.
        return func(*args, **kwargs)

    # Return the closure wrapper function.
    return wrapper_func


class SymmetricNormalize(mpl_colors.Normalize, object):
    """
    Provides a symmetric normalization class around a given pivot point.
    """

    def __init__(self, pivot, *args, **kwargs):
        self.pivot = pivot
        self._vmin = None
        self._vmax = None
        mpl_colors.Normalize.__init__(self, *args, **kwargs)

    def __repr__(self):
        return '%s(%r)' % (self.__class__.__name__, self.pivot)

    def _update(self, val, update_min=True, update_max=True):
        # Update both _vmin and _vmax from given value.
        val_diff = np.abs(val - self.pivot)
        vmin_diff = np.abs(self._vmin - self.pivot) if self._vmin else 0.0
        vmax_diff = np.abs(self._vmax - self.pivot) if self._vmax else 0.0
        diff = max(val_diff, vmin_diff, vmax_diff)

        if update_min:
            self._vmin = self.pivot - diff
        if update_max:
            self._vmax = self.pivot + diff

    def _get_vmin(self):
        return getattr(self, '_vmin')

    def _set_vmin(self, val):
        if val is None:
            self._vmin = None
        elif self._vmax is None:
            # Don't set _vmax, it'll stop matplotlib from giving us one.
            self._update(val, update_max=False)
        else:
            # Set both _vmin and _vmax from value
            self._update(val)

    vmin = property(_get_vmin, _set_vmin)

    def _get_vmax(self):
        return getattr(self, '_vmax')

    def _set_vmax(self, val):
        if val is None:
            self._vmax = None
        elif self._vmin is None:
            # Don't set _vmin, it'll stop matplotlib from giving us one.
            self._update(val, update_min=False)
        else:
            # Set both _vmin and _vmax from value
            self._update(val)

    vmax = property(_get_vmax, _set_vmax)


def _load_palette():
    """
    Load, configure and register color map palettes and initialise
    color map metadata mappings.

    """
    # Reference these module level namespace variables.
    global CMAP_BREWER, _CMAP_BY_SCHEME, _CMAP_BY_KEYWORD, _CMAP_BY_STD_NAME

    _CMAP_BY_SCHEME = {}
    _CMAP_BY_KEYWORD = {}
    _CMAP_BY_STD_NAME = {}

    filenames = []

    # Identify all .txt color map palette files.
    for root, dirs, files in os.walk(iris.config.PALETTE_PATH):
        # Prune any .svn directory from the tree walk.
        if '.svn' in dirs:
            del dirs[dirs.index('.svn')]
        # Identify any target .txt color map palette files.
        filenames.extend([os.path.join(root, filename)
                          for filename in files
                          if os.path.splitext(filename)[1] == '.txt'])

    for filename in filenames:
        # Default color map name based on the file base-name (case-SENSITIVE).
        cmap_name = os.path.splitext(os.path.basename(filename))[0]
        cmap_scheme = None
        cmap_keywords = []
        cmap_std_names = []
        cmap_type = None

        # Perform default color map interpolation for quantization
        # levels per primary color.
        interpolate_flag = True

        # Read the file header.
        with open(filename) as file_handle:
            header = filter(lambda line: re.match('^\s*#.*:\s+.*$', line),
                            file_handle.readlines())

        # Extract the file header metadata.
        for line in header:
            line = line.replace('#', '', 1).split(':')
            head = line[0].strip().lower()
            body = line[1].strip()

            if head == 'name':
                # Case-SENSITIVE.
                cmap_name = 'brewer_{}'.format(body)

            if head == 'scheme':
                # Case-insensitive.
                cmap_scheme = body.lower()

            if head == 'keyword':
                # Case-insensitive.
                keywords = [part.strip().lower() for part in body.split(',')]
                cmap_keywords.extend(keywords)

            if head == 'std_name':
                # Case-insensitive.
                std_names = [part.strip().lower() for part in body.split(',')]
                cmap_std_names.extend(std_names)

            if head == 'interpolate':
                # Case-insensitive.
                interpolate_flag = body.lower() != 'off'

            if head == 'type':
                # Case-insensitive.
                cmap_type = body.lower()

        # Integrity check for meta-data 'type' field.
        assert cmap_type is not None, \
            'Missing meta-data "type" keyword for color map file, "%s"' % \
            filename
        assert cmap_type == 'rgb', \
            'Invalid type [%s] for color map file "%s"' % (cmap_type, filename)

        # Update the color map look-up dictionaries.
        CMAP_BREWER.add(cmap_name)

        if cmap_scheme is not None:
            scheme_group = _CMAP_BY_SCHEME.setdefault(cmap_scheme, set())
            scheme_group.add(cmap_name)

        for keyword in cmap_keywords:
            keyword_group = _CMAP_BY_KEYWORD.setdefault(keyword, set())
            keyword_group.add(cmap_name)

        for std_name in cmap_std_names:
            std_name_group = _CMAP_BY_STD_NAME.setdefault(std_name, set())
            std_name_group.add(cmap_name)

        # Load palette data and create the associated color map.
        cmap_data = np.loadtxt(filename)
        # Ensure to restrict the number of RGB quantization levels to
        # prevent color map interpolation.

        if interpolate_flag:
            # Perform default color map interpolation for quantization
            # levels per primary color.
            cmap = mpl_colors.LinearSegmentedColormap.from_list(
                cmap_name, cmap_data)
        else:
            # Restrict quantization levels per primary color (turn-off
            # interpolation).
            # Typically used for Brewer color maps.
            cmap = mpl_colors.LinearSegmentedColormap.from_list(
                cmap_name, cmap_data, N=len(cmap_data))

        # Register the color map for use.
        mpl_cm.register_cmap(cmap=cmap)


# Ensure to load the color map palettes.
_load_palette()

########NEW FILE########
__FILENAME__ = pandas
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Provide conversion to and from Pandas data structures.

See also: http://pandas.pydata.org/

"""
from __future__ import absolute_import

import datetime

import netcdftime
import numpy as np
import pandas

import iris
from iris.coords import AuxCoord, DimCoord
from iris.cube import Cube
from iris.unit import Unit


def _add_iris_coord(cube, name, points, dim, calendar=None):
    """
    Add a Coord to a Cube from a Pandas index or columns array.

    If no calendar is specified for a time series, Gregorian is assumed.

    """
    units = Unit("unknown")
    if calendar is None:
        calendar = iris.unit.CALENDAR_GREGORIAN

    # Convert pandas datetime objects to python datetime obejcts.
    if isinstance(points, pandas.tseries.index.DatetimeIndex):
        points = np.array([i.to_datetime() for i in points])

    # Convert datetime objects to Iris' current datetime representation.
    if points.dtype == object:
        dt_types = (datetime.datetime, netcdftime.datetime)
        if all([isinstance(i, dt_types) for i in points]):
            units = Unit("hours since epoch", calendar=calendar)
            points = units.date2num(points)

    points = np.array(points)
    if (np.issubdtype(points.dtype, np.number) and
            iris.util.monotonic(points, strict=True)):
                coord = DimCoord(points, units=units)
                coord.rename(name)
                cube.add_dim_coord(coord, dim)
    else:
        coord = AuxCoord(points, units=units)
        coord.rename(name)
        cube.add_aux_coord(coord, dim)


def as_cube(pandas_array, copy=True, calendars=None):
    """
    Convert a Pandas array into an Iris cube.

    Args:

        * pandas_array - A Pandas Series or DataFrame.

    Kwargs:

        * copy      - Whether to make a copy of the data.
                      Defaults to True.

        * calendars - A dict mapping a dimension to a calendar.
                      Required to convert datetime indices/columns.

    Example usage::

        as_cube(series, calendars={0: iris.unit.CALENDAR_360_DAY})
        as_cube(data_frame, calendars={1: iris.unit.CALENDAR_GREGORIAN})

    .. note:: This function will copy your data by default.

    """
    calendars = calendars or {}
    if pandas_array.ndim not in [1, 2]:
        raise ValueError("Only 1D or 2D Pandas arrays "
                         "can currently be conveted to Iris cubes.")

    # Make the copy work consistently across NumPy 1.6 and 1.7.
    # (When 1.7 takes a copy it preserves the C/Fortran ordering, but
    # 1.6 doesn't. Since we don't care about preserving the order we can
    # just force it back to C-order.)
    order = 'C' if copy else 'A'
    data = np.array(pandas_array, copy=copy, order=order)
    cube = Cube(np.ma.masked_invalid(data, copy=False))
    _add_iris_coord(cube, "index", pandas_array.index, 0,
                    calendars.get(0, None))
    if pandas_array.ndim == 2:
        _add_iris_coord(cube, "columns", pandas_array.columns, 1,
                        calendars.get(1, None))
    return cube


def _as_pandas_coord(coord):
    """Convert an Iris Coord into a Pandas index or columns array."""
    index = coord.points
    if coord.units.is_time_reference():
        index = coord.units.num2date(index)
    return index


def _assert_shared(np_obj, pandas_obj):
    """Ensure the pandas object shares memory."""
    if hasattr(pandas_obj, 'base'):
        base = pandas_obj.base
    else:
        base = pandas_obj[0].base
    # Chase the stack of NumPy `base` references back to see if any of
    # them are our original array.
    while base is not None:
        if base is np_obj:
            return
        # Take the next step up the stack of `base` references.
        base = base.base
    msg = 'Pandas {} does not share memory'.format(type(pandas_obj).__name__)
    raise AssertionError(msg)


def as_series(cube, copy=True):
    """
    Convert a 1D cube to a Pandas Series.

    Args:

        * cube - The cube to convert to a Pandas Series.

    Kwargs:

        * copy - Whether to make a copy of the data.
                 Defaults to True. Must be True for masked data.

    .. note::

        This function will copy your data by default.
        If you have a large array that cannot be copied,
        make sure it is not masked and use copy=False.

    """
    data = cube.data
    if isinstance(data, np.ma.MaskedArray):
        if not copy:
            raise ValueError("Masked arrays must always be copied.")
        data = data.astype('f').filled(np.nan)
    elif copy:
        data = data.copy()

    index = None
    if cube.dim_coords:
        index = _as_pandas_coord(cube.dim_coords[0])

    series = pandas.Series(data, index)
    if not copy:
        _assert_shared(data, series)

    return series


def as_data_frame(cube, copy=True):
    """
    Convert a 2D cube to a Pandas DataFrame.

    Args:

        * cube - The cube to convert to a Pandas DataFrame.

    Kwargs:

        * copy - Whether to make a copy of the data.
                 Defaults to True. Must be True for masked data
                 and some data types (see notes below).

    .. note::

        This function will copy your data by default.
        If you have a large array that cannot be copied,
        make sure it is not masked and use copy=False.

    .. note::

        Pandas will sometimes make a copy of the array,
        for example when creating from an int32 array.
        Iris will detect this and raise an exception if copy=False.

    """
    data = cube.data
    if isinstance(data, np.ma.MaskedArray):
        if not copy:
            raise ValueError("Masked arrays must always be copied.")
        data = data.astype('f').filled(np.nan)
    elif copy:
        data = data.copy()

    index = columns = None
    if cube.coords(dimensions=[0]):
        index = _as_pandas_coord(cube.coord(dimensions=[0]))
    if cube.coords(dimensions=[1]):
        columns = _as_pandas_coord(cube.coord(dimensions=[1]))

    data_frame = pandas.DataFrame(data, index, columns)
    if not copy:
        _assert_shared(data, data_frame)

    return data_frame

########NEW FILE########
__FILENAME__ = plot
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Iris-specific extensions to matplotlib, mimicking the :mod:`matplotlib.pyplot`
interface.

See also: :ref:`matplotlib <matplotlib:users-guide-index>`.

"""


import collections
import datetime
import functools
import warnings

import matplotlib.axes
import matplotlib.collections as mpl_collections
import matplotlib.dates as mpl_dates
import matplotlib.transforms as mpl_transforms
import matplotlib.pyplot as plt
import matplotlib.ticker as mpl_ticker
from mpl_toolkits.axes_grid.anchored_artists import AnchoredText
import numpy as np
import numpy.ma as ma
import cartopy.crs
import cartopy.mpl.geoaxes


import iris.cube
import iris.coord_systems
import iris.analysis.cartography as cartography
import iris.coords
import iris.palette
import iris.unit


# Cynthia Brewer citation text.
BREWER_CITE = 'Colours based on ColorBrewer.org'


PlotDefn = collections.namedtuple('PlotDefn', ('coords', 'transpose'))


def _get_plot_defn_custom_coords_picked(cube, coords, mode, ndims=2):
    def as_coord(coord):
        coord = cube.coord(coord)
        return coord
    coords = map(as_coord, coords)

    # Check that we were given the right number of coordinates
    if len(coords) != ndims:
        coord_names = ', '.join([coord.name() for coord in coords])
        raise ValueError('The list of coordinates given (%s) should have the'
                         ' same length (%s) as the dimensionality of the'
                         ' required plot (%s)' % (coord_names,
                                                  len(coords), ndims))

    # Check which dimensions are spanned by each coordinate.
    get_span = lambda coord: set(cube.coord_dims(coord))
    spans = map(get_span, coords)
    for span, coord in zip(spans, coords):
        if not span:
            msg = 'The coordinate {!r} doesn\'t span a data dimension.'
            raise ValueError(msg.format(coord.name()))
        if mode == iris.coords.BOUND_MODE and len(span) != 1:
            raise ValueError('The coordinate {!r} is multi-dimensional and'
                             ' cannot be used in a cell-based plot.'
                             .format(coord.name()))

    # Check the combination of coordinates spans enough (ndims) data
    # dimensions.
    total_span = set().union(*spans)
    if len(total_span) != ndims:
        coord_names = ', '.join([coord.name() for coord in coords])
        raise ValueError('The given coordinates ({}) don\'t span the {} data'
                         ' dimensions.'.format(coord_names, ndims))

    # If we have 2-dimensional data, and one or more 1-dimensional
    # coordinates, check if we need to transpose.
    transpose = False
    if ndims == 2 and min(map(len, spans)) == 1:
        for i, span in enumerate(spans):
            if len(span) == 1:
                if list(span)[0] == i:
                    transpose = True
                    break

    # Note the use of `reversed` to convert from the X-then-Y
    # convention of the end-user API to the V-then-U convention used by
    # the plotting routines.
    plot_coords = list(reversed(coords))
    return PlotDefn(plot_coords, transpose)


def _valid_bound_coord(coord):
    result = None
    if coord and coord.ndim == 1 and coord.nbounds:
        result = coord
    return result


def _get_plot_defn(cube, mode, ndims=2):
    """
    Return data and plot-axis coords given a cube & a mode of either
    POINT_MODE or BOUND_MODE.

    """
    if cube.ndim != ndims:
        msg = 'Cube must be %s-dimensional. Got %s dimensions.'
        raise ValueError(msg % (ndims, cube.ndim))

    # Start by taking the DimCoords from each dimension.
    coords = [None] * ndims
    for dim_coord in cube.dim_coords:
        dim = cube.coord_dims(dim_coord)[0]
        coords[dim] = dim_coord

    # When appropriate, restrict to 1D with bounds.
    if mode == iris.coords.BOUND_MODE:
        coords = map(_valid_bound_coord, coords)

    def guess_axis(coord):
        axis = None
        if coord is not None:
            axis = iris.util.guess_coord_axis(coord)
        return axis

    # Allow DimCoords in aux_coords to fill in for missing dim_coords.
    for dim, coord in enumerate(coords):
        if coord is None:
            aux_coords = cube.coords(dimensions=dim)
            aux_coords = filter(lambda coord:
                                isinstance(coord, iris.coords.DimCoord),
                                aux_coords)
            if aux_coords:
                key_func = lambda coord: coord._as_defn()
                aux_coords.sort(key=key_func)
                coords[dim] = aux_coords[0]

    if mode == iris.coords.POINT_MODE:
        # Allow multi-dimensional aux_coords to override the dim_coords
        # along the Z axis. This results in a preference for using the
        # derived altitude over model_level_number or level_height.
        # Limit to Z axis to avoid preferring latitude over grid_latitude etc.
        axes = map(guess_axis, coords)
        axis = 'Z'
        if axis in axes:
            for coord in cube.coords(dim_coords=False):
                if max(coord.shape) > 1 and \
                        iris.util.guess_coord_axis(coord) == axis:
                    coords[axes.index(axis)] = coord

    # Re-order the coordinates to achieve the preferred
    # horizontal/vertical associations.
    def sort_key(coord):
        order = {'X': 2, 'T': 1, 'Y': -1, 'Z': -2}
        axis = guess_axis(coord)
        return (order.get(axis, 0), coord and coord.name())
    sorted_coords = sorted(coords, key=sort_key)

    transpose = (sorted_coords != coords)
    return PlotDefn(sorted_coords, transpose)


def _can_draw_map(plot_coords):
    std_names = [coord and coord.standard_name for coord in plot_coords]
    valid_std_names = [
        ['latitude', 'longitude'],
        ['grid_latitude', 'grid_longitude'],
        ['projection_y_coordinate', 'projection_x_coordinate']
    ]
    return std_names in valid_std_names


def _broadcast_2d(u, v):
    # Matplotlib needs the U and V coordinates to have the same
    # dimensionality (either both 1D, or both 2D). So we simply
    # broadcast both to 2D to be on the safe side.
    u = np.atleast_2d(u)
    v = np.atleast_2d(v.T).T
    u, v = np.broadcast_arrays(u, v)
    return u, v


def _string_coord_axis_tick_labels(string_axes):
    """Apply tick labels for string coordinates."""

    ax = plt.gca()
    for axis, ticks in string_axes.items():
        formatter = mpl_ticker.IndexFormatter(ticks)
        locator = mpl_ticker.MaxNLocator(integer=True)
        this_axis = getattr(ax, axis)
        this_axis.set_major_formatter(formatter)
        this_axis.set_major_locator(locator)


def _invert_yaxis(v_coord):
    """
    Inverts the y-axis of the current plot based on conditions:

        * If the y-axis is already inverted we don't want to re-invert it.
        * If v_coord is None then it will not have any attributes.
        * If neither of the above are true then invert y if v_coord has
          attribute 'positive' set to 'down'.

    Args:

        * v_coord - the coord to be plotted on the y-axis

    """
    yaxis_is_inverted = plt.gca().yaxis_inverted()
    if not yaxis_is_inverted and v_coord is not None:
        attr_pve = v_coord.attributes.get('positive')
        if attr_pve is not None and attr_pve.lower() == 'down':
            plt.gca().invert_yaxis()


def _draw_2d_from_bounds(draw_method_name, cube, *args, **kwargs):
    # NB. In the interests of clarity we use "u" and "v" to refer to the
    # horizontal and vertical axes on the matplotlib plot.
    mode = iris.coords.BOUND_MODE
    # Get & remove the coords entry from kwargs.
    coords = kwargs.pop('coords', None)
    if coords is not None:
        plot_defn = _get_plot_defn_custom_coords_picked(cube, coords, mode)
    else:
        plot_defn = _get_plot_defn(cube, mode, ndims=2)

    if _can_draw_map(plot_defn.coords):
        result = _map_common(draw_method_name, None, iris.coords.BOUND_MODE,
                             cube, plot_defn, *args, **kwargs)
    else:
        # Obtain data array.
        data = cube.data
        if plot_defn.transpose:
            data = data.T

        # Obtain U and V coordinates
        v_coord, u_coord = plot_defn.coords

        # Track numpy arrays to use for the actual plotting.
        plot_arrays = []

        # Map axis name to associated values.
        string_axes = {}

        for coord, axis_name, data_dim in zip([u_coord, v_coord],
                                              ['xaxis', 'yaxis'],
                                              [1, 0]):
            if coord:
                if coord.points.dtype.char == 'S':
                    if coord.points.ndim != 1:
                        msg = 'Coord {!r} must be one-dimensional.'
                        raise ValueError(msg.format(coord))
                    if coord.bounds is not None:
                        msg = 'Cannot plot bounded string coordinate.'
                        raise ValueError(msg)
                    string_axes[axis_name] = coord.points
                    values = np.arange(data.shape[data_dim] + 1) - 0.5
                else:
                    values = coord.contiguous_bounds()
            else:
                values = np.arange(data.shape[data_dim] + 1)

            plot_arrays.append(values)

        u, v = plot_arrays
        u, v = _broadcast_2d(u, v)
        draw_method = getattr(plt, draw_method_name)
        result = draw_method(u, v, data, *args, **kwargs)

        # Apply tick labels for string coordinates.
        _string_coord_axis_tick_labels(string_axes)

        # Invert y-axis if necessary.
        _invert_yaxis(v_coord)

    return result


def _draw_2d_from_points(draw_method_name, arg_func, cube, *args, **kwargs):
    # NB. In the interests of clarity we use "u" and "v" to refer to the
    # horizontal and vertical axes on the matplotlib plot.
    mode = iris.coords.POINT_MODE
    # Get & remove the coords entry from kwargs.
    coords = kwargs.pop('coords', None)
    if coords is not None:
        plot_defn = _get_plot_defn_custom_coords_picked(cube, coords, mode)
    else:
        plot_defn = _get_plot_defn(cube, mode, ndims=2)

    if _can_draw_map(plot_defn.coords):
        result = _map_common(draw_method_name, arg_func,
                             iris.coords.POINT_MODE, cube, plot_defn,
                             *args, **kwargs)
    else:
        # Obtain data array.
        data = cube.data
        if plot_defn.transpose:
            data = data.T

        # Obtain U and V coordinates
        v_coord, u_coord = plot_defn.coords
        if u_coord:
            u = u_coord.points
            u = _fixup_dates(u_coord, u)
        else:
            u = np.arange(data.shape[1])
        if v_coord:
            v = v_coord.points
            v = _fixup_dates(v_coord, v)
        else:
            v = np.arange(data.shape[0])

        if plot_defn.transpose:
            u = u.T
            v = v.T

        # Track numpy arrays to use for the actual plotting.
        plot_arrays = []

        # Map axis name to associated values.
        string_axes = {}

        for values, axis_name in zip([u, v], ['xaxis', 'yaxis']):
            # Replace any string coordinates with "index" coordinates.
            if values.dtype.char == 'S':
                if values.ndim != 1:
                    raise ValueError('Multi-dimensional string coordinates '
                                     'not supported.')
                plot_arrays.append(np.arange(values.size))
                string_axes[axis_name] = values
            elif (values.dtype == np.dtype(object) and
                  isinstance(values[0], datetime.datetime)):
                plot_arrays.append(mpl_dates.date2num(values))
            else:
                plot_arrays.append(values)

        u, v = plot_arrays
        u, v = _broadcast_2d(u, v)

        draw_method = getattr(plt, draw_method_name)
        if arg_func is not None:
            args, kwargs = arg_func(u, v, data, *args, **kwargs)
            result = draw_method(*args, **kwargs)
        else:
            result = draw_method(u, v, data, *args, **kwargs)

        # Apply tick labels for string coordinates.
        _string_coord_axis_tick_labels(string_axes)

        # Invert y-axis if necessary.
        _invert_yaxis(v_coord)

    return result


def _fixup_dates(coord, values):
    if coord.units.calendar is not None and values.ndim == 1:
        r = [datetime.datetime(*(coord.units.num2date(val).timetuple()[0:6]))
             for val in values]
        values = np.empty(len(r), dtype=object)
        values[:] = r
    return values


def _data_from_coord_or_cube(c):
    if isinstance(c, iris.cube.Cube):
        data = c.data
    elif isinstance(c, iris.coords.Coord):
        data = _fixup_dates(c, c.points)
    else:
        raise TypeError('Plot arguments must be cubes or coordinates.')
    return data


def _uv_from_u_object_v_object(u_object, v_object):
    ndim_msg = 'Cube or coordinate must be 1-dimensional. Got {} dimensions.'
    if u_object is not None and u_object.ndim > 1:
        raise ValueError(ndim_msg.format(u_object.ndim))
    if v_object.ndim > 1:
        raise ValueError(ndim_msg.format(v_object.ndim))
    v = _data_from_coord_or_cube(v_object)
    if u_object is None:
        u = np.arange(v.shape[0])
    else:
        u = _data_from_coord_or_cube(u_object)
    return u, v


def _u_object_from_v_object(v_object):
    u_object = None
    if isinstance(v_object, iris.cube.Cube):
        plot_defn = _get_plot_defn(v_object, iris.coords.POINT_MODE, ndims=1)
        u_object, = plot_defn.coords
    return u_object


def _get_plot_objects(args):
    if len(args) > 1 and isinstance(args[1],
                                    (iris.cube.Cube, iris.coords.Coord)):
        # two arguments
        u_object, v_object = args[:2]
        u, v = _uv_from_u_object_v_object(*args[:2])
        args = args[2:]
        if len(u) != len(v):
            msg = "The x and y-axis objects are not compatible. They should " \
                  "have equal sizes but got ({}: {}) and ({}: {})."
            raise ValueError(msg.format(u_object.name(), len(u),
                                        v_object.name(), len(v)))
    else:
        # single argument
        v_object = args[0]
        u_object = _u_object_from_v_object(v_object)
        u, v = _uv_from_u_object_v_object(u_object, args[0])
        args = args[1:]
    return u_object, v_object, u, v, args


def _draw_1d_from_points(draw_method_name, arg_func, *args, **kwargs):
    # NB. In the interests of clarity we use "u" to refer to the horizontal
    # axes on the matplotlib plot and "v" for the vertical axes.

    # retrieve the objects that are plotted on the horizontal and vertical
    # axes (cubes or coordinates) and their respective values, along with the
    # argument tuple with these objects removed
    u_object, v_object, u, v, args = _get_plot_objects(args)

    # Track numpy arrays to use for the actual plotting.
    plot_arrays = []

    # Map axis name to associated values.
    string_axes = {}

    for values, axis_name in zip([u, v], ['xaxis', 'yaxis']):
        # Replace any string coordinates with "index" coordinates.
        if values.dtype.char == 'S':
            if values.ndim != 1:
                msg = 'Multi-dimensional string coordinates are not supported.'
                raise ValueError(msg)
            plot_arrays.append(np.arange(values.size))
            string_axes[axis_name] = values
        else:
            plot_arrays.append(values)

    u, v = plot_arrays

    # if both u_object and v_object are coordinates then check if a map
    # should be drawn
    if isinstance(u_object, iris.coords.Coord) and \
            isinstance(v_object, iris.coords.Coord) and \
            _can_draw_map([v_object, u_object]):
        # Replace non-cartopy subplot/axes with a cartopy alternative and set
        # the transform keyword.
        kwargs = _ensure_cartopy_axes_and_determine_kwargs(u_object, v_object,
                                                           kwargs)

    draw_method = getattr(plt, draw_method_name)
    if arg_func is not None:
        args, kwargs = arg_func(u, v, *args, **kwargs)
        result = draw_method(*args, **kwargs)
    else:
        result = draw_method(u, v, *args, **kwargs)

    # Apply tick labels for string coordinates.
    _string_coord_axis_tick_labels(string_axes)

    # Invert y-axis if necessary.
    _invert_yaxis(v_object)

    return result


def _replace_axes_with_cartopy_axes(cartopy_proj):
    """
    Replace non-cartopy subplot/axes with a cartopy alternative
    based on the provided projection. If the current axes are already an
    instance of :class:`cartopy.mpl.geoaxes.GeoAxes` then no action is taken.

    """

    ax = plt.gca()
    if not isinstance(ax,
                      cartopy.mpl.geoaxes.GeoAxes):
        fig = plt.gcf()
        if isinstance(ax, matplotlib.axes.SubplotBase):
            new_ax = fig.add_subplot(ax.get_subplotspec(),
                                     projection=cartopy_proj,
                                     title=ax.get_title(),
                                     xlabel=ax.get_xlabel(),
                                     ylabel=ax.get_ylabel())
        else:
            new_ax = fig.add_axes(projection=cartopy_proj,
                                  title=ax.get_title(),
                                  xlabel=ax.get_xlabel(),
                                  ylabel=ax.get_ylabel())

        # delete the axes which didn't have a cartopy projection
        fig.delaxes(ax)


def _ensure_cartopy_axes_and_determine_kwargs(x_coord, y_coord, kwargs):
    """
    Replace the current non-cartopy axes with :class:`cartopy.mpl.GeoAxes`
    and return the appropriate kwargs dict based on the provided coordinates
    and kwargs.

    """
    # Determine projection.
    if x_coord.coord_system != y_coord.coord_system:
        raise ValueError('The X and Y coordinates must have equal coordinate'
                         ' systems.')
    cs = x_coord.coord_system
    if cs is not None:
        cartopy_proj = cs.as_cartopy_projection()
    else:
        cartopy_proj = cartopy.crs.PlateCarree()

    # Ensure the current axes are a cartopy.mpl.GeoAxes instance.
    _replace_axes_with_cartopy_axes(cartopy_proj)

    # Set the "from transform" keyword.
    if 'transform' in kwargs:
        raise ValueError("The 'transform' keyword is not allowed as it "
                         "automatically determined from the coordinate "
                         "metadata.")
    new_kwargs = kwargs.copy()
    new_kwargs['transform'] = cartopy_proj

    return new_kwargs


def _map_common(draw_method_name, arg_func, mode, cube, plot_defn,
                *args, **kwargs):
    """
    Draw the given cube on a map using its points or bounds.

    "Mode" parameter will switch functionality between POINT or BOUND plotting.


    """
    # Generate 2d x and 2d y grids.
    y_coord, x_coord = plot_defn.coords
    if mode == iris.coords.POINT_MODE:
        if x_coord.ndim == y_coord.ndim == 1:
            x, y = np.meshgrid(x_coord.points, y_coord.points)
        elif x_coord.ndim == y_coord.ndim == 2:
            x = x_coord.points
            y = y_coord.points
        else:
            raise ValueError("Expected 1D or 2D XY coords")
    else:
        try:
            x, y = np.meshgrid(x_coord.contiguous_bounds(),
                               y_coord.contiguous_bounds())
        # Exception translation.
        except iris.exceptions.CoordinateMultiDimError:
            raise ValueError("Could not get XY grid from bounds. "
                             "X or Y coordinate not 1D.")
        except ValueError:
            raise ValueError("Could not get XY grid from bounds. "
                             "X or Y coordinate doesn't have 2 bounds "
                             "per point.")

    # Obtain the data array.
    data = cube.data
    if plot_defn.transpose:
        data = data.T

    # If we are global, then append the first column of data the array to the
    # last (and add 360 degrees) NOTE: if it is found that this block of code
    # is useful in anywhere other than this plotting routine, it may be better
    # placed in the CS.
    if getattr(x_coord, 'circular', False):
        _, direction = iris.util.monotonic(x_coord.points,
                                           return_direction=True)
        y = np.append(y, y[:, 0:1], axis=1)
        x = np.append(x, x[:, 0:1] + 360 * direction, axis=1)
        data = ma.concatenate([data, data[:, 0:1]], axis=1)

    # Replace non-cartopy subplot/axes with a cartopy alternative and set the
    # transform keyword.
    kwargs = _ensure_cartopy_axes_and_determine_kwargs(x_coord, y_coord,
                                                       kwargs)

    if arg_func is not None:
        new_args, kwargs = arg_func(x, y, data, *args, **kwargs)
    else:
        new_args = (x, y, data) + args

    # Draw the contour lines/filled contours.
    return getattr(plt, draw_method_name)(*new_args, **kwargs)


def contour(cube, *args, **kwargs):
    """
    Draws contour lines based on the given Cube.

    Kwargs:

    * coords: list of :class:`~iris.coords.Coord` objects or coordinate names
        Use the given coordinates as the axes for the plot. The order of the
        given coordinates indicates which axis to use for each, where the first
        element is the horizontal axis of the plot and the second element is
        the vertical axis of the plot.

    See :func:`matplotlib.pyplot.contour` for details of other valid keyword
    arguments.

    """
    result = _draw_2d_from_points('contour', None, cube, *args, **kwargs)
    return result


def contourf(cube, *args, **kwargs):
    """
    Draws filled contours based on the given Cube.

    Kwargs:

    * coords: list of :class:`~iris.coords.Coord` objects or coordinate names
        Use the given coordinates as the axes for the plot. The order of the
        given coordinates indicates which axis to use for each, where the first
        element is the horizontal axis of the plot and the second element is
        the vertical axis of the plot.

    See :func:`matplotlib.pyplot.contourf` for details of other valid keyword
    arguments.

    """
    coords = kwargs.get('coords')
    kwargs.setdefault('antialiased', True)
    result = _draw_2d_from_points('contourf', None, cube, *args, **kwargs)

    # Matplotlib produces visible seams between anti-aliased polygons.
    # But if the polygons are virtually opaque then we can cover the seams
    # by drawing anti-aliased lines *underneath* the polygon joins.

    # Figure out the alpha level for the contour plot
    if result.alpha is None:
        alpha = result.collections[0].get_facecolor()[0][3]
    else:
        alpha = result.alpha
    # If the contours are anti-aliased and mostly opaque then draw lines under
    # the seams.
    if result.antialiased and alpha > 0.95:
        levels = result.levels
        colors = [c[0] for c in result.tcolors]
        if result.extend == 'neither':
            levels = levels[1:-1]
            colors = colors[:-1]
        elif result.extend == 'min':
            levels = levels[:-1]
            colors = colors[:-1]
        elif result.extend == 'max':
            levels = levels[1:]
            colors = colors[:-1]
        else:
            colors = colors[:-1]
        if len(levels) > 0:
            # Draw the lines just *below* the polygons to ensure we minimise
            # any boundary shift.
            zorder = result.collections[0].zorder - .1
            contour(cube, levels=levels, colors=colors, antialiased=True,
                    zorder=zorder, coords=coords)
            # Restore the current "image" to 'result' rather than the mappable
            # resulting from the additional call to contour().
            plt.sci(result)

    return result


def default_projection(cube):
    """
    Return the primary map projection for the given cube.

    Using the returned projection, one can create a cartopy map with::

        import matplotlib.pyplot as plt
        ax = plt.ax(projection=default_projection(cube))

    """
    # XXX logic seems flawed, but it is what map_setup did...
    cs = cube.coord_system("CoordSystem")
    projection = cs.as_cartopy_projection() if cs else None
    return projection


def default_projection_extent(cube, mode=iris.coords.POINT_MODE):
    """
    Return the cube's extents ``(x0, x1, y0, y1)`` in its default projection.

    Keyword arguments:

     * mode - Either ``iris.coords.POINT_MODE`` or ``iris.coords.BOUND_MODE``.
              Triggers whether the extent should be representative of the cell
              points, or the limits of the cell's bounds.
              The default is iris.coords.POINT_MODE.

    """
    extents = cartography._xy_range(cube, mode)
    xlim = extents[0]
    ylim = extents[1]
    return tuple(xlim) + tuple(ylim)


def _fill_orography(cube, coords, mode, vert_plot, horiz_plot, style_args):
    # Find the orography coordinate.
    orography = cube.coord('surface_altitude')

    if coords is not None:
        plot_defn = _get_plot_defn_custom_coords_picked(cube, coords, mode,
                                                        ndims=2)
    else:
        plot_defn = _get_plot_defn(cube, mode, ndims=2)
    v_coord, u_coord = plot_defn.coords

    # Find which plot coordinate corresponds to the derived altitude, so that
    # we can replace altitude with the surface altitude.
    if v_coord and v_coord.standard_name == 'altitude':
        # v is altitude, so plot u and orography with orog in the y direction.
        result = vert_plot(u_coord, orography, style_args)
    elif u_coord and u_coord.standard_name == 'altitude':
        # u is altitude, so plot v and orography with orog in the x direction.
        result = horiz_plot(v_coord, orography, style_args)
    else:
        raise ValueError('Plot does not use hybrid height. One of the '
                         'coordinates to plot must be altitude, but %s and %s '
                         'were given.' % (u_coord.name(), v_coord.name()))
    return result


def orography_at_bounds(cube, facecolor='#888888', coords=None):
    """Plots orography defined at cell boundaries from the given Cube."""

    # XXX Needs contiguous orography corners to work.
    raise NotImplementedError('This operation is temporarily not provided '
                              'until coordinates can expose 2d contiguous '
                              'bounds (corners).')

    style_args = {'edgecolor': 'none', 'facecolor': facecolor}

    def vert_plot(u_coord, orography, style_args):
        u = u_coord.contiguous_bounds()
        left = u[:-1]
        height = orography.points
        width = u[1:] - left
        return plt.bar(left, height, width, **style_args)

    def horiz_plot(v_coord, orography, style_args):
        v = v_coord.contiguous_bounds()
        bottom = v[:-1]
        width = orography.points
        height = v[1:] - bottom
        return plt.barh(bottom, width, height, **style_args)

    return _fill_orography(cube, coords, iris.coords.BOUND_MODE, vert_plot,
                           horiz_plot, style_args)


def orography_at_points(cube, facecolor='#888888', coords=None):
    """Plots orography defined at sample points from the given Cube."""

    style_args = {'facecolor': facecolor}

    def vert_plot(u_coord, orography, style_args):
        x = u_coord.points
        y = orography.points
        return plt.fill_between(x, y, **style_args)

    def horiz_plot(v_coord, orography, style_args):
        y = v_coord.points
        x = orography.points
        return plt.fill_betweenx(y, x, **style_args)

    return _fill_orography(cube, coords, iris.coords.POINT_MODE, vert_plot,
                           horiz_plot, style_args)


def outline(cube, coords=None):
    """
    Draws cell outlines based on the given Cube.

    Kwargs:

    * coords: list of :class:`~iris.coords.Coord` objects or coordinate names
        Use the given coordinates as the axes for the plot. The order of the
        given coordinates indicates which axis to use for each, where the first
        element is the horizontal axis of the plot and the second element is
        the vertical axis of the plot.

    """
    result = _draw_2d_from_bounds('pcolormesh', cube, facecolors='none',
                                  edgecolors='k', antialiased=True,
                                  coords=coords)
    # set the _is_stroked property to get a single color grid.
    # See https://github.com/matplotlib/matplotlib/issues/1302
    result._is_stroked = False
    if hasattr(result, '_wrapped_collection_fix'):
        result._wrapped_collection_fix._is_stroked = False
    return result


def pcolor(cube, *args, **kwargs):
    """
    Draws a pseudocolor plot based on the given Cube.

    Kwargs:

    * coords: list of :class:`~iris.coords.Coord` objects or coordinate names
        Use the given coordinates as the axes for the plot. The order of the
        given coordinates indicates which axis to use for each, where the first
        element is the horizontal axis of the plot and the second element is
        the vertical axis of the plot.

    See :func:`matplotlib.pyplot.pcolor` for details of other valid keyword
    arguments.

    """
    kwargs.setdefault('antialiased', True)
    kwargs.setdefault('snap', False)
    result = _draw_2d_from_bounds('pcolor', cube, *args, **kwargs)
    return result


def pcolormesh(cube, *args, **kwargs):
    """
    Draws a pseudocolor plot based on the given Cube.

    Kwargs:

    * coords: list of :class:`~iris.coords.Coord` objects or coordinate names
        Use the given coordinates as the axes for the plot. The order of the
        given coordinates indicates which axis to use for each, where the first
        element is the horizontal axis of the plot and the second element is
        the vertical axis of the plot.

    See :func:`matplotlib.pyplot.pcolormesh` for details of other valid keyword
    arguments.

    """
    result = _draw_2d_from_bounds('pcolormesh', cube, *args, **kwargs)
    return result


def points(cube, *args, **kwargs):
    """
    Draws sample point positions based on the given Cube.

    Kwargs:

    * coords: list of :class:`~iris.coords.Coord` objects or coordinate names
        Use the given coordinates as the axes for the plot. The order of the
        given coordinates indicates which axis to use for each, where the first
        element is the horizontal axis of the plot and the second element is
        the vertical axis of the plot.

    See :func:`matplotlib.pyplot.scatter` for details of other valid keyword
    arguments.

    """
    _scatter_args = lambda u, v, data, *args, **kwargs: ((u, v) + args, kwargs)
    return _draw_2d_from_points('scatter', _scatter_args, cube,
                                *args, **kwargs)


def _1d_coords_deprecation_handler(func):
    """
    Manage the deprecation of the coords keyword argument to 1d plot
    functions.

    """
    @functools.wraps(func)
    def _wrapper(*args, **kwargs):
        coords = kwargs.pop('coords', None)
        if coords is not None:
            # issue a deprecation warning and check to see if the old
            # interface should be mimicked for the deprecation period
            warnings.warn('The coords keyword argument is deprecated.',
                          stacklevel=2)
            if len(coords) != 1:
                msg = 'The list of coordinates given should have length 1 ' \
                      'but it has length {}.'
                raise ValueError(msg.format(len(coords)))
            if isinstance(args[0], iris.cube.Cube):
                if len(args) < 2 or not isinstance(args[1], (iris.cube.Cube,
                                                   iris.coords.Coord)):
                    coord = args[0].coord(coords[0])
                    if not args[0].coord_dims(coord):
                        raise ValueError("The coordinate {!r} doesn't "
                                         "span a data dimension."
                                         "".format(coord.name()))
                    args = (coord,) + args
        return func(*args, **kwargs)
    return _wrapper


@_1d_coords_deprecation_handler
def plot(*args, **kwargs):
    """
    Draws a line plot based on the given cube(s) or coordinate(s).

    The first one or two arguments may be cubes or coordinates to plot.
    Each of the following is valid::

        # plot a 1d cube against its dimension coordinate
        plot(cube)

        # plot a 1d coordinate
        plot(coord)

        # plot a 1d cube against a given 1d coordinate, with the cube
        # values on the y-axis and the coordinate on the x-axis
        plot(coord, cube)

        # plot a 1d cube against a given 1d coordinate, with the cube
        # values on the x-axis and the coordinate on the y-axis
        plot(cube, coord)

        # plot two 1d coordinates against one-another
        plot(coord1, coord2)

        # plot two 1d cubes against one-another
        plot(cube1, cube2)

    Kwargs:

    * coords: list of :class:`~iris.coords.Coord` objects or coordinate names
        Use the given coordinates as the axes for the plot. The order of the
        given coordinates indicates which axis to use for each, where the first
        element is the horizontal axis of the plot and the second element is
        the vertical axis of the plot.

        .. deprecated:: 1.5

           The plot coordinates can be specified explicitly as in the
           above examples, so this keyword is no longer needed.

    See :func:`matplotlib.pyplot.plot` for details of valid keyword
    arguments.

    """
    _plot_args = None
    return _draw_1d_from_points('plot', _plot_args, *args, **kwargs)


def scatter(x, y, *args, **kwargs):
    """
    Draws a scatter plot based on the given cube(s) or coordinate(s).

    Args:

    * x: :class:`~iris.cube.Cube` or :class:`~iris.coords.Coord`
        A cube or a coordinate to plot on the x-axis.

    * y: :class:`~iris.cube.Cube` or :class:`~iris.coords.Coord`
        A cube or a coordinate to plot on the y-axis.

    See :func:`matplotlib.pyplot.scatter` for details of valid keyword
    arguments.

    """
    # here we are more specific about argument types than generic 1d plotting
    if not isinstance(x, (iris.cube.Cube, iris.coords.Coord)):
        raise TypeError('x must be a cube or a coordinate.')
    if not isinstance(y, (iris.cube.Cube, iris.coords.Coord)):
        raise TypeError('y must be a cube or a coordinate.')
    args = (x, y) + args
    _plot_args = None
    return _draw_1d_from_points('scatter', _plot_args, *args, **kwargs)


# Provide convenience show method from pyplot
show = plt.show


def symbols(x, y, symbols, size, axes=None, units='inches'):
    """
    Draws fixed-size symbols.

    See :mod:`iris.symbols` for available symbols.

    Args:

    * x: iterable
        The x coordinates where the symbols will be plotted.

    * y: iterable
        The y coordinates where the symbols will be plotted.

    * symbols: iterable
        The symbols (from :mod:`iris.symbols`) to plot.

    * size: float
        The symbol size in `units`.

    Kwargs:

    * axes:
        The :class:`matplotlib.axes.Axes` in which the symbols will be added.
        Defaults to the current axes.

    * units: ['inches', 'points']
        The unit for the symbol size.

    """
    if axes is None:
        axes = plt.gca()

    offsets = np.array(zip(x, y))

    # XXX "match_original" doesn't work ... so brute-force it instead.
    #   PatchCollection constructor ignores all non-style keywords when using
    #   match_original
    #   See matplotlib.collections.PatchCollection.__init__
    #   Specifically matplotlib/collections line 1053
    # pc = PatchCollection(symbols, offsets=offsets, transOffset=ax.transData,
    #                      match_original=True)
    facecolors = [p.get_facecolor() for p in symbols]
    edgecolors = [p.get_edgecolor() for p in symbols]
    linewidths = [p.get_linewidth() for p in symbols]

    pc = mpl_collections.PatchCollection(symbols, offsets=offsets,
                                         transOffset=axes.transData,
                                         facecolors=facecolors,
                                         edgecolors=edgecolors,
                                         linewidths=linewidths)

    if units == 'inches':
        scale = axes.figure.dpi
    elif units == 'points':
        scale = axes.figure.dpi / 72.0
    else:
        raise ValueError("Unrecognised units: '%s'" % units)
    pc.set_transform(mpl_transforms.Affine2D().scale(0.5 * size * scale))

    axes.add_collection(pc)
    axes.autoscale_view()


def citation(text, figure=None):
    """
    Add a text citation to a plot.

    Places an anchored text citation in the bottom right
    hand corner of the plot.

    Args:

    * text:
        Citation text to be plotted.

    Kwargs:

    * figure:
        Target :class:`matplotlib.figure.Figure` instance. Defaults
        to the current figure if none provided.

    """

    if text is not None and len(text):
        if figure is None:
            figure = plt.gcf()
        anchor = AnchoredText(text, prop=dict(size=6), frameon=True, loc=4)
        anchor.patch.set_boxstyle('round, pad=0, rounding_size=0.2')
        figure.gca().add_artist(anchor)

########NEW FILE########
__FILENAME__ = proxy
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Provision of a service to handle missing packages at runtime.
Current just a very thin layer but gives the option to extend
handling as much as needed

"""

import sys


class FakeModule(object):
    __slots__ = ('_name',)

    def __init__(self, name):
        self._name = name

    def __setattr__(self, name, value):
        object.__setattr__(self, name, value)

    def __getattr__(self, name):
        raise AttributeError(
            'Module "{}" not available or not installed'.format(self._name))


def apply_proxy(module_name, dic):
    """
    Attempt the import else use the proxy module.
    It is important to note that '__import__()' must be used
    instead of the higher-level 'import' as we need to
    ensure the scope of the import can be propagated out of this package.
    Also, note the splitting of name - this is because '__import__()'
    requires full package path, unlike 'import' (this issue is
    explicitly seen in lib/iris/fileformats/pp.py importing pp_packing)

    """
    name = module_name.split('.')[-1]
    try:
        __import__(module_name)
        dic[name] = sys.modules[module_name]
    except ImportError:
        dic[name] = sys.modules[name] = FakeModule(name)

########NEW FILE########
__FILENAME__ = quickplot
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
High-level plotting extensions to :mod:`iris.plot`.

These routines work much like their :mod:`iris.plot` counterparts, but they
automatically add a plot title, axis titles, and a colour bar when appropriate.

See also: :ref:`matplotlib <matplotlib:users-guide-index>`.

"""
import warnings   # deprecation of coords keyword in plot

import matplotlib.pyplot as plt

import iris.config
import iris.coords
import iris.plot as iplt


def _use_symbol(units):
    # For non-time units use the shortest unit representation.
    # E.g. prefer 'K' over 'kelvin', but not '0.0174532925199433 rad'
    # over 'degrees'
    return (not units.is_time() and
            not units.is_time_reference() and
            len(units.symbol) < len(str(units)))


def _title(cube_or_coord, with_units):
    if cube_or_coord is None:
        title = ''
    else:
        title = cube_or_coord.name().replace('_', ' ').capitalize()
        units = cube_or_coord.units
        if with_units and not (units.is_unknown() or
                               units.is_no_unit() or
                               units == iris.unit.Unit('1')):

            if _use_symbol(units):
                units = units.symbol
            title += ' / {}'.format(units)

    return title


def _label(cube, mode, result=None, ndims=2, coords=None):
    """Puts labels on the current plot using the given cube."""

    plt.title(_title(cube, with_units=False))

    if result is not None:
        draw_edges = mode == iris.coords.POINT_MODE
        bar = plt.colorbar(result, orientation='horizontal',
                           drawedges=draw_edges)
        has_known_units = not (cube.units.is_unknown() or
                               cube.units.is_no_unit())
        if has_known_units and cube.units != iris.unit.Unit('1'):
            # Use shortest unit representation for anything other than time
            if _use_symbol(cube.units):
                bar.set_label(cube.units.symbol)
            else:
                bar.set_label(cube.units)
        # Remove the tick which is put on the colorbar by default.
        bar.ax.tick_params(length=0)

    if coords is None:
        plot_defn = iplt._get_plot_defn(cube, mode, ndims)
    else:
        plot_defn = iplt._get_plot_defn_custom_coords_picked(
            cube, coords, mode, ndims=ndims)

    if ndims == 2:
        if not iplt._can_draw_map(plot_defn.coords):
            plt.ylabel(_title(plot_defn.coords[0], with_units=True))
            plt.xlabel(_title(plot_defn.coords[1], with_units=True))
    elif ndims == 1:
        plt.xlabel(_title(plot_defn.coords[0], with_units=True))
        plt.ylabel(_title(cube, with_units=True))
    else:
        msg = 'Unexpected number of dimensions (%s) given to _label.' % ndims
        raise ValueError(msg)


def _label_with_bounds(cube, result=None, ndims=2, coords=None):
    _label(cube, iris.coords.BOUND_MODE, result, ndims, coords)


def _label_with_points(cube, result=None, ndims=2, coords=None):
    _label(cube, iris.coords.POINT_MODE, result, ndims, coords)


def _get_titles(u_object, v_object):
    if u_object is None:
        u_object = iplt._u_object_from_v_object(v_object)
    xunits = u_object is not None and not u_object.units.is_time_reference()
    yunits = not v_object.units.is_time_reference()
    xlabel = _title(u_object, with_units=xunits)
    ylabel = _title(v_object, with_units=yunits)
    title = ''
    if u_object is None:
        title = _title(v_object, with_units=False)
    elif isinstance(u_object, iris.cube.Cube) and \
            not isinstance(v_object, iris.cube.Cube):
        title = _title(u_object, with_units=False)
    elif isinstance(v_object, iris.cube.Cube) and \
            not isinstance(u_object, iris.cube.Cube):
        title = _title(v_object, with_units=False)
    return xlabel, ylabel, title


def _label_1d_plot(*args):
    if len(args) > 1 and isinstance(args[1],
                                    (iris.cube.Cube, iris.coords.Coord)):
        xlabel, ylabel, title = _get_titles(*args[:2])
    else:
        xlabel, ylabel, title = _get_titles(None, args[0])
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)


def contour(cube, *args, **kwargs):
    """
    Draws contour lines on a labelled plot based on the given Cube.

    With the basic call signature, contour "level" values are chosen
    automatically::

        contour(cube)

    Supply a number to use *N* automatically chosen levels::

        contour(cube, N)

    Supply a sequence *V* to use explicitly defined levels::

        contour(cube, V)

    See :func:`iris.plot.contour` for details of valid keyword arguments.

    """
    coords = kwargs.get('coords')
    result = iplt.contour(cube, *args, **kwargs)
    _label_with_points(cube, coords=coords)
    return result


def contourf(cube, *args, **kwargs):
    """
    Draws filled contours on a labelled plot based on the given Cube.

    With the basic call signature, contour "level" values are chosen
    automatically::

        contour(cube)

    Supply a number to use *N* automatically chosen levels::

        contour(cube, N)

    Supply a sequence *V* to use explicitly defined levels::

        contour(cube, V)

    See :func:`iris.plot.contourf` for details of valid keyword arguments.

    """
    coords = kwargs.get('coords')
    result = iplt.contourf(cube, *args, **kwargs)
    _label_with_points(cube, result, coords=coords)
    return result


def outline(cube, coords=None):
    """Draws cell outlines on a labelled plot based on the given Cube."""
    result = iplt.outline(cube, coords=coords)
    _label_with_bounds(cube, coords=coords)
    return result


def pcolor(cube, *args, **kwargs):
    """
    Draws a labelled pseudocolor plot based on the given Cube.

    See :func:`iris.plot.pcolor` for details of valid keyword arguments.

    """
    coords = kwargs.get('coords')
    result = iplt.pcolor(cube, *args, **kwargs)
    _label_with_bounds(cube, result, coords=coords)
    return result


def pcolormesh(cube, *args, **kwargs):
    """
    Draws a labelled pseudocolour plot based on the given Cube.

    See :func:`iris.plot.pcolormesh` for details of valid keyword arguments.

    """
    coords = kwargs.get('coords')
    result = iplt.pcolormesh(cube, *args, **kwargs)
    _label_with_bounds(cube, result, coords=coords)
    return result


def points(cube, *args, **kwargs):
    """
    Draws sample point positions on a labelled plot based on the given Cube.

    See :func:`iris.plot.points` for details of valid keyword arguments.

    """
    coords = kwargs.get('coords')
    result = iplt.points(cube, *args, **kwargs)
    _label_with_points(cube, coords=coords)
    return result


@iplt._1d_coords_deprecation_handler
def plot(*args, **kwargs):
    """
    Draws a labelled line plot based on the given cube(s) or
    coordinate(s).

    See :func:`iris.plot.plot` for details of valid arguments and
    keyword arguments.

    """
    result = iplt.plot(*args, **kwargs)
    _label_1d_plot(*args)
    return result


def scatter(x, y, *args, **kwargs):
    """
    Draws a labelled scatter plot based on the given cubes or
    coordinates.

    See :func:`iris.plot.scatter` for details of valid arguments and
    keyword arguments.

    """
    result = iplt.scatter(x, y, *args, **kwargs)
    _label_1d_plot(x, y)
    return result


# Provide a convenience show method from pyplot.
show = plt.show

########NEW FILE########
__FILENAME__ = symbols
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Contains symbol definitions for use with :func:`iris.plot.symbols`.

"""

import itertools
import math

from matplotlib.patches import PathPatch
from matplotlib.path import Path

import numpy as np


__all__ = ('CLOUD_COVER',)


# The thickness to use for lines, circles, etc.
_THICKNESS = 0.1


def _make_merged_patch(paths):
    # Convert a list of Path instances into a single, black PathPatch.

    # Prepare empty vertex/code arrays for the merged path.
    # The vertex array is initially flat for convenient initialisation,
    # but is then reshaped to (N, 2).
    total_len = sum(len(path) for path in paths)
    all_vertices = np.empty(total_len * 2)
    all_codes = np.empty(total_len, dtype=Path.code_type)

    # Copy vertex/code details from the source paths
    all_segments = itertools.chain(*(path.iter_segments() for path in paths))
    i_vertices = 0
    i_codes = 0
    for vertices, code in all_segments:
        n_vertices = len(vertices)
        all_vertices[i_vertices:i_vertices + n_vertices] = vertices
        i_vertices += n_vertices

        n_codes = n_vertices / 2
        if code == Path.STOP:
            code = Path.MOVETO
        all_codes[i_codes:i_codes + n_codes] = code
        i_codes += n_codes

    all_vertices.shape = (total_len, 2)

    return PathPatch(Path(all_vertices, all_codes), facecolor='black',
                     edgecolor='none')


def _ring_path():
    # Returns a Path for a hollow ring.
    # The outer radius is 1, the inner radius is 1 - _THICKNESS.
    circle = Path.unit_circle()
    inner_radius = 1.0 - _THICKNESS
    vertices = np.concatenate([circle.vertices[:-1],
                               circle.vertices[-2::-1] * inner_radius])
    codes = np.concatenate([circle.codes[:-1], circle.codes[:-1]])
    return Path(vertices, codes)


def _vertical_bar_path():
    # Returns a Path for a vertical rectangle, with width _THICKNESS, that will
    # nicely overlap the result of _ring_path().
    width = _THICKNESS / 2.0
    inner_radius = 1.0 - _THICKNESS
    vertices = np.array([
        [-width, -inner_radius],
        [width, -inner_radius],
        [width, inner_radius],
        [-width, inner_radius],
        [-width, inner_radius]
    ])
    codes = np.array([Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO,
                      Path.CLOSEPOLY])
    return Path(vertices, codes)


def _slot_path():
    # Returns a Path for a filled unit circle with a vertical rectangle
    # removed.
    circle = Path.unit_circle()
    vertical_bar = _vertical_bar_path()
    vertices = np.concatenate([circle.vertices[:-1],
                               vertical_bar.vertices[-2::-1]])
    codes = np.concatenate([circle.codes[:-1], vertical_bar.codes[:-1]])
    return Path(vertices, codes)


def _left_bar_path():
    # Returns a Path for the left-hand side of a horizontal rectangle, with
    # height _THICKNESS, that will nicely overlap the result of _ring_path().
    inner_radius = 1.0 - _THICKNESS
    height = _THICKNESS / 2.0
    vertices = np.array([
        [-inner_radius, -height],
        [0, -height],
        [0, height],
        [-inner_radius, height],
        [-inner_radius, height]
    ])
    codes = np.array([Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO,
                      Path.CLOSEPOLY])
    return Path(vertices, codes)


def _slash_path():
    # Returns a Path for diagonal, bottom-left to top-right rectangle, with
    # width _THICKNESS, that will nicely overlap the result of _ring_path().
    half_width = _THICKNESS / 2.0
    central_radius = 1.0 - half_width

    cos45 = math.cos(math.radians(45))

    end_point_offset = cos45 * central_radius
    half_width_offset = cos45 * half_width

    vertices = np.array([
        [-end_point_offset - half_width_offset,
         -end_point_offset + half_width_offset],
        [-end_point_offset + half_width_offset,
         -end_point_offset - half_width_offset],
        [end_point_offset + half_width_offset,
         end_point_offset - half_width_offset],
        [end_point_offset - half_width_offset,
         end_point_offset + half_width_offset],
        [-end_point_offset - half_width_offset,
         -end_point_offset + half_width_offset]
    ])
    codes = np.array([Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO,
                      Path.CLOSEPOLY])
    return Path(vertices, codes)


def _backslash_path():
    # Returns a Path for diagonal, top-left to bottom-right rectangle, with
    # width _THICKNESS, that will nicely overlap the result of _ring_path().
    half_width = _THICKNESS / 2.0
    central_radius = 1.0 - half_width

    cos45 = math.cos(math.radians(45))

    end_point_offset = cos45 * central_radius
    half_width_offset = cos45 * half_width

    vertices = np.array([
        [-end_point_offset - half_width_offset,
         end_point_offset - half_width_offset],
        [end_point_offset - half_width_offset,
         -end_point_offset - half_width_offset],
        [end_point_offset + half_width_offset,
         -end_point_offset + half_width_offset],
        [-end_point_offset + half_width_offset,
         end_point_offset + half_width_offset],
        [-end_point_offset - half_width_offset,
         end_point_offset - half_width_offset]
    ])
    codes = np.array([Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO,
                      Path.CLOSEPOLY])
    return Path(vertices, codes)


def _wedge_fix(wedge_path):
    '''
    Fixes the problem with Path.wedge where it doesn't initialise the first,
    and last two vertices.
    This fix should not have any side-effects once Path.wedge has been fixed,
    but will then be redundant and should be removed.

    This is fixed in MPL v1.3, raising a RuntimeError. A check is performed to
    allow for backward compatibility with MPL v1.2.x.

    '''
    if wedge_path.vertices.flags.writeable:
        wedge_path.vertices[0] = 0
        wedge_path.vertices[-2:] = 0
    return wedge_path


CLOUD_COVER = {
    0: [_ring_path()],
    1: [_ring_path(), _vertical_bar_path()],
    2: [_ring_path(), _wedge_fix(Path.wedge(0, 90))],
    3: [_ring_path(), _wedge_fix(Path.wedge(0, 90)), _vertical_bar_path()],
    4: [_ring_path(), Path.unit_circle_righthalf()],
    5: [_ring_path(), Path.unit_circle_righthalf(), _left_bar_path()],
    6: [_ring_path(), _wedge_fix(Path.wedge(-180, 90))],
    7: [_slot_path()],
    8: [Path.unit_circle()],
    9: [_ring_path(), _slash_path(), _backslash_path()],
}
"""
A dictionary mapping WMO cloud cover codes to their corresponding symbol.

See http://www.wmo.int/pages/prog/www/DPFS/documents/485_Vol_I_en_colour.pdf
    Part II, Appendix II.4, Graphical Representation of Data, Analyses
    and Forecasts

"""


def _convert_paths_to_patches():
    # Convert the symbols defined as lists-of-paths into patches.
    for code, symbol in CLOUD_COVER.iteritems():
        CLOUD_COVER[code] = _make_merged_patch(symbol)


_convert_paths_to_patches()

########NEW FILE########
__FILENAME__ = test_interpolate
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the iris.analysis.interpolate module.

"""
# Import iris tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import numpy as np

import iris.analysis.interpolate as interpolate
from iris.coords import DimCoord
from iris.cube import Cube
from iris.tests.test_interpolation import normalise_order


class Test_linear__circular_wrapping(tests.IrisTest):
    def _create_cube(self, longitudes):
        # Return a Cube with circular longitude with the given values.
        data = np.arange(12).reshape((3, 4)) * 0.1
        cube = Cube(data)
        lon = DimCoord(longitudes, standard_name='longitude',
                       units='degrees', circular=True)
        cube.add_dim_coord(lon, 1)
        return cube

    def test_symmetric(self):
        # Check we can interpolate from a Cube defined over [-180, 180).
        cube = self._create_cube([-180, -90, 0, 90])
        samples = [('longitude', range(-360, 720, 45))]
        result = interpolate.linear(cube, samples, extrapolation_mode='nan')
        normalise_order(result)
        self.assertCMLApproxData(result, ('analysis', 'interpolation',
                                          'linear', 'circular_wrapping',
                                          'symmetric'))

    def test_positive(self):
        # Check we can interpolate from a Cube defined over [0, 360).
        cube = self._create_cube([0, 90, 180, 270])
        samples = [('longitude', range(-360, 720, 45))]
        result = interpolate.linear(cube, samples, extrapolation_mode='nan')
        normalise_order(result)
        self.assertCMLApproxData(result, ('analysis', 'interpolation',
                                          'linear', 'circular_wrapping',
                                          'positive'))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_stats
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the iris.analysis.stats module.

"""
# Import iris tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import numpy as np

import iris
import iris.analysis.stats as stats


@tests.skip_data
class Test_corr(tests.IrisTest):
    def setUp(self):
        self.cube_a = iris.load_cube(iris.sample_data_path('GloSea4',
                                                           'ensemble_001.pp'))
        self.cube_b = iris.load_cube(iris.sample_data_path('GloSea4',
                                                           'ensemble_002.pp'))

    def test_perfect_corr(self):
        r = stats.pearsonr(self.cube_a, self.cube_a,
                           ['latitude',   'longitude'])
        self.assertArrayEqual(r.data, np.array([1.]*6))

    def test_perfect_corr_all_dims(self):
        r = stats.pearsonr(self.cube_a, self.cube_a)
        self.assertArrayEqual(r.data, np.array([1.]))

    def test_incompatible_cubes(self):
        with self.assertRaises(ValueError):
            stats.pearsonr(self.cube_a, self.cube_b[0, :, :], 'time')

    def test_compatible_cubes(self):
        r = stats.pearsonr(self.cube_a, self.cube_b, ['latitude', 'longitude'])
        self.assertArrayAlmostEqual(r.data, [0.99733591,
                                             0.99501693,
                                             0.99674225,
                                             0.99495268,
                                             0.99217004,
                                             0.99362189])

    def test_4d_cube_2_dims(self):
        real_0_c = iris.coords.AuxCoord(np.int32(0), 'realization')
        real_1_c = iris.coords.AuxCoord(np.int32(1), 'realization')

        # Make cubes merge-able.
        self.cube_a.add_aux_coord(real_0_c)
        self.cube_b.add_aux_coord(real_1_c)
        self.cube_a.remove_coord('forecast_period')
        self.cube_a.remove_coord('forecast_reference_time')
        self.cube_b.remove_coord('forecast_period')
        self.cube_b.remove_coord('forecast_reference_time')
        four_d_cube_a = iris.cube\
            .CubeList([self.cube_a, self.cube_b]).merge()[0]
        self.cube_a.remove_coord('realization')
        self.cube_b.remove_coord('realization')
        self.cube_a.add_aux_coord(real_1_c)
        self.cube_b.add_aux_coord(real_0_c)
        four_d_cube_b = iris.cube\
            .CubeList([self.cube_a, self.cube_b]).merge()[0]

        r = stats.pearsonr(four_d_cube_a, four_d_cube_b,
                           ['latitude', 'longitude'])
        expected_corr = [[0.99733591,
                          0.99501693,
                          0.99674225,
                          0.99495268,
                          0.99217004,
                          0.99362189],
                         [0.99733591,
                          0.99501693,
                          0.99674225,
                          0.99495268,
                          0.99217004,
                          0.99362189]]
        self.assertArrayAlmostEqual(r.data, expected_corr)

    def test_non_existent_coord(self):
        with self.assertRaises(ValueError):
            stats.pearsonr(self.cube_a, self.cube_b, 'bad_coord')


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_get_xy_dim_coords
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the :func:`iris.experimental.regrid._get_xy_dim_coords` function.

"""
# import iris tests first so that some things can be initialised
# before importing anything else.
import iris.tests as tests

import copy

import numpy as np

import iris.coords
import iris.coord_systems
import iris.experimental.regrid
import iris.tests.stock


class TestGetXYCoords(tests.IrisTest):
    def test_grid_lat_lon(self):
        cube = iris.tests.stock.realistic_4d()
        x, y = iris.experimental.regrid._get_xy_dim_coords(cube)
        self.assertIs(x, cube.coord('grid_longitude'))
        self.assertIs(y, cube.coord('grid_latitude'))

    def test_lat_lon(self):
        cube = iris.tests.stock.lat_lon_cube()
        x, y = iris.experimental.regrid._get_xy_dim_coords(cube)
        self.assertIs(x, cube.coord('longitude'))
        self.assertIs(y, cube.coord('latitude'))

    def test_projection_coords(self):
        cube = iris.tests.stock.lat_lon_cube()
        cube.coord('longitude').rename('projection_x_coordinate')
        cube.coord('latitude').rename('projection_y_coordinate')
        x, y = iris.experimental.regrid._get_xy_dim_coords(cube)
        self.assertIs(x, cube.coord('projection_x_coordinate'))
        self.assertIs(y, cube.coord('projection_y_coordinate'))

    def test_missing_x_coord(self):
        cube = iris.tests.stock.realistic_4d()
        cube.remove_coord('grid_longitude')
        with self.assertRaises(ValueError):
            iris.experimental.regrid._get_xy_dim_coords(cube)

    def test_missing_y_coord(self):
        cube = iris.tests.stock.realistic_4d()
        cube.remove_coord('grid_latitude')
        with self.assertRaises(ValueError):
            iris.experimental.regrid._get_xy_dim_coords(cube)

    def test_multiple_coords(self):
        cube = iris.tests.stock.realistic_4d()
        cs = iris.coord_systems.GeogCS(6371229)
        time_coord = cube.coord('time')
        time_dims = cube.coord_dims(time_coord)
        lat_coord = iris.coords.DimCoord(np.arange(time_coord.shape[0]),
                                         standard_name='latitude',
                                         units='degrees',
                                         coord_system=cs)
        cube.remove_coord(time_coord)
        cube.add_dim_coord(lat_coord, time_dims)
        model_level_coord = cube.coord('model_level_number')
        model_level_dims = cube.coord_dims(model_level_coord)
        lon_coord = iris.coords.DimCoord(np.arange(model_level_coord.shape[0]),
                                         standard_name='longitude',
                                         units='degrees',
                                         coord_system=cs)
        cube.remove_coord(model_level_coord)
        cube.add_dim_coord(lon_coord, model_level_dims)

        with self.assertRaises(ValueError):
            iris.experimental.regrid._get_xy_dim_coords(cube)

        cube.remove_coord('grid_latitude')
        cube.remove_coord('grid_longitude')

        x, y = iris.experimental.regrid._get_xy_dim_coords(cube)
        self.assertIs(x, lon_coord)
        self.assertIs(y, lat_coord)

    def test_no_coordsystem(self):
        cube = iris.tests.stock.lat_lon_cube()
        for coord in cube.coords():
            coord.coord_system = None
        x, y = iris.experimental.regrid._get_xy_dim_coords(cube)
        self.assertIs(x, cube.coord('longitude'))
        self.assertIs(y, cube.coord('latitude'))

    def test_one_coordsystem(self):
        cube = iris.tests.stock.lat_lon_cube()
        cube.coord('longitude').coord_system = None
        with self.assertRaises(ValueError):
            iris.experimental.regrid._get_xy_dim_coords(cube)

    def test_different_coordsystem(self):
        cube = iris.tests.stock.lat_lon_cube()

        lat_cs = copy.copy(cube.coord('latitude').coord_system)
        lat_cs.semi_major_axis = 7000000
        cube.coord('latitude').coord_system = lat_cs

        lon_cs = copy.copy(cube.coord('longitude').coord_system)
        lon_cs.semi_major_axis = 7000001
        cube.coord('longitude').coord_system = lon_cs

        with self.assertRaises(ValueError):
            iris.experimental.regrid._get_xy_dim_coords(cube)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_regrid_area_weighted_rectilinear_src_and_grid
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test area weighted regridding.

"""
# import iris tests first so that some things can be initialised
# before importing anything else.
import iris.tests as tests

import copy
import random

import numpy as np
import numpy.ma as ma

from iris.experimental.regrid import \
    regrid_area_weighted_rectilinear_src_and_grid as regrid_area_weighted
import iris.tests.stock

# Run tests in no graphics mode if matplotlib is not available.
if tests.MPL_AVAILABLE:
    import matplotlib.pyplot as plt
    import iris.quickplot as qplt


RESULT_DIR = ('experimental', 'regrid',
              'regrid_area_weighted_rectilinear_src_and_grid')


def _scaled_and_offset_grid(cube, x_scalefactor, y_scalefactor,
                            x_offset=0.0, y_offset=0.0):
    """
    Return a cube with a horizontal grid that is scaled and offset
    from the horizontal grid of `src`.

    """
    x, y = iris.experimental.regrid._get_xy_dim_coords(cube)
    new_cube = cube.copy()
    new_cube.replace_coord(x * x_scalefactor + x_offset)
    new_cube.replace_coord(y * y_scalefactor + y_offset)
    return new_cube


def _subsampled_coord(coord, subsamplefactor):
    """
    Return a coordinate that is a subsampled copy of `coord`.

    .. note:: `subsamplefactor` must be an integer >= 1.

    """
    if not isinstance(subsamplefactor, int):
        raise ValueError('subsamplefactor must be an integer.')
    if subsamplefactor < 1:
        raise ValueError('subsamplefactor must be >= 1.')
    if not coord.has_bounds():
        raise ValueError('The coordinate must have bounds.')
    new_coord = coord[::subsamplefactor]
    new_bounds = new_coord.bounds.copy()
    new_bounds[:, 1] = coord.bounds[(subsamplefactor - 1)::subsamplefactor, 1]
    new_bounds[-1, 1] = coord.bounds[-1, 1]
    new_coord = coord.copy(points=new_coord.points, bounds=new_bounds)
    return new_coord


def _subsampled_grid(cube, x_subsamplefactor, y_subsamplefactor):
    """
    Return a cube that has a horizontal grid that is a subsampled
    version of the horizontal grid of `cube`.

    .. note:: The two subsamplefactors must both be integers >= 1.

    .. note:: The data of the returned cube is populated with zeros.

    """
    x, y = iris.experimental.regrid._get_xy_dim_coords(cube)
    x_dim = cube.coord_dims(x)[0]
    y_dim = cube.coord_dims(y)[0]
    new_x = _subsampled_coord(x, x_subsamplefactor)
    new_y = _subsampled_coord(y, y_subsamplefactor)
    new_shape = list(cube.shape)
    new_shape[x_dim] = len(new_x.points)
    new_shape[y_dim] = len(new_y.points)
    new_data = np.zeros(new_shape)
    new_cube = iris.cube.Cube(new_data)
    new_cube.metadata = cube.metadata
    new_cube.add_dim_coord(new_y, y_dim)
    new_cube.add_dim_coord(new_x, x_dim)
    return new_cube


def _resampled_coord(coord, samplefactor):
    """
    Return a coordinate that has the same extent as `coord` but has
    `samplefactor` times as many points and bounds.

    """
    bounds = coord.bounds
    lower = bounds[0, 0]
    upper = bounds[-1, 1]
    # Prevent fp-precision increasing the extent by "squeezing" the grid.
    delta = 0.00001 * np.sign(upper - lower) * abs(bounds[0, 1] - bounds[0, 0])
    lower = lower + delta
    upper = upper - delta
    new_points, step = np.linspace(lower, upper,
                                   len(bounds) * samplefactor,
                                   endpoint=False, retstep=True)
    new_points += step * 0.5
    new_coord = coord.copy(points=new_points)
    new_coord.guess_bounds()
    return new_coord


def _resampled_grid(cube, x_samplefactor, y_samplefactor):
    """
    Return a cube that has the same horizontal extent as `cube` but has
    a reduced (or increased) number of points (and bounds) along the X and Y
    dimensions.

    The resulting number of points for each dimension is determined by::

        int(len(coord.points) * samplefactor)

    This will be truncated if the result is not an integer.

    .. note:: The data of the returned cube is populated with zeros.

    """
    x, y = iris.experimental.regrid._get_xy_dim_coords(cube)
    x_dim = cube.coord_dims(x)[0]
    y_dim = cube.coord_dims(y)[0]
    new_x = _resampled_coord(x, x_samplefactor)
    new_y = _resampled_coord(y, y_samplefactor)
    new_shape = list(cube.shape)
    new_shape[x_dim] = len(new_x.points)
    new_shape[y_dim] = len(new_y.points)
    new_data = np.zeros(new_shape)
    new_cube = iris.cube.Cube(new_data)
    new_cube.metadata = cube.metadata
    new_cube.add_dim_coord(new_y, y_dim)
    new_cube.add_dim_coord(new_x, x_dim)
    return new_cube


class TestAreaWeightedRegrid(tests.GraphicsTest):
    def setUp(self):
        # A cube with a hybrid height derived coordinate.
        self.realistic_cube = iris.tests.stock.realistic_4d()[:2, :5, :20, :30]
        # A simple (3, 4) cube.
        self.simple_cube = iris.tests.stock.lat_lon_cube()
        self.simple_cube.coord('latitude').guess_bounds(0.0)
        self.simple_cube.coord('longitude').guess_bounds(0.0)

    def test_no_bounds(self):
        src = self.simple_cube.copy()
        src.coord('latitude').bounds = None
        dest = self.simple_cube.copy()
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)

        src = self.simple_cube.copy()
        src.coord('longitude').bounds = None
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)

        src = self.simple_cube.copy()
        dest = self.simple_cube.copy()
        dest.coord('latitude').bounds = None
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)

        dest = self.simple_cube.copy()
        dest.coord('longitude').bounds = None
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)

    def test_non_contiguous_bounds(self):
        src = self.simple_cube.copy()
        bounds = src.coord('latitude').bounds.copy()
        bounds[1, 1] -= 0.1
        src.coord('latitude').bounds = bounds
        dest = self.simple_cube.copy()
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)

        src = self.simple_cube.copy()
        dest = self.simple_cube.copy()
        bounds = dest.coord('longitude').bounds.copy()
        bounds[1, 1] -= 0.1
        dest.coord('longitude').bounds = bounds
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)

    def test_missing_coords(self):
        dest = self.simple_cube.copy()
        # Missing src_x.
        src = self.simple_cube.copy()
        src.remove_coord('longitude')
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)
        # Missing src_y.
        src = self.simple_cube.copy()
        src.remove_coord('latitude')
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)
        # Missing dest_x.
        src = self.simple_cube.copy()
        dest = self.simple_cube.copy()
        dest.remove_coord('longitude')
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)
        # Missing dest_y.
        src = self.simple_cube.copy()
        dest = self.simple_cube.copy()
        dest.remove_coord('latitude')
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)

    def test_different_cs(self):
        src = self.simple_cube.copy()
        src_cs = copy.copy(src.coord('latitude').coord_system)
        src_cs.semi_major_axis = 7000000
        src.coord('longitude').coord_system = src_cs
        src.coord('latitude').coord_system = src_cs
        dest = self.simple_cube.copy()
        dest_cs = copy.copy(src_cs)
        dest_cs.semi_major_axis = 7000001
        dest.coord('longitude').coord_system = dest_cs
        dest.coord('latitude').coord_system = dest_cs
        with self.assertRaises(ValueError):
            regrid_area_weighted(src, dest)

    def test_regrid_to_same_grid(self):
        src = self.simple_cube
        res = regrid_area_weighted(src, src)
        self.assertEqual(res, src)
        self.assertCMLApproxData(res, RESULT_DIR + ('simple.cml',))

    def test_equal_area_numbers(self):
        # Remove coords system and units so it is no longer spherical.
        self.simple_cube.coord('latitude').coord_system = None
        self.simple_cube.coord('latitude').units = None
        self.simple_cube.coord('longitude').coord_system = None
        self.simple_cube.coord('longitude').units = None
        # Reduce to a single cell
        src = self.simple_cube.copy()
        dest = _subsampled_grid(src, 4, 3)
        res = regrid_area_weighted(src, dest)
        expected_val = np.mean(src.data)
        self.assertAlmostEqual(expected_val, res.data)

        # Reduce to two cells along x
        src = self.simple_cube.copy()
        dest = _subsampled_grid(src, 2, 3)
        res = regrid_area_weighted(src, dest)
        expected_val_left = np.mean(src.data[:, 0:2])
        self.assertEqual(expected_val_left, res.data[0])
        expected_val_right = np.mean(src.data[:, 2:4])
        self.assertAlmostEqual(expected_val_right, res.data[1])

        # Reduce to two cells along x, one three times the size
        # of the other.
        src = self.simple_cube.copy()
        dest = _subsampled_grid(src, 2, 3)
        lon = dest.coord('longitude')
        points = lon.points.copy()
        bounds = [[-1, 0], [0, 3]]
        lon = lon.copy(points=points, bounds=bounds)
        dest.replace_coord(lon)
        res = regrid_area_weighted(src, dest)
        expected_val_left = np.mean(src.data[:, 0:1])
        self.assertEqual(expected_val_left, res.data[0])
        expected_val_right = np.mean(src.data[:, 1:4])
        self.assertAlmostEqual(expected_val_right, res.data[1])

    def test_unqeual_area_numbers(self):
        # Remove coords system and units so it is no longer spherical.
        self.simple_cube.coord('latitude').coord_system = None
        self.simple_cube.coord('latitude').units = None
        self.simple_cube.coord('longitude').coord_system = None
        self.simple_cube.coord('longitude').units = None
        # Reduce src to two cells along x, one three times the size
        # of the other.
        src = self.simple_cube.copy()
        src = _subsampled_grid(src, 2, 2)
        lon = src.coord('longitude')
        points = lon.points.copy()
        bounds = [[-1, 0], [0, 3]]
        lon = lon.copy(points=points, bounds=bounds)
        src.replace_coord(lon)
        # Reduce src to two cells along y, one 2 times the size
        # of the other.
        lat = src.coord('latitude')
        points = lat.points.copy()
        bounds = [[-1, 0], [0, 2]]
        lat = lat.copy(points=points, bounds=bounds)
        src.replace_coord(lat)
        # Populate with data
        src.data = np.arange(src.data.size).reshape(src.shape) + 1.23
        # dest is a single cell over the whole area.
        dest = _subsampled_grid(self.simple_cube, 4, 3)
        res = regrid_area_weighted(src, dest)
        expected_val = (1. / 12. * src.data[0, 0] +
                        2. / 12. * np.mean(src.data[1:, 0]) +
                        3. / 12. * np.mean(src.data[0, 1:]) +
                        6. / 12. * np.mean(src.data[1:, 1:]))
        self.assertAlmostEqual(expected_val, res.data)

    def test_regrid_latlon_reduced_res(self):
        src = self.simple_cube
        # Reduce from (3, 4) to (2, 2).
        dest = _subsampled_grid(src, 2, 2)
        res = regrid_area_weighted(src, dest)
        self.assertCMLApproxData(res, RESULT_DIR + ('latlonreduced.cml',))

    def test_regrid_transposed(self):
        src = self.simple_cube.copy()
        dest = _subsampled_grid(src, 2, 3)
        # Transpose src so that the coords are not y, x ordered.
        src.transpose()
        res = regrid_area_weighted(src, dest)
        self.assertCMLApproxData(res, RESULT_DIR + ('trasposed.cml',))
        # Using original and transposing the result should give the
        # same answer.
        src = self.simple_cube.copy()
        res = regrid_area_weighted(src, dest)
        res.transpose()
        self.assertCMLApproxData(res, RESULT_DIR + ('trasposed.cml',))

    def test_regrid_lon_to_half_res(self):
        src = self.simple_cube
        dest = _resampled_grid(src, 0.5, 1.0)
        res = regrid_area_weighted(src, dest)
        self.assertCMLApproxData(res, RESULT_DIR + ('lonhalved.cml',))

    def test_regrid_to_non_int_frac(self):
        # Create dest such that bounds do not line up
        # with src: src.shape = (3, 4), dest.shape = (2, 3)
        src = self.simple_cube
        dest = _resampled_grid(src, 0.75, 0.67)
        res = regrid_area_weighted(src, dest)
        self.assertCMLApproxData(res, RESULT_DIR + ('lower.cml',))

    def test_regrid_to_higher_res(self):
        src = self.simple_cube
        frac = 3.5
        dest = _resampled_grid(src, frac, frac)
        res = regrid_area_weighted(src, dest)
        self.assertCMLApproxData(res, RESULT_DIR + ('higher.cml',))

    @tests.skip_plot
    def test_hybrid_height(self):
        src = self.realistic_cube
        dest = _resampled_grid(src, 0.7, 0.8)
        res = regrid_area_weighted(src, dest)
        self.assertCMLApproxData(res, RESULT_DIR + ('hybridheight.cml',))
        # Consider a single slice to allow visual tests of altitudes.
        src = src[1, 2]
        res = res[1, 2]
        qplt.pcolormesh(res)
        self.check_graphic()
        plt.contourf(res.coord('grid_longitude').points,
                     res.coord('grid_latitude').points,
                     res.coord('altitude').points)
        self.check_graphic()
        plt.contourf(res.coord('grid_longitude').points,
                     res.coord('grid_latitude').points,
                     res.coord('surface_altitude').points)
        self.check_graphic()

    def test_missing_data(self):
        src = self.simple_cube.copy()
        src.data = ma.masked_array(src.data)
        src.data[1, 2] = ma.masked
        dest = _resampled_grid(self.simple_cube, 2.3, 2.4)
        res = regrid_area_weighted(src, dest)
        mask = np.zeros((7, 9), bool)
        mask[slice(2, 5), slice(4, 7)] = True
        self.assertArrayEqual(res.data.mask, mask)

    def test_no_x_overlap(self):
        src = self.simple_cube
        dest = _scaled_and_offset_grid(src, 1.0, 1.0,
                                       (np.max(src.coord('longitude').bounds) -
                                        np.min(src.coord('longitude').bounds)),
                                       0.0)
        res = regrid_area_weighted(src, dest)
        self.assertTrue(res.data.mask.all())

    def test_no_y_overlap(self):
        src = self.simple_cube
        dest = _scaled_and_offset_grid(src, 1.0, 1.0,
                                       0.0,
                                       (np.max(src.coord('latitude').bounds) -
                                        np.min(src.coord('latitude').bounds)))
        res = regrid_area_weighted(src, dest)
        self.assertTrue(res.data.mask.all())

    def test_scalar(self):
        src = self.realistic_cube
        i = 2
        j = 3
        dest = src[0, 0, i, j]
        res = regrid_area_weighted(src, dest)
        self.assertEqual(res, src[:, :, i, j])

    def test_one_point(self):
        src = self.simple_cube.copy()
        for n in range(10):
            i = random.randint(0, src.shape[0] - 1)
            j = random.randint(0, src.shape[1] - 1)
            indices = tuple([slice(i, i + 1), slice(j, j + 1)])
            dest = src[indices]
            res = regrid_area_weighted(src, dest)
            self.assertTrue(res, src[indices])

    def test_ten_by_ten_subset(self):
        src = _resampled_grid(self.simple_cube, 20, 20)
        for n in range(10):
            i = random.randint(0, src.shape[0] - 10)
            j = random.randint(0, src.shape[1] - 10)
            indices = tuple([slice(i, i + 10), slice(j, j + 10)])
            dest = src[indices]
            res = regrid_area_weighted(src, dest)
            self.assertTrue(res, src[indices])

    @tests.skip_plot
    def test_cross_section(self):
        # Slice to get a cross section.
        # Constant latitude
        src = self.realistic_cube[0, :, 10, :]
        lon = _resampled_coord(src.coord('grid_longitude'), 0.6)
        shape = list(src.shape)
        shape[1] = len(lon.points)
        data = np.zeros(shape)
        dest = iris.cube.Cube(data)
        dest.add_dim_coord(lon, 1)
        dest.add_aux_coord(src.coord('grid_latitude').copy(), None)
        res = regrid_area_weighted(src, dest)
        self.assertCMLApproxData(res, RESULT_DIR +
                                 ('const_lat_cross_section.cml',))
        # Plot a single slice.
        qplt.plot(res[0])
        qplt.plot(src[0], 'r')
        self.check_graphic()

        # Constant longitude
        src = self.realistic_cube[0, :, :, 10]
        lat = _resampled_coord(src.coord('grid_latitude'), 0.6)
        shape = list(src.shape)
        shape[1] = len(lat.points)
        data = np.zeros(shape)
        dest = iris.cube.Cube(data)
        dest.add_dim_coord(lat, 1)
        dest.add_aux_coord(src.coord('grid_longitude').copy(), None)
        res = regrid_area_weighted(src, dest)
        self.assertCMLApproxData(res, RESULT_DIR +
                                 ('const_lon_cross_section.cml',))
        # Plot a single slice.
        qplt.plot(res[0])
        qplt.plot(src[0], 'r')
        self.check_graphic()

    def test_scalar_source_cube(self):
        src = self.simple_cube[1, 2]
        # Extend dest beyond src grid
        dest = src.copy()
        dest.coord('latitude').bounds = np.array([[-0.5, 1.5]])
        res = regrid_area_weighted(src, dest)
        self.assertTrue(res.data.mask.all())
        # Shrink dest to 1/4 of src
        dest = src.copy()
        dest.coord('latitude').bounds = np.array([[0.25, 0.75]])
        dest.coord('longitude').bounds = np.array([[1.25, 1.75]])
        res = regrid_area_weighted(src, dest)
        self.assertEqual(res.data, src.data)

    @tests.skip_data
    @tests.skip_plot
    def test_global_data_reduce_res(self):
        src = iris.tests.stock.global_pp()
        src.coord('latitude').guess_bounds()
        src.coord('longitude').guess_bounds()
        dest = _resampled_grid(src, 0.4, 0.3)
        res = regrid_area_weighted(src, dest)
        qplt.pcolormesh(res)
        self.check_graphic()

    @tests.skip_data
    @tests.skip_plot
    def test_global_data_increase_res(self):
        src = iris.tests.stock.global_pp()
        src.coord('latitude').guess_bounds()
        src.coord('longitude').guess_bounds()
        dest = _resampled_grid(src, 1.5, 1.5)
        res = regrid_area_weighted(src, dest)
        qplt.pcolormesh(res)
        self.check_graphic()

    @tests.skip_data
    @tests.skip_plot
    def test_global_data_same_res(self):
        src = iris.tests.stock.global_pp()
        src.coord('latitude').guess_bounds()
        src.coord('longitude').guess_bounds()
        res = regrid_area_weighted(src, src)
        qplt.pcolormesh(res)
        self.check_graphic()

    @tests.skip_data
    @tests.skip_plot
    def test_global_data_subset(self):
        src = iris.tests.stock.global_pp()
        src.coord('latitude').guess_bounds()
        src.coord('longitude').guess_bounds()
        dest_lat = src.coord('latitude')[0:40]
        dest_lon = iris.coords.DimCoord(np.linspace(-160, -70, 30),
                                        standard_name='longitude',
                                        units='degrees',
                                        coord_system=dest_lat.coord_system)
        # Note target grid (in -180 to 180) src in 0 to 360
        dest_lon.guess_bounds()
        data = np.zeros((dest_lat.shape[0], dest_lon.shape[0]))
        dest = iris.cube.Cube(data)
        dest.add_dim_coord(dest_lat, 0)
        dest.add_dim_coord(dest_lon, 1)

        res = regrid_area_weighted(src, dest)
        qplt.pcolormesh(res)
        plt.gca().coastlines()
        self.check_graphic()

    @tests.skip_data
    @tests.skip_plot
    def test_circular_subset(self):
        src = iris.tests.stock.global_pp()
        src.coord('latitude').guess_bounds()
        src.coord('longitude').guess_bounds()
        dest_lat = src.coord('latitude')[0:40]
        dest_lon = iris.coords.DimCoord([-15., -10., -5., 0., 5., 10., 15.],
                                        standard_name='longitude',
                                        units='degrees',
                                        coord_system=dest_lat.coord_system)
        # Note target grid (in -180 to 180) src in 0 to 360
        dest_lon.guess_bounds()
        data = np.zeros((dest_lat.shape[0], dest_lon.shape[0]))
        dest = iris.cube.Cube(data)
        dest.add_dim_coord(dest_lat, 0)
        dest.add_dim_coord(dest_lon, 1)

        res = regrid_area_weighted(src, dest)
        qplt.pcolormesh(res)
        plt.gca().coastlines()
        self.check_graphic()

    @tests.skip_data
    @tests.skip_plot
    def test_non_circular_subset(self):
        src = iris.tests.stock.global_pp()
        src.coord('latitude').guess_bounds()
        src.coord('longitude').guess_bounds()
        src.coord('longitude').circular = False
        dest_lat = src.coord('latitude')[0:40]
        dest_lon = iris.coords.DimCoord([-15., -10., -5., 0., 5., 10., 15.],
                                        standard_name='longitude',
                                        units='degrees',
                                        coord_system=dest_lat.coord_system)
        # Note target grid (in -180 to 180) src in 0 to 360
        dest_lon.guess_bounds()
        data = np.zeros((dest_lat.shape[0], dest_lon.shape[0]))
        dest = iris.cube.Cube(data)
        dest.add_dim_coord(dest_lat, 0)
        dest.add_dim_coord(dest_lon, 1)

        res = regrid_area_weighted(src, dest)
        qplt.pcolormesh(res)
        plt.gca().coastlines()
        self.check_graphic()


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_regrid_bilinear_rectilinear_src_and_grid
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import numpy as np

import iris
from iris.experimental.regrid import \
    regrid_bilinear_rectilinear_src_and_grid as regrid
from iris.aux_factory import HybridHeightFactory
from iris.coord_systems import GeogCS, OSGB
from iris.coords import AuxCoord, DimCoord
from iris.cube import Cube
from iris.tests.stock import global_pp, realistic_4d

# Run tests in no graphics mode if matplotlib is not available.
if tests.MPL_AVAILABLE:
    import iris.quickplot as qplt


RESULT_DIR = ('experimental', 'regrid',
              'regrid_bilinear_rectilinear_src_and_grid')


@tests.skip_data
class TestRotatedToLatLon(tests.IrisTest):
    def setUp(self):
        self.src = realistic_4d()[:5, :2, ::40, ::30]

    def test_single_point(self):
        src = self.src[0, 0]
        grid = global_pp()[:1, :1]
        # These coordinate values have been derived by converting the
        # rotated coordinates of src[1, 1] into lat/lon by using cs2cs.
        grid.coord('longitude').points = -3.144870
        grid.coord('latitude').points = 52.406444
        result = regrid(src, grid)
        self.assertEqual(src.data[1, 1], result.data)

    def test_transposed_src(self):
        # The source dimensions are in a non-standard order.
        src = self.src
        src.transpose([3, 1, 2, 0])
        grid = self._grid_subset()
        result = regrid(src, grid)
        result.transpose([3, 1, 2, 0])
        self.assertCMLApproxData(result, RESULT_DIR + ('subset.cml',))

    def _grid_subset(self):
        # The destination grid points are entirely contained within the
        # src grid points.
        grid = global_pp()[:4, :5]
        grid.coord('longitude').points = np.linspace(-3.182, -3.06, 5)
        grid.coord('latitude').points = np.linspace(52.372, 52.44, 4)
        return grid

    def test_reversed(self):
        src = self.src
        grid = self._grid_subset()
        result = regrid(src, grid[::-1])
        self.assertCMLApproxData(result[:, :, ::-1],
                                 RESULT_DIR + ('subset.cml',))
        result = regrid(src[:, :, ::-1], grid[::-1])
        self.assertCMLApproxData(result[:, :, ::-1],
                                 RESULT_DIR + ('subset.cml',))
        result = regrid(src[:, :, :, ::-1], grid[::-1])
        self.assertCMLApproxData(result[:, :, ::-1],
                                 RESULT_DIR + ('subset.cml',))
        result = regrid(src[:, :, ::-1, ::-1], grid[::-1])
        self.assertCMLApproxData(result[:, :, ::-1],
                                 RESULT_DIR + ('subset.cml',))

        result = regrid(src, grid[:, ::-1])
        self.assertCMLApproxData(result[:, :, :, ::-1],
                                 RESULT_DIR + ('subset.cml',))
        result = regrid(src[:, :, ::-1], grid[:, ::-1])
        self.assertCMLApproxData(result[:, :, :, ::-1],
                                 RESULT_DIR + ('subset.cml',))
        result = regrid(src[:, :, :, ::-1], grid[:, ::-1])
        self.assertCMLApproxData(result[:, :, :, ::-1],
                                 RESULT_DIR + ('subset.cml',))
        result = regrid(src[:, :, ::-1, ::-1], grid[:, ::-1])
        self.assertCMLApproxData(result[:, :, :, ::-1],
                                 RESULT_DIR + ('subset.cml',))

        result = regrid(src, grid[::-1, ::-1])
        self.assertCMLApproxData(result[:, :, ::-1, ::-1],
                                 RESULT_DIR + ('subset.cml',))
        result = regrid(src[:, :, ::-1], grid[::-1, ::-1])
        self.assertCMLApproxData(result[:, :, ::-1, ::-1],
                                 RESULT_DIR + ('subset.cml',))
        result = regrid(src[:, :, :, ::-1], grid[::-1, ::-1])
        self.assertCMLApproxData(result[:, :, ::-1, ::-1],
                                 RESULT_DIR + ('subset.cml',))
        result = regrid(src[:, :, ::-1, ::-1], grid[::-1, ::-1])
        self.assertCMLApproxData(result[:, :, ::-1, ::-1],
                                 RESULT_DIR + ('subset.cml',))

    def test_grid_subset(self):
        # The destination grid points are entirely contained within the
        # src grid points.
        grid = self._grid_subset()
        result = regrid(self.src, grid)
        self.assertCMLApproxData(result, RESULT_DIR + ('subset.cml',))

    def _big_grid(self):
        grid = self._grid_subset()
        big_grid = Cube(np.zeros((5, 10, 3, 4, 5)))
        big_grid.add_dim_coord(grid.coord('latitude'), 3)
        big_grid.add_dim_coord(grid.coord('longitude'), 4)
        return big_grid

    def test_grid_subset_big(self):
        # Add some extra dimensions to the destination Cube and
        # these should be safely ignored.
        big_grid = self._big_grid()
        result = regrid(self.src, big_grid)
        self.assertCMLApproxData(result, RESULT_DIR + ('subset.cml',))

    def test_grid_subset_big_transposed(self):
        # The order of the grid's dimensions (including the X and Y
        # dimensions) must not affect the result.
        big_grid = self._big_grid()
        big_grid.transpose([4, 0, 3, 1, 2])
        result = regrid(self.src, big_grid)
        self.assertCMLApproxData(result, RESULT_DIR + ('subset.cml',))

    def test_grid_subset_anon(self):
        # Must cope OK with anonymous source dimensions.
        src = self.src
        src.remove_coord('time')
        grid = self._grid_subset()
        result = regrid(src, grid)
        self.assertCMLApproxData(result, RESULT_DIR + ('subset_anon.cml',))

    def test_grid_subset_missing_data_1(self):
        # The destination grid points are entirely contained within the
        # src grid points AND we have missing data.
        src = self.src
        src.data = np.ma.MaskedArray(src.data)
        src.data[:, :, 0, 0] = np.ma.masked
        grid = self._grid_subset()
        result = regrid(src, grid)
        self.assertCMLApproxData(result, RESULT_DIR + ('subset_masked_1.cml',))

    def test_grid_subset_missing_data_2(self):
        # The destination grid points are entirely contained within the
        # src grid points AND we have missing data.
        src = self.src
        src.data = np.ma.MaskedArray(src.data)
        src.data[:, :, 1, 2] = np.ma.masked
        grid = self._grid_subset()
        result = regrid(src, grid)
        self.assertCMLApproxData(result, RESULT_DIR + ('subset_masked_2.cml',))

    def test_grid_partial_overlap(self):
        # The destination grid points are partially contained within the
        # src grid points.
        grid = global_pp()[:4, :4]
        grid.coord('longitude').points = np.linspace(-3.3, -3.06, 4)
        grid.coord('latitude').points = np.linspace(52.377, 52.43, 4)
        result = regrid(self.src, grid)
        self.assertCMLApproxData(result, RESULT_DIR + ('partial_overlap.cml',))

    def test_grid_no_overlap(self):
        # The destination grid points are NOT contained within the
        # src grid points.
        grid = global_pp()[:4, :4]
        grid.coord('longitude').points = np.linspace(-3.3, -3.2, 4)
        grid.coord('latitude').points = np.linspace(52.377, 52.43, 4)
        result = regrid(self.src, grid)
        self.assertCMLApproxData(result, RESULT_DIR + ('no_overlap.cml',))

    def test_grid_subset_missing_data_aux(self):
        # The destination grid points are entirely contained within the
        # src grid points AND we have missing data on the aux coordinate.
        src = self.src
        src.coord('surface_altitude').points[1, 2] = np.ma.masked
        grid = self._grid_subset()
        result = regrid(src, grid)
        self.assertCMLApproxData(result, RESULT_DIR + ('masked_altitude.cml',))


class TestNOP(tests.IrisTest):
    def test_nop(self):
        # The destination grid points are exactly the same as the
        # src grid points.
        src = realistic_4d()[:5, :2, ::40, ::30]
        grid = src.copy()
        result = regrid(src, grid)
        self.assertEqual(result, src)


@tests.skip_data
class TestCircular(tests.IrisTest):
    def setUp(self):
        src = global_pp()[::10, ::10]
        level_height = AuxCoord(0, long_name='level_height', units='m',
                                attributes={'positive': 'up'})
        sigma = AuxCoord(1, long_name='sigma')
        surface_altitude = AuxCoord((src.data - src.data.min()) * 50,
                                    'surface_altitude', units='m')
        src.add_aux_coord(level_height)
        src.add_aux_coord(sigma)
        src.add_aux_coord(surface_altitude, [0, 1])
        hybrid_height = HybridHeightFactory(level_height, sigma,
                                            surface_altitude)
        src.add_aux_factory(hybrid_height)
        self.src = src

        grid = global_pp()[:4, :4]
        grid.coord('longitude').points = grid.coord('longitude').points - 5
        self.grid = grid

    def test_non_circular(self):
        # Non-circular src -> non-circular grid
        result = regrid(self.src, self.grid)
        self.assertFalse(result.coord('longitude').circular)
        self.assertCMLApproxData(result, RESULT_DIR + ('non_circular.cml',))

    def test_circular_src(self):
        # Circular src -> non-circular grid
        src = self.src
        src.coord('longitude').circular = True
        result = regrid(src, self.grid)
        self.assertFalse(result.coord('longitude').circular)
        self.assertCMLApproxData(result, RESULT_DIR + ('circular_src.cml',))

    def test_circular_grid(self):
        # Non-circular src -> circular grid
        grid = self.grid
        grid.coord('longitude').circular = True
        result = regrid(self.src, grid)
        self.assertTrue(result.coord('longitude').circular)
        self.assertCMLApproxData(result, RESULT_DIR + ('circular_grid.cml',))

    def test_circular_src_and_grid(self):
        # Circular src -> circular grid
        src = self.src
        src.coord('longitude').circular = True
        grid = self.grid
        grid.coord('longitude').circular = True
        result = regrid(src, grid)
        self.assertTrue(result.coord('longitude').circular)
        self.assertCMLApproxData(result, RESULT_DIR + ('both_circular.cml',))


@tests.skip_data
@tests.skip_plot
class TestVisual(tests.GraphicsTest):
    def test_osgb_to_latlon(self):
        path = tests.get_data_path(
            ('NIMROD', 'uk2km', 'WO0000000003452',
             '201007020900_u1096_ng_ey00_visibility0180_screen_2km'))
        src = iris.load_cube(path)[0]
        src.data = src.data.astype(np.float32)
        grid = Cube(np.empty((73, 96)))
        cs = GeogCS(6370000)
        lat = DimCoord(np.linspace(46, 65, 73), 'latitude', units='degrees',
                       coord_system=cs)
        lon = DimCoord(np.linspace(-14, 8, 96), 'longitude', units='degrees',
                       coord_system=cs)
        grid.add_dim_coord(lat, 0)
        grid.add_dim_coord(lon, 1)
        result = regrid(src, grid)
        qplt.pcolor(result, antialiased=False)
        qplt.plt.gca().coastlines()
        self.check_graphic()

    def test_subsample(self):
        src = global_pp()
        grid = src[::2, ::3]
        result = regrid(src, grid)
        qplt.pcolormesh(result)
        qplt.plt.gca().coastlines()
        self.check_graphic()


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_regrid_conservative_via_esmpy
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Tests for :func:`iris.experimental.regrid.regrid_conservative_via_esmpy`.

"""

# import iris tests first so that some things can be initialised
# before importing anything else.
import iris.tests as tests

import contextlib
import os
import unittest

import numpy as np

# Import ESMF if installed, else fail quietly + disable all the tests.
try:
    import ESMF
    # Check it *is* the real module, and not an iris.proxy FakeModule.
    ESMF.Manager
except ImportError, AttributeError:
    ESMF = None
skip_esmf = unittest.skipIf(
    condition=ESMF is None,
    reason='Requires ESMF, which is not available.')

import iris
import iris.analysis
import iris.analysis.cartography as i_cartog
from iris.experimental.regrid_conservative import \
    regrid_conservative_via_esmpy
import iris.tests.stock as istk


_PLAIN_GEODETIC_CS = iris.coord_systems.GeogCS(
    i_cartog.DEFAULT_SPHERICAL_EARTH_RADIUS)


def _make_test_cube(shape, xlims, ylims, pole_latlon=None):
    """
    Create latlon cube (optionally rotated) with given xy dimensions and bounds
    limit values.

    Produces a regular grid in source coordinates.
    Does not work for 1xN or Nx1 grids, because guess_bounds fails.

    """
    nx, ny = shape
    cube = iris.cube.Cube(np.zeros((ny, nx)))
    xvals = np.linspace(xlims[0], xlims[1], nx)
    yvals = np.linspace(ylims[0], ylims[1], ny)
    coordname_prefix = ''
    cs = _PLAIN_GEODETIC_CS
    if pole_latlon is not None:
        coordname_prefix = 'grid_'
        pole_lat, pole_lon = pole_latlon
        cs = iris.coord_systems.RotatedGeogCS(
            grid_north_pole_latitude=pole_lat,
            grid_north_pole_longitude=pole_lon,
            ellipsoid=cs)

    co_x = iris.coords.DimCoord(xvals,
                                standard_name=coordname_prefix + 'longitude',
                                units=iris.unit.Unit('degrees'),
                                coord_system=cs)
    co_x.guess_bounds()
    cube.add_dim_coord(co_x, 1)
    co_y = iris.coords.DimCoord(yvals,
                                standard_name=coordname_prefix + 'latitude',
                                units=iris.unit.Unit('degrees'),
                                coord_system=cs)
    co_y.guess_bounds()
    cube.add_dim_coord(co_y, 0)
    return cube


def _cube_area_sum(cube):
    """Calculate total area-sum - Iris can't do this in one operation."""
    area_sums = cube * i_cartog.area_weights(cube, normalize=False)
    area_sum = area_sums.collapsed(area_sums.coords(dim_coords=True),
                                   iris.analysis.SUM)
    return area_sum.data.flatten()[0]


def _reldiff(a, b):
    """
    Compute a relative-difference measure between real numbers.

    Result is:
        if a == b == 0:
            0.0
        otherwise:
            |a - b| / mean(|a|, |b|)

    """
    if a == 0.0 and b == 0.0:
        return 0.0
    return abs(a - b) * 2.0 / (abs(a) + abs(b))


def _minmax(v):
    """Calculate [min, max] of input."""
    return [f(v) for f in (np.min, np.max)]


@contextlib.contextmanager
def _donothing_context_manager():
    yield


@skip_esmf
class TestConservativeRegrid(tests.IrisTest):
    @classmethod
    def setUpClass(cls):
        # Pre-initialise ESMF, just to avoid warnings about no logfile.
        # NOTE: noisy if logging is off, and no control of filepath.  Boo!!
        if ESMF is not None:
            # WARNING: nosetest calls class setUp/tearDown even when "skipped".
            cls._emsf_logfile_path = os.path.join(os.getcwd(), 'ESMF_LogFile')
            ESMF.Manager(logkind=ESMF.LogKind.SINGLE, debug=False)

    @classmethod
    def tearDownClass(cls):
        # remove the logfile if we can, just to be tidy
        if ESMF is not None:
            # WARNING: nosetest calls class setUp/tearDown even when "skipped".
            if os.path.exists(cls._emsf_logfile_path):
                os.remove(cls._emsf_logfile_path)

    def setUp(self):
        # Compute basic test data cubes.
        shape1 = (5, 5)
        xlims1, ylims1 = ((-2, 2), (-2, 2))
        c1 = _make_test_cube(shape1, xlims1, ylims1)
        c1.data[:] = 0.0
        c1.data[2, 2] = 1.0

        shape2 = (4, 4)
        xlims2, ylims2 = ((-1.5, 1.5), (-1.5, 1.5))
        c2 = _make_test_cube(shape2, xlims2, ylims2)
        c2.data[:] = 0.0

        # Save timesaving pre-computed bits
        self.stock_c1_c2 = (c1, c2)
        self.stock_regrid_c1toc2 = regrid_conservative_via_esmpy(c1, c2)
        self.stock_c1_areasum = _cube_area_sum(c1)

    def test_simple_areas(self):
        """
        Test area-conserving regrid between simple "near-square" grids.

        Grids have overlapping areas in the same (lat-lon) coordinate system.
        Grids are "nearly flat" lat-lon spaces (small ranges near the equator).

        """
        c1, c2 = self.stock_c1_c2
        c1_areasum = self.stock_c1_areasum

        # main regrid
        c1to2 = regrid_conservative_via_esmpy(c1, c2)

        c1to2_areasum = _cube_area_sum(c1to2)

        # Check expected result (Cartesian equivalent, so not exact).
        d_expect = np.array([[0.00, 0.00, 0.00, 0.00],
                             [0.00, 0.25, 0.25, 0.00],
                             [0.00, 0.25, 0.25, 0.00],
                             [0.00, 0.00, 0.00, 0.00]])
        # Numbers are slightly off (~0.25000952).  This is expected.
        self.assertArrayAllClose(c1to2.data, d_expect, rtol=5.0e-5)

        # check that the area sums are equivalent, simple total is a bit off
        self.assertArrayAllClose(c1to2_areasum, c1_areasum)

        #
        # regrid back onto original grid again ...
        #
        c1to2to1 = regrid_conservative_via_esmpy(c1to2, c1)

        c1to2to1_areasum = _cube_area_sum(c1to2to1)

        # Check expected result (Cartesian/exact difference now greater)
        d_expect = np.array([[0.0, 0.0000, 0.0000, 0.0000, 0.0],
                             [0.0, 0.0625, 0.1250, 0.0625, 0.0],
                             [0.0, 0.1250, 0.2500, 0.1250, 0.0],
                             [0.0, 0.0625, 0.1250, 0.0625, 0.0],
                             [0.0, 0.0000, 0.0000, 0.0000, 0.0]])
        self.assertArrayAllClose(c1to2to1.data, d_expect, atol=0.00002)

        # check area sums again
        self.assertArrayAllClose(c1to2to1_areasum, c1_areasum)

    def test_simple_missing_data(self):
        """
        Check for missing data handling.

        Should mask cells that either ..
          (a) go partly outside the source grid
          (b) partially overlap masked source data

        """
        c1, c2 = self.stock_c1_c2

        # regrid from c2 to c1 -- should mask all the edges...
        c2_to_c1 = regrid_conservative_via_esmpy(c2, c1)
        self.assertArrayEqual(c2_to_c1.data.mask,
                              [[True, True, True, True, True],
                               [True, False, False, False, True],
                               [True, False, False, False, True],
                               [True, False, False, False, True],
                               [True, True, True, True, True]])

        # do same with a particular point masked
        c2m = c2.copy()
        c2m.data = np.ma.array(c2m.data)
        c2m.data[1, 1] = np.ma.masked
        c2m_to_c1 = regrid_conservative_via_esmpy(c2m, c1)
        self.assertArrayEqual(c2m_to_c1.data.mask,
                              [[True, True, True, True, True],
                               [True, True, True, False, True],
                               [True, True, True, False, True],
                               [True, False, False, False, True],
                               [True, True, True, True, True]])

    def test_multidimensional(self):
        """
        Check valid operation on a multidimensional cube.

        Calculation should repeat across multiple dimensions.
        Any attached orography is interpolated.

        NOTE: in future, extra dimensions may be passed through to ESMF:  At
        present, it repeats the calculation on 2d slices.  So we check that
        at least the results are equivalent (as it's quite easy to do).

        """
        # Get some higher-dimensional test data
        c1 = istk.realistic_4d()
        # Chop down to small size, and mask some data
        c1 = c1[:3, :4, :16, :12]
        c1.data[:, 2, :, :] = np.ma.masked
        c1.data[1, 1, 3:9, 4:7] = np.ma.masked
        # Give it a slightly more challenging indexing order: tzyx --> xzty
        c1.transpose((3, 1, 0, 2))

        # Construct a (coarser) target grid of about the same extent
        c1_cs = c1.coord(axis='x').coord_system
        xlims = _minmax(c1.coord(axis='x').contiguous_bounds())
        ylims = _minmax(c1.coord(axis='y').contiguous_bounds())
        # Reduce the dimensions slightly to avoid NaNs in regridded orography
        delta = 0.05
        # || NOTE: this is *not* a small amount.  Think there is a bug.
        # || NOTE: See https://github.com/SciTools/iris/issues/458
        xlims = np.interp([delta, 1.0 - delta], [0, 1], xlims)
        ylims = np.interp([delta, 1.0 - delta], [0, 1], ylims)
        pole_latlon = (c1_cs.grid_north_pole_latitude,
                       c1_cs.grid_north_pole_longitude)
        c2 = _make_test_cube((7, 8), xlims, ylims, pole_latlon=pole_latlon)

        # regrid onto new grid
        c1_to_c2 = regrid_conservative_via_esmpy(c1, c2)

        # check that all the original coords exist in the new cube
        # NOTE: this also effectively confirms we haven't lost the orography
        def list_coord_names(cube):
            return sorted([coord.name() for coord in cube.coords()])

        self.assertEqual(list_coord_names(c1_to_c2), list_coord_names(c1))

        # check that each xy 'slice' has same values as if done on its own.
        for i_p, i_t in np.ndindex(c1.shape[1:3]):
            c1_slice = c1[:, i_p, i_t]
            c2_slice = regrid_conservative_via_esmpy(c1_slice, c2)
            subcube = c1_to_c2[:, i_p, i_t]
            self.assertEqual(subcube, c2_slice)

        # check all other metadata
        self.assertEqual(c1_to_c2.metadata, c1.metadata)

    def test_xy_transposed(self):
        # Test effects of transposing X and Y in src/dst data.
        c1, c2 = self.stock_c1_c2
        testcube_xy = self.stock_regrid_c1toc2

        # Check that transposed data produces transposed results
        # - i.e.  regrid(data^T)^T == regrid(data)
        c1_yx = c1.copy()
        c1_yx.transpose()
        testcube_yx = regrid_conservative_via_esmpy(c1_yx, c2)
        testcube_yx.transpose()
        self.assertEqual(testcube_yx, testcube_xy)

        # Check that transposing destination does nothing
        c2_yx = c2.copy()
        c2_yx.transpose()
        testcube_dst_transpose = regrid_conservative_via_esmpy(c1, c2_yx)
        self.assertEqual(testcube_dst_transpose, testcube_xy)

    def test_same_grid(self):
        # Test regridding onto the identical grid.
        # Use regrid with self as target.
        c1, _ = self.stock_c1_c2
        testcube = regrid_conservative_via_esmpy(c1, c1)
        self.assertEqual(testcube, c1)

    def test_global(self):
        # Test global regridding.
        # Compute basic test data cubes.
        shape1 = (8, 6)
        xlim1 = 180.0 * (shape1[0] - 1) / shape1[0]
        ylim1 = 90.0 * (shape1[1] - 1) / shape1[1]
        c1 = _make_test_cube(shape1, (-xlim1, xlim1), (-ylim1, ylim1))
        # Create a small, plausible global array:
        # - top + bottom rows all the same
        # - left + right columns "mostly close" for checking across the seam
        basedata = np.array(
            [[1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 4, 4, 4, 2, 2, 1],
             [2, 1, 4, 4, 4, 2, 2, 2],
             [2, 5, 5, 1, 1, 1, 5, 5],
             [5, 5, 5, 1, 1, 1, 5, 5],
             [5, 5, 5, 5, 5, 5, 5, 5]])
        c1.data[:] = basedata

        # Create a rotated grid to regrid this onto.
        shape2 = (14, 11)
        xlim2 = 180.0 * (shape2[0] - 1) / shape2[0]
        ylim2 = 90.0 * (shape2[1] - 1) / shape2[1]
        c2 = _make_test_cube(shape2, (-xlim2, xlim2), (-ylim2, ylim2),
                             pole_latlon=(47.4, 25.7))

        # Perform regridding
        c1toc2 = regrid_conservative_via_esmpy(c1, c2)

        # Check that before+after area-sums match fairly well
        c1_areasum = _cube_area_sum(c1)
        c1toc2_areasum = _cube_area_sum(c1toc2)
        self.assertArrayAllClose(c1toc2_areasum, c1_areasum, rtol=0.006)

    def test_global_collapse(self):
        # Test regridding global data to a single cell.
        # Fetch 'standard' testcube data
        c1, _ = self.stock_c1_c2
        c1_areasum = self.stock_c1_areasum

        # Condense entire globe onto a single cell
        x_coord_2 = iris.coords.DimCoord([0.0], bounds=[-180.0, 180.0],
                                         standard_name='longitude',
                                         units='degrees',
                                         coord_system=_PLAIN_GEODETIC_CS)
        y_coord_2 = iris.coords.DimCoord([0.0], bounds=[-90.0, 90.0],
                                         standard_name='latitude',
                                         units='degrees',
                                         coord_system=_PLAIN_GEODETIC_CS)
        c2 = iris.cube.Cube([[0.0]])
        c2.add_dim_coord(y_coord_2, 0)
        c2.add_dim_coord(x_coord_2, 1)

        # NOTE: at present, this causes an error inside ESMF ...
        context = self.assertRaises(NameError)
        global_cell_supported = False
        if global_cell_supported:
            context = _donothing_context_manager()
        with context:
            c1_to_global = regrid_conservative_via_esmpy(c1, c2)
            # Check the total area sum is still the same
            self.assertArrayAllClose(c1_to_global.data[0, 0], c1_areasum)

    def test_single_cells(self):
        # Test handling of single-cell grids.
        # Fetch 'standard' testcube data
        c1, c2 = self.stock_c1_c2
        c1_areasum = self.stock_c1_areasum

        #
        # At present NxN -> 1x1 "in-place" doesn't seem to work properly
        # - result cell has missing-data ?
        #
        # Condense entire region into a single cell in the c1 grid
        xlims1 = _minmax(c1.coord(axis='x').bounds)
        ylims1 = _minmax(c1.coord(axis='y').bounds)
        x_c1x1 = iris.coords.DimCoord(xlims1[0], bounds=xlims1,
                                      standard_name='longitude',
                                      units='degrees',
                                      coord_system=_PLAIN_GEODETIC_CS)
        y_c1x1 = iris.coords.DimCoord(ylims1[0], bounds=ylims1,
                                      standard_name='latitude',
                                      units='degrees',
                                      coord_system=_PLAIN_GEODETIC_CS)
        c1x1_gridcube = iris.cube.Cube([[0.0]])
        c1x1_gridcube.add_dim_coord(y_c1x1, 0)
        c1x1_gridcube.add_dim_coord(x_c1x1, 1)
        c1x1 = regrid_conservative_via_esmpy(c1, c1x1_gridcube)
        c1x1_areasum = _cube_area_sum(c1x1)
        # Check the total area sum is still the same
        condense_to_1x1_supported = False
        # NOTE: currently disabled (ESMF gets this wrong)
        # NOTE ALSO: call hits numpy 1.7 bug in testing.assert_array_compare.
        if condense_to_1x1_supported:
            self.assertArrayAllClose(c1x1_areasum, c1_areasum)

        # Condense entire region onto a single cell covering the area of 'c2'
        xlims2 = _minmax(c2.coord(axis='x').bounds)
        ylims2 = _minmax(c2.coord(axis='y').bounds)
        x_c2x1 = iris.coords.DimCoord(xlims2[0], bounds=xlims2,
                                      standard_name='longitude',
                                      units=iris.unit.Unit('degrees'),
                                      coord_system=_PLAIN_GEODETIC_CS)
        y_c2x1 = iris.coords.DimCoord(ylims2[0], bounds=ylims2,
                                      standard_name='latitude',
                                      units=iris.unit.Unit('degrees'),
                                      coord_system=_PLAIN_GEODETIC_CS)
        c2x1_gridcube = iris.cube.Cube([[0.0]])
        c2x1_gridcube.add_dim_coord(y_c2x1, 0)
        c2x1_gridcube.add_dim_coord(x_c2x1, 1)
        c1_to_c2x1 = regrid_conservative_via_esmpy(c1, c2x1_gridcube)

        # Check the total area sum is still the same
        c1_to_c2x1_areasum = _cube_area_sum(c1_to_c2x1)
        self.assertArrayAllClose(c1_to_c2x1_areasum, c1_areasum, 0.0004)

        # 1x1 -> NxN : regrid single cell to NxN grid
        # construct a single-cell approximation to 'c1' with the same area sum.
        # NOTE: can't use _make_cube (see docstring)
        c1x1 = c1.copy()[0:1, 0:1]
        xlims1 = _minmax(c1.coord(axis='x').bounds)
        ylims1 = _minmax(c1.coord(axis='y').bounds)
        c1x1.coord(axis='x').bounds = xlims1
        c1x1.coord(axis='y').bounds = ylims1
        # Assign data mean as single cell value : Maybe not exact, but "close"
        c1x1.data[0, 0] = np.mean(c1.data)

        # Regrid this back onto the original NxN grid
        c1x1_to_c1 = regrid_conservative_via_esmpy(c1x1, c1)
        c1x1_to_c1_areasum = _cube_area_sum(c1x1_to_c1)

        # Check that area sum is ~unchanged, as expected
        self.assertArrayAllClose(c1x1_to_c1_areasum, c1_areasum, 0.0004)

        # Check 1x1 -> 1x1
        # NOTE: can *only* get any result with a fully overlapping cell, so
        # just regrid onto self
        c1x1toself = regrid_conservative_via_esmpy(c1x1, c1x1)
        c1x1toself_areasum = _cube_area_sum(c1x1toself)
        self.assertArrayAllClose(c1x1toself_areasum, c1_areasum, 0.0004)
        # NOTE: perhaps surprisingly, this has a similar level of error.

    def test_longitude_wraps(self):
        """Check results are independent of where the grid 'seams' are."""
        # First repeat global regrid calculation from 'test_global'.
        shape1 = (8, 6)
        xlim1 = 180.0 * (shape1[0] - 1) / shape1[0]
        ylim1 = 90.0 * (shape1[1] - 1) / shape1[1]
        xlims1 = (-xlim1, xlim1)
        ylims1 = (-ylim1, ylim1)
        c1 = _make_test_cube(shape1, xlims1, ylims1)

        # Create a small, plausible global array (see test_global).
        basedata = np.array(
            [[1, 1, 1, 1, 1, 1, 1, 1],
             [1, 1, 4, 4, 4, 2, 2, 1],
             [2, 1, 4, 4, 4, 2, 2, 2],
             [2, 5, 5, 1, 1, 1, 5, 5],
             [5, 5, 5, 1, 1, 1, 5, 5],
             [5, 5, 5, 5, 5, 5, 5, 5]])
        c1.data[:] = basedata

        shape2 = (14, 11)
        xlim2 = 180.0 * (shape2[0] - 1) / shape2[0]
        ylim2 = 90.0 * (shape2[1] - 1) / shape2[1]
        xlims_2 = (-xlim2, xlim2)
        ylims_2 = (-ylim2, ylim2)
        c2 = _make_test_cube(shape2, xlims_2, ylims_2,
                             pole_latlon=(47.4, 25.7))

        # Perform regridding
        c1toc2 = regrid_conservative_via_esmpy(c1, c2)

        # Now redo with dst longitudes rotated, so 'seam' is somewhere else.
        x2_shift_steps = int(shape2[0] / 3)
        xlims2_shifted = np.array(xlims_2) + 360.0 * x2_shift_steps / shape2[0]
        c2_shifted = _make_test_cube(shape2, xlims2_shifted, ylims_2,
                                     pole_latlon=(47.4, 25.7))
        c1toc2_shifted = regrid_conservative_via_esmpy(c1, c2_shifted)

        # Show that results are the same, when output rolled by same amount
        rolled_data = np.roll(c1toc2_shifted.data, x2_shift_steps, axis=1)
        self.assertArrayAllClose(rolled_data, c1toc2.data)

        # Repeat with rolled *source* data : result should be identical
        x1_shift_steps = int(shape1[0] / 3)
        x_shift_degrees = 360.0 * x1_shift_steps / shape1[0]
        xlims1_shifted = [x - x_shift_degrees for x in xlims1]
        c1_shifted = _make_test_cube(shape1, xlims1_shifted, ylims1)
        c1_shifted.data[:] = np.roll(basedata, x1_shift_steps, axis=1)
        c1shifted_toc2 = regrid_conservative_via_esmpy(c1_shifted, c2)
        self.assertEqual(c1shifted_toc2, c1toc2)

    def test_polar_areas(self):
        """
        Test area-conserving regrid between different grids.

        Grids have overlapping areas in the same (lat-lon) coordinate system.
        Cells are highly non-square (near the pole).

        """
        # Like test_basic_area, but not symmetrical + bigger overall errors.
        shape1 = (5, 5)
        xlims1, ylims1 = ((-2, 2), (84, 88))
        c1 = _make_test_cube(shape1, xlims1, ylims1)
        c1.data[:] = 0.0
        c1.data[2, 2] = 1.0
        c1_areasum = _cube_area_sum(c1)

        shape2 = (4, 4)
        xlims2, ylims2 = ((-1.5, 1.5), (84.5, 87.5))
        c2 = _make_test_cube(shape2, xlims2, ylims2)
        c2.data[:] = 0.0

        c1to2 = regrid_conservative_via_esmpy(c1, c2)

        # check for expected pattern
        d_expect = np.array([[0.0, 0.0, 0.0, 0.0],
                             [0.0, 0.23614, 0.23614, 0.0],
                             [0.0, 0.26784, 0.26784, 0.0],
                             [0.0, 0.0, 0.0, 0.0]])
        self.assertArrayAllClose(c1to2.data, d_expect, rtol=5.0e-5)

        # check sums
        c1to2_areasum = _cube_area_sum(c1to2)
        self.assertArrayAllClose(c1to2_areasum, c1_areasum)

        #
        # transform back again ...
        #
        c1to2to1 = regrid_conservative_via_esmpy(c1to2, c1)

        # check values
        d_expect = np.array([[0.0, 0.0, 0.0, 0.0, 0.0],
                             [0.0, 0.056091, 0.112181, 0.056091, 0.0],
                             [0.0, 0.125499, 0.250998, 0.125499, 0.0],
                             [0.0, 0.072534, 0.145067, 0.072534, 0.0],
                             [0.0, 0.0, 0.0, 0.0, 0.0]])
        self.assertArrayAllClose(c1to2to1.data, d_expect, atol=0.0005)

        # check sums
        c1to2to1_areasum = _cube_area_sum(c1to2to1)
        self.assertArrayAllClose(c1to2to1_areasum, c1_areasum)

    def test_fail_no_cs(self):
        # Test error when one coordinate has no coord_system.
        shape1 = (5, 5)
        xlims1, ylims1 = ((-2, 2), (-2, 2))
        c1 = _make_test_cube(shape1, xlims1, ylims1)
        c1.data[:] = 0.0
        c1.data[2, 2] = 1.0

        shape2 = (4, 4)
        xlims2, ylims2 = ((-1.5, 1.5), (-1.5, 1.5))
        c2 = _make_test_cube(shape2, xlims2, ylims2)
        c2.data[:] = 0.0
        c2.coord('latitude').coord_system = None

        with self.assertRaises(ValueError):
            regrid_conservative_via_esmpy(c1, c2)

    def test_fail_different_cs(self):
        # Test error when either src or dst coords have different
        # coord_systems.
        shape1 = (5, 5)
        xlims1, ylims1 = ((-2, 2), (-2, 2))
        shape2 = (4, 4)
        xlims2, ylims2 = ((-1.5, 1.5), (-1.5, 1.5))

        # Check basic regrid between these is ok.
        c1 = _make_test_cube(shape1, xlims1, ylims1,
                             pole_latlon=(45.0, 35.0))
        c2 = _make_test_cube(shape2, xlims2, ylims2)
        regrid_conservative_via_esmpy(c1, c2)

        # Replace the coord_system one of the source coords + check this fails.
        c1.coord('grid_longitude').coord_system = \
            c2.coord('longitude').coord_system
        with self.assertRaises(ValueError):
            regrid_conservative_via_esmpy(c1, c2)

        # Repeat with target coordinate fiddled.
        c1 = _make_test_cube(shape1, xlims1, ylims1,
                             pole_latlon=(45.0, 35.0))
        c2 = _make_test_cube(shape2, xlims2, ylims2)
        c2.coord('latitude').coord_system = \
            c1.coord('grid_latitude').coord_system
        with self.assertRaises(ValueError):
            regrid_conservative_via_esmpy(c1, c2)

    def test_rotated(self):
        """
        Test area-weighted regrid on more complex area.

        Use two mutually rotated grids, of similar area + same dims.
        Only a small central region in each is non-zero, which maps entirely
        inside the other region.
        So the area-sum totals should match exactly.

        """
        # create source test cube on rotated form
        pole_lat = 53.4
        pole_lon = -173.2
        deg_swing = 35.3
        pole_lon += deg_swing
        c1_nx = 9 + 6
        c1_ny = 7 + 6
        c1_xlims = -60.0, 60.0
        c1_ylims = -45.0, 20.0
        c1_xlims = [x - deg_swing for x in c1_xlims]
        c1 = _make_test_cube((c1_nx, c1_ny), c1_xlims, c1_ylims,
                             pole_latlon=(pole_lat, pole_lon))
        c1.data[3:-3, 3:-3] = np.array([
            [100, 100, 100, 100, 100, 100, 100, 100, 100],
            [100, 100, 100, 100, 100, 100, 100, 100, 100],
            [100, 100, 199, 199, 199, 199, 100, 100, 100],
            [100, 100, 100, 100, 199, 199, 100, 100, 100],
            [100, 100, 100, 100, 199, 199, 199, 100, 100],
            [100, 100, 100, 100, 100, 100, 100, 100, 100],
            [100, 100, 100, 100, 100, 100, 100, 100, 100]],
            dtype=np.float)

        c1_areasum = _cube_area_sum(c1)

        # construct target cube to receive
        nx2 = 9 + 6
        ny2 = 7 + 6
        c2_xlims = -100.0, 120.0
        c2_ylims = -20.0, 50.0
        c2 = _make_test_cube((nx2, ny2), c2_xlims, c2_ylims)
        c2.data = np.ma.array(c2.data, mask=True)

        # perform regrid
        c1to2 = regrid_conservative_via_esmpy(c1, c2)

        # check we have zeros (or nearly) all around the edge..
        c1toc2_zeros = np.ma.array(c1to2.data)
        c1toc2_zeros[c1toc2_zeros.mask] = 0.0
        c1toc2_zeros = np.abs(c1toc2_zeros.mask) < 1.0e-6
        self.assertArrayEqual(c1toc2_zeros[0, :], True)
        self.assertArrayEqual(c1toc2_zeros[-1, :], True)
        self.assertArrayEqual(c1toc2_zeros[:, 0], True)
        self.assertArrayEqual(c1toc2_zeros[:, -1], True)

        # check the area-sum operation
        c1to2_areasum = _cube_area_sum(c1to2)
        self.assertArrayAllClose(c1to2_areasum, c1_areasum, rtol=0.004)

        #
        # Now repeat, transforming backwards ...
        #
        c1.data = np.ma.array(c1.data, mask=True)
        c2.data[:] = 0.0
        c2.data[5:-5, 5:-5] = np.array([
            [199, 199, 199, 199, 100],
            [100, 100, 199, 199, 100],
            [100, 100, 199, 199, 199]],
            dtype=np.float)
        c2_areasum = _cube_area_sum(c2)

        c2toc1 = regrid_conservative_via_esmpy(c2, c1)

        # check we have zeros (or nearly) all around the edge..
        c2toc1_zeros = np.ma.array(c2toc1.data)
        c2toc1_zeros[c2toc1_zeros.mask] = 0.0
        c2toc1_zeros = np.abs(c2toc1_zeros.mask) < 1.0e-6
        self.assertArrayEqual(c2toc1_zeros[0, :], True)
        self.assertArrayEqual(c2toc1_zeros[-1, :], True)
        self.assertArrayEqual(c2toc1_zeros[:, 0], True)
        self.assertArrayEqual(c2toc1_zeros[:, -1], True)

        # check the area-sum operation
        c2toc1_areasum = _cube_area_sum(c2toc1)
        self.assertArrayAllClose(c2toc1_areasum, c2_areasum, rtol=0.004)

    def test_missing_data_rotated(self):
        """
        Check missing-data handling between different coordinate systems.

        Regrid between mutually rotated lat/lon systems, and check results for
        missing data due to grid edge overlap, and source-data masking.

        """
        for do_add_missing in (False, True):
            # create source test cube on rotated form
            pole_lat = 53.4
            pole_lon = -173.2
            deg_swing = 35.3
            pole_lon += deg_swing
            c1_nx = 9 + 6
            c1_ny = 7 + 6
            c1_xlims = -60.0, 60.0
            c1_ylims = -45.0, 20.0
            c1_xlims = [x - deg_swing for x in c1_xlims]
            c1 = _make_test_cube((c1_nx, c1_ny), c1_xlims, c1_ylims,
                                 pole_latlon=(pole_lat, pole_lon))
            c1.data = np.ma.array(c1.data, mask=False)
            c1.data[3:-3, 3:-3] = np.ma.array([
                [100, 100, 100, 100, 100, 100, 100, 100, 100],
                [100, 100, 100, 100, 100, 100, 100, 100, 100],
                [100, 100, 199, 199, 199, 199, 100, 100, 100],
                [100, 100, 100, 100, 199, 199, 100, 100, 100],
                [100, 100, 100, 100, 199, 199, 199, 100, 100],
                [100, 100, 100, 100, 100, 100, 100, 100, 100],
                [100, 100, 100, 100, 100, 100, 100, 100, 100]],
                dtype=np.float)

            if do_add_missing:
                c1.data = np.ma.array(c1.data)
                c1.data[7, 7] = np.ma.masked
                c1.data[3:5, 10:12] = np.ma.masked

            # construct target cube to receive
            nx2 = 9 + 6
            ny2 = 7 + 6
            c2_xlims = -80.0, 80.0
            c2_ylims = -20.0, 50.0
            c2 = _make_test_cube((nx2, ny2), c2_xlims, c2_ylims)
            c2.data = np.ma.array(c2.data, mask=True)

            # perform regrid + snapshot test results
            c1toc2 = regrid_conservative_via_esmpy(c1, c2)

            # check masking of result is as expected
            # (generated by inspecting plot of how src+dst grids overlap)
            expected_mask_valuemap = np.array(
                # KEY: 0=masked, 7=present, 5=masked with masked datapoints
                [[0, 0, 0, 0, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0, 0],
                 [0, 0, 0, 0, 7, 7, 7, 7, 0, 0, 0, 0, 0, 0, 0],
                 [0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 0, 0, 0, 0, 0],
                 [0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0, 0],
                 [0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 5, 5, 7, 0, 0],
                 [0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 5, 5, 7, 0, 0],
                 [0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 5, 5, 7, 0, 0],
                 [0, 0, 0, 7, 7, 7, 7, 5, 5, 7, 7, 7, 7, 0, 0],
                 [0, 0, 0, 0, 7, 7, 7, 5, 5, 7, 7, 7, 7, 0, 0],
                 [0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0],
                 [0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 7, 0],
                 [0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 7, 7, 0],
                 [0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7, 7, 0]])

            if do_add_missing:
                expected_mask = expected_mask_valuemap < 7
            else:
                expected_mask = expected_mask_valuemap == 0

            actual_mask = c1toc2.data.mask
            self.assertArrayEqual(actual_mask, expected_mask)

            if not do_add_missing:
                # check preservation of area-sums
                # NOTE: does *not* work with missing data, even theoretically,
                # as the 'missing areas' are not the same.
                c1_areasum = _cube_area_sum(c1)
                c1to2_areasum = _cube_area_sum(c1toc2)
                self.assertArrayAllClose(c1_areasum, c1to2_areasum, rtol=0.003)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_animate
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the animation of cubes within iris.

"""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import itertools

import numpy as np

import iris
from iris.coord_systems import GeogCS

# Run tests in no graphics mode if matplotlib is not available.
if tests.MPL_AVAILABLE:
    import iris.experimental.animate as animate
    import iris.plot as iplt


@tests.skip_plot
class IntegrationTest(tests.GraphicsTest):
    def setUp(self):
        cube = iris.cube.Cube(np.arange(36, dtype=np.int32).reshape((3, 3, 4)))
        cs = GeogCS(6371229)

        coord = iris.coords.DimCoord(
            points=np.array([1, 2, 3], dtype=np.int32), long_name='time')
        cube.add_dim_coord(coord, 0)

        coord = iris.coords.DimCoord(
            points=np.array([-1, 0, 1], dtype=np.int32),
            standard_name='latitude',
            units='degrees',
            coord_system=cs)
        cube.add_dim_coord(coord, 1)
        coord = iris.coords.DimCoord(
            points=np.array([-1, 0, 1, 2], dtype=np.int32),
            standard_name='longitude',
            units='degrees',
            coord_system=cs)
        cube.add_dim_coord(coord, 2)
        self.cube = cube

    def test_cube_animation(self):
        # This follows :meth:`~matplotlib.animation.FuncAnimation.save`
        # to ensure that each frame corresponds to known accepted frames for
        # the animation.
        cube_iter = self.cube.slices(('latitude', 'longitude'))

        ani = animate.animate(cube_iter, iplt.contourf)

        # Disconnect the first draw callback to stop the animation
        ani._fig.canvas.mpl_disconnect(ani._first_draw_id)

        ani = [ani]
        # Extract frame data
        for data in itertools.izip(*[a.new_saved_frame_seq() for a in ani]):
            # Draw each frame
            for anim, d in zip(ani, data):
                anim._draw_next_frame(d, blit=False)
                self.check_graphic()


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_raster
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
import iris.tests as tests
import iris.experimental.raster

import numpy as np
import PIL.Image


@tests.skip_data
class TestGeoTiffExport(tests.IrisTest):
    def check_tiff_header(self, geotiff_fh, reference_filename):
        """
        Checks the given tiff file handle's metadata matches the
        reference file contents.

        """
        im = PIL.Image.open(geotiff_fh)
        tiff_header = '\n'.join(str((tag, val))
                                for tag, val in sorted(im.tag.items()))

        reference_path = tests.get_result_path(reference_filename)

        self._check_same(tiff_header, reference_path, reference_filename,
                         type_comparison_name='Tiff header')

    def check_tiff(self, cube, tif_header):
        with self.temp_filename('.tif') as temp_filename:
            iris.experimental.raster.export_geotiff(cube, temp_filename)

            # Check the metadata is correct.
            with open(temp_filename) as fh:
                self.check_tiff_header(fh, ('experimental', 'raster',
                                            tif_header))

            # Ensure that north is at the top then check the data is correct.
            coord_y = cube.coord(axis='Y', dim_coords=True)
            data = cube.data
            if np.diff(coord_y.bounds[0]) > 0:
                data = cube.data[::-1, :]
            im = PIL.Image.open(temp_filename)
            im_data = np.array(im)
            # Currently we only support writing 32-bit tiff, when comparing
            # the data ensure that it is also 32-bit
            np.testing.assert_array_equal(im_data,
                                          data.astype(np.float32))

    def test_unmasked(self):
        tif_header = 'SMALL_total_column_co2.nc.tif_header.txt'
        fin = tests.get_data_path(('NetCDF', 'global', 'xyt',
                                   'SMALL_total_column_co2.nc'))
        cube = iris.load_cube(fin)[0]
        # PIL doesn't support float64
        cube.data = cube.data.astype('f4')

        # Ensure longitude values are continuous and monotonically increasing,
        # and discard the 'half cells' at the top and bottom of the UM output
        # by extracting a subset.
        east = iris.Constraint(longitude=lambda cell: cell < 180)
        non_edge = iris.Constraint(latitude=lambda cell: -90 < cell < 90)
        cube = cube.extract(east & non_edge)
        cube.coord('longitude').guess_bounds()
        cube.coord('latitude').guess_bounds()
        self.check_tiff(cube, tif_header)

        # Check again with the latitude coordinate (and the corresponding
        # cube.data) inverted. The output should be the same as before.
        coord = cube.coord('latitude')
        coord.points = coord.points[::-1]
        coord.bounds = None
        coord.guess_bounds()
        cube.data = cube.data[::-1, :]
        self.check_tiff(cube, tif_header)

    def test_masked(self):
        tif_header = 'SMALL_total_column_co2.nc.ma.tif_header.txt'
        fin = tests.get_data_path(('NetCDF', 'global', 'xyt',
                                   'SMALL_total_column_co2.nc'))
        cube = iris.load_cube(fin)[0]
        # PIL doesn't support float64
        cube.data = cube.data.astype('f4')

        # Repeat the same data extract as above
        east = iris.Constraint(longitude=lambda cell: cell < 180)
        non_edge = iris.Constraint(latitude=lambda cell: -90 < cell < 90)
        cube = cube.extract(east & non_edge)
        cube.coord('longitude').guess_bounds()
        cube.coord('latitude').guess_bounds()
        # Mask some of the data
        cube.data = np.ma.masked_where(cube.data <= 380, cube.data)
        self.check_tiff(cube, tif_header)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = idiff
#!/usr/bin/env python
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Provides "diff-like" comparison of images.

Currently relies on matplotlib for image processing so limited to PNG format.

"""

import os.path
import shutil
import sys

import matplotlib.pyplot as plt
import matplotlib.image as mimg
import matplotlib.widgets as mwidget


def diff_viewer(expected_fname, result_fname, diff_fname):
    plt.figure(figsize=(16, 16))
    plt.suptitle(os.path.basename(expected_fname))
    ax = plt.subplot(221)
    ax.imshow(mimg.imread(expected_fname))
    ax = plt.subplot(222, sharex=ax, sharey=ax)
    ax.imshow(mimg.imread(result_fname))
    ax = plt.subplot(223, sharex=ax, sharey=ax)
    ax.imshow(mimg.imread(diff_fname))

    def accept(event):
        # removes the expected result, and move the most recent result in
        print 'ACCEPTED NEW FILE: %s' % (os.path.basename(expected_fname), )
        os.remove(expected_fname)
        shutil.copy2(result_fname, expected_fname)
        os.remove(diff_fname)
        plt.close()

    def reject(event):
        print 'REJECTED: %s' % (os.path.basename(expected_fname), )
        plt.close()

    ax_accept = plt.axes([0.7, 0.05, 0.1, 0.075])
    ax_reject = plt.axes([0.81, 0.05, 0.1, 0.075])
    bnext = mwidget.Button(ax_accept, 'Accept change')
    bnext.on_clicked(accept)
    bprev = mwidget.Button(ax_reject, 'Reject')
    bprev.on_clicked(reject)

    plt.show()


def step_over_diffs():
    import iris.tests
    image_dir = os.path.join(os.path.dirname(iris.tests.__file__),
                             'results', 'visual_tests')
    diff_dir = os.path.join(os.path.dirname(iris.tests.__file__),
                            'result_image_comparison')

    for expected_fname in sorted(os.listdir(image_dir)):
        result_path = os.path.join(diff_dir, 'result-' + expected_fname)
        diff_path = result_path[:-4] + '-failed-diff.png'

        # if the test failed, there will be a diff file
        if os.path.exists(diff_path):
            expected_path = os.path.join(image_dir, expected_fname)
            diff_viewer(expected_path, result_path, diff_path)


if __name__ == '__main__':
    # Force iris.tests to use the ```tkagg``` backend by using the '-d'
    # command-line argument as idiff is an interactive tool that requires a
    # gui interface.
    sys.argv.append('-d')

    step_over_diffs()

########NEW FILE########
__FILENAME__ = test_concatenate
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Integration tests for concatenating cubes with differing time coord epochs
using :func:`iris.util.unify_time_units`.

"""

# import iris tests first so that some things can be initialised
# before importing anything else.
import iris.tests as tests

import numpy as np

import iris.coords
from iris._concatenate import concatenate
import iris.cube
import iris.unit
from iris.util import unify_time_units


class Test_concatenate__epoch(tests.IrisTest):
    def simple_1d_time_cubes(self, reftimes, coords_points):
        cubes = []
        data_points = [273, 275, 278, 277, 274]
        for reftime, coord_points in zip(reftimes, coords_points):
            cube = iris.cube.Cube(np.array(data_points, dtype=np.float32),
                                  standard_name='air_temperature',
                                  units='K')
            unit = iris.unit.Unit(reftime, calendar='gregorian')
            coord = iris.coords.DimCoord(points=np.array(coord_points,
                                                         dtype=np.float32),
                                         standard_name='time',
                                         units=unit)
            cube.add_dim_coord(coord, 0)
            cubes.append(cube)
        return cubes

    def test_concat_1d_with_differing_time_units(self):
        reftimes = ['hours since 1970-01-01 00:00:00',
                    'hours since 1970-01-02 00:00:00']
        coords_points = [[1, 2, 3, 4, 5], [1, 2, 3, 4, 5]]
        cubes = self.simple_1d_time_cubes(reftimes, coords_points)
        unify_time_units(cubes)
        result = concatenate(cubes)
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (10,))


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_name_grib
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Integration tests for NAME to GRIB2 interoperability."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import numpy as np

import iris
from iris.coords import DimCoord
import iris.unit


def name_cb(cube, field, filename):
    # NAME files give the time point at the end of the range but Iris'
    # GRIB loader creates it in the middle (the GRIB file itself doesn't
    # encode a time point). Here we make them consistent so we can
    # easily compare them.
    t_coord = cube.coord('time')
    t_coord.points = t_coord.bounds[0][1]
    fp_coord = cube.coord('forecast_period')
    fp_coord.points = fp_coord.bounds[0][1]
    # NAME contains extra vertical meta-data.
    z_coord = cube.coords('height')
    if z_coord:
        z_coord[0].long_name = 'height above ground level'
    z_coord = cube.coords('altitude')
    if z_coord:
        z_coord[0].long_name = 'altitude above sea level'


class TestNameToGRIB(tests.IrisTest):

    def check_common(self, name_cube, grib_cube):
        self.assertTrue(np.allclose(name_cube.data, name_cube.data))
        self.assertTrue(
            np.allclose(name_cube.coord('latitude').points,
                        grib_cube.coord('latitude').points))
        self.assertTrue(
            np.allclose(name_cube.coord('longitude').points,
                        grib_cube.coord('longitude').points - 360))

        for c in ['height', 'time']:
            if name_cube.coords(c):
                self.assertEqual(name_cube.coord(c),
                                 grib_cube.coord(c))

    @tests.skip_data
    def test_name2_field(self):
        filepath = tests.get_data_path(('NAME', 'NAMEII_field.txt'))
        name_cubes = iris.load(filepath)
        for i, name_cube in enumerate(name_cubes):
            with self.temp_filename('.grib2') as temp_filename:
                iris.save(name_cube, temp_filename)
                grib_cube = iris.load_cube(temp_filename, callback=name_cb)
                self.check_common(name_cube, grib_cube)
                self.assertCML(
                    grib_cube, tests.get_result_path(
                        ('integration', 'name_grib', 'NAMEII',
                         '{}_{}.cml'.format(i, name_cube.name()))))

    @tests.skip_data
    def test_name3_field(self):
        filepath = tests.get_data_path(('NAME', 'NAMEIII_field.txt'))
        name_cubes = iris.load(filepath)
        for i, name_cube in enumerate(name_cubes):
            with self.temp_filename('.grib2') as temp_filename:
                iris.save(name_cube, temp_filename)
                grib_cube = iris.load_cube(temp_filename, callback=name_cb)

                self.check_common(name_cube, grib_cube)
                self.assertCML(
                    grib_cube, tests.get_result_path(
                        ('integration', 'name_grib', 'NAMEIII',
                         '{}_{}.cml'.format(i, name_cube.name()))))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pp_grib
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Integration tests for PP/GRIB interoperability."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import iris


class TestBoundedTime(tests.IrisTest):
    @tests.skip_data
    def test_time_and_forecast_period_round_trip(self):
        pp_path = tests.get_data_path(('PP', 'meanMaxMin',
                                       '200806081200__qwpb.T24.pp'))
        # Choose the first time-bounded Cube in the PP dataset.
        original = [cube for cube in iris.load(pp_path) if
                    cube.coord('time').has_bounds()][0]
        # Save it to GRIB2 and re-load.
        with self.temp_filename('.grib2') as grib_path:
            iris.save(original, grib_path)
            from_grib = iris.load_cube(grib_path)
            # Avoid the downcasting warning when saving to PP.
            from_grib.data = from_grib.data.astype('f4')
        # Re-save to PP and re-load.
        with self.temp_filename('.pp') as pp_path:
            iris.save(from_grib, pp_path)
            from_pp = iris.load_cube(pp_path)
        self.assertEqual(original.coord('time'), from_grib.coord('time'))
        self.assertEqual(original.coord('forecast_period'),
                         from_grib.coord('forecast_period'))
        self.assertEqual(original.coord('time'), from_pp.coord('time'))
        self.assertEqual(original.coord('forecast_period'),
                         from_pp.coord('forecast_period'))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_colorbar
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test interaction between :mod:`iris.plot` and
:func:`matplotlib.pyplot.colorbar`

"""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np

from iris.coords import AuxCoord
import iris.tests.stock

# Run tests in no graphics mode if matplotlib is not available.
if tests.MPL_AVAILABLE:
    import matplotlib.pyplot as plt
    from iris.plot import contour, contourf, pcolormesh, pcolor,\
        points, scatter


@tests.skip_plot
class TestColorBarCreation(tests.GraphicsTest):
    def setUp(self):
        self.draw_functions = (contour, contourf, pcolormesh, pcolor)
        self.cube = iris.tests.stock.lat_lon_cube()
        self.cube.coord('longitude').guess_bounds()
        self.cube.coord('latitude').guess_bounds()
        self.traj_lon = AuxCoord(np.linspace(-180, 180, 50),
                                 standard_name='longitude',
                                 units='degrees')
        self.traj_lat = AuxCoord(np.sin(np.deg2rad(self.traj_lon.points))*30.0,
                                 standard_name='latitude',
                                 units='degrees')

    def test_common_draw_functions(self):
        for draw_function in self.draw_functions:
            mappable = draw_function(self.cube)
            cbar = plt.colorbar()
            self.assertIs(
                cbar.mappable, mappable,
                msg='Problem with draw function iris.plot.{}'.format(
                    draw_function.__name__))

    def test_common_draw_functions_specified_mappable(self):
        for draw_function in self.draw_functions:
            mappable_initial = draw_function(self.cube, cmap='cool')
            mappable = draw_function(self.cube)
            cbar = plt.colorbar(mappable_initial)
            self.assertIs(
                cbar.mappable, mappable_initial,
                msg='Problem with draw function iris.plot.{}'.format(
                    draw_function.__name__))

    def test_points_with_c_kwarg(self):
        mappable = points(self.cube, c=self.cube.data)
        cbar = plt.colorbar()
        self.assertIs(cbar.mappable, mappable)

    def test_points_with_c_kwarg_specified_mappable(self):
        mappable_initial = points(self.cube, c=self.cube.data, cmap='cool')
        mappable = points(self.cube, c=self.cube.data)
        cbar = plt.colorbar(mappable_initial)
        self.assertIs(cbar.mappable, mappable_initial)

    def test_scatter_with_c_kwarg(self):
        mappable = scatter(self.traj_lon, self.traj_lat,
                           c=self.traj_lon.points)
        cbar = plt.colorbar()
        self.assertIs(cbar.mappable, mappable)

    def test_scatter_with_c_kwarg_specified_mappable(self):
        mappable_initial = scatter(self.traj_lon, self.traj_lat,
                                   c=self.traj_lon.points)
        mappable = scatter(self.traj_lon, self.traj_lat,
                           c=self.traj_lon.points,
                           cmap='cool')
        cbar = plt.colorbar(mappable_initial)
        self.assertIs(cbar.mappable, mappable_initial)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_netcdf
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Integration tests for loading and saving netcdf files."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import iris
import iris.tests.stock as stock


class TestHybridPressure(tests.IrisTest):
    def setUp(self):
        # Modify stock cube so it is suitable to have a
        # hybrid pressure factory added to it.
        cube = stock.realistic_4d_no_derived()
        cube.coord('surface_altitude').rename('surface_air_pressure')
        cube.coord('surface_air_pressure').units = 'Pa'
        cube.coord('level_height').rename('level_pressure')
        cube.coord('level_pressure').units = 'Pa'
        # Construct and add hybrid pressure factory.
        factory = iris.aux_factory.HybridPressureFactory(
            cube.coord('level_pressure'),
            cube.coord('sigma'),
            cube.coord('surface_air_pressure'))
        cube.add_aux_factory(factory)
        self.cube = cube

    def test_save(self):
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(self.cube, filename)
            self.assertCDL(filename)

    def test_save_load_loop(self):
        # Tests an issue where the variable names in the formula
        # terms changed to the standard_names instead of the variable names
        # when loading a previously saved cube.
        with self.temp_filename(suffix='.nc') as filename, \
                self.temp_filename(suffix='.nc') as other_filename:
            iris.save(self.cube, filename)
            cube = iris.load_cube(filename)
            iris.save(cube, other_filename)
            other_cube = iris.load_cube(other_filename)
            self.assertEqual(cube, other_cube)


class TestSaveMultipleAuxFactories(tests.IrisTest):
    def test_hybrid_height_and_pressure(self):
        cube = stock.realistic_4d()
        cube.add_aux_coord(iris.coords.DimCoord(
            1200.0, long_name='level_pressure', units='hPa'))
        cube.add_aux_coord(iris.coords.DimCoord(
            0.5, long_name='other sigma'))
        cube.add_aux_coord(iris.coords.DimCoord(
            1000.0, long_name='surface_air_pressure', units='hPa'))
        factory = iris.aux_factory.HybridPressureFactory(
            cube.coord('level_pressure'),
            cube.coord('other sigma'),
            cube.coord('surface_air_pressure'))
        cube.add_aux_factory(factory)
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(cube, filename)
            self.assertCDL(filename)

    def test_shared_primary(self):
        cube = stock.realistic_4d()
        factory = iris.aux_factory.HybridHeightFactory(
            cube.coord('level_height'),
            cube.coord('sigma'),
            cube.coord('surface_altitude'))
        factory.rename('another altitude')
        cube.add_aux_factory(factory)
        with self.temp_filename(suffix='.nc') as filename, \
                self.assertRaisesRegexp(ValueError, 'multiple aux factories'):
            iris.save(cube, filename)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pp
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Integration tests for loading and saving PP files."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

from contextlib import nested

import mock
import numpy as np

from iris.aux_factory import HybridHeightFactory, HybridPressureFactory
from iris.coords import AuxCoord, CellMethod
from iris.cube import Cube
import iris.fileformats.pp
import iris.fileformats.pp_rules


class TestVertical(tests.IrisTest):
    def _test_coord(self, cube, point, bounds=None, **kwargs):
        coords = cube.coords(**kwargs)
        self.assertEqual(len(coords), 1, 'failed to find exactly one coord'
                                         ' using: {}'.format(kwargs))
        self.assertEqual(coords[0].points, point)
        if bounds is not None:
            self.assertArrayEqual(coords[0].bounds, [bounds])

    def test_soil_level_round_trip(self):
        # Use pp.load_cubes() to convert a fake PPField into a Cube.
        # NB. Use MagicMock so that SplittableInt header items, such as
        # LBCODE, support len().
        soil_level = 1234
        field = mock.MagicMock(lbvc=6, lblev=soil_level,
                               stash=iris.fileformats.pp.STASH(1, 0, 9),
                               lbuser=[0] * 7, lbrsvd=[0] * 4)
        load = mock.Mock(return_value=iter([field]))
        with mock.patch('iris.fileformats.pp.load', new=load) as load:
            cube = next(iris.fileformats.pp.load_cubes('DUMMY'))

        self.assertIn('soil', cube.standard_name)
        self._test_coord(cube, soil_level, long_name='soil_model_level_number')

        # Now use the save rules to convert the Cube back into a PPField.
        field = iris.fileformats.pp.PPField3()
        field.lbfc = 0
        field.lbvc = 0
        iris.fileformats.pp._ensure_save_rules_loaded()
        iris.fileformats.pp._save_rules.verify(cube, field)

        # Check the vertical coordinate is as originally specified.
        self.assertEqual(field.lbvc, 6)
        self.assertEqual(field.lblev, soil_level)

    def test_potential_temperature_level_round_trip(self):
        # Check save+load for data on 'potential temperature' levels.

        # Use pp.load_cubes() to convert a fake PPField into a Cube.
        # NB. Use MagicMock so that SplittableInt header items, such as
        # LBCODE, support len().
        potm_value = 22.5
        field = mock.MagicMock(lbvc=19, blev=potm_value,
                               lbuser=[0] * 7, lbrsvd=[0] * 4)
        load = mock.Mock(return_value=iter([field]))
        with mock.patch('iris.fileformats.pp.load', new=load) as load:
            cube = next(iris.fileformats.pp.load_cubes('DUMMY'))

        self._test_coord(cube, potm_value,
                         standard_name='air_potential_temperature')

        # Now use the save rules to convert the Cube back into a PPField.
        field = iris.fileformats.pp.PPField3()
        field.lbfc = 0
        field.lbvc = 0
        iris.fileformats.pp._ensure_save_rules_loaded()
        iris.fileformats.pp._save_rules.verify(cube, field)

        # Check the vertical coordinate is as originally specified.
        self.assertEqual(field.lbvc, 19)
        self.assertEqual(field.blev, potm_value)

    def test_hybrid_pressure_round_trip(self):
        # Use pp.load_cubes() to convert fake PPFields into Cubes.
        # NB. Use MagicMock so that SplittableInt header items, such as
        # LBCODE, support len().
        def field_with_data(scale=1):
            x, y = 40, 30
            field = mock.MagicMock(_data=np.arange(1200).reshape(y, x) * scale,
                                   lbcode=[1], lbnpt=x, lbrow=y,
                                   bzx=350, bdx=1.5, bzy=40, bdy=1.5,
                                   lbuser=[0] * 7, lbrsvd=[0] * 4)
            field._x_coord_name = lambda: 'longitude'
            field._y_coord_name = lambda: 'latitude'
            field.coord_system = lambda: None
            return field

        # Make a fake reference surface field.
        pressure_field = field_with_data(10)
        pressure_field.stash = iris.fileformats.pp.STASH(1, 0, 409)
        pressure_field.lbuser[3] = 409

        # Make a fake data field which needs the reference surface.
        model_level = 5678
        sigma_lower, sigma, sigma_upper = 0.85, 0.9, 0.95
        delta_lower, delta, delta_upper = 0.05, 0.1, 0.15
        data_field = field_with_data()
        data_field.configure_mock(lbvc=9, lblev=model_level,
                                  bhlev=delta, bhrlev=delta_lower,
                                  blev=sigma, brlev=sigma_lower,
                                  brsvd=[sigma_upper, delta_upper])

        # Convert both fields to cubes.
        load = mock.Mock(return_value=iter([pressure_field, data_field]))
        with mock.patch('iris.fileformats.pp.load', new=load) as load:
            pressure_cube, data_cube = iris.fileformats.pp.load_cubes('DUMMY')

        # Check the reference surface cube looks OK.
        self.assertEqual(pressure_cube.standard_name, 'surface_air_pressure')
        self.assertEqual(pressure_cube.units, 'Pa')

        # Check the data cube is set up to use hybrid-pressure.
        self._test_coord(data_cube, model_level,
                         standard_name='model_level_number')
        self._test_coord(data_cube, delta, [delta_lower, delta_upper],
                         long_name='level_pressure')
        self._test_coord(data_cube, sigma, [sigma_lower, sigma_upper],
                         long_name='sigma')
        aux_factories = data_cube.aux_factories
        self.assertEqual(len(aux_factories), 1)
        surface_coord = aux_factories[0].dependencies['surface_air_pressure']
        self.assertArrayEqual(surface_coord.points,
                              np.arange(12000, step=10).reshape(30, 40))

        # Now use the save rules to convert the Cubes back into PPFields.
        pressure_field = iris.fileformats.pp.PPField3()
        pressure_field.lbfc = 0
        pressure_field.lbvc = 0
        pressure_field.brsvd = [None, None]
        pressure_field.lbuser = [None] * 7
        iris.fileformats.pp._ensure_save_rules_loaded()
        iris.fileformats.pp._save_rules.verify(pressure_cube, pressure_field)

        data_field = iris.fileformats.pp.PPField3()
        data_field.lbfc = 0
        data_field.lbvc = 0
        data_field.brsvd = [None, None]
        data_field.lbuser = [None] * 7
        iris.fileformats.pp._save_rules.verify(data_cube, data_field)

        # The reference surface field should have STASH=409
        self.assertArrayEqual(pressure_field.lbuser,
                              [None, None, None, 409, None, None, 1])

        # Check the data field has the vertical coordinate as originally
        # specified.
        self.assertEqual(data_field.lbvc, 9)
        self.assertEqual(data_field.lblev, model_level)
        self.assertEqual(data_field.bhlev, delta)
        self.assertEqual(data_field.bhrlev, delta_lower)
        self.assertEqual(data_field.blev, sigma)
        self.assertEqual(data_field.brlev, sigma_lower)
        self.assertEqual(data_field.brsvd, [sigma_upper, delta_upper])

    def test_hybrid_pressure_with_duplicate_references(self):
        def field_with_data(scale=1):
            x, y = 40, 30
            field = mock.MagicMock(_data=np.arange(1200).reshape(y, x) * scale,
                                   lbcode=[1], lbnpt=x, lbrow=y,
                                   bzx=350, bdx=1.5, bzy=40, bdy=1.5,
                                   lbuser=[0] * 7, lbrsvd=[0] * 4)
            field._x_coord_name = lambda: 'longitude'
            field._y_coord_name = lambda: 'latitude'
            field.coord_system = lambda: None
            return field

        # Make a fake reference surface field.
        pressure_field = field_with_data(10)
        pressure_field.stash = iris.fileformats.pp.STASH(1, 0, 409)
        pressure_field.lbuser[3] = 409

        # Make a fake data field which needs the reference surface.
        model_level = 5678
        sigma_lower, sigma, sigma_upper = 0.85, 0.9, 0.95
        delta_lower, delta, delta_upper = 0.05, 0.1, 0.15
        data_field = field_with_data()
        data_field.configure_mock(lbvc=9, lblev=model_level,
                                  bhlev=delta, bhrlev=delta_lower,
                                  blev=sigma, brlev=sigma_lower,
                                  brsvd=[sigma_upper, delta_upper])

        # Convert both fields to cubes.
        load = mock.Mock(return_value=iter([data_field,
                                            pressure_field,
                                            pressure_field]))
        msg = 'Multiple reference cubes for surface_air_pressure'
        with nested(mock.patch('iris.fileformats.pp.load', new=load),
                    mock.patch('warnings.warn')) as (load, warn):
            _, _, _ = iris.fileformats.pp.load_cubes('DUMMY')
            warn.assert_called_with(msg)

    def test_hybrid_height_with_non_standard_coords(self):
        # Check the save rules are using the AuxFactory to find the
        # hybrid height coordinates and not relying on their names.
        ny, nx = 30, 40
        sigma_lower, sigma, sigma_upper = 0.75, 0.8, 0.75
        delta_lower, delta, delta_upper = 150, 200, 250

        cube = Cube(np.zeros((ny, nx)), 'air_temperature')
        level_coord = AuxCoord(0, 'model_level_number')
        cube.add_aux_coord(level_coord)
        delta_coord = AuxCoord(delta, bounds=[[delta_lower, delta_upper]],
                               long_name='moog', units='m')
        sigma_coord = AuxCoord(sigma, bounds=[[sigma_lower, sigma_upper]],
                               long_name='mavis')
        surface_altitude_coord = AuxCoord(np.zeros((ny, nx)),
                                          'surface_altitude', units='m')
        cube.add_aux_coord(delta_coord)
        cube.add_aux_coord(sigma_coord)
        cube.add_aux_coord(surface_altitude_coord, (0, 1))
        cube.add_aux_factory(HybridHeightFactory(delta_coord, sigma_coord,
                                                 surface_altitude_coord))

        field = iris.fileformats.pp.PPField3()
        field.lbfc = 0
        field.lbvc = 0
        field.brsvd = [None, None]
        field.lbuser = [None] * 7
        iris.fileformats.pp._ensure_save_rules_loaded()
        iris.fileformats.pp._save_rules.verify(cube, field)

        self.assertEqual(field.blev, delta)
        self.assertEqual(field.brlev, delta_lower)
        self.assertEqual(field.bhlev, sigma)
        self.assertEqual(field.bhrlev, sigma_lower)
        self.assertEqual(field.brsvd, [delta_upper, sigma_upper])

    def test_hybrid_pressure_with_non_standard_coords(self):
        # Check the save rules are using the AuxFactory to find the
        # hybrid pressure coordinates and not relying on their names.
        ny, nx = 30, 40
        sigma_lower, sigma, sigma_upper = 0.75, 0.8, 0.75
        delta_lower, delta, delta_upper = 0.15, 0.2, 0.25

        cube = Cube(np.zeros((ny, nx)), 'air_temperature')
        level_coord = AuxCoord(0, 'model_level_number')
        cube.add_aux_coord(level_coord)
        delta_coord = AuxCoord(delta, bounds=[[delta_lower, delta_upper]],
                               long_name='moog', units='Pa')
        sigma_coord = AuxCoord(sigma, bounds=[[sigma_lower, sigma_upper]],
                               long_name='mavis')
        surface_air_pressure_coord = AuxCoord(np.zeros((ny, nx)),
                                              'surface_air_pressure',
                                              units='Pa')
        cube.add_aux_coord(delta_coord)
        cube.add_aux_coord(sigma_coord)
        cube.add_aux_coord(surface_air_pressure_coord, (0, 1))
        cube.add_aux_factory(HybridPressureFactory(
            delta_coord, sigma_coord, surface_air_pressure_coord))

        field = iris.fileformats.pp.PPField3()
        field.lbfc = 0
        field.lbvc = 0
        field.brsvd = [None, None]
        field.lbuser = [None] * 7
        iris.fileformats.pp._ensure_save_rules_loaded()
        iris.fileformats.pp._save_rules.verify(cube, field)

        self.assertEqual(field.bhlev, delta)
        self.assertEqual(field.bhrlev, delta_lower)
        self.assertEqual(field.blev, sigma)
        self.assertEqual(field.brlev, sigma_lower)
        self.assertEqual(field.brsvd, [sigma_upper, delta_upper])


class TestSaveLBFT(tests.IrisTest):
    def create_cube(self, fp_min, fp_mid, fp_max, ref_offset, season=None):
        cube = Cube(np.zeros((3, 4)))
        cube.add_aux_coord(AuxCoord(standard_name='forecast_period',
                                    units='hours',
                                    points=fp_mid, bounds=[fp_min, fp_max]))
        cube.add_aux_coord(AuxCoord(standard_name='time',
                                    units='hours since epoch',
                                    points=ref_offset + fp_mid,
                                    bounds=[ref_offset + fp_min,
                                            ref_offset + fp_max]))
        if season:
            cube.add_aux_coord(AuxCoord(long_name='clim_season',
                                        points=season))
            cube.add_cell_method(CellMethod('DUMMY', 'clim_season'))
        return cube

    def convert_cube_to_field(self, cube):
        # Use the save rules to convert the Cube back into a PPField.
        field = iris.fileformats.pp.PPField3()
        field.lbfc = 0
        field.lbvc = 0
        field.lbtim = 0
        iris.fileformats.pp._ensure_save_rules_loaded()
        iris.fileformats.pp._save_rules.verify(cube, field)
        return field

    def test_time_mean_from_forecast_period(self):
        cube = self.create_cube(24, 36, 48, 72)
        field = self.convert_cube_to_field(cube)
        self.assertEqual(field.lbft, 48)

    def test_time_mean_from_forecast_reference_time(self):
        cube = Cube(np.zeros((3, 4)))
        cube.add_aux_coord(AuxCoord(standard_name='forecast_reference_time',
                                    units='hours since epoch',
                                    points=72))
        cube.add_aux_coord(AuxCoord(standard_name='time',
                                    units='hours since epoch',
                                    points=72 + 36, bounds=[72 + 24, 72 + 48]))
        field = self.convert_cube_to_field(cube)
        self.assertEqual(field.lbft, 48)

    def test_climatological_mean_single_year(self):
        cube = Cube(np.zeros((3, 4)))
        cube.add_aux_coord(AuxCoord(standard_name='forecast_period',
                                    units='hours',
                                    points=36, bounds=[24, 4 * 24]))
        cube.add_aux_coord(AuxCoord(standard_name='time',
                                    units='hours since epoch',
                                    points=240 + 36, bounds=[240 + 24,
                                                             240 + 4 * 24]))
        cube.add_aux_coord(AuxCoord(long_name='clim_season', points='DUMMY'))
        cube.add_cell_method(CellMethod('DUMMY', 'clim_season'))
        field = self.convert_cube_to_field(cube)
        self.assertEqual(field.lbft, 4 * 24)

    def test_climatological_mean_multi_year_djf(self):
        delta_start = 24
        delta_mid = 36
        delta_end = 369 * 24
        ref_offset = 10 * 24
        cube = self.create_cube(24, 36, 369 * 24, 240, 'djf')
        field = self.convert_cube_to_field(cube)
        self.assertEqual(field.lbft, 369 * 24)

    def test_climatological_mean_multi_year_mam(self):
        cube = self.create_cube(24, 36, 369 * 24, 240, 'mam')
        field = self.convert_cube_to_field(cube)
        self.assertEqual(field.lbft, 369 * 24)

    def test_climatological_mean_multi_year_jja(self):
        cube = self.create_cube(24, 36, 369 * 24, 240, 'jja')
        field = self.convert_cube_to_field(cube)
        self.assertEqual(field.lbft, 369 * 24)

    def test_climatological_mean_multi_year_son(self):
        cube = self.create_cube(24, 36, 369 * 24, 240, 'son')
        field = self.convert_cube_to_field(cube)
        self.assertEqual(field.lbft, 369 * 24)


class TestCoordinateForms(tests.IrisTest):
    def _common(self, x_coord):
        nx = len(x_coord.points)
        ny = 2
        data = np.zeros((ny, nx), dtype=np.float32)
        test_cube = iris.cube.Cube(data)
        y0 = np.float32(20.5)
        dy = np.float32(3.72)
        y_coord = iris.coords.DimCoord.from_regular(
            zeroth=y0,
            step=dy,
            count=ny,
            standard_name='latitude',
            units='degrees_north')
        test_cube.add_dim_coord(x_coord, 1)
        test_cube.add_dim_coord(y_coord, 0)
        # Write to a temporary PP file and read it back as a PPField
        with self.temp_filename('.pp') as pp_filepath:
            iris.save(test_cube, pp_filepath)
            pp_loader = iris.fileformats.pp.load(pp_filepath)
            pp_field = pp_loader.next()
        return pp_field

    def test_save_awkward_case_is_regular(self):
        # Check that specific "awkward" values still save in a regular form.
        nx = 3
        x0 = np.float32(355.626)
        dx = np.float32(0.0135)
        x_coord = iris.coords.DimCoord.from_regular(
            zeroth=x0,
            step=dx,
            count=nx,
            standard_name='longitude',
            units='degrees_east')
        pp_field = self._common(x_coord)
        # Check that the result has the regular coordinates as expected.
        self.assertEqual(pp_field.bzx, x0)
        self.assertEqual(pp_field.bdx, dx)
        self.assertEqual(pp_field.lbnpt, nx)

    def test_save_irregular(self):
        # Check that a non-regular coordinate saves as expected.
        nx = 3
        x_values = [0.0, 1.1, 2.0]
        x_coord = iris.coords.DimCoord(x_values,
                                       standard_name='longitude',
                                       units='degrees_east')
        pp_field = self._common(x_coord)
        # Check that the result has the regular/irregular Y and X as expected.
        self.assertEqual(pp_field.bdx, 0.0)
        self.assertArrayAllClose(pp_field.x, x_values)
        self.assertEqual(pp_field.lbnpt, nx)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = pp
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


import contextlib
import os.path

import iris
import iris.tests as tests


class PPTest(object):
    """
    A mixin class to provide PP-specific utilities to subclasses of tests.IrisTest.

    """
    @contextlib.contextmanager
    def cube_save_test(self, reference_txt_path, reference_cubes=None, reference_pp_path=None, **kwargs):
        """
        A context manager for testing the saving of Cubes to PP files.

        Args:

        * reference_txt_path:
            The path of the file containing the textual PP reference data.

        Kwargs:

        * reference_cubes:
            The cube(s) from which the textual PP reference can be re-built if necessary.
        * reference_pp_path:
            The location of a PP file from which the textual PP reference can be re-built if necessary.
            NB. The "reference_cubes" argument takes precedence over this argument.

        The return value from the context manager is the name of a temporary file
        into which the PP data to be tested should be saved.

        Example::
            with self.cube_save_test(reference_txt_path, reference_cubes=cubes) as temp_pp_path:
                iris.save(cubes, temp_pp_path)

        """
        # Watch out for a missing reference text file
        if not os.path.isfile(reference_txt_path):
            tests.logger.warning('Creating result file: %s', reference_txt_path)
            if reference_cubes:
                temp_pp_path = iris.util.create_temp_filename(".pp")
                try:
                    iris.save(reference_cubes, temp_pp_path, **kwargs)
                    self._create_reference_txt(reference_txt_path, temp_pp_path)
                finally:
                    os.remove(temp_pp_path)
            elif reference_pp_path:
                self._create_reference_txt(reference_txt_path, reference_pp_path)
            else:
                raise ValueError('Missing all of reference txt file, cubes, and PP path.')

        temp_pp_path = iris.util.create_temp_filename(".pp")
        try:
            # This value is returned to the target of the "with" statement's "as" clause.
            yield temp_pp_path

            # Load deferred data for all of the fields (but don't do anything with it)
            pp_fields = list(iris.fileformats.pp.load(temp_pp_path))
            for pp_field in pp_fields:
                pp_field.data

            reference = ''.join(open(reference_txt_path, 'r'))
            self._assert_str_same(reference + '\n', str(pp_fields) + '\n',
                                    reference_txt_path, type_comparison_name='PP files')
        finally:
            os.remove(temp_pp_path)

    def _create_reference_txt(self, txt_path, pp_path):
        # Load the reference data
        pp_fields = list(iris.fileformats.pp.load(pp_path))
        for pp_field in pp_fields:
            pp_field.data

        # Clear any header words we don't use
        unused = ('lbexp', 'lbegin', 'lbnrec', 'lbproj', 'lbtyp')
        for pp_field in pp_fields:
            for word_name in unused:
                setattr(pp_field, word_name, 0)

        # Save the textual representation of the PP fields
        with open(txt_path, 'w') as txt_file:
            txt_file.writelines(str(pp_fields))

########NEW FILE########
__FILENAME__ = stock
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
A collection of routines which create standard Cubes for test purposes.

"""

import os.path

import numpy as np
import numpy.ma as ma

from iris.cube import Cube
import iris.aux_factory
import iris.coords
import iris.coords as icoords
import iris.tests as tests
from iris.coord_systems import GeogCS, RotatedGeogCS


def lat_lon_cube():
    """
    Returns a cube with a latitude and longitude suitable for testing
    saving to PP/NetCDF etc.

    """
    cube = Cube(np.arange(12, dtype=np.int32).reshape((3, 4)))
    cs = GeogCS(6371229)
    coord = iris.coords.DimCoord(points=np.array([-1, 0, 1], dtype=np.int32),
                                 standard_name='latitude',
                                 units='degrees',
                                 coord_system=cs)
    cube.add_dim_coord(coord, 0)
    coord = iris.coords.DimCoord(points=np.array([-1, 0, 1, 2], dtype=np.int32),
                                 standard_name='longitude',
                                 units='degrees',
                                 coord_system=cs)
    cube.add_dim_coord(coord, 1)
    return cube


def global_pp():
    """
    Returns a two-dimensional cube derived from PP/aPPglob1/global.pp.

    The standard_name and unit attributes are added to compensate for the
    broken STASH encoding in that file.

    """
    def callback_global_pp(cube, field, filename):
        cube.standard_name = 'air_temperature'
        cube.units = 'K'
    path = tests.get_data_path(('PP', 'aPPglob1', 'global.pp'))
    cube = iris.load_cube(path, callback=callback_global_pp)
    return cube


def simple_pp():
    filename = tests.get_data_path(['PP', 'simple_pp', 'global.pp'])   # Differs from global_pp()
    cube = iris.load_cube(filename)
    return cube


def simple_1d(with_bounds=True):
    """
    Returns an abstract, one-dimensional cube.

    >>> print simple_1d()
    thingness                           (foo: 11)
         Dimension coordinates:
              foo                           x

    >>> print `simple_1d().data`
    [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10]

    """
    cube = Cube(np.arange(11, dtype=np.int32))
    cube.long_name = 'thingness'
    cube.units = '1'
    points = np.arange(11, dtype=np.int32) + 1
    bounds = np.column_stack([np.arange(11, dtype=np.int32), np.arange(11, dtype=np.int32) + 1])
    coord = iris.coords.DimCoord(points, long_name='foo', units='1', bounds=bounds)
    cube.add_dim_coord(coord, 0)
    return cube


def simple_2d(with_bounds=True):
    """
    Returns an abstract, two-dimensional, optionally bounded, cube.

    >>> print simple_2d()
    thingness                           (bar: 3; foo: 4)
         Dimension coordinates:
              bar                           x       -
              foo                           -       x

    >>> print `simple_2d().data`
    [[ 0  1  2  3]
     [ 4  5  6  7]
     [ 8  9 10 11]]


    """
    cube = Cube(np.arange(12, dtype=np.int32).reshape((3, 4)))
    cube.long_name = 'thingness'
    cube.units = '1'
    y_points = np.array([  2.5,   7.5,  12.5])
    y_bounds = np.array([[0, 5], [5, 10], [10, 15]], dtype=np.int32)
    y_coord = iris.coords.DimCoord(y_points, long_name='bar', units='1', bounds=y_bounds if with_bounds else None)
    x_points = np.array([ -7.5,   7.5,  22.5,  37.5])
    x_bounds = np.array([[-15, 0], [0, 15], [15, 30], [30, 45]], dtype=np.int32)
    x_coord = iris.coords.DimCoord(x_points, long_name='foo', units='1', bounds=x_bounds if with_bounds else None)

    cube.add_dim_coord(y_coord, 0)
    cube.add_dim_coord(x_coord, 1)
    return cube


def simple_2d_w_multidim_coords(with_bounds=True):
    """
    Returns an abstract, two-dimensional, optionally bounded, cube.

    >>> print simple_2d_w_multidim_coords()
    thingness                           (*ANONYMOUS*: 3; *ANONYMOUS*: 4)
         Auxiliary coordinates:
              bar                                   x               x
              foo                                   x               x

    >>> print `simple_2d().data`
    [[ 0,  1,  2,  3],
     [ 4,  5,  6,  7],
     [ 8,  9, 10, 11]]

    """
    cube = simple_3d_w_multidim_coords(with_bounds)[0, :, :]
    cube.remove_coord('wibble')
    cube.data = np.arange(12, dtype=np.int32).reshape((3, 4))
    return cube


def simple_3d_w_multidim_coords(with_bounds=True):
    """
    Returns an abstract, two-dimensional, optionally bounded, cube.

    >>> print simple_3d_w_multidim_coords()
    thingness                           (wibble: 2; *ANONYMOUS*: 3; *ANONYMOUS*: 4)
         Dimension coordinates:
              wibble                           x               -               -
         Auxiliary coordinates:
              bar                              -               x               x
              foo                              -               x               x

    >>> print simple_3d_w_multidim_coords().data
    [[[ 0  1  2  3]
      [ 4  5  6  7]
      [ 8  9 10 11]]

     [[12 13 14 15]
      [16 17 18 19]
      [20 21 22 23]]]

    """
    cube = Cube(np.arange(24, dtype=np.int32).reshape((2, 3, 4)))
    cube.long_name = 'thingness'
    cube.units = '1'

    y_points = np.array([[2.5, 7.5, 12.5, 17.5],
                         [10., 17.5, 27.5, 42.5],
                         [15., 22.5, 32.5, 50.]])
    y_bounds = np.array([[[0, 5], [5, 10], [10, 15], [15, 20]],
                         [[5, 15], [15, 20], [20, 35], [35, 50]],
                         [[10, 20], [20, 25], [25, 40], [40, 60]]],
                        dtype=np.int32)
    y_coord = iris.coords.AuxCoord(points=y_points, long_name='bar',
                                   units='1',
                                   bounds=y_bounds if with_bounds else None)
    x_points = np.array([[-7.5, 7.5, 22.5, 37.5],
                         [-12.5, 4., 26.5, 47.5],
                         [2.5, 14., 36.5, 44.]])
    x_bounds = np.array([[[-15, 0], [0, 15], [15, 30], [30, 45]],
                         [[-25, 0], [0, 8], [8, 45], [45, 50]],
                         [[-5, 10], [10, 18],  [18, 55], [18, 70]]],
                        dtype=np.int32)
    x_coord = iris.coords.AuxCoord(points=x_points, long_name='foo',
                                   units='1',
                                   bounds=x_bounds if with_bounds else None)
    wibble_coord = iris.coords.DimCoord(np.array([10., 30.],
                                                 dtype=np.float32),
                                        long_name='wibble', units='1')

    cube.add_dim_coord(wibble_coord, [0])
    cube.add_aux_coord(y_coord, [1, 2])
    cube.add_aux_coord(x_coord, [1, 2])
    return cube


def simple_3d():
    """
    Returns an abstract three dimensional cube.

    >>>print simple_3d()
    thingness / (1)                     (wibble: 2; latitude: 3; longitude: 4)
     Dimension coordinates:
          wibble                           x            -             -
          latitude                         -            x             -
          longitude                        -            -             x

    >>> print simple_3d().data
    [[[ 0  1  2  3]
      [ 4  5  6  7]
      [ 8  9 10 11]]

     [[12 13 14 15]
      [16 17 18 19]
      [20 21 22 23]]]

    """
    cube = Cube(np.arange(24, dtype=np.int32).reshape((2, 3, 4)))
    cube.long_name = 'thingness'
    cube.units = '1'
    wibble_coord = iris.coords.DimCoord(np.array([10., 30.],
                                                 dtype=np.float32),
                                        long_name='wibble', units='1')
    lon = iris.coords.DimCoord([-180, -90, 0, 90],
                               standard_name='longitude',
                               units='degrees', circular=True)
    lat = iris.coords.DimCoord([90, 0, -90],
                               standard_name='latitude', units='degrees')
    cube.add_dim_coord(wibble_coord, [0])
    cube.add_dim_coord(lat, [1])
    cube.add_dim_coord(lon, [2])
    return cube


def simple_3d_mask():
    """
    Returns an abstract three dimensional cube that has data masked.

    >>>print simple_3d_mask()
    thingness / (1)                     (wibble: 2; latitude: 3; longitude: 4)
     Dimension coordinates:
          wibble                           x            -             -
          latitude                         -            x             -
          longitude                        -            -             x

    >>> print simple_3d_mask().data
    [[[-- -- -- --]
      [-- -- -- --]
      [-- 9 10 11]]

    [[12 13 14 15]
     [16 17 18 19]
     [20 21 22 23]]]

    """
    cube = simple_3d()
    cube.data = ma.asanyarray(cube.data)
    cube.data = ma.masked_less_equal(cube.data, 8.)
    return cube


def track_1d(duplicate_x=False):
    """
    Returns a one-dimensional track through two-dimensional space.

    >>> print track_1d()
    air_temperature                     (y, x: 11)
         Dimensioned coords:
              x -> x
              y -> y
         Single valued coords:

    >>> print `track_1d().data`
    array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])

    """
    cube = Cube(np.arange(11, dtype=np.int32), standard_name='air_temperature', units='K')
    bounds = np.column_stack([np.arange(11, dtype=np.int32), np.arange(11, dtype=np.int32) + 1])
    pts = bounds[:, 1]
    coord = iris.coords.AuxCoord(pts, 'projection_x_coordinate', units='1', bounds=bounds)
    cube.add_aux_coord(coord, [0])
    if duplicate_x:
        coord = iris.coords.AuxCoord(pts, 'projection_x_coordinate', units='1', bounds=bounds)
        cube.add_aux_coord(coord, [0])
    coord = iris.coords.AuxCoord(pts * 2, 'projection_y_coordinate', units='1', bounds=bounds * 2)
    cube.add_aux_coord(coord, 0)
    return cube


def simple_2d_w_multidim_and_scalars():
    data = np.arange(50, dtype=np.int32).reshape((5, 10))
    cube = iris.cube.Cube(data, long_name='test 2d dimensional cube', units='meters')

    # DimCoords
    dim1 = iris.coords.DimCoord(np.arange(5, dtype=np.float32) * 5.1 + 3.0, long_name='dim1', units='meters')
    dim2 = iris.coords.DimCoord(np.arange(10, dtype=np.int32), long_name='dim2', units='meters',
                                bounds=np.arange(20, dtype=np.int32).reshape(10, 2))

    # Scalars
    an_other = iris.coords.AuxCoord(3.0, long_name='an_other', units='meters')
    yet_an_other = iris.coords.DimCoord(23.3, standard_name='air_temperature',
                                        long_name='custom long name',
                                        var_name='custom_var_name',
                                        units='K')

    # Multidim
    my_multi_dim_coord = iris.coords.AuxCoord(np.arange(50, dtype=np.int32).reshape(5, 10),
                                              long_name='my_multi_dim_coord', units='1',
                                              bounds=np.arange(200, dtype=np.int32).reshape(5, 10, 4))

    cube.add_dim_coord(dim1, 0)
    cube.add_dim_coord(dim2, 1)
    cube.add_aux_coord(an_other)
    cube.add_aux_coord(yet_an_other)
    cube.add_aux_coord(my_multi_dim_coord, [0, 1])

    return cube


def hybrid_height():
    """
    Returns a two-dimensional (Z, X), hybrid-height cube.

    >>> print hybrid_height()
    TODO: Update!
    air_temperature                     (level_height: 3; *ANONYMOUS*: 4)
         Dimension coordinates:
              level_height                           x               -
         Auxiliary coordinates:
              model_level_number                     x               -
              sigma                                  x               -
              surface_altitude                       -               x
         Derived coordinates:
              altitude                               x               x

    >>> print hybrid_height().data
    [[[ 0  1  2  3]
      [ 4  5  6  7]
      [ 8  9 10 11]]

    """
    data = np.arange(12, dtype='i8').reshape((3, 4))

    orography = icoords.AuxCoord([10, 25, 50, 5], standard_name='surface_altitude', units='m')
    model_level = icoords.AuxCoord([2, 1, 0], standard_name='model_level_number')
    level_height = icoords.DimCoord([100, 50, 10], long_name='level_height',
                                    units='m', attributes={'positive': 'up'},
                                    bounds=[[150, 75], [75, 20], [20, 0]])
    sigma = icoords.AuxCoord([0.8, 0.9, 0.95], long_name='sigma',
                             bounds=[[0.7, 0.85], [0.85, 0.97], [0.97, 1.0]])
    hybrid_height = iris.aux_factory.HybridHeightFactory(level_height, sigma, orography)

    cube = iris.cube.Cube(data, standard_name='air_temperature', units='K',
                          dim_coords_and_dims=[(level_height, 0)],
                          aux_coords_and_dims=[(orography, 1), (model_level, 0), (sigma, 0)],
                          aux_factories=[hybrid_height])
    return cube


def simple_4d_with_hybrid_height():
    cube = iris.cube.Cube(np.arange(3*4*5*6, dtype='i8').reshape(3,4,5,6),
                          "air_temperature", units="K")

    cube.add_dim_coord(iris.coords.DimCoord(np.arange(3, dtype='i8'), "time",
                                            units="hours since epoch"), 0)
    cube.add_dim_coord(iris.coords.DimCoord(np.arange(4, dtype='i8')+10,
                                            "model_level_number", units="1"), 1)
    cube.add_dim_coord(iris.coords.DimCoord(np.arange(5, dtype='i8')+20,
                                            "grid_latitude",
                                            units="degrees"), 2)
    cube.add_dim_coord(iris.coords.DimCoord(np.arange(6, dtype='i8')+30,
                                            "grid_longitude",
                                            units="degrees"), 3)

    cube.add_aux_coord(iris.coords.AuxCoord(np.arange(4, dtype='i8')+40,
                                            long_name="level_height",
                                            units="m"), 1)
    cube.add_aux_coord(iris.coords.AuxCoord(np.arange(4, dtype='i8')+50,
                                            long_name="sigma", units="1"), 1)
    cube.add_aux_coord(iris.coords.AuxCoord(np.arange(5*6, dtype='i8').reshape(5,6)+100,
                                            long_name="surface_altitude",
                                            units="m"), [2,3])

    cube.add_aux_factory(iris.aux_factory.HybridHeightFactory(
                                    delta=cube.coord("level_height"),
                                    sigma=cube.coord("sigma"),
                                    orography=cube.coord("surface_altitude")))
    return cube


def realistic_4d():
    """
    Returns a realistic 4d cube.

    >>> print repr(realistic_4d())
    <iris 'Cube' of air_potential_temperature (time: 6; model_level_number: 70; grid_latitude: 100; grid_longitude: 100)>

    """
    # the stock arrays were created in Iris 0.8 with:
#    >>> fname = iris.sample_data_path('PP', 'COLPEX', 'theta_and_orog_subset.pp')
#    >>> theta = iris.load_cube(fname, 'air_potential_temperature')
#    >>> for coord in theta.coords():
#    ...  print coord.name, coord.has_points(), coord.has_bounds(), coord.units
#    ...
#    grid_latitude True True degrees
#    grid_longitude True True degrees
#    level_height True True m
#    model_level True False 1
#    sigma True True 1
#    time True False hours since 1970-01-01 00:00:00
#    source True False no_unit
#    forecast_period True False hours
#    >>> arrays = []
#    >>> for coord in theta.coords():
#    ...  if coord.has_points(): arrays.append(coord.points)
#    ...  if coord.has_bounds(): arrays.append(coord.bounds)
#    >>> arrays.append(theta.data)
#    >>> arrays.append(theta.coord('sigma').coord_system.orography.data)
#    >>> np.savez('stock_arrays.npz', *arrays)

    data_path = os.path.join(os.path.dirname(__file__), 'stock_arrays.npz')
    r = np.load(data_path)
    # sort the arrays based on the order they were originally given. The names given are of the form 'arr_1' or 'arr_10'
    _, arrays =  zip(*sorted(r.iteritems(), key=lambda item: int(item[0][4:])))

    lat_pts, lat_bnds, lon_pts, lon_bnds, level_height_pts, \
    level_height_bnds, model_level_pts, sigma_pts, sigma_bnds, time_pts, \
    _source_pts, forecast_period_pts, data, orography = arrays


    ll_cs = RotatedGeogCS(37.5, 177.5, ellipsoid=GeogCS(6371229.0))

    lat = icoords.DimCoord(lat_pts, standard_name='grid_latitude', units='degrees',
                           bounds=lat_bnds, coord_system=ll_cs)
    lon = icoords.DimCoord(lon_pts, standard_name='grid_longitude', units='degrees',
                           bounds=lon_bnds, coord_system=ll_cs)
    level_height = icoords.DimCoord(level_height_pts, long_name='level_height',
                                    units='m', bounds=level_height_bnds,
                                    attributes={'positive': 'up'})
    model_level = icoords.DimCoord(model_level_pts, standard_name='model_level_number',
                                   units='1', attributes={'positive': 'up'})
    sigma = icoords.AuxCoord(sigma_pts, long_name='sigma', units='1', bounds=sigma_bnds)
    orography = icoords.AuxCoord(orography, standard_name='surface_altitude', units='m')
    time = icoords.DimCoord(time_pts, standard_name='time', units='hours since 1970-01-01 00:00:00')
    forecast_period = icoords.DimCoord(forecast_period_pts, standard_name='forecast_period', units='hours')

    hybrid_height = iris.aux_factory.HybridHeightFactory(level_height, sigma, orography)

    cube = iris.cube.Cube(data, standard_name='air_potential_temperature', units='K',
                          dim_coords_and_dims=[(time, 0), (model_level, 1), (lat, 2), (lon, 3)],
                          aux_coords_and_dims=[(orography, (2, 3)), (level_height, 1), (sigma, 1),
                                               (forecast_period, None)],
                          attributes={'source': 'Iris test case'},
                          aux_factories=[hybrid_height])
    return cube


def realistic_4d_no_derived():
    """
    Returns a realistic 4d cube without hybrid height

    >>> print repr(realistic_4d())
    <iris 'Cube' of air_potential_temperature (time: 6; model_level_number: 70; grid_latitude: 100; grid_longitude: 100)>

    """
    cube = realistic_4d()

    # TODO determine appropriate way to remove aux_factory from a cube
    cube._aux_factories = []

    return cube


def realistic_4d_w_missing_data():
    data_path = os.path.join(os.path.dirname(__file__), 'stock_mdi_arrays.npz')
    data_archive = np.load(data_path)
    data = ma.masked_array(data_archive['arr_0'], mask=data_archive['arr_1'])

    # sort the arrays based on the order they were originally given. The names given are of the form 'arr_1' or 'arr_10'

    ll_cs = GeogCS(6371229)

    lat = iris.coords.DimCoord(np.arange(20, dtype=np.float32), standard_name='grid_latitude',
                               units='degrees', coord_system=ll_cs)
    lon = iris.coords.DimCoord(np.arange(20, dtype=np.float32), standard_name='grid_longitude',
                               units='degrees', coord_system=ll_cs)
    time = iris.coords.DimCoord([1000., 1003., 1006.], standard_name='time',
                                units='hours since 1970-01-01 00:00:00')
    forecast_period = iris.coords.DimCoord([0.0, 3.0, 6.0], standard_name='forecast_period', units='hours')
    pressure = iris.coords.DimCoord(np.array([  800.,   900.,  1000.], dtype=np.float32),
                                    long_name='pressure', units='hPa')

    cube = iris.cube.Cube(data, long_name='missing data test data', units='K',
                          dim_coords_and_dims=[(time, 0), (pressure, 1), (lat, 2), (lon, 3)],
                          aux_coords_and_dims=[(forecast_period, 0)],
                          attributes={'source':'Iris test case'})
    return cube


def global_grib2():
    path = tests.get_data_path(('GRIB', 'global_t', 'global.grib2'))
    cube = iris.load_cube(path)
    return cube

########NEW FILE########
__FILENAME__ = system_test
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


"""
This system test module is useful to identify if some of the key components required for Iris are available.

The system tests can be run with ``python setup.py test --system-tests``.

"""
# import iris tests first so that some things can be initialised before importing anything else
import sys

import numpy as np

import iris
import iris.fileformats.grib as grib
import iris.fileformats.netcdf as netcdf
import iris.fileformats.pp as pp
import iris.tests as tests
import iris.unit

class SystemInitialTest(tests.IrisTest):

    def system_test_supported_filetypes(self):
        nx, ny = 60, 60
        dataarray = np.arange(nx * ny, dtype='>f4').reshape(nx, ny)

        laty = np.linspace(0, 59, ny).astype('f8')
        lonx = np.linspace(30, 89, nx).astype('f8')

        horiz_cs = lambda : iris.coord_systems.GeogCS(6371229)

        cm = iris.cube.Cube(data=dataarray, long_name="System test data", units='m s-1')
        cm.add_dim_coord(
            iris.coords.DimCoord(laty, 'latitude', units='degrees',
                                 coord_system=horiz_cs()),
            0)
        cm.add_dim_coord(
            iris.coords.DimCoord(lonx, 'longitude', units='degrees',
                coord_system=horiz_cs()),
            1)
        cm.add_aux_coord(iris.coords.AuxCoord(np.array([9], 'i8'),
                                              'forecast_period', units='hours'))
        hours_since_epoch = iris.unit.Unit('hours since epoch',
                                           iris.unit.CALENDAR_GREGORIAN)
        cm.add_aux_coord(iris.coords.AuxCoord(np.array([3], 'i8'),
                                              'time', units=hours_since_epoch))
        cm.add_aux_coord(iris.coords.AuxCoord(np.array([99], 'i8'),
                                              long_name='pressure', units='Pa'))

        cm.assert_valid()

        for filetype in ('.nc', '.pp' , '.grib2'):
            saved_tmpfile = iris.util.create_temp_filename(suffix=filetype)
            iris.save(cm, saved_tmpfile)

            new_cube = iris.load_cube(saved_tmpfile)

            self.assertCML(new_cube, ('system', 'supported_filetype_%s.cml' % filetype))

    def system_test_grib_patch(self):
        import gribapi
        gm = gribapi.grib_new_from_samples("GRIB2")
        result = gribapi.grib_get_double(gm, "missingValue")

        new_missing_value = 123456.0
        gribapi.grib_set_double(gm, "missingValue", new_missing_value)
        new_result = gribapi.grib_get_double(gm, "missingValue")

        self.assertEqual(new_result, new_missing_value)

    def system_test_imports_general(self):
        if tests.MPL_AVAILABLE:
            import matplotlib
        import netCDF4


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_abf
# (C) British Crown Copyright 2012 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np

import iris
import iris.fileformats.abf


@tests.skip_data
class TestAbfLoad(tests.IrisTest):
    def setUp(self):
        self.path = tests.get_data_path(('abf', 'AVHRRBUVI01.1985apra.abf'))

    def test_load(self):
        cubes = iris.load(self.path)
        # On a 32-bit platform the time coordinate will have 32-bit integers.
        # We force them to 64-bit to ensure consistent test results.
        time_coord = cubes[0].coord("time")
        time_coord.points = np.array(time_coord.points, dtype=np.int64)
        time_coord.bounds = np.array(time_coord.bounds, dtype=np.int64)
        # Normalise the different array orders returned by version 1.6
        # and 1.7 of NumPy.
        cubes[0].data = cubes[0].data.copy(order='C')
        self.assertCML(cubes, ("abf", "load.cml"))

    def test_fill_value(self):
        field = iris.fileformats.abf.ABFField(self.path)
        # Make sure the fill value is appropriate. It must avoid the
        # data range (0 to 100 inclusive) but still fit within the dtype
        # range (0 to 255 inclusive).
        self.assertGreater(field.data.fill_value, 100)
        self.assertLess(field.data.fill_value, 256)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_aggregate_by
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np
import numpy.ma as ma
import unittest

import iris
import iris.analysis
import iris.coord_systems
import iris.coords


class TestAggregateBy(tests.IrisTest):

    def setUp(self):
        #
        # common
        #
        cs_latlon = iris.coord_systems.GeogCS(6371229)
        points = np.arange(3, dtype=np.float32) * 3
        coord_lat = iris.coords.DimCoord(points, 'latitude', units='degrees',
                                         coord_system=cs_latlon)
        coord_lon = iris.coords.DimCoord(points, 'longitude', units='degrees',
                                         coord_system=cs_latlon)

        #
        # single coordinate aggregate-by
        #
        a = np.arange(9, dtype=np.int32).reshape(3, 3) + 1
        b = np.arange(36, dtype=np.int32).reshape(36, 1, 1)
        data = b * a

        self.cube_single = iris.cube.Cube(data,
                                          long_name='temperature',
                                          units='kelvin')

        z_points = np.array(
            [1, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6,
             6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8],
            dtype=np.int32)
        self.coord_z_single = iris.coords.AuxCoord(z_points,
                                                   long_name='height',
                                                   units='m')

        model_level = iris.coords.DimCoord(range(z_points.size),
                                           standard_name='model_level_number')

        self.cube_single.add_aux_coord(self.coord_z_single, 0)
        self.cube_single.add_dim_coord(model_level, 0)
        self.cube_single.add_dim_coord(coord_lon, 1)
        self.cube_single.add_dim_coord(coord_lat, 2)

        #
        # multi coordinate aggregate-by
        #
        a = np.arange(9, dtype=np.int32).reshape(3, 3) + 1
        b = np.arange(20, dtype=np.int32).reshape(20, 1, 1)
        data = b * a

        self.cube_multi = iris.cube.Cube(data, long_name='temperature',
                                         units='kelvin')

        z1_points = np.array(
            [1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 4, 4, 4, 4, 4, 1, 5, 5, 2, 2],
            dtype=np.int32)
        self.coord_z1_multi = iris.coords.AuxCoord(z1_points,
                                                   long_name='height',
                                                   units='m')
        z2_points = np.array(
            [1, 1, 1, 3, 3, 3, 5, 5, 5, 7, 7, 7, 9, 9, 9, 11, 11, 11, 3, 3],
            dtype=np.int32)
        self.coord_z2_multi = iris.coords.AuxCoord(z2_points,
                                                   long_name='level',
                                                   units='1')

        model_level = iris.coords.DimCoord(range(z1_points.size),
                                           standard_name='model_level_number')

        self.cube_multi.add_aux_coord(self.coord_z1_multi, 0)
        self.cube_multi.add_aux_coord(self.coord_z2_multi, 0)
        self.cube_multi.add_dim_coord(model_level, 0)
        self.cube_multi.add_dim_coord(coord_lon.copy(), 1)
        self.cube_multi.add_dim_coord(coord_lat.copy(), 2)

        #
        # expected data results
        #
        self.single_expected = np.array(
            [[[0., 0., 0.],      [0., 0., 0.],        [0., 0., 0.]],
             [[1.5, 3., 4.5],    [6., 7.5, 9.],       [10.5, 12., 13.5]],
             [[4., 8., 12.],     [16., 20., 24.],     [28., 32., 36.]],
             [[7.5, 15., 22.5],  [30., 37.5, 45.],    [52.5, 60., 67.5]],
             [[12., 24., 36.],   [48., 60., 72.],     [84., 96., 108.]],
             [[17.5, 35., 52.5], [70., 87.5, 105.],   [122.5, 140., 157.5]],
             [[24., 48., 72.],   [96., 120., 144.],   [168., 192., 216.]],
             [[31.5, 63., 94.5], [126., 157.5, 189.], [220.5, 252., 283.5]]],
            dtype=np.float64)

        row1 = [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]
        row2 = [list(np.sqrt([2.5, 10., 22.5])),
                list(np.sqrt([40., 62.5, 90.])),
                list(np.sqrt([122.5, 160., 202.5]))]
        row3 = [list(np.sqrt([16.66666667, 66.66666667, 150.])),
                list(np.sqrt([266.66666667, 416.66666667, 600.])),
                list(np.sqrt([816.66666667, 1066.66666667, 1350.]))]
        row4 = [list(np.sqrt([57.5, 230., 517.5])),
                list(np.sqrt([920., 1437.5, 2070.])),
                list(np.sqrt([2817.5, 3680., 4657.5]))]
        row5 = [list(np.sqrt([146., 584., 1314.])),
                list(np.sqrt([2336., 3650., 5256.])),
                list(np.sqrt([7154., 9344., 11826.]))]
        row6 = [list(np.sqrt([309.16666667, 1236.66666667, 2782.5])),
                list(np.sqrt([4946.66666667, 7729.16666667, 11130.])),
                list(np.sqrt([15149.16666667, 19786.66666667, 25042.5]))]
        row7 = [list(np.sqrt([580., 2320., 5220.])),
                list(np.sqrt([9280., 14500., 20880.])),
                list(np.sqrt([28420., 37120., 46980.]))]
        row8 = [list(np.sqrt([997.5, 3990., 8977.5])),
                list(np.sqrt([15960., 24937.5, 35910.])),
                list(np.sqrt([48877.5, 63840., 80797.5]))]
        self.single_rms_expected = np.array([row1,
                                             row2,
                                             row3,
                                             row4,
                                             row5,
                                             row6,
                                             row7,
                                             row8], dtype=np.float64)

        self.multi_expected = np.array(
            [[[1., 2., 3.],      [4., 5., 6.],     [7., 8., 9.]],
             [[3.5, 7., 10.5],   [14., 17.5, 21.], [24.5, 28., 31.5]],
             [[14., 28., 42.],   [56., 70., 84.],  [98., 112., 126.]],
             [[7., 14., 21.],    [28., 35., 42.],  [49., 56., 63.]],
             [[9., 18., 27.],    [36., 45., 54.],  [63., 72., 81.]],
             [[10.5, 21., 31.5], [42., 52.5, 63.], [73.5, 84., 94.5]],
             [[13., 26., 39.],   [52., 65., 78.],  [91., 104., 117.]],
             [[15., 30., 45.],   [60., 75., 90.],  [105., 120., 135.]],
             [[16.5, 33., 49.5], [66., 82.5, 99.], [115.5, 132., 148.5]]],
            dtype=np.float64)

    def test_single(self):
        # mean group-by with single coordinate name.
        aggregateby_cube = self.cube_single.aggregated_by('height',
                                                          iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'single.cml'))

        # mean group-by with single coordinate.
        aggregateby_cube = self.cube_single.aggregated_by(self.coord_z_single,
                                                          iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'single.cml'))

        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       self.single_expected)

        # rms group-by with single coordinate name.
        aggregateby_cube = self.cube_single.aggregated_by('height',
                                                          iris.analysis.RMS)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'single_rms.cml'))

        # rms group-by with single coordinate.
        aggregateby_cube = self.cube_single.aggregated_by(self.coord_z_single,
                                                          iris.analysis.RMS)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'single_rms.cml'))

        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       self.single_rms_expected)

    def test_single_shared(self):
        z2_points = np.arange(36, dtype=np.int32)
        coord_z2 = iris.coords.AuxCoord(z2_points, long_name='wibble',
                                        units='1')
        self.cube_single.add_aux_coord(coord_z2, 0)

        # group-by with single coordinate name on shared axis.
        aggregateby_cube = self.cube_single.aggregated_by('height',
                                                          iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'single_shared.cml'))

        # group-by with single coordinate on shared axis.
        aggregateby_cube = self.cube_single.aggregated_by(self.coord_z_single,
                                                          iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'single_shared.cml'))

        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       self.single_expected)

    def test_single_shared_circular(self):
        points = np.arange(36) * 10.0
        circ_coord = iris.coords.DimCoord(points,
                                          long_name='circ_height',
                                          units='degrees',
                                          circular=True)
        self.cube_single.add_aux_coord(circ_coord, 0)

        # group-by with single coordinate name on shared axis.
        aggregateby_cube = self.cube_single.aggregated_by('height',
                                                          iris.analysis.MEAN)
        self.assertCML(aggregateby_cube, ('analysis', 'aggregated_by',
                                          'single_shared_circular.cml'))

        # group-by with single coordinate on shared axis.
        coord = self.cube_single.coords('height')
        aggregateby_cube = self.cube_single.aggregated_by(coord,
                                                          iris.analysis.MEAN)
        self.assertCML(aggregateby_cube, ('analysis', 'aggregated_by',
                                          'single_shared_circular.cml'))
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       self.single_expected)

    def test_multi(self):
        # group-by with multiple coordinate names.
        aggregateby_cube = self.cube_multi.aggregated_by(['height', 'level'],
                                                         iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'multi.cml'))

        # group-by with multiple coordinate names (different order).
        aggregateby_cube = self.cube_multi.aggregated_by(['level', 'height'],
                                                         iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'multi.cml'))

        # group-by with multiple coordinates.
        aggregateby_cube = self.cube_multi.aggregated_by(
            [self.coord_z1_multi, self.coord_z2_multi], iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'multi.cml'))

        # group-by with multiple coordinates (different order).
        aggregateby_cube = self.cube_multi.aggregated_by(
            [self.coord_z2_multi, self.coord_z1_multi], iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'multi.cml'))

        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       self.multi_expected)

    def test_multi_shared(self):
        z3_points = np.arange(20, dtype=np.int32)
        coord_z3 = iris.coords.AuxCoord(z3_points, long_name='sigma',
                                        units='1')
        z4_points = np.arange(19, -1, -1, dtype=np.int32)
        coord_z4 = iris.coords.AuxCoord(z4_points, long_name='gamma',
                                        units='1')

        self.cube_multi.add_aux_coord(coord_z3, 0)
        self.cube_multi.add_aux_coord(coord_z4, 0)

        # group-by with multiple coordinate names on shared axis.
        aggregateby_cube = self.cube_multi.aggregated_by(['height', 'level'],
                                                         iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'multi_shared.cml'))

        # group-by with multiple coordinate names on shared axis (different
        # order).
        aggregateby_cube = self.cube_multi.aggregated_by(['level', 'height'],
                                                         iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'multi_shared.cml'))

        # group-by with multiple coordinates on shared axis.
        aggregateby_cube = self.cube_multi.aggregated_by(
            [self.coord_z1_multi, self.coord_z2_multi], iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'multi_shared.cml'))

        # group-by with multiple coordinates on shared axis (different order).
        aggregateby_cube = self.cube_multi.aggregated_by(
            [self.coord_z2_multi, self.coord_z1_multi], iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'multi_shared.cml'))

        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       self.multi_expected)

    def test_easy(self):
        data = np.array([[6, 10, 12, 18], [8, 12, 14, 20], [18, 12, 10, 6]],
                        dtype=np.float32)
        cube = iris.cube.Cube(data, long_name='temperature', units='kelvin')

        llcs = iris.coord_systems.GeogCS(6371229)
        cube.add_aux_coord(iris.coords.AuxCoord(
            np.array([0, 0, 10], dtype=np.float32),
            'latitude', units='degrees', coord_system=llcs), 0)
        cube.add_aux_coord(iris.coords.AuxCoord(
            np.array([0, 0, 10, 10], dtype=np.float32),
            'longitude', units='degrees', coord_system=llcs), 1)

        #
        # Easy mean aggregate test by each coordinate.
        #
        aggregateby_cube = cube.aggregated_by('longitude', iris.analysis.MEAN)
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       np.array([[8., 15.],
                                                 [10., 17.],
                                                 [15., 8.]], dtype=np.float32))
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'easy.cml'),
                       checksum=False)

        aggregateby_cube = cube.aggregated_by('latitude', iris.analysis.MEAN)
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       np.array([[7., 11., 13., 19.],
                                                 [18., 12., 10., 6.]],
                                                dtype=np.float32))

        #
        # Easy max aggregate test by each coordinate.
        #
        aggregateby_cube = cube.aggregated_by('longitude', iris.analysis.MAX)
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       np.array([[10., 18.],
                                                 [12., 20.],
                                                 [18., 10.]],
                                                dtype=np.float32))

        aggregateby_cube = cube.aggregated_by('latitude', iris.analysis.MAX)
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       np.array([[8., 12., 14., 20.],
                                                 [18., 12., 10., 6.]],
                                                dtype=np.float32))

        #
        # Easy sum aggregate test by each coordinate.
        #
        aggregateby_cube = cube.aggregated_by('longitude', iris.analysis.SUM)
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       np.array([[16., 30.],
                                                 [20., 34.],
                                                 [30., 16.]],
                                                dtype=np.float32))

        aggregateby_cube = cube.aggregated_by('latitude', iris.analysis.SUM)
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       np.array([[14., 22., 26., 38.],
                                                 [18., 12., 10., 6.]],
                                                dtype=np.float32))

        #
        # Easy percentile aggregate test by each coordinate.
        #
        aggregateby_cube = cube.aggregated_by('longitude',
                                              iris.analysis.PERCENTILE,
                                              percent=25)
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       np.array([[7.0, 13.5],
                                                 [9.0, 15.5],
                                                 [13.5, 7.0]],
                                                dtype=np.float32))

        aggregateby_cube = cube.aggregated_by('latitude',
                                              iris.analysis.PERCENTILE,
                                              percent=25)
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       np.array([[6.5, 10.5, 12.5, 18.5],
                                                 [18., 12., 10., 6.]],
                                                dtype=np.float32))

        #
        # Easy root mean square aggregate test by each coordinate.
        #
        aggregateby_cube = cube.aggregated_by('longitude', iris.analysis.RMS)
        row = [list(np.sqrt([68., 234.])),
               list(np.sqrt([104., 298.])),
               list(np.sqrt([234., 68.]))]
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       np.array(row, dtype=np.float32))

        aggregateby_cube = cube.aggregated_by('latitude', iris.analysis.RMS)
        row = [list(np.sqrt([50., 122., 170., 362.])), [18., 12., 10., 6.]]
        np.testing.assert_almost_equal(aggregateby_cube.data,
                                       np.array(row, dtype=np.float32))

    def test_single_missing(self):
        # aggregation correctly handles masked data
        mask = np.vstack(
            (np.array([[[0, 1, 0], [1, 0, 1], [0, 1, 0]]]).repeat(26, axis=0),
             np.zeros([10, 3, 3])))
        self.cube_single.data = ma.array(self.cube_single.data, mask=mask)
        single_expected = ma.masked_invalid(
            [[[0., np.nan, 0.],
              [np.nan, 0., np.nan],
              [0., np.nan, 0.]],
             [[1.5, np.nan, 4.5],
              [np.nan, 7.5, np.nan],
              [10.5, np.nan, 13.5]],
             [[4., np.nan, 12.],
              [np.nan, 20., np.nan],
              [28., np.nan, 36.]],
             [[7.5, np.nan, 22.5],
              [np.nan, 37.5, np.nan],
              [52.5, np.nan, 67.5]],
             [[12., np.nan, 36.],
              [np.nan, 60., np.nan],
              [84., np.nan, 108.]],
             [[17.5, np.nan, 52.5],
              [np.nan, 87.5, np.nan],
              [122.5, np.nan, 157.5]],
             [[24., 53., 72.],
              [106., 120., 159.],
              [168., 212., 216.]],
             [[31.5, 63., 94.5],
              [126., 157.5, 189.],
              [220.5, 252., 283.5]]])
        aggregateby_cube = self.cube_single.aggregated_by('height',
                                                          iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'single_missing.cml'),
                       checksum=False)
        self.assertMaskedArrayAlmostEqual(aggregateby_cube.data,
                                          single_expected)

    def test_multi_missing(self):
        # aggregation correctly handles masked data
        mask = np.vstack(
            (np.array([[[0, 1, 0], [1, 0, 1], [0, 1, 0]]]).repeat(16, axis=0),
             np.ones([2, 3, 3]),
             np.zeros([2, 3, 3])))
        self.cube_multi.data = ma.array(self.cube_multi.data, mask=mask)
        multi_expected = ma.masked_invalid(
            [[[1., np.nan, 3.],
              [np.nan, 5., np.nan],
              [7., np.nan, 9.]],
             [[3.5, np.nan, 10.5],
              [np.nan, 17.5, np.nan],
              [24.5, np.nan, 31.5]],
             [[14., 37., 42.],
              [74., 70., 111.],
              [98., 148., 126.]],
             [[7., np.nan, 21.],
              [np.nan, 35., np.nan],
              [49., np.nan, 63.]],
             [[9., np.nan, 27.],
              [np.nan, 45., np.nan],
              [63., np.nan, 81.]],
             [[10.5, np.nan, 31.5],
              [np.nan, 52.5, np.nan],
              [73.5, np.nan, 94.5]],
             [[13., np.nan, 39.],
              [np.nan, 65., np.nan],
              [91., np.nan, 117.]],
             [[15., np.nan, 45.],
              [np.nan, 75., np.nan],
              [105., np.nan, 135.]],
             [[np.nan, np.nan, np.nan],
              [np.nan, np.nan, np.nan],
              [np.nan, np.nan, np.nan]]])
        aggregateby_cube = self.cube_multi.aggregated_by(['height', 'level'],
                                                         iris.analysis.MEAN)
        self.assertCML(aggregateby_cube,
                       ('analysis', 'aggregated_by', 'multi_missing.cml'),
                       checksum=False)
        self.assertMaskedArrayAlmostEqual(aggregateby_cube.data,
                                          multi_expected)

    def test_returned_weights(self):
        self.assertRaises(ValueError, self.cube_single.aggregated_by,
                          'height', iris.analysis.MEAN, returned=True)
        self.assertRaises(ValueError, self.cube_single.aggregated_by,
                          'height', iris.analysis.MEAN,
                          weights=[1, 2, 3, 4, 5])


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_analysis
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


from __future__ import division
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import itertools

import cartopy.crs as ccrs
import numpy as np
import numpy.ma as ma
import shapely.geometry

import iris
import iris.analysis.cartography
import iris.analysis.geometry
import iris.analysis.maths
import iris.coord_systems
import iris.coords
import iris.cube
import iris.tests.stock

# Run tests in no graphics mode if matplotlib is not available.
if tests.MPL_AVAILABLE:
    import matplotlib
    import matplotlib.pyplot as plt


class TestAnalysisCubeCoordComparison(tests.IrisTest):
    def assertComparisonDict(self, comarison_dict, reference_filename):
        string = ''
        for key, coord_groups in comarison_dict.iteritems():
            string += ('%40s  ' % key)
            names = [[coord.name() if coord is not None else 'None' for coord in coords] for coords in coord_groups]
            string += str(sorted(names))
            string += '\n'
        self.assertString(string, reference_filename)

    def test_coord_comparison(self):
        cube1 = iris.cube.Cube(np.zeros((41, 41)))
        lonlat_cs = iris.coord_systems.GeogCS(6371229)
        lon_points1 = -180 + 4.5 * np.arange(41, dtype=np.float32)
        lat_points = -90 + 4.5 * np.arange(41, dtype=np.float32)
        cube1.add_dim_coord(iris.coords.DimCoord(lon_points1, 'longitude', units='degrees', coord_system=lonlat_cs), 0)
        cube1.add_dim_coord(iris.coords.DimCoord(lat_points, 'latitude', units='degrees', coord_system=lonlat_cs), 1)
        cube1.add_aux_coord(iris.coords.AuxCoord(0, long_name='z'))
        cube1.add_aux_coord(iris.coords.AuxCoord(['foobar'], long_name='f', units='no_unit'))

        cube2 = iris.cube.Cube(np.zeros((41, 41, 5)))
        lonlat_cs = iris.coord_systems.GeogCS(6371229)
        lon_points2 = -160 + 4.5 * np.arange(41, dtype=np.float32)
        cube2.add_dim_coord(iris.coords.DimCoord(lon_points2, 'longitude', units='degrees', coord_system=lonlat_cs), 0)
        cube2.add_dim_coord(iris.coords.DimCoord(lat_points, 'latitude', units='degrees', coord_system=lonlat_cs), 1)
        cube2.add_dim_coord(iris.coords.DimCoord([5, 7, 9, 11, 13], long_name='z'), 2)

        cube3 = cube1.copy()
        lon = cube3.coord("longitude")
        lat = cube3.coord("latitude")
        cube3.remove_coord(lon)
        cube3.remove_coord(lat)
        cube3.add_dim_coord(lon, 1)
        cube3.add_dim_coord(lat, 0)
        cube3.coord('z').points = [20]

        cube4 = cube2.copy()
        lon = cube4.coord("longitude")
        lat = cube4.coord("latitude")
        cube4.remove_coord(lon)
        cube4.remove_coord(lat)
        cube4.add_dim_coord(lon, 1)
        cube4.add_dim_coord(lat, 0)

        coord_comparison = iris.analysis.coord_comparison

        self.assertComparisonDict(coord_comparison(cube1, cube1), ('analysis', 'coord_comparison', 'cube1_cube1.txt'))
        self.assertComparisonDict(coord_comparison(cube1, cube2), ('analysis', 'coord_comparison', 'cube1_cube2.txt'))
        self.assertComparisonDict(coord_comparison(cube1, cube3), ('analysis', 'coord_comparison', 'cube1_cube3.txt'))
        self.assertComparisonDict(coord_comparison(cube1, cube4), ('analysis', 'coord_comparison', 'cube1_cube4.txt'))
        self.assertComparisonDict(coord_comparison(cube2, cube3), ('analysis', 'coord_comparison', 'cube2_cube3.txt'))
        self.assertComparisonDict(coord_comparison(cube2, cube4), ('analysis', 'coord_comparison', 'cube2_cube4.txt'))
        self.assertComparisonDict(coord_comparison(cube3, cube4), ('analysis', 'coord_comparison', 'cube3_cube4.txt'))

        self.assertComparisonDict(coord_comparison(cube1, cube1, cube1), ('analysis', 'coord_comparison', 'cube1_cube1_cube1.txt'))
        self.assertComparisonDict(coord_comparison(cube1, cube2, cube1), ('analysis', 'coord_comparison', 'cube1_cube2_cube1.txt'))

        # get a coord comparison result and check that we are getting back what was expected
        coord_group = coord_comparison(cube1, cube2)['grouped_coords'][0]
        self.assertIsInstance(coord_group, iris.analysis._CoordGroup)
        self.assertIsInstance(list(coord_group)[0], iris.coords.Coord)


class TestAnalysisWeights(tests.IrisTest):
    def test_weighted_mean_little(self):
        data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)
        weights = np.array([[9, 8, 7], [6, 5, 4], [3, 2, 1]], dtype=np.float32)

        cube = iris.cube.Cube(data, long_name="test_data", units="1")
        hcs = iris.coord_systems.GeogCS(6371229)
        lat_coord = iris.coords.DimCoord(np.array([1, 2, 3], dtype=np.float32), long_name="lat", units="1", coord_system=hcs)
        lon_coord = iris.coords.DimCoord(np.array([1, 2, 3], dtype=np.float32), long_name="lon", units="1", coord_system=hcs)
        cube.add_dim_coord(lat_coord, 0)
        cube.add_dim_coord(lon_coord, 1)
        cube.add_aux_coord(iris.coords.AuxCoord(np.arange(3, dtype=np.float32), long_name="dummy", units=1), 1)
        self.assertCML(cube, ('analysis', 'weighted_mean_source.cml'))

        a = cube.collapsed('lat', iris.analysis.MEAN, weights=weights)
        self.assertCMLApproxData(a, ('analysis', 'weighted_mean_lat.cml'))

        b = cube.collapsed(lon_coord, iris.analysis.MEAN, weights=weights)
        b.data = np.asarray(b.data)
        self.assertCMLApproxData(b, ('analysis', 'weighted_mean_lon.cml'))
        self.assertEquals(b.coord("dummy").shape, (1,))

        # test collapsing multiple coordinates (and the fact that one of the coordinates isn't the same coordinate instance as on the cube)
        c = cube.collapsed([lat_coord[:], lon_coord], iris.analysis.MEAN, weights=weights)
        self.assertCMLApproxData(c, ('analysis', 'weighted_mean_latlon.cml'))
        self.assertEquals(c.coord("dummy").shape, (1,))

        # Check new coord bounds - made from points
        self.assertArrayEqual(c.coord('lat').bounds, [[1, 3]])

        # Check new coord bounds - made from bounds
        cube.coord('lat').bounds = [[0.5, 1.5], [1.5, 2.5], [2.5, 3.5]]
        c = cube.collapsed(['lat', 'lon'], iris.analysis.MEAN, weights=weights)
        self.assertArrayEqual(c.coord('lat').bounds, [[0.5, 3.5]])
        cube.coord('lat').bounds = None

        # Check there was no residual change
        self.assertCML(cube, ('analysis', 'weighted_mean_source.cml'))

    @tests.skip_data
    def test_weighted_mean(self):
        ### compare with pp_area_avg - which collapses both lat and lon
        #
        #     pp = ppa('/data/local/dataZoo/PP/simple_pp/global.pp', 0)
        #     print, pp_area(pp, /box)
        #     print, pp_area_avg(pp, /box)  #287.927
        #     ;gives an answer of 287.927
        #
        ###
        e = iris.tests.stock.simple_pp()
        self.assertCML(e, ('analysis', 'weighted_mean_original.cml'))
        e.coord('latitude').guess_bounds()
        e.coord('longitude').guess_bounds()
        area_weights = iris.analysis.cartography.area_weights(e)
        e.coord('latitude').bounds = None
        e.coord('longitude').bounds = None
        f, collapsed_area_weights = e.collapsed('latitude', iris.analysis.MEAN, weights=area_weights, returned=True)
        g = f.collapsed('longitude', iris.analysis.MEAN, weights=collapsed_area_weights)
        # check it's a 1D, scalar cube (actually, it should really be 0D)!
        self.assertEquals(g.shape, (1,))
        # check the value - pp_area_avg's result of 287.927 differs by factor of 1.00002959
        np.testing.assert_approx_equal(g.data[0], 287.935, significant=5)

        #check we get summed weights even if we don't give any
        h, summed_weights = e.collapsed('latitude', iris.analysis.MEAN, returned=True)
        assert(summed_weights is not None)

        # Check there was no residual change
        e.coord('latitude').bounds = None
        e.coord('longitude').bounds = None
        self.assertCML(e, ('analysis', 'weighted_mean_original.cml'))

        # Test collapsing of missing coord
        self.assertRaises(iris.exceptions.CoordinateNotFoundError, e.collapsed, 'platitude', iris.analysis.MEAN)

        # Test collpasing of non data coord
        self.assertRaises(iris.exceptions.CoordinateCollapseError, e.collapsed, 'pressure', iris.analysis.MEAN)

@tests.skip_data
class TestAnalysisBasic(tests.IrisTest):
    def setUp(self):
        file = tests.get_data_path(('PP', 'aPProt1', 'rotatedMHtimecube.pp'))
        cubes = iris.load(file)
        self.cube = cubes[0]
        self.assertCML(self.cube, ('analysis', 'original.cml'))

    def _common(self, name, aggregate, original_name='original_common.cml', *args, **kwargs):
        self.cube.data = self.cube.data.astype(np.float64)

        self.assertCML(self.cube, ('analysis', original_name))

        a = self.cube.collapsed('grid_latitude', aggregate)
        self.assertCMLApproxData(a, ('analysis', '%s_latitude.cml' % name), *args, **kwargs)

        b = a.collapsed('grid_longitude', aggregate)
        self.assertCMLApproxData(b, ('analysis', '%s_latitude_longitude.cml' % name), *args, **kwargs)

        c = self.cube.collapsed(['grid_latitude', 'grid_longitude'], aggregate)
        self.assertCMLApproxData(c, ('analysis', '%s_latitude_longitude_1call.cml' % name), *args, **kwargs)

        # Check there was no residual change
        self.assertCML(self.cube, ('analysis', original_name))

    def test_mean(self):
        self._common('mean', iris.analysis.MEAN, decimal=1)

    def test_std_dev(self):
        # as the numbers are so high, trim off some trailing digits & compare to 0dp
        self._common('std_dev', iris.analysis.STD_DEV, decimal=1)

    def test_hmean(self):
        # harmonic mean requires data > 0
        self.cube.data *= self.cube.data
        self._common('hmean', iris.analysis.HMEAN, 'original_hmean.cml', decimal=1)

    def test_gmean(self):
        self._common('gmean', iris.analysis.GMEAN, decimal=1)

    def test_variance(self):
        # as the numbers are so high, trim off some trailing digits & compare to 0dp
        self._common('variance', iris.analysis.VARIANCE, decimal=1)

    def test_median(self):
        self._common('median', iris.analysis.MEDIAN)

    def test_sum(self):
        # as the numbers are so high, trim off some trailing digits & compare to 0dp
        self._common('sum', iris.analysis.SUM, decimal=1)

    def test_max(self):
        self._common('max', iris.analysis.MAX)

    def test_min(self):
        self._common('min', iris.analysis.MIN)

    def test_rms(self):
        self._common('rms', iris.analysis.RMS)

    def test_duplicate_coords(self):
        self.assertRaises(ValueError, tests.stock.track_1d, duplicate_x=True)

    def test_xy_range(self):
        result_non_circ = iris.analysis.cartography._xy_range(self.cube)
        self.assertEqual(self.cube.coord('grid_longitude').circular, False)
        np.testing.assert_array_almost_equal(
            result_non_circ, ((313.02, 392.11), (-22.49, 24.92)), decimal=0)

    def test_xy_range_geog_cs(self):
        cube = iris.tests.stock.global_pp()
        self.assertTrue(cube.coord('longitude').circular)
        result = iris.analysis.cartography._xy_range(cube)
        np.testing.assert_array_almost_equal(
            result, ((0, 360), (-90, 90)), decimal=0)

    def test_xy_range_geog_cs_regional(self):
        cube = iris.tests.stock.global_pp()
        cube = cube[10:20, 20:30]
        self.assertFalse(cube.coord('longitude').circular)
        result = iris.analysis.cartography._xy_range(cube)
        np.testing.assert_array_almost_equal(
            result, ((75, 108.75), (42.5, 65)), decimal=0)


class TestMissingData(tests.IrisTest):
    def setUp(self):
        self.cube_with_nan = tests.stock.simple_2d()

        data = self.cube_with_nan.data.astype(np.float32)
        self.cube_with_nan.data = data.copy()
        self.cube_with_nan.data[1, 0] = np.nan
        self.cube_with_nan.data[2, 2] = np.nan
        self.cube_with_nan.data[2, 3] = np.nan

        self.cube_with_mask = tests.stock.simple_2d()
        self.cube_with_mask.data = ma.array(self.cube_with_nan.data,
                                                  mask=np.isnan(self.cube_with_nan.data))

    def test_max(self):
        cube = self.cube_with_nan.collapsed('foo', iris.analysis.MAX)
        np.testing.assert_array_equal(cube.data, np.array([3, np.nan, np.nan]))

        cube = self.cube_with_mask.collapsed('foo', iris.analysis.MAX)
        np.testing.assert_array_equal(cube.data, np.array([3, 7, 9]))

    def test_min(self):
        cube = self.cube_with_nan.collapsed('foo', iris.analysis.MIN)
        np.testing.assert_array_equal(cube.data, np.array([0, np.nan, np.nan]))

        cube = self.cube_with_mask.collapsed('foo', iris.analysis.MIN)
        np.testing.assert_array_equal(cube.data, np.array([0, 5, 8]))

    def test_sum(self):
        cube = self.cube_with_nan.collapsed('foo', iris.analysis.SUM)
        np.testing.assert_array_equal(cube.data, np.array([6, np.nan, np.nan]))

        cube = self.cube_with_mask.collapsed('foo', iris.analysis.SUM)
        np.testing.assert_array_equal(cube.data, np.array([6, 18, 17]))


class TestAggregator_mdtol_keyword(tests.IrisTest):
    def setUp(self):
        data = ma.array([[1, 2], [4, 5]], dtype=np.float32,
                        mask=[[False, True], [False, True]])
        cube = iris.cube.Cube(data, long_name="test_data", units="1")
        lat_coord = iris.coords.DimCoord(np.array([1, 2], dtype=np.float32),
                                         long_name="lat", units="1")
        lon_coord = iris.coords.DimCoord(np.array([3, 4], dtype=np.float32),
                                         long_name="lon", units="1")
        cube.add_dim_coord(lat_coord, 0)
        cube.add_dim_coord(lon_coord, 1)
        self.cube = cube

    def test_single_coord_no_mdtol(self):
        collapsed = self.cube.collapsed(
            self.cube.coord('lat'), iris.analysis.MEAN)
        t = ma.array([2.5, 5.], mask=[False, True])
        self.assertMaskedArrayEqual(collapsed.data, t)

    def test_single_coord_mdtol(self):
        self.cube.data.mask = np.array([[False, True], [False, False]])
        collapsed = self.cube.collapsed(
            self.cube.coord('lat'), iris.analysis.MEAN, mdtol=0.5)
        t = ma.array([2.5, 5], mask=[False, False])
        self.assertMaskedArrayEqual(collapsed.data, t)

    def test_single_coord_mdtol_alt(self):
        self.cube.data.mask = np.array([[False, True], [False, False]])
        collapsed = self.cube.collapsed(
            self.cube.coord('lat'), iris.analysis.MEAN, mdtol=0.4)
        t = ma.array([2.5, 5], mask=[False, True])
        self.assertMaskedArrayEqual(collapsed.data, t)

    def test_multi_coord_no_mdtol(self):
        collapsed = self.cube.collapsed(
            [self.cube.coord('lat'), self.cube.coord('lon')],
            iris.analysis.MEAN)
        t = np.array([2.5])
        self.assertArrayEqual(collapsed.data, t)

    def test_multi_coord_mdtol(self):
        collapsed = self.cube.collapsed(
            [self.cube.coord('lat'), self.cube.coord('lon')],
            iris.analysis.MEAN, mdtol=0.4)
        t = ma.array([2.5], mask=[True])
        self.assertMaskedArrayEqual(collapsed.data, t)


class TestAggregators(tests.IrisTest):
    def test_percentile_1d(self):
        cube = tests.stock.simple_1d()

        first_quartile = cube.collapsed('foo', iris.analysis.PERCENTILE,
                                        percent=25)
        np.testing.assert_array_almost_equal(first_quartile.data,
                                             np.array([2.5], dtype=np.float32))
        self.assertCML(first_quartile, ('analysis',
                                        'first_quartile_foo_1d.cml'),
                       checksum=False)

        third_quartile = cube.collapsed('foo', iris.analysis.PERCENTILE,
                                        percent=75)
        np.testing.assert_array_almost_equal(third_quartile.data,
                                             np.array([7.5],
                                             dtype=np.float32))
        self.assertCML(third_quartile,
                       ('analysis', 'third_quartile_foo_1d.cml'),
                       checksum=False)

    def test_percentile_2d(self):
        cube = tests.stock.simple_2d()

        first_quartile = cube.collapsed('foo', iris.analysis.PERCENTILE,
                                        percent=25)
        np.testing.assert_array_almost_equal(first_quartile.data,
                                             np.array([0.75, 4.75, 8.75],
                                             dtype=np.float32))
        self.assertCML(first_quartile, ('analysis',
                                        'first_quartile_foo_2d.cml'),
                       checksum=False)

        first_quartile = cube.collapsed(('foo', 'bar'),
                                        iris.analysis.PERCENTILE, percent=25)
        np.testing.assert_array_almost_equal(first_quartile.data,
                                             np.array([2.75],
                                             dtype=np.float32))
        self.assertCML(first_quartile, ('analysis',
                                        'first_quartile_foo_bar_2d.cml'),
                       checksum=False)

    def test_percentile_3d(self):
        array_3d = np.arange(24, dtype=np.int32).reshape((2, 3, 4))

        last_quartile = iris.analysis._percentile(array_3d, 0, 50)
        np.testing.assert_array_almost_equal(last_quartile,
                                             np.array([[6., 7., 8., 9.],
                                                       [10., 11., 12., 13.],
                                                       [14., 15., 16., 17.]],
                                             dtype=np.float32))

    def test_percentile_3d_axis_one(self):
        array_3d = np.arange(24, dtype=np.int32).reshape((2, 3, 4))

        last_quartile = iris.analysis._percentile(array_3d, 1, 50)
        np.testing.assert_array_almost_equal(last_quartile,
                                             np.array([[4., 5., 6., 7.],
                                                       [16., 17., 18., 19.]],
                                             dtype=np.float32))

    def test_percentile_3d_axis_two(self):
        array_3d = np.arange(24, dtype=np.int32).reshape((2, 3, 4))

        last_quartile = iris.analysis._percentile(array_3d, 2, 50)
        np.testing.assert_array_almost_equal(last_quartile,
                                             np.array([[1.5, 5.5, 9.5],
                                                       [13.5, 17.5, 21.5]],
                                             dtype=np.float32))

    def test_percentile_3d_masked(self):
        cube = tests.stock.simple_3d_mask()

        last_quartile = cube.collapsed('wibble',
                                       iris.analysis.PERCENTILE, percent=75)
        np.testing.assert_array_almost_equal(last_quartile.data,
                                             np.array([[12., 13., 14., 15.],
                                                       [16., 17., 18., 19.],
                                                       [20., 18., 19., 20.]],
                                             dtype=np.float32))
        self.assertCML(last_quartile, ('analysis',
                                       'last_quartile_foo_3d_masked.cml'),
                       checksum=False)

    def test_percentile_3d_notmasked(self):
        cube = tests.stock.simple_3d()

        last_quartile = cube.collapsed('wibble',
                                       iris.analysis.PERCENTILE, percent=75)
        np.testing.assert_array_almost_equal(last_quartile.data,
                                             np.array([[9., 10., 11., 12.],
                                                       [13., 14., 15., 16.],
                                                       [17., 18., 19., 20.]],
                                             dtype=np.float32))
        self.assertCML(last_quartile, ('analysis',
                                       'last_quartile_foo_3d_notmasked.cml'),
                       checksum=False)

    def test_proportion(self):
        cube = tests.stock.simple_1d()
        r = cube.data >= 5
        gt5 = cube.collapsed('foo', iris.analysis.PROPORTION, function=lambda val: val >= 5)
        np.testing.assert_array_almost_equal(gt5.data, np.array([6 / 11.]))
        self.assertCML(gt5, ('analysis', 'proportion_foo_1d.cml'), checksum=False)

    def test_proportion_2d(self):
        cube = tests.stock.simple_2d()

        gt6 = cube.collapsed('foo', iris.analysis.PROPORTION, function=lambda val: val >= 6)
        np.testing.assert_array_almost_equal(gt6.data, np.array([0, 0.5, 1], dtype=np.float32))
        self.assertCML(gt6, ('analysis', 'proportion_foo_2d.cml'), checksum=False)

        gt6 = cube.collapsed('bar', iris.analysis.PROPORTION, function=lambda val: val >= 6)
        np.testing.assert_array_almost_equal(gt6.data, np.array([1 / 3, 1 / 3, 2 / 3, 2 / 3], dtype=np.float32))
        self.assertCML(gt6, ('analysis', 'proportion_bar_2d.cml'), checksum=False)

        gt6 = cube.collapsed(('foo', 'bar'), iris.analysis.PROPORTION, function=lambda val: val >= 6)
        np.testing.assert_array_almost_equal(gt6.data, np.array([0.5], dtype=np.float32))
        self.assertCML(gt6, ('analysis', 'proportion_foo_bar_2d.cml'), checksum=False)

        # mask the data
        cube.data = ma.array(cube.data, mask=cube.data % 2)
        cube.data.mask[1, 2] = True
        gt6_masked = cube.collapsed('bar', iris.analysis.PROPORTION, function=lambda val: val >= 6)
        np.testing.assert_array_almost_equal(gt6_masked.data, ma.array([1 / 3, None, 1 / 2, None],
                                                                                mask=[False, True, False, True],
                                                                                dtype=np.float32))
        self.assertCML(gt6_masked, ('analysis', 'proportion_foo_2d_masked.cml'), checksum=False)

    def test_count(self):
        cube = tests.stock.simple_1d()
        gt5 = cube.collapsed('foo', iris.analysis.COUNT, function=lambda val: val >= 5)
        np.testing.assert_array_almost_equal(gt5.data, np.array([6]))
        gt5.data = gt5.data.astype('i8')
        self.assertCML(gt5, ('analysis', 'count_foo_1d.cml'), checksum=False)

    def test_count_2d(self):
        cube = tests.stock.simple_2d()

        gt6 = cube.collapsed('foo', iris.analysis.COUNT, function=lambda val: val >= 6)
        np.testing.assert_array_almost_equal(gt6.data, np.array([0, 2, 4], dtype=np.float32))
        gt6.data = gt6.data.astype('i8')
        self.assertCML(gt6, ('analysis', 'count_foo_2d.cml'), checksum=False)

        gt6 = cube.collapsed('bar', iris.analysis.COUNT, function=lambda val: val >= 6)
        np.testing.assert_array_almost_equal(gt6.data, np.array([1, 1, 2, 2], dtype=np.float32))
        gt6.data = gt6.data.astype('i8')
        self.assertCML(gt6, ('analysis', 'count_bar_2d.cml'), checksum=False)

        gt6 = cube.collapsed(('foo', 'bar'), iris.analysis.COUNT, function=lambda val: val >= 6)
        np.testing.assert_array_almost_equal(gt6.data, np.array([6], dtype=np.float32))
        gt6.data = gt6.data.astype('i8')
        self.assertCML(gt6, ('analysis', 'count_foo_bar_2d.cml'), checksum=False)

    def test_weighted_sum_consistency(self):
        # weighted sum with unit weights should be the same as a sum
        cube = tests.stock.simple_1d()
        normal_sum = cube.collapsed('foo', iris.analysis.SUM)
        weights = np.ones_like(cube.data)
        weighted_sum = cube.collapsed('foo', iris.analysis.SUM, weights=weights)
        self.assertArrayAlmostEqual(normal_sum.data, weighted_sum.data)

    def test_weighted_sum_1d(self):
        # verify 1d weighted sum is correct
        cube = tests.stock.simple_1d()
        weights = np.array([.05, .05, .1, .1, .2, .3, .2, .1, .1, .05, .05])
        result = cube.collapsed('foo', iris.analysis.SUM, weights=weights)
        self.assertAlmostEqual(result.data, 6.5)
        self.assertCML(result, ('analysis', 'sum_weighted_1d.cml'),
                       checksum=False)

    def test_weighted_sum_2d(self):
        # verify 2d weighted sum is correct
        cube = tests.stock.simple_2d()
        weights = np.array([.3, .4, .3])
        weights = iris.util.broadcast_to_shape(weights, cube.shape, [0])
        result = cube.collapsed('bar', iris.analysis.SUM, weights=weights)
        self.assertArrayAlmostEqual(result.data, np.array([4., 5., 6., 7.]))
        self.assertCML(result, ('analysis', 'sum_weighted_2d.cml'),
                       checksum=False)

    def test_weighted_rms(self):
        cube = tests.stock.simple_2d()
        # modify cube data so that the results are nice numbers
        cube.data = np.array([[4, 7, 10, 8],
                              [21, 30, 12, 24],
                              [14, 16, 20, 8]],
                             dtype=np.float64)
        weights = np.array([[1, 4, 3, 2],
                            [6, 4.5, 1.5, 3],
                            [2, 1, 1.5, 0.5]],
                           dtype=np.float64)
        expected_result = np.array([8.0, 24.0, 16.0])
        result = cube.collapsed('foo', iris.analysis.RMS, weights=weights)
        self.assertArrayAlmostEqual(result.data, expected_result)
        self.assertCML(result, ('analysis', 'rms_weighted_2d.cml'),
                       checksum=False)


@tests.skip_data
class TestRotatedPole(tests.GraphicsTest):
    @tests.skip_plot
    def _check_both_conversions(self, cube):
        rlons, rlats = iris.analysis.cartography.get_xy_grids(cube)
        rcs = cube.coord_system('RotatedGeogCS')
        x, y = iris.analysis.cartography.unrotate_pole(
            rlons, rlats, rcs.grid_north_pole_longitude,
            rcs.grid_north_pole_latitude)
        plt.scatter(x, y)
        self.check_graphic()

        plt.scatter(rlons, rlats)
        self.check_graphic()

    def test_all(self):
        path = tests.get_data_path(('PP', 'ukVorog', 'ukv_orog_refonly.pp'))
        master_cube = iris.load_cube(path)

        # Check overall behaviour.
        cube = master_cube[::10, ::10]
        self._check_both_conversions(cube)

        # Check numerical stability.
        cube = master_cube[210:238, 424:450]
        self._check_both_conversions(cube)

    def test_unrotate_nd(self):
        rlons = np.array([[350., 352.],[350., 352.]])
        rlats = np.array([[-5., -0.],[-4., -1.]])

        resx, resy = iris.analysis.cartography.unrotate_pole(rlons, rlats,
                                                             178.0, 38.0)

        # Solutions derived by proj4 direct.
        solx = np.array([[-16.42176094, -14.85892262],
                            [-16.71055023, -14.58434624]])
        soly = np.array([[ 46.00724251,  51.29188893],
                            [ 46.98728486,  50.30706042]])

        self.assertArrayAlmostEqual(resx, solx)
        self.assertArrayAlmostEqual(resy, soly)

    def test_unrotate_1d(self):
        rlons = np.array([350., 352., 354., 356.])
        rlats = np.array([-5., -0., 5., 10.])

        resx, resy = iris.analysis.cartography.unrotate_pole(
            rlons.flatten(), rlats.flatten(), 178.0, 38.0)

        # Solutions derived by proj4 direct.
        solx = np.array([-16.42176094, -14.85892262,
                            -12.88946157, -10.35078336])
        soly = np.array([46.00724251, 51.29188893,
                            56.55031485, 61.77015703])

        self.assertArrayAlmostEqual(resx, solx)
        self.assertArrayAlmostEqual(resy, soly)

    def test_rotate_nd(self):
        rlons = np.array([[350., 351.],[352., 353.]])
        rlats = np.array([[10., 15.],[20., 25.]])

        resx, resy = iris.analysis.cartography.rotate_pole(rlons, rlats,
                                                           20., 80.)

        # Solutions derived by proj4 direct.
        solx = np.array([[148.69672569, 149.24727087],
                            [149.79067025, 150.31754368]])
        soly = np.array([[18.60905789, 23.67749384],
                            [28.74419024, 33.8087963 ]])

        self.assertArrayAlmostEqual(resx, solx)
        self.assertArrayAlmostEqual(resy, soly)

    def test_rotate_1d(self):
        rlons = np.array([350., 351., 352., 353.])
        rlats = np.array([10., 15., 20., 25.])

        resx, resy = iris.analysis.cartography.rotate_pole(rlons.flatten(),
                                     rlats.flatten(), 20., 80.)

        # Solutions derived by proj4 direct.
        solx = np.array([148.69672569, 149.24727087,
                            149.79067025, 150.31754368])
        soly = np.array([18.60905789, 23.67749384,
                            28.74419024, 33.8087963 ])

        self.assertArrayAlmostEqual(resx, solx)
        self.assertArrayAlmostEqual(resy, soly)


@tests.skip_data
class TestAreaWeights(tests.IrisTest):
    def test_area_weights(self):
        small_cube = iris.tests.stock.simple_pp()
        # Get offset, subsampled region: small enough to test against literals
        small_cube = small_cube[10:, 35:]
        small_cube = small_cube[::8, ::8]
        small_cube = small_cube[:5, :4]
        # pre-check non-data properties
        self.assertCML(small_cube, ('analysis', 'areaweights_original.cml'),
                       checksum=False)

        # check area-weights values
        small_cube.coord('latitude').guess_bounds()
        small_cube.coord('longitude').guess_bounds()
        area_weights = iris.analysis.cartography.area_weights(small_cube)
        expected_results = np.array(
            [[3.11955916e+12, 3.11956058e+12, 3.11955916e+12, 3.11956058e+12],
             [5.21950793e+12, 5.21951031e+12, 5.21950793e+12, 5.21951031e+12],
             [6.68991432e+12, 6.68991737e+12, 6.68991432e+12, 6.68991737e+12],
             [7.35341320e+12, 7.35341655e+12, 7.35341320e+12, 7.35341655e+12],
             [7.12998265e+12, 7.12998589e+12, 7.12998265e+12, 7.12998589e+12]],
            dtype=np.float64)
        self.assertArrayAllClose(area_weights, expected_results, rtol=1e-8)

        # Check there was no residual change
        small_cube.coord('latitude').bounds = None
        small_cube.coord('longitude').bounds = None
        self.assertCML(small_cube, ('analysis', 'areaweights_original.cml'),
                       checksum=False)

    def test_quadrant_area(self):

        degrees = iris.unit.Unit("degrees")
        radians = iris.unit.Unit("radians")

        def lon2radlon(lons):
            return [degrees.convert(lon, radians) for lon in lons]

        def lat2radcolat(lats):
            return [degrees.convert(lat + 90, radians) for lat in lats]

        lats = np.array([lat2radcolat([-80, -70])])
        lons = np.array([lon2radlon([0, 10])])
        area = iris.analysis.cartography._quadrant_area(lats, lons, iris.analysis.cartography.DEFAULT_SPHERICAL_EARTH_RADIUS)
        self.assertAlmostEquals(area, [[319251845980.763671875]])

        lats = np.array([lat2radcolat([0, 10])])
        lons = np.array([lon2radlon([0, 10])])
        area = iris.analysis.cartography._quadrant_area(lats, lons, iris.analysis.cartography.DEFAULT_SPHERICAL_EARTH_RADIUS)
        self.assertAlmostEquals(area, [[1228800593851.443115234375]])

        lats = np.array([lat2radcolat([10, 0])])
        lons = np.array([lon2radlon([0, 10])])
        area = iris.analysis.cartography._quadrant_area(lats, lons, iris.analysis.cartography.DEFAULT_SPHERICAL_EARTH_RADIUS)
        self.assertAlmostEquals(area, [[1228800593851.443115234375]])

        lats = np.array([lat2radcolat([70, 80])])
        lons = np.array([lon2radlon([0, 10])])
        area = iris.analysis.cartography._quadrant_area(lats, lons, iris.analysis.cartography.DEFAULT_SPHERICAL_EARTH_RADIUS)
        self.assertAlmostEquals(area, [[319251845980.7646484375]])

        lats = np.array([lat2radcolat([-80, -70]), lat2radcolat([0, 10]), lat2radcolat([70, 80])])
        lons = np.array([lon2radlon([0, 10])])
        area = iris.analysis.cartography._quadrant_area(lats, lons, iris.analysis.cartography.DEFAULT_SPHERICAL_EARTH_RADIUS)

        self.assertAlmostEquals(area[0], [319251845980.763671875])
        self.assertAlmostEquals(area[1], [1228800593851.443115234375])
        self.assertAlmostEquals(area[2], [319251845980.7646484375])


class TestAreaWeightGeneration(tests.IrisTest):
    def setUp(self):
        self.cube = iris.tests.stock.realistic_4d()

    def test_area_weights_std(self):
        # weights for stock 4d data
        weights = iris.analysis.cartography.area_weights(self.cube)
        self.assertEqual(weights.shape, self.cube.shape)

    def test_area_weights_order(self):
        # weights for data with dimensions in a different order
        order = [3, 2, 1, 0] # (lon, lat, level, time)
        self.cube.transpose(order)
        weights = iris.analysis.cartography.area_weights(self.cube)
        self.assertEqual(weights.shape, self.cube.shape)

    def test_area_weights_non_adjacent(self):
        # weights for cube with non-adjacent latitude/longitude dimensions
        order = [0, 3, 1, 2] # (time, lon, level, lat)
        self.cube.transpose(order)
        weights = iris.analysis.cartography.area_weights(self.cube)
        self.assertEqual(weights.shape, self.cube.shape)

    def test_area_weights_scalar_latitude(self):
        # weights for cube with a scalar latitude dimension
        cube = self.cube[:, :, 0, :]
        weights = iris.analysis.cartography.area_weights(cube)
        self.assertEqual(weights.shape, cube.shape)

    def test_area_weights_scalar_longitude(self):
        # weights for cube with a scalar longitude dimension
        cube = self.cube[:, :, :, 0]
        weights = iris.analysis.cartography.area_weights(cube)
        self.assertEqual(weights.shape, cube.shape)

    def test_area_weights_scalar(self):
        # weights for cube with scalar latitude and longitude dimensions
        cube = self.cube[:, :, 0, 0]
        weights = iris.analysis.cartography.area_weights(cube)
        self.assertEqual(weights.shape, cube.shape)

    def test_area_weights_singleton_latitude(self):
        # singleton (1-point) latitude dimension
        cube = self.cube[:, :, 0:1, :]
        weights = iris.analysis.cartography.area_weights(cube)
        self.assertEqual(weights.shape, cube.shape)

    def test_area_weights_singleton_longitude(self):
        # singleton (1-point) longitude dimension
        cube = self.cube[:, :, :, 0:1]
        weights = iris.analysis.cartography.area_weights(cube)
        self.assertEqual(weights.shape, cube.shape)

    def test_area_weights_singletons(self):
        # singleton (1-point) latitude and longitude dimensions
        cube = self.cube[:, :, 0:1, 0:1]
        weights = iris.analysis.cartography.area_weights(cube)
        self.assertEqual(weights.shape, cube.shape)

    def test_area_weights_normalized(self):
        # normalized area weights must sum to one over lat/lon dimensions.
        weights = iris.analysis.cartography.area_weights(self.cube,
                                                         normalize=True)
        sumweights = weights.sum(axis=3).sum(axis=2)  # sum over lon and lat
        self.assertArrayAlmostEqual(sumweights, 1)

    def test_area_weights_non_contiguous(self):
        # Slice the cube so that we have non-contiguous longitude
        # bounds.
        ind = (0, 1, 2, -3, -2, -1)
        cube = self.cube[..., ind]
        weights = iris.analysis.cartography.area_weights(cube)
        expected = iris.analysis.cartography.area_weights(self.cube)[..., ind]
        self.assertArrayEqual(weights, expected)

    def test_area_weights_no_lon_bounds(self):
        self.cube.coord('grid_longitude').bounds = None
        with self.assertRaises(ValueError):
            iris.analysis.cartography.area_weights(self.cube)

    def test_area_weights_no_lat_bounds(self):
        self.cube.coord('grid_latitude').bounds = None
        with self.assertRaises(ValueError):
            iris.analysis.cartography.area_weights(self.cube)


@tests.skip_data
class TestLatitudeWeightGeneration(tests.IrisTest):

    def setUp(self):
        path = iris.tests.get_data_path(['NetCDF', 'rotated', 'xyt',
                                         'small_rotPole_precipitation.nc'])
        self.cube = iris.load_cube(path)
        self.cube_dim_lat = self.cube.copy()
        self.cube_dim_lat.remove_coord('latitude')
        self.cube_dim_lat.remove_coord('longitude')
        # The 2d cubes are unrealistic, you would not want to weight by
        # anything other than grid latitude in real-world scenarios. However,
        # the technical details are suitable for testing purposes, providing
        # a nice analog for a 2d latitude coordinate from a curvilinear grid.
        self.cube_aux_lat = self.cube.copy()
        self.cube_aux_lat.remove_coord('grid_latitude')
        self.cube_aux_lat.remove_coord('grid_longitude')
        self.lat1d = self.cube.coord('grid_latitude').points
        self.lat2d = self.cube.coord('latitude').points

    def test_cosine_latitude_weights_range(self):
        # check the range of returned values, needs a cube that spans the full
        # latitude range
        lat_coord = iris.coords.DimCoord(np.linspace(-90, 90, 73),
                                         standard_name='latitude',
                                         units=iris.unit.Unit('degrees_north'))
        cube = iris.cube.Cube(np.ones([73], dtype=np.float64),
                              long_name='test_cube', units='1')
        cube.add_dim_coord(lat_coord, 0)
        weights = iris.analysis.cartography.cosine_latitude_weights(cube)
        self.assertTrue(weights.max() <= 1)
        self.assertTrue(weights.min() >= 0)

    def test_cosine_latitude_weights_0d(self):
        # 0d latitude dimension (scalar coordinate)
        weights = iris.analysis.cartography.cosine_latitude_weights(
            self.cube_dim_lat[:, 0, :])
        self.assertEqual(weights.shape, self.cube_dim_lat[:, 0, :].shape)
        self.assertAlmostEqual(weights[0, 0],
                               np.cos(np.deg2rad(self.lat1d[0])))

    def test_cosine_latitude_weights_1d_singleton(self):
        # singleton (1-point) 1d latitude coordinate (time, lat, lon)
        cube = self.cube_dim_lat[:, 0:1, :]
        weights = iris.analysis.cartography.cosine_latitude_weights(cube)
        self.assertEqual(weights.shape, cube.shape)
        self.assertAlmostEqual(weights[0, 0, 0],
                               np.cos(np.deg2rad(self.lat1d[0])))

    def test_cosine_latitude_weights_1d(self):
        # 1d latitude coordinate (time, lat, lon)
        weights = iris.analysis.cartography.cosine_latitude_weights(
            self.cube_dim_lat)
        self.assertEqual(weights.shape, self.cube.shape)
        self.assertArrayAlmostEqual(weights[0, :, 0],
                                    np.cos(np.deg2rad(self.lat1d)))

    def test_cosine_latitude_weights_1d_latitude_first(self):
        # 1d latitude coordinate with latitude first (lat, time, lon)
        order = [1, 0, 2] # (lat, time, lon)
        self.cube_dim_lat.transpose(order)
        weights = iris.analysis.cartography.cosine_latitude_weights(
            self.cube_dim_lat)
        self.assertEqual(weights.shape, self.cube_dim_lat.shape)
        self.assertArrayAlmostEqual(weights[:, 0, 0],
                                    np.cos(np.deg2rad(self.lat1d)))

    def test_cosine_latitude_weights_1d_latitude_last(self):
        # 1d latitude coordinate with latitude last (time, lon, lat)
        order = [0, 2, 1] # (time, lon, lat)
        self.cube_dim_lat.transpose(order)
        weights = iris.analysis.cartography.cosine_latitude_weights(
            self.cube_dim_lat)
        self.assertEqual(weights.shape, self.cube_dim_lat.shape)
        self.assertArrayAlmostEqual(weights[0, 0, :],
                                    np.cos(np.deg2rad(self.lat1d)))

    def test_cosine_latitude_weights_2d_singleton1(self):
        # 2d latitude coordinate with first dimension singleton
        cube = self.cube_aux_lat[:, 0:1, :]
        weights = iris.analysis.cartography.cosine_latitude_weights(cube)
        self.assertEqual(weights.shape, cube.shape)
        self.assertArrayAlmostEqual(weights[0, :, :],
                                    np.cos(np.deg2rad(self.lat2d[0:1, :])))

    def test_cosine_latitude_weights_2d_singleton2(self):
        # 2d latitude coordinate with second dimension singleton
        cube = self.cube_aux_lat[:, :, 0:1]
        weights = iris.analysis.cartography.cosine_latitude_weights(cube)
        self.assertEqual(weights.shape, cube.shape)
        self.assertArrayAlmostEqual(weights[0, :, :],
                                    np.cos(np.deg2rad(self.lat2d[:, 0:1])))

    def test_cosine_latitude_weights_2d_singleton3(self):
        # 2d latitude coordinate with both dimensions singleton
        cube = self.cube_aux_lat[:, 0:1, 0:1]
        weights = iris.analysis.cartography.cosine_latitude_weights(cube)
        self.assertEqual(weights.shape, cube.shape)
        self.assertArrayAlmostEqual(weights[0, :, :],
                                    np.cos(np.deg2rad(self.lat2d[0:1, 0:1])))

    def test_cosine_latitude_weights_2d(self):
        # 2d latitude coordinate (time, lat, lon)
        weights = iris.analysis.cartography.cosine_latitude_weights(
            self.cube_aux_lat)
        self.assertEqual(weights.shape, self.cube_aux_lat.shape)
        self.assertArrayAlmostEqual(weights[0, :, :],
                                    np.cos(np.deg2rad(self.lat2d)))

    def test_cosine_latitude_weights_2d_latitude_first(self):
        # 2d latitude coordinate with latitude first (lat, time, lon)
        order = [1, 0, 2] # (lat, time, lon)
        self.cube_aux_lat.transpose(order)
        weights = iris.analysis.cartography.cosine_latitude_weights(
            self.cube_aux_lat)
        self.assertEqual(weights.shape, self.cube_aux_lat.shape)
        self.assertArrayAlmostEqual(weights[:, 0, :],
                                    np.cos(np.deg2rad(self.lat2d)))

    def test_cosine_latitude_weights_2d_latitude_last(self):
        # 2d latitude coordinate with latitude last (time, lon, lat)
        order = [0, 2, 1] # (time, lon, lat)
        self.cube_aux_lat.transpose(order)
        weights = iris.analysis.cartography.cosine_latitude_weights(
            self.cube_aux_lat)
        self.assertEqual(weights.shape, self.cube_aux_lat.shape)
        self.assertArrayAlmostEqual(weights[0, :, :],
                                    np.cos(np.deg2rad(self.lat2d.T)))

    def test_cosine_latitude_weights_no_latitude(self):
        # no coordinate identified as latitude
        self.cube_dim_lat.remove_coord('grid_latitude')
        with self.assertRaises(ValueError):
            weights = iris.analysis.cartography.cosine_latitude_weights(
                self.cube_dim_lat)

    def test_cosine_latitude_weights_multiple_latitude(self):
        # two coordinates identified as latitude
        with self.assertRaises(ValueError):
            weights = iris.analysis.cartography.cosine_latitude_weights(
                self.cube)


class TestRollingWindow(tests.IrisTest):
    def setUp(self):
        # XXX Comes from test_aggregated_by
        cube = iris.cube.Cube(np.array([[6, 10, 12, 18], [8, 12, 14, 20], [18, 12, 10, 6]]), long_name='temperature', units='kelvin')
        cube.add_dim_coord(iris.coords.DimCoord(np.array([0, 5, 10], dtype=np.float64), 'latitude', units='degrees'), 0)
        cube.add_dim_coord(iris.coords.DimCoord(np.array([0, 2, 4, 6], dtype=np.float64), 'longitude', units='degrees'), 1)

        self.cube = cube

    def test_non_mean_operator(self):
        res_cube = self.cube.rolling_window('longitude', iris.analysis.MAX, window=2)
        expected_result = np.array([[10, 12, 18],
                                       [12, 14, 20],
                                       [18, 12, 10]], dtype=np.float64)
        self.assertArrayEqual(expected_result, res_cube.data)

    def test_longitude_simple(self):
        res_cube = self.cube.rolling_window('longitude', iris.analysis.MEAN, window=2)

        expected_result = np.array([[  8., 11., 15.],
                                      [ 10., 13., 17.],
                                      [ 15., 11., 8.]], dtype=np.float64)

        self.assertArrayEqual(expected_result, res_cube.data)

        self.assertCML(res_cube, ('analysis', 'rolling_window', 'simple_longitude.cml'))

        self.assertRaises(ValueError, self.cube.rolling_window, 'longitude', iris.analysis.MEAN, window=0)

    def test_longitude_masked(self):
        self.cube.data = ma.array(self.cube.data,
                                  mask=[[True, True, True, True],
                                        [True, False, True, True],
                                        [False, False, False, False]])
        res_cube = self.cube.rolling_window('longitude',
                                            iris.analysis.MEAN,
                                            window=2)

        expected_result = np.ma.array([[-99., -99., -99.],
                                       [12., 12., -99.],
                                       [15., 11., 8.]],
                                      mask=[[True, True, True],
                                            [False, False, True],
                                            [False, False, False]],
                                      dtype=np.float64)

        self.assertMaskedArrayEqual(expected_result, res_cube.data)

    def test_longitude_circular(self):
        cube = self.cube
        cube.coord('longitude').circular = True
        self.assertRaises(iris.exceptions.NotYetImplementedError, self.cube.rolling_window, 'longitude', iris.analysis.MEAN, window=0)

    def test_different_length_windows(self):
        res_cube = self.cube.rolling_window('longitude', iris.analysis.MEAN, window=4)

        expected_result = np.array([[ 11.5],
                                       [ 13.5],
                                       [ 11.5]], dtype=np.float64)

        self.assertArrayEqual(expected_result, res_cube.data)

        self.assertCML(res_cube, ('analysis', 'rolling_window', 'size_4_longitude.cml'))

        # Window too long:
        self.assertRaises(ValueError, self.cube.rolling_window, 'longitude', iris.analysis.MEAN, window=6)
        # Window too small:
        self.assertRaises(ValueError, self.cube.rolling_window, 'longitude', iris.analysis.MEAN, window=0)

    def test_bad_coordinate(self):
        self.assertRaises(KeyError, self.cube.rolling_window, 'wibble', iris.analysis.MEAN, window=0)

    def test_latitude_simple(self):
        res_cube = self.cube.rolling_window('latitude', iris.analysis.MEAN, window=2)

        expected_result = np.array([[  7., 11., 13., 19.],
                                       [ 13., 12., 12., 13.]], dtype=np.float64)

        self.assertArrayEqual(expected_result, res_cube.data)

        self.assertCML(res_cube, ('analysis', 'rolling_window', 'simple_latitude.cml'))

    def test_mean_with_weights_consistency(self):
        # equal weights should be the same as the mean with no weights
        wts = np.array([0.5, 0.5], dtype=np.float64)
        res_cube = self.cube.rolling_window('longitude',
                                            iris.analysis.MEAN,
                                            window=2,
                                            weights=wts)
        expected_result = self.cube.rolling_window('longitude',
                                                   iris.analysis.MEAN,
                                                   window=2)
        self.assertArrayEqual(expected_result.data, res_cube.data)

    def test_mean_with_weights(self):
        # rolling window mean with weights
        wts = np.array([0.1, 0.6, 0.3], dtype=np.float64)
        res_cube = self.cube.rolling_window('longitude',
                                            iris.analysis.MEAN,
                                            window=3,
                                            weights=wts)
        expected_result = np.array([[10.2, 13.6],
                                       [12.2, 15.6],
                                       [12.0, 9.0]], dtype=np.float64)
        # use almost equal to compare floats
        self.assertArrayAlmostEqual(expected_result, res_cube.data)


class TestGeometry(tests.IrisTest):

    @tests.skip_data
    def test_distinct_xy(self):
        cube = iris.tests.stock.simple_pp()
        cube = cube[:4, :4]
        lon = cube.coord('longitude')
        lat = cube.coord('latitude')
        lon.guess_bounds()
        lat.guess_bounds()
        from iris.fileformats.rules import regular_step
        quarter = abs(regular_step(lon) * regular_step(lat) * 0.25)
        half = abs(regular_step(lon) * regular_step(lat) * 0.5)
        minx = 3.7499990463256836
        maxx = 7.499998092651367
        miny = 84.99998474121094
        maxy = 89.99998474121094
        geometry = shapely.geometry.box(minx, miny, maxx, maxy)
        weights = iris.analysis.geometry.geometry_area_weights(cube, geometry)
        target = np.array([
            [0, quarter, quarter, 0],
            [0, half, half, 0],
            [0, quarter, quarter, 0],
            [0, 0, 0, 0]])
        self.assertTrue(np.allclose(weights, target))

    def test_shared_xy(self):
        cube = tests.stock.track_1d()
        geometry = shapely.geometry.box(1, 4, 3.5, 7)
        weights = iris.analysis.geometry.geometry_area_weights(cube, geometry)
        target = np.array([0, 0, 2, 0.5, 0, 0, 0, 0, 0, 0, 0])
        self.assertTrue(np.allclose(weights, target))


class TestProject(tests.GraphicsTest):
    def setUp(self):
        cube = iris.tests.stock.realistic_4d_no_derived()
        # Remove some slices to speed testing.
        self.cube = cube[0:2, 0:3]
        self.target_proj = ccrs.Robinson()

    def test_bad_resolution(self):
        with self.assertRaises(ValueError):
            iris.analysis.cartography.project(self.cube,
                                              self.target_proj,
                                              nx=-200, ny=200)
        with self.assertRaises(ValueError):
            iris.analysis.cartography.project(self.cube,
                                              self.target_proj,
                                              nx=200, ny='abc')

    def test_missing_latlon(self):
        cube = self.cube.copy()
        cube.remove_coord('grid_latitude')
        with self.assertRaises(ValueError):
            iris.analysis.cartography.project(cube, self.target_proj)
        cube = self.cube.copy()
        cube.remove_coord('grid_longitude')
        with self.assertRaises(ValueError):
            iris.analysis.cartography.project(cube, self.target_proj)
        self.cube.remove_coord('grid_longitude')
        self.cube.remove_coord('grid_latitude')
        with self.assertRaises(ValueError):
            iris.analysis.cartography.project(self.cube, self.target_proj)

    def test_default_resolution(self):
        new_cube, extent = iris.analysis.cartography.project(self.cube,
                                                             self.target_proj)
        self.assertEqual(new_cube.shape, self.cube.shape)

    @tests.skip_data
    @tests.skip_plot
    def test_cartopy_projection(self):
        cube = iris.load_cube(tests.get_data_path(('PP', 'aPPglob1',
                                                   'global.pp')))
        projections = {}
        projections['RotatedPole'] = ccrs.RotatedPole(pole_longitude=177.5,
                                                      pole_latitude=37.5)
        projections['Robinson'] = ccrs.Robinson()
        projections['PlateCarree'] = ccrs.PlateCarree()
        projections['NorthPolarStereo'] = ccrs.NorthPolarStereo()
        projections['Orthographic'] = ccrs.Orthographic(central_longitude=-90,
                                                        central_latitude=45)
        projections['InterruptedGoodeHomolosine'] = ccrs.InterruptedGoodeHomolosine()
        projections['LambertCylindrical'] = ccrs.LambertCylindrical()

        # Set up figure
        fig = plt.figure(figsize=(10, 10))
        gs = matplotlib.gridspec.GridSpec(nrows=3, ncols=3, hspace=1.5, wspace=0.5)
        for subplot_spec, (name, target_proj) in itertools.izip(gs, projections.iteritems()):
            # Set up axes and title
            ax = plt.subplot(subplot_spec, frameon=False, projection=target_proj)
            ax.set_title(name)
            # Transform cube to target projection
            new_cube, extent = iris.analysis.cartography.project(cube, target_proj,
                                                                 nx=150, ny=150)
            # Plot
            plt.pcolor(new_cube.coord('projection_x_coordinate').points,
                       new_cube.coord('projection_y_coordinate').points,
                       new_cube.data)
            # Add coastlines
            ax.coastlines()

        # Tighten up layout
        gs.tight_layout(plt.gcf())

        # Verify resulting plot
        self.check_graphic(tol=1.0)

    @tests.skip_data
    def test_no_coord_system(self):
        cube = iris.load_cube(tests.get_data_path(('PP', 'aPPglob1', 'global.pp')))
        cube.coord('longitude').coord_system = None
        cube.coord('latitude').coord_system = None
        new_cube, extent = iris.analysis.cartography.project(cube,
                                                             self.target_proj)
        self.assertCML(new_cube,
                       ('analysis', 'project', 'default_source_cs.cml'))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_analysis_calculus
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import unittest

import numpy as np

import iris
import iris.analysis.calculus
import iris.cube
import iris.coord_systems
import iris.coords
import iris.tests.stock

from iris.coords import DimCoord
from iris.tests.test_interpolation import normalise_order


class TestCubeDelta(tests.IrisTest):
    def test_invalid(self):
        cube = iris.tests.stock.realistic_4d()
        with self.assertRaises(iris.exceptions.CoordinateMultiDimError):
            t = iris.analysis.calculus.cube_delta(cube, 'surface_altitude')
        with self.assertRaises(iris.exceptions.CoordinateMultiDimError):
            t = iris.analysis.calculus.cube_delta(cube, 'altitude')
        with self.assertRaises(ValueError):
            t = iris.analysis.calculus.cube_delta(cube, 'forecast_period')

    def test_delta_coord_lookup(self):
        cube = iris.cube.Cube(np.arange(10), standard_name='air_temperature')
        # Add a coordinate with a lot of metadata.
        coord = iris.coords.DimCoord(np.arange(10),
                                     long_name='projection_x_coordinate',
                                     var_name='foo',
                                     attributes={'source': 'testing'},
                                     units='m',
                                     coord_system=iris.coord_systems.OSGB())
        cube.add_dim_coord(coord, 0)
        delta = iris.analysis.calculus.cube_delta(cube,
                                                  'projection_x_coordinate')
        delta_coord = delta.coord('projection_x_coordinate')
        self.assertEqual(delta_coord, delta.coord(coord))
        self.assertEqual(coord, cube.coord(delta_coord))


class TestDeltaAndMidpoint(tests.IrisTest):
    def _simple_filename(self, suffix):
        return tests.get_result_path(('analysis', 'delta_and_midpoint', 'simple%s.cml' % suffix))

    def test_simple1_delta_midpoint(self):
        a = iris.coords.DimCoord((np.arange(4, dtype=np.float32) * 90) - 180, long_name='foo',
                                 units='degrees', circular=True)
        self.assertXMLElement(a, self._simple_filename('1'))

        delta = iris.analysis.calculus._construct_delta_coord(a)
        self.assertXMLElement(delta, self._simple_filename('1_delta'))

        midpoint = iris.analysis.calculus._construct_midpoint_coord(a)
        self.assertXMLElement(midpoint, self._simple_filename('1_midpoint'))

    def test_simple2_delta_midpoint(self):
        a = iris.coords.DimCoord((np.arange(4, dtype=np.float32) * -90) + 180, long_name='foo',
                                 units='degrees', circular=True)
        self.assertXMLElement(a, self._simple_filename('2'))

        delta = iris.analysis.calculus._construct_delta_coord(a)
        self.assertXMLElement(delta, self._simple_filename('2_delta'))

        midpoint = iris.analysis.calculus._construct_midpoint_coord(a)
        self.assertXMLElement(midpoint, self._simple_filename('2_midpoint'))

    def test_simple3_delta_midpoint(self):
        a = iris.coords.DimCoord((np.arange(4, dtype=np.float32) * 90) - 180, long_name='foo',
                                 units='degrees', circular=True)
        a.guess_bounds(0.5)
        self.assertXMLElement(a, self._simple_filename('3'))

        delta = iris.analysis.calculus._construct_delta_coord(a)
        self.assertXMLElement(delta, self._simple_filename('3_delta'))

        midpoint = iris.analysis.calculus._construct_midpoint_coord(a)
        self.assertXMLElement(midpoint, self._simple_filename('3_midpoint'))

    def test_simple4_delta_midpoint(self):
        a = iris.coords.AuxCoord(np.arange(4, dtype=np.float32) * 90 - 180, long_name='foo', units='degrees')
        a.guess_bounds()
        b = a.copy()
        self.assertXMLElement(b, self._simple_filename('4'))

        delta = iris.analysis.calculus._construct_delta_coord(b)
        self.assertXMLElement(delta, self._simple_filename('4_delta'))

        midpoint = iris.analysis.calculus._construct_midpoint_coord(b)
        self.assertXMLElement(midpoint, self._simple_filename('4_midpoint'))

    def test_simple5_not_degrees_delta_midpoint(self):
        # Not sure it makes sense to have a circular coordinate which does not have a modulus but test it anyway.
        a = iris.coords.DimCoord(np.arange(4, dtype=np.float32) * 90 - 180,
                                 long_name='foo', units='meter', circular=True)
        self.assertXMLElement(a, self._simple_filename('5'))

        delta = iris.analysis.calculus._construct_delta_coord(a)
        self.assertXMLElement(delta, self._simple_filename('5_delta'))

        midpoints = iris.analysis.calculus._construct_midpoint_coord(a)
        self.assertXMLElement(midpoints, self._simple_filename('5_midpoint'))

    def test_simple6_delta_midpoint(self):
        a = iris.coords.DimCoord(np.arange(5, dtype=np.float32), long_name='foo',
                                 units='count', circular=True)
        midpoints = iris.analysis.calculus._construct_midpoint_coord(a)
        self.assertXMLElement(midpoints, self._simple_filename('6'))

    def test_singular_delta(self):
        # Test single valued coordinate mid-points when circular
        lon = iris.coords.DimCoord(np.float32(-180.), 'latitude', units='degrees', circular=True)

        r_expl = iris.analysis.calculus._construct_delta_coord(lon)
        self.assertXMLElement(r_expl, ('analysis', 'delta_and_midpoint', 'delta_one_element_explicit.xml'))

        # Test single valued coordinate mid-points when not circular
        lon.circular = False
        with self.assertRaises(ValueError):
            iris.analysis.calculus._construct_delta_coord(lon)

    def test_singular_midpoint(self):
        # Test single valued coordinate mid-points when circular
        lon = iris.coords.DimCoord(np.float32(-180.), 'latitude',  units='degrees', circular=True)

        r_expl = iris.analysis.calculus._construct_midpoint_coord(lon)
        self.assertXMLElement(r_expl, ('analysis', 'delta_and_midpoint', 'midpoint_one_element_explicit.xml'))

        # Test single valued coordinate mid-points when not circular
        lon.circular = False
        with self.assertRaises(ValueError):
            iris.analysis.calculus._construct_midpoint_coord(lon)


class TestCoordTrig(tests.IrisTest):
    def setUp(self):
        points = np.arange(20, dtype=np.float32) * 2.3
        bounds = np.concatenate([[points - 0.5 * 2.3],
                                    [points + 0.5 * 2.3]]).T
        self.lat = iris.coords.AuxCoord(points, 'latitude',  units='degrees', bounds=bounds)
        self.rlat = iris.coords.AuxCoord(np.deg2rad(points), 'latitude',  units='radians', bounds=np.deg2rad(bounds))

    def test_sin(self):
        sin_of_coord = iris.analysis.calculus._coord_sin(self.lat)
        sin_of_coord_radians = iris.analysis.calculus._coord_sin(self.rlat)

        # Check the values are correct (within a tolerance)
        np.testing.assert_array_almost_equal(np.sin(self.rlat.points), sin_of_coord.points)
        np.testing.assert_array_almost_equal(np.sin(self.rlat.bounds), sin_of_coord.bounds)

        # Check that the results of the sin function are almost equal when operating on a coord with degrees and radians
        np.testing.assert_array_almost_equal(sin_of_coord.points, sin_of_coord_radians.points)
        np.testing.assert_array_almost_equal(sin_of_coord.bounds, sin_of_coord_radians.bounds)

        self.assertEqual(sin_of_coord.name(), 'sin(latitude)')
        self.assertEqual(sin_of_coord.units, '1')

    def test_cos(self):
        cos_of_coord = iris.analysis.calculus._coord_cos(self.lat)
        cos_of_coord_radians = iris.analysis.calculus._coord_cos(self.rlat)

        # Check the values are correct (within a tolerance)
        np.testing.assert_array_almost_equal(np.cos(self.rlat.points), cos_of_coord.points)
        np.testing.assert_array_almost_equal(np.cos(self.rlat.bounds), cos_of_coord.bounds)

        # Check that the results of the cos function are almost equal when operating on a coord with degrees and radians
        np.testing.assert_array_almost_equal(cos_of_coord.points, cos_of_coord_radians.points)
        np.testing.assert_array_almost_equal(cos_of_coord.bounds, cos_of_coord_radians.bounds)

        # Now that we have tested the points & bounds, remove them and just test the xml
        cos_of_coord = cos_of_coord.copy(points=np.array([1], dtype=np.float32))
        cos_of_coord_radians = cos_of_coord_radians.copy(points=np.array([1], dtype=np.float32))

        self.assertXMLElement(cos_of_coord, ('analysis', 'calculus', 'cos_simple.xml'))
        self.assertXMLElement(cos_of_coord_radians, ('analysis', 'calculus', 'cos_simple_radians.xml'))


class TestCalculusSimple3(tests.IrisTest):

    def setUp(self):
        data = np.arange(2500, dtype=np.float32).reshape(50, 50)
        cube = iris.cube.Cube(data, standard_name="x_wind", units="km/h")

        self.lonlat_cs = iris.coord_systems.GeogCS(6371229)
        cube.add_dim_coord(DimCoord(np.arange(50, dtype=np.float32) * 4.5 -180, 'longitude', units='degrees', coord_system=self.lonlat_cs), 0)
        cube.add_dim_coord(DimCoord(np.arange(50, dtype=np.float32) * 4.5 -90,  'latitude', units='degrees', coord_system=self.lonlat_cs), 1)

        self.cube = cube

    def test_diff_wrt_lon(self):
        t = iris.analysis.calculus.differentiate(self.cube, 'longitude')

        self.assertCMLApproxData(t, ('analysis', 'calculus', 'handmade2_wrt_lon.cml'))

    def test_diff_wrt_lat(self):
        t = iris.analysis.calculus.differentiate(self.cube, 'latitude')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'handmade2_wrt_lat.cml'))


class TestCalculusSimple2(tests.IrisTest):

    def setUp(self):
        data = np.array( [[1, 2, 3, 4, 5],
                             [2, 3, 4, 5, 6],
                             [3, 4, 5, 6, 7],
                             [4, 5, 6, 7, 9]], dtype=np.float32)
        cube = iris.cube.Cube(data, standard_name="x_wind", units="km/h")

        self.lonlat_cs = iris.coord_systems.GeogCS(6371229)
        cube.add_dim_coord(DimCoord(np.arange(4, dtype=np.float32) * 90 -180, 'longitude', units='degrees', circular=True, coord_system=self.lonlat_cs), 0)
        cube.add_dim_coord(DimCoord(np.arange(5, dtype=np.float32) * 45 -90, 'latitude', units='degrees', coord_system=self.lonlat_cs), 1)

        cube.add_aux_coord(DimCoord(np.arange(4, dtype=np.float32), long_name='x', units='count', circular=True), 0)
        cube.add_aux_coord(DimCoord(np.arange(5, dtype=np.float32), long_name='y', units='count'), 1)

        self.cube = cube

    def test_diff_wrt_x(self):
        t = iris.analysis.calculus.differentiate(self.cube, 'x')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'handmade_wrt_x.cml'))

    def test_diff_wrt_y(self):
        t = iris.analysis.calculus.differentiate(self.cube, 'y')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'handmade_wrt_y.cml'))

    def test_diff_wrt_lon(self):
        t = iris.analysis.calculus.differentiate(self.cube, 'longitude')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'handmade_wrt_lon.cml'))

    def test_diff_wrt_lat(self):
        t = iris.analysis.calculus.differentiate(self.cube, 'latitude')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'handmade_wrt_lat.cml'))

    def test_delta_wrt_x(self):
        t = iris.analysis.calculus.cube_delta(self.cube, 'x')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'delta_handmade_wrt_x.cml'))

    def test_delta_wrt_y(self):
        t = iris.analysis.calculus.cube_delta(self.cube, 'y')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'delta_handmade_wrt_y.cml'))

    def test_delta_wrt_lon(self):
        t = iris.analysis.calculus.cube_delta(self.cube, 'longitude')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'delta_handmade_wrt_lon.cml'))

    def test_delta_wrt_lat(self):
        t = iris.analysis.calculus.cube_delta(self.cube, 'latitude')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'delta_handmade_wrt_lat.cml'))


class TestCalculusSimple1(tests.IrisTest):

    def setUp(self):
        data = np.array( [ [1, 2, 3, 4, 5],
                                   [2, 3, 4, 5, 6],
                                   [3, 4, 5, 6, 7],
                                   [4, 5, 6, 7, 8],
                                   [5, 6, 7, 8, 10] ], dtype=np.float32)
        cube = iris.cube.Cube(data, standard_name="x_wind", units="km/h")

        cube.add_dim_coord(DimCoord(np.arange(5, dtype=np.float32), long_name='x', units='count'), 0)
        cube.add_dim_coord(DimCoord(np.arange(5, dtype=np.float32), long_name='y', units='count'), 1)

        self.cube = cube

    def test_diff_wrt_x(self):
        t = iris.analysis.calculus.differentiate(self.cube, 'x')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'handmade_simple_wrt_x.cml'))

    def test_delta_wrt_x(self):
        t = iris.analysis.calculus.cube_delta(self.cube, 'x')
        self.assertCMLApproxData(t, ('analysis', 'calculus', 'delta_handmade_simple_wrt_x.cml'))


def build_cube(data, spherical=False):
    """
    Create a cube suitable for testing.

    """
    cube = iris.cube.Cube(data, standard_name="x_wind", units="km/h")

    nx = data.shape[-1]
    ny = data.shape[-2]
    nz = data.shape[-3] if data.ndim > 2 else None

    dimx = data.ndim - 1
    dimy = data.ndim - 2
    dimz = data.ndim - 3  if data.ndim > 2 else None

    if spherical:
        hcs = iris.coord_systems.GeogCS(6321)
        cube.add_dim_coord(DimCoord(np.arange(-180, 180, 360./nx, dtype=np.float32), 'longitude', units='degrees', coord_system=hcs, circular=True), dimx)
        cube.add_dim_coord(DimCoord(np.arange(-90, 90, 180./ny, dtype=np.float32), 'latitude', units='degrees',coord_system=hcs), dimy)

    else:
        cube.add_dim_coord(DimCoord(np.arange(nx, dtype=np.float32) * 2.21 + 2, 'projection_x_coordinate', units='meters'), dimx)
        cube.add_dim_coord(DimCoord(np.arange(ny, dtype=np.float32) * 25 -50, 'projection_y_coordinate', units='meters'), dimy)

    if nz is None:
        cube.add_aux_coord(DimCoord(np.array([10], dtype=np.float32), long_name='z', units='meters', attributes={"positive":"up"}))
    else:
        cube.add_dim_coord(DimCoord(np.arange(nz, dtype=np.float32) * 2, long_name='z', units='meters', attributes={"positive":"up"}), dimz)

    return cube


class TestCalculusWKnownSolutions(tests.IrisTest):

    def get_coord_pts(self, cube):
        """return (x_pts, x_ones, y_pts, y_ones, z_pts, z_ones) for the given cube."""
        x = cube.coord(axis='X')
        y = cube.coord(axis='Y')
        z = cube.coord(axis='Z')

        if z and z.shape[0] > 1:
            x_shp = (1, 1, x.shape[0])
            y_shp = (1, y.shape[0], 1)
            z_shp = (z.shape[0], 1, 1)
        else:
            x_shp = (1, x.shape[0])
            y_shp = (y.shape[0], 1)
            z_shp = None

        x_pts = x.points.reshape(x_shp)
        y_pts = y.points.reshape(y_shp)

        x_ones = np.ones(x_shp)
        y_ones = np.ones(y_shp)

        if z_shp:
            z_pts = z.points.reshape(z_shp)
            z_ones = np.ones(z_shp)
        else:
            z_pts = None
            z_ones = None

        return (x_pts, x_ones, y_pts, y_ones, z_pts, z_ones)

    def test_contrived_differential1(self):
        # testing :
        # F = ( cos(lat) cos(lon) )
        # dF/dLon = - sin(lon) cos(lat)     (and to simplify /cos(lat) )
        cube = build_cube(np.empty((30, 60)), spherical=True)

        x = cube.coord('longitude')
        y = cube.coord('latitude')
        y_dim = cube.coord_dims(y)[0]

        cos_x_pts = np.cos(np.radians(x.points)).reshape(1, x.shape[0])
        cos_y_pts = np.cos(np.radians(y.points)).reshape(y.shape[0], 1)

        cube.data = cos_y_pts * cos_x_pts

        lon_coord = x.copy()
        lon_coord.convert_units('radians')
        lat_coord = y.copy()
        lat_coord.convert_units('radians')
        cos_lat_coord = iris.coords.AuxCoord.from_coord(lat_coord)
        cos_lat_coord.points = np.cos(lat_coord.points)
        cos_lat_coord.units = '1'
        cos_lat_coord.rename('cos({})'.format(lat_coord.name()))

        temp = iris.analysis.calculus.differentiate(cube, lon_coord)
        df_dlon = iris.analysis.maths.divide(temp, cos_lat_coord, y_dim)

        x = df_dlon.coord('longitude')
        y = df_dlon.coord('latitude')

        sin_x_pts = np.sin(np.radians(x.points)).reshape(1, x.shape[0])
        y_ones = np.ones((y.shape[0] , 1))

        data = - sin_x_pts * y_ones
        result = df_dlon.copy(data=data)

        np.testing.assert_array_almost_equal(result.data, df_dlon.data, decimal=3)

    def test_contrived_differential2(self):
        # testing :
        # w = y^2
        # dw_dy = 2*y
        cube = build_cube(np.empty((10, 30, 60)), spherical=False)

        x_pts, x_ones, y_pts, y_ones, z_pts, z_ones = self.get_coord_pts(cube)

        w = cube.copy(data=z_ones * x_ones * pow(y_pts, 2.))

        r = iris.analysis.calculus.differentiate(w, 'projection_y_coordinate')

        x_pts, x_ones, y_pts, y_ones, z_pts, z_ones = self.get_coord_pts(r)
        result = r.copy(data = y_pts * 2. * x_ones * z_ones)

        np.testing.assert_array_almost_equal(result.data, r.data, decimal=6)

    def test_contrived_non_spherical_curl1(self):
        # testing :
        # F(x, y, z) = (y, 0, 0)
        # curl( F(x, y, z) ) = (0, 0, -1)

        cube = build_cube(np.empty((25, 50)), spherical=False)

        x_pts, x_ones, y_pts, y_ones, z_pts, z_ones = self.get_coord_pts(cube)

        u = cube.copy(data=x_ones * y_pts)
        u.rename("u_wind")
        v = cube.copy(data=u.data * 0)
        v.rename("v_wind")

        r = iris.analysis.calculus.curl(u, v)

        # Curl returns None when there is no components of Curl
        self.assertEqual(r[0], None)
        self.assertEqual(r[1], None)
        cube = r[2]
        self.assertCML(
            cube,
            ('analysis', 'calculus', 'grad_contrived_non_spherical1.cml'),
            checksum=False)
        self.assertTrue(np.all(np.abs(cube.data - (-1.0)) < 1.0e-7))

    def test_contrived_non_spherical_curl2(self):
        # testing :
        # F(x, y, z) = (z^3, x+2, y^2)
        # curl( F(x, y, z) ) = (2y, 3z^2, 1)

        cube = build_cube(np.empty((10, 25, 50)), spherical=False)

        x_pts, x_ones, y_pts, y_ones, z_pts, z_ones = self.get_coord_pts(cube)

        u = cube.copy(data=pow(z_pts, 3) * x_ones * y_ones)
        v = cube.copy(data=z_ones * (x_pts + 2.) * y_ones)
        w = cube.copy(data=z_ones * x_ones * pow(y_pts, 2.))
        u.rename('u_wind')
        v.rename('v_wind')
        w.rename('w_wind')

        r = iris.analysis.calculus.curl(u, v, w)


        # TODO #235 When regridding is not nearest neighbour: the commented out code could be made to work
        # r[0].data should now be tending towards result.data as the resolution of the grid gets higher.
#        result = r[0].copy(data=True)
#        x_pts, x_ones, y_pts, y_ones, z_pts, z_ones = self.get_coord_pts(result)
#        result.data = y_pts * 2. * x_ones * z_ones
#        print repr(r[0].data[0:1, 0:5, 0:25:5])
#        print repr(result.data[0:1, 0:5, 0:25:5])
#        np.testing.assert_array_almost_equal(result.data, r[0].data, decimal=2)
#
#        result = r[1].copy(data=True)
#        x_pts, x_ones, y_pts, y_ones, z_pts, z_ones = self.get_coord_pts(result)
#        result.data = pow(z_pts, 2) * x_ones * y_ones
#        np.testing.assert_array_almost_equal(result.data, r[1].data, decimal=6)

        result = r[2].copy()
        result.data = result.data * 0  + 1
        np.testing.assert_array_almost_equal(result.data, r[2].data, decimal=4)

        normalise_order(r[1])
        self.assertCML(r, ('analysis', 'calculus', 'curl_contrived_cartesian2.cml'), checksum=False)

    def test_contrived_spherical_curl1(self):
        # testing:
        # F(lon, lat, r) = (- r sin(lon), -r cos(lon) sin(lat), 0)
        # curl( F(x, y, z) ) = (0, 0, 0)
        cube = build_cube(np.empty((30, 60)), spherical=True)
        radius = iris.analysis.cartography.DEFAULT_SPHERICAL_EARTH_RADIUS

        x = cube.coord('longitude')
        y = cube.coord('latitude')

        cos_x_pts = np.cos(np.radians(x.points)).reshape(1, x.shape[0])
        sin_x_pts = np.sin(np.radians(x.points)).reshape(1, x.shape[0])
        cos_y_pts = np.cos(np.radians(y.points)).reshape(y.shape[0], 1)
        sin_y_pts = np.sin(np.radians(y.points)).reshape(y.shape[0], 1)
        y_ones = np.ones((cube.shape[0], 1))

        u = cube.copy(data=-sin_x_pts * y_ones * radius)
        v = cube.copy(data=-cos_x_pts * sin_y_pts * radius)
        u.rename('u_wind')
        v.rename('v_wind')

        r = iris.analysis.calculus.curl(u, v)[2]

        result = r.copy(data=r.data * 0)

        # Note: This numerical comparison was created when the radius was 1000 times smaller
        np.testing.assert_array_almost_equal(result.data[5:-5], r.data[5:-5]/1000.0, decimal=1)
        self.assertCML(r, ('analysis', 'calculus', 'grad_contrived1.cml'), checksum=False)

    def test_contrived_sphrical_curl2(self):
        # testing:
        # F(lon, lat, r) = (r sin(lat) cos(lon), -r sin(lon), 0)
        # curl( F(x, y, z) ) = (0, 0, -2 cos(lon) cos(lat) )
        cube = build_cube(np.empty((70, 150)), spherical=True)
        radius = iris.analysis.cartography.DEFAULT_SPHERICAL_EARTH_RADIUS

        x = cube.coord('longitude')
        y = cube.coord('latitude')

        cos_x_pts = np.cos(np.radians(x.points)).reshape(1, x.shape[0])
        sin_x_pts = np.sin(np.radians(x.points)).reshape(1, x.shape[0])
        cos_y_pts = np.cos(np.radians(y.points)).reshape(y.shape[0], 1)
        sin_y_pts = np.sin(np.radians(y.points)).reshape(y.shape[0], 1)
        y_ones = np.ones((cube.shape[0] , 1))

        u = cube.copy(data=sin_y_pts * cos_x_pts * radius)
        v = cube.copy(data=-sin_x_pts * y_ones * radius)
        u.rename('u_wind')
        v.rename('v_wind')

        lon_coord = x.copy()
        lon_coord.convert_units('radians')
        lat_coord = y.copy()
        lat_coord.convert_units('radians')
        cos_lat_coord = iris.coords.AuxCoord.from_coord(lat_coord)
        cos_lat_coord.points = np.cos(lat_coord.points)
        cos_lat_coord.units = '1'
        cos_lat_coord.rename('cos({})'.format(lat_coord.name()))

        r = iris.analysis.calculus.curl(u, v)[2]

        x = r.coord('longitude')
        y = r.coord('latitude')

        cos_x_pts = np.cos(np.radians(x.points)).reshape(1, x.shape[0])
        cos_y_pts = np.cos(np.radians(y.points)).reshape(y.shape[0], 1)

        result = r.copy(data=2*cos_x_pts*cos_y_pts)

        # Note: This numerical comparison was created when the radius was 1000 times smaller
        np.testing.assert_array_almost_equal(result.data[30:-30, :], r.data[30:-30, :]/1000.0, decimal=1)
        self.assertCML(r, ('analysis', 'calculus', 'grad_contrived2.cml'), checksum=False)


class TestCurlInterface(tests.IrisTest):
    def test_non_conformed(self):
        u = build_cube(np.empty((50, 20)), spherical=True)

        v = u.copy()
        y = v.coord('latitude')
        y.points += 5
        self.assertRaises(ValueError, iris.analysis.calculus.curl, u, v)

    def test_standard_name(self):
        nx = 20; ny = 50; nz = None;
        u = build_cube(np.empty((50, 20)), spherical=True)
        v = u.copy()
        w = u.copy()
        u.rename('u_wind')
        v.rename('v_wind')
        w.rename('w_wind')

        r = iris.analysis.calculus.spatial_vectors_with_phenom_name(u, v)
        self.assertEqual(r, (('u', 'v', 'w'), 'wind'))

        r = iris.analysis.calculus.spatial_vectors_with_phenom_name(u, v, w)
        self.assertEqual(r, (('u', 'v', 'w'), 'wind'))

        self.assertRaises(ValueError, iris.analysis.calculus.spatial_vectors_with_phenom_name, u, None, w)
        self.assertRaises(ValueError, iris.analysis.calculus.spatial_vectors_with_phenom_name, None, None, w)
        self.assertRaises(ValueError, iris.analysis.calculus.spatial_vectors_with_phenom_name, None, None, None)

        u.rename("x foobar wibble")
        v.rename("y foobar wibble")
        w.rename("z foobar wibble")
        r = iris.analysis.calculus.spatial_vectors_with_phenom_name(u, v)
        self.assertEqual(r, (('x', 'y', 'z'), 'foobar wibble'))

        r = iris.analysis.calculus.spatial_vectors_with_phenom_name(u, v, w)
        self.assertEqual(r, (('x', 'y', 'z'), 'foobar wibble'))

        u.rename("wibble foobar")
        v.rename("wobble foobar")
        w.rename("tipple foobar")
#        r = iris.analysis.calculus.spatial_vectors_with_phenom_name(u, v, w) #should raise a Value Error...
        self.assertRaises(ValueError, iris.analysis.calculus.spatial_vectors_with_phenom_name, u, v)
        self.assertRaises(ValueError, iris.analysis.calculus.spatial_vectors_with_phenom_name, u, v, w)

        u.rename("eastward_foobar")
        v.rename("northward_foobar")
        w.rename("upward_foobar")
        r = iris.analysis.calculus.spatial_vectors_with_phenom_name(u, v)
        self.assertEqual(r, (('eastward', 'northward', 'upward'), 'foobar'))

        r = iris.analysis.calculus.spatial_vectors_with_phenom_name(u, v, w)
        self.assertEqual(r, (('eastward', 'northward', 'upward'), 'foobar'))

        # Change it to have an inconsistent phenomenon
        v.rename('northward_foobar2')
        self.assertRaises(ValueError, iris.analysis.calculus.spatial_vectors_with_phenom_name, u, v)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_basic_maths
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import operator

import numpy as np
import numpy.ma as ma

import iris
import iris.analysis.maths
import iris.coords
import iris.exceptions
import iris.tests.stock


@tests.skip_data
class TestBasicMaths(tests.IrisTest):
    def setUp(self):
        self.cube = iris.tests.stock.global_pp()
        self.cube.data = self.cube.data - 260

    def test_abs(self):
        a = self.cube

        b = iris.analysis.maths.abs(a, in_place=False)
        self.assertCML(a, ('analysis', 'maths_original.cml'))
        self.assertCML(b, ('analysis', 'abs.cml'))

        iris.analysis.maths.abs(a, in_place=True)
        self.assertCML(b, ('analysis', 'abs.cml'))
        self.assertCML(a, ('analysis', 'abs.cml'))

    def test_minus(self):
        a = self.cube
        e = self.cube.copy()

        # Check that the subtraction has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

        d = a - a
        self.assertCML(d, ('analysis', 'subtract.cml'))

        # Check that the subtraction has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

        c = iris.analysis.maths.subtract(e, e)
        self.assertCML(c, ('analysis', 'subtract.cml'))

        # Check that the subtraction has had no effect on the original
        self.assertCML(e, ('analysis', 'maths_original.cml'))

    def test_minus_with_data_describing_coordinate(self):
        a = self.cube
        e = self.cube.copy()
        lat = e.coord('latitude')
        lat.points = lat.points+100

        # Cannot ignore a axis describing coordinate
        self.assertRaises(ValueError, iris.analysis.maths.subtract, a, e)

    def test_minus_scalar(self):
        a = self.cube

        self.assertCML(a, ('analysis', 'maths_original.cml'))

        b = a - 200
        self.assertCML(b, ('analysis', 'subtract_scalar.cml'))
        # Check that the subtraction has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_minus_array(self):
        a = self.cube
        data_array = self.cube.copy().data

        # check that the file has not changed (avoids false positives by failing early)
        self.assertCML(a, ('analysis', 'maths_original.cml'))

        # subtract an array of exactly the same shape as the original
        b = a - data_array
        self.assertArrayEqual(b.data, np.array(0, dtype=np.float32))
        self.assertCML(b, ('analysis', 'subtract_array.cml'), checksum=False)

        # subtract an array of the same number of dimensions, but with one of the dimensions having len 1
        b = a - data_array[:, 0:1]
        self.assertArrayEqual(b.data[:, 0:1], np.array(0, dtype=np.float32))
        self.assertArrayEqual(b.data[:, 1:2], b.data[:, 1:2])

        # subtract an array of 1 dimension fewer than the cube
        b = a - data_array[0, :]
        self.assertArrayEqual(b.data[0, :], np.array(0, dtype=np.float32))
        self.assertArrayEqual(b.data[:, 1:2], b.data[:, 1:2])

        # subtract an array of 1 dimension more than the cube
        d_array = data_array.reshape(data_array.shape[0], data_array.shape[1], 1)
        self.assertRaises(ValueError, iris.analysis.maths.subtract, a, d_array)

        # Check that the subtraction has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_minus_coord(self):
        a = self.cube

        xdim = a.ndim-1
        ydim = a.ndim-2
        c_x = iris.coords.DimCoord(points=range(a.shape[xdim]), long_name='x_coord', units=self.cube.units)
        c_y = iris.coords.AuxCoord(points=range(a.shape[ydim]), long_name='y_coord', units=self.cube.units)

        self.assertCML(a, ('analysis', 'maths_original.cml'))

        b = iris.analysis.maths.subtract(a, c_x, dim=1)
        self.assertCML(b, ('analysis', 'subtract_coord_x.cml'))
        # Check that the subtraction has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

        b = iris.analysis.maths.subtract(a, c_y, dim=0)
        self.assertCML(b, ('analysis', 'subtract_coord_y.cml'))
        # Check that the subtraction has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_addition_scalar(self):
        a = self.cube

        self.assertCML(a, ('analysis', 'maths_original.cml'))

        b = a + 200
        self.assertCML(b, ('analysis', 'addition_scalar.cml'))
        # Check that the addition has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_addition_coord(self):
        a = self.cube

        xdim = a.ndim-1
        ydim = a.ndim-2
        c_x = iris.coords.DimCoord(points=range(a.shape[xdim]), long_name='x_coord', units=self.cube.units)
        c_y = iris.coords.AuxCoord(points=range(a.shape[ydim]), long_name='y_coord', units=self.cube.units)

        self.assertCML(a, ('analysis', 'maths_original.cml'))

        b = iris.analysis.maths.add(a, c_x, dim=1)
        self.assertCML(b, ('analysis', 'addition_coord_x.cml'))
        # Check that the addition has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

        b = iris.analysis.maths.add(a, c_y, dim=0)
        self.assertCML(b, ('analysis', 'addition_coord_y.cml'))
        # Check that the addition has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_addition(self):
        a = self.cube

        c = a + a
        self.assertCML(c, ('analysis', 'addition.cml'))
        # Check that the addition has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_addition_different_standard_name(self):
        a = self.cube.copy()
        b = self.cube.copy()
        b.rename('my cube data')
        c = a + b
        self.assertCML(c, ('analysis', 'addition_different_std_name.cml'), checksum=False)

    def test_addition_fail(self):
        a = self.cube

        xdim = a.ndim-1
        ydim = a.ndim-2
        c_axis_length_fail = iris.coords.DimCoord(points=range(a.shape[ydim]), long_name='x_coord', units=self.cube.units)
        c_unit_fail = iris.coords.AuxCoord(points=range(a.shape[xdim]), long_name='x_coord', units='volts')

        self.assertRaises(ValueError, iris.analysis.maths.add, a, c_axis_length_fail)
        self.assertRaises(iris.exceptions.NotYetImplementedError, iris.analysis.maths.add, a, c_unit_fail)

    def test_addition_in_place(self):
        a = self.cube

        b = iris.analysis.maths.add(a, self.cube, in_place=True)
        self.assertTrue(b is a)
        self.assertCML(a, ('analysis', 'addition_in_place.cml'))

    def test_addition_in_place_coord(self):
        a = self.cube

        # scalar is promoted to a coordinate internally
        b = iris.analysis.maths.add(a, 1000, in_place=True)
        self.assertTrue(b is a)
        self.assertCML(a, ('analysis', 'addition_in_place_coord.cml'))

    def test_addition_different_attributes(self):
        a = self.cube.copy()
        b = self.cube.copy()
        b.attributes['my attribute'] = 'foobar'
        c = a + b
        self.assertIsNone(c.standard_name)
        self.assertEqual(c.attributes, {})

    def test_type_error(self):
        with self.assertRaises(TypeError):
            iris.analysis.maths.add('not a cube', 123)


@tests.skip_data
class TestDivideAndMultiply(tests.IrisTest):
    def setUp(self):
        self.cube = iris.tests.stock.global_pp()
        self.cube.data = self.cube.data - 260

    def test_divide(self):
        a = self.cube

        c = a / a

        np.testing.assert_array_almost_equal(a.data / a.data, c.data)
        self.assertCML(c, ('analysis', 'division.cml'), checksum=False)

        # Check that the division has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_divide_by_scalar(self):
        a = self.cube

        c = a / 10

        np.testing.assert_array_almost_equal(a.data / 10, c.data)
        self.assertCML(c, ('analysis', 'division_scalar.cml'), checksum=False)

        # Check that the division has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_divide_by_coordinate(self):
        a = self.cube

        c = a / a.coord('latitude')
        self.assertCML(c, ('analysis', 'division_by_latitude.cml'))

        # Check that the division has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_divide_by_array(self):
        a = self.cube
        data_array = self.cube.copy().data

        # test division by exactly the same shape data
        c = a / data_array
        self.assertArrayEqual(c.data, np.array(1, dtype=np.float32))
        self.assertCML(c, ('analysis', 'division_by_array.cml'), checksum=False)

        # test division by array of fewer dimensions
        c = a / data_array[0, :]
        self.assertArrayEqual(c.data[0, :], np.array(1, dtype=np.float32))

        # test division by array of more dimensions
        d_array = data_array.reshape(-1, data_array.shape[1], 1, 1)
        self.assertRaises(ValueError, iris.analysis.maths.divide, c, d_array)

        # Check that the division has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_divide_by_coordinate_dim2(self):
        a = self.cube

        # Prevent divide-by-zero warning
        a.coord('longitude').points = a.coord('longitude').points + 0.5

        c = a / a.coord('longitude')
        self.assertCML(c, ('analysis', 'division_by_longitude.cml'))

        # Reset to allow comparison with original
        a.coord('longitude').points = a.coord('longitude').points - 0.5

        # Check that the division has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_divide_by_singluar_coordinate(self):
        a = self.cube

        coord = iris.coords.DimCoord(points=2, long_name='foo', units='1')
        c = iris.analysis.maths.divide(a, coord)
        self.assertCML(c, ('analysis', 'division_by_singular_coord.cml'))

        # Check that the division is equivalent to dividing the whole of the data by 2
        self.assertArrayEqual(c.data, a.data/2.)

    def test_divide_by_different_len_coord(self):
        a = self.cube

        coord = iris.coords.DimCoord(points=np.arange(10) * 2 + 5, standard_name='longitude', units='degrees')

        self.assertRaises(ValueError, iris.analysis.maths.divide, a, coord)

    def test_divide_in_place(self):
        a = self.cube.copy()
        b = iris.analysis.maths.divide(a, 5, in_place=True)
        self.assertIs(a, b)

    def test_divide_not_in_place(self):
        a = self.cube.copy()
        b = iris.analysis.maths.divide(a, 5, in_place=False)
        self.assertIsNot(a, b)

    def test_multiply(self):
        a = self.cube

        c = a * a
        self.assertCML(c, ('analysis', 'multiply.cml'))

        # Check that the multiplication has had no effect on the original
        self.assertCML(a, ('analysis', 'maths_original.cml'))

    def test_multiplication_different_standard_name(self):
        a = self.cube.copy()
        b = self.cube.copy()
        b.rename('my cube data')
        c = a * b
        self.assertCML(c, ('analysis', 'multiply_different_std_name.cml'), checksum=False)

    def test_multiplication_different_attributes(self):
        a = self.cube.copy()
        b = self.cube.copy()
        b.attributes['my attribute'] = 'foobar'
        c = a * b
        self.assertIsNone(c.standard_name)
        self.assertEqual(c.attributes, {})

    def test_multiplication_in_place(self):
        a = self.cube.copy()
        b = iris.analysis.maths.multiply(a, 5, in_place=True)
        self.assertIs(a, b)

    def test_multiplication_not_in_place(self):
        a = self.cube.copy()
        b = iris.analysis.maths.multiply(a, 5, in_place=False)
        self.assertIsNot(a, b)

    def test_type_error(self):
        with self.assertRaises(TypeError):
            iris.analysis.maths.multiply('not a cube', 2)
        with self.assertRaises(TypeError):
            iris.analysis.maths.multiply(self.cube, 'not a cube')
        with self.assertRaises(TypeError):
            iris.analysis.maths.multiply(self.cube, 'not a cube',
                                         in_place=True)


@tests.skip_data
class TestExponentiate(tests.IrisTest):
    def setUp(self):
        self.cube = iris.tests.stock.global_pp()
        self.cube.data = self.cube.data - 260

    def test_exponentiate(self):
        a = self.cube
        a.data = a.data.astype(np.float64)
        e = pow(a, 4)
        self.assertCMLApproxData(e, ('analysis', 'exponentiate.cml'))

    def test_square_root(self):
        # Make sure we have something which we can take the root of.
        a = self.cube
        a.data = abs(a.data)
        a.units **= 2

        e = a ** 0.5

        self.assertCML(e, ('analysis', 'sqrt.cml'))
        self.assertArrayEqual(e.data, a.data ** 0.5)
        self.assertRaises(ValueError, iris.analysis.maths.exponentiate, a, 0.3)

    def test_type_error(self):
        with self.assertRaises(TypeError):
            iris.analysis.maths.exponentiate('not a cube', 2)


class TestExponential(tests.IrisTest):
    def setUp(self):
        self.cube = iris.tests.stock.simple_1d()

    def test_exp(self):
        e = iris.analysis.maths.exp(self.cube)
        self.assertCMLApproxData(e, ('analysis', 'exp.cml'))


@tests.skip_data
class TestLog(tests.IrisTest):
    def setUp(self):
        self.cube = iris.tests.stock.global_pp()

    def test_log(self):
        e = iris.analysis.maths.log(self.cube)
        self.assertCMLApproxData(e, ('analysis', 'log.cml'))

    def test_log2(self):
        e = iris.analysis.maths.log2(self.cube)
        self.assertCMLApproxData(e, ('analysis', 'log2.cml'))

    def test_log10(self):
        e = iris.analysis.maths.log10(self.cube)
        self.assertCMLApproxData(e, ('analysis', 'log10.cml'))


class TestMaskedArrays(tests.IrisTest):
    ops = (operator.add, operator.sub, operator.mul, operator.div)
    iops = (operator.iadd, operator.isub, operator.imul, operator.idiv)

    def setUp(self):
        self.data1 = ma.MaskedArray([[9,9,9],[8,8,8,]],mask=[[0,1,0],[0,0,1]])
        self.data2 = ma.MaskedArray([[3,3,3],[2,2,2,]],mask=[[0,1,0],[0,1,1]])

        self.cube1 = iris.cube.Cube(self.data1)
        self.cube2 = iris.cube.Cube(self.data2)

    def test_operator(self):
        for test_op in self.ops:
            result1 = test_op(self.cube1, self.cube2)
            result2 = test_op(self.data1, self.data2)

            np.testing.assert_array_equal(result1.data, result2)

    def test_operator_in_place(self):
        for test_op in self.iops:
            test_op(self.cube1, self.cube2)
            test_op(self.data1, self.data2)

            np.testing.assert_array_equal(self.cube1.data, self.data1)

    def test_operator_scalar(self):
        for test_op in self.ops:
            result1 = test_op(self.cube1, 2)
            result2 = test_op(self.data1, 2)

            np.testing.assert_array_equal(result1.data, result2)

    def test_operator_array(self):
        for test_op in self.ops:
            result1 = test_op(self.cube1, self.data2)
            result2 = test_op(self.data1, self.data2)

            np.testing.assert_array_equal(result1.data, result2)

    def test_incompatible_dimensions(self):
        data3 = ma.MaskedArray([[3,3,3,4],[2,2,2]],mask=[[0,1,0,0],[0,1,1]])
        with self.assertRaises(ValueError):
            # incompatible dimensions
            self.cube1 + data3

    def test_increase_cube_dimensionality(self):
        with self.assertRaises(ValueError):
            # This would increase the dimensionality of the cube due to auto broadcasting
            cubex = iris.cube.Cube(ma.MaskedArray([[9,]],mask=[[0]]))
            cubex + ma.MaskedArray([[3,3,3,3]],mask=[[0,1,0,1]])


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_cartography
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Tests elements of the cartography module.

"""

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import numpy as np

import iris
import iris.analysis.cartography


class Test_get_xy_grids(tests.IrisTest):
    # Testing for iris.analysis.carography.get_xy_grids().

    def test_1d(self):
        cube = iris.cube.Cube(np.arange(12).reshape(3, 4))
        cube.add_dim_coord(iris.coords.DimCoord(np.arange(3), "latitude"), 0)
        cube.add_dim_coord(iris.coords.DimCoord(np.arange(4), "longitude"), 1)
        x, y = iris.analysis.cartography.get_xy_grids(cube)
        self.assertRepr((x, y), ("cartography", "get_xy_grids", "1d.txt"))

    def test_2d(self):
        cube = iris.cube.Cube(np.arange(12).reshape(3, 4))
        cube.add_aux_coord(iris.coords.AuxCoord(
                                np.arange(12).reshape(3, 4),
                                "latitude"), (0, 1))
        cube.add_aux_coord(iris.coords.AuxCoord(
                                np.arange(100, 112).reshape(3, 4),
                                "longitude"), (0, 1))
        x, y = iris.analysis.cartography.get_xy_grids(cube)
        self.assertRepr((x, y), ("cartography", "get_xy_grids", "2d.txt"))

    def test_3d(self):
        cube = iris.cube.Cube(np.arange(60).reshape(5, 3, 4))
        cube.add_aux_coord(iris.coords.AuxCoord(
                                np.arange(60).reshape(5, 3, 4),
                                "latitude"), (0, 1, 2))
        cube.add_aux_coord(iris.coords.AuxCoord(
                                np.arange(100, 160).reshape(5, 3, 4),
                                "longitude"), (0, 1, 2))
        self.assertRaises(ValueError, iris.analysis.cartography.get_xy_grids, cube)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_cdm
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test cube indexing, slicing, and extracting, and also the dot graphs.

"""
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

from contextlib import contextmanager
import os
import re
import sys
import warnings

import biggus
import numpy as np
import numpy.ma as ma

import iris
import iris.analysis
import iris.coords
import iris.cube
import iris.fileformats
import iris.unit
import iris.tests.pp as pp
import iris.tests.stock


class IrisDotTest(tests.IrisTest):
    def check_dot(self, cube, reference_filename):
        test_string = iris.fileformats.dot.cube_text(cube)
        reference_path = tests.get_result_path(reference_filename)
        if os.path.isfile(reference_path):
            reference = ''.join(open(reference_path, 'r').readlines())
            self._assert_str_same(reference, test_string, reference_filename, type_comparison_name='DOT files')
        else:
            tests.logger.warning('Creating result file: %s', reference_path)
            open(reference_path, 'w').writelines(test_string)


class TestBasicCubeConstruction(tests.IrisTest):
    def setUp(self):
        self.cube = iris.cube.Cube(np.arange(12, dtype=np.int32).reshape((3, 4)), long_name='test cube')
        self.x = iris.coords.DimCoord(np.array([ -7.5,   7.5,  22.5,  37.5]), long_name='x')
        self.y = iris.coords.DimCoord(np.array([  2.5,   7.5,  12.5]), long_name='y')
        self.xy = iris.coords.AuxCoord(np.arange(12).reshape((3, 4)) * 3.0, long_name='xy')

    def test_add_dim_coord(self):
        # Lengths must match
        with self.assertRaises(ValueError):
            self.cube.add_dim_coord(self.y, 1)
        with self.assertRaises(ValueError):
            self.cube.add_dim_coord(self.x, 0)

        # Must specify a dimension
        with self.assertRaises(TypeError):
            self.cube.add_dim_coord(self.y)

        # Add y
        self.cube.add_dim_coord(self.y, 0)
        self.assertEqual(self.cube.coords(), [self.y])
        self.assertEqual(self.cube.dim_coords, (self.y,))
        # Add x
        self.cube.add_dim_coord(self.x, 1)
        self.assertEqual(self.cube.coords(), [self.y, self.x])
        self.assertEqual(self.cube.dim_coords, (self.y, self.x))

        # Cannot add a coord twice
        with self.assertRaises(ValueError):
            self.cube.add_dim_coord(self.y, 0)
        # ... even to cube.aux_coords
        with self.assertRaises(ValueError):
            self.cube.add_aux_coord(self.y, 0)

        # Can't add AuxCoord to dim_coords
        y_other = iris.coords.AuxCoord(np.array([  2.5,   7.5,  12.5]), long_name='y_other')
        with self.assertRaises(ValueError):
            self.cube.add_dim_coord(y_other, 0)

    def test_add_scalar_coord(self):
        scalar_dim_coord = iris.coords.DimCoord(23, long_name='scalar_dim_coord')
        scalar_aux_coord = iris.coords.AuxCoord(23, long_name='scalar_aux_coord')
        # Scalars cannot be in cube.dim_coords
        with self.assertRaises(TypeError):
            self.cube.add_dim_coord(scalar_dim_coord)
        with self.assertRaises(TypeError):
            self.cube.add_dim_coord(scalar_dim_coord, None)
        with self.assertRaises(ValueError):
            self.cube.add_dim_coord(scalar_dim_coord, [])
        with self.assertRaises(ValueError):
            self.cube.add_dim_coord(scalar_dim_coord, ())
        
        # Make sure that's still the case for a 0-dimensional cube.
        cube = iris.cube.Cube(666)
        self.assertEqual(cube.ndim, 0)
        with self.assertRaises(TypeError):
            self.cube.add_dim_coord(scalar_dim_coord)
        with self.assertRaises(TypeError):
            self.cube.add_dim_coord(scalar_dim_coord, None)
        with self.assertRaises(ValueError):
            self.cube.add_dim_coord(scalar_dim_coord, [])
        with self.assertRaises(ValueError):
            self.cube.add_dim_coord(scalar_dim_coord, ())

        cube = self.cube.copy()
        cube.add_aux_coord(scalar_dim_coord)
        cube.add_aux_coord(scalar_aux_coord)
        self.assertEqual(set(cube.aux_coords), {scalar_dim_coord, scalar_aux_coord})
        
        # Various options for dims
        cube = self.cube.copy()
        cube.add_aux_coord(scalar_dim_coord, [])
        self.assertEqual(cube.aux_coords, (scalar_dim_coord,))

        cube = self.cube.copy()
        cube.add_aux_coord(scalar_dim_coord, ())
        self.assertEqual(cube.aux_coords, (scalar_dim_coord,))

        cube = self.cube.copy()
        cube.add_aux_coord(scalar_dim_coord, None)
        self.assertEqual(cube.aux_coords, (scalar_dim_coord,))

        cube = self.cube.copy()
        cube.add_aux_coord(scalar_dim_coord)
        self.assertEqual(cube.aux_coords, (scalar_dim_coord,))

    def test_add_aux_coord(self):
        y_another = iris.coords.DimCoord(np.array([  2.5,   7.5,  12.5]), long_name='y_another')
        
        # DimCoords can live in cube.aux_coords
        self.cube.add_aux_coord(y_another, 0)
        self.assertEqual(self.cube.dim_coords, ())
        self.assertEqual(self.cube.coords(), [y_another])
        self.assertEqual(self.cube.aux_coords, (y_another,))

        # AuxCoords in cube.aux_coords
        self.cube.add_aux_coord(self.xy, [0, 1])
        self.assertEqual(self.cube.dim_coords, ())
        self.assertEqual(self.cube.coords(), [y_another, self.xy])
        self.assertEqual(set(self.cube.aux_coords), {y_another, self.xy})

        # Lengths must match up
        cube = self.cube.copy()
        with self.assertRaises(ValueError):
            cube.add_aux_coord(self.xy, [1, 0])

    def test_remove_coord(self):
        self.cube.add_dim_coord(self.y, 0)
        self.cube.add_dim_coord(self.x, 1)
        self.cube.add_aux_coord(self.xy, (0, 1))
        self.assertEqual(set(self.cube.coords()), {self.y, self.x, self.xy})
        
        self.cube.remove_coord('xy')
        self.assertEqual(set(self.cube.coords()), {self.y, self.x})

        self.cube.remove_coord('x')
        self.assertEqual(self.cube.coords(), [self.y])

        self.cube.remove_coord('y')
        self.assertEqual(self.cube.coords(), [])

    def test_immutable_dimcoord_dims(self):
        # Add DimCoord to dimension 1
        dims = [1]
        self.cube.add_dim_coord(self.x, dims)
        self.assertEqual(self.cube.coord_dims(self.x), (1,))

        # Change dims object
        dims[0] = 0
        # Check the cube is unchanged
        self.assertEqual(self.cube.coord_dims(self.x), (1,))

        # Check coord_dims cannot be changed
        dims = self.cube.coord_dims(self.x)
        with self.assertRaises(TypeError):
            dims[0] = 0

    def test_immutable_auxcoord_dims(self):
        # Add AuxCoord to dimensions (0, 1)
        dims = [0, 1]
        self.cube.add_aux_coord(self.xy, dims)
        self.assertEqual(self.cube.coord_dims(self.xy), (0, 1))

        # Change dims object
        dims[0] = 1
        dims[1] = 0
        # Check the cube is unchanged
        self.assertEqual(self.cube.coord_dims(self.xy), (0, 1))

        # Check coord_dims cannot be changed
        dims = self.cube.coord_dims(self.xy)
        with self.assertRaises(TypeError):
            dims[0] = 1


class TestStockCubeStringRepresentations(tests.IrisTest):
    def setUp(self):
        self.cube = iris.tests.stock.realistic_4d()

    def _check(self, cube):
        prefix = 'realistic_{}d'.format(cube.ndim)
        self.assertString(str(cube), ('cdm', 'str_repr', prefix + '.str.txt'))
        self.assertString(repr(cube), ('cdm', 'str_repr', prefix + '.repr.txt'))

    def test_4d(self):
        self._check(self.cube)

    def test_3d(self):
        self._check(self.cube[0])

    def test_2d(self):
        self._check(self.cube[0, 0])

    def test_1d(self):
        self._check(self.cube[0, 0, 0])

    def test_0d(self):
        self._check(self.cube[0, 0, 0, 0])


@tests.skip_data
class TestCubeStringRepresentations(IrisDotTest):
    def setUp(self):
        path = tests.get_data_path(('PP', 'simple_pp', 'global.pp'))
        self.cube_2d = iris.load_cube(path)
        # Generate the unicode cube up here now it's used in two tests.
        unicode_str = unichr(40960) + u'abcd' + unichr(1972)
        self.unicode_cube = iris.tests.stock.simple_1d()
        self.unicode_cube.attributes['source'] = unicode_str

    def test_dot_simple_pp(self):
        # Test dot output of a 2d cube loaded from pp.
        cube = self.cube_2d
        cube.attributes['my_attribute'] = 'foobar'
        self.check_dot(cube, ('file_load', 'global_pp.dot'))
        
        pt = cube.coord('time')
        # and with custom coord attributes
        pt.attributes['monty'] = 'python'
        pt.attributes['brain'] = 'hurts'
        self.check_dot(cube, ('file_load', 'coord_attributes.dot'))
        
        del pt.attributes['monty']
        del pt.attributes['brain']
        del cube.attributes['my_attribute']
       
    # TODO hybrid height and dot output - relatitionship links
    def test_dot_4d(self):
        cube = iris.tests.stock.realistic_4d()
        self.check_dot(cube, ('file_load', '4d_pp.dot'))

    def test_missing_coords(self):
        cube = iris.tests.stock.realistic_4d()
        cube.remove_coord('time')
        cube.remove_coord('model_level_number')
        self.assertString(repr(cube),
                          ('cdm', 'str_repr', 'missing_coords_cube.repr.txt'))
        self.assertString(str(cube),
                          ('cdm', 'str_repr', 'missing_coords_cube.str.txt'))

    def test_cubelist_string(self):
        cube_list = iris.cube.CubeList([iris.tests.stock.realistic_4d(),
                                        iris.tests.stock.global_pp()])
        self.assertString(str(cube_list), ('cdm', 'str_repr', 'cubelist.__str__.txt'))
        self.assertString(repr(cube_list), ('cdm', 'str_repr', 'cubelist.__repr__.txt'))

    def test_basic_0d_cube(self):
        self.assertString(repr(self.cube_2d[0, 0]),
                          ('cdm', 'str_repr', '0d_cube.__repr__.txt'))
        self.assertString(unicode(self.cube_2d[0, 0]),
                          ('cdm', 'str_repr', '0d_cube.__unicode__.txt'))
        self.assertString(str(self.cube_2d[0, 0]),
                          ('cdm', 'str_repr', '0d_cube.__str__.txt'))

    def test_similar_coord(self):
        cube = self.cube_2d.copy()

        lon = cube.coord('longitude')
        lon.attributes['flight'] = '218BX'
        lon.attributes['sensor_id'] = 808
        lon.attributes['status'] = 2
        lon2 = lon.copy()
        lon2.attributes['sensor_id'] = 810
        lon2.attributes['ref'] = 'A8T-22'
        del lon2.attributes['status']
        cube.add_aux_coord(lon2, [1])

        lat = cube.coord('latitude')
        lat2 = lat.copy()
        lat2.attributes['test'] = 'True'
        cube.add_aux_coord(lat2, [0])

        self.assertString(str(cube), ('cdm', 'str_repr', 'similar.__str__.txt'))

    def test_cube_summary_cell_methods(self):
        
        cube = self.cube_2d.copy()
        
        # Create a list of values used to create cell methods
        test_values = ((("mean",), (u'longitude', 'latitude'), (u'6 minutes', '12 minutes'), (u'This is a test comment',)),
                        (("average",), (u'longitude', 'latitude'), (u'6 minutes', '15 minutes'), (u'This is another test comment','This is another comment')),
                        (("average",), (u'longitude', 'latitude'), (), ()),
                        (("percentile",), (u'longitude',), (u'6 minutes',), (u'This is another test comment',)))
        
        for x in test_values:
            # Create a cell method
            cm = iris.coords.CellMethod(method=x[0][0], coords=x[1], intervals=x[2], comments=x[3])
            cube.add_cell_method(cm)
        
        self.assertString(str(cube), ('cdm', 'str_repr', 'cell_methods.__str__.txt'))

    def test_cube_summary_alignment(self):
        # Test the cube summary dimension alignment and coord name clipping
        cube = iris.tests.stock.simple_1d()
        aux = iris.coords.AuxCoord(range(11), long_name='This is a really, really, really long long_name that requires to be clipped because it is too long')
        cube.add_aux_coord(aux, 0)
        aux = iris.coords.AuxCoord(range(11), long_name='This is a short long_name')
        cube.add_aux_coord(aux, 0)
        self.assertString(str(cube), ('cdm', 'str_repr', 'simple.__str__.txt'))

    @contextmanager
    def unicode_encoding_change(self, new_encoding):
        default_encoding = sys.getdefaultencoding()
        reload(sys).setdefaultencoding(new_encoding)
        yield
        sys.setdefaultencoding(default_encoding)
        del sys.setdefaultencoding

    def test_adjusted_default_encoding(self):
        # Test cube str representation on non-system-default encodings.
        # Doing this requires access to a sys method that is removed by default
        # so reload sys to restore access.
        # Note this does not currently work with utf-16 or utf-32.

        # Run assertions inside 'with' statement to ensure test file is 
        # accurately re-created.
        with self.unicode_encoding_change('utf-8'):
            self.assertString(str(self.unicode_cube),
                              ('cdm', 'str_repr',
                               'unicode_attribute.__str__.utf8.txt'))
        with self.unicode_encoding_change('ascii'):
            self.assertString(str(self.unicode_cube),
                              ('cdm', 'str_repr',
                               'unicode_attribute.__str__.ascii.txt'))

    def test_unicode_attribute(self):
        self.assertString(
            unicode(self.unicode_cube), ('cdm', 'str_repr',
                                         'unicode_attribute.__unicode__.txt'))


@tests.skip_data
class TestValidity(tests.IrisTest):
    def setUp(self):
        self.cube_2d = iris.load_cube(tests.get_data_path(('PP', 'simple_pp', 'global.pp')))

    def test_wrong_length_vector_coord(self):
        wobble = iris.coords.DimCoord(points=[1, 2], long_name='wobble', units='1')
        with self.assertRaises(ValueError):
            self.cube_2d.add_aux_coord(wobble, 0)

    def test_invalid_dimension_vector_coord(self):
        wobble = iris.coords.DimCoord(points=[1, 2], long_name='wobble', units='1')
        with self.assertRaises(ValueError):
            self.cube_2d.add_dim_coord(wobble, 99)


class TestQueryCoord(tests.IrisTest):
    def setUp(self):
        self.t = iris.tests.stock.simple_2d_w_multidim_and_scalars()

    def test_name(self):
        coords = self.t.coords('dim1')
        self.assertEqual([coord.name() for coord in coords], ['dim1'])
        
        coords = self.t.coords('dim2')
        self.assertEqual([coord.name() for coord in coords], ['dim2'])
        
        coords = self.t.coords('an_other')
        self.assertEqual([coord.name() for coord in coords], ['an_other'])

        coords = self.t.coords('air_temperature')
        self.assertEqual([coord.name() for coord in coords], ['air_temperature'])

        coords = self.t.coords('wibble')
        self.assertEqual(coords, [])

    def test_long_name(self):
        # Both standard_name and long_name defined
        coords = self.t.coords(long_name='custom long name')
        # coord.name() returns standard_name if available
        self.assertEqual([coord.name() for coord in coords], ['air_temperature'])

    def test_standard_name(self):
        # Both standard_name and long_name defined
        coords = self.t.coords(standard_name='custom long name')
        self.assertEqual([coord.name() for coord in coords], [])
        coords = self.t.coords(standard_name='air_temperature')
        self.assertEqual([coord.name() for coord in coords], ['air_temperature'])

    def test_var_name(self):
        coords = self.t.coords(var_name='custom_var_name')
        # Matching coord in test cube has a standard_name of 'air_temperature'.
        self.assertEqual([coord.name() for coord in coords], ['air_temperature'])

    def test_axis(self):
        cube = self.t.copy()
        cube.coord("dim1").rename("latitude")
        cube.coord("dim2").rename("longitude")
        
        coords = cube.coords(axis='y')
        self.assertEqual([coord.name() for coord in coords], ['latitude'])
        
        coords = cube.coords(axis='x')
        self.assertEqual([coord.name() for coord in coords], ['longitude'])

        # Renaming shoudn't be enough
        cube.coord("an_other").rename("time")
        coords = cube.coords(axis='t')
        self.assertEqual([coord.name() for coord in coords], [])
        # Change units to "hours since ..." as it's the presence of a
        # time unit that identifies a time axis.
        cube.coord("time").units = 'hours since 1970-01-01 00:00:00'
        coords = cube.coords(axis='t')
        self.assertEqual([coord.name() for coord in coords], ['time'])
        
        coords = cube.coords(axis='z')
        self.assertEqual(coords, [])

    def test_contains_dimension(self):
        coords = self.t.coords(contains_dimension=0)
        self.assertEqual([coord.name() for coord in coords], ['dim1', 'my_multi_dim_coord'])
        
        coords = self.t.coords(contains_dimension=1)
        self.assertEqual([coord.name() for coord in coords], ['dim2', 'my_multi_dim_coord'])
        
        coords = self.t.coords(contains_dimension=2)
        self.assertEqual(coords, [])

    def test_dimensions(self):
        coords = self.t.coords(dimensions=0)
        self.assertEqual([coord.name() for coord in coords], ['dim1'])
        
        coords = self.t.coords(dimensions=1)
        self.assertEqual([coord.name() for coord in coords], ['dim2'])
        
        # find all coordinates which do not describe a dimension
        coords = self.t.coords(dimensions=[])
        self.assertEqual([coord.name() for coord in coords], ['air_temperature', 'an_other'])
        
        coords = self.t.coords(dimensions=2)
        self.assertEqual(coords, [])
        
        coords = self.t.coords(dimensions=[0, 1])
        self.assertEqual([coord.name() for coord in coords], ['my_multi_dim_coord'])
       
    def test_coord_dim_coords_keyword(self):
        coords = self.t.coords(dim_coords=True)
        self.assertEqual(set([coord.name() for coord in coords]), {'dim1', 'dim2'})

        coords = self.t.coords(dim_coords=False)
        self.assertEqual(set([coord.name() for coord in coords]), {'an_other', 'my_multi_dim_coord', 'air_temperature'})

    def test_coords_empty(self):
        coords = self.t.coords()
        self.assertEqual(set([coord.name() for coord in coords]), {'dim1', 'dim2', 'an_other', 'my_multi_dim_coord', 'air_temperature'})
    
    def test_coord(self):
        coords = self.t.coords(self.t.coord('dim1'))
        self.assertEqual([coord.name() for coord in coords], ['dim1'])
        # check for metadata look-up by modifying points
        coord = self.t.coord('dim1').copy()
        coord.points = np.arange(5) * 1.23
        coords = self.t.coords(coord)
        self.assertEqual([coord.name() for coord in coords], ['dim1'])
        
    def test_str_repr(self):
        # TODO consolidate with the TestCubeStringRepresentations class
        self.assertString(str(self.t), ('cdm', 'str_repr', 'multi_dim_coord.__str__.txt'))
        self.assertString(repr(self.t), ('cdm', 'str_repr', 'multi_dim_coord.__repr__.txt'))
    

class TestCube2d(tests.IrisTest):
    def setUp(self):
        self.t = iris.tests.stock.simple_2d_w_multidim_and_scalars()
        self.t.remove_coord('air_temperature')


class Test2dIndexing(TestCube2d):
    def test_indexing_of_0d_cube(self):
        c = self.t[0, 0]
        self.assertRaises(IndexError, c.__getitem__, (slice(None, None), ) )
        
    def test_cube_indexing_0d(self):
        self.assertCML([self.t[0, 0]], ('cube_slice', '2d_to_0d_cube_slice.cml'))
        
    def test_cube_indexing_1d(self):
        self.assertCML([self.t[0, 0:]], ('cube_slice', '2d_to_1d_cube_slice.cml'))
    
    def test_cube_indexing_1d_multi_slice(self):
        self.assertCML([self.t[0, (0, 1)]], ('cube_slice', '2d_to_1d_cube_multi_slice.cml'))
        self.assertCML([self.t[0, np.array([0, 1])]], ('cube_slice', '2d_to_1d_cube_multi_slice.cml'))
    
    def test_cube_indexing_1d_multi_slice2(self):
        self.assertCML([self.t[(0, 2), (0, 1, 3)]], ('cube_slice', '2d_to_1d_cube_multi_slice2.cml'))
        self.assertCML([self.t[np.array([0, 2]), (0, 1, 3)]], ('cube_slice', '2d_to_1d_cube_multi_slice2.cml'))
        self.assertCML([self.t[np.array([0, 2]), np.array([0, 1, 3])]], ('cube_slice', '2d_to_1d_cube_multi_slice2.cml'))
        
    def test_cube_indexing_1d_multi_slice3(self):
        self.assertCML([self.t[(0, 2), :]], ('cube_slice', '2d_to_1d_cube_multi_slice3.cml'))
        self.assertCML([self.t[np.array([0, 2]), :]], ('cube_slice', '2d_to_1d_cube_multi_slice3.cml'))

    def test_cube_indexing_no_change(self):
        self.assertCML([self.t[0:, 0:]], ('cube_slice', '2d_orig.cml'))
    
    def test_cube_indexing_reverse_coords(self):
        self.assertCML([self.t[::-1, ::-1]], ('cube_slice', '2d_to_2d_revesed.cml'))
        
    def test_cube_indexing_no_residual_change(self):
        self.t[0:3]
        self.assertCML([self.t], ('cube_slice', '2d_orig.cml'))
        
    def test_overspecified(self):
        self.assertRaises(IndexError, self.t.__getitem__, (0, 0, Ellipsis, 0))
        self.assertRaises(IndexError, self.t.__getitem__, (0, 0, 0))
    
    def test_ellipsis(self):
        self.assertCML([self.t[Ellipsis]], ('cube_slice', '2d_orig.cml'))
        self.assertCML([self.t[:, :, :]], ('cube_slice', '2d_orig.cml'))
        self.assertCML([self.t[Ellipsis, Ellipsis]], ('cube_slice', '2d_orig.cml'))
        self.assertCML([self.t[Ellipsis, Ellipsis, Ellipsis]], ('cube_slice', '2d_orig.cml'))
       
        self.assertCML([self.t[Ellipsis, 0, 0]], ('cube_slice', '2d_to_0d_cube_slice.cml'))
        self.assertCML([self.t[0, Ellipsis, 0]], ('cube_slice', '2d_to_0d_cube_slice.cml'))
        self.assertCML([self.t[0, 0, Ellipsis]], ('cube_slice', '2d_to_0d_cube_slice.cml'))
        
        self.assertCML([self.t[Ellipsis, (0, 2), :]], ('cube_slice', '2d_to_1d_cube_multi_slice3.cml'))
        self.assertCML([self.t[(0, 2), Ellipsis, :]], ('cube_slice', '2d_to_1d_cube_multi_slice3.cml'))
        self.assertCML([self.t[(0, 2), :, Ellipsis]], ('cube_slice', '2d_to_1d_cube_multi_slice3.cml'))
        

class TestIteration(TestCube2d):
    def test_cube_iteration(self):
        with self.assertRaises(TypeError):
            for subcube in self.t:
                pass


class Test2dSlicing(TestCube2d):
    def test_cube_slice_all_dimensions(self):
        for cube in self.t.slices(['dim1', 'dim2']):
            self.assertCML(cube, ('cube_slice', '2d_orig.cml'))
            
    def test_cube_slice_with_transpose(self):
        for cube in self.t.slices(['dim2', 'dim1']):
            self.assertCML(cube, ('cube_slice', '2d_transposed.cml'))

    def test_cube_slice_without_transpose(self):
        for cube in self.t.slices(['dim2', 'dim1'], ordered=False):
            self.assertCML(cube, ('cube_slice', '2d_orig.cml'))
            
    def test_cube_slice_1dimension(self):
        # Result came from the equivalent test test_cube_indexing_1d which
        # does self.t[0, 0:]
        slices = [res for res in self.t.slices(['dim2'])]
        self.assertCML(slices[0], ('cube_slice', '2d_to_1d_cube_slice.cml'))
    
    def test_cube_slice_zero_len_slice(self):
        self.assertRaises(IndexError, self.t.__getitem__, (slice(0, 0)))
    
    def test_cube_slice_with_non_existant_coords(self):
        with self.assertRaises(iris.exceptions.CoordinateNotFoundError):
            self.t.slices(['dim2', 'dim1', 'doesnt exist'])

    def test_cube_extract_coord_with_non_describing_coordinates(self):
        with self.assertRaises(ValueError):
            self.t.slices(['an_other'])


class Test2dSlicing_ByDim(TestCube2d):
    def test_cube_slice_all_dimensions(self):
        for cube in self.t.slices([0, 1]):
            self.assertCML(cube, ('cube_slice', '2d_orig.cml'))
            
    def test_cube_slice_with_transpose(self):
        for cube in self.t.slices([1, 0]):
            self.assertCML(cube, ('cube_slice', '2d_transposed.cml'))

    def test_cube_slice_without_transpose(self):
        for cube in self.t.slices([1, 0], ordered=False):
            self.assertCML(cube, ('cube_slice', '2d_orig.cml'))
            
    def test_cube_slice_1dimension(self):
        # Result came from the equivalent test test_cube_indexing_1d which
        # does self.t[0, 0:]
        slices = [res for res in self.t.slices([1])]
        self.assertCML(slices[0], ('cube_slice', '2d_to_1d_cube_slice.cml'))

    def test_cube_slice_nodimension(self):
        slices = [res for res in self.t.slices([])]
        self.assertCML(slices[0], ('cube_slice', '2d_to_0d_cube_slice.cml'))
    
    def test_cube_slice_with_non_existant_dims(self):
        with self.assertRaises(IndexError):
            self.t.slices([1, 0, 2])

    def test_cube_slice_duplicate_dimensions(self):
        with self.assertRaises(ValueError):
            self.t.slices([1, 1])


class Test2dSlicing_ByMix(TestCube2d):
    def test_cube_slice_all_dimensions(self):
        for cube in self.t.slices([0, 'dim2']):
            self.assertCML(cube, ('cube_slice', '2d_orig.cml'))
            
    def test_cube_slice_with_transpose(self):
        for cube in self.t.slices(['dim2', 0]):
            self.assertCML(cube, ('cube_slice', '2d_transposed.cml'))

    def test_cube_slice_with_non_existant_dims(self):
        with self.assertRaises(ValueError):
            self.t.slices([1, 0, 'an_other'])


class Test2dExtraction(TestCube2d):
    def test_cube_extract_0d(self):
        # Extract the first value from each of the coords in the cube
        # this result is shared with the self.t[0, 0] test
        self.assertCML([self.t.extract(iris.Constraint(dim1=3.0, dim2=iris.coords.Cell(0, (0, 1))))], ('cube_slice', '2d_to_0d_cube_slice.cml'))
    
    def test_cube_extract_1d(self):
        # Extract the first value from the second coord in the cube
        # this result is shared with the self.t[0, 0:] test
        self.assertCML([self.t.extract(iris.Constraint(dim1=3.0))], ('cube_slice', '2d_to_1d_cube_slice.cml'))
        
    def test_cube_extract_2d(self):
        # Do nothing - return the original
        self.assertCML([self.t.extract(iris.Constraint())], ('cube_slice', '2d_orig.cml'))

    def test_cube_extract_coord_which_does_not_exist(self):
        self.assertEqual(self.t.extract(iris.Constraint(doesnt_exist=8.1)), None)
            
    def test_cube_extract_coord_with_non_existant_values(self):
        self.assertEqual(self.t.extract(iris.Constraint(dim1=8)), None)
            
    
class Test2dExtractionByCoord(TestCube2d):
    def test_cube_extract_by_coord_advanced(self):
        # This test reverses the coordinate in the cube and also takes a subset of the original coordinate
        points = np.array([9, 8, 7, 5, 4, 3, 2, 1, 0], dtype=np.int32)
        bounds = np.array([[18, 19], [16, 17], [14, 15], [10, 11], [ 8,  9], [ 6,  7], [ 4,  5], [ 2,  3], [ 0,  1]], dtype=np.int32)
        c = iris.coords.DimCoord(points, long_name='dim2', units='meters', bounds=bounds)
        self.assertCML(self.t.subset(c), ('cube_slice', '2d_intersect_and_reverse.cml'))
        

@tests.skip_data
class TestCubeExtract(tests.IrisTest):
    def setUp(self):
        self.single_cube = iris.load_cube(tests.get_data_path(('PP', 'globClim1', 'theta.pp')), 'air_potential_temperature')

    def test_simple(self):
        constraint = iris.Constraint(latitude=10)
        cube = self.single_cube.extract(constraint)
        self.assertCML(cube, ('cdm', 'extract', 'lat_eq_10.cml'))
        constraint = iris.Constraint(latitude=lambda c: c > 10)
        self.assertCML(self.single_cube.extract(constraint), ('cdm', 'extract', 'lat_gt_10.cml'))
        
    def test_combined(self):
        constraint = iris.Constraint(latitude=lambda c: c > 10, longitude=lambda c: c >= 10)

        self.assertCML(self.single_cube.extract(constraint), ('cdm', 'extract', 'lat_gt_10_and_lon_ge_10.cml'))
    
    def test_no_results(self):
        constraint = iris.Constraint(latitude=lambda c: c > 1000000)
        self.assertEqual(self.single_cube.extract(constraint), None)
        
        
class TestCubeAPI(TestCube2d):
    def test_getting_standard_name(self):
        self.assertEqual(self.t.name(), 'test 2d dimensional cube')

    def test_rename(self):
        self.t.rename('foo')
        self.assertEqual(self.t.name(), 'foo')

    def test_var_name(self):
        self.t.var_name = None
        self.assertEqual(self.t.var_name, None)
        self.t.var_name = 'bar'
        self.assertEqual(self.t.var_name, 'bar')

    def test_name_and_var_name(self):
        # Assign only var_name.
        self.t.standard_name = None
        self.t.long_name = None
        self.t.var_name = 'foo'
        # name() should return var_name if standard_name and
        # long_name are None.
        self.assertEqual(self.t.name(), 'foo')

    def test_rename_and_var_name(self):
        self.t.var_name = 'bar'
        self.t.rename('foo')
        # Rename should clear var_name.
        self.assertIsNone(self.t.var_name)

    def test_setting_invalid_var_name(self):
        # Name with whitespace should raise an exception.
        with self.assertRaises(ValueError):
            self.t.var_name = 'foo bar'

    def test_setting_empty_var_name(self):
        # Empty string should raise an exception.
        with self.assertRaises(ValueError):
            self.t.var_name = ''

    def test_getting_units(self):
        self.assertEqual(self.t.units, iris.unit.Unit('meters'))

    def test_setting_units(self):
        self.assertEqual(self.t.units, iris.unit.Unit('meters'))
        self.t.units = 'kelvin'
        self.assertEqual(self.t.units, iris.unit.Unit('kelvin'))

    def test_clearing_units(self):
        self.t.units = None
        self.assertEqual(str(self.t.units), 'unknown')

    def test_convert_units(self):
        # Set to 'volt'
        self.t.units = iris.unit.Unit('volt')
        data = self.t.data.copy()
        # Change to 'kV' - data should be scaled automatically.
        self.t.convert_units('kV')
        self.assertEqual(str(self.t.units), 'kV')
        self.assertArrayAlmostEqual(self.t.data, data / 1000.0)

    def test_coords_are_copies(self):
        self.assertIsNot(self.t.coord('dim1'), self.t.copy().coord('dim1'))

    def test_metadata_nop(self):
        self.t.metadata = self.t.metadata
        self.assertIsNone(self.t.standard_name)
        self.assertEqual(self.t.long_name, 'test 2d dimensional cube')
        self.assertIsNone(self.t.var_name)
        self.assertEqual(self.t.units, 'meters')
        self.assertEqual(self.t.attributes, {})
        self.assertEqual(self.t.cell_methods, ())

    def test_metadata_tuple(self):
        metadata = ('air_pressure', 'foo', 'bar', '', {'random': '12'}, ())
        self.t.metadata = metadata
        self.assertEqual(self.t.standard_name, 'air_pressure')
        self.assertEqual(self.t.long_name, 'foo')
        self.assertEqual(self.t.var_name, 'bar')
        self.assertEqual(self.t.units, '')
        self.assertEqual(self.t.attributes, metadata[4])
        self.assertIsNot(self.t.attributes, metadata[4])
        self.assertEqual(self.t.cell_methods, ())

    def test_metadata_dict(self):
        metadata = {'standard_name': 'air_pressure',
                    'long_name': 'foo',
                    'var_name': 'bar',
                    'units': '',
                    'attributes': {'random': '12'},
                    'cell_methods': ()}
        self.t.metadata = metadata
        self.assertEqual(self.t.standard_name, 'air_pressure')
        self.assertEqual(self.t.long_name, 'foo')
        self.assertEqual(self.t.var_name, 'bar')
        self.assertEqual(self.t.units, '')
        self.assertEqual(self.t.attributes, metadata['attributes'])
        self.assertIsNot(self.t.attributes, metadata['attributes'])
        self.assertEqual(self.t.cell_methods, ())

    def test_metadata_attrs(self):
        class Metadata(object): pass
        metadata = Metadata()
        metadata.standard_name = 'air_pressure'
        metadata.long_name = 'foo'
        metadata.var_name = 'bar'
        metadata.units = ''
        metadata.attributes = {'random': '12'}
        metadata.cell_methods = ()
        self.t.metadata = metadata
        self.assertEqual(self.t.standard_name, 'air_pressure')
        self.assertEqual(self.t.long_name, 'foo')
        self.assertEqual(self.t.var_name, 'bar')
        self.assertEqual(self.t.units, '')
        self.assertEqual(self.t.attributes, metadata.attributes)
        self.assertIsNot(self.t.attributes, metadata.attributes)
        self.assertEqual(self.t.cell_methods, ())

    def test_metadata_fail(self):
        with self.assertRaises(TypeError):
            self.t.metadata = ('air_pressure', 'foo', 'bar', '', {'random': '12'})
        with self.assertRaises(TypeError):
            self.t.metadata = ('air_pressure', 'foo', 'bar', '', {'random': '12'}, (), ())
        with self.assertRaises(TypeError):
            self.t.metadata = {'standard_name': 'air_pressure',
                               'long_name': 'foo',
                               'var_name': 'bar',
                               'units': '',
                               'attributes': {'random': '12'}}
        with self.assertRaises(TypeError):
            class Metadata(object): pass
            metadata = Metadata()
            metadata.standard_name = 'air_pressure'
            metadata.long_name = 'foo'
            metadata.var_name = 'bar'
            metadata.units = ''
            metadata.attributes = {'random': '12'}
            self.t.metadata = metadata


class TestCubeEquality(TestCube2d):
    def test_simple_equality(self):
        self.assertEqual(self.t, self.t.copy())
    
    def test_data_inequality(self):
        self.assertNotEqual(self.t, self.t + 1)
    
    def test_coords_inequality(self):
        r = self.t.copy()
        r.remove_coord(r.coord('an_other'))
        self.assertNotEqual(self.t, r)
    
    def test_attributes_inequality(self):
        r = self.t.copy()
        r.attributes['new_thing'] = None
        self.assertNotEqual(self.t, r)

    def test_array_attributes(self):
        r = self.t.copy()
        r.attributes['things'] = np.arange(3)
        s = r.copy()
        self.assertEqual(s, r)

        s.attributes['things'] = np.arange(2)
        self.assertNotEqual(s, r)

        del s.attributes['things']
        self.assertNotEqual(s, r)

    def test_cell_methods_inequality(self):
        r = self.t.copy()
        r.add_cell_method(iris.coords.CellMethod('mean'))
        self.assertNotEqual(self.t, r)

    def test_not_compatible(self):
        r = self.t.copy()
        self.assertTrue(self.t.is_compatible(r))
        # The following changes should make the cubes incompatible.
        # Different units.
        r.units = 'kelvin'
        self.assertFalse(self.t.is_compatible(r))
        # Different cell_methods.
        r = self.t.copy()
        r.add_cell_method(iris.coords.CellMethod('mean', coords='dim1'))
        self.assertFalse(self.t.is_compatible(r))
        # Different attributes.
        r = self.t.copy()
        self.t.attributes['source']= 'bob'
        r.attributes['source'] = 'alice'
        self.assertFalse(self.t.is_compatible(r))

    def test_compatible(self):
        r = self.t.copy()
        self.assertTrue(self.t.is_compatible(r))
        # The following changes should not affect compatibility.
        # Different non-common attributes.
        self.t.attributes['source']= 'bob'
        r.attributes['origin'] = 'alice'
        self.assertTrue(self.t.is_compatible(r))
        # Different coordinates.
        r.remove_coord('dim1')
        self.assertTrue(self.t.is_compatible(r))
        # Different data.
        r.data = np.zeros(r.shape)
        self.assertTrue(self.t.is_compatible(r))
        # Different var_names (but equal name()).
        r.var_name = 'foo'
        self.assertTrue(self.t.is_compatible(r))

    def test_is_compatible_ignore(self):
        r = self.t.copy()
        self.assertTrue(self.t.is_compatible(r))
        # Different histories.
        self.t.attributes['history'] = 'One history.'
        r.attributes['history'] = 'An alternative history.'
        self.assertFalse(self.t.is_compatible(r))
        # Use ignore keyword.
        self.assertTrue(self.t.is_compatible(r, ignore='history'))
        self.assertTrue(self.t.is_compatible(r, ignore=('history',)))
        self.assertTrue(self.t.is_compatible(r, ignore=r.attributes))

    def test_is_compatible_metadata(self):
        metadata = self.t.metadata
        self.assertTrue(self.t.is_compatible(metadata))


@tests.skip_data
class TestDataManagerIndexing(TestCube2d):
    def setUp(self):
        self.cube = iris.load_cube(tests.get_data_path(('PP', 'aPProt1', 'rotatedMHtimecube.pp')))

    def _is_lazy(self, cube):
        self.assertTrue(cube.has_lazy_data())

    def _is_concrete(self, cube):
        self.assertFalse(cube.has_lazy_data())

    def test_slices(self):
        lat_cube = self.cube.slices(['grid_latitude', ]).next()
        self._is_lazy(lat_cube)
        self._is_lazy(self.cube)
 
    def test_cube_empty_indexing(self):
        test_filename = ('cube_slice', 'real_empty_data_indexing.cml')
        r = self.cube[:5, ::-1][3]
        rshape = r.shape

        # Make sure we still have deferred data.
        self._is_lazy(r)
        # check the CML of this result
        self.assertCML(r, test_filename)
        # The CML was checked, meaning the data must have been loaded.
        # Check that the cube no longer has deferred data.
        self._is_concrete(r)
        
        r_data = r.data
        
        #finally, load the data before indexing and check that it generates the same result
        c = self.cube
        c.data
        c = c[:5, ::-1][3]
        self.assertCML(c, test_filename)
        
        self.assertEqual(rshape, c.shape)
        
        np.testing.assert_array_equal(r_data, c.data)
        
    def test_real_data_cube_indexing(self):
        cube = self.cube[(0, 4, 5, 2), 0, 0]
        self.assertCML(cube, ('cube_slice', 'real_data_dual_tuple_indexing1.cml'))

        cube = self.cube[0, (0, 4, 5, 2), (3, 5, 5)]
        self.assertCML(cube, ('cube_slice', 'real_data_dual_tuple_indexing2.cml'))
        
        cube = self.cube[(0, 4, 5, 2), 0, (3, 5, 5)]
        self.assertCML(cube, ('cube_slice', 'real_data_dual_tuple_indexing3.cml'))

        self.assertRaises(IndexError, self.cube.__getitem__, ((0, 4, 5, 2), (3, 5, 5), 0, 0, 4) )
        self.assertRaises(IndexError, self.cube.__getitem__, (Ellipsis, Ellipsis, Ellipsis, Ellipsis, Ellipsis, Ellipsis) )

    def test_fancy_indexing_bool_array(self):
        cube = self.cube
        cube.data = np.ma.masked_array(cube.data, mask=cube.data > 100000)
        r = cube[:, cube.coord('grid_latitude').points > 1]
        self.assertEqual(r.shape, (10, 218, 720))
        data = cube.data[:, self.cube.coord('grid_latitude').points > 1, :]
        np.testing.assert_array_equal(data, r.data)
        np.testing.assert_array_equal(data.mask, r.data.mask)


class TestCubeCollapsed(tests.IrisTest):
    def partial_compare(self, dual, single):
        result = iris.analysis.coord_comparison(dual, single)
        self.assertEqual(len(result['not_equal']), 0)
        self.assertEqual(dual.name(), single.name(), "dual and single stage standard_names differ")
        self.assertEqual(dual.units, single.units, "dual and single stage units differ")
        self.assertEqual(dual.shape, single.shape, "dual and single stage shape differ")

    def collapse_test_common(self, cube, a_name, b_name, *args, **kwargs):
        
        # preserve filenames from before the introduction of "grid_" in rotated coord names.
        a_filename = a_name.replace("grid_", "")
        b_filename = b_name.replace("grid_", "")
        
        # compare dual and single stage collapsing
        dual_stage = cube.collapsed(a_name, iris.analysis.MEAN)
        dual_stage = dual_stage.collapsed(b_name, iris.analysis.MEAN)
        self.assertCMLApproxData(dual_stage, ('cube_collapsed', '%s_%s_dual_stage.cml' % (a_filename, b_filename)), *args, **kwargs)

        single_stage = cube.collapsed([a_name, b_name], iris.analysis.MEAN)
        self.assertCMLApproxData(single_stage, ('cube_collapsed', '%s_%s_single_stage.cml' % (a_filename, b_filename)), *args, **kwargs)

        # Compare the cube bits that should match
        self.partial_compare(dual_stage, single_stage)

    @tests.skip_data
    def test_multi_d(self):
        cube = iris.tests.stock.realistic_4d()

        # TODO: Re-instate surface_altitude & hybrid-height once we're
        # using the post-CF test results.
        cube.remove_aux_factory(cube.aux_factories[0])
        cube.remove_coord('surface_altitude')

        self.assertCML(cube, ('cube_collapsed', 'original.cml'))

        # Compare 2-stage collapsing with a single stage collapse over 2 Coords.
        self.collapse_test_common(cube, 'grid_latitude', 'grid_longitude', decimal=1)
        self.collapse_test_common(cube, 'grid_longitude', 'grid_latitude', decimal=1)

        self.collapse_test_common(cube, 'time', 'grid_latitude', decimal=1)
        self.collapse_test_common(cube, 'grid_latitude', 'time', decimal=1)

        self.collapse_test_common(cube, 'time', 'grid_longitude', decimal=1)
        self.collapse_test_common(cube, 'grid_longitude', 'time', decimal=1)

        self.collapse_test_common(cube, 'grid_latitude', 'model_level_number', decimal=1)
        self.collapse_test_common(cube, 'model_level_number', 'grid_latitude', decimal=1)

        self.collapse_test_common(cube, 'grid_longitude', 'model_level_number', decimal=1)
        self.collapse_test_common(cube, 'model_level_number', 'grid_longitude', decimal=1)

        self.collapse_test_common(cube, 'time', 'model_level_number', decimal=1)
        self.collapse_test_common(cube, 'model_level_number', 'time', decimal=1)

        self.collapse_test_common(cube, 'model_level_number', 'time', decimal=1)
        self.collapse_test_common(cube, 'time', 'model_level_number', decimal=1)

        # Collapse 3 things at once.
        triple_collapse = cube.collapsed(['model_level_number', 'time', 'grid_longitude'], iris.analysis.MEAN)
        self.assertCMLApproxData(triple_collapse, ('cube_collapsed', 'triple_collapse_ml_pt_lon.cml'), decimal=1)

        triple_collapse = cube.collapsed(['grid_latitude', 'model_level_number', 'time'], iris.analysis.MEAN)
        self.assertCMLApproxData(triple_collapse, ('cube_collapsed', 'triple_collapse_lat_ml_pt.cml'), decimal=1)

        # Ensure no side effects
        self.assertCML(cube, ('cube_collapsed', 'original.cml'))
        
        
class TestTrimAttributes(tests.IrisTest):
    def test_non_string_attributes(self):
        cube = iris.tests.stock.realistic_4d()
        attrib_key = "gorf"
        attrib_val = 23
        cube.attributes[attrib_key] = attrib_val
        
        summary = cube.summary() # Get the cube summary
        
        # Check through the lines of the summary to see that our attribute is there
        attrib_re = re.compile("%s.*?%s" % (attrib_key, attrib_val))

        for line in summary.split("\n"):
            result = re.match(attrib_re, line.strip())
            if result:
                break
        else: # No match found for our attribute
            self.fail('Attribute not found in summary output of cube.')


@tests.skip_data
class TestMaskedData(tests.IrisTest, pp.PPTest):
    def _load_3d_cube(self):
        # This 3D data set has a missing a slice with SOME missing values.
        # The missing data is in the pressure = 1000 hPa, forcast_period = 0,
        # time = 1970-02-11 16:00:00 slice.
        return iris.load_cube(tests.get_data_path(["PP", "mdi_handmade_small", "*.pp"]))
    
    def test_complete_field(self):
        # This pp field has no missing data values
        cube = iris.load_cube(tests.get_data_path(["PP", "mdi_handmade_small", "mdi_test_1000_3.pp"]))

        self.assertIsInstance(cube.data, np.ndarray)

    def test_masked_field(self):
        # This pp field has some missing data values
        cube = iris.load_cube(tests.get_data_path(["PP", "mdi_handmade_small", "mdi_test_1000_0.pp"]))
        self.assertIsInstance(cube.data, ma.core.MaskedArray)

    def test_missing_file(self):
        cube = self._load_3d_cube()
        self.assertIsInstance(cube.data, ma.core.MaskedArray)
        self.assertCML(cube, ('cdm', 'masked_cube.cml'))
        
    def test_slicing(self):
        cube = self._load_3d_cube()

        # Test the slicing before deferred loading
        full_slice = cube[3]
        partial_slice = cube[0]
        self.assertIsInstance(full_slice.data, np.ndarray)
        self.assertIsInstance(partial_slice.data, ma.core.MaskedArray)
        self.assertEqual(ma.count_masked(partial_slice.data), 25)

        # Test the slicing is consistent after deferred loading
        full_slice = cube[3]
        partial_slice = cube[0]
        self.assertIsInstance(full_slice.data, np.ndarray)
        self.assertIsInstance(partial_slice.data, ma.core.MaskedArray)
        self.assertEqual(ma.count_masked(partial_slice.data), 25)

    def test_save_and_merge(self):
        cube = self._load_3d_cube()

        # extract the 2d field that has SOME missing values
        masked_slice = cube[0]
        masked_slice.data.fill_value = 123456
        
        # test saving masked data
        reference_txt_path = tests.get_result_path(('cdm', 'masked_save_pp.txt'))
        with self.cube_save_test(reference_txt_path, reference_cubes=masked_slice) as temp_pp_path:
            iris.save(masked_slice, temp_pp_path)
        
            # test merge keeps the mdi we just saved
            cube1 = iris.load_cube(temp_pp_path)
            cube2 = cube1.copy()
            # make cube1 and cube2 differ on a scalar coord, to make them mergeable into a 3d cube
            cube2.coord("pressure").points[0] = 1001.0
            merged_cubes = iris.cube.CubeList([cube1, cube2]).merge()
            self.assertEqual(len(merged_cubes), 1, "expected a single merged cube")
            merged_cube = merged_cubes[0]
            self.assertEqual(merged_cube.data.fill_value, 123456)


class TestConversionToCoordList(tests.IrisTest):
    def test_coord_conversion(self):
        cube = iris.tests.stock.realistic_4d()
        
        # Single string
        self.assertEquals(len(cube._as_list_of_coords('grid_longitude')), 1)
        
        # List of string and unicode
        self.assertEquals(len(cube._as_list_of_coords(['grid_longitude', u'grid_latitude'], )), 2)
        
        # Coord object(s)
        lat = cube.coords("grid_latitude")[0]
        lon = cube.coords("grid_longitude")[0]
        self.assertEquals(len(cube._as_list_of_coords(lat)), 1)
        self.assertEquals(len(cube._as_list_of_coords([lat, lon])), 2)
        
        # Mix of string-like and coord
        self.assertEquals(len(cube._as_list_of_coords(["grid_latitude", lon])), 2)

        # Empty list
        self.assertEquals(len(cube._as_list_of_coords([])), 0)
        
        # Invalid coords
        invalid_choices = [iris.analysis.MEAN, # Caused by mixing up argument order in call to cube.collasped for example
                           None,
                           ['grid_latitude', None],
                           [lat, None],
                          ]

        for coords in invalid_choices:
            with self.assertRaises(TypeError):
                cube._as_list_of_coords(coords)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_cell
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


from __future__ import division

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import unittest

import numpy as np

import iris.coords
from iris.coords import Cell


class TestCells(unittest.TestCase):
    def setUp(self):
        self.cell1 = iris.coords.Cell(3, [2, 4])
        self.cell2 = iris.coords.Cell(360., [350., 370.])

    def test_cell_from_coord(self):
        Cell = iris.coords.Cell
        coord = iris.coords.AuxCoord(np.arange(4) * 1.5, long_name='test', units='1')
        self.assertEqual(Cell(point=0.0, bound=None), coord.cell(0))
        self.assertEqual(Cell(point=1.5, bound=None), coord.cell(1))
        self.assertEqual(Cell(point=4.5, bound=None), coord.cell(-1))
        self.assertEqual(Cell(point=3.0, bound=None), coord.cell(-2))

        # put bounds on the regular coordinate
        coord.guess_bounds()
        self.assertEqual(Cell(point=0.0, bound=(-0.75, 0.75)), coord.cell(0))
        self.assertEqual(Cell(point=1.5, bound=(0.75, 2.25)), coord.cell(1))
        self.assertEqual(Cell(point=4.5, bound=(3.75, 5.25)), coord.cell(-1))
        self.assertEqual(Cell(point=3.0, bound=(2.25, 3.75)), coord.cell(-2))
        self.assertEqual(Cell(point=4.5, bound=(3.75, 5.25)), coord.cell(slice(-1, None)))

    def test_cell_from_multidim_coord(self):
        Cell = iris.coords.Cell
        coord = iris.coords.AuxCoord(points=np.arange(12).reshape(3, 4), long_name='test', units='1',
                                     bounds=np.arange(48).reshape(3, 4, 4))
        self.assertRaises(IndexError, coord.cell, 0)
        self.assertEqual(Cell(point=3, bound=(12, 13, 14, 15)), coord.cell((0, 3)))

    def test_mod(self):
        # Check that applying the mod function is not modifying the original
        c =  self.cell1 % 3
        self.assertNotEqual(c, self.cell1)

        c = self.cell2 % 360
        self.assertEqual(str(c), 'Cell(point=0.0, bound=(350.0, 10.0))')

        c = self.cell2 % 359.13
        self.assertEqual(str(c), 'Cell(point=0.8700000000000045, bound=(350.0, 10.870000000000005))')

    def test_contains_point(self):
        c = iris.coords.Cell(359.5, (359.49951, 359.5004))
        self.assertTrue(c.contains_point(359.49951))

    def test_pointless(self):
        self.assertRaises(ValueError, iris.coords.Cell, None, (359.49951, 359.5004))

    def test_add(self):
        # Check that applying the mod function is not modifying the original
        c =  self.cell1 + 3
        self.assertNotEqual(c, self.cell1)

        c = self.cell2 + 360
        self.assertEqual(str(c), 'Cell(point=720.0, bound=(710.0, 730.0))')

        c = self.cell2 + 359.13
        self.assertEqual(str(c), 'Cell(point=719.13, bound=(709.13, 729.13))')

    def test_in(self):
        c = iris.coords.Cell(4, None)
        self.assertFalse(c in [3, 5])
        self.assertTrue(c in [3, 4])

        c = iris.coords.Cell(4, [4, 5])
        self.assertFalse(c in [3, 6])
        self.assertTrue(c in [3, 4])
        self.assertTrue(c in [3, 5])

        c = iris.coords.Cell(4, [4, 5])
        c1 = iris.coords.Cell(5, [4, 5])
        c2 = iris.coords.Cell(4, [3, 6])

        self.assertTrue(c in [3, c])
        self.assertFalse(c in [3, c1])
        self.assertFalse(c in [3, c2])

    def test_coord_equality(self):
        self.d = iris.coords.Cell(1.9, None)
        self.assertTrue(self.d == 1.9)
        self.assertFalse(self.d == [1.5, 1.9])
        self.assertFalse(self.d != 1.9)
        self.assertTrue(self.d >= 1.9)
        self.assertTrue(self.d <= 1.9)
        self.assertFalse(self.d > 1.9)
        self.assertFalse(self.d < 1.9)
        self.assertFalse(self.d in [1.5, 3.5])
        self.assertTrue(self.d in [1.5, 1.9])

        self.assertTrue(self.d != 1)
        self.assertFalse(self.d == 1)
        self.assertFalse(self.d >= 2)
        self.assertFalse(self.d <= 1)
        self.assertTrue(self.d > 1)
        self.assertTrue(self.d < 2)
        
        # Ensure the Cell's operators return NotImplemented.
        class Terry(object): pass
        self.assertEquals(self.d.__eq__(Terry()), NotImplemented)
        self.assertEquals(self.d.__ne__(Terry()), NotImplemented)

    def test_numpy_int_equality(self):
        dtypes = (np.int, np.int16, np.int32, np.int64)
        for dtype in dtypes:
            val = dtype(3)
            cell = iris.coords.Cell(val, None)
            self.assertEqual(cell, val)

    def test_numpy_float_equality(self):
        dtypes = (np.float, np.float16, np.float32, np.float64,
                  np.float128, np.double)
        for dtype in dtypes:
            val = dtype(3.2)
            cell = iris.coords.Cell(val, None)
            self.assertEqual(cell, val, dtype)

    def test_coord_bounds_cmp(self):
        self.e = iris.coords.Cell(0.7, [1.1, 1.9])
        self.assertTrue(self.e == 1.6)
        self.assertFalse(self.e != 1.6)
        self.assertTrue(self.e >= 1.9)
        self.assertTrue(self.e <= 1.9)
        self.assertFalse(self.e > 1.9)
        self.assertFalse(self.e < 1.9)


        self.assertFalse(self.e in [1.0, 3.5])
        self.assertTrue(self.e in [1.5, 1.9])
        self.assertTrue(self.e != 1)
        self.assertFalse(self.e == 1)
        self.assertFalse(self.e >= 2)
        self.assertFalse(self.e <= 1)
        self.assertTrue(self.e > 1)
        self.assertTrue(self.e < 2)

    def test_cell_cell_cmp(self):
        self.e = iris.coords.Cell(0.7, [1.1, 1.9])
        self.f = iris.coords.Cell(0.8, [1.1, 1.9])

        self.assertFalse( self.e > self.f )
        self.assertTrue( self.e <= self.f )
        self.assertTrue( self.f >= self.e )
        self.assertFalse( self.f < self.e )

        self.e = iris.coords.Cell(0.9, [2, 2.1])
        self.f = iris.coords.Cell(0.8, [1.1, 1.9])

        self.assertTrue( self.e > self.f )
        self.assertFalse( self.e <= self.f )
        self.assertFalse( self.f >= self.e )
        self.assertTrue( self.f < self.e )

    def test_cmp_contig(self):
        # Test cells that share an edge
        a = iris.coords.Cell(point=1054440.0, bound=(1054080.0, 1054800.0))
        b = iris.coords.Cell(point=1055160.0, bound=(1054800.0, 1055520.0))
        self.assertTrue(a < b)
        self.assertTrue(a <= b)
        self.assertFalse(a == b)
        self.assertFalse(a >= b)
        self.assertFalse(a > b)

    def test_overlap_order(self):
        # Test cells that overlap still sort correctly.
        cells = [Cell(point=375804.0, bound=(375792.0, 375816.0)),
                 Cell(point=375672.0, bound=(375660.0, 375684.0)),
                 Cell(point=375792.0, bound=(375780.0, 375804.0)),
                 Cell(point=375960.0, bound=(375948.0, 375972.0))]
        sorted_cells = [Cell(point=375672.0, bound=(375660.0, 375684.0)),
                        Cell(point=375792.0, bound=(375780.0, 375804.0)),
                        Cell(point=375804.0, bound=(375792.0, 375816.0)),
                        Cell(point=375960.0, bound=(375948.0, 375972.0))]
        self.assertEqual(sorted(cells), sorted_cells)

    def _check_permutations(self, a, b, a_lt_b, a_le_b, a_eq_b):
        self.assertEqual(a < b, a_lt_b)
        self.assertEqual(a <= b, a_le_b)
        self.assertEqual(a == b, a_eq_b)

        self.assertNotEqual(a > b, a_le_b)
        self.assertNotEqual(a >= b, a_lt_b)
        self.assertNotEqual(a != b, a_eq_b)

    def _check_all_permutations(self, a, b, a_lt_b, a_le_b, a_eq_b):
        self._check_permutations(a, b, a_lt_b, a_le_b, a_eq_b)
        self._check_permutations(b, a, not a_le_b, not a_lt_b, a_eq_b)

    def test_comparison_numeric(self):
        # Check what happens when you compare a simple number with a
        # point-only Cell.
        self._check_permutations(9, Cell(10), True, True, False)
        self._check_permutations(10, Cell(10), False, True, True)
        self._check_permutations(11, Cell(10), False, False, False)

    def test_comparison_numeric_with_bounds(self):
        # Check what happens when you compare a simple number with a
        # point-and-bound Cell.
        self._check_permutations(7, Cell(10, [8, 12]), True, True, False)
        self._check_permutations(8, Cell(10, [8, 12]), False, True, True)
        self._check_permutations(9, Cell(10, [8, 12]), False, True, True)
        self._check_permutations(10, Cell(10, [8, 12]), False, True, True)
        self._check_permutations(11, Cell(10, [8, 12]), False, True, True)
        self._check_permutations(12, Cell(10, [8, 12]), False, True, True)
        self._check_permutations(13, Cell(10, [8, 12]), False, False, False)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_cf
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the cf module.

"""
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import unittest

import mock

import iris
import iris.fileformats.cf as cf


class TestCaching(unittest.TestCase):
    def test_cached(self):
        # Make sure attribute access to the underlying netCDF4.Variable
        # is cached.
        name = 'foo'
        nc_var = mock.MagicMock()
        cf_var = cf.CFAncillaryDataVariable(name, nc_var)
        self.assertEqual(nc_var.ncattrs.call_count, 1)

        # Accessing a netCDF attribute should result in no further calls
        # to nc_var.ncattrs() and the creation of an attribute on the
        # cf_var.
        # NB. Can't use hasattr() because that triggers the attribute
        # to be created!
        self.assertTrue('coordinates' not in cf_var.__dict__)
        _ = cf_var.coordinates
        self.assertEqual(nc_var.ncattrs.call_count, 1)
        self.assertTrue('coordinates' in cf_var.__dict__)

        # Trying again results in no change.
        _ = cf_var.coordinates
        self.assertEqual(nc_var.ncattrs.call_count, 1)
        self.assertTrue('coordinates' in cf_var.__dict__)

        # Trying another attribute results in just a new attribute.
        self.assertTrue('standard_name' not in cf_var.__dict__)
        _ = cf_var.standard_name
        self.assertEqual(nc_var.ncattrs.call_count, 1)
        self.assertTrue('standard_name' in cf_var.__dict__)


@tests.skip_data
class TestCFReader(tests.IrisTest):
    def setUp(self):
        filename = tests.get_data_path(
            ('NetCDF', 'rotated', 'xyt', 'small_rotPole_precipitation.nc'))
        self.cfr = cf.CFReader(filename)

    def test_ancillary_variables_pass_0(self):
        self.assertEqual(self.cfr.cf_group.ancillary_variables, {})

    def test_auxiliary_coordinates_pass_0(self):
        self.assertEqual(sorted(self.cfr.cf_group.auxiliary_coordinates.keys()),
                         ['lat', 'lon'])

        lat = self.cfr.cf_group['lat']
        self.assertEqual(lat.shape, (190, 174))
        self.assertEqual(lat.dimensions, ('rlat', 'rlon'))
        self.assertEqual(lat.ndim, 2)
        self.assertEqual(lat.cf_attrs(), 
                         (('long_name', 'latitude'), 
                          ('standard_name', 'latitude'), 
                          ('units', 'degrees_north')))

        lon = self.cfr.cf_group['lon']
        self.assertEqual(lon.shape, (190, 174))
        self.assertEqual(lon.dimensions, ('rlat', 'rlon'))
        self.assertEqual(lon.ndim, 2)
        self.assertEqual(lon.cf_attrs(), 
                         (('long_name', 'longitude'), 
                          ('standard_name', 'longitude'), 
                          ('units', 'degrees_east')))

    def test_bounds_pass_0(self):
        self.assertEqual(sorted(self.cfr.cf_group.bounds.keys()), ['time_bnds'])

        time_bnds = self.cfr.cf_group['time_bnds']
        self.assertEqual(time_bnds.shape, (4, 2))
        self.assertEqual(time_bnds.dimensions, ('time', 'time_bnds'))
        self.assertEqual(time_bnds.ndim, 2)
        self.assertEqual(time_bnds.cf_attrs(), ())

    def test_coordinates_pass_0(self):
        self.assertEqual(sorted(self.cfr.cf_group.coordinates.keys()), 
                         ['rlat', 'rlon', 'time'])

        rlat = self.cfr.cf_group['rlat']
        self.assertEqual(rlat.shape, (190,))
        self.assertEqual(rlat.dimensions, ('rlat',))
        self.assertEqual(rlat.ndim, 1)
        attr = []
        attr.append(('axis', 'Y'))
        attr.append(('long_name', 'rotated latitude'))
        attr.append(('standard_name', 'grid_latitude'))
        attr.append(('units', 'degrees'))
        self.assertEqual(rlat.cf_attrs(), tuple(attr))

        rlon = self.cfr.cf_group['rlon']
        self.assertEqual(rlon.shape, (174,))
        self.assertEqual(rlon.dimensions, ('rlon',))
        self.assertEqual(rlon.ndim, 1)
        attr = []
        attr.append(('axis', 'X'))
        attr.append(('long_name', 'rotated longitude'))
        attr.append(('standard_name', 'grid_longitude'))
        attr.append(('units', 'degrees'))
        self.assertEqual(rlon.cf_attrs(), tuple(attr))

        time = self.cfr.cf_group['time']
        self.assertEqual(time.shape, (4,))
        self.assertEqual(time.dimensions, ('time',))
        self.assertEqual(time.ndim, 1)
        attr = []
        attr.append(('axis', 'T'))
        attr.append(('bounds', 'time_bnds'))
        attr.append(('calendar', 'gregorian'))
        attr.append(('long_name', 'Julian Day'))
        attr.append(('units', 'days since 1950-01-01 00:00:00.0'))
        self.assertEqual(time.cf_attrs(), tuple(attr))

    def test_data_pass_0(self):
        self.assertEqual(sorted(self.cfr.cf_group.data_variables.keys()), 
                         ['pr'])

        data = self.cfr.cf_group['pr']
        self.assertEqual(data.shape, (4, 190, 174))
        self.assertEqual(data.dimensions, ('time', 'rlat', 'rlon'))
        self.assertEqual(data.ndim, 3)
        attr = []
        attr.append(('_FillValue', 1e+30))
        attr.append(('cell_methods', 'time: mean'))
        attr.append(('coordinates', 'lon lat'))
        attr.append(('grid_mapping', 'rotated_pole'))
        attr.append(('long_name', 'Precipitation'))
        attr.append(('missing_value', 1e+30))
        attr.append(('standard_name', 'precipitation_flux'))
        attr.append(('units', 'kg m-2 s-1'))
        attr = tuple(attr)
        self.assertEqual(data.cf_attrs()[0][0], attr[0][0])
        self.assertAlmostEqual(data.cf_attrs()[0][1], attr[0][1], delta=1.6e+22)
        self.assertEqual(data.cf_attrs()[1:5], attr[1:5])
        self.assertAlmostEqual(data.cf_attrs()[5][1], attr[5][1], delta=1.6e+22)
        self.assertEqual(data.cf_attrs()[6:], attr[6:])

    def test_formula_terms_pass_0(self):
        self.assertEqual(self.cfr.cf_group.formula_terms, {})

    def test_grid_mapping_pass_0(self):
        self.assertEqual(sorted(self.cfr.cf_group.grid_mappings.keys()), 
                         ['rotated_pole'])

        rotated_pole = self.cfr.cf_group['rotated_pole']
        self.assertEqual(rotated_pole.shape, ())
        self.assertEqual(rotated_pole.dimensions, ())
        self.assertEqual(rotated_pole.ndim, 0)
        attr = []
        attr.append(('grid_mapping_name', 'rotated_latitude_longitude'))
        attr.append(('grid_north_pole_latitude', 18.0))
        attr.append(('grid_north_pole_longitude', -140.75))
        self.assertEqual(rotated_pole.cf_attrs(), tuple(attr))

    def test_cell_measures_pass_0(self):
        self.assertEqual(self.cfr.cf_group.cell_measures, {})

    def test_global_attributes_pass_0(self):
        self.assertEqual(
            sorted(self.cfr.cf_group.global_attributes.keys()),
            ['Conventions', 'NCO', 'experiment', 
                'history', 'institution', 'source',]
        )

        self.assertEqual(self.cfr.cf_group.global_attributes['Conventions'], 
                         'CF-1.0')
        self.assertEqual(self.cfr.cf_group.global_attributes['experiment'], 
                         'ER3')
        self.assertEqual(self.cfr.cf_group.global_attributes['institution'], 
                         'DMI')
        self.assertEqual(self.cfr.cf_group.global_attributes['source'], 
                         'HIRHAM')

    def test_variable_cf_group_pass_0(self):
        self.assertEqual(sorted(self.cfr.cf_group['time'].cf_group.keys()), 
                         ['time_bnds'])
        self.assertEqual(sorted(self.cfr.cf_group['pr'].cf_group.keys()), 
                         ['lat', 'lon', 'rlat', 'rlon', 'rotated_pole', 'time'])

    def test_variable_attribute_touch_pass_0(self):
        lat = self.cfr.cf_group['lat']
        
        self.assertEqual(lat.cf_attrs(), 
                         (('long_name', 'latitude'), 
                          ('standard_name', 'latitude'), 
                          ('units', 'degrees_north')))
        self.assertEqual(lat.cf_attrs_used(), ())
        self.assertEqual(lat.cf_attrs_unused(), 
                         (('long_name', 'latitude'), 
                          ('standard_name', 'latitude'), 
                          ('units', 'degrees_north')))

        # touch some variable attributes.
        lat.long_name
        lat.units
        self.assertEqual(lat.cf_attrs_used(), 
                         (('long_name', 'latitude'), 
                          ('units', 'degrees_north')))
        self.assertEqual(lat.cf_attrs_unused(), 
                         (('standard_name', 'latitude'),))

        # clear the attribute touch history.
        lat.cf_attrs_reset()
        self.assertEqual(lat.cf_attrs_used(), ())
        self.assertEqual(lat.cf_attrs_unused(), 
                         (('long_name', 'latitude'), 
                          ('standard_name', 'latitude'), 
                          ('units', 'degrees_north')))


@tests.skip_data
class TestLoad(tests.IrisTest):
    def test_attributes_empty(self):
        filename = tests.get_data_path(('NetCDF', 'global', 'xyt',
                                        'SMALL_hires_wind_u_for_ipcc4.nc'))
        cube = iris.load_cube(filename)
        self.assertEquals(cube.coord('time').attributes, {})

    def test_attributes_contain_positive(self):
        filename = tests.get_data_path(('NetCDF', 'global', 'xyt',
                                        'SMALL_hires_wind_u_for_ipcc4.nc'))
        cube = iris.load_cube(filename)
        self.assertEquals(cube.coord('height').attributes['positive'], 'up')

    def test_attributes_populated(self):
        filename = tests.get_data_path(
            ('NetCDF', 'label_and_climate', 'small_FC_167_mon_19601101.nc'))
        cube = iris.load_cube(filename)
        self.assertEquals(
            sorted(cube.coord('longitude').attributes.items()), 
            [('data_type', 'float'), 
             ('modulo', 360), 
             ('topology', 'circular')
            ]
        )

    def test_cell_methods(self):
        filename = tests.get_data_path(('NetCDF', 'global', 'xyt', 'SMALL_hires_wind_u_for_ipcc4.nc'))
        cube = iris.load_cube(filename)
        self.assertEquals(cube.cell_methods, (iris.coords.CellMethod(method=u'mean', coords=(u'time',), intervals=(u'6 minutes',), comments=()),))


@tests.skip_data
class TestClimatology(tests.IrisTest):
    def setUp(self):
        filename = tests.get_data_path(('NetCDF', 'label_and_climate',
                                        'A1B-99999a-river-sep-2070-2099.nc'))
        self.cfr = cf.CFReader(filename)

    def test_bounds(self):
        time = self.cfr.cf_group['temp_dmax_tmean_abs'].cf_group.coordinates['time']
        climatology = time.cf_group.climatology
        self.assertEqual(len(climatology), 1)
        self.assertEqual(climatology.keys(), ['climatology_bounds'])

        climatology_var = climatology['climatology_bounds']
        self.assertEqual(climatology_var.ndim, 2)
        self.assertEqual(climatology_var.shape, (1, 2))


@tests.skip_data
class TestLabels(tests.IrisTest):
    def setUp(self):
        filename = tests.get_data_path(
            ('NetCDF', 'label_and_climate', 
             'A1B-99999a-river-sep-2070-2099.nc'))
        self.cfr_start = cf.CFReader(filename)

        filename = tests.get_data_path(
            ('NetCDF', 'label_and_climate', 
             'small_FC_167_mon_19601101.nc'))
        self.cfr_end = cf.CFReader(filename)

    def test_label_dim_start(self):
        cf_data_var = self.cfr_start.cf_group['temp_dmax_tmean_abs']

        region_group = self.cfr_start.cf_group.labels['region_name']
        self.assertEqual(sorted(self.cfr_start.cf_group.labels.keys()), 
                         [u'region_name'])
        self.assertEqual(sorted(cf_data_var.cf_group.labels.keys()), 
                         [u'region_name'])

        self.assertEqual(region_group.cf_label_dimensions(cf_data_var), 
                         (u'georegion',))
        self.assertEqual(region_group.cf_label_data(cf_data_var)[0], 
                         'Anglian')

        cf_data_var = self.cfr_start.cf_group['cdf_temp_dmax_tmean_abs']

        self.assertEqual(sorted(self.cfr_start.cf_group.labels.keys()), 
                         [u'region_name'])
        self.assertEqual(sorted(cf_data_var.cf_group.labels.keys()), 
                         [u'region_name'])

        self.assertEqual(region_group.cf_label_dimensions(cf_data_var), 
                         (u'georegion',))
        self.assertEqual(region_group.cf_label_data(cf_data_var)[0], 
                         'Anglian') 

    def test_label_dim_end(self):
        cf_data_var = self.cfr_end.cf_group['tas']

        self.assertEqual(sorted(self.cfr_end.cf_group.labels.keys()), [u'experiment_id', u'institution', u'source'])
        self.assertEqual(sorted(cf_data_var.cf_group.labels.keys()), [u'experiment_id', u'institution', u'source'])

        self.assertEqual(self.cfr_end.cf_group.labels['experiment_id'].cf_label_dimensions(cf_data_var), (u'ensemble',))
        self.assertEqual(self.cfr_end.cf_group.labels['experiment_id'].cf_label_data(cf_data_var)[0], '2005')

        self.assertEqual(self.cfr_end.cf_group.labels['institution'].cf_label_dimensions(cf_data_var), (u'ensemble',))
        self.assertEqual(self.cfr_end.cf_group.labels['institution'].cf_label_data(cf_data_var)[0], 'ECMWF')

        self.assertEqual(self.cfr_end.cf_group.labels['source'].cf_label_dimensions(cf_data_var), (u'ensemble',))
        self.assertEqual(self.cfr_end.cf_group.labels['source'].cf_label_data(cf_data_var)[0], 'IFS33R1/HOPE-E, Sys 1, Met 1, ENSEMBLES')



if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_coding_standards
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

from datetime import datetime
from fnmatch import fnmatch
from glob import glob
import os
import re
import subprocess
import unittest

import pep8

import iris


LICENSE_TEMPLATE = """
# (C) British Crown Copyright {YEARS}, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.""".strip()


LICENSE_RE_PATTERN = re.escape(LICENSE_TEMPLATE).replace('\{YEARS\}', '(.*?)')
# Add shebang possibility to the LICENSE_RE_PATTERN
LICENSE_RE_PATTERN = r'(\#\!.*\n)?' + LICENSE_RE_PATTERN
LICENSE_RE = re.compile(LICENSE_RE_PATTERN, re.MULTILINE)


# Guess iris repo directory of Iris - realpath is used to mitigate against
# Python finding the iris package via a symlink.
IRIS_DIR = os.path.realpath(os.path.dirname(iris.__file__))
REPO_DIR = os.path.dirname(os.path.dirname(IRIS_DIR))
DOCS_DIR = os.path.join(REPO_DIR, 'docs', 'iris')
exclusion = ['Makefile', 'build']
DOCS_DIRS = glob(os.path.join(DOCS_DIR, '*'))
DOCS_DIRS = [DOC_DIR for DOC_DIR in DOCS_DIRS if os.path.basename(DOC_DIR) not
             in exclusion]


class StandardReportWithExclusions(pep8.StandardReport):
    expected_bad_files = [
        '*/iris/std_names.py',
        '*/iris/analysis/calculus.py',
        '*/iris/analysis/geometry.py',
        '*/iris/analysis/interpolate.py',
        '*/iris/analysis/maths.py',
        '*/iris/analysis/trajectory.py',
        '*/iris/fileformats/cf.py',
        '*/iris/fileformats/dot.py',
        '*/iris/fileformats/grib/__init__.py',
        '*/iris/fileformats/grib/_grib_cf_map.py',
        '*/iris/fileformats/grib/load_rules.py',
        '*/iris/fileformats/pp.py',
        '*/iris/fileformats/pp_rules.py',
        '*/iris/fileformats/rules.py',
        '*/iris/fileformats/um_cf_map.py',
        '*/iris/fileformats/_pyke_rules/compiled_krb/compiled_pyke_files.py',
        '*/iris/fileformats/_pyke_rules/compiled_krb/fc_rules_cf_fc.py',
        '*/iris/io/__init__.py',
        '*/iris/io/format_picker.py',
        '*/iris/tests/__init__.py',
        '*/iris/tests/pp.py',
        '*/iris/tests/stock.py',
        '*/iris/tests/system_test.py',
        '*/iris/tests/test_analysis.py',
        '*/iris/tests/test_analysis_calculus.py',
        '*/iris/tests/test_basic_maths.py',
        '*/iris/tests/test_cartography.py',
        '*/iris/tests/test_cdm.py',
        '*/iris/tests/test_cell.py',
        '*/iris/tests/test_cf.py',
        '*/iris/tests/test_constraints.py',
        '*/iris/tests/test_coord_api.py',
        '*/iris/tests/test_coord_categorisation.py',
        '*/iris/tests/test_coordsystem.py',
        '*/iris/tests/test_cube_to_pp.py',
        '*/iris/tests/test_file_load.py',
        '*/iris/tests/test_file_save.py',
        '*/iris/tests/test_grib_save.py',
        '*/iris/tests/test_grib_save_rules.py',
        '*/iris/tests/test_hybrid.py',
        '*/iris/tests/test_interpolation.py',
        '*/iris/tests/test_intersect.py',
        '*/iris/tests/test_io_init.py',
        '*/iris/tests/test_iterate.py',
        '*/iris/tests/test_load.py',
        '*/iris/tests/test_mapping.py',
        '*/iris/tests/test_merge.py',
        '*/iris/tests/test_pickling.py',
        '*/iris/tests/test_pp_cf.py',
        '*/iris/tests/test_pp_module.py',
        '*/iris/tests/test_pp_stash.py',
        '*/iris/tests/test_pp_to_cube.py',
        '*/iris/tests/test_quickplot.py',
        '*/iris/tests/test_regrid.py',
        '*/iris/tests/test_rules.py',
        '*/iris/tests/test_std_names.py',
        '*/iris/tests/test_trajectory.py',
        '*/iris/tests/test_unit.py',
        '*/iris/tests/test_uri_callback.py',
        '*/iris/tests/test_util.py',
        '*/iris/tests/test_verbose_logging.py']

    if DOCS_DIRS:
        expected_bad_docs_files = [
            '*/example_code/graphics/COP_1d_plot.py',
            '*/example_code/graphics/COP_maps.py',
            '*/example_code/graphics/SOI_filtering.py',
            '*/example_code/graphics/cross_section.py',
            '*/example_code/graphics/custom_file_loading.py',
            '*/example_code/graphics/global_map.py',
            '*/example_code/graphics/hovmoller.py',
            '*/example_code/graphics/lagged_ensemble.py',
            '*/example_tests/test_COP_1d_plot.py',
            '*/example_tests/test_COP_maps.py',
            '*/example_tests/test_SOI_filtering.py',
            '*/example_tests/test_TEC.py',
            '*/example_tests/test_cross_section.py',
            '*/example_tests/test_custom_file_loading.py',
            '*/example_tests/test_deriving_phenomena.py',
            '*/example_tests/test_global_map.py',
            '*/example_tests/test_hovmoller.py',
            '*/example_tests/test_lagged_ensemble.py',
            '*/example_tests/test_lineplot_with_legend.py',
            '*/example_tests/test_rotated_pole_mapping.py',
            '*/src/conf.py',
            '*/src/developers_guide/gitwash_dumper.py',
            '*/src/sphinxext/custom_class_autodoc.py',
            '*/src/sphinxext/gen_example_directory.py',
            '*/src/sphinxext/gen_gallery.py',
            '*/src/sphinxext/gen_rst.py',
            '*/src/sphinxext/generate_package_rst.py',
            '*/src/sphinxext/plot_directive.py',
            '*/src/userguide/plotting_examples/1d_with_legend.py',
            '*/src/userguide/plotting_examples/brewer.py']

        expected_bad_files += expected_bad_docs_files

    matched_exclusions = set()

    def get_file_results(self):
        # If the file had no errors, return self.file_errors (which will be 0)
        if not self._deferred_print:
            return self.file_errors

        # Iterate over all of the patterns, to find a possible exclusion. If we
        # the filename is to be excluded, go ahead and remove the counts that
        # self.error added.
        for pattern in self.expected_bad_files:
            if fnmatch(self.filename, pattern):
                self.matched_exclusions.add(pattern)
                # invert the error method's counters.
                for _, _, code, _, _ in self._deferred_print:
                    self.counters[code] -= 1
                    if self.counters[code] == 0:
                        self.counters.pop(code)
                        self.messages.pop(code)
                    self.file_errors -= 1
                    self.total_errors -= 1
                return self.file_errors

        # Otherwise call the superclass' method to print the bad results.
        return super(StandardReportWithExclusions,
                     self).get_file_results()


class TestCodeFormat(unittest.TestCase):
    def test_pep8_conformance(self):
        #
        #    Tests the iris codebase against the "pep8" tool.
        #
        #    Users can add their own excluded files (should files exist in the
        #    local directory which is not in the repository) by adding a
        #    ".pep8_test_exclude.txt" file in the same directory as this test.
        #    The file should be a line separated list of filenames/directories
        #    as can be passed to the "pep8" tool's exclude list.

        # To get a list of bad files, rather than the specific errors, add
        # "reporter=pep8.FileReport" to the StyleGuide constructor.
        pep8style = pep8.StyleGuide(quiet=False,
                                    reporter=StandardReportWithExclusions)

        # Allow users to add their own exclude list.
        extra_exclude_file = os.path.join(os.path.dirname(__file__),
                                          '.pep8_test_exclude.txt')
        if os.path.exists(extra_exclude_file):
            with open(extra_exclude_file, 'r') as fh:
                extra_exclude = [line.strip() for line in fh if line.strip()]
            pep8style.options.exclude.extend(extra_exclude)

        check_paths = [os.path.dirname(iris.__file__)]
        if DOCS_DIRS:
            check_paths.extend(DOCS_DIRS)

        result = pep8style.check_files(check_paths)
        self.assertEqual(result.total_errors, 0, "Found code syntax "
                                                 "errors (and warnings).")

        reporter = pep8style.options.reporter
        # If we've been using the exclusions reporter, check that we didn't
        # exclude files unnecessarily.
        if reporter is StandardReportWithExclusions:
            unexpectedly_good = sorted(set(reporter.expected_bad_files) -
                                       reporter.matched_exclusions)

            if unexpectedly_good:
                self.fail('Some exclude patterns were unnecessary as the '
                          'files they pointed to either passed the PEP8 tests '
                          'or do not point to a file:\n  '
                          '{}'.format('\n  '.join(unexpectedly_good)))


class TestLicenseHeaders(unittest.TestCase):
    @staticmethod
    def years_of_license_in_file(fh):
        """
        Using :data:`LICENSE_RE` look for the years defined in the license
        header of the given file handle.

        If the license cannot be found in the given fh, None will be returned,
        else a tuple of (start_year, end_year) will be returned.

        """
        license_matches = LICENSE_RE.match(fh.read())
        if not license_matches:
            # no license found in file.
            return None

        years = license_matches.groups()[-1]
        if len(years) == 4:
            start_year = end_year = int(years)
        elif len(years) == 11:
            start_year, end_year = int(years[:4]), int(years[7:])
        else:
            fname = getattr(fh, 'name', 'unknown filename')
            raise ValueError("Unexpected year(s) string in {}'s copyright "
                             "notice: {!r}".format(fname, years))
        return (start_year, end_year)

    @staticmethod
    def whatchanged_parse(whatchanged_output):
        """
        Returns a generator of tuples of data parsed from
        "git whatchanged --pretty='TIME:%at". The tuples are of the form
        ``(filename, last_commit_datetime)``

        Sample input::

            ['TIME:1366884020', '',
             ':000000 100644 0000000... 5862ced... A\tlib/iris/cube.py']

        """
        dt = None
        for line in whatchanged_output:
            if not line.strip():
                continue
            elif line.startswith('TIME:'):
                dt = datetime.fromtimestamp(int(line[5:]))
            else:
                # Non blank, non date, line -> must be the lines
                # containing the file info.
                fname = ' '.join(line.split('\t')[1:])
                yield fname, dt

    @staticmethod
    def last_change_by_fname():
        """
        Return a dictionary of all the files under git which maps to
        the datetime of their last modification in the git history.

        .. note::

            This function raises a ValueError if the repo root does
            not have a ".git" folder. If git is not installed on the system,
            or cannot be found by subprocess, an IOError may also be raised.

        """
        # Check the ".git" folder exists at the repo dir.
        if not os.path.isdir(os.path.join(REPO_DIR, '.git')):
            raise ValueError('{} is not a git repository.'.format(REPO_DIR))

        # Call "git whatchanged" to get the details of all the files and when
        # they were last changed.
        output = subprocess.check_output(['git', 'whatchanged',
                                          "--pretty=TIME:%ct"],
                                         cwd=REPO_DIR)
        output = output.split('\n')
        res = {}
        for fname, dt in TestLicenseHeaders.whatchanged_parse(output):
            if fname not in res or dt > res[fname]:
                res[fname] = dt

        return res

    def test_license_headers(self):
        exclude_patterns = ('setup.py',
                            'build/*',
                            'dist/*',
                            'docs/iris/example_code/graphics/*.py',
                            'docs/iris/src/developers_guide/documenting/*.py',
                            'docs/iris/src/sphinxext/gen_gallery.py',
                            'docs/iris/src/sphinxext/gen_rst.py',
                            'docs/iris/src/sphinxext/plot_directive.py',
                            'docs/iris/src/userguide/plotting_examples/*.py',
                            'docs/iris/src/developers_guide/gitwash_dumper.py',
                            'docs/iris/build/*',
                            'lib/iris/analysis/_scipy_interpolate.py',
                            'lib/iris/fileformats/_pyke_rules/*',
                            'lib/iris/fileformats/grib/_grib_cf_map.py')

        try:
            last_change_by_fname = self.last_change_by_fname()
        except ValueError:
            # Caught the case where this is not a git repo.
            return self.skipTest('Iris installation did not look like a '
                                 'git repo.')

        failed = False
        for fname, last_change in sorted(last_change_by_fname.items()):
            full_fname = os.path.join(REPO_DIR, fname)
            if full_fname.endswith('.py') and os.path.isfile(full_fname) and \
                    not any(fnmatch(fname, pat) for pat in exclude_patterns):
                with open(full_fname) as fh:
                    years = TestLicenseHeaders.years_of_license_in_file(fh)
                    if years is None:
                        print('The file {} has no valid header license and '
                              'has not been excluded from the license header '
                              'test.'.format(fname))
                        failed = True
                    elif last_change.year > years[1]:
                        print('The file header at {} is out of date. The last'
                              ' commit was in {}, but the copyright states it'
                              ' was {}.'.format(fname, last_change.year,
                                                years[1]))
                        failed = True

        if failed:
            raise ValueError('There were license header failures. See stdout.')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_concatenate
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the cube concatenate mechanism.

"""

# import iris tests first so that some things can be initialised
# before importing anything else.
import iris.tests as tests

import numpy as np
import numpy.ma as ma

from iris._concatenate import _CubeSignature as ConcatenateCubeSignature
import iris.cube
from iris.coords import DimCoord, AuxCoord
import iris.tests.stock as stock


def _make_cube(x, y, data, aux=None, offset=0, scalar=None):
    """
    A convenience test function that creates a custom 2D cube.

    Args:

    * x:
        A (start, stop, step) tuple for specifying the
        x-axis dimensional coordinate points. Bounds are
        automatically guessed.

    * y:
        A (start, stop, step) tuple for specifying the
        y-axis dimensional coordinate points. Bounds are
        automatically guessed.

    * data:
        The data payload for the cube.

    Kwargs:

    * aux:
        A CSV string specifying which points only auxiliary
        coordinates to create. Accepts either of 'x', 'y', 'xy'.

    * offset:
        Offset value to be added to the 'xy' auxiliary coordinate
        points.

    * scalar:
        Create a 'height' scalar coordinate with the given value.

    Returns:
        The newly created 2D :class:`iris.cube.Cube`.

    """
    x_range = np.arange(*x, dtype=np.float32)
    y_range = np.arange(*y, dtype=np.float32)
    x_size = len(x_range)
    y_size = len(y_range)

    cube_data = np.empty((y_size, x_size), dtype=np.float32)
    cube_data[:] = data
    cube = iris.cube.Cube(cube_data)
    coord = DimCoord(y_range, long_name='y')
    coord.guess_bounds()
    cube.add_dim_coord(coord, 0)
    coord = DimCoord(x_range, long_name='x')
    coord.guess_bounds()
    cube.add_dim_coord(coord, 1)

    if aux is not None:
        aux = aux.split(',')
        if 'y' in aux:
            coord = AuxCoord(y_range * 10, long_name='y-aux')
            cube.add_aux_coord(coord, (0,))
        if 'x' in aux:
            coord = AuxCoord(x_range * 10, long_name='x-aux')
            cube.add_aux_coord(coord, (1,))
        if 'xy' in aux:
            payload = np.arange(y_size * x_size,
                                dtype=np.float32).reshape(y_size, x_size)
            coord = AuxCoord(payload * 100 + offset, long_name='xy-aux')
            cube.add_aux_coord(coord, (0, 1))

    if scalar is not None:
        data = np.array([scalar], dtype=np.float32)
        coord = AuxCoord(data, long_name='height', units='m')
        cube.add_aux_coord(coord, ())

    return cube


def _make_cube_3d(x, y, z, data, aux=None, offset=0):
    """
    A convenience test function that creates a custom 3D cube.

    Args:

    * x:
        A (start, stop, step) tuple for specifying the
        x-axis dimensional coordinate points. Bounds are
        automatically guessed.

    * y:
        A (start, stop, step) tuple for specifying the
        y-axis dimensional coordinate points. Bounds are
        automatically guessed.

    * z:
        A (start, stop, step) tuple for specifying the
        z-axis dimensional coordinate points. Bounds are
        automatically guessed.

    * data:
        The data payload for the cube.

    Kwargs:

    * aux:
        A CSV string specifying which points only auxiliary
        coordinates to create. Accepts either of 'x', 'y', 'z',
        'xy', 'xz', 'yz', 'xyz'.

    * offset:
        Offset value to be added to non-1D auxiliary coordinate
        points.

    Returns:
        The newly created 3D :class:`iris.cube.Cube`.

    """
    x_range = np.arange(*x, dtype=np.float32)
    y_range = np.arange(*y, dtype=np.float32)
    z_range = np.arange(*z, dtype=np.float32)
    x_size, y_size, z_size = len(x_range), len(y_range), len(z_range)

    cube_data = np.empty((x_size, y_size, z_size), dtype=np.float32)
    cube_data[:] = data
    cube = iris.cube.Cube(cube_data)
    coord = DimCoord(z_range, long_name='z')
    coord.guess_bounds()
    cube.add_dim_coord(coord, 0)
    coord = DimCoord(y_range, long_name='y')
    coord.guess_bounds()
    cube.add_dim_coord(coord, 1)
    coord = DimCoord(x_range, long_name='x')
    coord.guess_bounds()
    cube.add_dim_coord(coord, 2)

    if aux is not None:
        aux = aux.split(',')
        if 'z' in aux:
            coord = AuxCoord(z_range * 10, long_name='z-aux')
            cube.add_aux_coord(coord, (0,))
        if 'y' in aux:
            coord = AuxCoord(y_range * 10, long_name='y-aux')
            cube.add_aux_coord(coord, (1,))
        if 'x' in aux:
            coord = AuxCoord(x_range * 10, long_name='x-aux')
            cube.add_aux_coord(coord, (2,))
        if 'xy' in aux:
            payload = np.arange(x_size * y_size,
                                dtype=np.float32).reshape(y_size, x_size)
            coord = AuxCoord(payload + offset, long_name='xy-aux')
            cube.add_aux_coord(coord, (1, 2))
        if 'xz' in aux:
            payload = np.arange(x_size * z_size,
                                dtype=np.float32).reshape(z_size, x_size)
            coord = AuxCoord(payload * 10 + offset, long_name='xz-aux')
            cube.add_aux_coord(coord, (0, 2))
        if 'yz' in aux:
            payload = np.arange(y_size * z_size,
                                dtype=np.float32).reshape(z_size, y_size)
            coord = AuxCoord(payload * 100 + offset, long_name='yz-aux')
            cube.add_aux_coord(coord, (0, 1))
        if 'xyz' in aux:
            payload = np.arange(x_size * y_size * z_size,
                                dtype=np.float32).reshape(z_size,
                                                          y_size,
                                                          x_size)
            coord = AuxCoord(payload * 1000 + offset, long_name='xyz-aux')
            cube.add_aux_coord(coord, (0, 1, 2))

    return cube


def concatenate(cubes, order=None):
    """
    Explicitly force the contiguous major order of cube data
    alignment to ensure consistent CML crc32 checksums.

    Defaults to contiguous 'C' row-major order.

    """
    if order is None:
        order = 'C'

    cubelist = iris.cube.CubeList(cubes)
    result = cubelist.concatenate()

    for cube in result:
        if ma.isMaskedArray(cube.data):
            # cube.data = ma.copy(cube.data, order=order)
            data = np.array(cube.data.data, copy=True, order=order)
            mask = np.array(cube.data.mask, copy=True, order=order)
            fill_value = cube.data.fill_value
            cube.data = ma.array(data, mask=mask, fill_value=fill_value)
        else:
            # cube.data = np.copy(cube.data, order=order)
            cube.data = np.array(cube.data, copy=True, order=order)

    return result


class TestSimple(tests.IrisTest):
    def test_empty(self):
        cubes = iris.cube.CubeList()
        self.assertEqual(concatenate(cubes), iris.cube.CubeList())

    def test_single(self):
        cubes = [stock.simple_2d()]
        self.assertEqual(concatenate(cubes), cubes)

    def test_multi_equal(self):
        cubes = [stock.simple_2d()] * 2
        self.assertEqual(concatenate(cubes), cubes)


class TestNoConcat(tests.IrisTest):
    def test_anonymous(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 2), y, 1))
        cubes.append(_make_cube((2, 4), y, 2))
        cube = _make_cube((4, 6), y, 3)
        cube.remove_coord('x')
        cubes.append(cube)
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_anonymous.cml'))
        self.assertEqual(len(result), 2)
        self.assertEqual(result[0].shape, (2, 4))
        self.assertEqual(result[1].shape, (2, 2))

    def test_points_overlap_increasing(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 2), y, 1))
        cubes.append(_make_cube((1, 3), y, 2))
        result = concatenate(cubes)
        self.assertEqual(len(result), 2)

    def test_points_overlap_decreasing(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (3, 0, -1), 1))
        cubes.append(_make_cube(x, (1, -1, -1), 2))
        result = concatenate(cubes)
        self.assertEqual(len(result), 2)

    def test_bounds_overlap_increasing(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 2), y, 1))
        cube = _make_cube((2, 4), y, 1)
        cube.coord('x').bounds = np.array([[0.5, 2.5], [2.5, 3.5]])
        cubes.append(cube)
        result = concatenate(cubes)
        self.assertEqual(len(result), 2)

    def test_bounds_overlap_decreasing(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((3, 1, -1), y, 1))
        cube = _make_cube((1, -1, -1), y, 2)
        cube.coord('x').bounds = np.array([[2.5, 0.5], [0.5, -0.5]])
        cubes.append(cube)
        result = concatenate(cubes)
        self.assertEqual(len(result), 2)

    def test_scalar_difference(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 2), y, 1, scalar=10))
        cubes.append(_make_cube((2, 4), y, 2, scalar=20))
        result = concatenate(cubes)
        self.assertEqual(len(result), 2)

    def test_uncommon(self):
        cubes = []
        cubes.append(_make_cube((0, 2), (0, 2), 1))
        cubes.append(_make_cube((2, 4), (2, 4), 2))
        result = concatenate(cubes)
        self.assertEqual(len(result), 2)

    def test_order(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 2), y, 1))
        cubes.append(_make_cube((6, 1, -1), y, 2))
        result = concatenate(cubes)
        self.assertEqual(len(result), 2)

    def test_masked_and_unmasked(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 2), y, 1))
        cube = _make_cube((2, 4), y, 2)
        cube.data = ma.asarray(cube.data)
        cubes.append(cube)
        result = concatenate(cubes)
        self.assertEqual(len(result), 1)

    def test_masked_fill_value(self):
        cubes = []
        y = (0, 2)
        cube = _make_cube((0, 2), y, 1)
        cube.data = ma.asarray(cube.data)
        cube.data.fill_value = 10
        cubes.append(cube)
        cube = _make_cube((2, 4), y, 1)
        cube.data = ma.asarray(cube.data)
        cube.data.fill_value = 20
        cubes.append(cube)
        result = concatenate(cubes)
        self.assertEqual(len(result), 1)


class Test2D(tests.IrisTest):
    def test_concat_masked_2x2d(self):
        cubes = []
        y = (0, 2)
        cube = _make_cube((0, 2), y, 1)
        cube.data = ma.asarray(cube.data)
        cube.data[(0, 1), (0, 1)] = ma.masked
        cubes.append(cube)
        cube = _make_cube((2, 4), y, 2)
        cube.data = ma.asarray(cube.data)
        cube.data[(0, 1), (1, 0)] = ma.masked
        cubes.append(cube)
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_masked_2x2d.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 4))
        mask = np.array([[True, False, False, True],
                         [False, True, True, False]], dtype=np.bool)
        self.assertArrayEqual(result[0].data.mask, mask)

    def test_concat_masked_2y2d(self):
        cubes = []
        x = (0, 2)
        cube = _make_cube(x, (0, 2), 1)
        cube.data = np.ma.asarray(cube.data)
        cube.data[(0, 1), (0, 1)] = ma.masked
        cubes.append(cube)
        cube = _make_cube(x, (2, 4), 2)
        cube.data = ma.asarray(cube.data)
        cube.data[(0, 1), (1, 0)] = ma.masked
        cubes.append(cube)
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_masked_2y2d.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (4, 2))
        mask = np.array([[True, False],
                         [False, True],
                         [False, True],
                         [True, False]], dtype=np.bool)
        self.assertArrayEqual(result[0].data.mask, mask)

    def test_concat_2x2d(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 4), y, 1))
        cubes.append(_make_cube((4, 6), y, 2))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2x2d.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 6))

    def test_concat_2y2d(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 4), 1))
        cubes.append(_make_cube(x, (4, 6), 2))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2y2d.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 2))

    def test_concat_2x2d_aux_x(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 4), y, 1, aux='x'))
        cubes.append(_make_cube((4, 6), y, 2, aux='x'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2x2d_aux_x.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 6))

    def test_concat_2y2d_aux_x(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 4), 1, aux='x'))
        cubes.append(_make_cube(x, (4, 6), 2, aux='x'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2y2d_aux_x.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 2))

    def test_concat_2x2d_aux_y(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 4), y, 1, aux='y'))
        cubes.append(_make_cube((4, 6), y, 2, aux='y'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2x2d_aux_y.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 6))

    def test_concat_2y2d_aux_y(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 4), 1, aux='y'))
        cubes.append(_make_cube(x, (4, 6), 2, aux='y'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2y2d_aux_y.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 2))

    def test_concat_2x2d_aux_x_y(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 4), y, 1, aux='x,y'))
        cubes.append(_make_cube((4, 6), y, 2, aux='x,y'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2x2d_aux_x_y.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 6))

    def test_concat_2y2d_aux_x_y(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 4), 1, aux='x,y'))
        cubes.append(_make_cube(x, (4, 6), 2, aux='x,y'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2y2d_aux_x_y.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 2))

    def test_concat_2x2d_aux_xy(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 4), y, 1, aux='xy'))
        cubes.append(_make_cube((4, 6), y, 2, aux='xy'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2x2d_aux_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 6))

    def test_concat_2y2d_aux_xy(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 4), 1, aux='xy'))
        cubes.append(_make_cube(x, (4, 6), 2, aux='xy'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2y2d_aux_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 2))

    def test_concat_2x2d_aux_x_xy(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 4), y, 1, aux='x,xy'))
        cubes.append(_make_cube((4, 6), y, 2, aux='x,xy'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2x2d_aux_x_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 6))

    def test_concat_2y2d_aux_x_xy(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 4), 1, aux='x,xy'))
        cubes.append(_make_cube(x, (4, 6), 2, aux='x,xy'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2y2d_aux_x_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 2))

    def test_concat_2x2d_aux_y_xy(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 4), y, 1, aux='y,xy'))
        cubes.append(_make_cube((4, 6), y, 2, aux='y,xy'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2x2d_aux_y_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 6))

    def test_concat_2y2d_aux_y_xy(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 4), 1, aux='y,xy'))
        cubes.append(_make_cube(x, (4, 6), 2, aux='y,xy'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2y2d_aux_y_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 2))

    def test_concat_2x2d_aux_x_y_xy(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 4), y, 1, aux='x,y,xy'))
        cubes.append(_make_cube((4, 6), y, 2, aux='x,y,xy'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2x2d_aux_x_y_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 6))

    def test_concat_2y2d_aux_x_y_xy(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 4), 1, aux='x,y,xy'))
        cubes.append(_make_cube(x, (4, 6), 2, aux='x,y,xy'))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_2y2d_aux_x_y_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 2))

    def test_concat_2x2d_aux_x_bounds(self):
        cubes = []
        y = (0, 2)
        cube = _make_cube((0, 4), y, 1, aux='x')
        cube.coord('x-aux').guess_bounds()
        cubes.append(cube)
        cube = _make_cube((4, 6), y, 2, aux='x')
        cube.coord('x-aux').guess_bounds()
        cubes.append(cube)
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate',
                                'concat_2x2d_aux_x_bounds.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 6))

    def test_concat_2x2d_aux_xy_bounds(self):
        cubes = []
        y = (0, 2)
        cube = _make_cube((0, 2), y, 1, aux='xy', offset=1)
        coord = cube.coord('xy-aux')
        coord.bounds = np.array([1, 2, 3, 4,
                                 101, 102, 103, 104,
                                 201, 202, 203, 204,
                                 301, 302, 303, 304]).reshape(2, 2, 4)
        cubes.append(cube)
        cube = _make_cube((2, 4), y, 2, aux='xy', offset=2)
        coord = cube.coord('xy-aux')
        coord.bounds = np.array([2, 3, 4, 5,
                                 102, 103, 104, 105,
                                 202, 203, 204, 205,
                                 302, 303, 304, 305]).reshape(2, 2, 4)
        cubes.append(cube)
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate',
                                'concat_2x2d_aux_xy_bounds.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 4))


class TestMulti2D(tests.IrisTest):
    def test_concat_4x2d_aux_xy(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 2), y, 1, aux='xy', offset=1))
        cubes.append(_make_cube((2, 4), y, 2, aux='xy', offset=2))
        y = (2, 4)
        cubes.append(_make_cube((0, 2), y, 3, aux='xy', offset=3))
        cubes.append(_make_cube((2, 4), y, 4, aux='xy', offset=4))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_4x2d_aux_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (4, 4))

    def test_concat_4y2d_aux_xy(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 2), 1, aux='xy', offset=1))
        cubes.append(_make_cube(x, (2, 4), 2, aux='xy', offset=2))
        x = (2, 4)
        cubes.append(_make_cube(x, (0, 2), 3, aux='xy', offset=3))
        cubes.append(_make_cube(x, (2, 4), 4, aux='xy', offset=4))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_4y2d_aux_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (4, 4))

    def test_concat_4mix2d_aux_xy(self):
        cubes = []
        cubes.append(_make_cube((0, 2), (0, 2), 1, aux='xy', offset=1))
        cubes.append(_make_cube((2, 4), (2, 4), 2, aux='xy', offset=2))
        cubes.append(_make_cube((2, 4), (0, 2), 3, aux='xy', offset=3))
        cubes.append(_make_cube((0, 2), (2, 4), 4, aux='xy', offset=4))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_4mix2d_aux_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (4, 4))

    def test_concat_9x2d_aux_xy(self):
        cubes = []
        y = (0, 2)
        cubes.append(_make_cube((0, 2), y, 1, aux='xy', offset=1))
        cubes.append(_make_cube((2, 4), y, 2, aux='xy', offset=2))
        cubes.append(_make_cube((4, 6), y, 3, aux='xy', offset=3))
        y = (2, 4)
        cubes.append(_make_cube((0, 2), y, 4, aux='xy', offset=4))
        cubes.append(_make_cube((2, 4), y, 5, aux='xy', offset=5))
        cubes.append(_make_cube((4, 6), y, 6, aux='xy', offset=6))
        y = (4, 6)
        cubes.append(_make_cube((0, 2), y, 7, aux='xy', offset=7))
        cubes.append(_make_cube((2, 4), y, 8, aux='xy', offset=8))
        cubes.append(_make_cube((4, 6), y, 9, aux='xy', offset=9))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_9x2d_aux_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 6))

    def test_concat_9y2d_aux_xy(self):
        cubes = []
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 2), 1, aux='xy', offset=1))
        cubes.append(_make_cube(x, (2, 4), 2, aux='xy', offset=2))
        cubes.append(_make_cube(x, (4, 6), 3, aux='xy', offset=3))
        x = (2, 4)
        cubes.append(_make_cube(x, (0, 2), 4, aux='xy', offset=4))
        cubes.append(_make_cube(x, (2, 4), 5, aux='xy', offset=5))
        cubes.append(_make_cube(x, (4, 6), 6, aux='xy', offset=6))
        x = (4, 6)
        cubes.append(_make_cube(x, (0, 2), 7, aux='xy', offset=7))
        cubes.append(_make_cube(x, (2, 4), 8, aux='xy', offset=8))
        cubes.append(_make_cube(x, (4, 6), 9, aux='xy', offset=9))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_9y2d_aux_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 6))

    def test_concat_9mix2d_aux_xy(self):
        cubes = []
        cubes.append(_make_cube((0, 2), (0, 2), 1, aux='xy', offset=1))
        cubes.append(_make_cube((2, 4), (2, 4), 2, aux='xy', offset=2))
        cubes.append(_make_cube((4, 6), (4, 6), 3, aux='xy', offset=3))
        cubes.append(_make_cube((4, 6), (0, 2), 4, aux='xy', offset=4))
        cubes.append(_make_cube((0, 2), (4, 6), 5, aux='xy', offset=5))
        cubes.append(_make_cube((0, 2), (2, 4), 6, aux='xy', offset=6))
        cubes.append(_make_cube((2, 4), (0, 2), 7, aux='xy', offset=7))
        cubes.append(_make_cube((4, 6), (2, 4), 8, aux='xy', offset=8))
        cubes.append(_make_cube((2, 4), (4, 6), 9, aux='xy', offset=9))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_9mix2d_aux_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (6, 6))


class TestMulti2DScalar(tests.IrisTest):
    def test_concat_scalar_4x2d_aux_xy(self):
        cubes = iris.cube.CubeList()
        # Level 1.
        y = (0, 2)
        cubes.append(_make_cube((0, 2), y, 1, aux='xy', offset=1, scalar=10))
        cubes.append(_make_cube((2, 4), y, 2, aux='xy', offset=2, scalar=10))
        y = (2, 4)
        cubes.append(_make_cube((0, 2), y, 3, aux='xy', offset=3, scalar=10))
        cubes.append(_make_cube((2, 4), y, 4, aux='xy', offset=4, scalar=10))
        # Level 2.
        y = (0, 2)
        cubes.append(_make_cube((0, 2), y, 5, aux='xy', offset=1, scalar=20))
        cubes.append(_make_cube((2, 4), y, 6, aux='xy', offset=2, scalar=20))
        y = (2, 4)
        cubes.append(_make_cube((0, 2), y, 7, aux='xy', offset=3, scalar=20))
        cubes.append(_make_cube((2, 4), y, 8, aux='xy', offset=4, scalar=20))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate',
                                'concat_scalar_4x2d_aux_xy.cml'))
        self.assertEqual(len(result), 2)
        for cube in result:
            self.assertEqual(cube.shape, (4, 4))

        merged = result.merge()
        self.assertCML(merged, ('concatenate',
                                'concat_merged_scalar_4x2d_aux_xy.cml'))
        self.assertEqual(len(merged), 1)
        self.assertEqual(merged[0].shape, (2, 4, 4))

        # Test concatenate and merge are commutative operations.
        merged = cubes.merge()
        self.assertCML(merged, ('concatenate',
                                'concat_pre_merged_scalar_4x2_aux_xy.cml'))
        self.assertEqual(len(merged), 4)

        result = concatenate(merged)
        self.assertCML(result, ('concatenate',
                                'concat_merged_scalar_4x2d_aux_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 4, 4))

    def test_concat_scalar_4y2d_aux_xy(self):
        cubes = iris.cube.CubeList()
        # Level 1.
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 2), 1, aux='xy', offset=1, scalar=10))
        cubes.append(_make_cube(x, (2, 4), 2, aux='xy', offset=2, scalar=10))
        x = (2, 4)
        cubes.append(_make_cube(x, (0, 2), 3, aux='xy', offset=3, scalar=10))
        cubes.append(_make_cube(x, (2, 4), 4, aux='xy', offset=4, scalar=10))
        # Level 2.
        x = (0, 2)
        cubes.append(_make_cube(x, (0, 2), 5, aux='xy', offset=1, scalar=20))
        cubes.append(_make_cube(x, (2, 4), 6, aux='xy', offset=2, scalar=20))
        x = (2, 4)
        cubes.append(_make_cube(x, (0, 2), 7, aux='xy', offset=3, scalar=20))
        cubes.append(_make_cube(x, (2, 4), 8, aux='xy', offset=4, scalar=20))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate',
                                'concat_scalar_4y2d_aux_xy.cml'))
        self.assertEqual(len(result), 2)
        for cube in result:
            self.assertEqual(cube.shape, (4, 4))

        merged = result.merge()
        self.assertCML(merged, ('concatenate',
                                'concat_merged_scalar_4y2d_aux_xy.cml'))
        self.assertEqual(len(merged), 1)
        self.assertEqual(merged[0].shape, (2, 4, 4))

        # Test concatenate and merge are commutative operations.
        merged = cubes.merge()
        self.assertCML(merged, ('concatenate',
                                'concat_pre_merged_scalar_4y2d_aux_xy.cml'))
        self.assertEqual(len(merged), 4)

        result = concatenate(merged)
        self.assertEqual(len(result), 1)
        self.assertCML(result, ('concatenate',
                                'concat_merged_scalar_4y2d_aux_xy.cml'))
        self.assertEqual(result[0].shape, (2, 4, 4))

    def test_concat_scalar_4mix2d_aux_xy(self):
        cubes = iris.cube.CubeList()
        cubes.append(_make_cube((0, 2), (0, 2), 1, aux='xy',
                                offset=1, scalar=10))
        cubes.append(_make_cube((2, 4), (2, 4), 8, aux='xy',
                                offset=4, scalar=20))
        cubes.append(_make_cube((0, 2), (0, 2), 5, aux='xy',
                                offset=1, scalar=20))
        cubes.append(_make_cube((2, 4), (0, 2), 2, aux='xy',
                                offset=2, scalar=10))
        cubes.append(_make_cube((0, 2), (2, 4), 7, aux='xy',
                                offset=3, scalar=20))
        cubes.append(_make_cube((0, 2), (2, 4), 3, aux='xy',
                                offset=3, scalar=10))
        cubes.append(_make_cube((2, 4), (2, 4), 4, aux='xy',
                                offset=4, scalar=10))
        cubes.append(_make_cube((2, 4), (0, 2), 6, aux='xy',
                                offset=2, scalar=20))
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate',
                                'concat_scalar_4mix2d_aux_xy.cml'))
        self.assertEqual(len(result), 2)
        for cube in result:
            self.assertEqual(cube.shape, (4, 4))

        merged = result.merge()
        self.assertCML(merged, ('concatenate',
                                'concat_merged_scalar_4mix2d_aux_xy.cml'))
        self.assertEqual(len(merged), 1)
        self.assertEqual(merged[0].shape, (2, 4, 4))

        # Test concatenate and merge are commutative operations.
        merged = cubes.merge()
        self.assertCML(merged, ('concatenate',
                                'concat_pre_merged_scalar_4mix2d_aux_xy.cml'))
        self.assertEqual(len(merged), 4)

        result = concatenate(merged)
        self.assertCML(result, ('concatenate',
                                'concat_merged_scalar_4mix2d_aux_xy.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (2, 4, 4))


class Test3D(tests.IrisTest):
    def _make_group(self, xoff=0, yoff=0, zoff=0, doff=0):
        xoff *= 4
        yoff *= 4
        zoff *= 4
        doff *= 8
        cubes = []
        cubes.append(_make_cube_3d((0 + xoff, 2 + xoff),
                                   (0 + yoff, 2 + yoff),
                                   (0 + zoff, 2 + zoff),
                                   1 + doff, aux='x,y,z,xy,xz,yz,xyz'))
        cubes.append(_make_cube_3d((2 + xoff, 4 + xoff),
                                   (0 + yoff, 2 + yoff),
                                   (0 + zoff, 2 + zoff),
                                   2 + doff, aux='x,y,z,xy,xz,yz,xyz'))
        cubes.append(_make_cube_3d((0 + xoff, 2 + xoff),
                                   (2 + yoff, 4 + yoff),
                                   (0 + zoff, 2 + zoff),
                                   3 + doff, aux='x,y,z,xy,xz,yz,xyz'))
        cubes.append(_make_cube_3d((2 + xoff, 4 + xoff),
                                   (2 + yoff, 4 + yoff),
                                   (0 + zoff, 2 + zoff),
                                   4 + doff, aux='x,y,z,xy,xz,yz,xyz'))

        cubes.append(_make_cube_3d((0 + xoff, 2 + xoff),
                                   (0 + yoff, 2 + yoff),
                                   (2 + zoff, 4 + zoff),
                                   5 + doff, aux='x,y,z,xy,xz,yz,xyz'))
        cubes.append(_make_cube_3d((2 + xoff, 4 + xoff),
                                   (0 + yoff, 2 + yoff),
                                   (2 + zoff, 4 + zoff),
                                   6 + doff, aux='x,y,z,xy,xz,yz,xyz'))
        cubes.append(_make_cube_3d((0 + xoff, 2 + xoff),
                                   (2 + yoff, 4 + yoff),
                                   (2 + zoff, 4 + zoff),
                                   7 + doff, aux='x,y,z,xy,xz,yz,xyz'))
        cubes.append(_make_cube_3d((2 + xoff, 4 + xoff),
                                   (2 + yoff, 4 + yoff),
                                   (2 + zoff, 4 + zoff),
                                   8 + doff, aux='x,y,z,xy,xz,yz,xyz'))

        return cubes

    def test_concat_3d_simple(self):
        cubes = self._make_group()
        result = concatenate(cubes)
        self.assertCML(result, ('concatenate', 'concat_3d_simple.cml'))
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (4, 4, 4))

    def test_concat_3d_mega(self):
        cubes = []
        cubes.extend(self._make_group(xoff=0, doff=0))
        cubes.extend(self._make_group(xoff=1, doff=1))
        cubes.extend(self._make_group(yoff=1, doff=2))
        cubes.extend(self._make_group(xoff=1, yoff=1, doff=3))

        cubes.extend(self._make_group(xoff=0, zoff=1, doff=4))
        cubes.extend(self._make_group(xoff=1, zoff=1, doff=5))
        cubes.extend(self._make_group(yoff=1, zoff=1, doff=6))
        cubes.extend(self._make_group(xoff=1, yoff=1, zoff=1, doff=7))
        result = concatenate(cubes)

        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (8, 8, 8))


class TestCubeSignatureEquality(tests.IrisTest):
    def test_not_implemented(self):
        class Terry(object):
            pass
        sig = ConcatenateCubeSignature(iris.cube.Cube(0))
        self.assertIs(sig.__eq__(Terry()), NotImplemented)
        self.assertIs(sig.__ne__(Terry()), NotImplemented)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_constraints
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the constrained cube loading mechanism.

"""
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import biggus

import iris
import iris.tests.stock as stock


SN_AIR_POTENTIAL_TEMPERATURE = 'air_potential_temperature'
SN_SPECIFIC_HUMIDITY = 'specific_humidity'


# TODO: Workaround, pending #1262
def workaround_pending_1262(cubes):
    """Reverse the cube if sigma was chosen as a dim_coord."""
    for i, cube in enumerate(cubes):
        ml = cube.coord("model_level_number").points
        if ml[0] > ml[1]:
            cubes[i] = cube[::-1]


class TestSimple(tests.IrisTest):
    slices = iris.cube.CubeList(stock.realistic_4d().slices(['grid_latitude', 'grid_longitude']))

    def test_constraints(self):
        constraint = iris.Constraint(model_level_number=10)
        sub_list = self.slices.extract(constraint)
        self.assertEqual(len(sub_list), 6)

        constraint = iris.Constraint(model_level_number=[10, 22])
        sub_list = self.slices.extract(constraint)
        self.assertEqual(len(sub_list), 2 * 6)

        constraint = iris.Constraint(model_level_number=lambda c: ( c > 30 ) | (c <= 3))
        sub_list = self.slices.extract(constraint)
        self.assertEqual(len(sub_list), 43 * 6)

        constraint = iris.Constraint(coord_values={'model_level_number': lambda c: c > 1000})
        sub_list = self.slices.extract(constraint)
        self.assertEqual(len(sub_list), 0)

        constraint = (iris.Constraint(model_level_number=10) &
                      iris.Constraint(time=347922.))
        sub_list = self.slices.extract(constraint)
        self.assertEqual(len(sub_list), 1)

        constraint = iris.Constraint(SN_AIR_POTENTIAL_TEMPERATURE)
        sub_list = self.slices.extract(constraint)
        self.assertEqual(len(sub_list), 70 * 6)

    def test_mismatched_type(self):
        constraint = iris.Constraint(model_level_number='aardvark')
        sub_list = self.slices.extract(constraint)
        self.assertEqual(len(sub_list), 0)

    def test_cell(self):
        cell = iris.coords.Cell(10)
        constraint = iris.Constraint(model_level_number=cell)
        sub_list = self.slices.extract(constraint)
        self.assertEqual(len(sub_list), 6)

    def test_cell_equal_bounds(self):
        cell = self.slices[0].coord('level_height').cell(0)
        constraint = iris.Constraint(level_height=cell)
        sub_list = self.slices.extract(constraint)
        self.assertEqual(len(sub_list), 6)

    def test_cell_different_bounds(self):
        cell = iris.coords.Cell(10, bound=(9, 11))
        constraint = iris.Constraint(model_level_number=cell)
        sub_list = self.slices.extract(constraint)
        self.assertEqual(len(sub_list), 0)


class TestMixin(object):
    """
    Mix-in class for attributes & utilities common to the "normal" and "strict" test cases.

    """
    def setUp(self):
        self.dec_path = tests.get_data_path(['PP', 'globClim1', 'dec_subset.pp'])
        self.theta_path = tests.get_data_path(['PP', 'globClim1', 'theta.pp'])

        self.humidity = iris.Constraint(SN_SPECIFIC_HUMIDITY)
        self.theta = iris.Constraint(SN_AIR_POTENTIAL_TEMPERATURE)

        # Coord based constraints
        self.level_10 = iris.Constraint(model_level_number=10)
        self.level_22 = iris.Constraint(model_level_number=22)

        # Value based coord constraint
        self.level_30 = iris.Constraint(model_level_number=30)
        self.level_gt_30_le_3 = iris.Constraint(model_level_number=lambda c: ( c > 30 ) | (c <= 3))
        self.invalid_inequality = iris.Constraint(coord_values={'model_level_number': lambda c: c > 1000})

        # bound based coord constraint
        self.level_height_of_model_level_number_10 = iris.Constraint(level_height=1900)
        self.model_level_number_10_22 = iris.Constraint(model_level_number=[10, 22])

        # Invalid constraints
        self.pressure_950 = iris.Constraint(model_level_number=950)

        self.lat_30 = iris.Constraint(latitude=30)
        self.lat_gt_45 = iris.Constraint(latitude=lambda c: c > 45)


class RelaxedConstraintMixin(TestMixin):
    @staticmethod
    def fixup_sigma_to_be_aux(cubes):
        # XXX Fix the cubes such that the sigma coordinate is always an AuxCoord. Pending gh issue #18
        if isinstance(cubes, iris.cube.Cube):
            cubes = [cubes]

        for cube in cubes:
            sigma = cube.coord('sigma')
            sigma = iris.coords.AuxCoord.from_coord(sigma)
            cube.replace_coord(sigma)

    def assertCML(self, cubes, filename):
        filename = "%s_%s.cml" % (filename, self.suffix)
        tests.IrisTest.assertCML(self, cubes, ('constrained_load', filename))

    def load_match(self, files, constraints):
        raise NotImplementedError()  # defined in subclasses

    def test_single_atomic_constraint(self):
        cubes = self.load_match(self.dec_path, self.level_10)
        self.fixup_sigma_to_be_aux(cubes)
        self.assertCML(cubes, 'all_10')

        cubes = self.load_match(self.dec_path, self.theta)
        self.assertCML(cubes, 'theta')

        cubes = self.load_match(self.dec_path, self.model_level_number_10_22)
        self.fixup_sigma_to_be_aux(cubes)
        workaround_pending_1262(cubes)
        self.assertCML(cubes, 'all_ml_10_22')

        # Check that it didn't matter that we provided sets & tuples to the model_level
        for constraint in [iris.Constraint(model_level_number=set([10, 22])), iris.Constraint(model_level_number=tuple([10, 22]))]:
            cubes = self.load_match(self.dec_path, constraint)
            self.fixup_sigma_to_be_aux(cubes)
            workaround_pending_1262(cubes)
            self.assertCML(cubes, 'all_ml_10_22')

    def test_string_standard_name(self):
        cubes = self.load_match(self.dec_path, SN_AIR_POTENTIAL_TEMPERATURE)
        self.assertCML(cubes, 'theta')

        cubes = self.load_match(self.dec_path, [SN_AIR_POTENTIAL_TEMPERATURE])
        self.assertCML(cubes, 'theta')

        cubes = self.load_match(self.dec_path, iris.Constraint(SN_AIR_POTENTIAL_TEMPERATURE))
        self.assertCML(cubes, 'theta')

        cubes = self.load_match(self.dec_path, iris.Constraint(SN_AIR_POTENTIAL_TEMPERATURE, model_level_number=10))
        self.fixup_sigma_to_be_aux(cubes)
        self.assertCML(cubes, 'theta_10')

    def test_latitude_constraint(self):
        cubes = self.load_match(self.theta_path, self.lat_30)
        self.assertCML(cubes, 'theta_lat_30')

        cubes = self.load_match(self.theta_path, self.lat_gt_45)
        self.assertCML(cubes, 'theta_lat_gt_30')

    def test_single_expression_constraint(self):
        cubes = self.load_match(self.theta_path, self.theta & self.level_10)
        self.fixup_sigma_to_be_aux(cubes)
        self.assertCML(cubes, 'theta_10')

        cubes = self.load_match(self.theta_path, self.level_10 & self.theta)
        self.fixup_sigma_to_be_aux(cubes)
        self.assertCML(cubes, 'theta_10')

    def test_dual_atomic_constraint(self):
        cubes = self.load_match(self.dec_path, [self.theta, self.level_10])
        self.fixup_sigma_to_be_aux(cubes)
        self.assertCML(cubes, 'theta_and_all_10')

    def test_dual_repeated_constraint(self):
        cubes = self.load_match(self.dec_path, [self.theta, self.theta])
        self.fixup_sigma_to_be_aux(cubes)
        self.assertCML(cubes, 'theta_and_theta')

    def test_dual_expression_constraint(self):
        cubes = self.load_match(self.dec_path, [self.theta & self.level_10, self.level_gt_30_le_3 & self.theta])
        self.fixup_sigma_to_be_aux(cubes)
        self.assertCML(cubes, 'theta_10_and_theta_level_gt_30_le_3')

    def test_invalid_constraint(self):
        cubes = self.load_match(self.theta_path, self.pressure_950)
        self.assertCML(cubes, 'pressure_950')

        cubes = self.load_match(self.theta_path, self.invalid_inequality)
        self.assertCML(cubes, 'invalid_inequality')

    def test_inequality_constraint(self):
        cubes = self.load_match(self.theta_path, self.level_gt_30_le_3)
        self.assertCML(cubes, 'theta_gt_30_le_3')


class StrictConstraintMixin(RelaxedConstraintMixin):
    def test_single_atomic_constraint(self):
        cubes = self.load_match(self.theta_path, self.theta)
        self.assertCML(cubes, 'theta')

        cubes = self.load_match(self.theta_path, self.level_10)
        self.fixup_sigma_to_be_aux(cubes)
        self.assertCML(cubes, 'theta_10')

    def test_invalid_constraint(self):
        with self.assertRaises(iris.exceptions.ConstraintMismatchError):
            self.load_match(self.theta_path, self.pressure_950)

    def test_dual_atomic_constraint(self):
        cubes = self.load_match(self.dec_path, [self.theta, self.level_10 & self.theta])
        self.fixup_sigma_to_be_aux(cubes)
        self.assertCML(cubes, 'theta_and_theta_10')


@tests.skip_data
class TestCubeLoadConstraint(RelaxedConstraintMixin, tests.IrisTest):
    suffix = 'load_match'

    def load_match(self, files, constraints):
        cubes = iris.load(files, constraints)
        if not isinstance(cubes, iris.cube.CubeList):
            raise Exception("NOT A CUBE LIST! " + str(type(cubes)))
        return cubes


@tests.skip_data
class TestCubeListConstraint(RelaxedConstraintMixin, tests.IrisTest):
    suffix = 'load_match'

    def load_match(self, files, constraints):
        cubes = iris.load(files).extract(constraints)
        if not isinstance(cubes, iris.cube.CubeList):
            raise Exception("NOT A CUBE LIST! " + str(type(cubes)))
        return cubes


@tests.skip_data
class TestCubeListStrictConstraint(StrictConstraintMixin, tests.IrisTest):
    suffix = 'load_strict'

    def load_match(self, files, constraints):
        cubes = iris.load(files).extract_strict(constraints)
        return cubes


@tests.skip_data
class TestCubeExtract(TestMixin, tests.IrisTest):
    def setUp(self):
        TestMixin.setUp(self)
        self.cube = iris.load_cube(self.theta_path)

    def test_attribute_constraint(self):
        # there is no my_attribute attribute on the cube, so ensure it returns None
        cube = self.cube.extract(iris.AttributeConstraint(my_attribute='foobar'))
        self.assertIsNone(cube)

        orig_cube = self.cube
        # add an attribute to the cubes
        orig_cube.attributes['my_attribute'] = 'foobar'

        cube = orig_cube.extract(iris.AttributeConstraint(my_attribute='foobar'))
        self.assertCML(cube, ('constrained_load', 'attribute_constraint.cml'))

        cube = orig_cube.extract(iris.AttributeConstraint(my_attribute='not me'))
        self.assertIsNone(cube)

        cube = orig_cube.extract(iris.AttributeConstraint(my_attribute=lambda val: val.startswith('foo')))
        self.assertCML(cube, ('constrained_load', 'attribute_constraint.cml'))

        cube = orig_cube.extract(iris.AttributeConstraint(my_attribute=lambda val: not val.startswith('foo')))
        self.assertIsNone(cube)

        cube = orig_cube.extract(iris.AttributeConstraint(my_non_existant_attribute='hello world'))
        self.assertIsNone(cube)

    def test_standard_name(self):
        r = iris.Constraint(SN_AIR_POTENTIAL_TEMPERATURE)
        self.assertTrue(self.cube.extract(r).standard_name, SN_AIR_POTENTIAL_TEMPERATURE)

        r = iris.Constraint('wibble')
        self.assertEqual(self.cube.extract(r), None)

    def test_empty_data(self):
        # Ensure that the process of WHERE does not load data if there
        # was empty data to start with...
        cube = self.cube
        self.assertTrue(cube.has_lazy_data())
        cube = self.cube.extract(self.level_10)
        self.assertTrue(cube.has_lazy_data())
        cube = self.cube.extract(self.level_10).extract(self.level_10)
        self.assertTrue(cube.has_lazy_data())

    def test_non_existant_coordinate(self):
        # Check the behaviour when a constraint is given for a coordinate which does not exist/span a dimension
        self.assertEqual(self.cube[0, :, :].extract(self.level_10), None)

        self.assertEqual(self.cube.extract(iris.Constraint(wibble=10)), None)


@tests.skip_data
class TestConstraints(TestMixin, tests.IrisTest):
    def test_constraint_expressions(self):
        rt = repr(self.theta)
        rl10 = repr(self.level_10)

        rt_l10 = repr(self.theta & self.level_10)
        self.assertEqual(rt_l10, "ConstraintCombination(%s, %s, <built-in function __and__>)" % (rt, rl10))

    def test_string_repr(self):
        rt = repr(iris.Constraint(SN_AIR_POTENTIAL_TEMPERATURE))
        self.assertEqual(rt, "Constraint(name='%s')" % SN_AIR_POTENTIAL_TEMPERATURE)

        rt = repr(iris.Constraint(SN_AIR_POTENTIAL_TEMPERATURE, model_level_number=10))
        self.assertEqual(rt, "Constraint(name='%s', coord_values={'model_level_number': 10})" % SN_AIR_POTENTIAL_TEMPERATURE)

    def test_number_of_raw_cubes(self):
        # Test the constraints generate the correct number of raw cubes.
        raw_cubes = iris.load_raw(self.theta_path)
        self.assertEqual(len(raw_cubes), 38)

        raw_cubes = iris.load_raw(self.theta_path, [self.level_10])
        self.assertEqual(len(raw_cubes), 1)

        raw_cubes = iris.load_raw(self.theta_path, [self.theta])
        self.assertEqual(len(raw_cubes), 38)

        raw_cubes = iris.load_raw(self.dec_path, [self.level_30])
        self.assertEqual(len(raw_cubes), 4)

        raw_cubes = iris.load_raw(self.dec_path, [self.theta])
        self.assertEqual(len(raw_cubes), 38)


class TestBetween(tests.IrisTest):
    def run_test(self, function, numbers, results):
        for number, result in zip(numbers, results):
            self.assertEqual(function(number), result)

    def test_le_ge(self):
        function = iris.util.between(2, 4)
        numbers = [1, 2, 3, 4, 5]
        results = [False, True, True, True, False]
        self.run_test(function, numbers, results)

    def test_lt_gt(self):
        function = iris.util.between(2, 4, rh_inclusive=False, lh_inclusive=False)
        numbers = [1, 2, 3, 4, 5]
        results = [False, False, True, False, False]
        self.run_test(function, numbers, results)

    def test_le_gt(self):
        function = iris.util.between(2, 4, rh_inclusive=False)
        numbers = [1, 2, 3, 4, 5]
        results = [False, True, True, False, False]
        self.run_test(function, numbers, results)

    def test_lt_ge(self):
        function = iris.util.between(2, 4, lh_inclusive=False)
        numbers = [1, 2, 3, 4, 5]
        results = [False, False, True, True, False]
        self.run_test(function, numbers, results)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_coordsystem
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


from __future__ import division

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import logging

import cartopy.crs
import numpy as np

import iris.cube
import iris.coords
import iris.tests.stock
import iris.unit

from iris.coord_systems import *


logger = logging.getLogger('tests')


def osgb():
    return TransverseMercator(latitude_of_projection_origin=49, longitude_of_central_meridian=-2,
                              false_easting=-400, false_northing=100,
                              scale_factor_at_central_meridian=0.9996012717,
                              ellipsoid=GeogCS(6377563.396, 6356256.909))


class TestCoordSystemLookup(tests.IrisTest):
    def setUp(self):
        self.cube = iris.tests.stock.lat_lon_cube()

    def test_hit_name(self):
        self.assertIsInstance(self.cube.coord_system('GeogCS'),
                              GeogCS)

    def test_hit_type(self):
        self.assertIsInstance(self.cube.coord_system(GeogCS),
                              GeogCS)

    def test_miss(self):
        self.assertIsNone(self.cube.coord_system(RotatedGeogCS))

    def test_empty(self):
        self.assertIsInstance(self.cube.coord_system(GeogCS),
                              GeogCS)
        self.assertIsNotNone(self.cube.coord_system(None))
        self.assertIsInstance(self.cube.coord_system(None),
                              GeogCS)
        self.assertIsNotNone(self.cube.coord_system())
        self.assertIsInstance(self.cube.coord_system(),
                              GeogCS)

        for coord in self.cube.coords():
            coord.coord_system = None

        self.assertIsNone(self.cube.coord_system(GeogCS))
        self.assertIsNone(self.cube.coord_system(None))
        self.assertIsNone(self.cube.coord_system())


class TestCoordSystemSame(tests.IrisTest):

    def setUp(self):
        self.cs1 = iris.coord_systems.GeogCS(6371229)
        self.cs2 = iris.coord_systems.GeogCS(6371229)
        self.cs3 = iris.coord_systems.RotatedGeogCS(30, 30, ellipsoid=GeogCS(6371229))

    def test_simple(self):
        a = self.cs1
        b = self.cs2
        self.assertEquals(a, b)

    def test_different_class(self):
        a = self.cs1
        b = self.cs3
        self.assertNotEquals(a, b)

    def test_different_public_attributes(self):
        a = self.cs1
        b = self.cs2
        a.foo = 'a'

        # check that that attribute was added (just in case)
        self.assertEqual(a.foo, 'a')

        # a and b should not be the same
        self.assertNotEquals(a, b)

        # a and b should be the same
        b.foo = 'a'
        self.assertEquals(a, b)

        b.foo = 'b'
        # a and b should not be the same
        self.assertNotEquals(a, b)


class Test_CoordSystem_xml_element(tests.IrisTest):
    def test_rotated(self):
        cs = RotatedGeogCS(30, 40, ellipsoid=GeogCS(6371229))
        self.assertXMLElement(cs, ("coord_systems", "CoordSystem_xml_element.xml"))


class Test_GeogCS_construction(tests.IrisTest):
    # Test Ellipsoid constructor
    # Don't care about testing the units, it has no logic specific to this class.

    def test_sphere_param(self):
        cs = GeogCS(6543210)
        self.assertXMLElement(cs, ("coord_systems", "GeogCS_init_sphere.xml"))

    def test_no_major(self):
        cs = GeogCS(semi_minor_axis=6500000, inverse_flattening=151.42814163388104)
        self.assertXMLElement(cs, ("coord_systems", "GeogCS_init_no_major.xml"))

    def test_no_minor(self):
        cs = GeogCS(semi_major_axis=6543210, inverse_flattening=151.42814163388104)
        self.assertXMLElement(cs, ("coord_systems", "GeogCS_init_no_minor.xml"))

    def test_no_invf(self):
        cs = GeogCS(semi_major_axis=6543210, semi_minor_axis=6500000)
        self.assertXMLElement(cs, ("coord_systems", "GeogCS_init_no_invf.xml"))

    def test_invalid_ellipsoid_params(self):
        # no params
        with self.assertRaises(ValueError):
            GeogCS()

        # over specified
        with self.assertRaises(ValueError):
            GeogCS(6543210, 6500000, 151.42814163388104)

        # under specified
        with self.assertRaises(ValueError):
            GeogCS(None, 6500000, None)
        with self.assertRaises(ValueError):
            GeogCS(None, None, 151.42814163388104)


class Test_GeogCS_repr(tests.IrisTest):
    def test_repr(self):
        cs = GeogCS(6543210, 6500000)
        expected = "GeogCS(semi_major_axis=6543210.0, semi_minor_axis=6500000.0)"
        self.assertEqual(expected, repr(cs))

class Test_GeogCS_str(tests.IrisTest):
    def test_str(self):
        cs = GeogCS(6543210, 6500000)
        expected = "GeogCS(semi_major_axis=6543210.0, semi_minor_axis=6500000.0)"
        self.assertEqual(expected, str(cs))


class Test_GeogCS_as_cartopy_globe(tests.IrisTest):
    def test_as_cartopy_globe(self):
        cs = GeogCS(6543210, 6500000)
        # Can't check equality directly, so use the proj4 params instead.
        res = cs.as_cartopy_globe().to_proj4_params()
        expected = {'a': 6543210, 'b': 6500000}
        self.assertEqual(res, expected)


class Test_GeogCS_as_cartopy_crs(tests.IrisTest):
    def test_as_cartopy_crs(self):
        cs = GeogCS(6543210, 6500000)
        res = cs.as_cartopy_crs()
        globe = cartopy.crs.Globe(semimajor_axis=6543210.0,
                                  semiminor_axis=6500000.0, ellipse=None)
        expected = cartopy.crs.Geodetic(globe)
        self.assertEqual(res, expected)


class Test_RotatedGeogCS_construction(tests.IrisTest):
    def test_init(self):
        rcs = RotatedGeogCS(30, 40, north_pole_grid_longitude=50, ellipsoid=GeogCS(6371229))
        self.assertXMLElement(rcs, ("coord_systems", "RotatedGeogCS_init.xml"))

        rcs = RotatedGeogCS(30, 40, north_pole_grid_longitude=50)
        self.assertXMLElement(rcs, ("coord_systems", "RotatedGeogCS_init_a.xml"))

        rcs = RotatedGeogCS(30, 40)
        self.assertXMLElement(rcs, ("coord_systems", "RotatedGeogCS_init_b.xml"))


class Test_RotatedGeogCS_repr(tests.IrisTest):
    def test_repr(self):
        rcs = RotatedGeogCS(30, 40, north_pole_grid_longitude=50, ellipsoid=GeogCS(6371229))
        expected = "RotatedGeogCS(30.0, 40.0, "\
                    "north_pole_grid_longitude=50.0, ellipsoid=GeogCS(6371229.0))"
        self.assertEqual(expected, repr(rcs))

        rcs = RotatedGeogCS(30, 40, north_pole_grid_longitude=50)
        expected = "RotatedGeogCS(30.0, 40.0, north_pole_grid_longitude=50.0)"
        self.assertEqual(expected, repr(rcs))

        rcs = RotatedGeogCS(30, 40)
        expected = "RotatedGeogCS(30.0, 40.0)"
        self.assertEqual(expected, repr(rcs))


class Test_RotatedGeogCS_str(tests.IrisTest):
    def test_str(self):
        rcs = RotatedGeogCS(30, 40, north_pole_grid_longitude=50, ellipsoid=GeogCS(6371229))
        expected = "RotatedGeogCS(30.0, 40.0, "\
                    "north_pole_grid_longitude=50.0, ellipsoid=GeogCS(6371229.0))"
        self.assertEqual(expected, str(rcs))

        rcs = RotatedGeogCS(30, 40, north_pole_grid_longitude=50)
        expected = "RotatedGeogCS(30.0, 40.0, north_pole_grid_longitude=50.0)"
        self.assertEqual(expected, str(rcs))

        rcs = RotatedGeogCS(30, 40)
        expected = "RotatedGeogCS(30.0, 40.0)"
        self.assertEqual(expected, str(rcs))


class Test_TransverseMercator_construction(tests.IrisTest):
    def test_osgb(self):
        tm = osgb()
        self.assertXMLElement(tm, ("coord_systems", "TransverseMercator_osgb.xml"))


class Test_TransverseMercator_repr(tests.IrisTest):
    def test_osgb(self):
        tm = osgb()
        expected = "TransverseMercator(latitude_of_projection_origin=49.0, longitude_of_central_meridian=-2.0, "\
                    "false_easting=-400.0, false_northing=100.0, scale_factor_at_central_meridian=0.9996012717, "\
                    "ellipsoid=GeogCS(semi_major_axis=6377563.396, semi_minor_axis=6356256.909))"
        self.assertEqual(expected, repr(tm))


class Test_TransverseMercator_as_cartopy_crs(tests.IrisTest):
    def test_as_cartopy_crs(self):
        latitude_of_projection_origin = 49.0
        longitude_of_central_meridian = -2.0
        false_easting = -40000.0
        false_northing = 10000.0
        scale_factor_at_central_meridian = 0.9996012717
        ellipsoid = GeogCS(semi_major_axis=6377563.396,
                           semi_minor_axis=6356256.909)

        tmerc_cs = TransverseMercator(
            latitude_of_projection_origin,
            longitude_of_central_meridian,
            false_easting,
            false_northing,
            scale_factor_at_central_meridian,
            ellipsoid=ellipsoid)

        expected = cartopy.crs.TransverseMercator(
            central_longitude=longitude_of_central_meridian,
            central_latitude=latitude_of_projection_origin,
            false_easting=false_easting,
            false_northing=false_northing,
            scale_factor=scale_factor_at_central_meridian,
            globe=cartopy.crs.Globe(semimajor_axis=6377563.396,
                                    semiminor_axis=6356256.909, ellipse=None))

        res = tmerc_cs.as_cartopy_crs()
        self.assertEqual(res, expected)


class Test_TransverseMercator_as_cartopy_projection(tests.IrisTest):
    def test_as_cartopy_projection(self):
        latitude_of_projection_origin = 49.0
        longitude_of_central_meridian = -2.0
        false_easting = -40000.0
        false_northing = 10000.0
        scale_factor_at_central_meridian = 0.9996012717
        ellipsoid = GeogCS(semi_major_axis=6377563.396,
                           semi_minor_axis=6356256.909)

        tmerc_cs = TransverseMercator(
            latitude_of_projection_origin,
            longitude_of_central_meridian,
            false_easting,
            false_northing,
            scale_factor_at_central_meridian,
            ellipsoid=ellipsoid)

        expected = cartopy.crs.TransverseMercator(
            central_longitude=longitude_of_central_meridian,
            central_latitude=latitude_of_projection_origin,
            false_easting=false_easting,
            false_northing=false_northing,
            scale_factor=scale_factor_at_central_meridian,
            globe=cartopy.crs.Globe(semimajor_axis=6377563.396,
                                    semiminor_axis=6356256.909, ellipse=None))

        res = tmerc_cs.as_cartopy_projection()
        self.assertEqual(res, expected)


class Test_LambertConformal(tests.GraphicsTest):

    def test_north_cutoff(self):
        lcc = LambertConformal(0, 0, secant_latitudes=(30, 60))
        ccrs = lcc.as_cartopy_crs()
        self.assertEqual(ccrs.cutoff, -30)

    def test_south_cutoff(self):
        lcc = LambertConformal(0, 0, secant_latitudes=(-30, -60))
        ccrs = lcc.as_cartopy_crs()
        self.assertEqual(ccrs.cutoff, 30)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_coord_api
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


from __future__ import division

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import unittest
from xml.dom.minidom import Document
import logging

import numpy as np

import iris
import iris.aux_factory
import iris.coord_systems
import iris.coords
import iris.exceptions
import iris.unit
import iris.tests.stock


logger = logging.getLogger('tests')


class TestLazy(unittest.TestCase):
    def setUp(self):
        # Start with a coord with LazyArray points.
        shape = (3, 4)
        point_func = lambda: np.arange(12).reshape(shape)
        points = iris.aux_factory.LazyArray(shape, point_func)
        self.coord = iris.coords.AuxCoord(points=points)

    def _check_lazy(self, coord):
        self.assertIsInstance(self.coord._points, iris.aux_factory.LazyArray)
        self.assertIsNone(self.coord._points._array)

    def test_nop(self):
        self._check_lazy(self.coord)

    def _check_both_lazy(self, new_coord):
        # Make sure both coords have an "empty" LazyArray.
        self._check_lazy(self.coord)
        self._check_lazy(new_coord)

    def test_lazy_slice1(self):
        self._check_both_lazy(self.coord[:])

    def test_lazy_slice2(self):
        self._check_both_lazy(self.coord[:, :])

    def test_lazy_slice3(self):
        self._check_both_lazy(self.coord[...])

    def _check_concrete(self, new_coord):
        # Taking a genuine subset slice should trigger the evaluation
        # of the original LazyArray, and result in a normal ndarray for
        # the new coord.
        self.assertIsInstance(self.coord._points, iris.aux_factory.LazyArray)
        self.assertIsInstance(self.coord._points._array, np.ndarray)
        self.assertIsInstance(new_coord._points, np.ndarray)

    def test_concrete_slice1(self):
        self._check_concrete(self.coord[0])

    def test_concrete_slice2(self):
        self._check_concrete(self.coord[0, :])

    def test_shape(self):
        # Checking the shape shouldn't trigger a lazy load.
        self.assertEqual(self.coord.shape, (3, 4))
        self._check_lazy(self.coord)

    def _check_shared_data(self, coord):
        # Updating the original coord's points should update the sliced
        # coord's points too.
        points = coord.points
        new_points = coord[:].points
        np.testing.assert_array_equal(points, new_points)
        points[0, 0] = 999
        self.assertEqual(points[0, 0], new_points[0, 0])

    def test_concrete_shared_data(self):
        coord = iris.coords.AuxCoord(np.arange(12).reshape((3, 4)))
        self._check_shared_data(coord)

    def test_lazy_shared_data(self):
        self._check_shared_data(self.coord)


class TestCoordSlicing(unittest.TestCase):
    def setUp(self):
        cube = iris.tests.stock.realistic_4d()
        self.lat = cube.coord('grid_latitude')
        self.surface_altitude = cube.coord('surface_altitude')
        
    def test_slice_copy(self):
        a = self.lat
        b = a.copy()
        self.assertEqual(a, b)
        self.assertFalse(a is b)
        
        a = self.lat
        b = a[:]
        self.assertEqual(a, b)
        self.assertFalse(a is b)
        
    def test_slice_multiple_indices(self):
        aux_lat = iris.coords.AuxCoord.from_coord(self.lat)
        aux_sliced = aux_lat[(3, 4), :]
        dim_sliced   = self.lat[(3, 4), :]
        
        self.assertEqual(dim_sliced, aux_sliced)

    def test_slice_reverse(self):
        b = self.lat[::-1]
        np.testing.assert_array_equal(b.points, self.lat.points[::-1])
        np.testing.assert_array_equal(b.bounds, self.lat.bounds[::-1, :])
        
        c = b[::-1]
        self.assertEqual(self.lat, c)
        
    def test_multidim(self):
        a = self.surface_altitude
        # make some arbitrary bounds
        bound_shape = a.shape + (2,)
        a.bounds = np.arange(np.prod(bound_shape)).reshape(bound_shape)
        b = a[(0, 2), (0, -1)]
        np.testing.assert_array_equal(b.points, a.points[(0, 2), :][:, (0, -1)])
        np.testing.assert_array_equal(b.bounds, a.bounds[(0, 2), :, :][:, (0, -1), :])


class TestCoordIntersection(tests.IrisTest):
    def setUp(self):
        self.a = iris.coords.DimCoord(np.arange(9., dtype=np.float32) * 3 + 9., long_name='foo', units='meter')# 0.75)
        self.a.guess_bounds(0.75)
        pts = np.array([  3.,   6.,   9.,  12.,  15.,  18.,  21.,  24.,  27.,  30.], dtype=np.float32)
        bnds = np.array([[  0.75,   3.75],
           [  3.75,   6.75],
           [  6.75,   9.75],
           [  9.75,  12.75],
           [ 12.75,  15.75],
           [ 15.75,  18.75],
           [ 18.75,  21.75],
           [ 21.75,  24.75],
           [ 24.75,  27.75],
           [ 27.75,  30.75]], dtype=np.float32)
        self.b = iris.coords.AuxCoord(pts, long_name='foo', units='meter', bounds=bnds)
    
    def test_basic_intersection(self):
        inds = self.a.intersect(self.b, return_indices=True)
        self.assertEqual((0, 1, 2, 3, 4, 5, 6, 7), tuple(inds))
            
        c = self.a.intersect(self.b)
        self.assertXMLElement(c, ('coord_api', 'intersection.xml'))
    
    def test_intersection_reverse(self):
        inds = self.a.intersect(self.b[::-1], return_indices=True)    
        self.assertEqual((7, 6, 5, 4, 3, 2, 1, 0), tuple(inds))
        
        c = self.a.intersect(self.b[::-1])
        self.assertXMLElement(c, ('coord_api', 'intersection_reversed.xml'))
    
    def test_no_intersection_on_points(self):    
        # Coordinates which do not share common points but with common bounds should fail
        self.a.points = self.a.points + 200
        self.assertRaises(ValueError, self.a.intersect, self.b)
        
    def test_intersection_one_fewer_upper_bound_than_lower(self):
        self.b.bounds[4, 1] = self.b.bounds[0, 1]        
        c = self.a.intersect(self.b)
        self.assertXMLElement(c, ('coord_api', 'intersection_missing.xml'))
        
    def test_no_intersection_on_bounds(self):        
        # Coordinates which do not share common bounds but with common points should fail
        self.a.bounds = None
        a = self.a.copy()
        a.bounds = None
        a.guess_bounds(bound_position=0.25)
        self.assertRaises(ValueError, a.intersect, self.b)
    
    def test_no_intersection_on_name(self):
        # Coordinates which do not share the same name should fail
        self.a.long_name = 'foobar'
        self.assertRaises(ValueError, self.a.intersect, self.b)
        
    def test_no_intersection_on_unit(self):
        # Coordinates which do not share the same unit should fail
        self.a.units = 'kilometer'
        self.assertRaises(ValueError, self.a.intersect, self.b)

    def test_commutative(self):
        cube = iris.tests.stock.realistic_4d()
        coord = cube.coord('grid_longitude')
        offset_coord = coord.copy()
        offset_coord = offset_coord - (offset_coord.points[20] - offset_coord.points[0])
        self.assertEqual(coord.intersect(offset_coord), offset_coord.intersect(coord))


class TestXML(tests.IrisTest):
    def test_minimal(self):
        coord = iris.coords.DimCoord(np.arange(10, dtype=np.int32))
        element = coord.xml_element(Document())
        self.assertXMLElement(coord, ('coord_api', 'minimal.xml'))

    def test_complex(self):
        crs = iris.coord_systems.GeogCS(6370000)
        coord = iris.coords.AuxCoord(np.arange(4, dtype=np.float32),
                                     'air_temperature', 'my_long_name',
                                     units='K',
                                     attributes={'foo': 'bar', 'count': 2},
                                     coord_system=crs)
        coord.guess_bounds(0.5)
        self.assertXMLElement(coord, ('coord_api', 'complex.xml'))


class TestCoord_ReprStr_nontime(tests.IrisTest):
    def setUp(self):
        self.lat = iris.tests.stock.realistic_4d().coord('grid_latitude')[:10]

    def test_DimCoord_repr(self):
        self.assertRepr(self.lat,
                        ('coord_api', 'str_repr', 'dim_nontime_repr.txt'))

    def test_AuxCoord_repr(self):
        self.assertRepr(self.lat,
                        ('coord_api', 'str_repr', 'aux_nontime_repr.txt'))

    def test_DimCoord_str(self):
        self.assertString(str(self.lat),
                          ('coord_api', 'str_repr', 'dim_nontime_str.txt'))

    def test_AuxCoord_str(self):
        self.assertString(str(self.lat),
                          ('coord_api', 'str_repr', 'aux_nontime_str.txt'))


class TestCoord_ReprStr_time(tests.IrisTest):
    def setUp(self):
        self.time = iris.tests.stock.realistic_4d().coord('time')
        
    def test_DimCoord_repr(self):
        self.assertRepr(self.time,
                        ('coord_api', 'str_repr', 'dim_time_repr.txt'))

    def test_AuxCoord_repr(self):
        self.assertRepr(self.time,
                        ('coord_api', 'str_repr', 'aux_time_repr.txt'))

    def test_DimCoord_str(self):
        self.assertString(str(self.time),
                          ('coord_api', 'str_repr', 'dim_time_str.txt'))

    def test_AuxCoord_str(self):
        self.assertString(str(self.time),
                          ('coord_api', 'str_repr', 'aux_time_str.txt'))


class TestAuxCoordCreation(unittest.TestCase):
    def test_basic(self):
        a = iris.coords.AuxCoord(range(10), 'air_temperature', units='kelvin')
        result = "AuxCoord(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), standard_name='air_temperature', units=Unit('kelvin'))"
        self.assertEqual(result, str(a))

        b = iris.coords.AuxCoord(range(10), attributes={'monty': 'python'})
        result = "AuxCoord(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), standard_name=None, units=Unit('1'), attributes={'monty': 'python'})"
        self.assertEqual(result, str(b))
        
    def test_excluded_attributes(self):
        with self.assertRaises(ValueError):
            iris.coords.AuxCoord(range(10), 'air_temperature', units='kelvin', attributes={'standard_name': 'whoopsy'})
        
        a = iris.coords.AuxCoord(range(10), 'air_temperature', units='kelvin')
        with self.assertRaises(ValueError):
            a.attributes['standard_name'] = 'whoopsy'
        with self.assertRaises(ValueError):
            a.attributes.update({'standard_name': 'whoopsy'})

    def test_coord_system(self):
        a = iris.coords.AuxCoord(range(10), 'air_temperature', units='kelvin', coord_system=iris.coord_systems.GeogCS(6000))
        result = "AuxCoord(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), standard_name='air_temperature', units=Unit('kelvin'), "\
                 "coord_system=GeogCS(6000.0))"
        self.assertEqual(result, str(a))
        
    def test_bounded(self):
        a = iris.coords.AuxCoord(range(10), 'air_temperature', units='kelvin', bounds=np.arange(0, 20).reshape(10, 2))
        result = ("AuxCoord(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
                  ", bounds=array([[ 0,  1],\n       [ 2,  3],\n       [ 4,  5],\n       [ 6,  7],\n       [ 8,  9],\n       "\
                  "[10, 11],\n       [12, 13],\n       [14, 15],\n       [16, 17],\n       [18, 19]])"
                  ", standard_name='air_temperature', units=Unit('kelvin'))"
                  )
        self.assertEqual(result, str(a))
        
    def test_string_coord_equality(self):
        b = iris.coords.AuxCoord(['Jan', 'Feb', 'March'], units='no_unit')
        c = iris.coords.AuxCoord(['Jan', 'Feb', 'March'], units='no_unit')
        self.assertEqual(b, c)

    def test_AuxCoord_fromcoord(self):
        # Check the coordinate returned by `from_coord` doesn't reference the
        # same coordinate system as the source coordinate.
        crs = iris.coord_systems.GeogCS(6370000)
        a = iris.coords.DimCoord(10, coord_system=crs)
        b = iris.coords.AuxCoord.from_coord(a)
        self.assertIsNot(a.coord_system, b.coord_system)
  
  
class TestDimCoordCreation(unittest.TestCase):
    def test_basic(self):
        a = iris.coords.DimCoord(range(10), 'air_temperature', units='kelvin')
        result = "DimCoord(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), standard_name='air_temperature', units=Unit('kelvin'))"
        self.assertEqual(result, str(a))

        b = iris.coords.DimCoord(range(10), attributes={'monty': 'python'})
        result = "DimCoord(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), standard_name=None, units=Unit('1'), attributes={'monty': 'python'})"
        self.assertEqual(result, str(b))
        
    def test_excluded_attributes(self):
        with self.assertRaises(ValueError):
            iris.coords.DimCoord(range(10), 'air_temperature', units='kelvin', attributes={'standard_name': 'whoopsy'})
        
        a = iris.coords.DimCoord(range(10), 'air_temperature', units='kelvin')
        with self.assertRaises(ValueError):
            a.attributes['standard_name'] = 'whoopsy'
        with self.assertRaises(ValueError):
            a.attributes.update({'standard_name': 'whoopsy'})

    def test_coord_system(self):
        a = iris.coords.DimCoord(range(10), 'air_temperature', units='kelvin', coord_system=iris.coord_systems.GeogCS(6000))
        result = "DimCoord(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), standard_name='air_temperature', units=Unit('kelvin'), "\
                 "coord_system=GeogCS(6000.0))"
        self.assertEqual(result, str(a))
        
    def test_bounded(self):
        a = iris.coords.DimCoord(range(10), 'air_temperature', units='kelvin', bounds=np.arange(0, 20).reshape(10, 2))
        result = ("DimCoord(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
                  ", bounds=array([[ 0,  1],\n       [ 2,  3],\n       [ 4,  5],\n       [ 6,  7],\n       [ 8,  9],\n       "\
                  "[10, 11],\n       [12, 13],\n       [14, 15],\n       [16, 17],\n       [18, 19]])"
                  ", standard_name='air_temperature', units=Unit('kelvin'))"
                  )
        self.assertEqual(result, str(a))      
        
    def test_dim_coord_restrictions(self):
        # 1d
        with self.assertRaisesRegexp(ValueError, 'must be 1-dim'):
            iris.coords.DimCoord([[1,2,3], [4,5,6]]) 
        # monotonic
        with self.assertRaisesRegexp(ValueError, 'must be strictly monotonic'):
            iris.coords.DimCoord([1,2,99,4,5]) 
        # monotonic bounds
        with self.assertRaisesRegexp(ValueError, 
                                     'monotonicity.*consistent.*all bounds'):
            iris.coords.DimCoord([1,2,3], bounds=[[1, 12], [2, 9], [3, 6]])
        # shapes of points and bounds
        with self.assertRaisesRegexp(ValueError, 'shape of the bounds array'):
            iris.coords.DimCoord([1,2,3], bounds=[0.5, 1.5, 2.5, 3.5])
        # another example of shapes of points and bounds
        with self.assertRaisesRegexp(ValueError, 'shape of the bounds array'):
            iris.coords.DimCoord([1,2,3], bounds=[[0.5, 1.5],[1.5, 2.5]])

        # numeric
        with self.assertRaises(ValueError):
            iris.coords.DimCoord(['Jan', 'Feb', 'March'])
            
    def test_DimCoord_equality(self):
        # basic regular coord
        b = iris.coords.DimCoord([1, 2]) 
        c = iris.coords.DimCoord([1, 2.]) 
        d = iris.coords.DimCoord([1, 2], circular=True)
        self.assertEqual(b, c)
        self.assertNotEqual(b, d)
        
    def test_Dim_to_Aux(self):
        a = iris.coords.DimCoord(range(10), standard_name='air_temperature', long_name='custom air temp',
                                 units='kelvin', attributes={'monty': 'python'}, 
                                 bounds=np.arange(20).reshape(10, 2), circular=True)
        b = iris.coords.AuxCoord.from_coord(a)
        # Note - circular attribute is not a factor in equality comparison
        self.assertEqual(a, b)

    def test_DimCoord_fromcoord(self):
        # Check the coordinate returned by `from_coord` doesn't reference the
        # same coordinate system as the source coordinate.
        crs = iris.coord_systems.GeogCS(6370000)
        a = iris.coords.AuxCoord(10, coord_system=crs)
        b = iris.coords.DimCoord.from_coord(a)
        self.assertIsNot(a.coord_system, b.coord_system)

    def test_DimCoord_from_regular(self):
        zeroth = 10.0
        step = 20.0
        count = 100
        kwargs = dict(standard_name='latitude',
                      long_name='latitude',
                      var_name='lat',
                      units='degrees',
                      attributes=dict(fruit='pear'),
                      coord_system=iris.coord_systems.GeogCS(6371229),
                      circular=False)

        coord = iris.coords.DimCoord.from_regular(zeroth, step, count,
                                                  **kwargs)
        expected_points = np.arange(zeroth + step,
                                    zeroth + (count + 1)*step,
                                    step)
        expected = iris.coords.DimCoord(expected_points, **kwargs)
        self.assertIsInstance(coord, iris.coords.DimCoord)
        self.assertEqual(coord, expected)

    def test_DimCoord_from_regular_with_bounds(self):
        zeroth = 3.0
        step = 0.5
        count = 20
        kwargs = dict(standard_name='latitude',
                      long_name='latitude',
                      var_name='lat',
                      units='degrees',
                      attributes=dict(fruit='pear'),
                      coord_system=iris.coord_systems.GeogCS(6371229),
                      circular=False)

        coord = iris.coords.DimCoord.from_regular(zeroth, step, count,
                                                  with_bounds=True, **kwargs)
        expected_points = np.arange(zeroth + step,
                                    zeroth + (count + 1)*step,
                                    step)
        expected_bounds = np.transpose([expected_points - 0.5 * step,
                                        expected_points + 0.5 * step])
        expected = iris.coords.DimCoord(expected_points,
                                        bounds=expected_bounds, **kwargs)
        self.assertIsInstance(coord, iris.coords.DimCoord)
        self.assertEqual(coord, expected)


class TestCoordMaths(tests.IrisTest):
    def _build_coord(self, start=None, step=None, count=None):
        # Create points and bounds akin to an old RegularCoord.
        dtype = np.float32
        start = dtype(start or self.start)
        step = dtype(step or self.step)
        count = int(count or self.count)
        bound_position = dtype(0.5)
        points = np.arange(count, dtype=dtype) * step + start
        bounds = np.concatenate([[points - bound_position * step], 
                                    [points + (1 - bound_position) * step]]).T
        self.lon = iris.coords.AuxCoord(points, 'latitude',  units='degrees', bounds=bounds)
        self.rlon = iris.coords.AuxCoord(np.deg2rad(points), 'latitude',  units='radians', bounds=np.deg2rad(bounds))

    def setUp(self):
        self.start = 0
        self.step = 2.3
        self.count = 20
        self._build_coord()


class TestCoordAdditionSubtract(TestCoordMaths):
    def test_subtract(self):
        r_expl = self.lon - 10
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'subtract_simple_expl.xml'))
        
    def test_subtract_in_place(self):
        r_expl = self.lon.copy()
        r_expl -= 10
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'subtract_simple_expl.xml'))
        
    def test_neg(self):
        self._build_coord(start=8)
        r_expl = -self.lon
        np.testing.assert_array_equal(r_expl.points, -(self.lon.points))
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'negate_expl.xml'))
        
    def test_right_subtract(self):
        r_expl = 10 - self.lon
        # XXX original xml was for regular case, not explicit.
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'r_subtract_simple_exl.xml'))
        
    def test_add(self):
        r_expl = self.lon + 10
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'add_simple_expl.xml'))
        
    def test_add_in_place(self):
        r_expl = self.lon.copy()
        r_expl += 10
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'add_simple_expl.xml'))
        
    def test_add_float(self):
        r_expl = self.lon + 10.321
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'add_float_expl.xml'))
        self.assertEqual(r_expl, 10.321 + self.lon.copy() )
        
        
class TestCoordMultDivide(TestCoordMaths):
    def test_divide(self):
        r_expl = self.lon.copy() / 10
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'divide_simple_expl.xml'))
        
    def test_right_divide(self):
        self._build_coord(start=10)
        test_coord = self.lon.copy()
        
        r_expl = 1 / test_coord
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'right_divide_simple_expl.xml'))

    def test_divide_in_place(self):
        r_expl = self.lon.copy()
        r_expl /= 10
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'divide_simple_expl.xml'))
        
    def test_multiply(self):
        r_expl = self.lon.copy() * 10
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'multiply_simple_expl.xml'))
        
    def test_multiply_in_place_reg(self):
        r_expl = self.lon.copy()
        r_expl *= 10
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'multiply_simple_expl.xml'))
        
    def test_multiply_float(self):
        r_expl = self.lon.copy() * 10.321
        self.assertXMLElement(r_expl, ('coord_api', 'coord_maths', 'mult_float_expl.xml'))
        self.assertEqual(r_expl, 10.321 * self.lon.copy() )
        

class TestCoordCollapsed(tests.IrisTest):
    def create_1d_coord(self, bounds=None, points=None, units='meter'):
        coord = iris.coords.DimCoord(points, long_name='test', units=units, 
                                     bounds=bounds)
        return coord
        
    def test_explicit(self):
        orig_coord = self.create_1d_coord(points=range(10), 
                                          bounds=[(b, b+1) for b in range(10)])
        coord_expected = self.create_1d_coord(points=5, bounds=[(0, 10)])

        # test points & bounds
        self.assertEqual(coord_expected, orig_coord.collapsed())
        
        # test points only
        coord = orig_coord.copy()
        coord_expected = self.create_1d_coord(points=4, bounds=[(0, 9)])
        coord.bounds = None
        self.assertEqual(coord_expected, coord.collapsed())        

    def test_circular_collapse(self):
        # set up a coordinate that wraps 360 degrees in points using the circular flag
        coord = self.create_1d_coord(None, np.arange(10) * 36, 'degrees')
        expected_coord = self.create_1d_coord([0., 360.], [180.], 'degrees')
        coord.circular = True
        
        # test collapsing
        self.assertEqual(expected_coord, coord.collapsed())
        # the order of the points/bounds should not affect the resultant bounded coordinate 
        coord = coord[::-1]
        self.assertEqual(expected_coord, coord.collapsed())
        
    def test_nd_bounds(self):
        cube = iris.tests.stock.simple_2d_w_multidim_coords(with_bounds=True)
        pcube = cube.collapsed(['bar','foo'], iris.analysis.SUM)
        pcube.data = pcube.data.astype('i8')
        self.assertCML(pcube, ("coord_api", "nd_bounds.cml"))


class TestGetterSetter(tests.IrisTest):
    def test_get_set_points_and_bounds(self):
        cube = iris.tests.stock.realistic_4d()
        coord = cube.coord("grid_latitude")
        
        # get bounds
        bounds = coord.bounds
        self.assertEquals(bounds.shape, (100, 2))
        
        self.assertEqual(bounds.shape[-1], coord.nbounds)
        
        # set bounds
        coord.bounds = bounds + 1
        
        np.testing.assert_array_equal(coord.bounds, bounds + 1)

        # set bounds - different length to existing points
        with self.assertRaises(ValueError):
            coord.bounds = bounds[::2, :]
        
        # set points/bounds to None
        with self.assertRaises(ValueError):
            coord.points = None
        coord.bounds = None
        
        # set bounds from non-numpy pair
        coord._points = None  # reset the undelying shape of the coordinate
        coord.points = 1
        coord.bounds = [123, 456]
        self.assertEqual(coord.shape, (1, ))
        self.assertEqual(coord.bounds.shape, (1, 2))
        
        # set bounds from non-numpy pairs
        coord._points = None # reset the undelying shape of the coordinate
        coord.points = range(3)
        coord.bounds = [[123, 456], [234, 567], [345, 678]]
        self.assertEqual(coord.shape, (3, ))
        self.assertEqual(coord.bounds.shape, (3, 2))
        

class TestGuessBounds(tests.IrisTest):
    def test_guess_bounds(self):
        coord = iris.coords.DimCoord(np.array([0, 10, 20, 30]), long_name="foo", units="1")
        coord.guess_bounds()
        self.assertArrayEqual(coord.bounds, np.array([[-5,5], [5,15], [15,25], [25,35]]))
        
        coord.bounds = None
        coord.guess_bounds(0.25)
        self.assertArrayEqual(coord.bounds, np.array([[-5,5], [5,15], [15,25], [25,35]]) + 2.5)
        
        coord.bounds = None
        coord.guess_bounds(0.75)
        self.assertArrayEqual(coord.bounds, np.array([[-5,5], [5,15], [15,25], [25,35]]) - 2.5)

        points = coord.points.copy()
        points[2] = 25
        coord.points = points
        coord.bounds = None
        coord.guess_bounds()
        self.assertArrayEqual(coord.bounds, np.array([[-5.,5.], [5.,17.5], [17.5,27.5], [27.5,32.5]]))
        
        # if the points are not monotonic, then guess_bounds should fail
        points[2] = 32
        coord = iris.coords.AuxCoord.from_coord(coord)
        coord.points = points
        coord.bounds = None
        with self.assertRaises(ValueError):
            coord.guess_bounds()


class TestIsContiguous(tests.IrisTest):
    def test_scalar(self):
        coord = iris.coords.DimCoord(23., bounds=[20., 26.])
        self.assertTrue(coord.is_contiguous())

    def test_equal_int(self):
        coord = iris.coords.DimCoord([0, 10, 20],
                                     bounds=[[0, 10],
                                             [10, 20],
                                             [20, 30]])
        self.assertTrue(coord.is_contiguous())

    def test_equal_float(self):
        coord = iris.coords.DimCoord([0., 10., 20.],
                                     bounds=[[0., 10.],
                                             [10., 20.],
                                             [20., 30.]])
        self.assertTrue(coord.is_contiguous())

    def test_guessed_bounds(self):
        delta = np.float64(0.00001)
        lower = -1.0 + delta
        upper = 3.0 - delta
        points, step = np.linspace(lower, upper, 2,
                                   endpoint=False, retstep=True)
        points += step * 0.5
        coord = iris.coords.DimCoord(points)
        coord.guess_bounds()
        self.assertTrue(coord.is_contiguous())

    def test_nobounds(self):
        coord = iris.coords.DimCoord([0, 10, 20])
        self.assertFalse(coord.is_contiguous())

    def test_multidim(self):
        points = np.arange(12, dtype=np.float64).reshape(3, 4)
        bounds = np.array([points, points + 1.0]).transpose(1, 2, 0)
        coord = iris.coords.AuxCoord(points, bounds=bounds)
        with self.assertRaises(ValueError):
            coord.is_contiguous()

    def test_one_bound(self):
        coord = iris.coords.DimCoord([0, 10, 20],
                                     bounds=[[0], [10], [20]])
        with self.assertRaises(ValueError):
            coord.is_contiguous()

    def test_three_bound(self):
        coord = iris.coords.DimCoord([0, 10, 20],
                                     bounds=[[0, 1, 2],
                                             [10, 11, 12],
                                             [20, 21, 22]])
        with self.assertRaises(ValueError):
            coord.is_contiguous()

    def test_non_contiguous(self):
        # Large enough difference to exceed default tolerance.
        delta = 1e-3
        points = np.array([0., 10., 20.])
        bounds = np.array([[0., 10.],
                           [10., 20],
                           [20., 30.]])
        coord = iris.coords.DimCoord(points, bounds=bounds)
        self.assertTrue(coord.is_contiguous())

        non_contig_bounds = bounds.copy()
        non_contig_bounds[0, 1] -= delta
        coord = iris.coords.DimCoord(points, bounds=non_contig_bounds)
        self.assertFalse(coord.is_contiguous())

        non_contig_bounds = bounds.copy()
        non_contig_bounds[1, 1] -= delta
        coord = iris.coords.DimCoord(points, bounds=non_contig_bounds)
        self.assertFalse(coord.is_contiguous())

        non_contig_bounds = bounds.copy()
        non_contig_bounds[1, 0] -= delta
        coord = iris.coords.DimCoord(points, bounds=non_contig_bounds)
        self.assertFalse(coord.is_contiguous())

        non_contig_bounds = bounds.copy()
        non_contig_bounds[1, 0] += delta
        coord = iris.coords.DimCoord(points, bounds=non_contig_bounds)
        self.assertFalse(coord.is_contiguous())

        non_contig_bounds = bounds.copy()
        non_contig_bounds[2, 0] -= delta
        coord = iris.coords.DimCoord(points, bounds=non_contig_bounds)
        self.assertFalse(coord.is_contiguous())

    def test_default_tol(self):
        # Smaller difference that default tolerance.
        delta = 1e-6
        points = np.array([0., 10., 20.])
        bounds = np.array([[0., 10.],
                           [10., 20],
                           [20., 30.]])
        bounds[1, 0] -= delta
        coord = iris.coords.DimCoord(points, bounds=bounds)
        self.assertTrue(coord.is_contiguous())

    def test_specified_tol(self):
        delta = 1e-6
        points = np.array([0., 10., 20.])
        bounds = np.array([[0., 10.],
                           [10., 20],
                           [20., 30.]])
        bounds[1, 0] += delta
        coord = iris.coords.DimCoord(points, bounds=bounds)
        self.assertTrue(coord.is_contiguous())
        # No tolerance.
        rtol = 0
        atol = 0
        self.assertFalse(coord.is_contiguous(rtol, atol))
        # Absolute only.
        rtol = 0
        atol = 1e-5  # larger than delta.
        self.assertTrue(coord.is_contiguous(rtol, atol))
        atol = 1e-7  # smaller than delta
        self.assertFalse(coord.is_contiguous(rtol, atol))
        # Relative only.
        atol = 0
        rtol = 1e-6  # is multiplied by upper bound (10.0) in comparison.
        self.assertTrue(coord.is_contiguous(rtol, atol))
        rtol = 1e-8
        self.assertFalse(coord.is_contiguous(rtol, atol))


class TestCoordCompatibility(tests.IrisTest):
    def setUp(self):
        self.aux_coord = iris.coords.AuxCoord([1., 2. ,3.],
                                              standard_name='longitude',
                                              var_name='lon',
                                              units='degrees')
        self.dim_coord = iris.coords.DimCoord(np.arange(0, 360, dtype=np.float64),
                                              standard_name='longitude',
                                              var_name='lon',
                                              units='degrees',
                                              circular=True)

    def test_not_compatible(self):
        r = self.aux_coord.copy()
        self.assertTrue(self.aux_coord.is_compatible(r))
        # The following changes should make the coords incompatible.
        # Different units.
        r.units = 'radians'
        self.assertFalse(self.aux_coord.is_compatible(r))
        # Different coord_systems.
        r = self.aux_coord.copy()
        r.coord_system = iris.coord_systems.GeogCS(6371229)
        self.assertFalse(self.aux_coord.is_compatible(r))
        # Different attributes.
        r = self.aux_coord.copy()
        self.aux_coord.attributes['source']= 'bob'
        r.attributes['source'] = 'alice'
        self.assertFalse(self.aux_coord.is_compatible(r))

    def test_compatible(self):
        # The following changes should not affect compatibility.
        # Different non-common attributes.
        r = self.aux_coord.copy()
        self.aux_coord.attributes['source']= 'bob'
        r.attributes['origin'] = 'alice'
        self.assertTrue(self.aux_coord.is_compatible(r))
        # Different points.
        r.points = np.zeros(r.points.shape)
        self.assertTrue(self.aux_coord.is_compatible(r))
        # Different var_names (but equal name()).
        r.var_name = 'foo'
        self.assertTrue(self.aux_coord.is_compatible(r))
        # With/without bounds.
        r.bounds = np.array([[0.5, 1.5],[1.5, 2.5],[2.5, 3.5]])
        self.assertTrue(self.aux_coord.is_compatible(r))

    def test_circular(self):
        # Test that circular has no effect on compatibility.
        # AuxCoord and circular DimCoord.
        self.assertTrue(self.aux_coord.is_compatible(self.dim_coord))
        # circular and non-circular DimCoord.
        r = self.dim_coord.copy()
        r.circular = False
        self.assertTrue(r.is_compatible(self.dim_coord))

    def test_defn(self):
        coord_defn = self.aux_coord._as_defn()
        self.assertTrue(self.aux_coord.is_compatible(coord_defn))
        coord_defn = self.dim_coord._as_defn()
        self.assertTrue(self.dim_coord.is_compatible(coord_defn))

    def test_is_ignore(self):
        r = self.aux_coord.copy()
        self.aux_coord.attributes['source']= 'bob'
        r.attributes['source'] = 'alice'
        self.assertFalse(self.aux_coord.is_compatible(r))
        # Use ignore keyword.
        self.assertTrue(self.aux_coord.is_compatible(r, ignore='source'))
        self.assertTrue(self.aux_coord.is_compatible(r, ignore=('source',)))
        self.assertTrue(self.aux_coord.is_compatible(r, ignore=r.attributes))


class TestAuxCoordEquality(tests.IrisTest):
    def test_not_implmemented(self):
        class Terry(object): pass
        aux = iris.coords.AuxCoord(0)
        self.assertIs(aux.__eq__(Terry()), NotImplemented)
        self.assertIs(aux.__ne__(Terry()), NotImplemented)


class TestDimCoordEquality(tests.IrisTest):
    def test_not_implmemented(self):
        class Terry(object): pass
        dim = iris.coords.DimCoord(0)
        aux = iris.coords.AuxCoord(0)
        self.assertIs(dim.__eq__(Terry()), NotImplemented)
        self.assertIs(dim.__ne__(Terry()), NotImplemented)
        self.assertIs(dim.__eq__(aux), NotImplemented)
        self.assertIs(dim.__ne__(aux), NotImplemented)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_coord_categorisation
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the coordinate categorisation functions.
"""

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import warnings

import numpy as np

import iris
import iris.coord_categorisation as ccat


CATEGORISATION_FUNCS = (
    ccat.add_day_of_month,
    ccat.add_day_of_year,
    ccat.add_weekday,
    ccat.add_weekday_fullname,
    ccat.add_weekday_number,
    ccat.add_month,
    ccat.add_month_fullname,
    ccat.add_month_number,
    ccat.add_year,
    ccat.add_season,
    ccat.add_season_number,
    ccat.add_season_year,
    ccat.add_season_membership,
)


class TestCategorisations(tests.IrisTest):
    def setUp(self):
        # make a series of 'day numbers' for the time, that slide across month
        # boundaries
        day_numbers = np.arange(0, 600, 27, dtype=np.int32)

        cube = iris.cube.Cube(
            day_numbers, long_name='test cube', units='metres')

        # use day numbers as data values also (don't actually use this for
        # anything)
        cube.data = day_numbers

        time_coord = iris.coords.DimCoord(
            day_numbers, standard_name='time',
            units=iris.unit.Unit('days since epoch', 'gregorian'))
        cube.add_dim_coord(time_coord, 0)

        self.cube = cube
        self.time_coord = time_coord

    def test_bad_coord(self):
        for func in CATEGORISATION_FUNCS:
            kwargs = {'name': 'my_category'}
            if func is ccat.add_season_membership:
                kwargs['season'] = 'djf'
            with self.assertRaises(iris.exceptions.CoordinateNotFoundError):
                func(self.cube, 'DOES NOT EXIST', **kwargs)

    def test_explicit_result_names(self):
        result_name = 'my_category'
        fmt = 'Missing/incorrectly named result for {0!r}'
        for func in CATEGORISATION_FUNCS:
            # Specify source coordinate by name
            cube = self.cube.copy()
            kwargs = {'name': result_name}
            if func is ccat.add_season_membership:
                kwargs['season'] = 'djf'
            with warnings.catch_warnings(record=True):
                func(cube, 'time', **kwargs)
            result_coords = cube.coords(result_name)
            self.assertEqual(len(result_coords), 1, fmt.format(func.func_name))
            # Specify source coordinate by coordinate reference
            cube = self.cube.copy()
            time = cube.coord('time')
            with warnings.catch_warnings(record=True):
                func(cube, time, **kwargs)
            result_coords = cube.coords(result_name)
            self.assertEqual(len(result_coords), 1, fmt.format(func.func_name))

    def test_basic(self):
        cube = self.cube
        time_coord = self.time_coord

        ccat.add_year(cube, time_coord, 'my_year')
        ccat.add_day_of_month(cube, time_coord, 'my_day_of_month')
        ccat.add_day_of_year(cube, time_coord, 'my_day_of_year')

        ccat.add_month(cube, time_coord, 'my_month')
        ccat.add_month_fullname(cube, time_coord, 'my_month_fullname')
        ccat.add_month_number(cube, time_coord, 'my_month_number')

        ccat.add_weekday(cube, time_coord, 'my_weekday')
        ccat.add_weekday_number(cube, time_coord, 'my_weekday_number')
        ccat.add_weekday_fullname(cube, time_coord, 'my_weekday_fullname')

        ccat.add_season(cube, time_coord, 'my_season')
        ccat.add_season_number(cube, time_coord, 'my_season_number')
        ccat.add_season_year(cube, time_coord, 'my_season_year')

        # also test 'generic' categorisation interface
        def _month_in_quarter(coord, pt_value):
            date = coord.units.num2date(pt_value)
            return (date.month - 1) % 3

        ccat.add_categorised_coord(cube,
                                   'my_month_in_quarter',
                                   time_coord,
                                   _month_in_quarter)

        # To ensure consistent results between 32-bit and 64-bit
        # platforms, ensure all the numeric categorisation coordinates
        # are always stored as int64.
        for coord in cube.coords():
            if coord.long_name is not None and coord.points.dtype.kind == 'i':
                coord.points = coord.points.astype(np.int64)

        # check values
        self.assertCML(cube, ('categorisation', 'quickcheck.cml'))

    def test_add_season_nonstandard(self):
        # season categorisations work for non-standard seasons?
        cube = self.cube
        time_coord = self.time_coord
        seasons = ['djfm', 'amjj', 'ason']
        ccat.add_season(cube, time_coord, name='seasons', seasons=seasons)
        ccat.add_season_number(cube, time_coord, name='season_numbers',
                               seasons=seasons)
        ccat.add_season_year(cube, time_coord, name='season_years',
                             seasons=seasons)
        self.assertCML(cube, ('categorisation', 'customcheck.cml'))

    def test_add_season_membership(self):
        # season membership identifies correct seasons?
        season = 'djf'
        ccat.add_season_membership(self.cube, 'time', season,
                                   name='in_season')
        ccat.add_season(self.cube, 'time')
        coord_season = self.cube.coord('season')
        coord_membership = self.cube.coord('in_season')
        season_locations = np.where(coord_season.points == season)[0]
        membership_locations = np.where(coord_membership.points)[0]
        self.assertArrayEqual(membership_locations, season_locations)

    def test_add_season_invalid_spec(self):
        # custom seasons with an invalid season raises an error?
        seasons = ('djf', 'maj', 'jja', 'son')   # MAJ not a season!
        for func in (ccat.add_season, ccat.add_season_year,
                     ccat.add_season_number):
            with self.assertRaises(ValueError):
                func(self.cube, 'time', name='my_category', seasons=seasons)

    def test_add_season_repeated_months(self):
        # custom seasons with repeated months raises an error?
        seasons = ('djfm', 'mam', 'jja', 'son')
        for func in (ccat.add_season, ccat.add_season_year,
                     ccat.add_season_number):
            with self.assertRaises(ValueError):
                func(self.cube, 'time', name='my_category', seasons=seasons)

    def test_add_season_missing_months(self):
        # custom seasons with missing months raises an error?
        seasons = ('djfm', 'amjj')
        for func in (ccat.add_season, ccat.add_season_year,
                     ccat.add_season_number):
            with self.assertRaises(ValueError):
                func(self.cube, 'time', name='my_category', seasons=seasons)

    def test_add_season_membership_invalid_spec(self):
        season = 'maj'   # not a season!
        with self.assertRaises(ValueError):
            ccat.add_season_membership(self.cube, 'time', season,
                                       name='maj_season')


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_cube
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np

import iris.cube


class Test_CubeList_getitem(tests.IrisTest):
    def setUp(self):
        self.cube0 = iris.cube.Cube(0)
        self.cube1 = iris.cube.Cube(1)
        self.src_list = [self.cube0, self.cube1]
        self.cube_list = iris.cube.CubeList(self.src_list)

    def test_single(self):
        # Check that simple indexing returns the relevant member Cube.
        for i, cube in enumerate(self.src_list):
            self.assertIs(self.cube_list[i], cube)

    def _test_slice(self, keys):
        subset = self.cube_list[keys]
        self.assertIsInstance(subset, iris.cube.CubeList)
        self.assertEqual(subset, self.src_list[keys])

    def test_slice(self):
        # Check that slicing returns a CubeList containing the relevant
        # members.
        self._test_slice(slice(None))
        self._test_slice(slice(1))
        self._test_slice(slice(1, None))
        self._test_slice(slice(0, 1))
        self._test_slice(slice(None, None, -1))


class Test_CubeList_getslice(tests.IrisTest):
    def setUp(self):
        self.cube0 = iris.cube.Cube(0)
        self.cube1 = iris.cube.Cube(1)
        self.src_list = [self.cube0, self.cube1]
        self.cube_list = iris.cube.CubeList(self.src_list)

    def _test_slice(self, cube_list, equivalent):
        self.assertIsInstance(cube_list, iris.cube.CubeList)
        self.assertEqual(cube_list, equivalent)

    def test_slice(self):
        # Check that slicing returns a CubeList containing the relevant
        # members.
        # NB. We have to use explicit [:1] syntax to trigger the call
        # to __getslice__. Using [slice(1)] still calls __getitem__!
        self._test_slice(self.cube_list[:1], self.src_list[:1])
        self._test_slice(self.cube_list[1:], self.src_list[1:])
        self._test_slice(self.cube_list[0:1], self.src_list[0:1:])


class Test_Cube_add_dim_coord(tests.IrisTest):
    def setUp(self):
        self.cube = iris.cube.Cube(np.arange(4).reshape(2, 2))

    def test_no_dim(self):
        self.assertRaises(TypeError,
                          self.cube.add_dim_coord,
                          iris.coords.DimCoord(np.arange(2), "latitude"))

    def test_adding_aux_coord(self):
        coord = iris.coords.AuxCoord(np.arange(2), "latitude")
        with self.assertRaises(ValueError):
            self.cube.add_dim_coord(coord, 0)


class TestEquality(tests.IrisTest):
    def test_not_implmemented(self):
        class Terry(object):
            pass
        cube = iris.cube.Cube(0)
        self.assertIs(cube.__eq__(Terry()), NotImplemented)
        self.assertIs(cube.__ne__(Terry()), NotImplemented)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_cube_to_pp
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import os
import tempfile

import mock
import numpy as np

import iris
import iris.coords
import iris.coord_systems
import iris.fileformats.pp as ff_pp
from iris.fileformats.pp import PPField3
import iris.io
import iris.unit
import iris.tests.pp as pp
import iris.util
import iris.tests.stock as stock


def itab_callback(cube, field, filename):
    cube.add_aux_coord(iris.coords.AuxCoord([field.lbrel], long_name='MOUMHeaderReleaseNumber', units='no_unit')) 
    cube.add_aux_coord(iris.coords.AuxCoord([field.lbexp], long_name='ExperimentNumber(ITAB)', units='no_unit')) 


@tests.skip_data
class TestPPSave(tests.IrisTest, pp.PPTest):
    def test_no_forecast_time(self):
        cube = stock.lat_lon_cube()
        coord = iris.coords.DimCoord(np.array([24], dtype=np.int64),
                                     standard_name='time',
                                     units='hours since epoch')
        cube.add_aux_coord(coord)
        self.assertCML(cube, ['cube_to_pp', 'no_forecast_time.cml'])

        reference_txt_path = tests.get_result_path(('cube_to_pp', 'no_forecast_time.txt'))
        with self.cube_save_test(reference_txt_path, reference_cubes=cube) as temp_pp_path:
            iris.save(cube, temp_pp_path)

    def test_no_forecast_period(self):
        cube = stock.lat_lon_cube()
        # Add a bounded scalar time coord and a forecast_reference_time.
        time_coord = iris.coords.DimCoord(
            10.958333, standard_name='time',
            units='days since 2013-05-10 12:00',
            bounds=[10.916667, 11.0])
        cube.add_aux_coord(time_coord)
        forecast_reference_time = iris.coords.DimCoord(
            2.0, standard_name='forecast_reference_time',
            units='weeks since 2013-05-07')
        cube.add_aux_coord(forecast_reference_time)

        self.assertCML(cube, ['cube_to_pp', 'no_forecast_period.cml'])
        reference_txt_path = tests.get_result_path(('cube_to_pp',
                                                    'no_forecast_period.txt'))
        with self.cube_save_test(reference_txt_path, reference_cubes=cube) as \
                temp_pp_path:
            iris.save(cube, temp_pp_path)

    def test_pp_save_rules(self):
        # Test pp save rules without user rules.

        #read
        in_filename = tests.get_data_path(('PP', 'simple_pp', 'global.pp'))
        cubes = iris.load(in_filename, callback=itab_callback)

        reference_txt_path = tests.get_result_path(('cube_to_pp', 'simple.txt'))
        with self.cube_save_test(reference_txt_path, reference_cubes=cubes) as temp_pp_path:
            iris.save(cubes, temp_pp_path)

    def test_user_pp_save_rules(self):
        # Test pp save rules with user rules.
        
        #create a user rules file
        user_rules_filename = iris.util.create_temp_filename(suffix='.txt')
        try:
            with open(user_rules_filename, "wt") as user_rules_file:
                user_rules_file.write("IF\ncm.standard_name == 'air_temperature'\nTHEN\npp.lbuser[3] = 9222")
            iris.fileformats.pp.add_save_rules(user_rules_filename)
            try:
                #read pp
                in_filename = tests.get_data_path(('PP', 'simple_pp', 'global.pp'))
                cubes = iris.load(in_filename, callback=itab_callback)

                reference_txt_path = tests.get_result_path(('cube_to_pp', 'user_rules.txt'))
                with self.cube_save_test(reference_txt_path, reference_cubes=cubes) as temp_pp_path:
                    iris.save(cubes, temp_pp_path)

            finally:
                iris.fileformats.pp.reset_save_rules()
        finally:
            os.remove(user_rules_filename)

    def test_pp_append_singles(self):
        # Test pp append saving - single cubes.
        
        # load 2 arrays of >2D cubes
        cube = stock.simple_pp()
        
        reference_txt_path = tests.get_result_path(('cube_to_pp', 'append_single.txt'))
        with self.cube_save_test(reference_txt_path, reference_cubes=[cube, cube]) as temp_pp_path:
            iris.save(cube, temp_pp_path)                # Create file
            iris.save(cube, temp_pp_path, append=True)   # Append to file

        reference_txt_path = tests.get_result_path(('cube_to_pp', 'replace_single.txt'))
        with self.cube_save_test(reference_txt_path, reference_cubes=cube) as temp_pp_path:
            iris.save(cube, temp_pp_path)                # Create file
            iris.save(cube, temp_pp_path)                # Replace file

    def test_pp_append_lists(self):
        # Test PP append saving - lists of cubes.
        # For each of the first four time-steps in the 4D cube,
        # pull out the bottom two levels.
        cube_4d = stock.realistic_4d()
        cubes = [cube_4d[i, :2, :, :] for i in range(4)]

        reference_txt_path = tests.get_result_path(('cube_to_pp', 'append_multi.txt'))
        with self.cube_save_test(reference_txt_path, reference_cubes=cubes) as temp_pp_path:
            iris.save(cubes[:2], temp_pp_path)
            iris.save(cubes[2:], temp_pp_path, append=True)

        reference_txt_path = tests.get_result_path(('cube_to_pp', 'replace_multi.txt'))
        with self.cube_save_test(reference_txt_path, reference_cubes=cubes[2:]) as temp_pp_path:
            iris.save(cubes[:2], temp_pp_path)
            iris.save(cubes[2:], temp_pp_path)

    def add_coords_to_cube_and_test(self, coord1, coord2):
        # a wrapper for creating arbitrary 2d cross-sections and run pp-saving tests
        dataarray = np.arange(16, dtype='>f4').reshape(4,4)
        cm = iris.cube.Cube(data=dataarray)

        cm.add_dim_coord(coord1, 0)
        cm.add_dim_coord(coord2, 1)
        
        # TODO: This is the desired line of code...
        # reference_txt_path = tests.get_result_path(('cube_to_pp', '%s.%s.pp.txt' % (coord1.name(), coord2.name())))
        # ...but this is required during the CF change, to maintain the original filename.
        coord1_name = coord1.name().replace("air_", "")
        coord2_name = coord2.name().replace("air_", "") 
        reference_txt_path = tests.get_result_path(('cube_to_pp', '%s.%s.pp.txt' % (coord1_name, coord2_name))) 

        # test with name
        with self.cube_save_test(reference_txt_path, reference_cubes=cm, 
                field_coords=[coord1.name(), coord2.name()]) as temp_pp_path:
            iris.save(cm, temp_pp_path, field_coords=[coord1.name(), coord2.name()])
        # test with coord
        with self.cube_save_test(reference_txt_path, reference_cubes=cm, 
                field_coords=[coord1, coord2]) as temp_pp_path:
            iris.save(cm, temp_pp_path, field_coords=[coord1, coord2])

    def test_non_standard_cross_sections(self):
        #ticket #1037, the five variants being dealt with are
        #    'pressure.latitude',
        #    'depth.latitude',
        #    'eta.latitude',
        #    'pressure.time',
        #    'depth.time',

        f = FakePPEnvironment()

        self.add_coords_to_cube_and_test(
            iris.coords.DimCoord(f.z, long_name='air_pressure', units='hPa', bounds=f.z_bounds),
            iris.coords.DimCoord(f.y, standard_name='latitude', units='degrees', bounds=f.y_bounds, coord_system=f.geog_cs()))
            
        self.add_coords_to_cube_and_test(
            iris.coords.DimCoord(f.z, long_name='depth', units='m', bounds=f.z_bounds),
            iris.coords.DimCoord(f.y, standard_name='latitude', units='degrees', bounds=f.y_bounds, coord_system=f.geog_cs()))
            
        self.add_coords_to_cube_and_test(
            iris.coords.DimCoord(f.z, long_name='eta', units='1', bounds=f.z_bounds),
            iris.coords.DimCoord(f.y, standard_name='latitude', units='degrees', bounds=f.y_bounds, coord_system=f.geog_cs()))
            
        self.add_coords_to_cube_and_test(
            iris.coords.DimCoord(f.z, long_name='air_pressure', units='hPa', bounds=f.z_bounds),
            iris.coords.DimCoord(f.y, standard_name='time', units=iris.unit.Unit('days since 0000-01-01 00:00:00', calendar=iris.unit.CALENDAR_360_DAY), bounds=f.y_bounds))
            
        self.add_coords_to_cube_and_test(
            iris.coords.DimCoord(f.z, standard_name='depth', units='m', bounds=f.z_bounds),
            iris.coords.DimCoord(f.y, standard_name='time', units=iris.unit.Unit('days since 0000-01-01 00:00:00', calendar=iris.unit.CALENDAR_360_DAY), bounds=f.y_bounds))

    def test_365_calendar_export(self):
        # test for 365 day calendar export
        cube = stock.simple_pp()
        new_unit = iris.unit.Unit('hours since 1970-01-01 00:00:00',
                                  calendar=iris.unit.CALENDAR_365_DAY)
        cube.coord('time').units = new_unit
        pp_field = mock.MagicMock(spec=PPField3)
        iris.fileformats.pp._ensure_save_rules_loaded()
        iris.fileformats.pp._save_rules.verify(cube, pp_field)
        self.assertEqual(pp_field.lbtim.ic, 4)

            
class FakePPEnvironment(object):
    ''' fake a minimal PP environment for use in cross-section coords, as in PP save rules '''
    y = [1,2,3,4]
    z = [111,222,333,444]
    y_bounds = [[0.9,1.1], [1.9,2.1], [2.9,3.1], [3.9,4.1]]
    z_bounds = [[110.9,111.1], [221.9,222.1], [332.9,333.1], [443.9,444.1]]

    def geog_cs(self):
        """Return a GeogCS for this PPField.

        Returns:
            A GeogCS with the appropriate earth shape, meridian and pole position.
        """
        return iris.coord_systems.GeogCS(6371229.0)


class TestPPSaveRules(tests.IrisTest, pp.PPTest):
    def test_default_coord_system(self):
        GeogCS = iris.coord_systems.GeogCS
        cube = iris.tests.stock.lat_lon_cube()
        reference_txt_path = tests.get_result_path(('cube_to_pp',
                                                    'default_coord_system.txt'))
        # Remove all coordinate systems.
        for coord in cube.coords():
            coord.coord_system = None
        # Ensure no coordinate systems available.
        self.assertIsNone(cube.coord_system(GeogCS))
        self.assertIsNone(cube.coord_system(None))
        with self.cube_save_test(reference_txt_path, reference_cubes=cube) as \
                temp_pp_path:
            # Save cube to PP with no coordinate system.
            iris.save(cube, temp_pp_path)
            pp_cube = iris.load_cube(temp_pp_path)
            # Ensure saved cube has the default coordinate system.
            self.assertIsInstance(pp_cube.coord_system(GeogCS),
                                  iris.coord_systems.GeogCS)
            self.assertIsNotNone(pp_cube.coord_system(None))
            self.assertIsInstance(pp_cube.coord_system(None),
                                  iris.coord_systems.GeogCS)
            self.assertIsNotNone(pp_cube.coord_system())
            self.assertIsInstance(pp_cube.coord_system(),
                                  iris.coord_systems.GeogCS)

    def lbproc_from_pp(self, filename):
        # Gets the lbproc field from the ppfile
        pp_file = iris.fileformats.pp.load(filename)
        field = pp_file.next()
        return field.lbproc

    def test_pp_save_rules(self):
        # Test single process flags
        for _, process_desc in iris.fileformats.pp.LBPROC_PAIRS[1:]:
            # Get basic cube and set process flag manually
            ll_cube = stock.lat_lon_cube()
            ll_cube.attributes["ukmo__process_flags"] = (process_desc,)
    
            # Save cube to pp
            temp_filename = iris.util.create_temp_filename(".pp")
            iris.save(ll_cube, temp_filename)
     
            # Check the lbproc is what we expect
            self.assertEquals(self.lbproc_from_pp(temp_filename), iris.fileformats.pp.lbproc_map[process_desc])

            os.remove(temp_filename)

        # Test mutiple process flags
        multiple_bit_values = ((128, 64), (4096, 1024), (8192, 1024))
        
        # Maps lbproc value to the process flags that should be created
        multiple_map = {sum(bits) : [iris.fileformats.pp.lbproc_map[bit] for bit in bits] for bits in multiple_bit_values}

        for lbproc, descriptions in multiple_map.iteritems():
            ll_cube = stock.lat_lon_cube()
            ll_cube.attributes["ukmo__process_flags"] = descriptions
            
            # Save cube to pp
            temp_filename = iris.util.create_temp_filename(".pp")
            iris.save(ll_cube, temp_filename)
            
            # Check the lbproc is what we expect
            self.assertEquals(self.lbproc_from_pp(temp_filename), lbproc)

            os.remove(temp_filename)
            
    def test_lbvc(self):
        cube = stock.realistic_4d_no_derived()[0, :4, ...]
        
        v_coord = iris.coords.DimCoord(standard_name='depth', 
                                       units='m', points=[-5, -10, -15, -20])
        
        cube.remove_coord('level_height')
        cube.remove_coord('sigma')
        cube.remove_coord('surface_altitude')
        cube.add_aux_coord(v_coord, 0)
    
        expected = ([2, 1, -5.0],
                    [2, 2, -10.0],
                    [2, 3, -15.0],
                    [2, 4, -20.0])
                    
        for field, (lbvc, lblev, blev) in zip(fields_from_cube(cube), expected):
            self.assertEqual(field.lbvc, lbvc)
            self.assertEqual(field.lblev, lblev)
            self.assertEqual(field.blev, blev)
        
        
def fields_from_cube(cubes):
    """
    Return an iterator of PP fields generated from saving the given cube(s)
    to a temporary file, and then subsequently loading them again 
    """
    with tempfile.NamedTemporaryFile('w+b', suffix='.pp') as tmp_file:
        fh = tmp_file.file
        iris.save(cubes, fh, saver='pp')
        
        # make sure the fh is written to disk, and move it back to the
        # start of the file 
        fh.flush()
        os.fsync(fh)        
        fh.seek(0)
        
        # load in the saved pp fields and check the appropriate metadata
        for field in ff_pp.load(tmp_file.name):
            yield field
            

if __name__ == "__main__":
    tests.main()
    

########NEW FILE########
__FILENAME__ = test_ff
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the Fieldsfile file loading plugin and FFHeader.

"""


# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import collections
import warnings

import mock
import numpy as np

import iris
import iris.fileformats.ff as ff
import iris.fileformats.pp as pp


_MockField = collections.namedtuple('_MockField',
                                    'lbext lblrec lbnrec raw_lbpack lbuser')

# PP-field: LBPACK N1 values.
_UNPACKED = 0
_WGDOS = 1
_CRAY = 2
_GRIB = 3  # Not implemented.
_RLE = 4   # Not supported, deprecated FF format.

# PP-field: LBUSER(1) values.
_REAL = 1
_INTEGER = 2
_LOGICAL = 3  # Not implemented.


class TestFF_HEADER(tests.IrisTest):
    def test_initialisation(self):
        self.assertEqual(ff.FF_HEADER[0], ('data_set_format_version', (0,)))
        self.assertEqual(ff.FF_HEADER[17], ('integer_constants', (99, 100)))

    def test_size(self):
        self.assertEqual(len(ff.FF_HEADER), 31)


@tests.skip_data
class TestFFHeader(tests.IrisTest):
    def setUp(self):
        self.filename = tests.get_data_path(('FF', 'n48_multi_field'))
        self.ff_header = ff.FFHeader(self.filename)
        self.valid_headers = (
            'integer_constants', 'real_constants', 'level_dependent_constants',
            'lookup_table', 'data'
        )
        self.invalid_headers = (
            'row_dependent_constants', 'column_dependent_constants',
            'fields_of_constants', 'extra_constants', 'temp_historyfile',
            'compressed_field_index1', 'compressed_field_index2',
            'compressed_field_index3'
        )

    def test_constructor(self):
        # Test FieldsFile header attribute lookup.
        self.assertEqual(self.ff_header.data_set_format_version, 20)
        self.assertEqual(self.ff_header.sub_model, 1)
        self.assertEqual(self.ff_header.vert_coord_type, 5)
        self.assertEqual(self.ff_header.horiz_grid_type, 0)
        self.assertEqual(self.ff_header.dataset_type, 3)
        self.assertEqual(self.ff_header.run_identifier, 0)
        self.assertEqual(self.ff_header.experiment_number, -32768)
        self.assertEqual(self.ff_header.calendar, 1)
        self.assertEqual(self.ff_header.grid_staggering, 3)
        self.assertEqual(self.ff_header.time_type, -32768)
        self.assertEqual(self.ff_header.projection_number, -32768)
        self.assertEqual(self.ff_header.model_version, 802)
        self.assertEqual(self.ff_header.obs_file_type, -32768)
        self.assertEqual(self.ff_header.last_fieldop_type, -32768)
        self.assertEqual(self.ff_header.first_validity_time,
                         (2011, 7, 10, 18, 0, 0, 191))
        self.assertEqual(self.ff_header.last_validity_time,
                         (2011, 7, 10, 21, 0, 0, 191))
        self.assertEqual(self.ff_header.misc_validity_time,
                         (2012, 4, 30, 18, 12, 13, -32768))
        self.assertEqual(self.ff_header.integer_constants.shape, (46, ))
        self.assertEqual(self.ff_header.real_constants.shape, (38, ))
        self.assertEqual(self.ff_header.level_dependent_constants.shape,
                         (71, 8))
        self.assertIsNone(self.ff_header.row_dependent_constants)
        self.assertIsNone(self.ff_header.column_dependent_constants)
        self.assertIsNone(self.ff_header.fields_of_constants)
        self.assertIsNone(self.ff_header.extra_constants)
        self.assertIsNone(self.ff_header.temp_historyfile)
        self.assertIsNone(self.ff_header.compressed_field_index1)
        self.assertIsNone(self.ff_header.compressed_field_index2)
        self.assertIsNone(self.ff_header.compressed_field_index3)
        self.assertEqual(self.ff_header.lookup_table, (909, 64, 5))
        self.assertEqual(self.ff_header.total_prognostic_fields, 3119)
        self.assertEqual(self.ff_header.data, (2049, 2961, -32768))

    def test_str(self):
        self.assertString(str(self.ff_header), ('FF', 'ffheader.txt'))

    def test_repr(self):
        target = "FFHeader('" + self.filename + "')"
        self.assertEqual(repr(self.ff_header), target)

    def test_shape(self):
        self.assertEqual(self.ff_header.shape('data'), (2961, -32768))


@tests.skip_data
class TestFF2PP2Cube(tests.IrisTest):
    def setUp(self):
        self.filename = tests.get_data_path(('FF', 'n48_multi_field'))

    def test_unit_pass_0(self):
        # Test FieldsFile to PPFields cube load.
        cube_by_name = collections.defaultdict(int)
        cubes = iris.load(self.filename)
        while cubes:
            cube = cubes.pop(0)
            standard_name = cube.standard_name
            cube_by_name[standard_name] += 1
            filename = '{}_{}.cml'.format(standard_name,
                                          cube_by_name[standard_name])
            self.assertCML(cube, ('FF', filename))


@tests.skip_data
class TestFFieee32(tests.IrisTest):
    def test_iris_loading(self):
        ff32_fname = tests.get_data_path(('FF', 'n48_multi_field.ieee32'))
        ff64_fname = tests.get_data_path(('FF', 'n48_multi_field'))

        ff32_cubes = iris.load(ff32_fname)
        ff64_cubes = iris.load(ff64_fname)

        for ff32, ff64 in zip(ff32_cubes, ff64_cubes):
            # load the data
            _, _ = ff32.data, ff64.data
            self.assertEqual(ff32, ff64)


@tests.skip_data
class TestFFVariableResolutionGrid(tests.IrisTest):
    def setUp(self):
        self.filename = tests.get_data_path(('FF', 'n48_multi_field'))

        self.ff2pp = ff.FF2PP(self.filename)
        self.ff_header = self.ff2pp._ff_header

        data_shape = (73, 96)
        delta = np.sin(np.linspace(0, np.pi * 5, data_shape[1])) * 5
        lons = np.linspace(0, 180, data_shape[1]) + delta
        lons = np.vstack([lons[:-1], lons[:-1] + 0.5 * np.diff(lons)]).T
        lons = np.reshape(lons, lons.shape, order='F')

        delta = np.sin(np.linspace(0, np.pi * 5, data_shape[0])) * 5
        lats = np.linspace(-90, 90, data_shape[0]) + delta
        lats = np.vstack([lats[:-1], lats[:-1] + 0.5 * np.diff(lats)]).T
        lats = np.reshape(lats, lats.shape, order='F')

        self.ff_header.column_dependent_constants = lons
        self.ff_header.row_dependent_constants = lats

        self.U_grid_x = lons[:-1, 1]
        self.V_grid_y = lats[:-1, 1]
        self.P_grid_x = lons[:, 0]
        self.P_grid_y = lats[:, 0]

        self.orig_make_pp_field = pp.make_pp_field

        def new_make_pp_field(header):
            field = self.orig_make_pp_field(header)
            field.stash = self.ff2pp._custom_stash
            field.bdx = field.bdy = field.bmdi
            return field

        # Replace the pp module function with this new function;
        # this gets called in PP2FF.
        pp.make_pp_field = new_make_pp_field

    def tearDown(self):
        pp.make_pp_field = self.orig_make_pp_field

    def _check_stash(self, stash, x_coord, y_coord):
        self.ff2pp._custom_stash = stash
        field = next(iter(self.ff2pp))
        self.assertArrayEqual(x_coord, field.x, ('x_coord was incorrect for '
                                                 'stash {}'.format(stash)))
        self.assertArrayEqual(y_coord, field.y, ('y_coord was incorrect for '
                                                 'stash {}'.format(stash)))

    def test_p(self):
        self._check_stash('m01s00i001', self.P_grid_x, self.P_grid_y)

    def test_u(self):
        self._check_stash('m01s00i002', self.U_grid_x, self.P_grid_y)

    def test_v(self):
        self._check_stash('m01s00i003', self.P_grid_x, self.V_grid_y)

    def test_unhandled_grid_type(self):
        with mock.patch('warnings.warn') as warn_fn:
            self._check_stash('m01s00i005', self.P_grid_x, self.P_grid_y)
            self.assertIn("Assuming the data is on a P grid.",
                          warn_fn.call_args[0][0])


class TestFFPayload(tests.IrisTest):
    def _test_payload(self, mock_field, expected_depth, expected_type):
        with mock.patch('iris.fileformats.ff.FFHeader') as mock_header:
            mock_header.return_value = None
            ff2pp = ff.FF2PP('Not real')
            data_depth, data_type = ff2pp._payload(mock_field)
            self.assertEqual(data_depth, expected_depth)
            self.assertEqual(data_type, expected_type)

    def test_payload_unpacked_real(self):
        mock_field = _MockField(lbext=0, lblrec=100, lbnrec=-1,
                                raw_lbpack=_UNPACKED,
                                lbuser=[_REAL])
        expected_type = ff._LBUSER_DTYPE_LOOKUP[_REAL].format(word_depth=8)
        expected_type = np.dtype(expected_type)
        self._test_payload(mock_field, 800, expected_type)

    def test_payload_unpacked_real_ext(self):
        mock_field = _MockField(lbext=50, lblrec=100, lbnrec=-1,
                                raw_lbpack=_UNPACKED,
                                lbuser=[_REAL])
        expected_type = ff._LBUSER_DTYPE_LOOKUP[_REAL].format(word_depth=8)
        expected_type = np.dtype(expected_type)
        self._test_payload(mock_field, 400, expected_type)

    def test_payload_unpacked_integer(self):
        mock_field = _MockField(lbext=0, lblrec=200, lbnrec=-1,
                                raw_lbpack=_UNPACKED,
                                lbuser=[_INTEGER])
        expected_type = ff._LBUSER_DTYPE_LOOKUP[_INTEGER].format(word_depth=8)
        expected_type = np.dtype(expected_type)
        self._test_payload(mock_field, 1600, expected_type)

    def test_payload_unpacked_integer_ext(self):
        mock_field = _MockField(lbext=100, lblrec=200, lbnrec=-1,
                                raw_lbpack=_UNPACKED,
                                lbuser=[_INTEGER])
        expected_type = ff._LBUSER_DTYPE_LOOKUP[_INTEGER].format(word_depth=8)
        expected_type = np.dtype(expected_type)
        self._test_payload(mock_field, 800, expected_type)

    def test_payload_wgdos_real(self):
        mock_field = _MockField(lbext=0, lblrec=-1, lbnrec=100,
                                raw_lbpack=_WGDOS,
                                lbuser=[_REAL])
        self._test_payload(mock_field, 796, pp.LBUSER_DTYPE_LOOKUP[_REAL])

    def test_payload_wgdos_real_ext(self):
        mock_field = _MockField(lbext=50, lblrec=-1, lbnrec=100,
                                raw_lbpack=_WGDOS,
                                lbuser=[_REAL])
        self._test_payload(mock_field, 796, pp.LBUSER_DTYPE_LOOKUP[_REAL])

    def test_payload_wgdos_integer(self):
        mock_field = _MockField(lbext=0, lblrec=-1, lbnrec=200,
                                raw_lbpack=_WGDOS,
                                lbuser=[_INTEGER])
        self._test_payload(mock_field, 1596, pp.LBUSER_DTYPE_LOOKUP[_INTEGER])

    def test_payload_wgdos_integer_ext(self):
        mock_field = _MockField(lbext=100, lblrec=-1, lbnrec=200,
                                raw_lbpack=_WGDOS,
                                lbuser=[_INTEGER])
        self._test_payload(mock_field, 1596, pp.LBUSER_DTYPE_LOOKUP[_INTEGER])

    def test_payload_cray_real(self):
        mock_field = _MockField(lbext=0, lblrec=100, lbnrec=-1,
                                raw_lbpack=_CRAY,
                                lbuser=[_REAL])
        self._test_payload(mock_field, 400, pp.LBUSER_DTYPE_LOOKUP[_REAL])

    def test_payload_cray_real_ext(self):
        mock_field = _MockField(lbext=50, lblrec=100, lbnrec=-1,
                                raw_lbpack=_CRAY,
                                lbuser=[_REAL])
        self._test_payload(mock_field, 200, pp.LBUSER_DTYPE_LOOKUP[_REAL])

    def test_payload_cray_integer(self):
        mock_field = _MockField(lbext=0, lblrec=200, lbnrec=-1,
                                raw_lbpack=_CRAY,
                                lbuser=[_INTEGER])
        self._test_payload(mock_field, 800, pp.LBUSER_DTYPE_LOOKUP[_INTEGER])

    def test_payload_cray_integer_ext(self):
        mock_field = _MockField(lbext=100, lblrec=200, lbnrec=-1,
                                raw_lbpack=_CRAY,
                                lbuser=[_INTEGER])
        self._test_payload(mock_field, 400, pp.LBUSER_DTYPE_LOOKUP[_INTEGER])


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_file_load
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the file loading mechanism.

"""

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests
import iris


@tests.skip_data
class TestFileLoad(tests.IrisTest):
    def _test_file(self, src_path, reference_filename):
        """
        Checks the result of loading the given file spec, or creates the
        reference file if it doesn't exist.

        """
        cubes = iris.load_raw(tests.get_data_path(src_path))
        self.assertCML(cubes, ['file_load', reference_filename])

    def test_no_file(self):
        # Test an IOError is received when a filename is given which doesn't match any files
        real_file = ['PP', 'globClim1', 'theta.pp']
        non_existant_file = ['PP', 'globClim1', 'no_such_file*']

        with self.assertRaises(IOError):
            iris.load(tests.get_data_path(non_existant_file))
        with self.assertRaises(IOError):
            iris.load([tests.get_data_path(non_existant_file), tests.get_data_path(real_file)])
        with self.assertRaises(IOError):
            iris.load([tests.get_data_path(real_file), tests.get_data_path(non_existant_file)])

    def test_single_file(self):
        src_path = ['PP', 'globClim1', 'theta.pp']
        self._test_file(src_path, 'theta_levels.cml')

    def test_star_wildcard(self):
        src_path = ['PP', 'globClim1', '*_wind.pp']
        self._test_file(src_path, 'wind_levels.cml')

    def test_query_wildcard(self):
        src_path = ['PP', 'globClim1', '?_wind.pp']
        self._test_file(src_path, 'wind_levels.cml')

    def test_charset_wildcard(self):
        src_path = ['PP', 'globClim1', '[rstu]_wind.pp']
        self._test_file(src_path, 'u_wind_levels.cml')

    def test_negative_charset_wildcard(self):
        src_path = ['PP', 'globClim1', '[!rstu]_wind.pp']
        self._test_file(src_path, 'v_wind_levels.cml')

    def test_empty_file(self):
        with self.temp_filename(suffix='.pp') as temp_filename:
            with open(temp_filename, "a") as file:
                with self.assertRaises(iris.exceptions.TranslationError):
                    iris.load(temp_filename)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_file_save
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the file saving mechanism.

"""

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import cStringIO
import os
import unittest

import iris
import iris.cube
import iris.util
import iris.fileformats.pp as pp
import iris.fileformats.dot as dot

# Make a test skip decorator, for when DOT not available
skip_dotpng = unittest.skipIf(
    not dot.DOT_AVAILABLE,
    'Test(s) require the "dot" executable, which was not found. '
    'Check the dot_path setting in site.cfg.')

CHKSUM_ERR = "Mismatch between checksum of iris.save and {}.save."


def save_by_filename(filename1, filename2, cube, saver_fn, iosaver=None):
    """ Saves a cube to two different filenames using iris.save and the save method of the object representing the file type directly"""
    # Save from object direct
    saver_fn(cube, filename1)

    # Call save on iris
    iris.save(cube, filename2, iosaver) # Optional iris.io.find_saver passed in from test

def save_by_filehandle(filehandle1, filehandle2, cube, fn_saver, binary_mode = True):
    """ Saves a cube to two different filehandles using iris.save and the save method of the object representing the file type directly"""
    mode = "wb" if binary_mode else "w"

    # Save from object direct
    with open(filehandle1, mode) as outfile:
        fn_saver(cube, outfile)

    # Call save on iris
    with open(filehandle2, mode) as outfile:
        iris.save(cube, outfile)


@tests.skip_data
class TestSaveMethods(tests.IrisTest):
    """ Base class for file saving tests. Loads data and creates/deletes tempfiles"""
    def setUp(self):
        self.cube1 = iris.load_cube(tests.get_data_path(('PP', 'aPPglob1', 'global.pp')))
        self.cube2 = iris.load_cube(tests.get_data_path(('PP', 'aPPglob1', 'global_t_forecast.pp')))
        self.temp_filename1 = iris.util.create_temp_filename(self.ext)
        self.temp_filename2 = iris.util.create_temp_filename(self.ext)

    def tearDown(self):
        for tempfile in (self.temp_filename1, self.temp_filename2):
            try:
                os.remove(tempfile)
            except Exception:
                pass

class TestSavePP(TestSaveMethods):
    """Test saving cubes to PP format"""
    ext = ".pp"

    def test_filename(self):
        # Save using iris.save and pp.save
        save_by_filename(self.temp_filename1, self.temp_filename2, self.cube1, pp.save)

        # Compare files
        self.assertEquals(self.file_checksum(self.temp_filename2), self.file_checksum(self.temp_filename1), CHKSUM_ERR.format(self.ext))

    def test_filehandle(self):
        # Save using iris.save and pp.save
        save_by_filehandle(self.temp_filename1, self.temp_filename2, self.cube1, pp.save, binary_mode = True)

        # Compare files
        self.assertEquals(self.file_checksum(self.temp_filename2), self.file_checksum(self.temp_filename1), CHKSUM_ERR.format(self.ext))

        # Check we can't save when file handle is not binary
        with self.assertRaises(ValueError):
            save_by_filehandle(self.temp_filename1, self.temp_filename2, self.cube1, pp.save, binary_mode = False)


class TestSaveDot(TestSaveMethods):
    """Test saving cubes to DOT format"""
    ext = ".dot"

    def test_filename(self):
        # Save using iris.save and dot.save
        save_by_filename(self.temp_filename1, self.temp_filename2, self.cube1, dot.save)

        # Compare files
        self.assertEquals(self.file_checksum(self.temp_filename2), self.file_checksum(self.temp_filename1), CHKSUM_ERR.format(self.ext))

    def test_filehandle(self):
        # Save using iris.save and dot.save
        save_by_filehandle(self.temp_filename1, self.temp_filename2, self.cube1, dot.save, binary_mode = False)

        # Compare files
        self.assertEquals(self.file_checksum(self.temp_filename2), self.file_checksum(self.temp_filename1), CHKSUM_ERR.format(self.ext))

        # Check we can't save when file handle is binary
        with self.assertRaises(ValueError):
            save_by_filehandle(self.temp_filename1, self.temp_filename2, self.cube1, dot.save, binary_mode = True)

    def test_cstringio(self):
        string_io = cStringIO.StringIO()

        # Save from dot direct
        dot.save(self.cube1, self.temp_filename1)

        # Call save on iris
        iris.save(self.cube1, string_io, iris.io.find_saver(self.ext))

        with open(self.temp_filename1) as infile:
            data = infile.read()

        # Compare files
        self.assertEquals(data, string_io.getvalue(), "Mismatch in data when comparing iris cstringio save and dot.save.")


@skip_dotpng
class TestSavePng(TestSaveMethods):
    """Test saving cubes to png"""
    ext = ".dotpng"

    def test_filename(self):
        # Save using iris.save and dot.save_png
        save_by_filename(self.temp_filename1, self.temp_filename2, self.cube1, dot.save_png)

        # Compare files
        self.assertEquals(self.file_checksum(self.temp_filename2), self.file_checksum(self.temp_filename1), CHKSUM_ERR.format(self.ext))

    def test_filehandle(self):
        # Save using iris.save and dot.save_png
        save_by_filehandle(self.temp_filename1, self.temp_filename2, self.cube1, dot.save_png, binary_mode = True)

        # Compare files
        self.assertEquals(self.file_checksum(self.temp_filename2), self.file_checksum(self.temp_filename1), CHKSUM_ERR.format(self.ext))

        # Check we can't save when file handle is not binary
        with self.assertRaises(ValueError):
            save_by_filehandle(self.temp_filename1, self.temp_filename2, self.cube1, dot.save_png, binary_mode = False)

class TestSaver(TestSaveMethods):
    """Test saving to Iris when we define the saver type to use"""
    ext = ".spam"

    def test_pp(self):
        # Make our own saver
        pp_saver = iris.io.find_saver("PP")
        save_by_filename(self.temp_filename1, self.temp_filename2, self.cube1, pp.save, pp_saver)

        # Compare files
        self.assertEquals(self.file_checksum(self.temp_filename2), self.file_checksum(self.temp_filename1), CHKSUM_ERR.format(self.ext))

    def test_dot(self):
        # Make our own saver
        dot_saver = iris.io.find_saver("DOT")
        save_by_filename(self.temp_filename1, self.temp_filename2, self.cube1, dot.save, dot_saver)

        # Compare files
        self.assertEquals(self.file_checksum(self.temp_filename2), self.file_checksum(self.temp_filename1), CHKSUM_ERR.format(self.ext))

    @skip_dotpng
    def test_png(self):
        # Make our own saver
        png_saver = iris.io.find_saver("DOTPNG")
        save_by_filename(self.temp_filename1, self.temp_filename2, self.cube1, dot.save_png, png_saver)

        # Compare files
        self.assertEquals(self.file_checksum(self.temp_filename2), self.file_checksum(self.temp_filename1), CHKSUM_ERR.format(self.ext))

class TestSaveInvalid(TestSaveMethods):
    """Test iris cannot automatically save to file extensions it does not know about"""
    ext = ".invalid"

    def test_filename(self):
        # Check we can't save a file with an unhandled extension
        with self.assertRaises(ValueError):
            iris.save(self.cube1, self.temp_filename2)

if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_grib_load
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

# Import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import datetime
import os

import gribapi
import mock
import numpy as np

import iris
import iris.exceptions
import iris.fileformats.grib
import iris.tests.stock
import iris.util

# Run tests in no graphics mode if matplotlib is not available.
if tests.MPL_AVAILABLE:
    import matplotlib.pyplot as plt
    from matplotlib.colors import LogNorm
    import iris.plot as iplt
    import iris.quickplot as qplt


# Construct a mock object to mimic the gribapi for GribWrapper testing.
_mock_gribapi = mock.Mock(spec=gribapi)
_mock_gribapi.GribInternalError = Exception


def _mock_gribapi_fetch(message, key):
    """
    Fake the gribapi key-fetch.

    Fetch key-value from the fake message (dictionary).
    If the key is not present, raise the diagnostic exception.

    """
    if key in message.keys():
        return message[key]
    else:
        raise _mock_gribapi.GribInternalError


def _mock_gribapi__grib_is_missing(grib_message, keyname):
    """
    Fake the gribapi key-existence enquiry.

    Return whether the key exists in the fake message (dictionary).

    """
    return (keyname not in grib_message)


def _mock_gribapi__grib_get_native_type(grib_message, keyname):
    """
    Fake the gribapi type-discovery operation.

    Return type of key-value in the fake message (dictionary).
    If the key is not present, raise the diagnostic exception.

    """
    if keyname in grib_message:
        return type(grib_message[keyname])
    raise _mock_gribapi.GribInternalError(keyname)

_mock_gribapi.grib_get_long = mock.Mock(side_effect=_mock_gribapi_fetch)
_mock_gribapi.grib_get_string = mock.Mock(side_effect=_mock_gribapi_fetch)
_mock_gribapi.grib_get_double = mock.Mock(side_effect=_mock_gribapi_fetch)
_mock_gribapi.grib_get_double_array = mock.Mock(
    side_effect=_mock_gribapi_fetch)
_mock_gribapi.grib_is_missing = mock.Mock(
    side_effect=_mock_gribapi__grib_is_missing)
_mock_gribapi.grib_get_native_type = mock.Mock(
    side_effect=_mock_gribapi__grib_get_native_type)

# define seconds in an hour, for general test usage
_hour_secs = 3600.0


class FakeGribMessage(dict):
    """
    A 'fake grib message' object, for testing GribWrapper construction.

    Behaves as a dictionary, containing key-values for message keys.

    """
    def __init__(self, **kwargs):
        """
        Create a fake message object.

        General keys can be set/add as required via **kwargs.
        The keys 'edition' and 'time_code' are specially managed.

        """
        # Start with a bare dictionary
        dict.__init__(self)
        # Extract specially-recognised keys.
        edition = kwargs.pop('edition', 1)
        time_code = kwargs.pop('time_code', None)
        # Set the minimally required keys.
        self._init_minimal_message(edition=edition)
        # Also set a time-code, if given.
        if time_code is not None:
            self.set_timeunit_code(time_code)
        # Finally, add any remaining passed key-values.
        self.update(**kwargs)

    def _init_minimal_message(self, edition=1):
        # Set values for all the required keys.
        # 'edition' controls the edition-specific keys.
        self.update({
            'Ni': 1,
            'Nj': 1,
            'alternativeRowScanning': 0,
            'centre': 'ecmf',
            'year': 2007,
            'month': 3,
            'day': 23,
            'hour': 12,
            'minute': 0,
            'indicatorOfUnitOfTimeRange': 1,
            'shapeOfTheEarth': 6,
            'gridType': 'rotated_ll',
            'angleOfRotation': 0.0,
            'iDirectionIncrementInDegrees': 0.036,
            'jDirectionIncrementInDegrees': 0.036,
            'iScansNegatively': 0,
            'jScansPositively': 1,
            'longitudeOfFirstGridPointInDegrees': -5.70,
            'latitudeOfFirstGridPointInDegrees': -4.452,
            'jPointsAreConsecutive': 0,
            'values': np.array([[1.0]]),
            'indicatorOfParameter': 9999,
            'parameterNumber': 9999,
        })
        # Add edition-dependent settings.
        self['edition'] = edition
        if edition == 1:
            self.update({
                'startStep': 24,
                'timeRangeIndicator': 1,
                'P1': 2, 'P2': 0,
                # time unit - needed AS WELL as 'indicatorOfUnitOfTimeRange'
                'unitOfTime': 1,
                'table2Version': 9999,
            })
        if edition == 2:
            self.update({
                'iDirectionIncrementGiven': 1,
                'jDirectionIncrementGiven': 1,
                'uvRelativeToGrid': 0,
                'forecastTime': 24,
                'productDefinitionTemplateNumber': 0,
                'stepRange': 24,
                'discipline': 9999,
                'parameterCategory': 9999,
                'tablesVersion': 4
            })

    def set_timeunit_code(self, timecode):
        # Do timecode setting (somewhat edition-dependent).
        self['indicatorOfUnitOfTimeRange'] = timecode
        if self['edition'] == 1:
            # for some odd reason, GRIB1 code uses *both* of these
            # NOTE kludge -- the 2 keys are really the same thing
            self['unitOfTime'] = timecode


@tests.skip_data
class TestGribLoad(tests.GraphicsTest):

    def setUp(self):
        iris.fileformats.grib.hindcast_workaround = True

    def tearDown(self):
        iris.fileformats.grib.hindcast_workaround = False

    def test_load(self):

        cubes = iris.load(tests.get_data_path(('GRIB', 'rotated_uk',
                                               "uk_wrongparam.grib1")))
        self.assertCML(cubes, ("grib_load", "rotated.cml"))

        cubes = iris.load(tests.get_data_path(('GRIB', "time_processed",
                                               "time_bound.grib1")))
        self.assertCML(cubes, ("grib_load", "time_bound_grib1.cml"))

        cubes = iris.load(tests.get_data_path(('GRIB', "time_processed",
                                               "time_bound.grib2")))
        self.assertCML(cubes, ("grib_load", "time_bound_grib2.cml"))

        cubes = iris.load(tests.get_data_path(('GRIB', "3_layer_viz",
                                               "3_layer.grib2")))
        cubes = iris.cube.CubeList([cubes[1], cubes[0], cubes[2]])
        self.assertCML(cubes, ("grib_load", "3_layer.cml"))

    def test_load_masked(self):

        gribfile = tests.get_data_path(
            ('GRIB', 'missing_values', 'missing_values.grib2'))
        cubes = iris.load(gribfile)
        self.assertCML(cubes, ('grib_load', 'missing_values_grib2.cml'))

    @tests.skip_plot
    def test_y_fastest(self):
        cubes = iris.load(tests.get_data_path(("GRIB", "y_fastest",
                                               "y_fast.grib2")))
        self.assertCML(cubes, ("grib_load", "y_fastest.cml"))
        iplt.contourf(cubes[0])
        plt.gca().coastlines()
        plt.title("y changes fastest")
        self.check_graphic()

    @tests.skip_plot
    def test_ij_directions(self):

        def old_compat_load(name):
            cube = iris.load(tests.get_data_path(('GRIB', 'ij_directions',
                                                  name)))[0]
            return [cube]

        cubes = old_compat_load("ipos_jpos.grib2")
        self.assertCML(cubes, ("grib_load", "ipos_jpos.cml"))
        iplt.contourf(cubes[0])
        plt.gca().coastlines()
        plt.title("ipos_jpos cube")
        self.check_graphic()

        cubes = old_compat_load("ipos_jneg.grib2")
        self.assertCML(cubes, ("grib_load", "ipos_jneg.cml"))
        iplt.contourf(cubes[0])
        plt.gca().coastlines()
        plt.title("ipos_jneg cube")
        self.check_graphic()

        cubes = old_compat_load("ineg_jneg.grib2")
        self.assertCML(cubes, ("grib_load", "ineg_jneg.cml"))
        iplt.contourf(cubes[0])
        plt.gca().coastlines()
        plt.title("ineg_jneg cube")
        self.check_graphic()

        cubes = old_compat_load("ineg_jpos.grib2")
        self.assertCML(cubes, ("grib_load", "ineg_jpos.cml"))
        iplt.contourf(cubes[0])
        plt.gca().coastlines()
        plt.title("ineg_jpos cube")
        self.check_graphic()

    def test_shape_of_earth(self):

        def old_compat_load(name):
            cube = iris.load(tests.get_data_path(('GRIB', 'shape_of_earth',
                                                  name)))[0]
            return cube

        # pre-defined sphere
        cube = old_compat_load("0.grib2")
        self.assertCML(cube, ("grib_load", "earth_shape_0.cml"))

        # custom sphere
        cube = old_compat_load("1.grib2")
        self.assertCML(cube, ("grib_load", "earth_shape_1.cml"))

        # IAU65 oblate sphere
        cube = old_compat_load("2.grib2")
        self.assertCML(cube, ("grib_load", "earth_shape_2.cml"))

        # custom oblate spheroid (km)
        cube = old_compat_load("3.grib2")
        self.assertCML(cube, ("grib_load", "earth_shape_3.cml"))

        # IAG-GRS80 oblate spheroid
        cube = old_compat_load("4.grib2")
        self.assertCML(cube, ("grib_load", "earth_shape_4.cml"))

        # WGS84
        cube = old_compat_load("5.grib2")
        self.assertCML(cube, ("grib_load", "earth_shape_5.cml"))

        # pre-defined sphere
        cube = old_compat_load("6.grib2")
        self.assertCML(cube, ("grib_load", "earth_shape_6.cml"))

        # custom oblate spheroid (m)
        cube = old_compat_load("7.grib2")
        self.assertCML(cube, ("grib_load", "earth_shape_7.cml"))

        # grib1 - same as grib2 shape 6, above
        cube = old_compat_load("global.grib1")
        self.assertCML(cube, ("grib_load", "earth_shape_grib1.cml"))

    def test_custom_rules(self):
        # Test custom rule evaluation.
        # Default behaviour
        cube = tests.stock.global_grib2()
        self.assertEqual(cube.name(), 'air_temperature')

        # Custom behaviour
        temp_path = iris.util.create_temp_filename()
        f = open(temp_path, 'w')
        f.write('\n'.join((
            'IF',
            'grib.edition == 2',
            'grib.discipline == 0',
            'grib.parameterCategory == 0',
            'grib.parameterNumber == 0',
            'THEN',
            'CMAttribute("long_name", "customised")',
            'CMAttribute("standard_name", None)')))
        f.close()
        iris.fileformats.grib.add_load_rules(temp_path)
        cube = tests.stock.global_grib2()
        self.assertEqual(cube.name(), 'customised')
        os.remove(temp_path)

        # Back to default
        iris.fileformats.grib.reset_load_rules()
        cube = tests.stock.global_grib2()
        self.assertEqual(cube.name(), 'air_temperature')

    @tests.skip_plot
    def test_polar_stereo_grib1(self):
        cube = iris.load_cube(tests.get_data_path(
            ("GRIB", "polar_stereo", "ST4.2013052210.01h")))
        self.assertCML(cube, ("grib_load", "polar_stereo_grib1.cml"))
        qplt.contourf(cube, norm=LogNorm())
        plt.gca().coastlines()
        plt.gca().gridlines()
        plt.title("polar stereo grib1")
        self.check_graphic()

    @tests.skip_plot
    def test_polar_stereo_grib2(self):
        cube = iris.load_cube(tests.get_data_path(
            ("GRIB", "polar_stereo",
             "CMC_glb_TMP_ISBL_1015_ps30km_2013052000_P006.grib2")))
        self.assertCML(cube, ("grib_load", "polar_stereo_grib2.cml"))

        qplt.contourf(cube)
        plt.gca().coastlines()
        plt.gca().gridlines()
        plt.title("polar stereo grib2")
        self.check_graphic()

    @tests.skip_plot
    def test_lambert_grib1(self):
        cube = iris.load_cube(tests.get_data_path(
            ("GRIB", "lambert", "lambert.grib1")))
        self.assertCML(cube, ("grib_load", "lambert_grib1.cml"))

        qplt.contourf(cube)
        plt.gca().coastlines()
        plt.gca().gridlines()
        plt.title("lambert grib1")
        self.check_graphic()

    @tests.skip_plot
    def test_lambert_grib2(self):
        cube = iris.load_cube(tests.get_data_path(
            ("GRIB", "lambert", "lambert.grib2")))
        self.assertCML(cube, ("grib_load", "lambert_grib2.cml"))

        qplt.contourf(cube)
        plt.gca().coastlines()
        plt.gca().gridlines()
        plt.title("lambert grib2")
        self.check_graphic()

    def test_regular_gg_grib1(self):
        cube = iris.load_cube(tests.get_data_path(
            ('GRIB', 'gaussian', 'regular_gg.grib1')))
        self.assertCML(cube, ('grib_load', 'regular_gg_grib1.cml'))

    def test_regular_gg_grib2(self):
        cube = iris.load_cube(tests.get_data_path(
            ('GRIB', 'gaussian', 'regular_gg.grib2')))
        self.assertCML(cube, ('grib_load', 'regular_gg_grib2.cml'))

    def test_reduced_ll(self):
        cube = iris.load_cube(tests.get_data_path(
            ("GRIB", "reduced", "reduced_ll.grib1")))
        self.assertCML(cube, ("grib_load", "reduced_ll_grib1.cml"))

    def test_reduced_gg(self):
        cube = iris.load_cube(tests.get_data_path(
            ("GRIB", "reduced", "reduced_gg.grib2")))
        self.assertCML(cube, ("grib_load", "reduced_gg_grib2.cml"))

    def test_reduced_missing(self):
        cube = iris.load_cube(tests.get_data_path(
            ("GRIB", "reduced", "reduced_ll_missing.grib1")))
        self.assertCML(cube, ("grib_load", "reduced_ll_missing_grib1.cml"))


class TestGribTimecodes(tests.IrisTest):
    def _run_timetests(self, test_set):
        # Check the unit-handling for given units-codes and editions.

        # Operates on lists of cases for various time-units and grib-editions.
        # Format: (edition, code, expected-exception,
        #          equivalent-seconds, description-string)
        with mock.patch('iris.fileformats.grib.gribapi', _mock_gribapi):
            for test_controls in test_set:
                (
                    grib_edition, timeunit_codenum,
                    expected_error,
                    timeunit_secs, timeunit_str
                ) = test_controls

                # Construct a suitable fake test message.
                message = FakeGribMessage(
                    edition=grib_edition,
                    time_code=timeunit_codenum
                )

                if expected_error:
                    # Expect GribWrapper construction to fail.
                    with self.assertRaises(type(expected_error)) as ar_context:
                        msg = iris.fileformats.grib.GribWrapper(message)
                    self.assertEqual(
                        ar_context.exception.args,
                        expected_error.args)
                    continue

                # 'ELSE'...
                # Expect the wrapper construction to work.
                # Make a GribWrapper object and test it.
                wrapped_msg = iris.fileformats.grib.GribWrapper(message)

                # Check the units string.
                forecast_timeunit = wrapped_msg._forecastTimeUnit
                self.assertEqual(
                    forecast_timeunit, timeunit_str,
                    'Bad unit string for edition={ed:01d}, '
                    'unitcode={code:01d} : '
                    'expected="{wanted}" GOT="{got}"'.format(
                        ed=grib_edition,
                        code=timeunit_codenum,
                        wanted=timeunit_str,
                        got=forecast_timeunit
                    )
                )

                # Check the data-starttime calculation.
                interval_start_to_end = (
                    wrapped_msg._phenomenonDateTime
                    - wrapped_msg._referenceDateTime
                )
                if grib_edition == 1:
                    interval_from_units = wrapped_msg.P1
                else:
                    interval_from_units = wrapped_msg.forecastTime
                interval_from_units *= datetime.timedelta(0, timeunit_secs)
                self.assertEqual(
                    interval_start_to_end, interval_from_units,
                    'Inconsistent start time offset for edition={ed:01d}, '
                    'unitcode={code:01d} : '
                    'from-unit="{unit_str}" '
                    'from-phenom-minus-ref="{e2e_str}"'.format(
                        ed=grib_edition,
                        code=timeunit_codenum,
                        unit_str=interval_from_units,
                        e2e_str=interval_start_to_end
                    )
                )

    # Test groups of testcases for various time-units and grib-editions.
    # Format: (edition, code, expected-exception,
    #          equivalent-seconds, description-string)
    def test_timeunits_common(self):
        tests = (
            (1, 0, None, 60.0, 'minutes'),
            (1, 1, None, _hour_secs, 'hours'),
            (1, 2, None, 24.0 * _hour_secs, 'days'),
            (1, 10, None, 3.0 * _hour_secs, '3 hours'),
            (1, 11, None, 6.0 * _hour_secs, '6 hours'),
            (1, 12, None, 12.0 * _hour_secs, '12 hours'),
        )
        TestGribTimecodes._run_timetests(self, tests)

    @staticmethod
    def _err_bad_timeunit(code):
        return iris.exceptions.NotYetImplementedError(
            'Unhandled time unit for forecast '
            'indicatorOfUnitOfTimeRange : {code}'.format(code=code)
        )

    def test_timeunits_grib1_specific(self):
        tests = (
            (1, 13, None, 0.25 * _hour_secs, '15 minutes'),
            (1, 14, None, 0.5 * _hour_secs, '30 minutes'),
            (1, 254, None, 1.0, 'seconds'),
            (1, 111, TestGribTimecodes._err_bad_timeunit(111), 1.0, '??'),
        )
        TestGribTimecodes._run_timetests(self, tests)

    def test_timeunits_grib2_specific(self):
        tests = (
            (2, 13, None, 1.0, 'seconds'),
            # check the extra grib1 keys FAIL
            (2, 14, TestGribTimecodes._err_bad_timeunit(14), 0.0, '??'),
            (2, 254, TestGribTimecodes._err_bad_timeunit(254), 0.0, '??'),
        )
        TestGribTimecodes._run_timetests(self, tests)

    def test_timeunits_calendar(self):
        tests = (
            (1, 3, TestGribTimecodes._err_bad_timeunit(3), 0.0, 'months'),
            (1, 4, TestGribTimecodes._err_bad_timeunit(4), 0.0, 'years'),
            (1, 5, TestGribTimecodes._err_bad_timeunit(5), 0.0, 'decades'),
            (1, 6, TestGribTimecodes._err_bad_timeunit(6), 0.0, '30 years'),
            (1, 7, TestGribTimecodes._err_bad_timeunit(7), 0.0, 'centuries'),
        )
        TestGribTimecodes._run_timetests(self, tests)

    def test_timeunits_invalid(self):
        tests = (
            (1, 111, TestGribTimecodes._err_bad_timeunit(111), 1.0, '??'),
            (2, 27, TestGribTimecodes._err_bad_timeunit(27), 1.0, '??'),
        )
        TestGribTimecodes._run_timetests(self, tests)

    def test_load_probability_forecast(self):
        # Test GribWrapper interpretation of PDT 4.9 data.
        # NOTE:
        #   Currently Iris has only partial support for PDT 4.9.
        #   Though it can load the data, key metadata (thresholds) is lost.
        #   At present, we are not testing for this.

        # Make a testing grib message in memory, with gribapi.
        grib_message = gribapi.grib_new_from_samples('GRIB2')
        gribapi.grib_set_long(grib_message, 'productDefinitionTemplateNumber',
                              9)
        gribapi.grib_set_string(grib_message, 'stepRange', '10-55')
        grib_wrapper = iris.fileformats.grib.GribWrapper(grib_message)

        # Check that it captures the statistics time period info.
        # (And for now, nothing else)
        self.assertEqual(
            grib_wrapper._referenceDateTime,
            datetime.datetime(year=2007, month=03, day=23,
                              hour=12, minute=0, second=0)
        )
        self.assertEqual(
            grib_wrapper._periodStartDateTime,
            datetime.datetime(year=2007, month=03, day=23,
                              hour=22, minute=0, second=0)
        )
        self.assertEqual(
            grib_wrapper._periodEndDateTime,
            datetime.datetime(year=2007, month=03, day=25,
                              hour=12, minute=0, second=0)
        )

    def test_warn_unknown_pdts(self):
        # Test loading of an unrecognised GRIB Product Definition Template.

        # Get a temporary file by name (deleted afterward by context).
        with self.temp_filename() as temp_gribfile_path:
            # Write a test grib message to the temporary file.
            with open(temp_gribfile_path, 'wb') as temp_gribfile:
                grib_message = gribapi.grib_new_from_samples('GRIB2')
                # Set the PDT to something unexpected.
                gribapi.grib_set_long(
                    grib_message, 'productDefinitionTemplateNumber', 5)
                gribapi.grib_write(grib_message, temp_gribfile)

            # Load the message from the file as a cube.
            cube_generator = iris.fileformats.grib.load_cubes(
                temp_gribfile_path)
            cube = cube_generator.next()

            # Check the cube has an extra "warning" attribute.
            self.assertEqual(
                cube.attributes['GRIB_LOAD_WARNING'],
                'unsupported GRIB2 ProductDefinitionTemplate: #4.5'
            )


class TestGribSimple(tests.IrisTest):
    # A testing class that does not need the test data.
    def mock_grib(self):
        # A mock grib message, with attributes that can't be Mocks themselves.
        grib = mock.Mock()
        grib.startStep = 0
        grib.phenomenon_points = lambda unit: 3
        grib._forecastTimeUnit = "hours"
        grib.productDefinitionTemplateNumber = 0
        # define a level type (NB these 2 are effectively the same)
        grib.levelType = 1
        grib.typeOfFirstFixedSurface = 1
        grib.typeOfSecondFixedSurface = 1
        return grib

    def cube_from_message(self, grib):
        # Parameter translation now uses the GribWrapper, so we must convert
        # the Mock-based fake message to a FakeGribMessage.
        with mock.patch('iris.fileformats.grib.gribapi', _mock_gribapi):
                grib_message = FakeGribMessage(**grib.__dict__)
                wrapped_msg = iris.fileformats.grib.GribWrapper(grib_message)
                cube, _, _ = iris.fileformats.rules._make_cube(
                    wrapped_msg, iris.fileformats.grib.load_rules.convert)
        return cube


class TestGrib1LoadPhenomenon(TestGribSimple):
    # Test recognition of grib phenomenon types.
    def mock_grib(self):
        grib = super(TestGrib1LoadPhenomenon, self).mock_grib()
        grib.edition = 1
        return grib

    def test_grib1_unknownparam(self):
        grib = self.mock_grib()
        grib.table2Version = 0
        grib.indicatorOfParameter = 9999
        cube = self.cube_from_message(grib)
        self.assertEqual(cube.standard_name, None)
        self.assertEqual(cube.long_name, None)
        self.assertEqual(cube.units, iris.unit.Unit("???"))

    def test_grib1_unknown_local_param(self):
        grib = self.mock_grib()
        grib.table2Version = 128
        grib.indicatorOfParameter = 999
        cube = self.cube_from_message(grib)
        self.assertEqual(cube.standard_name, None)
        self.assertEqual(cube.long_name, 'UNKNOWN LOCAL PARAM 999.128')
        self.assertEqual(cube.units, iris.unit.Unit("???"))

    def test_grib1_unknown_standard_param(self):
        grib = self.mock_grib()
        grib.table2Version = 1
        grib.indicatorOfParameter = 975
        cube = self.cube_from_message(grib)
        self.assertEqual(cube.standard_name, None)
        self.assertEqual(cube.long_name, 'UNKNOWN LOCAL PARAM 975.1')
        self.assertEqual(cube.units, iris.unit.Unit("???"))

    def known_grib1(self, param, standard_str, units_str):
        grib = self.mock_grib()
        grib.table2Version = 1
        grib.indicatorOfParameter = param
        cube = self.cube_from_message(grib)
        self.assertEqual(cube.standard_name, standard_str)
        self.assertEqual(cube.long_name, None)
        self.assertEqual(cube.units, iris.unit.Unit(units_str))

    def test_grib1_known_standard_params(self):
        # at present, there are just a very few of these
        self.known_grib1(11, 'air_temperature', 'kelvin')
        self.known_grib1(33, 'x_wind', 'm s-1')
        self.known_grib1(34, 'y_wind', 'm s-1')


class TestGrib2LoadPhenomenon(TestGribSimple):
    # Test recognition of grib phenomenon types.
    def mock_grib(self):
        grib = super(TestGrib2LoadPhenomenon, self).mock_grib()
        grib.edition = 2
        grib._forecastTimeUnit = 'hours'
        grib._forecastTime = 0.0
        grib.phenomenon_points = lambda unit: [0.0]
        return grib

    def known_grib2(self, discipline, category, param,
                    standard_name, long_name, units_str):
        grib = self.mock_grib()
        grib.discipline = discipline
        grib.parameterCategory = category
        grib.parameterNumber = param
        cube = self.cube_from_message(grib)
        try:
            iris_unit = iris.unit.Unit(units_str)
        except ValueError:
            iris_unit = iris.unit.Unit('???')
        self.assertEqual(cube.standard_name, standard_name)
        self.assertEqual(cube.long_name, long_name)
        self.assertEqual(cube.units, iris_unit)

    def test_grib2_unknownparam(self):
        grib = self.mock_grib()
        grib.discipline = 999
        grib.parameterCategory = 999
        grib.parameterNumber = 9999
        cube = self.cube_from_message(grib)
        self.assertEqual(cube.standard_name, None)
        self.assertEqual(cube.long_name, None)
        self.assertEqual(cube.units, iris.unit.Unit("???"))

    def test_grib2_known_standard_params(self):
        # check we know how to translate at least these params
        # I.E. all the ones the older scheme provided.
        full_set = [
            (0, 0, 0, "air_temperature", None, "kelvin"),
            (0, 0, 2, "air_potential_temperature", None, "K"),
            (0, 1, 0, "specific_humidity", None, "kg kg-1"),
            (0, 1, 1, "relative_humidity", None, "%"),
            (0, 1, 3, None, "precipitable_water", "kg m-2"),
            (0, 1, 22, None, "cloud_mixing_ratio", "kg kg-1"),
            (0, 1, 13, "liquid_water_content_of_surface_snow", None, "kg m-2"),
            (0, 2, 1, "wind_speed", None, "m s-1"),
            (0, 2, 2, "x_wind", None, "m s-1"),
            (0, 2, 3, "y_wind", None, "m s-1"),
            (0, 2, 8, "lagrangian_tendency_of_air_pressure", None, "Pa s-1"),
            (0, 2, 10, "atmosphere_absolute_vorticity", None, "s-1"),
            (0, 3, 0, "air_pressure", None, "Pa"),
            (0, 3, 1, "air_pressure_at_sea_level", None, "Pa"),
            (0, 3, 3, None, "icao_standard_atmosphere_reference_height", "m"),
            (0, 3, 5, "geopotential_height", None, "m"),
            (0, 3, 9, "geopotential_height_anomaly", None, "m"),
            (0, 6, 1, "cloud_area_fraction", None, "%"),
            (0, 6, 6, "atmosphere_mass_content_of_cloud_liquid_water", None,
                "kg m-2"),
            (0, 7, 6,
             "atmosphere_specific_convective_available_potential_energy",
             None, "J kg-1"),
            (0, 7, 7, None, "convective_inhibition", "J kg-1"),
            (0, 7, 8, None, "storm_relative_helicity", "J kg-1"),
            (0, 14, 0, "atmosphere_mole_content_of_ozone", None, "Dobson"),
            (2, 0, 0, "land_area_fraction", None, "1"),
            (10, 2, 0, "sea_ice_area_fraction", None, "1")]

        for (discipline, category, number,
             standard_name, long_name, units) in full_set:
            self.known_grib2(discipline, category, number,
                             standard_name, long_name, units)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_grib_phenomenon_translations
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
'''
Created on Apr 26, 2013

@author: itpp
'''
# Import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as itests

import iris.fileformats.grib.grib_phenom_translation as gptx
import iris.unit


class TestGribLookupTableType(itests.IrisTest):
    def test_lookuptable_type(self):
        ll = gptx.LookupTable([('a', 1), ('b', 2)])
        assert ll['a'] == 1
        assert ll['q'] is None
        ll['q'] = 15
        assert ll['q'] == 15
        ll['q'] = 15
        assert ll['q'] == 15
        with self.assertRaises(KeyError):
            ll['q'] = 7
        del ll['q']
        ll['q'] = 7
        assert ll['q'] == 7


class TestGribPhenomenonLookup(itests.IrisTest):
    def test_grib1_cf_lookup(self):
        def check_grib1_cf(param,
                           standard_name, long_name, units,
                           height=None,
                           t2version=128, centre=98, expect_none=False):
            iris_units = iris.unit.Unit(units)
            cfdata = gptx.grib1_phenom_to_cf_info(param_number=param,
                                                  table2_version=t2version,
                                                  centre_number=centre)
            if expect_none:
                self.assertIsNone(cfdata)
            else:
                self.assertEqual(cfdata.standard_name, standard_name)
                self.assertEqual(cfdata.long_name, long_name)
                self.assertEqual(cfdata.units, iris_units)
                if height is None:
                    self.assertIsNone(cfdata.set_height)
                else:
                    self.assertEqual(cfdata.set_height, float(height))

        check_grib1_cf(165, 'x_wind', None, 'm s-1', 10.0)
        check_grib1_cf(168, 'dew_point_temperature', None, 'K', 2)
        check_grib1_cf(130, 'air_temperature', None, 'K')
        check_grib1_cf(235, None, "grib_skin_temperature", "K")
        check_grib1_cf(235, None, "grib_skin_temperature", "K",
                       t2version=9999, expect_none=True)
        check_grib1_cf(235, None, "grib_skin_temperature", "K",
                       centre=9999, expect_none=True)
        check_grib1_cf(9999, None, "grib_skin_temperature", "K",
                       expect_none=True)

    def test_grib2_cf_lookup(self):
        def check_grib2_cf(discipline, category, number,
                           standard_name, long_name, units,
                           expect_none=False):
            iris_units = iris.unit.Unit(units)
            cfdata = gptx.grib2_phenom_to_cf_info(param_discipline=discipline,
                                                  param_category=category,
                                                  param_number=number)
            if expect_none:
                self.assertIsNone(cfdata)
            else:
                self.assertEqual(cfdata.standard_name, standard_name)
                self.assertEqual(cfdata.long_name, long_name)
                self.assertEqual(cfdata.units, iris_units)

        # These should work
        check_grib2_cf(0, 0, 2, "air_potential_temperature", None, "K")
        check_grib2_cf(0, 19, 1, None, "grib_physical_atmosphere_albedo", "%")
        check_grib2_cf(2, 0, 2, "soil_temperature", None, "K")
        check_grib2_cf(10, 2, 0, "sea_ice_area_fraction", None, 1)
        check_grib2_cf(2, 0, 0, "land_area_fraction", None, 1)
        check_grib2_cf(0, 19, 1, None, "grib_physical_atmosphere_albedo", "%")
        check_grib2_cf(0, 1, 64,
                       "atmosphere_mass_content_of_water_vapor", None,
                       "kg m-2")
        check_grib2_cf(2, 0, 7, "surface_altitude", None, "m")

        # These should fail
        check_grib2_cf(9999, 2, 0, "sea_ice_area_fraction", None, 1,
                       expect_none=True)
        check_grib2_cf(10, 9999, 0, "sea_ice_area_fraction", None, 1,
                       expect_none=True)
        check_grib2_cf(10, 2, 9999, "sea_ice_area_fraction", None, 1,
                       expect_none=True)

    def test_cf_grib2_lookup(self):
        def check_cf_grib2(standard_name, long_name,
                           discipline, category, number, units,
                           expect_none=False):
            iris_units = iris.unit.Unit(units)
            gribdata = gptx.cf_phenom_to_grib2_info(standard_name, long_name)
            if expect_none:
                self.assertIsNone(gribdata)
            else:
                self.assertEqual(gribdata.discipline, discipline)
                self.assertEqual(gribdata.category, category)
                self.assertEqual(gribdata.number, number)
                self.assertEqual(gribdata.units, iris_units)

        # These should work
        check_cf_grib2("sea_surface_temperature", None,
                       10, 3, 0, 'K')
        check_cf_grib2("air_temperature", None,
                       0, 0, 0, 'K')
        check_cf_grib2("soil_temperature", None,
                       2, 0, 2, "K")
        check_cf_grib2("land_area_fraction", None,
                       2, 0, 0, '1')
        check_cf_grib2("land_binary_mask", None,
                       2, 0, 0, '1')
        check_cf_grib2("atmosphere_mass_content_of_water_vapor", None,
                       0, 1, 64, "kg m-2")
        check_cf_grib2("surface_altitude", None,
                       2, 0, 7, "m")

        # These should fail
        check_cf_grib2("air_temperature", "user_long_UNRECOGNISED",
                       0, 0, 0, 'K')
        check_cf_grib2("air_temperature_UNRECOGNISED", None,
                       0, 0, 0, 'K',
                       expect_none=True)
        check_cf_grib2(None, "user_long_UNRECOGNISED",
                       0, 0, 0, 'K',
                       expect_none=True)
        check_cf_grib2(None, "precipitable_water",
                       0, 1, 3, 'kg m-2')
        check_cf_grib2("invalid_unknown", "precipitable_water",
                       0, 1, 3, 'kg m-2',
                       expect_none=True)
        check_cf_grib2(None, None, 0, 0, 0, '',
                       expect_none=True)


if __name__ == '__main__':
    itests.main()

########NEW FILE########
__FILENAME__ = test_grib_save
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import os
import warnings
import datetime

import gribapi
import numpy as np

import iris
import iris.cube
import iris.coord_systems
import iris.coords


@tests.skip_data
class TestLoadSave(tests.IrisTest):
    # load and save grib

    def setUp(self):
        iris.fileformats.grib.hindcast_workaround = True

    def tearDown(self):
        iris.fileformats.grib.hindcast_workaround = False

    def save_and_compare(self, source_grib, reference_text):
        """Load and save grib data, generate diffs, compare with expected diffs."""

        # load and save from Iris
        cubes = iris.load(source_grib)

        saved_grib = iris.util.create_temp_filename(suffix='.grib2')
        iris.save(cubes, saved_grib)

        # missing reference? (the expected diffs between source_grib and saved_grib)
        if not os.path.exists(reference_text):
            warnings.warn("Creating grib compare reference %s" % reference_text)
            os.system("grib_compare %s %s > %s" % (source_grib, saved_grib, reference_text))

        # generate and compare diffs
        compare_text = iris.util.create_temp_filename(suffix='.grib_compare.txt')
        os.system("grib_compare %s %s > %s" % (source_grib, saved_grib, compare_text))
        self.assertTextFile(compare_text, reference_text, "grib_compare output")

        os.remove(saved_grib)
        os.remove(compare_text)

    def test_latlon_forecast_plev(self):
        source_grib = tests.get_data_path(("GRIB", "uk_t", "uk_t.grib2"))
        reference_text = tests.get_result_path(("grib_save", "latlon_forecast_plev.grib_compare.txt"))
        self.save_and_compare(source_grib, reference_text)

    def test_rotated_latlon(self):
        source_grib = tests.get_data_path(("GRIB", "rotated_nae_t", "sensible_pole.grib2"))
        reference_text = tests.get_result_path(("grib_save", "rotated_latlon.grib_compare.txt"))
        # TODO: Investigate small change in test result:
        #       long [iDirectionIncrement]: [109994] != [109993]
        #       Consider the change in dx_dy() to "InDegrees" too.
        self.save_and_compare(source_grib, reference_text)

# XXX Addressed in #1118 pending #1039 for hybrid levels
#    def test_hybrid_pressure_levels(self):
#        source_grib = tests.get_data_path(("GRIB", "ecmwf_standard", "t0.grib2"))
#        reference_text = tests.get_result_path(("grib_save", "hybrid_pressure.grib_compare.txt"))
#        self.save_and_compare(source_grib, reference_text)

    def test_time_mean(self):
        # This test for time-mean fields also tests negative forecast time.
        # Because the results depend on the presence of our api patch,
        # we currently have results for both a patched and unpatched api.
        # If the api ever allows -ve ft, we should revert to a single result.
        source_grib = tests.get_data_path(("GRIB", "time_processed",
                                           "time_bound.grib2"))
        reference_text = tests.get_result_path(("grib_save",
                                                "time_mean.grib_compare.txt"))
        # TODO: It's not ideal to have grib patch awareness here...
        import unittest
        try:
            self.save_and_compare(source_grib, reference_text)
        except unittest.TestCase.failureException:
            reference_text = tests.get_result_path((
                                        "grib_save",
                                        "time_mean.grib_compare.FT_PATCH.txt"))
            self.save_and_compare(source_grib, reference_text)


@tests.skip_data
class TestCubeSave(tests.IrisTest):
    # save fabricated cubes

    def _load_basic(self):
        path = tests.get_data_path(("GRIB", "uk_t", "uk_t.grib2"))
        return iris.load(path)[0]

    def test_params(self):
        # TODO
        pass

    def test_originating_centre(self):
        # TODO
        pass

    def test_irregular(self):
        cube = self._load_basic()
        lat_coord = cube.coord("latitude")
        cube.remove_coord("latitude")

        new_lats = np.append(lat_coord.points[:-1], lat_coord.points[0])  # Irregular
        cube.add_aux_coord(iris.coords.AuxCoord(new_lats, "latitude", units="degrees", coord_system=lat_coord.coord_system), 0)

        saved_grib = iris.util.create_temp_filename(suffix='.grib2')
        self.assertRaises(iris.exceptions.TranslationError, iris.save, cube, saved_grib)
        os.remove(saved_grib)

    def test_non_latlon(self):
        cube = self._load_basic()
        cube.coord(dimensions=[0]).coord_system = None
        saved_grib = iris.util.create_temp_filename(suffix='.grib2')
        self.assertRaises(iris.exceptions.TranslationError, iris.save, cube, saved_grib)
        os.remove(saved_grib)

    def test_forecast_period(self):
        # unhandled unit
        cube = self._load_basic()
        cube.coord("forecast_period").units = iris.unit.Unit("years")
        saved_grib = iris.util.create_temp_filename(suffix='.grib2')
        self.assertRaises(iris.exceptions.TranslationError, iris.save, cube, saved_grib)
        os.remove(saved_grib)

    def test_unhandled_vertical(self):
        # unhandled level type
        cube = self._load_basic()
        # Adjust the 'pressure' coord to make it into an "unrecognised Z coord"
        p_coord = cube.coord("pressure")
        p_coord.rename("not the messiah")
        p_coord.units = 'K'
        p_coord.attributes['positive'] = 'up'
        saved_grib = iris.util.create_temp_filename(suffix='.grib2')
        with self.assertRaises(iris.exceptions.TranslationError):
            iris.save(cube, saved_grib)
        os.remove(saved_grib)

    def test_scalar_int32_pressure(self):
        # Make sure we can save a scalar int32 coordinate with unit conversion.
        cube = self._load_basic()
        cube.coord("pressure").points = np.array([200], dtype=np.int32)
        cube.coord("pressure").units = "hPa"
        with self.temp_filename(".grib2") as testfile:
            iris.save(cube, testfile)

    def test_bounded_level(self):
        cube = iris.load_cube(tests.get_data_path(("GRIB", "uk_t",
                                                   "uk_t.grib2")))
        # Changing pressure to altitude due to grib api bug:
        # https://github.com/SciTools/iris/pull/715#discussion_r5901538
        cube.remove_coord("pressure")
        cube.add_aux_coord(iris.coords.AuxCoord(
            1030.0, long_name='altitude', units='m',
            bounds=np.array([111.0, 1949.0])))
        with self.temp_filename(".grib2") as testfile:
            iris.save(cube, testfile)
            with open(testfile, "rb") as saved_file:
                g = gribapi.grib_new_from_file(saved_file)
                self.assertEqual(
                    gribapi.grib_get_double(g,
                                            "scaledValueOfFirstFixedSurface"),
                    111.0)
                self.assertEqual(
                    gribapi.grib_get_double(g,
                                            "scaledValueOfSecondFixedSurface"),
                    1949.0)


class TestHandmade(tests.IrisTest):

    def _lat_lon_cube_no_time(self):
        """Returns a cube with a latitude and longitude suitable for testing saving to PP/NetCDF etc."""
        cube = iris.cube.Cube(np.arange(12, dtype=np.int32).reshape((3, 4)))
        cs = iris.coord_systems.GeogCS(6371229)
        cube.add_dim_coord(iris.coords.DimCoord(np.arange(4) * 90 + -180, 'longitude', units='degrees', coord_system=cs), 1)
        cube.add_dim_coord(iris.coords.DimCoord(np.arange(3) * 45 + -90, 'latitude', units='degrees', coord_system=cs), 0)

        return cube

    def _cube_time_no_forecast(self):
        cube = self._lat_lon_cube_no_time()
        unit = iris.unit.Unit('hours since epoch', calendar=iris.unit.CALENDAR_GREGORIAN)
        dt = datetime.datetime(2010, 12, 31, 12, 0)
        cube.add_aux_coord(iris.coords.AuxCoord(np.array([unit.date2num(dt)], dtype=np.float64), 'time', units=unit))
        return cube

    def _cube_with_forecast(self):
        cube = self._cube_time_no_forecast()
        cube.add_aux_coord(iris.coords.AuxCoord(np.array([6], dtype=np.int32), 'forecast_period', units='hours'))
        return cube

    def _cube_with_pressure(self):
        cube = self._cube_with_forecast()
        cube.add_aux_coord(iris.coords.DimCoord(np.int32(10), 'air_pressure', units='Pa'))
        return cube

    def _cube_with_time_bounds(self):
        cube = self._cube_with_pressure()
        cube.coord("time").bounds = np.array([[0,100]])
        return cube

    def test_no_time_cube(self):
        cube = self._lat_lon_cube_no_time()
        saved_grib = iris.util.create_temp_filename(suffix='.grib2')
        self.assertRaises(iris.exceptions.TranslationError, iris.save, cube, saved_grib)
        os.remove(saved_grib)

    def test_cube_with_time_bounds(self):
        cube = self._cube_with_time_bounds()
        saved_grib = iris.util.create_temp_filename(suffix='.grib2')
        self.assertRaises(iris.exceptions.TranslationError, iris.save, cube, saved_grib)
        os.remove(saved_grib)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_grib_save_rules
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for iris.fileformats.grib_save_rules"""

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import gribapi
import numpy as np
import numpy.ma as ma
import mock
import warnings

import iris.cube
import iris.coords
import iris.fileformats.grib.grib_save_rules as grib_save_rules


class Test_non_hybrid_surfaces(tests.IrisTest):
    # Test grib_save_rules.non_hybrid_surfaces()

    @mock.patch.object(gribapi, "grib_set_long")
    def test_altitude_point(self, mock_set_long):
        grib = None
        cube = iris.cube.Cube([1,2,3,4,5])
        cube.add_aux_coord(iris.coords.AuxCoord([12345], "altitude", units="m"))

        grib_save_rules.non_hybrid_surfaces(cube, grib)

        mock_set_long.assert_any_call(grib, "typeOfFirstFixedSurface", 102)
        mock_set_long.assert_any_call(grib, "scaleFactorOfFirstFixedSurface", 0)
        mock_set_long.assert_any_call(grib, "scaledValueOfFirstFixedSurface", 12345)
        mock_set_long.assert_any_call(grib, "typeOfSecondFixedSurface", -1)
        mock_set_long.assert_any_call(grib, "scaleFactorOfSecondFixedSurface", 255)
        mock_set_long.assert_any_call(grib, "scaledValueOfSecondFixedSurface", -1)

    @mock.patch.object(gribapi, "grib_set_long")
    def test_height_point(self, mock_set_long):
        grib = None
        cube = iris.cube.Cube([1,2,3,4,5])
        cube.add_aux_coord(iris.coords.AuxCoord([12345], "height", units="m"))

        grib_save_rules.non_hybrid_surfaces(cube, grib)

        mock_set_long.assert_any_call(grib, "typeOfFirstFixedSurface", 103)
        mock_set_long.assert_any_call(grib, "scaleFactorOfFirstFixedSurface", 0)
        mock_set_long.assert_any_call(grib, "scaledValueOfFirstFixedSurface", 12345)
        mock_set_long.assert_any_call(grib, "typeOfSecondFixedSurface", -1)
        mock_set_long.assert_any_call(grib, "scaleFactorOfSecondFixedSurface", 255)
        mock_set_long.assert_any_call(grib, "scaledValueOfSecondFixedSurface", -1)

    @mock.patch.object(gribapi, "grib_set_long")
    def test_no_vertical(self, mock_set_long):
        grib = None
        cube = iris.cube.Cube([1,2,3,4,5])
        grib_save_rules.non_hybrid_surfaces(cube, grib)
        mock_set_long.assert_any_call(grib, "typeOfFirstFixedSurface", 1)
        mock_set_long.assert_any_call(grib, "scaleFactorOfFirstFixedSurface", 0)
        mock_set_long.assert_any_call(grib, "scaledValueOfFirstFixedSurface", 0)
        mock_set_long.assert_any_call(grib, "typeOfSecondFixedSurface", -1)
        mock_set_long.assert_any_call(grib, "scaleFactorOfSecondFixedSurface", 255)
        mock_set_long.assert_any_call(grib, "scaledValueOfSecondFixedSurface", -1)


class Test_phenomenon(tests.IrisTest):
    @mock.patch.object(gribapi, "grib_set_long")
    def test_phenom_unknown(self, mock_set_long):
        grib = None
        cube = iris.cube.Cube(np.array([1.0]))
        # Force reset of warnings registry to avoid suppression of
        # repeated warnings. warnings.resetwarnings() does not do this.
        if hasattr(grib_save_rules, '__warningregistry__'):
            grib_save_rules.__warningregistry__.clear()
        with warnings.catch_warnings():
            # This should issue a warning about unrecognised data
            warnings.simplefilter("error")
            with self.assertRaises(UserWarning):
                grib_save_rules.param_code(cube, grib)
        # do it all again, and this time check the results
        grib = None
        cube = iris.cube.Cube(np.array([1.0]))
        grib_save_rules.param_code(cube, grib)
        mock_set_long.assert_any_call(grib, "discipline", 255)
        mock_set_long.assert_any_call(grib, "parameterCategory", 255)
        mock_set_long.assert_any_call(grib, "parameterNumber", 255)

    @mock.patch.object(gribapi, "grib_set_long")
    def test_phenom_known_standard_name(self, mock_set_long):
        grib = None
        cube = iris.cube.Cube(np.array([1.0]),
                              standard_name='sea_surface_temperature')
        grib_save_rules.param_code(cube, grib)
        mock_set_long.assert_any_call(grib, "discipline", 10)
        mock_set_long.assert_any_call(grib, "parameterCategory", 3)
        mock_set_long.assert_any_call(grib, "parameterNumber", 0)

    @mock.patch.object(gribapi, "grib_set_long")
    def test_phenom_known_long_name(self, mock_set_long):
        grib = None
        cube = iris.cube.Cube(np.array([1.0]),
                              long_name='cloud_mixing_ratio')
        grib_save_rules.param_code(cube, grib)
        mock_set_long.assert_any_call(grib, "discipline", 0)
        mock_set_long.assert_any_call(grib, "parameterCategory", 1)
        mock_set_long.assert_any_call(grib, "parameterNumber", 22)


class Test_type_of_statistical_processing(tests.IrisTest):
    @mock.patch.object(gribapi, "grib_set_long")
    def test_stats_type_min(self, mock_set_long):
        grib = None
        cube = iris.cube.Cube(np.array([1.0]))
        time_unit = iris.unit.Unit('hours since 1970-01-01 00:00:00')
        time_coord = iris.coords.DimCoord([0.0],
                                          bounds=[0.0, 1],
                                          standard_name='time',
                                          units=time_unit)
        cube.add_aux_coord(time_coord, ())
        cube.add_cell_method(iris.coords.CellMethod('maximum', time_coord))
        grib_save_rules.type_of_statistical_processing(cube, grib, time_coord)
        mock_set_long.assert_any_call(grib, "typeOfStatisticalProcessing", 2)

    @mock.patch.object(gribapi, "grib_set_long")
    def test_stats_type_max(self, mock_set_long):
        grib = None
        cube = iris.cube.Cube(np.array([1.0]))
        time_unit = iris.unit.Unit('hours since 1970-01-01 00:00:00')
        time_coord = iris.coords.DimCoord([0.0],
                                          bounds=[0.0, 1],
                                          standard_name='time',
                                          units=time_unit)
        cube.add_aux_coord(time_coord, ())
        cube.add_cell_method(iris.coords.CellMethod('minimum', time_coord))
        grib_save_rules.type_of_statistical_processing(cube, grib, time_coord)
        mock_set_long.assert_any_call(grib, "typeOfStatisticalProcessing", 3)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_hybrid
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the hybrid vertical coordinate representations.

"""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import warnings

import numpy as np

from iris.aux_factory import HybridHeightFactory, HybridPressureFactory
import iris
import iris.tests.stock


@tests.skip_plot
class TestRealistic4d(tests.GraphicsTest):
    def setUp(self):
        self.cube = iris.tests.stock.realistic_4d()
        self.altitude = self.cube.coord('altitude')

    def test_metadata(self):
        self.assertEqual(self.altitude.units, 'm')
        self.assertIsNone(self.altitude.coord_system)
        self.assertEqual(self.altitude.attributes, {'positive': 'up'})

    def test_points(self):
        self.assertAlmostEqual(self.altitude.points.min(), np.float32(191.84892))
        self.assertAlmostEqual(self.altitude.points.max(), np.float32(40000))

    def test_transpose(self):
        self.assertCML(self.cube, ('stock', 'realistic_4d.cml'))
        self.cube.transpose()
        self.assertCML(self.cube, ('derived', 'transposed.cml'))

    def test_indexing(self):
        cube = self.cube[:, :, 0, 0]
        # Make sure the derived 'altitude' coordinate survived the indexing.
        altitude = cube.coord('altitude')
        self.assertCML(cube, ('derived', 'column.cml'))

    def test_removing_sigma(self):
        # Check the cube remains OK when sigma is removed.
        cube = self.cube
        cube.remove_coord('sigma')
        self.assertCML(cube, ('derived', 'removed_sigma.cml'))
        self.assertString(str(cube), ('derived', 'removed_sigma.__str__.txt'))

        # Check the factory now only has surface_altitude and delta dependencies.
        factory = cube.aux_factory(name='altitude')
        t = [key for key, coord in factory.dependencies.iteritems() if coord is not None]
        self.assertItemsEqual(t, ['orography', 'delta'])

    def test_removing_orography(self):
        # Check the cube remains OK when the orography is removed.
        cube = self.cube
        cube.remove_coord('surface_altitude')
        self.assertCML(cube, ('derived', 'removed_orog.cml'))
        self.assertString(str(cube), ('derived', 'removed_orog.__str__.txt'))

        # Check the factory now only has sigma and delta dependencies.
        factory = cube.aux_factory(name='altitude')
        t = [key for key, coord in factory.dependencies.iteritems() if coord is not None]
        self.assertItemsEqual(t, ['sigma', 'delta'])

    def test_derived_coords(self):
        derived_coords = self.cube.derived_coords
        self.assertEqual(len(derived_coords), 1)
        altitude = derived_coords[0]
        self.assertEqual(altitude.standard_name, 'altitude')
        self.assertEqual(altitude.attributes, {'positive': 'up'})

    def test_aux_factory(self):
        factory = self.cube.aux_factory(name='altitude')
        self.assertEqual(factory.standard_name, 'altitude')
        self.assertEqual(factory.attributes, {'positive': 'up'})

    def test_aux_factory_var_name(self):
        factory = self.cube.aux_factory(name='altitude')
        factory.var_name = 'alt'
        factory = self.cube.aux_factory(var_name='alt')
        self.assertEqual(factory.standard_name, 'altitude')
        self.assertEqual(factory.attributes, {'positive': 'up'})

    def test_no_orography(self):
        # Get rid of the normal hybrid-height factory.
        cube = self.cube
        factory = cube.aux_factory(name='altitude')
        cube.remove_aux_factory(factory)

        # Add a new one which only references level_height & sigma.
        delta = cube.coord('level_height')
        sigma = cube.coord('sigma')
        factory = HybridHeightFactory(delta, sigma)
        cube.add_aux_factory(factory)

        self.assertEqual(len(cube.aux_factories), 1)
        self.assertEqual(len(cube.derived_coords), 1)
        self.assertString(str(cube), ('derived', 'no_orog.__str__.txt'))
        self.assertCML(cube, ('derived', 'no_orog.cml'))

    def test_invalid_dependencies(self):
        # Must have either delta or orography
        with self.assertRaises(ValueError):
            factory = HybridHeightFactory()
        sigma = self.cube.coord('sigma')
        with self.assertRaises(ValueError):
            factory = HybridHeightFactory(sigma=sigma)

        # Orography must not have bounds
        with warnings.catch_warnings():
            # Cause all warnings to raise Exceptions
            warnings.simplefilter("error")
            with self.assertRaises(UserWarning):
                factory = HybridHeightFactory(orography=sigma)

    def test_bounded_orography(self):
        # Start with everything normal
        orog = self.cube.coord('surface_altitude')
        altitude = self.cube.coord('altitude')
        self.assertIsInstance(altitude.bounds, np.ndarray)

        # Make sure altitude still works OK if orography was messed
        # with *after* altitude was created.
        altitude = self.cube.coord('altitude')
        orog.bounds = np.zeros(orog.shape + (4,))
        self.assertIsInstance(altitude.bounds, np.ndarray)

        # Make sure altitude.bounds now raises an error.
        altitude = self.cube.coord('altitude')
        with self.assertRaises(ValueError):
            bounds = altitude.bounds


class TestHybridPressure(tests.IrisTest):
    def setUp(self):
        # Convert the hybrid-height into hybrid-pressure...
        cube = iris.tests.stock.realistic_4d()

        # Get rid of the normal hybrid-height factory.
        factory = cube.aux_factory(name='altitude')
        cube.remove_aux_factory(factory)

        # Mangle the height coords into pressure coords.
        delta = cube.coord('level_height')
        delta.rename('level_pressure')
        delta.units = 'Pa'
        sigma = cube.coord('sigma')
        ref = cube.coord('surface_altitude')
        ref.rename('surface_air_pressure')
        ref.units = 'Pa'

        factory = HybridPressureFactory(delta, sigma, ref)
        cube.add_aux_factory(factory)
        self.cube = cube
        self.air_pressure = self.cube.coord('air_pressure')

    def test_metadata(self):
        self.assertEqual(self.air_pressure.units, 'Pa')
        self.assertIsNone(self.air_pressure.coord_system)
        self.assertEqual(self.air_pressure.attributes, {})

    def test_points(self):
        points = self.air_pressure.points
        self.assertEqual(points.dtype, np.float32)
        self.assertAlmostEqual(points.min(), np.float32(191.84892))
        self.assertAlmostEqual(points.max(), np.float32(40000))

        # Convert the reference surface to float64 and check the
        # derived coordinate becomes float64.
        temp = self.cube.coord('surface_air_pressure').points
        temp = temp.astype('f8')
        self.cube.coord('surface_air_pressure').points = temp
        points = self.cube.coord('air_pressure').points
        self.assertEqual(points.dtype, np.float64)
        self.assertAlmostEqual(points.min(), 191.8489257)
        self.assertAlmostEqual(points.max(), 40000)

    def test_invalid_dependencies(self):
        # Must have either delta or surface_air_pressure
        with self.assertRaises(ValueError):
            factory = HybridPressureFactory()
        sigma = self.cube.coord('sigma')
        with self.assertRaises(ValueError):
            factory = HybridPressureFactory(sigma=sigma)

        # Surface pressure must not have bounds
        with warnings.catch_warnings():
            # Cause all warnings to raise Exceptions
            warnings.simplefilter("error")
            with self.assertRaises(UserWarning):
                factory = HybridPressureFactory(
                    sigma=sigma, surface_air_pressure=sigma)

    def test_bounded_surface_pressure(self):
        # Start with everything normal
        surface_pressure = self.cube.coord('surface_air_pressure')
        pressure = self.cube.coord('air_pressure')
        self.assertIsInstance(pressure.bounds, np.ndarray)

        # Make sure pressure still works OK if surface pressure was messed
        # with *after* pressure was created.
        pressure = self.cube.coord('air_pressure')
        surface_pressure.bounds = np.zeros(surface_pressure.shape + (4,))
        self.assertIsInstance(pressure.bounds, np.ndarray)

        # Make sure pressure.bounds now raises an error.
        pressure = self.cube.coord('air_pressure')
        with self.assertRaises(ValueError):
            bounds = pressure.bounds


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_interpolation
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the interpolation of Iris cubes.

"""
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import biggus
import numpy as np
import numpy.ma as ma
from scipy.interpolate import interp1d

import iris
import iris.coord_systems
import iris.cube
import iris.analysis.interpolate
import iris.tests.stock
from iris.analysis.interpolate import Linear1dExtrapolator
import iris.analysis.interpolate as iintrp


def normalise_order(cube):
    # Avoid the crazy array ordering which results from using:
    #   * np.append() in NumPy 1.6, which is triggered in the `linear()`
    #     function when the circular flag is true.
    #   * scipy.interpolate.interp1d in 0.11.0 which is used in
    #     `Linear1dExtrapolator`.
    cube.data = np.ascontiguousarray(cube.data)


class TestLinearExtrapolator(tests.IrisTest):
    def test_simple_axis0(self):
        a = np.arange(12.).reshape(3, 4)
        r = Linear1dExtrapolator(interp1d(np.arange(3), a, axis=0))
        
        np.testing.assert_array_equal(r(0), np.array([ 0.,  1.,  2.,  3.]))
        np.testing.assert_array_equal(r(-1), np.array([-4.,  -3.,  -2., -1.]))
        np.testing.assert_array_equal(r(3), np.array([ 12.,  13.,  14.,  15.]))        
        np.testing.assert_array_equal(r(2.5), np.array([ 10.,  11.,  12.,  13.]))
        
        # 2 Non-extrapolation point
        np.testing.assert_array_equal(r(np.array([1.5, 2])), np.array([[  6.,   7.,   8.,   9.],
                                                                                [  8.,   9.,  10.,  11.]]))
        
        # 1 Non-extrapolation point & 1 upper value extrapolation
        np.testing.assert_array_equal(r(np.array([1.5, 3])), np.array([[  6.,   7.,   8.,   9.],
                                                                                [ 12.,  13.,  14.,  15.]]))
        
        # 2 upper value extrapolation
        np.testing.assert_array_equal(r(np.array([2.5, 3])), np.array([[ 10.,  11.,  12.,  13.],
                                                                                [ 12.,  13.,  14.,  15.]]))

        # 1 lower value extrapolation & 1 Non-extrapolation point
        np.testing.assert_array_equal(r(np.array([-1, 1.5])), np.array([[-4., -3., -2., -1.],
                                                                                 [ 6.,  7.,  8.,  9.]]))

        # 2 lower value extrapolation
        np.testing.assert_array_equal(r(np.array([-1.5, -1])), np.array([[-6., -5., -4., -3.],
                                                                                  [-4., -3., -2., -1.]]))
        
        # 2 lower value extrapolation, 2 Non-extrapolation point & 2 upper value extrapolation
        np.testing.assert_array_equal(r(np.array([-1.5, -1, 1, 1.5, 2.5, 3])),
                                         np.array([[ -6.,  -5.,  -4.,  -3.],
                                                      [ -4.,  -3.,  -2.,  -1.],
                                                      [  4.,   5.,   6.,   7.],
                                                      [  6.,   7.,   8.,   9.],
                                                      [ 10.,  11.,  12.,  13.],
                                                      [ 12.,  13.,  14.,  15.]]))
   
    def test_simple_axis1(self):
        a = np.arange(12).reshape(3, 4)
        r = Linear1dExtrapolator(interp1d(np.arange(4), a, axis=1))
        
        # check non-extrapolation given the Extrapolator object 
        np.testing.assert_array_equal(r(0), np.array([ 0.,  4.,  8.]))
        
        # check the result's shape in a 1d array (of len 0 & 1)
        np.testing.assert_array_equal(r(np.array(0)), np.array([ 0.,  4.,  8.]))        
        np.testing.assert_array_equal(r(np.array([0])), np.array([ [0.],  [4.],  [8.]]))
        
        # check extrapolation below the minimum value (and check the equivalent 0d & 1d arrays)
        np.testing.assert_array_equal(r(-1), np.array([-1., 3., 7.]))
        np.testing.assert_array_equal(r(np.array(-1)), np.array([-1., 3., 7.]))
        np.testing.assert_array_equal(r(np.array([-1])), np.array([[-1.], [ 3.], [ 7.]]))
        
        # check extrapolation above the maximum value
        np.testing.assert_array_equal(r(3), np.array([  3.,   7.,  11.]))
        np.testing.assert_array_equal(r(2.5), np.array([  2.5,   6.5,  10.5]))
        
        # 2 Non-extrapolation point 
        np.testing.assert_array_equal(r(np.array([1.5, 2])), np.array([[  1.5,   2. ],
                                                                                [  5.5,   6. ],
                                                                                [  9.5,  10. ]]))
        
        # 1 Non-extrapolation point & 1 upper value extrapolation
        np.testing.assert_array_equal(r(np.array([1.5, 5])), np.array([[  1.5,   5. ],
                                                                                [  5.5,   9. ],
                                                                                [  9.5,  13. ]]))
        
        # 2 upper value extrapolation        
        np.testing.assert_array_equal(r(np.array([4.5, 5])), np.array([[  4.5,   5. ],
                                                                                [  8.5,   9. ],
                                                                                [ 12.5,  13. ]]))

        # 1 lower value extrapolation & 1 Non-extrapolation point
        np.testing.assert_array_equal(r(np.array([-0.5, 1.5])), np.array([[-0.5,  1.5],
                                                                                   [ 3.5,  5.5],
                                                                                   [ 7.5,  9.5]]))        
        
        # 2 lower value extrapolation
        np.testing.assert_array_equal(r(np.array([-1.5, -1])), np.array([[-1.5, -1. ],
                                                                                  [ 2.5,  3. ],
                                                                                  [ 6.5,  7. ]]))
        
        # 2 lower value extrapolation, 2 Non-extrapolation point & 2 upper value extrapolation
        np.testing.assert_array_equal(r(np.array([-1.5, -1, 1.5, 2, 4.5, 5])), 
                                         np.array([[ -1.5,  -1. ,   1.5,   2. ,   4.5,   5. ],
                                                      [  2.5,   3. ,   5.5,   6. ,   8.5,   9. ],
                                                      [  6.5,   7. ,   9.5,  10. ,  12.5,  13. ]]))
        
        
    def test_simple_3d_axis1(self):
        a = np.arange(24.).reshape(3, 4, 2)
        r = Linear1dExtrapolator(interp1d(np.arange(4.), a, axis=1))
        
#       a:
#        [[[ 0  1]
#          [ 2  3]
#          [ 4  5]
#          [ 6  7]]
#        
#         [[ 8  9]
#          [10 11]
#          [12 13]
#          [14 15]]
#        
#         [[16 17]
#          [18 19]
#          [20 21]
#          [22 23]]
#         ] 

        np.testing.assert_array_equal(r(0), np.array([[  0.,   1.],
                                                            [  8.,   9.],
                                                            [ 16.,  17.]]))
        
        np.testing.assert_array_equal(r(1), np.array([[  2.,   3.],
                                                            [ 10.,  11.],
                                                            [ 18.,  19.]]))
        
        np.testing.assert_array_equal(r(-1), np.array([[ -2.,  -1.],
                                                             [  6.,   7.],
                                                             [ 14.,  15.]]))
        
        np.testing.assert_array_equal(r(4), np.array([[  8.,   9.],
                                                            [ 16.,  17.],
                                                            [ 24.,  25.]]))

        np.testing.assert_array_equal(r(0.25), np.array([[  0.5,   1.5],
                                                               [  8.5,   9.5],
                                                               [ 16.5,  17.5]]))
        
        np.testing.assert_array_equal(r(-0.25), np.array([[ -0.5,   0.5],
                                                                [  7.5,   8.5],
                                                                [ 15.5,  16.5]]))
        
        np.testing.assert_array_equal(r(4.25), np.array([[  8.5,   9.5],
                                                               [ 16.5,  17.5],
                                                               [ 24.5,  25.5]]))
        
        np.testing.assert_array_equal(r(np.array([0.5, 1])), np.array([[[  1.,   2.], [  2.,   3.]],
                                                                                [[  9.,  10.], [ 10.,  11.]],
                                                                                [[ 17.,  18.], [ 18.,  19.]]]))
        
        np.testing.assert_array_equal(r(np.array([0.5, 4])), np.array([[[  1.,   2.], [  8.,   9.]],
                                                                                [[  9.,  10.], [ 16.,  17.]],
                                                                                [[ 17.,  18.], [ 24.,  25.]]]))

        np.testing.assert_array_equal(r(np.array([-0.5, 0.5])), np.array([[[ -1.,   0.], [  1.,   2.]],
                                                                                   [[  7.,   8.], [  9.,  10.]],
                                                                                   [[ 15.,  16.], [ 17.,  18.]]]))        

        np.testing.assert_array_equal(r(np.array([-1.5, -1, 0.5, 1, 4.5, 5])), 
                                         np.array([[[ -3.,  -2.], [ -2.,  -1.], [  1.,   2.], [  2.,   3.], [  9.,  10.], [ 10.,  11.]],
                                                      [[  5.,   6.], [  6.,   7.], [  9.,  10.], [ 10.,  11.], [ 17.,  18.], [ 18.,  19.]],
                                                      [[ 13.,  14.], [ 14.,  15.], [ 17.,  18.], [ 18.,  19.], [ 25.,  26.], [ 26.,  27.]]]))
        
    def test_variable_gradient(self):
        a = np.array([[2, 4, 8], [0, 5, 11]])
        r = Linear1dExtrapolator(interp1d(np.arange(2), a, axis=0))
        
        np.testing.assert_array_equal(r(0), np.array([ 2.,  4.,  8.]))
        np.testing.assert_array_equal(r(-1), np.array([ 4.,  3.,  5.]))
        np.testing.assert_array_equal(r(3), np.array([ -4.,   7.,  17.]))        
        np.testing.assert_array_equal(r(2.5), np.array([ -3. ,   6.5,  15.5]))
        
        np.testing.assert_array_equal(r(np.array([1.5, 2])), np.array([[ -1. ,   5.5,  12.5],
                                                                                [ -2. ,   6. ,  14. ]]))
    
        np.testing.assert_array_equal(r(np.array([-1.5, 3.5])), np.array([[  5. ,   2.5,   3.5],
                                                                                   [ -5. ,   7.5,  18.5]]))


class TestLinearLengthOneCoord(tests.IrisTest):
    def setUp(self):
        self.cube = iris.tests.stock.lat_lon_cube()
        self.cube.data = self.cube.data.astype(float)

    def test_single_point(self):
        # Slice to form (3, 1) shaped cube.
        cube = self.cube[:, 2:3]
        r = iris.analysis.interpolate.linear(cube, [('longitude', [1.])])
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_single_pt_0'))

        # Slice to form (1, 4) shaped cube.
        cube = self.cube[1:2, :]
        r = iris.analysis.interpolate.linear(cube, [('latitude', [1.])])
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_single_pt_1'))

    def test_multiple_points(self):
        # Slice to form (3, 1) shaped cube.
        cube = self.cube[:, 2:3]
        r = iris.analysis.interpolate.linear(cube, [('longitude',
                                                     [1., 2., 3., 4.])])
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_many_0'))

        # Slice to form (1, 4) shaped cube.
        cube = self.cube[1:2, :]
        r = iris.analysis.interpolate.linear(cube, [('latitude',
                                                     [1., 2., 3., 4.])])
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_many_1'))

    def test_single_point_to_scalar(self):
        # Slice to form (3, 1) shaped cube.
        cube = self.cube[:, 2:3]
        r = iris.analysis.interpolate.linear(cube, [('longitude', 1.)])
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_scalar_0'))

        # Slice to form (1, 4) shaped cube.
        cube = self.cube[1:2, :]
        r = iris.analysis.interpolate.linear(cube, [('latitude', 1.)])
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_scalar_1'))

    def test_extrapolation_mode_same_pt(self):
        # Slice to form (3, 1) shaped cube.
        cube = self.cube[:, 2:3]
        src_points = cube.coord('longitude').points
        r = iris.analysis.interpolate.linear(cube, [('longitude', src_points)],
                                             extrapolation_mode='linear')
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_same_pt'))
        r = iris.analysis.interpolate.linear(cube, [('longitude', src_points)],
                                             extrapolation_mode='nan')
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_same_pt'))
        r = iris.analysis.interpolate.linear(cube, [('longitude', src_points)],
                                             extrapolation_mode='error')
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_same_pt'))

    def test_extrapolation_mode_multiple_same_pts(self):
        # Slice to form (3, 1) shaped cube.
        cube = self.cube[:, 2:3]
        src_points = cube.coord('longitude').points
        new_points = [src_points[0]] * 3
        r = iris.analysis.interpolate.linear(cube, [('longitude', new_points)],
                                             extrapolation_mode='linear')
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_many_same'))
        r = iris.analysis.interpolate.linear(cube, [('longitude', new_points)],
                                             extrapolation_mode='nan')
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_many_same'))
        r = iris.analysis.interpolate.linear(cube, [('longitude', new_points)],
                                             extrapolation_mode='error')
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_many_same'))

    def test_extrapolation_mode_different_pts(self):
        # Slice to form (3, 1) shaped cube.
        cube = self.cube[:, 2:3]
        src_points = cube.coord('longitude').points
        new_points_single = src_points + 0.2
        new_points_multiple = [src_points[0],
                               src_points[0] + 0.2,
                               src_points[0] + 0.4]
        new_points_scalar = src_points[0] + 0.2

        # 'nan' mode
        r = iris.analysis.interpolate.linear(cube, [('longitude',
                                                     new_points_single)],
                                             extrapolation_mode='nan')
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_single_pt_nan'))
        r = iris.analysis.interpolate.linear(cube, [('longitude',
                                                     new_points_multiple)],
                                             extrapolation_mode='nan')
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_many_nan'))
        r = iris.analysis.interpolate.linear(cube, [('longitude',
                                                     new_points_scalar)],
                                             extrapolation_mode='nan')
        self.assertCMLApproxData(r, ('analysis', 'interpolation', 'linear',
                                     'single_pt_to_scalar_nan'))

        # 'error' mode
        with self.assertRaises(ValueError):
            r = iris.analysis.interpolate.linear(cube, [('longitude',
                                                         new_points_single)],
                                                 extrapolation_mode='error')
        with self.assertRaises(ValueError):
            r = iris.analysis.interpolate.linear(cube, [('longitude',
                                                         new_points_multiple)],
                                                 extrapolation_mode='error')
        with self.assertRaises(ValueError):
            r = iris.analysis.interpolate.linear(cube, [('longitude',
                                                         new_points_scalar)],
                                                 extrapolation_mode='error')


class TestLinear1dInterpolation(tests.IrisTest):
    def setUp(self):
        data = np.arange(12., dtype=np.float32).reshape((4, 3))
        c2 = iris.cube.Cube(data)
           
        c2.long_name = 'test 2d dimensional cube'
        c2.units = 'kelvin'
        
        pts = 3 + np.arange(4, dtype=np.float32) * 2
        b = iris.coords.DimCoord(pts, long_name='dim1', units=1)
        d = iris.coords.AuxCoord([3, 3.5, 6], long_name='dim2', units=1)
        e = iris.coords.AuxCoord(3.0, long_name='an_other', units=1)

        c2.add_dim_coord(b, 0)
        c2.add_aux_coord(d, 1)
        c2.add_aux_coord(e)

        self.simple2d_cube = c2
        
        d = iris.coords.AuxCoord([5, 9, 20], long_name='shared_x_coord', units=1)
        c3 = c2.copy()
        c3.add_aux_coord(d, 1)
        self.simple2d_cube_extended = c3

        pts = 0.1 + np.arange(5, dtype=np.float32) * 0.1
        f = iris.coords.DimCoord(pts, long_name='r', units=1)
        g = iris.coords.DimCoord([0.0, 90.0, 180.0, 270.0], long_name='theta', units='degrees', circular=True)
        data = np.arange(20., dtype=np.float32).reshape((5, 4))
        c4 = iris.cube.Cube(data)
        c4.add_dim_coord(f, 0)
        c4.add_dim_coord(g, 1)
        self.simple2d_cube_circular = c4

    def test_dim_to_aux(self):
        cube = self.simple2d_cube
        other = iris.coords.DimCoord([1, 2, 3, 4], long_name='was_dim')
        cube.add_aux_coord(other, 0)
        r = iris.analysis.interpolate.linear(cube, [('dim1', [7, 3, 5])])
        normalise_order(r)
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'dim_to_aux.cml'))

    def test_bad_sample_point_format(self):
        self.assertRaises(TypeError, iris.analysis.interpolate.linear, self.simple2d_cube, ('dim1', 4))
    
    def test_simple_single_point(self):
        r = iris.analysis.interpolate.linear(self.simple2d_cube, [('dim1', 4)])
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_single_point.cml'), checksum=False)
        np.testing.assert_array_equal(r.data, np.array([1.5, 2.5, 3.5], dtype=self.simple2d_cube.data.dtype))
        
    def test_monotonic_decreasing_coord(self):
        c = self.simple2d_cube[::-1]
        r = iris.analysis.interpolate.linear(c, [('dim1', 4)])
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_single_point.cml'), checksum=False)
        np.testing.assert_array_equal(r.data, np.array([1.5, 2.5, 3.5], dtype=self.simple2d_cube.data.dtype))
        
    def test_overspecified(self):
        self.assertRaises(ValueError, iris.analysis.interpolate.linear, self.simple2d_cube[0, :], [('dim1', 4)])
        
    def test_bounded_coordinate(self):
        # The results should be exactly the same as for the
        # non-bounded case.
        cube = self.simple2d_cube
        cube.coord('dim1').guess_bounds()
        r = iris.analysis.interpolate.linear(cube, [('dim1', [4, 5])])
        np.testing.assert_array_equal(r.data, np.array([[ 1.5,  2.5,  3.5], [ 3. ,  4. ,  5. ]]))
        normalise_order(r)
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_multiple_points.cml'))

    def test_simple_multiple_point(self):
        r = iris.analysis.interpolate.linear(self.simple2d_cube, [('dim1', [4, 5])])
        np.testing.assert_array_equal(r.data, np.array([[ 1.5,  2.5,  3.5], [ 3. ,  4. ,  5. ]]))
        normalise_order(r)
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_multiple_points.cml'))
        
        # Check that numpy arrays specifications work
        r = iris.analysis.interpolate.linear(self.simple2d_cube, [('dim1', np.array([4, 5]))])
        normalise_order(r)
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_multiple_points.cml'))

    def test_circular_vs_non_circular_coord(self):
        cube = self.simple2d_cube_circular
        other = iris.coords.AuxCoord([10, 6, 7, 4], long_name='other')
        cube.add_aux_coord(other, 1)
        samples = [0, 60, 300]
        r = iris.analysis.interpolate.linear(cube, [('theta', samples)])
        normalise_order(r)
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'circular_vs_non_circular.cml'))

    def test_simple_multiple_points_circular(self):
        r = iris.analysis.interpolate.linear(self.simple2d_cube_circular, [('theta', [0. , 60. , 120. , 180. ])])
        normalise_order(r)
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_multiple_points_circular.cml'))
        
        # check that the values returned by theta 0 & 360 are the same...
        r1 = iris.analysis.interpolate.linear(self.simple2d_cube_circular, [('theta', 360)])
        r2 = iris.analysis.interpolate.linear(self.simple2d_cube_circular, [('theta', 0)])
        np.testing.assert_array_almost_equal(r1.data, r2.data)
        
    def test_simple_multiple_coords(self):
        expected_result = np.array(2.5)
        r = iris.analysis.interpolate.linear(self.simple2d_cube, [('dim1', 4), ('dim2', 3.5), ])
        np.testing.assert_array_equal(r.data, expected_result)
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_multiple_coords.cml'), checksum=False)
        
        # Check that it doesn't matter if you do the interpolation in separate steps...
        r = iris.analysis.interpolate.linear(self.simple2d_cube, [('dim2', 3.5)])
        r = iris.analysis.interpolate.linear(r, [('dim1', 4)])
        np.testing.assert_array_equal(r.data, expected_result)
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_multiple_coords.cml'), checksum=False)
        
        r = iris.analysis.interpolate.linear(self.simple2d_cube, [('dim1', 4)])
        r = iris.analysis.interpolate.linear(r, [('dim2', 3.5)])
        np.testing.assert_array_equal(r.data, expected_result)
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_multiple_coords.cml'), checksum=False)
    
    def test_coord_not_found(self):
        self.assertRaises(KeyError, iris.analysis.interpolate.linear, self.simple2d_cube, 
                          [('non_existant_coord', [3.5, 3.25])])
    
    def test_simple_coord_error_extrapolation(self):
        self.assertRaises(ValueError, iris.analysis.interpolate.linear, self.simple2d_cube, [('dim2', 2.5)], extrapolation_mode='error')

    def test_simple_coord_linear_extrapolation(self):
        r = iris.analysis.interpolate.linear( self.simple2d_cube, [('dim2', 2.5)], extrapolation_mode='linear')
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_coord_linear_extrapolation.cml'))
        
        np.testing.assert_array_equal(r.data, np.array([-1.,  2.,  5.,  8.], dtype=self.simple2d_cube.data.dtype))
        
        r = iris.analysis.interpolate.linear(self.simple2d_cube, [('dim1', 1)])
        np.testing.assert_array_equal(r.data, np.array([-3., -2., -1.], dtype=self.simple2d_cube.data.dtype))
                
    def test_simple_coord_linear_extrapolation_multipoint1(self):
        r = iris.analysis.interpolate.linear( self.simple2d_cube, [('dim1', [-1, 1, 10, 11])], extrapolation_mode='linear')
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_coord_linear_extrapolation_multipoint1.cml'))
        
    def test_simple_coord_linear_extrapolation_multipoint2(self):
        r = iris.analysis.interpolate.linear( self.simple2d_cube, [('dim1', [1, 10])], extrapolation_mode='linear')
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_coord_linear_extrapolation_multipoint2.cml'))
        
    def test_simple_coord_nan_extrapolation(self):
        r = iris.analysis.interpolate.linear( self.simple2d_cube, [('dim2', 2.5)], extrapolation_mode='nan')
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_coord_nan_extrapolation.cml'))
    
    def test_multiple_coord_extrapolation(self):
        self.assertRaises(ValueError, iris.analysis.interpolate.linear, self.simple2d_cube, [('dim2', 2.5), ('dim1', 12.5)], extrapolation_mode='error')    
        
    def test_multiple_coord_linear_extrapolation(self):
        r = iris.analysis.interpolate.linear(self.simple2d_cube, [('dim2', 9), ('dim1', 1.5)])
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_multiple_coords_extrapolation.cml'))
        
    def test_lots_of_points(self):
        r = iris.analysis.interpolate.linear(self.simple2d_cube, [('dim1', np.linspace(3, 9, 20))])
        # XXX Implement a test!?!
        
    def test_shared_axis(self):
        c = self.simple2d_cube_extended
        r = iris.analysis.interpolate.linear(c, [('dim2', [3.5, 3.25])])
        normalise_order(r)
        
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_shared_axis.cml'))
        
        self.assertRaises(ValueError, iris.analysis.interpolate.linear, c, [('dim2', [3.5, 3.25]), ('shared_x_coord', [9, 7])])

    def test_points_datatype_casting(self):
        # this test tries to extract a float from an array of type integer. the result should be of type float.
        r = iris.analysis.interpolate.linear(self.simple2d_cube_extended, [('shared_x_coord', 7.5)])
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'simple_casting_datatype.cml'))
        
    def test_mask(self):
        # Test np.append bug with masked arrays.
        # Based on the bug reported in https://github.com/SciTools/iris/issues/106
        cube = tests.stock.realistic_4d_w_missing_data()
        cube = cube[0, 2, 18::-1]
        cube.coord('grid_longitude').circular = True
        _ = iris.analysis.interpolate.linear(cube, [('grid_longitude',0), ('grid_latitude',0)])
        # Did np.append go wrong?
        self.assertArrayEqual(cube.data.data.shape, cube.data.mask.shape)
    
    def test_scalar_mask(self):
        # Testing the bug raised in https://github.com/SciTools/iris/pull/123#issuecomment-9309872
        # (the fix workaround for the np.append bug failed for scalar masks) 
        cube = tests.stock.realistic_4d_w_missing_data()
        cube.data = ma.arange(np.product(cube.shape), dtype=np.float32).reshape(cube.shape)
        cube.coord('grid_longitude').circular = True
        # There's no result to test, just make sure we don't cause an exception with the scalar mask.
        _ = iris.analysis.interpolate.linear(cube, [('grid_longitude',0), ('grid_latitude',0)])


@tests.skip_data
class TestNearestLinearInterpolRealData(tests.IrisTest):
    def setUp(self):
        file = tests.get_data_path(('PP', 'globClim1', 'theta.pp'))
        self.cube = iris.load_cube(file)

    def test_slice(self):
        r = iris.analysis.interpolate.linear(self.cube, [('latitude', 0)])
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'real_2dslice.cml'))
    
    def test_2slices(self):
        r = iris.analysis.interpolate.linear(self.cube, [('latitude', 0.0), ('longitude', 0.0)])
        self.assertCML(r, ('analysis', 'interpolation', 'linear', 'real_2slices.cml'))

    def test_circular(self):
        res = iris.analysis.interpolate.linear(self.cube,
                                               [('longitude', 359.8)])
        normalise_order(res)
        lon_coord = self.cube.coord('longitude').points
        expected = self.cube.data[..., 0] + \
            ((self.cube.data[..., -1] - self.cube.data[..., 0]) *
             (((360 - 359.8) - lon_coord[0]) /
              ((360 - lon_coord[-1]) - lon_coord[0])))
        self.assertArrayAllClose(res.data, expected, rtol=1.0e-6)

        # check that the values returned by lon 0 & 360 are the same...
        r1 = iris.analysis.interpolate.linear(self.cube, [('longitude', 360)])
        r2 = iris.analysis.interpolate.linear(self.cube, [('longitude', 0)])
        np.testing.assert_array_equal(r1.data, r2.data)

        self.assertCML(res, ('analysis', 'interpolation', 'linear',
                             'real_circular_2dslice.cml'), checksum=False)


@tests.skip_data
class TestNearestNeighbour(tests.IrisTest):
    def setUp(self):
        self.cube = iris.tests.stock.global_pp()
        points = np.arange(self.cube.coord('latitude').shape[0], dtype=np.float32)
        coord_to_add = iris.coords.DimCoord(points, long_name='i', units='meters')
        self.cube.add_aux_coord(coord_to_add, 0)

    def test_nearest_neighbour(self):
        point_spec = [('latitude', 40), ('longitude', 39)]
        
        indices = iris.analysis.interpolate.nearest_neighbour_indices(self.cube, point_spec)
        self.assertEqual(indices, (20, 10))
        
        b = iris.analysis.interpolate.extract_nearest_neighbour(self.cube, point_spec) 

        # Check that the data has not been loaded on either the original cube,
        # nor the interpolated one.
        self.assertTrue(b.has_lazy_data())
        self.assertTrue(self.cube.has_lazy_data())
        self.assertCML(b, ('analysis', 'interpolation', 'nearest_neighbour_extract_latitude_longitude.cml'))
        
        value = iris.analysis.interpolate.nearest_neighbour_data_value(self.cube, point_spec)
        self.assertEqual(value, np.array(285.98785, dtype=np.float32))

        # Check that the value back is that which was returned by the extract method
        self.assertEqual(value, b.data)
        
    def test_nearest_neighbour_slice(self):
        point_spec = [('latitude', 40)]
        indices = iris.analysis.interpolate.nearest_neighbour_indices(self.cube, point_spec)
        self.assertEqual(indices, (20, slice(None, None)))

        b = iris.analysis.interpolate.extract_nearest_neighbour(self.cube, point_spec) 
        self.assertCML(b, ('analysis', 'interpolation', 'nearest_neighbour_extract_latitude.cml'))
        
        # cannot get a specific point from these point specifications
        self.assertRaises(ValueError, iris.analysis.interpolate.nearest_neighbour_data_value, self.cube, point_spec)

    def test_nearest_neighbour_over_specification_which_is_consistent(self):
        # latitude 40 is the 20th point
        point_spec = [('latitude', 40), ('i', 20), ('longitude', 38)]
        
        indices = iris.analysis.interpolate.nearest_neighbour_indices(self.cube, point_spec)
        self.assertEqual(indices, (20, 10))
        
        b = iris.analysis.interpolate.extract_nearest_neighbour(self.cube, point_spec) 
        self.assertCML(b, ('analysis', 'interpolation', 'nearest_neighbour_extract_latitude_longitude.cml'))
        
        value = iris.analysis.interpolate.nearest_neighbour_data_value(self.cube, point_spec)
        # Check that the value back is that which was returned by the extract method
        self.assertEqual(value, b.data)

    def test_nearest_neighbour_over_specification_mis_aligned(self):
        # latitude 40 is the 20th point
        point_spec = [('latitude', 40), ('i', 10), ('longitude', 38)]
        
        # assert that we get a ValueError for over specifying our interpolation
        self.assertRaises(ValueError, iris.analysis.interpolate.nearest_neighbour_data_value, self.cube, point_spec)

    def test_nearest_neighbour_bounded_simple(self):
        point_spec = [('latitude', 37), ('longitude', 38)]
    
        coord = self.cube.coord('latitude')
        coord.guess_bounds(0.5)
    
        b = iris.analysis.interpolate.extract_nearest_neighbour(self.cube, point_spec) 
        self.assertCML(b, ('analysis', 'interpolation', 'nearest_neighbour_extract_bounded.cml'))
        
    def test_nearest_neighbour_bounded_requested_midpoint(self):
        # This test checks the "point inside cell" logic
        point_spec = [('latitude', 38), ('longitude', 38)]
    
        coord = self.cube.coord('latitude')
        coord.guess_bounds(0.5)
        
        b = iris.analysis.interpolate.extract_nearest_neighbour(self.cube, point_spec) 
        self.assertCML(b, ('analysis', 'interpolation', 'nearest_neighbour_extract_bounded_mid_point.cml'))
    
    def test_nearest_neighbour_locator_style_coord(self):
        point_spec = [('latitude', 39)]
        
        b = iris.analysis.interpolate.extract_nearest_neighbour(self.cube, point_spec) 
        self.assertCML(b, ('analysis', 'interpolation', 'nearest_neighbour_extract_latitude.cml'))

    def test_nearest_neighbour_circular(self):
        # test on non-circular coordinate (latitude)
        lat_vals = np.array([
            [-150.0, -90], [-97, -90], [-92, -90], [-91, -90],  [-90.1, -90],
            [-90.0, -90],  [-89.9, -90],
            [-89, -90],  [-88, -87.5],  [-87, -87.5],
            [-86, -85], [-85.5, -85],
            [81, 80], [84, 85], [84.8, 85], [85, 85], [86, 85],
            [87, 87.5], [88, 87.5], [89, 90],
            [89.9, 90], [90.0,  90], [90.1, 90],
            [95, 90], [100, 90], [150, 90]])
        lat_test_vals = lat_vals[:, 0]
        lat_expect_vals = lat_vals[:, 1]
        lat_coord_vals = self.cube.coord('latitude').points

        def near_value(val, vals):
            # return the *exact* value from vals that is closest to val.
            # - and raise an exception if there isn't a close match.
            best_val = vals[np.argmin(np.abs(vals - val))]
            if val == 0.0:
                # absolute tolerance to 0.0 (ok for magnitudes >= 1.0 or so)
                error_level = best_val
            else:
                # calculate relative-tolerance
                error_level = abs(0.5 * (val - best_val) / (val + best_val))
            self.assertTrue(error_level < 1.0e-6,
                            'error_level {}% match of {} to one of {}'.format(
                                100.0 * error_level, val, vals))
            return best_val

        lat_expect_vals = [near_value(v, lat_coord_vals)
                           for v in lat_expect_vals]
        lat_nearest_inds = [
            iintrp.nearest_neighbour_indices(
                self.cube, [('latitude', point_val)])
            for point_val in lat_test_vals]
        lat_nearest_vals = [lat_coord_vals[i[0]] for i in lat_nearest_inds]
        self.assertArrayAlmostEqual(lat_nearest_vals, lat_expect_vals)

        # repeat with *circular* coordinate (longitude)
        lon_vals = np.array([
            [0.0, 0.0],
            [-3.75, 356.25],
            [-1.0, 0], [-0.01, 0], [0.5, 0],
            [2, 3.75], [3, 3.75], [4, 3.75], [5, 3.75], [6, 7.5],
            [350.5, 348.75], [351, 352.5], [354, 352.5],
            [355, 356.25], [358, 356.25],
            [358.7, 0], [359, 0], [360, 0], [361, 0],
            [362, 3.75], [363, 3.75], [364, 3.75], [365, 3.75], [366, 7.5],
            [-725.0, 356.25], [-722, 356.25], [-721, 0], [-719, 0.0],
            [-718, 3.75],
            [1234.56, 153.75], [-1234.56, 206.25]])
        lon_test_vals = lon_vals[:, 0]
        lon_expect_vals = lon_vals[:, 1]
        lon_coord_vals = self.cube.coord('longitude').points
        lon_expect_vals = [near_value(v, lon_coord_vals)
                           for v in lon_expect_vals]
        lon_nearest_inds = [
            iintrp.nearest_neighbour_indices(self.cube,
                                             [('longitude', point_val)])
            for point_val in lon_test_vals]
        lon_nearest_vals = [lon_coord_vals[i[1]] for i in lon_nearest_inds]
        self.assertArrayAlmostEqual(lon_nearest_vals, lon_expect_vals)


class TestNearestNeighbourAdditional(tests.IrisTest):
    """
    More detailed testing for coordinate nearest_neighbour function.

    Includes (especially) circular operation.

    """
    def _test_nn_breakpoints(self, points, breaks, expected,
                             bounds=None, guess_bounds=False, bounds_point=0.2,
                             circular=False,
                             test_min=-500.0, test_max=750.0):
        """
        Make a test coordinate + test the nearest-neighbour calculation

        Check that the result index changes at the specified points.
        Includes support for circular and bounded cases.

        Args:

            points : 1-d array-like
                Points of the test coordinate (see also Kwargs)
            breaks : list
                Input points at which we expect the output result to change
            expected : list
                Expected results (cell index values)
                Length must == len(breaks) + 1
                result == expected[i] between breaks[i] and breaks[i+1]

        Kwargs:

            bounds : 2d array-like
                use these bounds
            guess_bounds, bounds_point : bool, float
                use guessed bounds
            circular : bool
                make a circular coordinate (360 degrees)
            test_min, test_max : float
                outer extreme values (non-circular only)

        """
        points = np.array(points, np.float)
        if bounds:
            bounds = np.array(bounds, np.float)
        assert len(expected) == len(breaks) + 1
        if circular:
            breaks = np.array(breaks)
            breaks = np.hstack([i * 360.0 + breaks for i in range(-2, 3)])
            lower_lims = breaks[:-1]
            upper_lims = breaks[1:]
            expected = np.hstack([expected[1:] for i in range(-2, 3)])
        else:
            lower_lims = np.hstack(([test_min], breaks))
            upper_lims = np.hstack((breaks, [test_max]))

        # construct coord : AuxCoord, or DimCoord if it needs to be circular
        if circular:
            test_coord = iris.coords.DimCoord(points,
                                              bounds=bounds,
                                              long_name='x',
                                              units=iris.unit.Unit('degrees'),
                                              circular=True)
        else:
            test_coord = iris.coords.AuxCoord(points,
                                              bounds=bounds,
                                              long_name='x',
                                              units=iris.unit.Unit('degrees'))
        if guess_bounds:
            test_coord.guess_bounds(bounds_point)

        # test at a few 'random' points within each supposed result region
        test_fractions = np.array([0.01, 0.2, 0.45, 0.75, 0.99])
        for (lower, upper, expect) in zip(lower_lims, upper_lims, expected):
            test_pts = lower + test_fractions * (upper - lower)
            results = [test_coord.nearest_neighbour_index(x) for x in test_pts]
            self.assertTrue(np.all([r == expect for r in results]))

    def test_nearest_neighbour_circular(self):
        # First test (simplest): ascending-order, unbounded
        points = [0.0, 90.0, 180.0, 270.0]
        breaks = [45.0, 135.0, 225.0]
        results = [0, 1, 2, 3]
        self._test_nn_breakpoints(points, breaks, results)

        # same, but *CIRCULAR*
        breaks_circ = [-45.0] + breaks
        results_circ = [3] + results
        self._test_nn_breakpoints(points, breaks_circ, results_circ,
                                  circular=True)

        # repeat circular test with different coordinate offsets
        offset = 32.7
        points_offset = np.array(points) + offset
        breaks_offset = np.array(breaks_circ) + offset
        self._test_nn_breakpoints(points_offset, breaks_offset, results_circ,
                                  circular=True)

        offset = -106.3
        points_offset = np.array(points) + offset
        breaks_offset = np.array(breaks_circ) + offset
        self._test_nn_breakpoints(points_offset, breaks_offset, results_circ,
                                  circular=True)

        # ascending order, guess-bounded
        # N.B. effect of bounds_position = 2/3
        #   x_bounds = [[-60, 30], [30, 120], [120, 210], [210, 300]]
        points = [0.0, 90, 180, 270]
        breaks = [30.0, 120.0, 210.0]
        results = [0, 1, 2, 3]
        self._test_nn_breakpoints(points, breaks, results,
                                  guess_bounds=True, bounds_point=2.0 / 3)

        # same but circular...
        breaks_circ = [-60.0] + breaks
        results_circ = [3] + results
        self._test_nn_breakpoints(points, breaks_circ, results_circ,
                                  guess_bounds=True, bounds_point=2.0 / 3,
                                  circular=True)

    def test_nearest_neighbour_descending_circular(self):
        # descending order, unbounded
        points = [270.0, 180, 90, 0]
        breaks = [45.0, 135.0, 225.0]
        results = [3, 2, 1, 0]
        self._test_nn_breakpoints(points, breaks, results)

        # same but circular...
        breaks = [-45.0] + breaks
        results = [0] + results
        self._test_nn_breakpoints(points, breaks, results, circular=True)

        # repeat circular test with different coordinate offsets
        offset = 32.7
        points_offset = np.array(points) + offset
        breaks_offset = np.array(breaks) + offset
        self._test_nn_breakpoints(points_offset, breaks_offset, results,
                                  circular=True)

        offset = -106.3
        points_offset = np.array(points) + offset
        breaks_offset = np.array(breaks) + offset
        self._test_nn_breakpoints(points_offset, breaks_offset, results,
                                  circular=True)

        # descending order, guess-bounded
        points = [250.0, 150, 50, -50]
        # N.B. equivalent effect of bounds_position = 0.4
        # x_bounds = [[290, 190], [190, 90], [90, -10], [-10, -110]]
        breaks = [-10.0, 90.0, 190.0]
        results = [3, 2, 1, 0]
        self._test_nn_breakpoints(points, breaks, results,
                                  guess_bounds=True, bounds_point=0.4)
        # same but circular...
        breaks = [-110.0] + breaks
        results = [0] + results
        self._test_nn_breakpoints(points, breaks, results,
                                  guess_bounds=True, bounds_point=0.4,
                                  circular=True)

    def test_nearest_neighbour_odd_bounds(self):
        # additional: test with overlapping bounds
        points = [0.0, 90, 180, 270]
        bounds = [[-90.0, 90], [0, 180], [90, 270], [180, 360]]
        breaks = [45.0, 135.0, 225.0]
        results = [0, 1, 2, 3]
        self._test_nn_breakpoints(points, breaks, results, bounds=bounds)

        # additional: test with disjoint bounds
        points = [40.0, 90, 150, 270]
        bounds = [[0, 60], [70, 90], [140, 200], [210, 360]]
        breaks = [65.0, 115.0, 205.0]
        results = [0, 1, 2, 3]
        self._test_nn_breakpoints(points, breaks, results, bounds=bounds)

    def test_nearest_neighbour_scalar(self):
        points = [1.0]
        breaks = []
        results = [0]
        self._test_nn_breakpoints(points, breaks, results)

    def test_nearest_neighbour_nonmonotonic(self):
        # a bounded example
        points = [3.0,  4.0,  1.0,  7.0, 10.0]
        bounds = [[2.5,  3.5],
                  [3.5,  4.5],
                  [0.5,  1.5],
                  [6.5,  7.5],
                  [9.5, 10.5]]
        breaks = [2.0, 3.5, 5.5, 8.5]
        results = [2, 0, 1, 3, 4]
        self._test_nn_breakpoints(points, breaks, results, bounds=bounds)

        # a pointwise example
        points = [3.0,  3.5,  1.0,  8.0, 12.0]
        breaks = [2.0, 3.25, 5.75, 10.0]
        results = [2, 0, 1, 3, 4]
        self._test_nn_breakpoints(points, breaks, results)

        # NOTE: no circular cases, as AuxCoords _cannot_ be circular.


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_intersect
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the intersection of Coords

"""
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import numpy as np

import iris
import iris.cube
import iris.coord_systems
import iris.coords
import iris.tests.stock


class TestCubeIntersectTheoretical(tests.IrisTest):
    def test_simple_intersect(self):
        cube = iris.cube.Cube(np.array([[1,2,3,4,5],
                                           [2,3,4,5,6],
                                           [3,4,5,6,7],
                                           [4,5,6,7,8],
                                           [5,6,7,8,9]], dtype=np.int32))

        lonlat_cs = iris.coord_systems.RotatedGeogCS(10, 20)
        cube.add_dim_coord(iris.coords.DimCoord(np.arange(5, dtype=np.float32) * 90 - 180, 'longitude', units='degrees', coord_system=lonlat_cs), 1)
        cube.add_dim_coord(iris.coords.DimCoord(np.arange(5, dtype=np.float32) * 45 - 90, 'latitude', units='degrees', coord_system=lonlat_cs), 0)
        cube.add_aux_coord(iris.coords.DimCoord(points=np.int32(11), long_name='pressure', units='Pa'))
        cube.rename("temperature")
        cube.units = "K"

        cube2 = iris.cube.Cube(np.array([[1,2,3,4,5],
                                            [2,3,4,5,6],
                                            [3,4,5,6,7],
                                            [4,5,6,7,8],
                                            [5,6,7,8,50]], dtype=np.int32))

        lonlat_cs = iris.coord_systems.RotatedGeogCS(10, 20)
        cube2.add_dim_coord(iris.coords.DimCoord(np.arange(5, dtype=np.float32) * 90, 'longitude', units='degrees', coord_system=lonlat_cs), 1)
        cube2.add_dim_coord(iris.coords.DimCoord(np.arange(5, dtype=np.float32) * 45 - 90, 'latitude', units='degrees', coord_system=lonlat_cs), 0)
        cube2.add_aux_coord(iris.coords.DimCoord(points=np.int32(11), long_name='pressure', units='Pa'))
        cube2.rename("")

        r = iris.analysis.maths.intersection_of_cubes(cube, cube2)
        self.assertCML(r, ('cdm', 'test_simple_cube_intersection.cml'))


class TestCoordIntersect(tests.IrisTest):
    def test_commutative(self):
        step = 4.0
        c1 = iris.coords.DimCoord(np.arange(100) * step)
        offset_points = c1.points.copy()
        offset_points -= step * 30
        c2 = c1.copy(points=offset_points)

        i1 = c1.intersect(c2)
        i2 = c2.intersect(c1)
        self.assertEqual(i1, i2)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_io_init
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the io/__init__.py module.

"""
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import unittest
from io import BytesIO

import iris.fileformats as iff
import iris.io.format_picker as fp
import iris.io


class TestDecodeUri(unittest.TestCase):
    def test_decode_uri(self):
        tests = {
            '/data/local/someDir/PP/COLPEX/COLPEX_16a_pj001.pp': (
                'file', '/data/local/someDir/PP/COLPEX/COLPEX_16a_pj001.pp'
            ),
            'C:\data\local\someDir\PP\COLPEX\COLPEX_16a_pj001.pp': (
                'file', 'C:\data\local\someDir\PP\COLPEX\COLPEX_16a_pj001.pp'
            ),
            'file:///data/local/someDir/PP/COLPEX/COLPEX_16a_pj001.pp': (
                'file', '///data/local/someDir/PP/COLPEX/COLPEX_16a_pj001.pp'
            ),
            'http://www.somehost.com:8080/resource/thing.grib': (
                'http', '//www.somehost.com:8080/resource/thing.grib'
            ),
            '/data/local/someDir/2013-11-25T13:49:17.632797': (
                'file', '/data/local/someDir/2013-11-25T13:49:17.632797'
            ),
        }
        for uri, pair in tests.items():
            self.assertEqual(pair, iris.io.decode_uri(uri))


class TestFileFormatPicker(tests.IrisTest):
    def test_known_formats(self):
        self.assertString(str(iff.FORMAT_AGENT),
                          tests.get_result_path(('file_load',
                                                 'known_loaders.txt')))

    @tests.skip_data
    def test_format_picker(self):
        # ways to test the format picker = list of (format-name, file-spec)
        test_specs = [
            ('NetCDF',
                ['NetCDF', 'global', 'xyt', 'SMALL_total_column_co2.nc']),
            ('NetCDF 64 bit offset format',
                ['NetCDF', 'global', 'xyt', 'SMALL_total_column_co2.nc.k2']),
            ('NetCDF_v4',
                ['NetCDF', 'global', 'xyt', 'SMALL_total_column_co2.nc4.k3']),
            ('NetCDF_v4',
                ['NetCDF', 'global', 'xyt', 'SMALL_total_column_co2.nc4.k4']),
            ('UM Fieldsfile (FF) post v5.2',
                ['FF', 'n48_multi_field']),
            ('GRIB',
                ['GRIB', 'grib1_second_order_packing', 'GRIB_00008_FRANX01']),
            ('GRIB',
                ['GRIB', 'jpeg2000', 'file.grib2']),
            ('UM Post Processing file (PP)',
                ['PP', 'simple_pp', 'global.pp']),
            ('UM Fieldsfile (FF) ancillary',
                ['FF', 'ancillary_fixed_length_header']),
#            ('BUFR',
#                ['BUFR', 'mss', 'BUFR_Samples',
#                 'JUPV78_EGRR_121200_00002501']),
            ('NIMROD',
                ['NIMROD', 'uk2km', 'WO0000000003452',
                 '201007020900_u1096_ng_ey00_visibility0180_screen_2km']),
#            ('NAME',
#                ['NAME', '20100509_18Z_variablesource_12Z_VAAC',
#                 'Fields_grid1_201005110000.txt']),
        ]

        # test that each filespec is identified as the expected format
        for (expected_format_name, file_spec) in test_specs:
            test_path = tests.get_data_path(file_spec)
            with open(test_path, 'r') as test_file:
                a = iff.FORMAT_AGENT.get_spec(test_path, test_file)
                self.assertEqual(a.name, expected_format_name)

    def test_format_picker_nodata(self):
        # The following is to replace the above at some point as no real files
        # are required.
        # (Used binascii.unhexlify() to convert from hex to binary)

        # Packaged grib, magic number offset by set length, this length is
        # specific to WMO bulletin headers
        header_lengths = [21, 80, 41, 42]
        for header_length in header_lengths:
            binary_string = header_length * '\x00' + 'GRIB' + '\x00' * 100
            with BytesIO('rw') as bh:
                bh.write(binary_string)
                bh.name = 'fake_file_handle'
                a = iff.FORMAT_AGENT.get_spec(bh.name, bh)
            self.assertEqual(a.name, 'GRIB')

    def test_open_dap(self):
        # tests that *ANY* http or https URL is seen as an OPeNDAP service.
        # This may need to change in the future if other protocols are
        # supported.
        DAP_URI = 'http://geoport.whoi.edu/thredds/dodsC/bathy/gom15'
        a = iff.FORMAT_AGENT.get_spec(DAP_URI, None)
        self.assertEqual(a.name, 'NetCDF OPeNDAP')


@tests.skip_data
class TestFileExceptions(tests.IrisTest):
    def test_pp_little_endian(self):
        filename = tests.get_data_path(('PP', 'aPPglob1', 'global_little_endian.pp'))
        self.assertRaises(ValueError, iris.load_cube, filename)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_iterate
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the iteration of cubes in step.

"""
# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import itertools
import operator
import random
import warnings

import numpy as np

import iris
import iris.analysis
import iris.iterate
import iris.tests.stock


class TestIterateFunctions(tests.IrisTest):

    def setUp(self):
        self.cube_a = iris.tests.stock.realistic_4d()[0, 0:5, 0:10, 0:12]
        self.cube_b = iris.tests.stock.realistic_4d()[1, 0:5, 0:10, 0:12]
        self.coord_names = ['grid_latitude', 'grid_longitude']

        # Modify elements of cube_b to introduce additional differences
        self.cube_b.attributes['source'] = 'Iris iterate test case'
        self.cube_b.add_aux_coord(iris.coords.AuxCoord(23, long_name='other'))

    def test_izip_no_args(self):
        with self.assertRaises(TypeError):
            iris.iterate.izip()
        with self.assertRaises(TypeError):
            iris.iterate.izip(coords=self.coord_names)
        with self.assertRaises(TypeError):
            iris.iterate.izip(coords=self.coord_names, ordered=False)

    def test_izip_input_collections(self):
        # Should work with one or more cubes as args
        iris.iterate.izip(self.cube_a, coords=self.coord_names)
        iris.iterate.izip(self.cube_a, self.cube_a, coords=self.coord_names)
        iris.iterate.izip(self.cube_a, self.cube_b, coords=self.coord_names)
        iris.iterate.izip(self.cube_a, self.cube_b, self.cube_a,
                          coords=self.coord_names)
        # Check unpacked collections
        cubes = [self.cube_a] * 10
        iris.iterate.izip(*cubes, coords=self.coord_names)
        cubes = tuple(cubes)
        iris.iterate.izip(*cubes, coords=self.coord_names)

    def test_izip_returns_iterable(self):
        try:
            # Raises an exception if arg is not iterable
            iter(iris.iterate.izip(self.cube_a, coords=self.coord_names))
        except TypeError:
            self.fail('iris.iterate.izip is not returning an iterable')

    def test_izip_unequal_slice_coords(self):
        # Create a cube with grid_latitude and grid_longitude coords
        # that differ in size from cube_a's
        other_cube = self.cube_a[0, 0:3, 0:3]
        nslices = self.cube_a.shape[0]
        i = 0
        for slice_a, slice_other in iris.iterate.izip(self.cube_a, other_cube,
                                                      coords=self.coord_names):
            slice_a_truth = self.cube_a[i, :, :]
            slice_other_truth = other_cube
            self.assertEqual(slice_a_truth, slice_a)
            self.assertEqual(slice_other_truth, slice_other)
            i += 1
        self.assertEqual(i, nslices)
        # Attempting to iterate over these incompatible coords should
        # raise an exception
        with self.assertRaises(ValueError):
            iris.iterate.izip(self.cube_a, other_cube)

    def test_izip_missing_slice_coords(self):
        # Remove latitude coordinate from one of the cubes
        other_cube = self.cube_b.copy()
        other_cube.remove_coord('grid_latitude')
        with self.assertRaises(iris.exceptions.CoordinateNotFoundError):
            iris.iterate.izip(self.cube_a, other_cube, coords=self.coord_names)
        # Create a cube with latitude and longitude rather than grid_latitude
        # and grid_longitude
        self.cube_b.coord('grid_latitude').rename('latitude')
        self.cube_b.coord('grid_longitude').rename('longitude')
        with self.assertRaises(iris.exceptions.CoordinateNotFoundError):
            iris.iterate.izip(self.cube_a, self.cube_b, coords=self.coord_names)

    def test_izip_onecube(self):
        # Should do the same as slices() but bearing in mind izip.next()
        # returns a tuple of cubes

        # Empty list as coords
        slice_iterator = self.cube_b.slices([])
        zip_iterator = iris.iterate.izip(self.cube_b, coords=[])
        for cube_slice in slice_iterator:
            # First element of tuple: (extractedcube, )
            zip_slice = zip_iterator.next()[0]
            self.assertEqual(cube_slice, zip_slice)
        with self.assertRaises(StopIteration):
            zip_iterator.next()    # Should raise exception if we continue
                                   # to try to iterate
        # Two coords
        slice_iterator = self.cube_b.slices(self.coord_names)
        zip_iterator = iris.iterate.izip(self.cube_b, coords=self.coord_names)
        for cube_slice in slice_iterator:
            # First element of tuple: (extractedcube, )
            zip_slice = zip_iterator.next()[0]
            self.assertEqual(cube_slice, zip_slice)
        with self.assertRaises(StopIteration):
            zip_iterator.next()    # Should raise exception if we continue
                                   #to try to iterate
        # One coord
        slice_iterator = self.cube_b.slices('grid_latitude')
        zip_iterator = iris.iterate.izip(self.cube_b, coords='grid_latitude')
        for cube_slice in slice_iterator:
            # First element of tuple: (extractedcube, )
            zip_slice = zip_iterator.next()[0]
            self.assertEqual(cube_slice, zip_slice)
        with self.assertRaises(StopIteration):
            zip_iterator.next()    # Should raise exception if we continue
                                   # to try to iterate
        # All coords
        slice_iterator = self.cube_b.slices(['level_height', 'grid_latitude',
                                             'grid_longitude'])
        zip_iterator = iris.iterate.izip(self.cube_b, coords=['level_height',
                                                              'grid_latitude',
                                                              'grid_longitude'])
        for cube_slice in slice_iterator:
            # First element of tuple: (extractedcube, )
            zip_slice = zip_iterator.next()[0]
            self.assertEqual(cube_slice, zip_slice)
        with self.assertRaises(StopIteration):
            zip_iterator.next()    # Should raise exception if we continue
                                   #to try to iterate

    def test_izip_same_cube(self):
        nslices = self.cube_b.shape[0]
        slice_iterator = self.cube_b.slices(self.coord_names)
        count = 0
        for slice_first, slice_second in iris.iterate.izip(self.cube_b,
                                                           self.cube_b,
                                                           coords=self.coord_names):
            self.assertEqual(slice_first, slice_second)  # Equal to each other
            self.assertEqual(slice_first, slice_iterator.next()) # Equal to the truth (from slice())
            count += 1
        self.assertEqual(count, nslices)
        # Another case
        nslices = self.cube_a.shape[0] * self.cube_a.shape[2] # Calc product of dimensions
                                                              # excluding the latitude
                                                              # (2nd data dim)
        slice_iterator = self.cube_a.slices('grid_latitude')
        count = 0
        for slice_first, slice_second in iris.iterate.izip(self.cube_a,
                                                           self.cube_a,
                                                           coords=['grid_latitude']):
            self.assertEqual(slice_first, slice_second)
            self.assertEqual(slice_first, slice_iterator.next()) # Equal to the truth (from slice())
            count += 1
        self.assertEqual(count, nslices)
        # third case - full iteration
        nslices = reduce(operator.mul, self.cube_b.shape)
        slice_iterator = self.cube_b.slices([])
        count = 0
        for slice_first, slice_second in iris.iterate.izip(self.cube_b,
                                                           self.cube_b,
                                                           coords=[]):
            self.assertEqual(slice_first, slice_second)
            self.assertEqual(slice_first, slice_iterator.next()) # Equal to the truth (from slice())
            count += 1
        self.assertEqual(count, nslices)

    def test_izip_subcube_of_same(self):
        for _ in xrange(3):
            super_cube = self.cube_a
            # Random int to pick coord value to calc subcube
            k = random.randint(0, super_cube.shape[0]-1)
            sub_cube = super_cube[k, :, :]
            super_slice_iterator = super_cube.slices(self.coord_names)
            j = 0
            for super_slice, sub_slice in iris.iterate.izip(super_cube, sub_cube,
                                                            coords=self.coord_names):
                self.assertEqual(sub_slice, sub_cube)    # This cube should not change
                                                         # as lat and long are the only
                                                         # data dimensions in this cube)
                self.assertEqual(super_slice, super_slice_iterator.next())
                if j == k:
                    self.assertEqual(super_slice, sub_slice)
                else:
                    self.assertNotEqual(super_slice, sub_slice)
                j += 1
            nslices = super_cube.shape[0]
            self.assertEqual(j, nslices)

    def test_izip_same_dims(self):
        # Check single coords slice
        nslices = reduce(operator.mul, self.cube_a.shape[1:])
        nslices_to_check = 20       # This is only approximate as we use random to select slices
        # Fraction of slices to check
        check_eq_probability = max(0.0, min(1.0, float(nslices_to_check)/nslices))

        ij_iterator = np.ndindex(self.cube_a.shape[1], self.cube_a.shape[2])
        count = 0
        for slice_a, slice_b in iris.iterate.izip(self.cube_a, self.cube_b,
                                                      coords='level_height'):
            i, j = ij_iterator.next()
            if random.random() <  check_eq_probability: # Check these slices
                slice_a_truth = self.cube_a[:, i, j]
                slice_b_truth = self.cube_b[:, i, j]
                self.assertEqual(slice_a_truth, slice_a)
                self.assertEqual(slice_b_truth, slice_b)
            count += 1
        self.assertEqual(count, nslices)
        # Two coords
        nslices = self.cube_a.shape[0]
        i_iterator = iter(xrange(self.cube_a.shape[0]))
        count = 0
        for slice_a, slice_b in iris.iterate.izip(self.cube_a, self.cube_b,
                                                      coords=self.coord_names):
            i = i_iterator.next()
            slice_a_truth = self.cube_a[i, :, :]
            slice_b_truth = self.cube_b[i, :, :]
            self.assertEqual(slice_a_truth, slice_a)
            self.assertEqual(slice_b_truth, slice_b)
            count += 1
        self.assertEqual(count, nslices)

    def test_izip_extra_dim(self):
        big_cube = self.cube_a
        # Remove first data dimension and associated coords
        little_cube = self.cube_b.copy()
        for factory in little_cube.aux_factories:
            little_cube.remove_aux_factory(factory)
        little_cube = little_cube[0]
        little_cube.remove_coord('model_level_number')
        little_cube.remove_coord('level_height')
        little_cube.remove_coord('sigma')
        # little_slice should remain the same as there are no other data dimensions
        little_slice_truth = little_cube
        i = 0
        for big_slice, little_slice in iris.iterate.izip(big_cube, little_cube,
                                                         coords=self.coord_names):
            big_slice_truth = big_cube[i, :, :]
            self.assertEqual(little_slice_truth, little_slice)
            self.assertEqual(big_slice_truth, big_slice)
            i += 1
        nslices = big_cube.shape[0]
        self.assertEqual(nslices, i)

        # Leave middle coord but move it from a data dimension to a scalar coord by slicing
        little_cube = self.cube_b[:, 0, :]

        # Now remove associated coord
        little_cube.remove_coord('grid_latitude')
        # Check we raise an exception if we request coords one of the cubes doesn't have
        with self.assertRaises(iris.exceptions.CoordinateNotFoundError):
            iris.iterate.izip(big_cube, little_cube, coords=self.coord_names)

        #little_slice should remain the same as there are no other data dimensions
        little_slice_truth = little_cube
        i = 0
        for big_slice, little_slice in iris.iterate.izip(big_cube, little_cube,
                                                         coords=['model_level_number',
                                                                 'grid_longitude']):
            big_slice_truth = big_cube[:, i, :]
            self.assertEqual(little_slice_truth, little_slice)
            self.assertEqual(big_slice_truth, big_slice)
            i += 1
        nslices = big_cube.shape[1]
        self.assertEqual(nslices, i)

        # Take a random slice reducing it to a 1d cube
        p = random.randint(0, self.cube_b.shape[0]-1)
        q = random.randint(0, self.cube_b.shape[2]-1)
        little_cube = self.cube_b[p, :, q]
        nslices = big_cube.shape[0]*big_cube.shape[2]
        nslices_to_check = 20   # This is only approximate as we use random to select slices
        # Fraction of slices to check
        check_eq_probability = max(0.0, min(1.0, float(nslices_to_check)/nslices))
        ij_iterator = np.ndindex(big_cube.shape[0], big_cube.shape[2])
        count = 0
        for big_slice, little_slice in iris.iterate.izip(big_cube, little_cube,
                                                         coords='grid_latitude'):
            i, j = ij_iterator.next()
            if random.random() <  check_eq_probability:
                big_slice_truth = big_cube[i, :, j]
                little_slice_truth = little_cube    # Just 1d so slice is entire cube
                self.assertEqual(little_slice_truth, little_slice)
                self.assertEqual(big_slice_truth, big_slice)
            count += 1
        self.assertEqual(count, nslices)

    def test_izip_different_shaped_coords(self):
        other = self.cube_b[0:-1]
        # Different 'z' coord shape - expect a ValueError
        with self.assertRaises(ValueError):
            iris.iterate.izip(self.cube_a, other, coords=self.coord_names)

    def test_izip_different_valued_coords(self):
        # Change a value in one of the coord points arrays so they are no longer identical
        new_points = self.cube_b.coord('model_level_number').points.copy()
        new_points[0] = 0
        self.cube_b.coord('model_level_number').points = new_points
        # slice coords
        latitude = self.cube_b.coord('grid_latitude')
        longitude = self.cube_b.coord('grid_longitude')
        # Same coord metadata and shape, but different values - check it produces a warning
        with warnings.catch_warnings():
            warnings.simplefilter("error")    # Cause all warnings to raise Exceptions
            with self.assertRaises(UserWarning):
                iris.iterate.izip(self.cube_a, self.cube_b,
                                  coords=self.coord_names)
            # Call with coordinates, rather than names
            with self.assertRaises(UserWarning):
                iris.iterate.izip(self.cube_a, self.cube_b, coords=[latitude,
                                                                    longitude])
        # Check it still iterates through as expected
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            nslices = self.cube_a.shape[0]
            i = 0
            for slice_a, slice_b in iris.iterate.izip(self.cube_a, self.cube_b,
                                                          coords=self.coord_names):
                slice_a_truth = self.cube_a[i, :, :]
                slice_b_truth = self.cube_b[i, :, :]
                self.assertEqual(slice_a_truth, slice_a)
                self.assertEqual(slice_b_truth, slice_b)
                self.assertNotEqual(slice_b, None)
                i += 1
            self.assertEqual(i, nslices)
            # Call with coordinate instances rather than coord names
            i = 0
            for slice_a, slice_b in iris.iterate.izip(self.cube_a, self.cube_b,
                                                          coords=[latitude,
                                                                  longitude]):
                slice_a_truth = self.cube_a[i, :, :]
                slice_b_truth = self.cube_b[i, :, :]
                self.assertEqual(slice_a_truth, slice_a)
                self.assertEqual(slice_b_truth, slice_b)
                i += 1
            self.assertEqual(i, nslices)

    def test_izip_ordered(self):
        # Remove coordinate that spans grid_latitude and
        # grid_longitude dimensions as this will be common between
        # the resulting cubes but differ in shape
        self.cube_b.remove_coord('surface_altitude')
        cube = self.cube_b.copy()
        cube.transpose([0, 2, 1])     #switch order of lat and lon
        nslices = self.cube_b.shape[0]
        # Default behaviour: ordered = True
        i = 0
        for slice_b, cube_slice in iris.iterate.izip(self.cube_b, cube,
                                                     coords=self.coord_names,
                                                     ordered=True):
            slice_b_truth = self.cube_b[i, :, :]
            cube_slice_truth = cube[i, :, :]
            # izip should transpose the slice to ensure order is [lat, lon]
            cube_slice_truth.transpose()
            self.assertEqual(slice_b_truth, slice_b)
            self.assertEqual(cube_slice_truth, cube_slice)
            i += 1
        self.assertEqual(i, nslices)
        # Alternative behaviour: ordered=False (retain original ordering)
        i = 0
        for slice_b, cube_slice in iris.iterate.izip(self.cube_b, cube,
                                                     coords=self.coord_names,
                                                     ordered=False):
            slice_b_truth = self.cube_b[i, :, :]
            cube_slice_truth = cube[i, :, :]
            self.assertEqual(slice_b_truth, slice_b)
            self.assertEqual(cube_slice_truth, cube_slice)
            i += 1
        self.assertEqual(i, nslices)

    def test_izip_use_in_analysis(self):
        # Calculate mean, collapsing vertical dimension
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            vertical_mean = self.cube_b.collapsed('model_level_number',
                                                  iris.analysis.MEAN)
        nslices = self.cube_b.shape[0]
        i = 0
        for slice_b, mean_slice in iris.iterate.izip(self.cube_b, vertical_mean,
                                                     coords=self.coord_names):
            slice_b_truth = self.cube_b[i, :, :]
            self.assertEqual(slice_b_truth, slice_b)
            # Should return same cube in each iteration
            self.assertEqual(vertical_mean, mean_slice)
            i += 1
        self.assertEqual(i, nslices)

    def test_izip_nd_non_ortho(self):
        cube1 = iris.cube.Cube(np.zeros((5, 5, 5)))
        cube1.add_aux_coord(iris.coords.AuxCoord(np.arange(5),
                                                 long_name='z'), [0])
        cube1.add_aux_coord(iris.coords.AuxCoord(np.arange(25).reshape(5, 5),
                                                 long_name='y'), [1, 2])
        cube1.add_aux_coord(iris.coords.AuxCoord(np.arange(25).reshape(5, 5),
                                                 long_name='x'), [1, 2])
        cube2 = cube1.copy()

        # The two coords are not orthogonal so we cannot use them with izip
        with self.assertRaises(ValueError):
            iris.iterate.izip(cube1, cube2, coords=['y', 'x'])

    def test_izip_nd_ortho(self):
        cube1 = iris.cube.Cube(np.zeros((5, 5, 5, 5, 5), dtype='f8'))
        cube1.add_dim_coord(iris.coords.DimCoord(np.arange(5, dtype='i8'),
                                                 long_name='z'), [0])
        cube1.add_aux_coord(iris.coords.AuxCoord(np.arange(25, dtype='i8').reshape(5, 5),
                                                 long_name='y'), [1,2])
        cube1.add_aux_coord(iris.coords.AuxCoord(np.arange(25, dtype='i8').reshape(5, 5),
                                                 long_name='x'), [3, 4])
        cube2 = cube1.copy()

        # The two coords are orthogonal so we can use them with izip
        it = iris.iterate.izip(cube1, cube2, coords=['y', 'x'])
        cubes = list(np.array(list(it)).flatten())
        self.assertCML(cubes, ('iterate', 'izip_nd_ortho.cml'))

    def _check_2d_slices(self):
        # Helper method to verify slices from izip match those from
        # cube.slices().
        slice_a_iterator = self.cube_a.slices(self.coord_names)
        slice_b_iterator = self.cube_b.slices(self.coord_names)
        nslices = self.cube_b.shape[0]
        count = 0
        for slice_a, slice_b in iris.iterate.izip(self.cube_a,
                                                  self.cube_b,
                                                  coords=self.coord_names):
            self.assertEqual(slice_a, next(slice_a_iterator))
            self.assertEqual(slice_b, next(slice_b_iterator))
            count += 1
        self.assertEqual(count, nslices)

    def test_izip_extra_coords_step_dim(self):
        # Add extra different coords to cubes along the dimension we are
        # stepping through.
        coord_a = iris.coords.AuxCoord(np.arange(self.cube_a.shape[0]),
                                       long_name='another on a')
        self.cube_a.add_aux_coord(coord_a, 0)
        coord_b = iris.coords.AuxCoord(np.arange(self.cube_b.shape[0]),
                                       long_name='another on b')
        self.cube_b.add_aux_coord(coord_b, 0)
        # Check slices.
        self._check_2d_slices()

    def test_izip_extra_coords_slice_dim(self):
        # Add extra different coords to cubes along a dimension we are
        # not stepping through.
        coord_a = iris.coords.AuxCoord(np.arange(self.cube_a.shape[1]),
                                       long_name='another on a')
        self.cube_a.add_aux_coord(coord_a, 1)
        coord_b = iris.coords.AuxCoord(np.arange(self.cube_b.shape[1]),
                                       long_name='another on b')
        self.cube_b.add_aux_coord(coord_b, 1)
        self._check_2d_slices()

    def test_izip_extra_coords_both_slice_dims(self):
        # Add extra different coords to cubes along the dimensions we are
        # not stepping through.
        coord_a = iris.coords.AuxCoord(np.arange(self.cube_a.shape[1]),
                                       long_name='another on a')
        self.cube_a.add_aux_coord(coord_a, 1)
        coord_b = iris.coords.AuxCoord(np.arange(self.cube_b.shape[2]),
                                       long_name='another on b')
        self.cube_b.add_aux_coord(coord_b, 2)
        self._check_2d_slices()

    def test_izip_no_common_coords_on_step_dim(self):
        # Change metadata on all coords along the dimension we are
        # stepping through.
        self.cube_a.coord('model_level_number').rename('foo')
        self.cube_a.coord('sigma').rename('bar')
        self.cube_a.coord('level_height').rename('woof')
        # izip should step through them as a product.
        slice_a_iterator = self.cube_a.slices(self.coord_names)
        slice_b_iterator = self.cube_b.slices(self.coord_names)
        product_iterator = itertools.product(slice_a_iterator,
                                             slice_b_iterator)
        nslices = self.cube_a.shape[0] * self.cube_b.shape[0]
        count = 0
        for slice_a, slice_b in iris.iterate.izip(self.cube_a,
                                                  self.cube_b,
                                                  coords=self.coord_names):
            expected_a, expected_b = next(product_iterator)
            self.assertEqual(slice_a, expected_a)
            self.assertEqual(slice_b, expected_b)
            count += 1
        self.assertEqual(count, nslices)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_load
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the main loading API.

"""
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import iris
import iris.io


@tests.skip_data
class TestLoad(tests.IrisTest):
    def test_normal(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
        )
        cubes = iris.load(paths)
        self.assertEqual(len(cubes), 1)

    def test_nonexist(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
            tests.get_data_path(['PP', '_guaranteed_non_exist.pp']),
        )
        with self.assertRaises(IOError) as error_trap:
            cubes = iris.load(paths)
        self.assertTrue(error_trap.exception.message.startswith(
            'One or more of the files specified did not exist'))

    def test_nonexist_wild(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
            tests.get_data_path(['PP', '_guaranteed_non_exist_*.pp']),
        )
        with self.assertRaises(IOError) as error_trap:
            cubes = iris.load(paths)
        self.assertTrue(error_trap.exception.message.startswith(
            'One or more of the files specified did not exist'))

    def test_bogus(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
        )
        cubes = iris.load(paths, 'wibble')
        self.assertEqual(len(cubes), 0)

    def test_real_and_bogus(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
        )
        cubes = iris.load(paths, ('air_temperature', 'wibble'))
        self.assertEqual(len(cubes), 1)

    def test_duplicate(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
            tests.get_data_path(['PP', 'aPPglob1', 'gl?bal.pp'])
        )
        cubes = iris.load(paths)
        self.assertEqual(len(cubes), 2)


@tests.skip_data
class TestLoadCube(tests.IrisTest):
    def test_normal(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
        )
        cube = iris.load_cube(paths)

    def test_not_enough(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
        )
        with self.assertRaises(iris.exceptions.ConstraintMismatchError):
            iris.load_cube(paths, 'wibble')

    def test_too_many(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
            tests.get_data_path(['PP', 'aPPglob1', 'gl?bal.pp'])
        )
        with self.assertRaises(iris.exceptions.ConstraintMismatchError):
            iris.load_cube(paths)


@tests.skip_data
class TestLoadCubes(tests.IrisTest):
    def test_normal(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
        )
        cubes = iris.load_cubes(paths)
        self.assertEqual(len(cubes), 1)

    def test_not_enough(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
        )
        with self.assertRaises(iris.exceptions.ConstraintMismatchError):
            iris.load_cubes(paths, 'wibble')

    def test_not_enough_multi(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
        )
        with self.assertRaises(iris.exceptions.ConstraintMismatchError):
            iris.load_cubes(paths, ('air_temperature', 'wibble'))

    def test_too_many(self):
        paths = (
            tests.get_data_path(['PP', 'aPPglob1', 'global.pp']),
            tests.get_data_path(['PP', 'aPPglob1', 'gl?bal.pp'])
        )
        with self.assertRaises(iris.exceptions.ConstraintMismatchError):
            iris.load_cube(paths)


class TestOpenDAP(tests.IrisTest):
    def test_load(self):
        # Check that calling iris.load_* with a http URI triggers a call to
        # ``iris.io.load_http``

        url = 'http://geoport.whoi.edu:80/thredds/dodsC/bathy/gom15'

        class LoadHTTPCalled(Exception):
            pass

        def new_load_http(passed_urls, *args, **kwargs):
            self.assertEqual(len(passed_urls), 1)
            self.assertEqual(url, passed_urls[0])
            raise LoadHTTPCalled()

        try:
            orig = iris.io.load_http
            iris.io.load_http = new_load_http

            for fn in [iris.load, iris.load_raw,
                       iris.load_cube, iris.load_cubes]:
                with self.assertRaises(LoadHTTPCalled):
                    fn(url)

        finally:
            iris.io.load_http = orig


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_mapping
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Tests map creation.

"""

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import numpy as np
import numpy.testing as np_testing
import cartopy.crs as ccrs

import iris
import iris.coord_systems
import iris.cube
import iris.tests.stock

# Run tests in no graphics mode if matplotlib is not available.
if tests.MPL_AVAILABLE:
    import matplotlib.pyplot as plt
    import iris.plot as iplt


@tests.skip_plot
class TestBasic(tests.GraphicsTest):
    cube = iris.tests.stock.realistic_4d()

    def test_contourf(self):
        cube = self.cube[0, 0]
        iplt.contourf(cube)
        self.check_graphic()

    def test_pcolor(self):
        cube = self.cube[0, 0]
        iplt.pcolor(cube)
        self.check_graphic()

    def test_unmappable(self):
        cube = self.cube[0, 0]
        cube.coord('grid_longitude').standard_name = None
        iplt.contourf(cube)
        self.check_graphic()

    def test_default_projection_and_extent(self):
        self.assertEqual(iplt.default_projection(self.cube),
                         ccrs.RotatedPole(357.5 - 180, 37.5)
                         )

        np_testing.assert_array_almost_equal(iplt.default_projection_extent(self.cube),
                                             (3.59579163e+02, 3.59669159e+02, -1.28250003e-01, -3.82499993e-02),
                                             decimal=3
                                             )


@tests.skip_data
@tests.skip_plot
class TestUnmappable(tests.GraphicsTest):
    def setUp(self):
        src_cube = iris.tests.stock.global_pp()

        # Make a cube that can't be located on the globe.
        cube = iris.cube.Cube(src_cube.data)
        cube.add_dim_coord(iris.coords.DimCoord(np.arange(96, dtype=np.float32) * 100, long_name='x', units='m'), 1)
        cube.add_dim_coord(iris.coords.DimCoord(np.arange(73, dtype=np.float32) * 100, long_name='y', units='m'), 0)
        cube.standard_name = 'air_temperature'
        cube.units = 'K'
        cube.assert_valid()
        self.cube = cube

    def test_simple(self):
        iplt.contourf(self.cube)
        self.check_graphic()


@tests.skip_data
@tests.skip_plot
class TestMappingSubRegion(tests.GraphicsTest):
    def setUp(self):
        cube_path = tests.get_data_path(('PP', 'aPProt1', 'rotatedMHtimecube.pp'))
        cube = iris.load_cube(cube_path)[0]
        # make the data smaller to speed things up.
        self.cube = cube[::10, ::10]

    def test_simple(self):
        # First sub-plot
        plt.subplot(221)
        plt.title('Default')
        iplt.contourf(self.cube)
        plt.gca().coastlines()

        # Second sub-plot
        plt.subplot(222, projection=ccrs.Mollweide(central_longitude=120))
        plt.title('Molleweide')
        iplt.contourf(self.cube)
        plt.gca().coastlines()

        # Third sub-plot (the projection part is redundant, but a useful
        # test none-the-less)
        ax = plt.subplot(223, projection=iplt.default_projection(self.cube))
        plt.title('Native')
        iplt.contour(self.cube)
        ax.coastlines()

        # Fourth sub-plot
        ax = plt.subplot(2, 2, 4, projection=ccrs.PlateCarree())
        plt.title('PlateCarree')
        iplt.contourf(self.cube)
        ax.coastlines()

        self.check_graphic()

    def test_default_projection_and_extent(self):
        self.assertEqual(iplt.default_projection(self.cube),
                          ccrs.RotatedPole(357.5 - 180, 37.5)
                          )

        np_testing.assert_array_almost_equal(iplt.default_projection_extent(self.cube),
                                             (313.01998901, 391.11999512, -22.48999977, 24.80999947)
                                             )

@tests.skip_data
@tests.skip_plot
class TestLowLevel(tests.GraphicsTest):
    def setUp(self):
        self.cube = iris.tests.stock.global_pp()
        self.few = 4
        self.few_levels = range(280, 300, 5)
        self.many_levels = np.linspace(self.cube.data.min(), self.cube.data.max(), 40)

    def test_simple(self):
        iplt.contour(self.cube)
        self.check_graphic()

    def test_params(self):
        iplt.contourf(self.cube, self.few)
        self.check_graphic()

        iplt.contourf(self.cube, self.few_levels)
        self.check_graphic()

        iplt.contourf(self.cube, self.many_levels)
        self.check_graphic()

    def test_keywords(self):
        iplt.contourf(self.cube, levels=self.few_levels)
        self.check_graphic()

        iplt.contourf(self.cube, levels=self.many_levels, alpha=0.5)
        self.check_graphic()


@tests.skip_data
@tests.skip_plot
class TestBoundedCube(tests.GraphicsTest):
    def setUp(self):
        self.cube = iris.tests.stock.global_pp()
        # Add some bounds to this data (this will actually make the bounds invalid as they
        # will straddle the north pole and overlap on the date line, but that doesn't matter for this test.)
        self.cube.coord('latitude').guess_bounds()
        self.cube.coord('longitude').guess_bounds()

    def test_pcolormesh(self):
        # pcolormesh can only be drawn in native coordinates (or more specifically, in coordinates that don't wrap).
        plt.axes(projection=ccrs.PlateCarree(central_longitude=180))
        iplt.pcolormesh(self.cube)
        self.check_graphic()

    def test_grid(self):
        iplt.outline(self.cube)
        self.check_graphic()

    def test_default_projection_and_extent(self):
        self.assertEqual(iplt.default_projection(self.cube),
                         ccrs.PlateCarree()
                         )

        np_testing.assert_array_almost_equal(
             iplt.default_projection_extent(self.cube),
             [0., 360., -89.99995422, 89.99998474]
                                             )

        np_testing.assert_array_almost_equal(
             iplt.default_projection_extent(self.cube, mode=iris.coords.BOUND_MODE),
             (-1.87499952, 358.12500048, -91.24995422, 91.24998474)
                                             )


@tests.skip_data
@tests.skip_plot
class TestLimitedAreaCube(tests.GraphicsTest):
    def setUp(self):
        cube_path = tests.get_data_path(('PP', 'aPProt1', 'rotated.pp'))
        self.cube = iris.load_cube(cube_path)[::20, ::20]
        self.cube.coord('grid_latitude').guess_bounds()
        self.cube.coord('grid_longitude').guess_bounds()

    def test_pcolormesh(self):
        iplt.pcolormesh(self.cube)
        self.check_graphic()

    def test_grid(self):
        iplt.pcolormesh(self.cube, facecolors='none', edgecolors='blue')
        # the result is a graphic which has coloured edges. This is a mpl bug, see
        # https://github.com/matplotlib/matplotlib/issues/1302
        self.check_graphic()

    def test_outline(self):
        iplt.outline(self.cube)
        self.check_graphic()

    def test_scatter(self):
        iplt.points(self.cube)
        plt.gca().coastlines()
        self.check_graphic()


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_merge
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test the cube merging mechanism.

"""

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import numpy as np

import iris
import iris.cube
import iris.exceptions
from iris.coords import DimCoord, AuxCoord
import iris.coords
import iris.tests.stock


class TestMixin(object):
    """
    Mix-in class for attributes & utilities common to these test cases.

    """
    def test_normal_cubes(self):
        cubes = iris.load(self._data_path)
        self.assertEqual(len(cubes), self._num_cubes)
        names = ['forecast_period', 'forecast_reference_time', 'level_height', 'model_level_number', 'sigma', 'source']
        axes = ['forecast_period', 'rt', 'z', 'z', 'z', 'source']
        self.assertCML(cubes, ['merge', self._prefix + '.cml'])

    def test_remerge(self):
        # After the merge process the coordinates within each cube can be in a
        # different order. Until that changes we can't compare the cubes
        # directly or with the CML ... so we just make sure the count stays
        # the same.
        cubes = iris.load(self._data_path)
        cubes2 = cubes.merge()
        self.assertEqual(len(cubes), len(cubes2))

    def test_duplication(self):
        cubes = iris.load(self._data_path)
        self.assertRaises(iris.exceptions.DuplicateDataError, (cubes + cubes).merge)
        cubes2 = (cubes + cubes).merge(unique=False)
        self.assertEqual(len(cubes2), 2 * len(cubes))


@tests.skip_data
class TestSingleCube(tests.IrisTest, TestMixin):
    def setUp(self):
        self._data_path = tests.get_data_path(('PP', 'globClim1', 'theta.pp'))
        self._num_cubes = 1
        self._prefix = 'theta'


@tests.skip_data
class TestMultiCube(tests.IrisTest, TestMixin):
    def setUp(self):
        self._data_path = tests.get_data_path(('PP', 'globClim1', 'dec_subset.pp'))
        self._num_cubes = 4
        self._prefix = 'dec'

    def test_coord_attributes(self):
        def custom_coord_callback(cube, field, filename):
            cube.coord('time').attributes['monty'] = 'python'
            cube.coord('time').attributes['brain'] = 'hurts'

        # Load slices, decorating a coord with custom attributes
        cubes = iris.load_raw(self._data_path, callback=custom_coord_callback)
        # Merge
        merged = iris.cube.CubeList._extract_and_merge(cubes, constraints=None, strict=False, merge_unique=False)
        # Check the custom attributes are in the merged cube
        for cube in merged:
            assert(cube.coord('time').attributes['monty'] == 'python')
            assert(cube.coord('time').attributes['brain'] == 'hurts')


@tests.skip_data
class TestColpex(tests.IrisTest):
    def setUp(self):
        self._data_path = tests.get_data_path(('PP', 'COLPEX', 'small_colpex_theta_p_alt.pp'))

    def test_colpex(self):
        cubes = iris.load(self._data_path)
        self.assertEqual(len(cubes), 3)
        self.assertCML(cubes, ('COLPEX', 'small_colpex_theta_p_alt.cml'))


@tests.skip_data
class TestDataMerge(tests.IrisTest):
    def test_extended_proxy_data(self):
        # Get the empty theta cubes for T+1.5 and T+2
        data_path = tests.get_data_path(
            ('PP', 'COLPEX', 'theta_and_orog_subset.pp'))
        phenom_constraint = iris.Constraint('air_potential_temperature')
        time_value_1 = 347921.33333332836627960205
        time_value_2 = 347921.83333333209156990051
        time_constraint1 = iris.Constraint(time=time_value_1)
        time_constraint2 = iris.Constraint(time=time_value_2)
        time_constraint_1_and_2 = iris.Constraint(
            time=lambda c: c in (time_value_1, time_value_2))
        cube1 = iris.load_cube(data_path, phenom_constraint & time_constraint1)
        cube2 = iris.load_cube(data_path, phenom_constraint & time_constraint2)

        # Merge the two halves
        cubes = iris.cube.CubeList([cube1, cube2]).merge(True)
        self.assertCML(cubes, ('merge', 'theta_two_times.cml'))

        # Make sure we get the same result directly from load
        cubes = iris.load_cube(data_path,
            phenom_constraint & time_constraint_1_and_2)
        self.assertCML(cubes, ('merge', 'theta_two_times.cml'))

    def test_real_data(self):
        data_path = tests.get_data_path(('PP', 'globClim1', 'theta.pp'))
        cubes = iris.load_raw(data_path)
        # Force the source 2-D cubes to load their data before the merge
        for cube in cubes:
            data = cube.data
        cubes = cubes.merge()
        self.assertCML(cubes, ['merge', 'theta.cml'])


class TestDimensionSplitting(tests.IrisTest):
    def _make_cube(self, a, b, c, data):
        cube_data = np.empty((4, 5), dtype=np.float32)
        cube_data[:] = data
        cube = iris.cube.Cube(cube_data)
        cube.add_dim_coord(DimCoord(np.array([0, 1, 2, 3, 4], dtype=np.int32), long_name='x', units='1'), 1)
        cube.add_dim_coord(DimCoord(np.array([0, 1, 2, 3], dtype=np.int32), long_name='y', units='1'), 0)
        cube.add_aux_coord(DimCoord(np.array([a], dtype=np.int32), long_name='a', units='1'))
        cube.add_aux_coord(DimCoord(np.array([b], dtype=np.int32), long_name='b', units='1'))
        cube.add_aux_coord(DimCoord(np.array([c], dtype=np.int32), long_name='c', units='1'))
        return cube

    def test_single_split(self):
        # Test what happens when a cube forces a simple, two-way split.
        cubes = []
        cubes.append(self._make_cube(0, 0, 0, 0))
        cubes.append(self._make_cube(0, 1, 1, 1))
        cubes.append(self._make_cube(1, 0, 2, 2))
        cubes.append(self._make_cube(1, 1, 3, 3))
        cubes.append(self._make_cube(2, 0, 4, 4))
        cubes.append(self._make_cube(2, 1, 5, 5))
        cube = iris.cube.CubeList(cubes).merge()
        self.assertCML(cube, ('merge', 'single_split.cml'))

    def test_multi_split(self):
        # Test what happens when a cube forces a three-way split.
        cubes = []
        cubes.append(self._make_cube(0, 0, 0, 0))
        cubes.append(self._make_cube(0, 0, 1, 1))
        cubes.append(self._make_cube(0, 1, 0, 2))
        cubes.append(self._make_cube(0, 1, 1, 3))
        cubes.append(self._make_cube(1, 0, 0, 4))
        cubes.append(self._make_cube(1, 0, 1, 5))
        cubes.append(self._make_cube(1, 1, 0, 6))
        cubes.append(self._make_cube(1, 1, 1, 7))
        cubes.append(self._make_cube(2, 0, 0, 8))
        cubes.append(self._make_cube(2, 0, 1, 9))
        cubes.append(self._make_cube(2, 1, 0, 10))
        cubes.append(self._make_cube(2, 1, 1, 11))
        cube = iris.cube.CubeList(cubes).merge()
        self.assertCML(cube, ('merge', 'multi_split.cml'))


class TestCombination(tests.IrisTest):
    def _make_cube(self, a, b, c, d, data=0):
        cube_data = np.empty((4, 5), dtype=np.float32)
        cube_data[:] = data
        cube = iris.cube.Cube(cube_data)
        cube.add_dim_coord(DimCoord(np.array([0, 1, 2, 3, 4], dtype=np.int32),
                                    long_name='x', units='1'), 1)
        cube.add_dim_coord(DimCoord(np.array([0, 1, 2, 3], dtype=np.int32),
                                    long_name='y', units='1'), 0)

        for name, value in zip(['a', 'b', 'c', 'd'], [a, b, c, d]):
            dtype = np.str if isinstance(value, basestring) else np.float32
            cube.add_aux_coord(AuxCoord(np.array([value], dtype=dtype),
                                        long_name=name, units='1'))

        return cube

    def test_separable_combination(self):
        cubes = iris.cube.CubeList()
        cubes.append(self._make_cube('2005', 'ECMWF',
                                     'HOPE-E, Sys 1, Met 1, ENSEMBLES', 0))
        cubes.append(self._make_cube('2005', 'ECMWF',
                                     'HOPE-E, Sys 1, Met 1, ENSEMBLES', 1))
        cubes.append(self._make_cube('2005', 'ECMWF',
                                     'HOPE-E, Sys 1, Met 1, ENSEMBLES', 2))
        cubes.append(self._make_cube('2026', 'UK Met Office',
                                     'HadGEM2, Sys 1, Met 1, ENSEMBLES', 0))
        cubes.append(self._make_cube('2026', 'UK Met Office',
                                     'HadGEM2, Sys 1, Met 1, ENSEMBLES', 1))
        cubes.append(self._make_cube('2026', 'UK Met Office',
                                     'HadGEM2, Sys 1, Met 1, ENSEMBLES', 2))
        cubes.append(self._make_cube('2002', 'CERFACS',
                                     'GELATO, Sys 0, Met 1, ENSEMBLES', 0))
        cubes.append(self._make_cube('2002', 'CERFACS',
                                     'GELATO, Sys 0, Met 1, ENSEMBLES', 1))
        cubes.append(self._make_cube('2002', 'CERFACS',
                                     'GELATO, Sys 0, Met 1, ENSEMBLES', 2))
        cubes.append(self._make_cube('2002', 'IFM-GEOMAR',
                                     'ECHAM5, Sys 1, Met 10, ENSEMBLES', 0))
        cubes.append(self._make_cube('2002', 'IFM-GEOMAR',
                                     'ECHAM5, Sys 1, Met 10, ENSEMBLES', 1))
        cubes.append(self._make_cube('2002', 'IFM-GEOMAR',
                                     'ECHAM5, Sys 1, Met 10, ENSEMBLES', 2))
        cubes.append(self._make_cube('2502', 'UK Met Office',
                                     'HadCM3, Sys 51, Met 10, ENSEMBLES', 0))
        cubes.append(self._make_cube('2502', 'UK Met Office',
                                     'HadCM3, Sys 51, Met 11, ENSEMBLES', 0))
        cubes.append(self._make_cube('2502', 'UK Met Office',
                                     'HadCM3, Sys 51, Met 12, ENSEMBLES', 0))
        cubes.append(self._make_cube('2502', 'UK Met Office',
                                     'HadCM3, Sys 51, Met 13, ENSEMBLES', 0))
        cubes.append(self._make_cube('2502', 'UK Met Office',
                                     'HadCM3, Sys 51, Met 14, ENSEMBLES', 0))
        cubes.append(self._make_cube('2502', 'UK Met Office',
                                     'HadCM3, Sys 51, Met 15, ENSEMBLES', 0))
        cubes.append(self._make_cube('2502', 'UK Met Office',
                                     'HadCM3, Sys 51, Met 16, ENSEMBLES', 0))
        cubes.append(self._make_cube('2502', 'UK Met Office',
                                     'HadCM3, Sys 51, Met 17, ENSEMBLES', 0))
        cubes.append(self._make_cube('2502', 'UK Met Office',
                                     'HadCM3, Sys 51, Met 18, ENSEMBLES', 0))
        cube = cubes.merge()
        self.assertCML(cube, ('merge', 'separable_combination.cml'),
                       checksum=False)


class TestDimSelection(tests.IrisTest):
    def _make_cube(self, a, b, data=0, a_dim=False, b_dim=False):
        cube_data = np.empty((4, 5), dtype=np.float32)
        cube_data[:] = data
        cube = iris.cube.Cube(cube_data)
        cube.add_dim_coord(DimCoord(np.array([0, 1, 2, 3, 4], dtype=np.int32),
                                    long_name='x', units='1'), 1)
        cube.add_dim_coord(DimCoord(np.array([0, 1, 2, 3], dtype=np.int32),
                                    long_name='y', units='1'), 0)

        for name, value, dim in zip(['a', 'b'], [a, b], [a_dim, b_dim]):
            dtype = np.str if isinstance(value, basestring) else np.float32
            ctype = DimCoord if dim else AuxCoord
            coord = ctype(np.array([value], dtype=dtype),
                          long_name=name, units='1')
            cube.add_aux_coord(coord)

        return cube

    def test_string_a_with_aux(self):
        templates = (('a', 0), ('b', 1), ('c', 2), ('d', 3))
        cubes = [self._make_cube(a, b) for a, b in templates]
        cube = iris.cube.CubeList(cubes).merge()[0]
        self.assertCML(cube, ('merge', 'string_a_with_aux.cml'),
                       checksum=False)
        self.assertIsInstance(cube.coord('a'), AuxCoord)
        self.assertIsInstance(cube.coord('b'), DimCoord)
        self.assertTrue(cube.coord('b') in cube.dim_coords)

    def test_string_b_with_aux(self):
        templates = ((0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'))
        cubes = [self._make_cube(a, b) for a, b in templates]
        cube = iris.cube.CubeList(cubes).merge()[0]
        self.assertCML(cube, ('merge', 'string_b_with_aux.cml'),
                       checksum=False)
        self.assertIsInstance(cube.coord('a'), DimCoord)
        self.assertTrue(cube.coord('a') in cube.dim_coords)
        self.assertIsInstance(cube.coord('b'), AuxCoord)

    def test_string_a_with_dim(self):
        templates = (('a', 0), ('b', 1), ('c', 2), ('d', 3))
        cubes = [self._make_cube(a, b, b_dim=True) for a, b in templates]
        cube = iris.cube.CubeList(cubes).merge()[0]
        self.assertCML(cube, ('merge', 'string_a_with_dim.cml'),
                       checksum=False)
        self.assertIsInstance(cube.coord('a'), AuxCoord)
        self.assertIsInstance(cube.coord('b'), DimCoord)
        self.assertTrue(cube.coord('b') in cube.dim_coords)

    def test_string_b_with_aux(self):
        templates = ((0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'))
        cubes = [self._make_cube(a, b, a_dim=True) for a, b in templates]
        cube = iris.cube.CubeList(cubes).merge()[0]
        self.assertCML(cube, ('merge', 'string_b_with_dim.cml'),
                       checksum=False)
        self.assertIsInstance(cube.coord('a'), DimCoord)
        self.assertTrue(cube.coord('a') in cube.dim_coords)
        self.assertIsInstance(cube.coord('b'), AuxCoord)

    def test_string_a_b(self):
        templates = (('a', '0'), ('b', '1'), ('c', '2'), ('d', '3'))
        cubes = [self._make_cube(a, b) for a, b in templates]
        cube = iris.cube.CubeList(cubes).merge()[0]
        self.assertCML(cube, ('merge', 'string_a_b.cml'),
                       checksum=False)
        self.assertIsInstance(cube.coord('a'), AuxCoord)
        self.assertIsInstance(cube.coord('b'), AuxCoord)

    def test_a_aux_b_aux(self):
        templates = ((0, 10), (1, 11), (2, 12), (3, 13))
        cubes = [self._make_cube(a, b) for a, b in templates]
        cube = iris.cube.CubeList(cubes).merge()[0]
        self.assertCML(cube, ('merge', 'a_aux_b_aux.cml'),
                       checksum=False)
        self.assertIsInstance(cube.coord('a'), DimCoord)
        self.assertTrue(cube.coord('a') in cube.dim_coords)
        self.assertIsInstance(cube.coord('b'), DimCoord)
        self.assertTrue(cube.coord('b') in cube.aux_coords)

    def test_a_aux_b_dim(self):
        templates = ((0, 10), (1, 11), (2, 12), (3, 13))
        cubes = [self._make_cube(a, b, b_dim=True) for a, b in templates]
        cube = iris.cube.CubeList(cubes).merge()[0]
        self.assertCML(cube, ('merge', 'a_aux_b_dim.cml'),
                       checksum=False)
        self.assertIsInstance(cube.coord('a'), DimCoord)
        self.assertTrue(cube.coord('a') in cube.aux_coords)
        self.assertIsInstance(cube.coord('b'), DimCoord)
        self.assertTrue(cube.coord('b') in cube.dim_coords)

    def test_a_dim_b_aux(self):
        templates = ((0, 10), (1, 11), (2, 12), (3, 13))
        cubes = [self._make_cube(a, b, a_dim=True) for a, b in templates]
        cube = iris.cube.CubeList(cubes).merge()[0]
        self.assertCML(cube, ('merge', 'a_dim_b_aux.cml'),
                       checksum=False)
        self.assertIsInstance(cube.coord('a'), DimCoord)
        self.assertTrue(cube.coord('a') in cube.dim_coords)
        self.assertIsInstance(cube.coord('b'), DimCoord)
        self.assertTrue(cube.coord('b') in cube.aux_coords)

    def test_a_dim_b_dim(self):
        templates = ((0, 10), (1, 11), (2, 12), (3, 13))
        cubes = [self._make_cube(a, b, a_dim=True, b_dim=True) \
                     for a, b in templates]
        cube = iris.cube.CubeList(cubes).merge()[0]
        self.assertCML(cube, ('merge', 'a_dim_b_dim.cml'),
                       checksum=False)
        self.assertIsInstance(cube.coord('a'), DimCoord)
        self.assertTrue(cube.coord('a') in cube.dim_coords)
        self.assertIsInstance(cube.coord('b'), DimCoord)
        self.assertTrue(cube.coord('b') in cube.aux_coords)


class TestTimeTripleMerging(tests.IrisTest):
    def _make_cube(self, a, b, c, data=0):
        cube_data = np.empty((4, 5), dtype=np.float32)
        cube_data[:] = data
        cube = iris.cube.Cube(cube_data)
        cube.add_dim_coord(DimCoord(np.array([0, 1, 2, 3, 4], dtype=np.int32), long_name='x', units='1'), 1)
        cube.add_dim_coord(DimCoord(np.array([0, 1, 2, 3], dtype=np.int32), long_name='y', units='1'), 0)
        cube.add_aux_coord(DimCoord(np.array([a], dtype=np.int32), standard_name='forecast_period', units='1'))
        cube.add_aux_coord(DimCoord(np.array([b], dtype=np.int32), standard_name='forecast_reference_time', units='1'))
        cube.add_aux_coord(DimCoord(np.array([c], dtype=np.int32), standard_name='time', units='1'))
        return cube

    def _test_triples(self, triples, filename):
        cubes = [self._make_cube(fp, rt, t) for fp, rt, t in triples]
        cube = iris.cube.CubeList(cubes).merge()
        self.assertCML(cube, ('merge', 'time_triple_' + filename + '.cml'), checksum=False)

    def test_single_forecast(self):
        # A single forecast series (i.e. from a single reference time)
        # => fp, t: 4; rt: scalar
        triples = (
            (0, 10, 10), (1, 10, 11), (2, 10, 12), (3, 10, 13),
        )
        self._test_triples(triples, 'single_forecast')

    def test_successive_forecasts(self):
        # Three forecast series from successively later reference times
        # => rt, t: 3; fp, t: 4
        triples = (
            (0, 10, 10), (1, 10, 11), (2, 10, 12), (3, 10, 13),
            (0, 11, 11), (1, 11, 12), (2, 11, 13), (3, 11, 14),
            (0, 12, 12), (1, 12, 13), (2, 12, 14), (3, 12, 15),
        )
        self._test_triples(triples, 'successive_forecasts')

    def test_time_vs_ref_time(self):
        # => fp, t: 4; fp, rt: 3
        triples = (
            (2, 10, 12), (3, 10, 13), (4, 10, 14), (5, 10, 15),
            (1, 11, 12), (2, 11, 13), (3, 11, 14), (4, 11, 15),
            (0, 12, 12), (1, 12, 13), (2, 12, 14), (3, 12, 15),
        )
        self._test_triples(triples, 'time_vs_ref_time')

    def test_time_vs_forecast(self):
        # => rt, t: 4, fp, rt: 3
        triples = (
            (0, 10, 10), (0, 11, 11), (0, 12, 12), (0, 13, 13),
            (1,  9, 10), (1, 10, 11), (1, 11, 12), (1, 12, 13),
            (2,  8, 10), (2,  9, 11), (2, 10, 12), (2, 11, 13),
        )
        self._test_triples(triples, 'time_vs_forecast')

    def test_time_non_dim_coord(self):
        # => rt: 1 fp, t (bounded): 2
        triples = (
            (5, 0, 2.5), (10, 0, 5),
        )
        cubes = [self._make_cube(fp, rt, t) for fp, rt, t in triples]
        for end_time, cube in zip([5, 10], cubes):
            cube.coord('time').bounds = [0, end_time]
        cube, = iris.cube.CubeList(cubes).merge()
        self.assertCML(cube, ('merge', 'time_triple_time_non_dim_coord.cml'), checksum=False)
        # make sure that forecast_period is the dimensioned coordinate (as time becomes an AuxCoord)
        self.assertEqual(cube.coord(dimensions=0, dim_coords=True).name(), 'forecast_period')

    def test_independent(self):
        # => fp: 2; rt: 2; t: 2
        triples = (
            (0, 10, 10), (0, 11, 10),
            (0, 10, 11), (0, 11, 11),
            (1, 10, 10), (1, 11, 10),
            (1, 10, 11), (1, 11, 11),
        )
        self._test_triples(triples, 'independent')

    def test_series(self):
        # => fp, rt, t: 5 (with only t being definitive).
        triples = (
            (0, 10, 10),
            (0, 11, 11),
            (0, 12, 12),
            (1, 12, 13),
            (2, 12, 14),
        )
        self._test_triples(triples, 'series')

    def test_non_expanding_dimension(self):
        triples = (
            (0, 10, 0), (0, 20, 1), (0, 20, 0),
        )
        # => fp: scalar; rt, t: 3 (with no time being definitive)
        self._test_triples(triples, 'non_expanding')

    def test_duplicate_data(self):
        # test what happens when we have repeated time coordinates (i.e. duplicate data)
        cube1 = self._make_cube(0, 10, 0)
        cube2 = self._make_cube(1, 20, 1)
        cube3 = self._make_cube(1, 20, 1)

        # check that we get a duplicate data error when unique is True
        with self.assertRaises(iris.exceptions.DuplicateDataError):
            iris.cube.CubeList([cube1, cube2, cube3]).merge()

        cubes = iris.cube.CubeList([cube1, cube2, cube3]).merge(unique=False)
        self.assertCML(cubes, ('merge', 'time_triple_duplicate_data.cml'), checksum=False)

    def test_simple1(self):
        cube1 = self._make_cube(0, 10, 0)
        cube2 = self._make_cube(1, 20, 1)
        cube3 = self._make_cube(2, 20, 0)
        cube = iris.cube.CubeList([cube1, cube2, cube3]).merge()
        self.assertCML(cube, ('merge', 'time_triple_merging1.cml'), checksum=False)

    def test_simple2(self):
        cubes = iris.cube.CubeList([
                                    self._make_cube(0, 0, 0),
                                    self._make_cube(1, 0, 1),
                                    self._make_cube(2, 0, 2),
                                    self._make_cube(0, 1, 3),
                                    self._make_cube(1, 1, 4),
                                    self._make_cube(2, 1, 5),
                                   ])
        cube = cubes.merge()[0]
        self.assertCML(cube, ('merge', 'time_triple_merging2.cml'), checksum=False)
        self.assertIsNone(cube.assert_valid())

        cube = iris.cube.CubeList(cubes[:-1]).merge()[0]
        self.assertCML(cube, ('merge', 'time_triple_merging3.cml'), checksum=False)
        self.assertIsNone(cube.assert_valid())

    def test_simple3(self):
        cubes = iris.cube.CubeList([
                                    self._make_cube(0, 0, 0),
                                    self._make_cube(0, 1, 1),
                                    self._make_cube(0, 2, 2),
                                    self._make_cube(1, 0, 3),
                                    self._make_cube(1, 1, 4),
                                    self._make_cube(1, 2, 5),
                                   ])
        cube = cubes.merge()[0]
        self.assertCML(cube, ('merge', 'time_triple_merging4.cml'), checksum=False)
        self.assertIsNone(cube.assert_valid())

        cube = iris.cube.CubeList(cubes[:-1]).merge()[0]
        self.assertCML(cube, ('merge', 'time_triple_merging5.cml'), checksum=False)
        self.assertIsNone(cube.assert_valid())


class TestCubeMergeTheoretical(tests.IrisTest):
    def test_simple_bounds_merge(self):
        cube1 = iris.tests.stock.simple_2d()
        cube2 = iris.tests.stock.simple_2d()

        cube1.add_aux_coord(DimCoord(np.int32(10), long_name='pressure', units='Pa'))
        cube2.add_aux_coord(DimCoord(np.int32(11), long_name='pressure', units='Pa'))

        r = iris.cube.CubeList([cube1, cube2]).merge()
        self.assertCML(r, ('cube_merge', 'test_simple_bound_merge.cml'))

    def test_simple_multidim_merge(self):
        cube1 = iris.tests.stock.simple_2d_w_multidim_coords()
        cube2 = iris.tests.stock.simple_2d_w_multidim_coords()

        cube1.add_aux_coord(DimCoord(np.int32(10), long_name='pressure', units='Pa'))
        cube2.add_aux_coord(DimCoord(np.int32(11), long_name='pressure', units='Pa'))

        r = iris.cube.CubeList([cube1, cube2]).merge()[0]
        self.assertCML(r, ('cube_merge', 'multidim_coord_merge.cml'))

        # try transposing the cubes first
        cube1.transpose([1, 0])
        cube2.transpose([1, 0])
        r = iris.cube.CubeList([cube1, cube2]).merge()[0]
        self.assertCML(r, ('cube_merge', 'multidim_coord_merge_transpose.cml'))

    def test_simple_points_merge(self):
        cube1 = iris.tests.stock.simple_2d(with_bounds=False)
        cube2 = iris.tests.stock.simple_2d(with_bounds=False)

        cube1.add_aux_coord(DimCoord(np.int32(10), long_name='pressure', units='Pa'))
        cube2.add_aux_coord(DimCoord(np.int32(11), long_name='pressure', units='Pa'))

        r = iris.cube.CubeList([cube1, cube2]).merge()
        self.assertCML(r, ('cube_merge', 'test_simple_merge.cml'))

        # check that the unique merging raises a Duplicate data error
        self.assertRaises(iris.exceptions.DuplicateDataError, iris.cube.CubeList([cube1, cube1]).merge, unique=True)

        # check that non unique merging returns both cubes
        r = iris.cube.CubeList([cube1, cube1]).merge(unique=False)
        self.assertCML(r[0], ('cube_merge', 'test_orig_point_cube.cml'))
        self.assertCML(r[1], ('cube_merge', 'test_orig_point_cube.cml'))

        # test attribute merging
        cube1.attributes['my_attr1'] = 'foo'
        r = iris.cube.CubeList([cube1, cube2]).merge()
        # result should be 2 cubes
        self.assertCML(r, ('cube_merge', 'test_simple_attributes1.cml'))

        cube2.attributes['my_attr1'] = 'bar'
        r = iris.cube.CubeList([cube1, cube2]).merge()
        # result should be 2 cubes
        self.assertCML(r, ('cube_merge', 'test_simple_attributes2.cml'))

        cube2.attributes['my_attr1'] = 'foo'
        r = iris.cube.CubeList([cube1, cube2]).merge()
        # result should be 1 cube
        self.assertCML(r, ('cube_merge', 'test_simple_attributes3.cml'))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_name
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Tests for NAME loading."""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests
import iris


@tests.skip_data
class TestLoad(tests.IrisTest):
    def test_NAMEIII_field(self):
        cubes = iris.load(tests.get_data_path(('NAME', 'NAMEIII_field.txt')))
        self.assertCMLApproxData(cubes, ('name', 'NAMEIII_field.cml'))

    def test_NAMEII_field(self):
        cubes = iris.load(tests.get_data_path(('NAME', 'NAMEII_field.txt')))
        self.assertCMLApproxData(cubes, ('name', 'NAMEII_field.cml'))

    def test_NAMEIII_timeseries(self):
        cubes = iris.load(tests.get_data_path(('NAME',
                                               'NAMEIII_timeseries.txt')))
        self.assertCMLApproxData(cubes, ('name', 'NAMEIII_timeseries.cml'))

    def test_NAMEII_timeseries(self):
        cubes = iris.load(tests.get_data_path(('NAME',
                                               'NAMEII_timeseries.txt')))
        self.assertCMLApproxData(cubes, ('name', 'NAMEII_timeseries.cml'))

    def test_NAMEII_trajectory(self):
        cubes = iris.load(tests.get_data_path(('NAME',
                                              'NAMEIII_trajectory.txt')))
        self.assertCML(cubes[0], ('name', 'NAMEIII_trajectory0.cml'))
        self.assertCML(cubes, ('name', 'NAMEIII_trajectory.cml'),
                       checksum=False)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_netcdf
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test CF-NetCDF file loading and saving.

"""

# Import iris tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import os
import shutil
import stat
import tempfile
import warnings

import mock
import netCDF4 as nc
import numpy as np
import numpy.ma as ma

import iris
import iris.analysis.trajectory
import iris.fileformats._pyke_rules.compiled_krb.fc_rules_cf_fc as pyke_rules
import iris.fileformats.netcdf
import iris.std_names
import iris.util
import iris.coord_systems as icoord_systems
import iris.tests.stock as stock


@tests.skip_data
class TestNetCDFLoad(tests.IrisTest):
    def test_monotonic(self):
        cubes = iris.load(tests.get_data_path(
            ('NetCDF', 'testing', 'test_monotonic_coordinate.nc')))
        self.assertCML(cubes, ('netcdf', 'netcdf_monotonic.cml'))

    def test_load_global_xyt_total(self):
        # Test loading single xyt CF-netCDF file.
        cube = iris.load_cube(
            tests.get_data_path(('NetCDF', 'global', 'xyt',
                                 'SMALL_total_column_co2.nc')))
        self.assertCML(cube, ('netcdf', 'netcdf_global_xyt_total.cml'))

    def test_load_global_xyt_hires(self):
        # Test loading another single xyt CF-netCDF file.
        cube = iris.load_cube(tests.get_data_path(
            ('NetCDF', 'global', 'xyt', 'SMALL_hires_wind_u_for_ipcc4.nc')))
        self.assertCML(cube, ('netcdf', 'netcdf_global_xyt_hires.cml'))

    def test_missing_time_bounds(self):
        # Check we can cope with a missing bounds variable.
        with self.temp_filename(suffix='nc') as filename:
            # Tweak a copy of the test data file to rename (we can't delete)
            # the time bounds variable.
            src = tests.get_data_path(('NetCDF', 'global', 'xyt',
                                       'SMALL_hires_wind_u_for_ipcc4.nc'))
            shutil.copyfile(src, filename)
            dataset = nc.Dataset(filename, mode='a')
            dataset.renameVariable('time_bnds', 'foo')
            dataset.close()
            cube = iris.load_cube(filename, 'eastward_wind')

    def test_load_global_xyzt_gems(self):
        # Test loading single xyzt CF-netCDF file (multi-cube).
        cubes = iris.load(tests.get_data_path(('NetCDF', 'global', 'xyz_t',
                                               'GEMS_CO2_Apr2006.nc')))
        self.assertCML(cubes, ('netcdf', 'netcdf_global_xyzt_gems.cml'))

        # Check the masked array fill value is propogated through the data
        # manager loading.
        lnsp = cubes[1]
        self.assertTrue(ma.isMaskedArray(lnsp.data))
        self.assertEqual(-32767.0, lnsp.data.fill_value)

    def test_load_global_xyzt_gems_iter(self):
        # Test loading stepped single xyzt CF-netCDF file (multi-cube).
        for i, cube in enumerate(iris.load(
            tests.get_data_path(('NetCDF', 'global', 'xyz_t',
                                 'GEMS_CO2_Apr2006.nc')))):
            self.assertCML(cube, ('netcdf',
                                  'netcdf_global_xyzt_gems_iter_%d.cml' % i))

    def test_load_rotated_xy_land(self):
        # Test loading single xy rotated pole CF-netCDF file.
        cube = iris.load_cube(tests.get_data_path(
            ('NetCDF', 'rotated', 'xy', 'rotPole_landAreaFraction.nc')))
        self.assertCML(cube, ('netcdf', 'netcdf_rotated_xy_land.cml'))

        # Make sure the AuxCoords have lazy data.
        self.assertIsInstance(cube.coord('latitude')._points,
                              iris.aux_factory.LazyArray)

    def test_load_rotated_xyt_precipitation(self):
        # Test loading single xyt rotated pole CF-netCDF file.
        cube = iris.load_cube(
            tests.get_data_path(('NetCDF', 'rotated', 'xyt',
                                 'small_rotPole_precipitation.nc')))
        self.assertCML(cube, ('netcdf',
                              'netcdf_rotated_xyt_precipitation.cml'))

    def test_load_tmerc_grid_and_clim_bounds(self):
        # Test loading a single CF-netCDF file with a transverse Mercator
        # grid_mapping and a time variable with climatology.
        cube = iris.load_cube(
            tests.get_data_path(('NetCDF', 'transverse_mercator',
                                 'tmean_1910_1910.nc')))
        self.assertCML(cube, ('netcdf', 'netcdf_tmerc_and_climatology.cml'))

    def test_load_tmerc_grid_with_projection_origin(self):
        # Test loading a single CF-netCDF file with a transverse Mercator
        # grid_mapping that uses longitude_of_projection_origin and
        # scale_factor_at_projection_origin instead of
        # longitude_of_central_meridian and scale_factor_at_central_meridian.
        cube = iris.load_cube(
            tests.get_data_path(('NetCDF', 'transverse_mercator',
                                 'projection_origin_attributes.nc')))

        expected = icoord_systems.TransverseMercator(
            latitude_of_projection_origin=49.0,
            longitude_of_central_meridian=-2.0,
            false_easting=400000.0,
            false_northing=-100000.0,
            scale_factor_at_central_meridian=0.9996012717,
            ellipsoid=icoord_systems.GeogCS(
                semi_major_axis=6377563.396, semi_minor_axis=6356256.91))
        self.assertEqual(cube.coord('projection_x_coordinate').coord_system,
                         expected)
        self.assertEqual(cube.coord('projection_y_coordinate').coord_system,
                         expected)

    def test_missing_climatology(self):
        # Check we can cope with a missing climatology variable.
        with self.temp_filename(suffix='nc') as filename:
            # Tweak a copy of the test data file to rename (we can't delete)
            # the climatology variable.
            src = tests.get_data_path(('NetCDF', 'transverse_mercator',
                                       'tmean_1910_1910.nc'))
            shutil.copyfile(src, filename)
            dataset = nc.Dataset(filename, mode='a')
            dataset.renameVariable('climatology_bounds', 'foo')
            dataset.close()
            cube = iris.load_cube(filename, 'Mean temperature')

    def test_cell_methods(self):
        # Test exercising CF-netCDF cell method parsing.
        cubes = iris.load(tests.get_data_path(('NetCDF', 'testing',
                                               'cell_methods.nc')))

        # TEST_COMPAT mod - new cube merge doesn't sort in the same way - test
        # can pass by manual sorting...
        cubes = iris.cube.CubeList(sorted(cubes, key=lambda cube: cube.name()))

        self.assertCML(cubes, ('netcdf', 'netcdf_cell_methods.cml'))

    def test_deferred_loading(self):
        # Test exercising CF-netCDF deferred loading and deferred slicing.
        # shape (31, 161, 320)
        cube = iris.load_cube(tests.get_data_path(
            ('NetCDF', 'global', 'xyt', 'SMALL_total_column_co2.nc')))

        # Consecutive index on same dimension.
        self.assertCML(cube[0], ('netcdf', 'netcdf_deferred_index_0.cml'))
        self.assertCML(cube[0][0], ('netcdf', 'netcdf_deferred_index_1.cml'))
        self.assertCML(cube[0][0][0], ('netcdf',
                                       'netcdf_deferred_index_2.cml'))

        # Consecutive slice on same dimension.
        self.assertCML(cube[0:20], ('netcdf', 'netcdf_deferred_slice_0.cml'))
        self.assertCML(cube[0:20][0:10], ('netcdf',
                                          'netcdf_deferred_slice_1.cml'))
        self.assertCML(cube[0:20][0:10][0:5], ('netcdf',
                                               'netcdf_deferred_slice_2.cml'))

        # Consecutive tuple index on same dimension.
        self.assertCML(cube[(0, 8, 4, 2, 14, 12), ],
                       ('netcdf', 'netcdf_deferred_tuple_0.cml'))
        self.assertCML(cube[(0, 8, 4, 2, 14, 12), ][(0, 2, 4, 1), ],
                       ('netcdf', 'netcdf_deferred_tuple_1.cml'))
        subcube = cube[(0, 8, 4, 2, 14, 12), ][(0, 2, 4, 1), ][(1, 3), ]
        self.assertCML(subcube, ('netcdf', 'netcdf_deferred_tuple_2.cml'))

        # Consecutive mixture on same dimension.
        self.assertCML(cube[0:20:2][(9, 5, 8, 0), ][3],
                       ('netcdf', 'netcdf_deferred_mix_0.cml'))
        self.assertCML(cube[(2, 7, 3, 4, 5, 0, 9, 10), ][2:6][3],
                       ('netcdf', 'netcdf_deferred_mix_0.cml'))
        self.assertCML(cube[0][(0, 2), (1, 3)],
                       ('netcdf', 'netcdf_deferred_mix_1.cml'))

    def test_units(self):
        # Test exercising graceful cube and coordinate units loading.
        cube0, cube1 = iris.load(tests.get_data_path(('NetCDF', 'testing',
                                                      'units.nc')))

        self.assertCML(cube0, ('netcdf', 'netcdf_units_0.cml'))
        self.assertCML(cube1, ('netcdf', 'netcdf_units_1.cml'))


class TestNetCDFCRS(tests.IrisTest):
    def setUp(self):
        class Var(object):
            pass

        self.grid = Var()

    def test_lat_lon_major_minor(self):
        major = 63781370
        minor = 63567523
        self.grid.semi_major_axis = major
        self.grid.semi_minor_axis = minor
        crs = pyke_rules.build_coordinate_system(self.grid)
        self.assertEqual(crs, icoord_systems.GeogCS(major, minor))

    def test_lat_lon_earth_radius(self):
        earth_radius = 63700000
        self.grid.earth_radius = earth_radius
        crs = pyke_rules.build_coordinate_system(self.grid)
        self.assertEqual(crs, icoord_systems.GeogCS(earth_radius))


class SaverPermissions(tests.IrisTest):
    def test_noexist_directory(self):
        # Test capture of suitable exception raised on writing to a
        # non-existent directory.
        dir_name = os.path.join(tempfile.gettempdir(), 'non_existent_dir')
        fnme = os.path.join(dir_name, 'tmp.nc')
        with self.assertRaises(IOError):
            with iris.fileformats.netcdf.Saver(fnme, 'NETCDF4'):
                pass

    def test_bad_permissions(self):
        # Non-exhaustive check that wrong permissions results in a suitable
        # exception being raised.
        dir_name = tempfile.mkdtemp()
        fnme = os.path.join(dir_name, 'tmp.nc')
        try:
            os.chmod(dir_name, stat.S_IREAD)
            with self.assertRaises(IOError):
                iris.fileformats.netcdf.Saver(fnme, 'NETCDF4')
            self.assertFalse(os.path.exists(fnme))
        finally:
            os.rmdir(dir_name)


class TestSave(tests.IrisTest):
    def test_hybrid(self):
        cube = stock.realistic_4d()

        # Write Cube to netCDF file.
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cube, file_out, netcdf_format='NETCDF3_CLASSIC')

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_realistic_4d.cdl'))
        os.remove(file_out)

    def test_no_hybrid(self):
        cube = stock.realistic_4d()
        cube.remove_aux_factory(cube.aux_factories[0])

        # Write Cube to netCDF file.
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cube, file_out, netcdf_format='NETCDF3_CLASSIC')

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf',
                                  'netcdf_save_realistic_4d_no_hybrid.cdl'))
        os.remove(file_out)

    def test_scalar_cube(self):
        cube = stock.realistic_4d()[0, 0, 0, 0]

        with self.temp_filename(suffix='.nc') as filename:
            iris.save(cube, filename, netcdf_format='NETCDF3_CLASSIC')
            self.assertCDL(filename, ('netcdf',
                                      'netcdf_save_realistic_0d.cdl'))

    def test_no_name_cube(self):
        # Cube with no names.
        cube = iris.cube.Cube(np.arange(20, dtype=np.float64).reshape((4, 5)))
        dim0 = iris.coords.DimCoord(np.arange(4, dtype=np.float64))
        dim1 = iris.coords.DimCoord(np.arange(5, dtype=np.float64), units='m')
        other = iris.coords.AuxCoord('foobar', units='no_unit')
        cube.add_dim_coord(dim0, 0)
        cube.add_dim_coord(dim1, 1)
        cube.add_aux_coord(other)
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(cube, filename, netcdf_format='NETCDF3_CLASSIC')
            self.assertCDL(filename, ('netcdf', 'netcdf_save_no_name.cdl'))


class TestNetCDFSave(tests.IrisTest):
    def setUp(self):
        self.cubell = iris.cube.Cube(np.arange(4).reshape(2, 2),
                                     'air_temperature')
        self.cube = iris.cube.Cube(np.zeros([2, 2]),
                                   standard_name='surface_temperature',
                                   long_name=None,
                                   var_name='temp',
                                   units='K')
        self.cube2 = iris.cube.Cube(np.ones([1, 2, 2]),
                                    standard_name=None,
                                    long_name='Something Random',
                                    var_name='temp2',
                                    units='K')
        self.cube3 = iris.cube.Cube(np.ones([2, 2, 2]),
                                    standard_name=None,
                                    long_name='Something Random',
                                    var_name='temp3',
                                    units='K')
        self.cube4 = iris.cube.Cube(np.zeros([10]),
                                    standard_name='air_temperature',
                                    long_name=None,
                                    var_name='temp',
                                    units='K')
        self.cube5 = iris.cube.Cube(np.ones([20]),
                                    standard_name=None,
                                    long_name='air_temperature',
                                    var_name='temp2',
                                    units='K')
        self.cube6 = iris.cube.Cube(np.ones([10]),
                                    standard_name=None,
                                    long_name='air_temperature',
                                    var_name='temp3',
                                    units='K')

    @tests.skip_data
    def test_netcdf_save_format(self):
        # Read netCDF input file.
        file_in = tests.get_data_path(
            ('NetCDF', 'global', 'xyt', 'SMALL_total_column_co2.nc'))
        cube = iris.load_cube(file_in)

        file_out = iris.util.create_temp_filename(suffix='.nc')

        # Test default NETCDF4 file format saving.
        iris.save(cube, file_out)
        ds = nc.Dataset(file_out)
        self.assertEqual(ds.file_format, 'NETCDF4',
                         'Failed to save as NETCDF4 format')
        ds.close()

        # Test NETCDF4_CLASSIC file format saving.
        iris.save(cube, file_out, netcdf_format='NETCDF4_CLASSIC')
        ds = nc.Dataset(file_out)
        self.assertEqual(ds.file_format, 'NETCDF4_CLASSIC',
                         'Failed to save as NETCDF4_CLASSIC format')
        ds.close()

        # Test NETCDF3_CLASSIC file format saving.
        iris.save(cube, file_out, netcdf_format='NETCDF3_CLASSIC')
        ds = nc.Dataset(file_out)
        self.assertEqual(ds.file_format, 'NETCDF3_CLASSIC',
                         'Failed to save as NETCDF3_CLASSIC format')
        ds.close()

        # Test NETCDF4_64BIT file format saving.
        iris.save(cube, file_out, netcdf_format='NETCDF3_64BIT')
        ds = nc.Dataset(file_out)
        self.assertEqual(ds.file_format, 'NETCDF3_64BIT',
                         'Failed to save as NETCDF3_64BIT format')
        ds.close()

        # Test invalid file format saving.
        with self.assertRaises(ValueError):
            iris.save(cube, file_out, netcdf_format='WIBBLE')

        os.remove(file_out)

    @tests.skip_data
    def test_netcdf_save_single(self):
        # Test saving a single CF-netCDF file.
        # Read PP input file.
        file_in = tests.get_data_path(
            ('PP', 'cf_processing',
             '000003000000.03.236.000128.1990.12.01.00.00.b.pp'))
        cube = iris.load_cube(file_in)

        # Write Cube to netCDF file.
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cube, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_single.cdl'))
        os.remove(file_out)

    # TODO investigate why merge now make time an AuxCoord rather than a
    # DimCoord and why forecast_period is 'preferred'.
    @tests.skip_data
    def test_netcdf_save_multi2multi(self):
        # Test saving multiple CF-netCDF files.
        # Read PP input file.
        file_in = tests.get_data_path(('PP', 'cf_processing',
                                       'abcza_pa19591997_daily_29.b.pp'))
        cubes = iris.load(file_in)

        # Save multiple cubes to multiple files.
        for index, cube in enumerate(cubes):
            # Write Cube to netCDF file.
            file_out = iris.util.create_temp_filename(suffix='.nc')

            iris.save(cube, file_out)

            # Check the netCDF file against CDL expected output.
            self.assertCDL(file_out, ('netcdf',
                                      'netcdf_save_multi_%d.cdl' % index))
            os.remove(file_out)

    @tests.skip_data
    def test_netcdf_save_multi2single(self):
        # Test saving multiple cubes to a single CF-netCDF file.
        # Read PP input file.
        file_in = tests.get_data_path(('PP', 'cf_processing',
                                       'abcza_pa19591997_daily_29.b.pp'))
        cubes = iris.load(file_in)

        # Write Cube to netCDF file.
        file_out = iris.util.create_temp_filename(suffix='.nc')

        # Check that it is the same on loading
        iris.save(cubes, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_multiple.cdl'))

    def test_netcdf_multi_nocoord(self):
        # Testing the saving of a cublist with no coords.
        cubes = iris.cube.CubeList([self.cube, self.cube2, self.cube3])
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cubes, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_nocoord.cdl'))
        os.remove(file_out)

    def test_netcdf_multi_samevarnme(self):
        # Testing the saving of a cublist with cubes of the same var_name.
        self.cube2.var_name = self.cube.var_name
        cubes = iris.cube.CubeList([self.cube, self.cube2])
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cubes, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_samevar.cdl'))
        os.remove(file_out)

    def test_netcdf_multi_with_coords(self):
        # Testing the saving of a cublist with coordinates.
        lat = iris.coords.DimCoord(np.arange(2),
                                   long_name=None, var_name='lat',
                                   units='degree_north')
        lon = iris.coords.DimCoord(np.arange(2), standard_name='longitude',
                                   long_name=None, var_name='lon',
                                   units='degree_east')
        rcoord = iris.coords.DimCoord(np.arange(1), standard_name=None,
                                      long_name='Rnd Coordinate',
                                      units=None)
        self.cube.add_dim_coord(lon, 0)
        self.cube.add_dim_coord(lat, 1)
        self.cube2.add_dim_coord(lon, 1)
        self.cube2.add_dim_coord(lat, 2)
        self.cube2.add_dim_coord(rcoord, 0)

        cubes = iris.cube.CubeList([self.cube, self.cube2])
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cubes, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_wcoord.cdl'))
        os.remove(file_out)

    def test_netcdf_multi_wtih_samedimcoord(self):
        time1 = iris.coords.DimCoord(np.arange(10),
                                     standard_name='time',
                                     var_name='time')
        time2 = iris.coords.DimCoord(np.arange(20),
                                     standard_name='time',
                                     var_name='time')

        self.cube4.add_dim_coord(time1, 0)
        self.cube5.add_dim_coord(time2, 0)
        self.cube6.add_dim_coord(time1, 0)

        cubes = iris.cube.CubeList([self.cube4, self.cube5, self.cube6])
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cubes, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_samedimcoord.cdl'))
        os.remove(file_out)

    def test_netcdf_multi_conflict_name_dup_coord(self):
        # Duplicate coordinates with modified variable names lookup.
        latitude1 = iris.coords.DimCoord(np.arange(10),
                                         standard_name='latitude')
        time2 = iris.coords.DimCoord(np.arange(2),
                                     standard_name='time')
        latitude2 = iris.coords.DimCoord(np.arange(2),
                                         standard_name='latitude')

        self.cube6.add_dim_coord(latitude1, 0)
        self.cube.add_dim_coord(latitude2[:], 1)
        self.cube.add_dim_coord(time2[:], 0)

        cubes = iris.cube.CubeList([self.cube, self.cube6, self.cube6.copy()])
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cubes, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf',
                                  'multi_dim_coord_slightly_different.cdl'))
        os.remove(file_out)

    @tests.skip_data
    def test_netcdf_hybrid_height(self):
        # Test saving a CF-netCDF file which contains a hybrid height
        # (i.e. dimensionless vertical) coordinate.
        # Read PP input file.
        file_in = tests.get_data_path(
            ('PP', 'COLPEX', 'small_colpex_theta_p_alt.pp'))
        cube = iris.load_cube(file_in, 'air_potential_temperature')

        # Write Cube to netCDF file.
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cube, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_hybrid_height.cdl'))

        # Read netCDF file.
        cube = iris.load_cube(file_out)

        # Check the PP read, netCDF write, netCDF read mechanism.
        self.assertCML(cube, ('netcdf', 'netcdf_save_load_hybrid_height.cml'))

        os.remove(file_out)

    @tests.skip_data
    def test_netcdf_save_ndim_auxiliary(self):
        # Test saving CF-netCDF with multi-dimensional auxiliary coordinates.
        # Read netCDF input file.
        file_in = tests.get_data_path(
            ('NetCDF', 'rotated', 'xyt', 'small_rotPole_precipitation.nc'))
        cube = iris.load_cube(file_in)

        # Write Cube to nerCDF file.
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cube, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_ndim_auxiliary.cdl'))

        # Read the netCDF file.
        cube = iris.load_cube(file_out)

        # Check the netCDF read, write, read mechanism.
        self.assertCML(cube, ('netcdf', 'netcdf_save_load_ndim_auxiliary.cml'))

        os.remove(file_out)

    def test_netcdf_save_conflicting_aux(self):
        # Test saving CF-netCDF with multi-dimensional auxiliary coordinates,
        # with conflicts.
        self.cube4.add_aux_coord(iris.coords.AuxCoord(np.arange(10),
                                                      'time'), 0)
        self.cube6.add_aux_coord(iris.coords.AuxCoord(np.arange(10, 20),
                                                      'time'), 0)

        cubes = iris.cube.CubeList([self.cube4, self.cube6])
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cubes, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_conf_aux.cdl'))
        os.remove(file_out)

    def test_netcdf_save_gridmapping(self):
        # Test saving CF-netCDF from a cubelist with various grid mappings.

        c1 = self.cubell
        c2 = self.cubell.copy()
        c3 = self.cubell.copy()

        coord_system = icoord_systems.GeogCS(6371229)
        coord_system2 = icoord_systems.GeogCS(6371228)
        coord_system3 = icoord_systems.RotatedGeogCS(30, 30)

        c1.add_dim_coord(iris.coords.DimCoord(
            np.arange(1, 3), 'latitude', long_name='1',
            coord_system=coord_system), 1)
        c1.add_dim_coord(iris.coords.DimCoord(
            np.arange(1, 3), 'longitude', long_name='1',
            coord_system=coord_system), 0)

        c2.add_dim_coord(iris.coords.DimCoord(
            np.arange(1, 3), 'latitude', long_name='2',
            coord_system=coord_system2), 1)
        c2.add_dim_coord(iris.coords.DimCoord(
            np.arange(1, 3), 'longitude', long_name='2',
            coord_system=coord_system2), 0)

        c3.add_dim_coord(iris.coords.DimCoord(
            np.arange(1, 3), 'grid_latitude', long_name='3',
            coord_system=coord_system3), 1)
        c3.add_dim_coord(iris.coords.DimCoord(
            np.arange(1, 3), 'grid_longitude', long_name='3',
            coord_system=coord_system3), 0)

        cubes = iris.cube.CubeList([c1, c2, c3])
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cubes, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_gridmapmulti.cdl'))
        os.remove(file_out)

    def test_netcdf_save_conflicting_names(self):
        # Test saving CF-netCDF with a dimension name corresponding to
        # an existing variable name (conflict).
        self.cube4.add_dim_coord(iris.coords.DimCoord(np.arange(10),
                                                      'time'), 0)
        self.cube6.add_aux_coord(iris.coords.AuxCoord(1, 'time'), None)

        cubes = iris.cube.CubeList([self.cube4, self.cube6])
        file_out = iris.util.create_temp_filename(suffix='.nc')
        iris.save(cubes, file_out)

        # Check the netCDF file against CDL expected output.
        self.assertCDL(file_out, ('netcdf', 'netcdf_save_conf_name.cdl'))
        os.remove(file_out)

    @tests.skip_data
    def test_trajectory(self):
        file_in = tests.get_data_path(('PP', 'aPPglob1', 'global.pp'))
        cube = iris.load_cube(file_in)

        # extract a trajectory
        xpoint = cube.coord('longitude').points[:10]
        ypoint = cube.coord('latitude').points[:10]
        sample_points = [('latitude', xpoint), ('longitude', ypoint)]
        traj = iris.analysis.trajectory.interpolate(cube, sample_points)

        # save, reload and check
        with self.temp_filename(suffix='.nc') as temp_filename:
            iris.save(traj, temp_filename)
            reloaded = iris.load_cube(temp_filename)
            self.assertCML(reloaded,
                           ('netcdf', 'save_load_traj.cml'),
                           checksum=False)
            self.assertArrayEqual(traj.data, reloaded.data)

    def test_attributes(self):
        # Should be global attributes.
        self.cube.attributes['history'] = 'A long time ago...'
        self.cube.attributes['title'] = 'Attribute test'
        self.cube.attributes['foo'] = 'bar'
        # Should be data varible attributes.
        self.cube.attributes['standard_error_multiplier'] = 23
        self.cube.attributes['flag_masks'] = 'a'
        self.cube.attributes['flag_meanings'] = 'b'
        self.cube.attributes['flag_values'] = 'c'
        self.cube.attributes['STASH'] = iris.fileformats.pp.STASH(1, 2, 3)
        # Should be overriden.
        self.cube.attributes['conventions'] = 'TEST'
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(self.cube, filename)
            self.assertCDL(filename, ('netcdf', 'netcdf_save_attr.cdl'))

    def test_conflicting_attributes(self):
        # Should be data variable attributes.
        self.cube.attributes['foo'] = 'bar'
        self.cube2.attributes['foo'] = 'orange'
        with self.temp_filename(suffix='.nc') as filename:
            iris.save([self.cube, self.cube2], filename)
            self.assertCDL(filename, ('netcdf', 'netcdf_save_confl_attr.cdl'))

    def test_conflicting_global_attributes(self):
        # Should be data variable attributes, but raise a warning.
        attr_name = 'history'
        self.cube.attributes[attr_name] = 'Team A won.'
        self.cube2.attributes[attr_name] = 'Team B won.'
        expected_msg = '{attr_name!r} is being added as CF data variable ' \
                       'attribute, but {attr_name!r} should only be a CF ' \
                       'global attribute.'.format(attr_name=attr_name)
        with self.temp_filename(suffix='.nc') as filename:
            with mock.patch('warnings.warn') as warn:
                iris.save([self.cube, self.cube2], filename)
                warn.assert_called_with(expected_msg)
                self.assertCDL(filename,
                               ('netcdf', 'netcdf_save_confl_global_attr.cdl'))

    def test_no_global_attributes(self):
        # Should all be data variable attributes.
        # Different keys.
        self.cube.attributes['a'] = 'a'
        self.cube2.attributes['b'] = 'a'
        self.cube3.attributes['c'] = 'a'
        self.cube4.attributes['d'] = 'a'
        self.cube5.attributes['e'] = 'a'
        self.cube6.attributes['f'] = 'a'
        # Different values.
        self.cube.attributes['g'] = 'p'
        self.cube2.attributes['g'] = 'q'
        self.cube3.attributes['g'] = 'r'
        self.cube4.attributes['g'] = 's'
        self.cube5.attributes['g'] = 't'
        self.cube6.attributes['g'] = 'u'
        # One different value.
        self.cube.attributes['h'] = 'v'
        self.cube2.attributes['h'] = 'v'
        self.cube3.attributes['h'] = 'v'
        self.cube4.attributes['h'] = 'w'
        self.cube5.attributes['h'] = 'v'
        self.cube6.attributes['h'] = 'v'
        cubes = [self.cube, self.cube2, self.cube3,
                 self.cube4, self.cube5, self.cube6]
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(cubes, filename)
            self.assertCDL(filename, ('netcdf',
                                      'netcdf_save_no_global_attr.cdl'))


class TestNetCDF3SaveInteger(tests.IrisTest):
    def setUp(self):
        self.cube = iris.cube.Cube(np.zeros((2, 2), dtype=np.float64),
                                   standard_name='surface_temperature',
                                   long_name=None,
                                   var_name='temp',
                                   units='K')

    def test_int64_dimension_coord_netcdf3(self):
        coord = iris.coords.DimCoord(np.array([1, 2], dtype=np.int64),
                                     long_name='x')
        self.cube.add_dim_coord(coord, 0)
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(self.cube, filename, netcdf_format='NETCDF3_CLASSIC')
            reloaded = iris.load_cube(filename)
            self.assertCML(reloaded, ('netcdf',
                                      'int64_dimension_coord_netcdf3.cml'),
                           checksum=False)

    def test_int64_auxiliary_coord_netcdf3(self):
        coord = iris.coords.AuxCoord(np.array([1, 2], dtype=np.int64),
                                     long_name='x')
        self.cube.add_aux_coord(coord, 0)
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(self.cube, filename, netcdf_format='NETCDF3_CLASSIC')
            reloaded = iris.load_cube(filename)
            self.assertCML(reloaded, ('netcdf',
                                      'int64_auxiliary_coord_netcdf3.cml'),
                           checksum=False)

    def test_int64_data_netcdf3(self):
        self.cube.data = self.cube.data.astype(np.int64)
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(self.cube, filename, netcdf_format='NETCDF3_CLASSIC')
            reloaded = iris.load_cube(filename)
            self.assertCML(reloaded, ('netcdf',
                                      'int64_data_netcdf3.cml'))

    def test_uint32_dimension_coord_netcdf3(self):
        coord = iris.coords.DimCoord(np.array([1, 2], dtype=np.uint32),
                                     long_name='x')
        self.cube.add_dim_coord(coord, 0)
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(self.cube, filename, netcdf_format='NETCDF3_CLASSIC')
            reloaded = iris.load_cube(filename)
            self.assertCML(reloaded, ('netcdf',
                                      'uint32_dimension_coord_netcdf3.cml'),
                           checksum=False)

    def test_uint32_auxiliary_coord_netcdf3(self):
        coord = iris.coords.AuxCoord(np.array([1, 2], dtype=np.uint32),
                                     long_name='x')
        self.cube.add_aux_coord(coord, 0)
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(self.cube, filename, netcdf_format='NETCDF3_CLASSIC')
            reloaded = iris.load_cube(filename)
            self.assertCML(reloaded, ('netcdf',
                                      'uint32_auxiliary_coord_netcdf3.cml'),
                           checksum=False)

    def test_uint32_data_netcdf3(self):
        self.cube.data = self.cube.data.astype(np.uint32)
        with self.temp_filename(suffix='.nc') as filename:
            iris.save(self.cube, filename, netcdf_format='NETCDF3_CLASSIC')
            reloaded = iris.load_cube(filename)
            self.assertCML(reloaded, ('netcdf',
                                      'uint32_data_netcdf3.cml'))

    def test_uint64_dimension_coord_netcdf3(self):
        # Points that cannot be safely cast to int32.
        coord = iris.coords.DimCoord(np.array([0, 18446744073709551615],
                                              dtype=np.uint64),
                                     long_name='x')
        self.cube.add_dim_coord(coord, 0)
        with self.temp_filename(suffix='.nc') as filename:
            with self.assertRaises(ValueError):
                iris.save(self.cube, filename, netcdf_format='NETCDF3_CLASSIC')

    def test_uint64_auxiliary_coord_netcdf3(self):
        # Points that cannot be safely cast to int32.
        coord = iris.coords.AuxCoord(np.array([0, 18446744073709551615],
                                              dtype=np.uint64),
                                     long_name='x')
        self.cube.add_aux_coord(coord, 0)
        with self.temp_filename(suffix='.nc') as filename:
            with self.assertRaises(ValueError):
                iris.save(self.cube, filename, netcdf_format='NETCDF3_CLASSIC')

    def test_uint64_data_netcdf3(self):
        # Data that cannot be safely cast to int32.
        self.cube.data = self.cube.data.astype(np.uint64)
        self.cube.data[0, 1] = 18446744073709551615
        with self.temp_filename(suffix='.nc') as filename:
            with self.assertRaises(ValueError):
                iris.save(self.cube, filename, netcdf_format='NETCDF3_CLASSIC')


class TestCFStandardName(tests.IrisTest):
    def setUp(self):
        pass

    def test_std_name_lookup_pass(self):
        # Test performing a CF standard name look-up hit.
        self.assertTrue('time' in iris.std_names.STD_NAMES)

    def test_std_name_lookup_fail(self):
        # Test performing a CF standard name look-up miss.
        self.assertFalse('phenomenon_time' in iris.std_names.STD_NAMES)


@tests.skip_data
class TestNetCDFUKmoProcessFlags(tests.IrisTest):
    def test_process_flags(self):
        # Test single process flags
        for _, process_desc in iris.fileformats.pp.LBPROC_PAIRS[1:]:
            # Get basic cube and set process flag manually
            ll_cube = stock.lat_lon_cube()
            ll_cube.attributes["ukmo__process_flags"] = (process_desc,)

            # Save cube to netCDF
            temp_filename = iris.util.create_temp_filename(".nc")
            iris.save(ll_cube, temp_filename)

            # Reload cube
            cube = iris.load_cube(temp_filename)

            # Check correct number and type of flags
            self.assertTrue(len(cube.attributes["ukmo__process_flags"]) == 1,
                            "Mismatch in number of process flags.")
            process_flag = cube.attributes["ukmo__process_flags"][0]
            self.assertEquals(process_flag, process_desc)

            os.remove(temp_filename)

        # Test mutiple process flags
        multiple_bit_values = ((128, 64), (4096, 1024), (8192, 1024))

        # Maps lbproc value to the process flags that should be created
        multiple_map = {bits: [iris.fileformats.pp.lbproc_map[bit] for
                               bit in bits] for bits in multiple_bit_values}

        for bits, descriptions in multiple_map.iteritems():

            ll_cube = stock.lat_lon_cube()
            ll_cube.attributes["ukmo__process_flags"] = descriptions

            # Save cube to netCDF
            temp_filename = iris.util.create_temp_filename(".nc")
            iris.save(ll_cube, temp_filename)

            # Reload cube
            cube = iris.load_cube(temp_filename)

            # Check correct number and type of flags
            process_flags = cube.attributes["ukmo__process_flags"]
            self.assertTrue(len(process_flags) == len(bits), 'Mismatch in '
                            'number of process flags.')
            self.assertEquals(set(process_flags), set(descriptions))

            os.remove(temp_filename)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_nimrod
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np

import iris
import iris.fileformats.nimrod_load_rules as nimrod_load_rules


def mock_nimrod_field():
    field = iris.fileformats.nimrod.NimrodField()
    field.int_mdi = -32767
    field.float32_mdi = -32767.0
    return field


class TestLoad(tests.IrisTest):
    @tests.skip_data
    def test_multi_field_load(self):
        # load a cube with two fields
        cube = iris.load(tests.get_data_path(
            ('NIMROD', 'uk2km', 'WO0000000003452',
             '201007020900_u1096_ng_ey00_visibility0180_screen_2km')))
        self.assertCML(cube, ("nimrod", "load_2flds.cml"))

    def test_orography(self):
        # Mock an orography field we've seen.
        field = mock_nimrod_field()
        cube = iris.cube.Cube(np.arange(100).reshape(10, 10))

        field.dt_year = field.dt_month = field.dt_day = field.int_mdi
        field.dt_hour = field.dt_minute = field.int_mdi
        field.proj_biaxial_ellipsoid = 0
        field.tm_meridian_scaling = 0.999601
        field.field_code = 73
        field.vertical_coord_type = 1
        field.title = "(MOCK) 2km mean orography"
        field.units = "metres"
        field.source = "GLOBE DTM"

        nimrod_load_rules.name(cube, field)
        nimrod_load_rules.units(cube, field)
        nimrod_load_rules.reference_time(cube, field)
        nimrod_load_rules.proj_biaxial_ellipsoid(cube, field)
        nimrod_load_rules.tm_meridian_scaling(cube, field)
        nimrod_load_rules.vertical_coord(cube, field)
        nimrod_load_rules.attributes(cube, field)

        self.assertCML(cube, ("nimrod", "mockography.cml"))

    def test_levels_below_ground(self):
        # Mock a soil temperature field we've seen.
        field = mock_nimrod_field()
        cube = iris.cube.Cube(np.arange(100).reshape(10, 10))

        field.field_code = -1  # Not orography
        field.reference_vertical_coord_type = field.int_mdi  # Not bounded
        field.vertical_coord_type = 12
        field.vertical_coord = 42
        nimrod_load_rules.vertical_coord(cube, field)

        self.assertCML(cube, ("nimrod", "levels_below_ground.cml"))

    def test_period_of_interest(self):
        # mock a pressure field
        field = mock_nimrod_field()
        cube = iris.cube.Cube(np.arange(100).reshape(10, 10))

        field.field_code = 0
        field.vt_year = 2013
        field.vt_month = 5
        field.vt_day = 7
        field.vt_hour = 6
        field.vt_minute = 0
        field.vt_second = 0
        field.dt_year = 2013
        field.dt_month = 5
        field.dt_day = 7
        field.dt_hour = 6
        field.dt_minute = 0
        field.dt_second = 0
        field.period_minutes = 60

        nimrod_load_rules.time(cube, field)

        self.assertCML(cube, ("nimrod", "period_of_interest.cml"))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pandas
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import datetime
import unittest

import netcdftime
import numpy as np

try:
    import pandas
except ImportError:
    # Disable all these tests if pandas is not installed.
    pandas = None

skip_pandas = unittest.skipIf(pandas is None,
                              'Test(s) require "pandas", '
                              'which is not available.')

if pandas is not None:
    from iris.coords import DimCoord
    from iris.cube import Cube
    import iris.pandas
    import iris.unit


@skip_pandas
class TestAsSeries(tests.IrisTest):
    """Test conversion of 1D cubes to Pandas using as_series()"""

    def test_no_dim_coord(self):
        cube = Cube(np.array([0, 1, 2, 3, 4]), long_name="foo")
        series = iris.pandas.as_series(cube)
        self.assertArrayEqual(series, cube.data)
        self.assertString(
            str(series),
            tests.get_result_path(('pandas', 'as_series',
                                   'no_dim_coord.txt')))

    def test_simple(self):
        cube = Cube(np.array([0, 1, 2, 3, 4.4]), long_name="foo")
        dim_coord = DimCoord([5, 6, 7, 8, 9], long_name="bar")
        cube.add_dim_coord(dim_coord, 0)
        series = iris.pandas.as_series(cube)
        self.assertArrayEqual(series, cube.data)
        self.assertString(
            str(series),
            tests.get_result_path(('pandas', 'as_series', 'simple.txt')))

    def test_masked(self):
        data = np.ma.MaskedArray([0, 1, 2, 3, 4.4], mask=[0, 1, 0, 1, 0])
        cube = Cube(data, long_name="foo")
        series = iris.pandas.as_series(cube)
        self.assertArrayEqual(series, cube.data.astype('f').filled(np.nan))
        self.assertString(
            str(series),
            tests.get_result_path(('pandas', 'as_series', 'masked.txt')))

    def test_time_gregorian(self):
        cube = Cube(np.array([0, 1, 2, 3, 4]), long_name="ts")
        time_coord = DimCoord([0, 100.1, 200.2, 300.3, 400.4],
                              long_name="time",
                              units="days since 2000-01-01 00:00")
        cube.add_dim_coord(time_coord, 0)
        series = iris.pandas.as_series(cube)
        self.assertArrayEqual(series, cube.data)
        self.assertString(
            str(series),
            tests.get_result_path(('pandas', 'as_series',
                                  'time_gregorian.txt')))

    def test_time_360(self):
        cube = Cube(np.array([0, 1, 2, 3, 4]), long_name="ts")
        time_unit = iris.unit.Unit("days since 2000-01-01 00:00",
                                   calendar=iris.unit.CALENDAR_360_DAY)
        time_coord = DimCoord([0, 100.1, 200.2, 300.3, 400.4],
                              long_name="time", units=time_unit)
        cube.add_dim_coord(time_coord, 0)
        series = iris.pandas.as_series(cube)
        self.assertArrayEqual(series, cube.data)
        self.assertString(
            str(series),
            tests.get_result_path(('pandas', 'as_series',
                                   'time_360.txt')))

    def test_copy_true(self):
        cube = Cube(np.array([0, 1, 2, 3, 4]), long_name="foo")
        series = iris.pandas.as_series(cube)
        series[0] = 99
        self.assertEqual(cube.data[0], 0)

    def test_copy_int32_false(self):
        cube = Cube(np.array([0, 1, 2, 3, 4], dtype=np.int32), long_name="foo")
        series = iris.pandas.as_series(cube, copy=False)
        series[0] = 99
        self.assertEqual(cube.data[0], 99)

    def test_copy_int64_false(self):
        cube = Cube(np.array([0, 1, 2, 3, 4], dtype=np.int32), long_name="foo")
        series = iris.pandas.as_series(cube, copy=False)
        series[0] = 99
        self.assertEqual(cube.data[0], 99)

    def test_copy_float_false(self):
        cube = Cube(np.array([0, 1, 2, 3.3, 4]), long_name="foo")
        series = iris.pandas.as_series(cube, copy=False)
        series[0] = 99
        self.assertEqual(cube.data[0], 99)

    def test_copy_masked_true(self):
        data = np.ma.MaskedArray([0, 1, 2, 3, 4], mask=[0, 1, 0, 1, 0])
        cube = Cube(data, long_name="foo")
        series = iris.pandas.as_series(cube)
        series[0] = 99
        self.assertEqual(cube.data[0], 0)

    def test_copy_masked_false(self):
        data = np.ma.MaskedArray([0, 1, 2, 3, 4], mask=[0, 1, 0, 1, 0])
        cube = Cube(data, long_name="foo")
        with self.assertRaises(ValueError):
            series = iris.pandas.as_series(cube, copy=False)


@skip_pandas
class TestAsDataFrame(tests.IrisTest):
    """Test conversion of 2D cubes to Pandas using as_data_frame()"""

    def test_no_dim_coords(self):
        cube = Cube(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),
                    long_name="foo")
        data_frame = iris.pandas.as_data_frame(cube)
        self.assertArrayEqual(data_frame, cube.data)
        self.assertString(
            str(data_frame),
            tests.get_result_path(('pandas', 'as_dataframe',
                                   'no_dim_coords.txt')))

    def test_no_x_coord(self):
        cube = Cube(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),
                    long_name="foo")
        y_coord = DimCoord([10, 11], long_name="bar")
        cube.add_dim_coord(y_coord, 0)
        data_frame = iris.pandas.as_data_frame(cube)
        self.assertArrayEqual(data_frame, cube.data)
        self.assertString(
            str(data_frame),
            tests.get_result_path(('pandas', 'as_dataframe',
                                   'no_x_coord.txt')))

    def test_no_y_coord(self):
        cube = Cube(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),
                    long_name="foo")
        x_coord = DimCoord([10, 11, 12, 13, 14], long_name="bar")
        cube.add_dim_coord(x_coord, 1)
        data_frame = iris.pandas.as_data_frame(cube)
        self.assertArrayEqual(data_frame, cube.data)
        self.assertString(
            str(data_frame),
            tests.get_result_path(('pandas', 'as_dataframe',
                                   'no_y_coord.txt')))

    def test_simple(self):
        cube = Cube(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),
                    long_name="foo")
        x_coord = DimCoord([10, 11, 12, 13, 14], long_name="bar")
        y_coord = DimCoord([15, 16], long_name="milk")
        cube.add_dim_coord(x_coord, 1)
        cube.add_dim_coord(y_coord, 0)
        data_frame = iris.pandas.as_data_frame(cube)
        self.assertArrayEqual(data_frame, cube.data)
        self.assertString(
            str(data_frame),
            tests.get_result_path(('pandas', 'as_dataframe',
                                   'simple.txt')))

    def test_masked(self):
        data = np.ma.MaskedArray([[0, 1, 2, 3, 4.4], [5, 6, 7, 8, 9]],
                                 mask=[[0, 1, 0, 1, 0], [1, 0, 1, 0, 1]])
        cube = Cube(data, long_name="foo")
        data_frame = iris.pandas.as_data_frame(cube)
        self.assertArrayEqual(data_frame, cube.data.astype('f').filled(np.nan))
        self.assertString(
            str(data_frame),
            tests.get_result_path(('pandas', 'as_dataframe',
                                   'masked.txt')))

    def test_time_gregorian(self):
        cube = Cube(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),
                    long_name="ts")
        day_offsets = [0, 100.1, 200.2, 300.3, 400.4]
        time_coord = DimCoord(day_offsets, long_name="time",
                              units="days since 2000-01-01 00:00")
        cube.add_dim_coord(time_coord, 1)
        data_frame = iris.pandas.as_data_frame(cube)
        self.assertArrayEqual(data_frame, cube.data)
        nanoseconds_per_day = 24 * 60 * 60 * 1000000000
        days_to_2000 = 365 * 30 + 7
        # pandas Timestamp class cannot handle floats in pandas <v0.12
        timestamps = [pandas.Timestamp(int(nanoseconds_per_day *
                                       (days_to_2000 + day_offset)))
                      for day_offset in day_offsets]
        self.assertTrue(all(data_frame.columns == timestamps))
        self.assertTrue(all(data_frame.index == [0, 1]))

    def test_time_360(self):
        cube = Cube(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),
                    long_name="ts")
        time_unit = iris.unit.Unit("days since 2000-01-01 00:00",
                                   calendar=iris.unit.CALENDAR_360_DAY)
        time_coord = DimCoord([100.1, 200.2], long_name="time",
                              units=time_unit)
        cube.add_dim_coord(time_coord, 0)
        data_frame = iris.pandas.as_data_frame(cube)
        self.assertArrayEqual(data_frame, cube.data)
        self.assertString(
            str(data_frame),
            tests.get_result_path(('pandas', 'as_dataframe',
                                   'time_360.txt')))

    def test_copy_true(self):
        cube = Cube(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]),
                    long_name="foo")
        data_frame = iris.pandas.as_data_frame(cube)
        data_frame[0][0] = 99
        self.assertEqual(cube.data[0, 0], 0)

    def test_copy_int32_false(self):
        cube = Cube(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]],
                             dtype=np.int32), long_name="foo")
        data_frame = iris.pandas.as_data_frame(cube, copy=False)
        data_frame[0][0] = 99
        self.assertEqual(cube.data[0, 0], 99)

    def test_copy_int64_false(self):
        cube = Cube(np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]],
                             dtype=np.int64), long_name="foo")
        data_frame = iris.pandas.as_data_frame(cube, copy=False)
        data_frame[0][0] = 99
        self.assertEqual(cube.data[0, 0], 99)

    def test_copy_float_false(self):
        cube = Cube(np.array([[0, 1, 2, 3, 4.4], [5, 6, 7, 8, 9]]),
                    long_name="foo")
        data_frame = iris.pandas.as_data_frame(cube, copy=False)
        data_frame[0][0] = 99
        self.assertEqual(cube.data[0, 0], 99)

    def test_copy_masked_true(self):
        data = np.ma.MaskedArray([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]],
                                 mask=[[0, 1, 0, 1, 0], [1, 0, 1, 0, 1]])
        cube = Cube(data, long_name="foo")
        data_frame = iris.pandas.as_data_frame(cube)
        data_frame[0][0] = 99
        self.assertEqual(cube.data[0, 0], 0)

    def test_copy_masked_false(self):
        data = np.ma.MaskedArray([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]],
                                 mask=[[0, 1, 0, 1, 0], [1, 0, 1, 0, 1]])
        cube = Cube(data, long_name="foo")
        with self.assertRaises(ValueError):
            data_frame = iris.pandas.as_data_frame(cube, copy=False)


@skip_pandas
class TestSeriesAsCube(tests.IrisTest):

    def test_series_simple(self):
        series = pandas.Series([0, 1, 2, 3, 4], index=[5, 6, 7, 8, 9])
        self.assertCML(
            iris.pandas.as_cube(series),
            tests.get_result_path(('pandas', 'as_cube', 'series_simple.cml')))

    def test_series_object(self):
        class Thing(object):
            def __repr__(self):
                return "A Thing"
        series = pandas.Series(
            [0, 1, 2, 3, 4],
            index=[Thing(), Thing(), Thing(), Thing(), Thing()])
        self.assertCML(
            iris.pandas.as_cube(series),
            tests.get_result_path(('pandas', 'as_cube', 'series_object.cml')))

    def test_series_masked(self):
        series = pandas.Series([0, float('nan'), 2, np.nan, 4],
                               index=[5, 6, 7, 8, 9])
        self.assertCML(
            iris.pandas.as_cube(series),
            tests.get_result_path(('pandas', 'as_cube', 'series_masked.cml')))

    def test_series_datetime_gregorian(self):
        series = pandas.Series(
            [0, 1, 2, 3, 4],
            index=[datetime.datetime(2001, 01, 01, 01, 01, 01),
                   datetime.datetime(2002, 02, 02, 02, 02, 02),
                   datetime.datetime(2003, 03, 03, 03, 03, 03),
                   datetime.datetime(2004, 04, 04, 04, 04, 04),
                   datetime.datetime(2005, 05, 05, 05, 05, 05)])
        self.assertCML(
            iris.pandas.as_cube(series),
            tests.get_result_path(('pandas', 'as_cube',
                                   'series_datetime_gregorian.cml')))

    def test_series_netcdftime_360(self):
        series = pandas.Series(
            [0, 1, 2, 3, 4],
            index=[netcdftime.datetime(2001, 01, 01, 01, 01, 01),
                   netcdftime.datetime(2002, 02, 02, 02, 02, 02),
                   netcdftime.datetime(2003, 03, 03, 03, 03, 03),
                   netcdftime.datetime(2004, 04, 04, 04, 04, 04),
                   netcdftime.datetime(2005, 05, 05, 05, 05, 05)])
        self.assertCML(
            iris.pandas.as_cube(series,
                                calendars={0: iris.unit.CALENDAR_360_DAY}),
            tests.get_result_path(('pandas', 'as_cube',
                                   'series_netcdfimte_360.cml')))

    def test_copy_true(self):
        series = pandas.Series([0, 1, 2, 3, 4], index=[5, 6, 7, 8, 9])
        cube = iris.pandas.as_cube(series)
        cube.data[0] = 99
        self.assertEqual(series[5], 0)

    def test_copy_false(self):
        series = pandas.Series([0, 1, 2, 3, 4], index=[5, 6, 7, 8, 9])
        cube = iris.pandas.as_cube(series, copy=False)
        cube.data[0] = 99
        self.assertEqual(series[5], 99)


@skip_pandas
class TestDataFrameAsCube(tests.IrisTest):

    def test_data_frame_simple(self):
        data_frame = pandas.DataFrame([[0, 1, 2, 3, 4],
                                       [5, 6, 7, 8, 9]],
                                      index=[10, 11],
                                      columns=[12, 13, 14, 15, 16])
        self.assertCML(
            iris.pandas.as_cube(data_frame),
            tests.get_result_path(('pandas', 'as_cube',
                                   'data_frame_simple.cml')))

    def test_data_frame_nonotonic(self):
        data_frame = pandas.DataFrame([[0, 1, 2, 3, 4],
                                       [5, 6, 7, 8, 9]],
                                      index=[10, 10],
                                      columns=[12, 12, 14, 15, 16])
        self.assertCML(
            iris.pandas.as_cube(data_frame),
            tests.get_result_path(('pandas', 'as_cube',
                                   'data_frame_nonotonic.cml')))

    def test_data_frame_masked(self):
        data_frame = pandas.DataFrame([[0, float('nan'), 2, 3, 4],
                                       [5, 6, 7, np.nan, 9]],
                                      index=[10, 11],
                                      columns=[12, 13, 14, 15, 16])
        self.assertCML(
            iris.pandas.as_cube(data_frame),
            tests.get_result_path(('pandas', 'as_cube',
                                   'data_frame_masked.cml')))

    def test_data_frame_netcdftime_360(self):
        data_frame = pandas.DataFrame(
            [[0, 1, 2, 3, 4],
             [5, 6, 7, 8, 9]],
            index=[netcdftime.datetime(2001, 01, 01, 01, 01, 01),
                   netcdftime.datetime(2002, 02, 02, 02, 02, 02)],
            columns=[10, 11, 12, 13, 14])
        self.assertCML(
            iris.pandas.as_cube(
                data_frame,
                calendars={0: iris.unit.CALENDAR_360_DAY}),
            tests.get_result_path(('pandas', 'as_cube',
                                   'data_frame_netcdftime_360.cml')))

    def test_data_frame_datetime_gregorian(self):
        data_frame = pandas.DataFrame(
            [[0, 1, 2, 3, 4],
             [5, 6, 7, 8, 9]],
            index=[datetime.datetime(2001, 01, 01, 01, 01, 01),
                   datetime.datetime(2002, 02, 02, 02, 02, 02)],
            columns=[10, 11, 12, 13, 14])
        self.assertCML(
            iris.pandas.as_cube(data_frame),
            tests.get_result_path(('pandas', 'as_cube',
                                   'data_frame_datetime_gregorian.cml')))

    def test_copy_true(self):
        data_frame = pandas.DataFrame([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])
        cube = iris.pandas.as_cube(data_frame)
        cube.data[0, 0] = 99
        self.assertEqual(data_frame[0][0], 0)

    def test_copy_false(self):
        data_frame = pandas.DataFrame([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])
        cube = iris.pandas.as_cube(data_frame, copy=False)
        cube.data[0, 0] = 99
        self.assertEqual(data_frame[0][0], 99)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_peak
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

import iris.tests as tests
import iris.tests.stock
import numpy as np
import numpy.ma as ma


class TestPeakAggregator(tests.IrisTest):
    def test_peak_coord_length_1(self):
        # Coordinate contains a single point.
        latitude = iris.coords.DimCoord(np.array([0]),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(np.array([1]),
                              standard_name='air_temperature',
                              units='kelvin')
        cube.add_dim_coord(latitude, 0)

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([1], dtype=np.float32))

    def test_peak_coord_length_2(self):
        # Coordinate contains 2 points.
        latitude = iris.coords.DimCoord(range(0, 2, 1),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(np.array([1, 2]),
                              standard_name='air_temperature',
                              units='kelvin')
        cube.add_dim_coord(latitude, 0)

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([2], dtype=np.float32))

    def test_peak_coord_length_3(self):
        # Coordinate contains 3 points.
        latitude = iris.coords.DimCoord(range(0, 3, 1),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(np.array([1, 2, 1]),
                              standard_name='air_temperature',
                              units='kelvin')
        cube.add_dim_coord(latitude, 0)

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([2], dtype=np.float32))

    def test_peak_1d(self):
        # Collapse a 1d cube.
        latitude = iris.coords.DimCoord(range(0, 11, 1),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(np.array([1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1]),
                              standard_name='air_temperature',
                              units='kelvin')
        cube.add_dim_coord(latitude, 0)

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([6], dtype=np.float32))

    def test_peak_duplicate_coords(self):
        # Collapse cube along 2 coordinates (both the same).
        latitude = iris.coords.DimCoord(range(0, 4, 1),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(np.array([1, 2, 3, 1]),
                              standard_name='air_temperature',
                              units='kelvin')
        cube.add_dim_coord(latitude, 0)

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([3], dtype=np.float32))

        collapsed_cube = cube.collapsed(('latitude', 'latitude'),
                                        iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([3], dtype=np.float32))

    def test_peak_2d(self):
        # Collapse a 2d cube.
        longitude = iris.coords.DimCoord(range(0, 4, 1),
                                         standard_name='longitude',
                                         units='degrees')
        latitude = iris.coords.DimCoord(range(0, 3, 1),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(np.array([[1, 2, 3, 1], [4, 5, 7, 4],
                                        [2, 3, 4, 2]]),
                              standard_name='air_temperature',
                              units='kelvin')

        cube.add_dim_coord(latitude, 0)
        cube.add_dim_coord(longitude, 1)

        collapsed_cube = cube.collapsed('longitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([3, 7.024054, 4],
                                             dtype=np.float32))

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([4.024977, 5.024977,
                                              7.017852, 4.024977],
                                             dtype=np.float32))

        collapsed_cube = cube.collapsed(('longitude', 'latitude'),
                                        iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([7.041787], dtype=np.float32))

        collapsed_cube = cube.collapsed(('latitude', 'longitude'),
                                        iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([7.041629], dtype=np.float32))

    def test_peak_without_peak_value(self):
        # No peak in column (values equal).
        latitude = iris.coords.DimCoord(range(0, 4, 1),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(np.array([1, 1, 1, 1]),
                              standard_name='air_temperature',
                              units='kelvin')
        cube.add_dim_coord(latitude, 0)

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([1], dtype=np.float32))

    def test_peak_with_nan(self):
        # Single nan in column.
        latitude = iris.coords.DimCoord(range(0, 5, 1),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(np.array([1, 4, 2, 3, 1], dtype=np.float32),
                              standard_name='air_temperature',
                              units='kelvin')
        cube.add_dim_coord(latitude, 0)

        cube.data[3] = np.nan

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([4.024977], dtype=np.float32))
        self.assertEqual(collapsed_cube.data.shape, (1,))

        # Only nans in column.
        cube.data[:] = np.nan

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertTrue(np.isnan(collapsed_cube.data).all())
        self.assertEqual(collapsed_cube.data.shape, (1,))

    def test_peak_with_mask(self):
        # Single value in column masked.
        latitude = iris.coords.DimCoord(range(0, 5, 1),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(ma.array([1, 4, 2, 3, 2], dtype=np.float32),
                              standard_name='air_temperature',
                              units='kelvin')
        cube.add_dim_coord(latitude, 0)

        cube.data[3] = ma.masked

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([4.024977], dtype=np.float32))
        self.assertTrue(ma.isMaskedArray(collapsed_cube.data))
        self.assertEqual(collapsed_cube.data.shape, (1,))

        # Whole column masked.
        cube.data[:] = ma.masked

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        masked_array = ma.array(ma.masked)
        self.assertTrue(ma.allequal(collapsed_cube.data, masked_array))
        self.assertTrue(ma.isMaskedArray(collapsed_cube.data))
        self.assertEqual(collapsed_cube.data.shape, (1,))

    def test_peak_with_nan_and_mask(self):
        # Single nan in column with single value masked.
        latitude = iris.coords.DimCoord(range(0, 5, 1),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(ma.array([1, 4, 2, 3, 1], dtype=np.float32),
                              standard_name='air_temperature',
                              units='kelvin')
        cube.add_dim_coord(latitude, 0)

        cube.data[3] = np.nan
        cube.data[4] = ma.masked

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([4.024977], dtype=np.float32))
        self.assertTrue(ma.isMaskedArray(collapsed_cube.data))
        self.assertEqual(collapsed_cube.data.shape, (1,))

        # Only nans in column where values not masked.
        cube.data[0:3] = np.nan

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertTrue(np.isnan(collapsed_cube.data).all())
        self.assertTrue(ma.isMaskedArray(collapsed_cube.data))
        self.assertEqual(collapsed_cube.data.shape, (1,))

    def test_peak_against_max(self):
        # Cube with data that infers a peak value greater than the column max.
        latitude = iris.coords.DimCoord(range(0, 7, 1),
                                        standard_name='latitude',
                                        units='degrees')
        cube = iris.cube.Cube(np.array([0, 1, 3, 7, 7, 4, 2],
                                       dtype=np.float32),
                              standard_name='air_temperature',
                              units='kelvin')
        cube.add_dim_coord(latitude, 0)

        collapsed_cube = cube.collapsed('latitude', iris.analysis.PEAK)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([7.630991], dtype=np.float32))

        collapsed_cube = cube.collapsed('latitude', iris.analysis.MAX)
        self.assertArrayAlmostEqual(collapsed_cube.data,
                                    np.array([7], dtype=np.float32))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pickling
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test pickling of Iris objects.

"""
from __future__ import with_statement

# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import cPickle
import StringIO

import biggus
import numpy as np

import iris


class TestPickle(tests.IrisTest):
    def pickle_then_unpickle(self, obj):
        """Returns a generator of ("cpickle protocol number", object) tuples."""
        for protocol in xrange(1 + cPickle.HIGHEST_PROTOCOL):
            str_buffer = StringIO.StringIO()
            cPickle.dump(obj, str_buffer, protocol)

            # move the str_buffer back to the start and reconstruct
            str_buffer.seek(0)
            reconstructed_obj = cPickle.load(str_buffer)

            yield protocol, reconstructed_obj

    def assertCubeData(self, cube1, cube2):
        np.testing.assert_array_equal(cube1.lazy_data().ndarray(),
                                      cube2.lazy_data().ndarray())

    @tests.skip_data
    def test_cube_pickle(self):
        cube = iris.load_cube(tests.get_data_path(('PP', 'globClim1', 'theta.pp')))
        self.assertTrue(cube.has_lazy_data())
        self.assertCML(cube, ('cube_io', 'pickling', 'theta.cml'), checksum=False)

        for _, recon_cube in self.pickle_then_unpickle(cube):
            self.assertTrue(recon_cube.has_lazy_data())
            self.assertCML(recon_cube, ('cube_io', 'pickling', 'theta.cml'), checksum=False)
            self.assertCubeData(cube, recon_cube)

    @tests.skip_data
    def test_cube_with_deferred_coord_points(self):
        # Data with 2d lats and lons that when loaded results in points that
        # are LazyArray objects.
        filename = tests.get_data_path(('NetCDF',
                                        'rotated',
                                        'xy',
                                        'rotPole_landAreaFraction.nc'))
        cube = iris.load_cube(filename)
        # Pickle and unpickle. Do not perform any CML tests
        # to avoid side effects.
        _, recon_cube = next(self.pickle_then_unpickle(cube))
        self.assertEqual(recon_cube, cube)

    @tests.skip_data
    def test_cubelist_pickle(self):
        cubelist = iris.load(tests.get_data_path(('PP', 'COLPEX', 'theta_and_orog_subset.pp')))
        single_cube = cubelist[0]

        self.assertCML(cubelist, ('cube_io', 'pickling', 'cubelist.cml'))
        self.assertCML(single_cube, ('cube_io', 'pickling', 'single_cube.cml'))

        for _, reconstructed_cubelist in self.pickle_then_unpickle(cubelist):
            self.assertCML(reconstructed_cubelist, ('cube_io', 'pickling', 'cubelist.cml'))
            self.assertCML(reconstructed_cubelist[0], ('cube_io', 'pickling', 'single_cube.cml'))

            for cube_orig, cube_reconstruct in zip(cubelist, reconstructed_cubelist):
                self.assertArrayEqual(cube_orig.data, cube_reconstruct.data)
                self.assertEqual(cube_orig, cube_reconstruct)

    def test_picking_equality_misc(self):
        items_to_test = [
                        iris.unit.Unit("hours since 2007-01-15 12:06:00", calendar=iris.unit.CALENDAR_STANDARD),
                        iris.unit.as_unit('1'),
                        iris.unit.as_unit('meters'),
                        iris.unit.as_unit('no-unit'),
                        iris.unit.as_unit('unknown')
                        ]

        for orig_item in items_to_test:
            for protocol, reconstructed_item in self.pickle_then_unpickle(orig_item):
                fail_msg = ('Items are different after pickling at protocol %s.'
                           '\nOrig item: %r\nNew item: %r' % (protocol, orig_item, reconstructed_item)
                            )
                self.assertEqual(orig_item, reconstructed_item, fail_msg)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_plot
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

from functools import wraps
import types
import warnings

import numpy as np

import iris
import iris.coords as coords
import iris.tests.stock

# Run tests in no graphics mode if matplotlib is not available.
if tests.MPL_AVAILABLE:
    import matplotlib.pyplot as plt
    import iris.plot as iplt
    import iris.quickplot as qplt
    import iris.symbols
    import iris.tests.test_mapping as test_mapping


def simple_cube():
    cube = iris.tests.stock.realistic_4d()
    cube = cube[:, 0, 0, :]
    cube.coord('time').guess_bounds()
    return cube


@tests.skip_plot
class TestSimple(tests.GraphicsTest):
    def test_points(self):
        cube = simple_cube()
        qplt.contourf(cube)
        self.check_graphic()

    def test_bounds(self):
        cube = simple_cube()
        qplt.pcolor(cube)
        self.check_graphic()


@tests.skip_plot
class TestMissingCoord(tests.GraphicsTest):
    def _check(self, cube):
        qplt.contourf(cube)
        self.check_graphic()

        qplt.pcolor(cube)
        self.check_graphic()

    def test_no_u(self):
        cube = simple_cube()
        cube.remove_coord('grid_longitude')
        self._check(cube)

    def test_no_v(self):
        cube = simple_cube()
        cube.remove_coord('time')
        self._check(cube)

    def test_none(self):
        cube = simple_cube()
        cube.remove_coord('grid_longitude')
        cube.remove_coord('time')
        self._check(cube)


@tests.skip_data
@tests.skip_plot
class TestMissingCS(tests.GraphicsTest):
    @tests.skip_data
    def test_missing_cs(self):
        cube = tests.stock.simple_pp()
        cube.coord("latitude").coord_system = None
        cube.coord("longitude").coord_system = None
        qplt.contourf(cube)
        qplt.plt.gca().coastlines()
        self.check_graphic()


@tests.skip_plot
class TestHybridHeight(tests.GraphicsTest):
    def setUp(self):
        self.cube = iris.tests.stock.realistic_4d()[0, :15, 0, :]

    def _check(self, plt_method, test_altitude=True):
        plt_method(self.cube)
        self.check_graphic()

        plt_method(self.cube, coords=['level_height', 'grid_longitude'])
        self.check_graphic()

        plt_method(self.cube, coords=['grid_longitude', 'level_height'])
        self.check_graphic()

        if test_altitude:
            plt_method(self.cube, coords=['grid_longitude', 'altitude'])
            self.check_graphic()

            plt_method(self.cube, coords=['altitude', 'grid_longitude'])
            self.check_graphic()

    def test_points(self):
        self._check(qplt.contourf)

    def test_bounds(self):
        self._check(qplt.pcolor, test_altitude=False)

    def test_orography(self):
        qplt.contourf(self.cube)
        iplt.orography_at_points(self.cube)
        iplt.points(self.cube)
        self.check_graphic()

        coords = ['altitude', 'grid_longitude']
        qplt.contourf(self.cube, coords=coords)
        iplt.orography_at_points(self.cube, coords=coords)
        iplt.points(self.cube, coords=coords)
        self.check_graphic()

        # TODO: Test bounds once they are supported.
        with self.assertRaises(NotImplementedError):
            qplt.pcolor(self.cube)
            iplt.orography_at_bounds(self.cube)
            iplt.outline(self.cube)
            self.check_graphic()


@tests.skip_plot
class Test1dPlotMultiArgs(tests.GraphicsTest):
    # tests for iris.plot using multi-argument calling convention

    def setUp(self):
        self.cube1d = _load_4d_testcube()[0, :, 0, 0]
        self.draw_method = iplt.plot

    def test_cube(self):
        # just plot a cube against its dim coord
        self.draw_method(self.cube1d)   # altitude vs temp
        self.check_graphic()

    def test_coord(self):
        # plot the altitude coordinate
        self.draw_method(self.cube1d.coord('altitude'))
        self.check_graphic()

    def test_coord_cube(self):
        # plot temperature against sigma
        self.draw_method(self.cube1d.coord('sigma'), self.cube1d)
        self.check_graphic()

    def test_cube_coord(self):
        # plot a vertical profile of temperature
        self.draw_method(self.cube1d, self.cube1d.coord('altitude'))
        self.check_graphic()

    def test_coord_coord(self):
        # plot two coordinates that are not mappable
        self.draw_method(self.cube1d.coord('sigma'),
                         self.cube1d.coord('altitude'))
        self.check_graphic()

    def test_coord_coord_map(self):
        # plot lat-lon aux coordinates of a trajectory, which draws a map
        lon = iris.coords.AuxCoord([0, 5, 10, 15, 20, 25, 30, 35, 40, 45],
                                   standard_name='longitude',
                                   units='degrees_north')
        lat = iris.coords.AuxCoord([45, 55, 50, 60, 55, 65, 60, 70, 65, 75],
                                   standard_name='latitude',
                                   units='degrees_north')
        self.draw_method(lon, lat)
        plt.gca().coastlines()
        self.check_graphic()

    def test_cube_cube(self):
        # plot two phenomena against eachother, in this case just dummy data
        cube1 = self.cube1d.copy()
        cube2 = self.cube1d.copy()
        cube1.rename('some phenomenon')
        cube2.rename('some other phenomenon')
        cube1.units = iris.unit.Unit('no_unit')
        cube2.units = iris.unit.Unit('no_unit')
        cube1.data[:] = np.linspace(0, 1, 7)
        cube2.data[:] = np.exp(cube1.data)
        self.draw_method(cube1, cube2)
        self.check_graphic()

    def test_incompatible_objects(self):
        # incompatible objects (not the same length) should raise an error
        with self.assertRaises(ValueError):
            self.draw_method(self.cube1d.coord('time'), (self.cube1d))

    def test_multimidmensional(self):
        # multidimensional cubes are not allowed
        cube = _load_4d_testcube()[0, :, :, 0]
        with self.assertRaises(ValueError):
            self.draw_method(cube)

    def test_not_cube_or_coord(self):
        # inputs must be cubes or coordinates, otherwise an error should be
        # raised
        xdim = np.arange(self.cube1d.shape[0])
        with self.assertRaises(TypeError):
            self.draw_method(xdim, self.cube1d)

    def test_coords_deprecated(self):
        # ensure a warning is raised if the old coords keyword argument is
        # used, and make sure the plot produced is consistent with the old
        # interface
        msg = 'Missing deprecation warning for coords keyword.'
        with warnings.catch_warnings(record=True) as w:
            warnings.simplefilter('always')
            self.draw_method(self.cube1d, coords=['sigma'])
            self.assertEqual(len(w), 1, msg)
        self.check_graphic()

    def test_coords_deprecation_too_many(self):
        # in deprecation mode, too many coords is an error
        with self.assertRaises(ValueError):
            self.draw_method(self.cube1d, coords=['sigma', 'sigma'])

    def test_coords_deprecation_invalid_span(self):
        # in deprecation mode, a coordinate that doesn't span data is an error
        with self.assertRaises(ValueError):
            self.draw_method(self.cube1d, coords=['time'])


@tests.skip_plot
class Test1dQuickplotPlotMultiArgs(Test1dPlotMultiArgs):
    # tests for iris.plot using multi-argument calling convention

    def setUp(self):
        self.cube1d = _load_4d_testcube()[0, :, 0, 0]
        self.draw_method = qplt.plot


@tests.skip_data
@tests.skip_plot
class Test1dScatter(tests.GraphicsTest):

    def setUp(self):
        self.cube = iris.load_cube(
            tests.get_data_path(('NAME', 'NAMEIII_trajectory.txt')),
            'Temperature')
        self.draw_method = iplt.scatter

    def test_coord_coord(self):
        x = self.cube.coord('longitude')
        y = self.cube.coord('height')
        c = self.cube.data
        self.draw_method(x, y, c=c, edgecolor='none')
        self.check_graphic()

    def test_coord_coord_map(self):
        x = self.cube.coord('longitude')
        y = self.cube.coord('latitude')
        c = self.cube.data
        self.draw_method(x, y, c=c, edgecolor='none')
        plt.gca().coastlines()
        self.check_graphic()

    def test_coord_cube(self):
        x = self.cube.coord('latitude')
        y = self.cube
        c = self.cube.coord('Travel Time').points
        self.draw_method(x, y, c=c, edgecolor='none')
        self.check_graphic()

    def test_cube_coord(self):
        x = self.cube
        y = self.cube.coord('height')
        c = self.cube.coord('Travel Time').points
        self.draw_method(x, y, c=c, edgecolor='none')
        self.check_graphic()

    def test_cube_cube(self):
        x = iris.load_cube(
            tests.get_data_path(('NAME', 'NAMEIII_trajectory.txt')),
            'Rel Humidity')
        y = self.cube
        c = self.cube.coord('Travel Time').points
        self.draw_method(x, y, c=c, edgecolor='none')
        self.check_graphic()

    def test_incompatible_objects(self):
        # cubes/coordinates of different sizes cannot be plotted
        x = self.cube
        y = self.cube.coord('height')[:-1]
        with self.assertRaises(ValueError):
            self.draw_method(x, y)

    def test_multidimensional(self):
        # multidimensional cubes/coordinates are not allowed
        x = _load_4d_testcube()[0, :, :, 0]
        y = x.coord('model_level_number')
        with self.assertRaises(ValueError):
            self.draw_method(x, y)

    def test_not_cube_or_coord(self):
        # inputs must be cubes or coordinates
        x = np.arange(self.cube.shape[0])
        y = self.cube
        with self.assertRaises(TypeError):
            self.draw_method(x, y)


@tests.skip_data
@tests.skip_plot
class Test1dQuickplotScatter(Test1dScatter):

    def setUp(self):
        self.cube = iris.load_cube(
            tests.get_data_path(('NAME', 'NAMEIII_trajectory.txt')),
            'Temperature')
        self.draw_method = qplt.scatter


@tests.skip_data
@tests.skip_plot
class TestAttributePositive(tests.GraphicsTest):
    def test_1d_positive_up(self):
        path = tests.get_data_path(('NetCDF', 'ORCA2', 'votemper.nc'))
        cube = iris.load_cube(path)
        qplt.plot(cube.coord('depth'), cube[0, :, 60, 80])
        self.check_graphic()

    def test_1d_positive_down(self):
        path = tests.get_data_path(('NetCDF', 'ORCA2', 'votemper.nc'))
        cube = iris.load_cube(path)
        qplt.plot(cube[0, :, 60, 80], cube.coord('depth'))
        self.check_graphic()

    def test_2d_positive_up(self):
        path = tests.get_data_path(('NetCDF', 'testing',
                                    'small_theta_colpex.nc'))
        cube = iris.load_cube(path)[0, :, 42, :]
        qplt.pcolormesh(cube)
        self.check_graphic()

    def test_2d_positive_down(self):
        path = tests.get_data_path(('NetCDF', 'ORCA2', 'votemper.nc'))
        cube = iris.load_cube(path)[0, :, 42, :]
        qplt.pcolormesh(cube)
        self.check_graphic()


# Caches _load_4d_testcube so subsequent calls are faster
def cache(fn, cache={}):
    def inner(*args, **kwargs):
        key = fn.__name__
        if key not in cache:
            cache[key] = fn(*args, **kwargs)
        return cache[key]
    return inner


@cache
def _load_4d_testcube():
    # Load example 4d data (TZYX).
    test_cube = iris.tests.stock.realistic_4d()
    # Replace forecast_period coord with a multi-valued version.
    time_coord = test_cube.coord('time')
    n_times = len(time_coord.points)
    forecast_dims = test_cube.coord_dims(time_coord)
    test_cube.remove_coord('forecast_period')
    # Make up values (including bounds), to roughly match older testdata.
    point_values = np.linspace((1 + 1.0 / 6), 2.0, n_times)
    point_uppers = point_values + (point_values[1] - point_values[0])
    bound_values = np.column_stack([point_values, point_uppers])
    # NOTE: this must be a DimCoord
    #  - an equivalent AuxCoord produces different plots.
    new_forecast_coord = iris.coords.DimCoord(
        points=point_values,
        bounds=bound_values,
        standard_name='forecast_period',
        units=iris.unit.Unit('hours')
    )
    test_cube.add_aux_coord(new_forecast_coord, forecast_dims)
    # Heavily reduce dimensions for faster testing.
    # NOTE: this makes ZYX non-contiguous.  Doesn't seem to matter for now.
    test_cube = test_cube[:, ::10, ::10, ::10]
    return test_cube


@cache
def _load_wind_no_bounds():
    # Load the COLPEX data => TZYX
    path = tests.get_data_path(('PP', 'COLPEX', 'small_eastward_wind.pp'))
    wind = iris.load_cube(path, 'x_wind')

    # Remove bounds from all coords that have them.
    wind.coord('grid_latitude').bounds = None
    wind.coord('grid_longitude').bounds = None
    wind.coord('level_height').bounds = None
    wind.coord('sigma').bounds = None

    return wind[:, :, :50, :50]


def _time_series(src_cube):
    # Until we have plotting support for multiple axes on the same dimension,
    # remove the time coordinate and its axis.
    cube = src_cube.copy()
    cube.remove_coord('time')
    return cube


def _date_series(src_cube):
    # Until we have plotting support for multiple axes on the same dimension,
    # remove the forecast_period coordinate and its axis.
    cube = src_cube.copy()
    cube.remove_coord('forecast_period')
    return cube


@tests.skip_plot
class SliceMixin(object):
    """Mixin class providing tests for each 2-dimensional permutation of axes.

    Requires self.draw_method to be the relevant plotting function,
    and self.results to be a dictionary containing the desired test results."""

    def test_yx(self):
        cube = self.wind[0, 0, :, :]
        self.draw_method(cube)
        self.check_graphic()

    def test_zx(self):
        cube = self.wind[0, :, 0, :]
        self.draw_method(cube)
        self.check_graphic()

    def test_tx(self):
        cube = _time_series(self.wind[:, 0, 0, :])
        self.draw_method(cube)
        self.check_graphic()

    def test_zy(self):
        cube = self.wind[0, :, :, 0]
        self.draw_method(cube)
        self.check_graphic()

    def test_ty(self):
        cube = _time_series(self.wind[:, 0, :, 0])
        self.draw_method(cube)
        self.check_graphic()

    def test_tz(self):
        cube = _time_series(self.wind[:, :, 0, 0])
        self.draw_method(cube)
        self.check_graphic()


@tests.skip_data
class TestContour(tests.GraphicsTest, SliceMixin):
    """Test the iris.plot.contour routine."""
    def setUp(self):
        self.wind = _load_4d_testcube()
        self.draw_method = iplt.contour


@tests.skip_data
class TestContourf(tests.GraphicsTest, SliceMixin):
    """Test the iris.plot.contourf routine."""
    def setUp(self):
        self.wind = _load_4d_testcube()
        self.draw_method = iplt.contourf


@tests.skip_data
class TestPcolor(tests.GraphicsTest, SliceMixin):
    """Test the iris.plot.pcolor routine."""
    def setUp(self):
        self.wind = _load_4d_testcube()
        self.draw_method = iplt.pcolor


@tests.skip_data
class TestPcolormesh(tests.GraphicsTest, SliceMixin):
    """Test the iris.plot.pcolormesh routine."""
    def setUp(self):
        self.wind = _load_4d_testcube()
        self.draw_method = iplt.pcolormesh


def check_warnings(method):
    """
    Decorator that adds a catch_warnings and filter to assert
    the method being decorated issues a UserWarning.

    """
    @wraps(method)
    def decorated_method(self, *args, **kwargs):
        # Force reset of iris.coords warnings registry to avoid suppression of
        # repeated warnings. warnings.resetwarnings() does not do this.
        if hasattr(coords, '__warningregistry__'):
            coords.__warningregistry__.clear()

        # Check that method raises warning.
        with warnings.catch_warnings():
            warnings.simplefilter("error")
            with self.assertRaises(UserWarning):
                return method(self, *args, **kwargs)
    return decorated_method


def ignore_warnings(method):
    """
    Decorator that adds a catch_warnings and filter to suppress
    any warnings issues by the method being decorated.

    """
    @wraps(method)
    def decorated_method(self, *args, **kwargs):
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            return method(self, *args, **kwargs)
    return decorated_method


class CheckForWarningsMetaclass(type):
    """
    Metaclass that adds a further test for each base class test
    that checks that each test raises a UserWarning. Each base
    class test is then overriden to ignore warnings in order to
    check the underlying functionality.

    """
    def __new__(cls, name, bases, local):
        def add_decorated_methods(attr_dict, target_dict, decorator):
            for key, value in attr_dict.items():
                if (isinstance(value, types.FunctionType) and
                        key.startswith('test')):
                    new_key = '_'.join((key, decorator.__name__))
                    if new_key not in target_dict:
                        wrapped = decorator(value)
                        wrapped.__name__ = new_key
                        target_dict[new_key] = wrapped
                    else:
                        raise RuntimeError('A attribute called {!r} '
                                           'already exists.'.format(new_key))

        def override_with_decorated_methods(attr_dict, target_dict,
                                            decorator):
            for key, value in attr_dict.items():
                if (isinstance(value, types.FunctionType) and
                        key.startswith('test')):
                    target_dict[key] = decorator(value)

        # Add decorated versions of base methods
        # to check for warnings.
        for base in bases:
            add_decorated_methods(base.__dict__, local, check_warnings)

        # Override base methods to ignore warnings.
        for base in bases:
            override_with_decorated_methods(base.__dict__, local,
                                            ignore_warnings)

        return type.__new__(cls, name, bases, local)


@tests.skip_data
class TestPcolorNoBounds(tests.GraphicsTest, SliceMixin):
    """
    Test the iris.plot.pcolor routine on a cube with coordinates
    that have no bounds.

    """
    __metaclass__ = CheckForWarningsMetaclass

    def setUp(self):
        self.wind = _load_wind_no_bounds()
        self.draw_method = iplt.pcolor


@tests.skip_data
class TestPcolormeshNoBounds(tests.GraphicsTest, SliceMixin):
    """
    Test the iris.plot.pcolormesh routine on a cube with coordinates
    that have no bounds.

    """
    __metaclass__ = CheckForWarningsMetaclass

    def setUp(self):
        self.wind = _load_wind_no_bounds()
        self.draw_method = iplt.pcolormesh


@tests.skip_plot
class Slice1dMixin(object):
    """Mixin class providing tests for each 1-dimensional permutation of axes.

    Requires self.draw_method to be the relevant plotting function,
    and self.results to be a dictionary containing the desired test results."""

    def test_x(self):
        cube = self.wind[0, 0, 0, :]
        self.draw_method(cube)
        self.check_graphic()

    def test_y(self):
        cube = self.wind[0, 0, :, 0]
        self.draw_method(cube)
        self.check_graphic()

    def test_z(self):
        cube = self.wind[0, :, 0, 0]
        self.draw_method(cube)
        self.check_graphic()

    def test_t(self):
        cube = _time_series(self.wind[:, 0, 0, 0])
        self.draw_method(cube)
        self.check_graphic()

    def test_t_dates(self):
        cube = _date_series(self.wind[:, 0, 0, 0])
        self.draw_method(cube)
        plt.gcf().autofmt_xdate()
        plt.xlabel('Phenomenon time')

        self.check_graphic()


@tests.skip_data
class TestPlot(tests.GraphicsTest, Slice1dMixin):
    """Test the iris.plot.plot routine."""
    def setUp(self):
        self.wind = _load_4d_testcube()
        self.draw_method = iplt.plot


@tests.skip_data
class TestQuickplotPlot(tests.GraphicsTest, Slice1dMixin):
    """Test the iris.quickplot.plot routine."""
    def setUp(self):
        self.wind = _load_4d_testcube()
        self.draw_method = qplt.plot


_load_cube_once_cache = {}


def load_cube_once(filename, constraint):
    """Same syntax as load_cube, but will only load a file once,

    then cache the answer in a dictionary.

    """
    global _load_cube_once_cache
    key = (filename, str(constraint))
    cube = _load_cube_once_cache.get(key, None)

    if cube is None:
        cube = iris.load_cube(filename, constraint)
        _load_cube_once_cache[key] = cube

    return cube


class LambdaStr(object):
    """Provides a callable function which has a sensible __repr__."""
    def __init__(self, repr, lambda_fn):
        self.repr = repr
        self.lambda_fn = lambda_fn

    def __call__(self, *args, **kwargs):
        return self.lambda_fn(*args, **kwargs)

    def __repr__(self):
        return self.repr


@tests.skip_data
@tests.skip_plot
class TestPlotCoordinatesGiven(tests.GraphicsTest):
    def setUp(self):
        filename = tests.get_data_path(('PP', 'COLPEX',
                                        'theta_and_orog_subset.pp'))
        self.cube = load_cube_once(filename, 'air_potential_temperature')

        self.draw_module = iris.plot
        self.contourf = LambdaStr('iris.plot.contourf',
                                  lambda cube, *args, **kwargs:
                                  iris.plot.contourf(cube, *args, **kwargs))
        self.contour = LambdaStr('iris.plot.contour',
                                 lambda cube, *args, **kwargs:
                                 iris.plot.contour(cube, *args, **kwargs))
        self.points = LambdaStr('iris.plot.points',
                                lambda cube, *args, **kwargs:
                                iris.plot.points(cube, c=cube.data,
                                                 *args, **kwargs))
        self.plot = LambdaStr('iris.plot.plot',
                              lambda cube, *args, **kwargs:
                              iris.plot.plot(cube, *args, **kwargs))

        self.results = {'yx': ([self.contourf, ['grid_latitude',
                                                'grid_longitude']],
                               [self.contourf, ['grid_longitude',
                                                'grid_latitude']],
                               [self.contour, ['grid_latitude',
                                               'grid_longitude']],
                               [self.contour, ['grid_longitude',
                                               'grid_latitude']],
                               [self.points, ['grid_latitude',
                                              'grid_longitude']],
                               [self.points, ['grid_longitude',
                                              'grid_latitude']],),
                        'zx': ([self.contourf, ['model_level_number',
                                                'grid_longitude']],
                               [self.contourf, ['grid_longitude',
                                                'model_level_number']],
                               [self.contour, ['model_level_number',
                                               'grid_longitude']],
                               [self.contour, ['grid_longitude',
                                               'model_level_number']],
                               [self.points, ['model_level_number',
                                              'grid_longitude']],
                               [self.points, ['grid_longitude',
                                              'model_level_number']],),
                        'tx': ([self.contourf, ['time', 'grid_longitude']],
                               [self.contourf, ['grid_longitude', 'time']],
                               [self.contour, ['time', 'grid_longitude']],
                               [self.contour, ['grid_longitude', 'time']],
                               [self.points, ['time', 'grid_longitude']],
                               [self.points, ['grid_longitude', 'time']],),
                        'x': ([self.plot, ['grid_longitude']],),
                        'y': ([self.plot, ['grid_latitude']],)
                        }

    def draw(self, draw_method, *args, **kwargs):
        draw_fn = getattr(self.draw_module, draw_method)
        draw_fn(*args, **kwargs)
        self.check_graphic()

    def run_tests(self, cube, results):
        for draw_method, coords in results:
            draw_method(cube, coords=coords)
            try:
                self.check_graphic()
            except AssertionError, err:
                self.fail('Draw method %r failed with coords: %r. '
                          'Assertion message: %s' % (draw_method, coords, err))

    def run_tests_1d(self, cube, results):
        # there is a different calling convention for 1d plots
        for draw_method, coords in results:
            draw_method(cube.coord(coords[0]), cube)
            try:
                self.check_graphic()
            except AssertionError as err:
                msg = 'Draw method {!r} failed with coords: {!r}. ' \
                      'Assertion message: {!s}'
                self.fail(msg.format(draw_method, coords, err))

    def test_yx(self):
        test_cube = self.cube[0, 0, :, :]
        self.run_tests(test_cube, self.results['yx'])

    def test_zx(self):
        test_cube = self.cube[0, :15, 0, :]
        self.run_tests(test_cube, self.results['zx'])

    def test_tx(self):
        test_cube = self.cube[:, 0, 0, :]
        self.run_tests(test_cube, self.results['tx'])

    def test_x(self):
        test_cube = self.cube[0, 0, 0, :]
        self.run_tests_1d(test_cube, self.results['x'])

    def test_y(self):
        test_cube = self.cube[0, 0, :, 0]
        self.run_tests_1d(test_cube, self.results['y'])

    def test_badcoords(self):
        cube = self.cube[0, 0, :, :]
        draw_fn = getattr(self.draw_module, 'contourf')
        self.assertRaises(ValueError, draw_fn, cube,
                          coords=['grid_longitude', 'grid_longitude'])
        self.assertRaises(ValueError, draw_fn, cube,
                          coords=['grid_longitude', 'grid_longitude',
                                  'grid_latitude'])
        self.assertRaises(iris.exceptions.CoordinateNotFoundError, draw_fn,
                          cube, coords=['grid_longitude', 'wibble'])
        self.assertRaises(ValueError, draw_fn, cube, coords=[])
        self.assertRaises(ValueError, draw_fn, cube,
                          coords=[cube.coord('grid_longitude'),
                                  cube.coord('grid_longitude')])
        self.assertRaises(ValueError, draw_fn, cube,
                          coords=[cube.coord('grid_longitude'),
                                  cube.coord('grid_longitude'),
                                  cube.coord('grid_longitude')])

    def test_non_cube_coordinate(self):
        cube = self.cube[0, :, :, 0]
        pts = -100 + np.arange(cube.shape[1]) * 13
        x = coords.DimCoord(pts, standard_name='model_level_number',
                            attributes={'positive': 'up'})
        self.draw('contourf', cube, coords=['grid_latitude', x])


@tests.skip_data
@tests.skip_plot
class TestPlotDimAndAuxCoordsKwarg(tests.GraphicsTest):
    def setUp(self):
        filename = tests.get_data_path(('NetCDF', 'rotated', 'xy',
                                        'rotPole_landAreaFraction.nc'))
        self.cube = iris.load_cube(filename)

    def test_default(self):
        iplt.contourf(self.cube)
        plt.gca().coastlines()
        self.check_graphic()

    def test_coords(self):
        # Pass in dimension coords.
        rlat = self.cube.coord('grid_latitude')
        rlon = self.cube.coord('grid_longitude')
        iplt.contourf(self.cube, coords=[rlon, rlat])
        plt.gca().coastlines()
        self.check_graphic()
        # Pass in auxiliary coords.
        lat = self.cube.coord('latitude')
        lon = self.cube.coord('longitude')
        iplt.contourf(self.cube, coords=[lon, lat])
        plt.gca().coastlines()
        self.check_graphic()

    def test_coord_names(self):
        # Pass in names of dimension coords.
        iplt.contourf(self.cube, coords=['grid_longitude', 'grid_latitude'])
        plt.gca().coastlines()
        self.check_graphic()
        # Pass in names of auxiliary coords.
        iplt.contourf(self.cube, coords=['longitude', 'latitude'])
        plt.gca().coastlines()
        self.check_graphic()

    def test_yx_order(self):
        # Do not attempt to draw coastlines as it is not a map.
        iplt.contourf(self.cube, coords=['grid_latitude', 'grid_longitude'])
        self.check_graphic()
        iplt.contourf(self.cube, coords=['latitude', 'longitude'])
        self.check_graphic()


@tests.skip_plot
class TestSymbols(tests.GraphicsTest):
    def test_cloud_cover(self):
        iplt.symbols(range(10), [0] * 10, [iris.symbols.CLOUD_COVER[i]
                                           for i in range(10)], 0.375)
        iplt.plt.axis('off')
        self.check_graphic()


@tests.skip_plot
class TestPlottingExceptions(tests.IrisTest):
    def setUp(self):
        self.bounded_cube = tests.stock.lat_lon_cube()
        self.bounded_cube.coord("latitude").guess_bounds()
        self.bounded_cube.coord("longitude").guess_bounds()

    def test_boundmode_multidim(self):
        # Test exception translation.
        # We can't get contiguous bounded grids from multi-d coords.
        cube = self.bounded_cube
        cube.remove_coord("latitude")
        cube.add_aux_coord(coords.AuxCoord(points=cube.data,
                                           standard_name='latitude',
                                           units='degrees'), [0, 1])
        with self.assertRaises(ValueError):
            iplt.pcolormesh(cube, coords=['longitude', 'latitude'])

    def test_boundmode_4bounds(self):
        # Test exception translation.
        # We can only get contiguous bounded grids with 2 bounds per point.
        cube = self.bounded_cube
        lat = coords.AuxCoord.from_coord(cube.coord("latitude"))
        lat.bounds = np.array([lat.points, lat.points + 1,
                               lat.points + 2, lat.points + 3]).transpose()
        cube.remove_coord("latitude")
        cube.add_aux_coord(lat, 0)
        with self.assertRaises(ValueError):
            iplt.pcolormesh(cube, coords=['longitude', 'latitude'])

    def test_different_coord_systems(self):
        cube = self.bounded_cube
        lat = cube.coord('latitude')
        lon = cube.coord('longitude')
        lat.coord_system = iris.coord_systems.GeogCS(7000000)
        lon.coord_system = iris.coord_systems.GeogCS(7000001)
        with self.assertRaises(ValueError):
            iplt.pcolormesh(cube, coords=['longitude', 'latitude'])


@tests.skip_data
@tests.skip_plot
class TestPlotOtherCoordSystems(tests.GraphicsTest):
    def test_plot_tmerc(self):
        filename = tests.get_data_path(('NetCDF', 'transverse_mercator',
                                        'tmean_1910_1910.nc'))
        self.cube = iris.load_cube(filename)
        iplt.pcolormesh(self.cube[0])
        plt.gca().coastlines()
        self.check_graphic()


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pp_cf
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests
import iris.coords

import os
import subprocess
import types
import warnings

import iris
import iris.tests.pp as pp
import iris.util
from iris.fileformats.pp import STASH


def callback_000003000000_16_202_000128_1860_09_01_00_00_b_pp(cube, field, filename):
    cube.attributes['STASH'] = STASH(1, 16, 202)
    cube.standard_name = 'geopotential_height'
    cube.units = 'm'


def callback_HadCM2_ts_SAT_ann_18602100_b_pp(cube, field, filename):
    def reset_pole(coord_name):
        coord = cube.coord(coord_name)
        coord.rename(coord.name().replace('grid_', ''))
        coord.coord_system = coord.coord_system.ellipsoid

    reset_pole('grid_latitude')
    reset_pole('grid_longitude')
    cube.standard_name = 'air_temperature'
    cube.units = 'Celsius'
    cube.attributes['STASH'] = STASH(1, 3, 236)
    # Force the height to 1.5m
    if cube.coords("height"):
        cube.remove_coord("height")
    height_coord = iris.coords.DimCoord(1.5, standard_name='height', units='m')
    cube.add_aux_coord(height_coord)


def callback_model_b_pp(cube, field, filename):
    cube.standard_name = 'air_temperature'
    cube.units = 'K'
    cube.attributes['STASH'] = STASH(1, 16, 203)


def callback_integer_b_pp(cube, field, filename):
    cube.standard_name = 'land_binary_mask'
    cube.units = '1'
    del cube.attributes['STASH']


def callback_001000000000_00_000_000000_1860_01_01_00_00_f_b_pp(cube, field, filename):
    cube.standard_name = "sea_surface_height_above_geoid"
    cube.units = "m"


def callback_aaxzc_n10r13xy_b_pp(cube, field, filename):
    height_coord = iris.coords.DimCoord(1.5, long_name='height', units='m')
    cube.add_aux_coord(height_coord)


@tests.skip_data
class TestAll(tests.IrisTest, pp.PPTest):
    _ref_dir = ('usecases', 'pp_to_cf_conversion')

    def _test_file(self, name):
        """This is the main test routine that is called for each of the files listed below."""
        pp_path = self._src_pp_path(name)

        # 1) Load the PP and check the Cube
        callback_name = 'callback_' + name.replace('.', '_')
        callback = globals().get(callback_name)
        cubes = iris.load(pp_path, callback=callback)

        if name.endswith('.pp'):
            fname_name = name[:-3]
        else:
            fname_name = name

        self.assertCML(cubes, self._ref_dir + ('from_pp', fname_name + '.cml',))

        # 2) Save the Cube and check the netCDF
        nc_filenames = []

        for index, cube in enumerate(cubes):
            # Write Cube to netCDF file - must be NETCDF3_CLASSIC format for the cfchecker.
            file_nc = os.path.join(os.path.sep, 'var', 'tmp', '%s_%d.nc' % (fname_name, index))
            #file_nc = tests.get_result_path(self._ref_dir + ('to_netcdf', '%s_%d.nc' % (fname_name, index)))
            iris.save(cube, file_nc, netcdf_format='NETCDF3_CLASSIC')

            # Check the netCDF file against CDL expected output.
            self.assertCDL(file_nc, self._ref_dir + ('to_netcdf', '%s_%d.cdl' % (fname_name, index)))
            nc_filenames.append(file_nc)

            # Perform CF-netCDF conformance checking.
            with open('/dev/null', 'w') as dev_null:
                try:
                    # Check for the availability of the "cfchecker" application
                    subprocess.check_call(['which', 'cfchecker'], stderr=dev_null, stdout=dev_null)
                except subprocess.CalledProcessError:
                    warnings.warn('CF-netCDF "cfchecker" application not available. Skipping CF-netCDF compliance checking.')
                else:
                    file_checker = os.path.join(os.path.dirname(file_nc), '%s_%d.txt' % (fname_name, index))

                    with open(file_checker, 'w') as report:
                        # Generate cfchecker text report on the file.
                        # Don't use check_call() here, as cfchecker returns a non-zero status code
                        # for any non-compliant file, causing check_call() to raise an exception.
                        subprocess.call(['cfchecker', file_nc], stderr=report, stdout=report)

                    if not os.path.isfile(file_checker):
                        os.remove(file_nc)
                        self.fail('Failed to process %r with cfchecker' % file_nc)

                    with open(file_checker, 'r') as report:
                        # Get the cfchecker report and purge unwanted lines.
                        checker_report = ''.join([line for line in report.readlines() if not line.startswith('Using')])

                    os.remove(file_checker)
                    self.assertString(checker_report, self._ref_dir + ('to_netcdf', 'cf_checker', '%s_%d.txt' % (fname_name, index)))

        # 3) Load the netCDF and check the Cube
        for index, nc_filename in enumerate(nc_filenames):
            # Read netCDF to Cube.
            cube = iris.load_cube(nc_filename)
            self.assertCML(cube, self._ref_dir + ('from_netcdf', '%s_%d.cml' % (fname_name, index)))
            os.remove(nc_filename)

        # 4) Save the Cube and check the PP
        # Only the first four files pass their tests at the moment.

        if name in self.files_to_check[:4]:
            self._test_pp_save(cubes, name)

    def _src_pp_path(self, name):
        return tests.get_data_path(('PP', 'cf_processing', name))

    def _test_pp_save(self, cubes, name):
        # If there's no existing reference file then make it from the *source* data
        reference_txt_path = tests.get_result_path(self._ref_dir + ('to_pp', name + '.txt'))
        reference_pp_path = self._src_pp_path(name)
        with self.cube_save_test(reference_txt_path, reference_pp_path=reference_pp_path) as temp_pp_path:
            iris.save(cubes, temp_pp_path)

    files_to_check = [
                      '000003000000.03.236.000128.1990.12.01.00.00.b.pp',
                      '000003000000.03.236.004224.1990.12.01.00.00.b.pp',
                      '000003000000.03.236.008320.1990.12.01.00.00.b.pp',
                      '000003000000.16.202.000128.1860.09.01.00.00.b.pp',
                      '001000000000.00.000.000000.1860.01.01.00.00.f.b.pp',
                      '002000000000.44.101.131200.1920.09.01.00.00.b.pp',
                      '008000000000.44.101.000128.1890.09.01.00.00.b.pp',
                      'HadCM2_ts_SAT_ann_18602100.b.pp',
                      'aaxzc_level_lat_orig.b.pp',
                      'aaxzc_lon_lat_press_orig.b.pp',
                      'abcza_pa19591997_daily_29.b.pp',
                      '12187.b.pp',
                      'ocean_xsect.b.pp',
                      'model.b.pp',
                      'integer.b.pp',
                      'aaxzc_lon_lat_several.b.pp',
                      'aaxzc_n10r13xy.b.pp',
                      'aaxzc_time_press.b.pp',
                      'aaxzc_tseries.b.pp',
                      'abxpa_press_lat.b.pp',
                      'st30211.b.pp',
                      'st0fc942.b.pp',
                      'st0fc699.b.pp',
                      ]


def make_test_function(func_name, file_name):
    """Builds a function which can be later turned into a bound method."""
    scope = {}
    exec("""def %s(self):
                name = %r
                self._test_file(name)
    """ % (func_name, file_name), scope, scope)
    # return the newly created function
    return scope[func_name]


def attach_tests():
    # attach a test method on TestAll for each file to test
    for file_name in TestAll.files_to_check:
        func_name = 'test_{}'.format(file_name.replace('.', '_'))
        test_func = make_test_function(func_name, file_name)
        test_method = types.MethodType(test_func, None, TestAll)
        setattr(TestAll, func_name, test_method)


attach_tests()


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pp_module
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

from copy import deepcopy
import os
from types import GeneratorType
import unittest

import biggus
import netcdftime

import iris.fileformats
import iris.fileformats.pp as pp
import iris.util


@tests.skip_data
class TestPPCopy(tests.IrisTest):
    def setUp(self):
        self.filename = tests.get_data_path(('PP', 'aPPglob1', 'global.pp'))

    def test_copy_field_deferred(self):
        field = pp.load(self.filename).next()
        clone = field.copy()
        self.assertIsInstance(clone._data, biggus.Array)
        self.assertEqual(field, clone)
        clone.lbyr = 666
        self.assertNotEqual(field, clone)

    def test_deepcopy_field_deferred(self):
        field = pp.load(self.filename).next()
        clone = deepcopy(field)
        self.assertIsInstance(clone._data, biggus.Array)
        self.assertEqual(field, clone)
        clone.lbyr = 666
        self.assertNotEqual(field, clone)

    def test_copy_field_non_deferred(self):
        field = pp.load(self.filename, True).next()
        clone = field.copy()
        self.assertEqual(field, clone)
        clone.data[0][0] = 666
        self.assertNotEqual(field, clone)

    def test_deepcopy_field_non_deferred(self):
        field = pp.load(self.filename, True).next()
        clone = deepcopy(field)
        self.assertEqual(field, clone)
        clone.data[0][0] = 666
        self.assertNotEqual(field, clone)


class IrisPPTest(tests.IrisTest):
    def check_pp(self, pp_fields, reference_filename):
        """
        Checks the given iterable of PPField objects matches the reference file, or creates the
        reference file if it doesn't exist.

        """
        # turn the generator into a list 
        pp_fields = list(pp_fields)
        
        # Load deferred data for all of the fields (but don't do anything with it)
        for pp_field in pp_fields:
            pp_field.data
            
        test_string = str(pp_fields)
        reference_path = tests.get_result_path(reference_filename)
        if os.path.isfile(reference_path):
            reference = ''.join(open(reference_path, 'r').readlines())
            self._assert_str_same(reference+'\n', test_string+'\n', reference_filename, type_comparison_name='PP files')
        else:
            tests.logger.warning('Creating result file: %s', reference_path)
            open(reference_path, 'w').writelines(test_string)


class TestPPHeaderDerived(unittest.TestCase):

    def setUp(self):
        self.pp = pp.PPField2()
        self.pp.lbuser = (0, 1, 2, 3, 4, 5, 6)
        self.pp.lbtim = 11
        self.pp.lbproc = 65539

    def test_standard_access(self):
        self.assertEqual(self.pp.lbtim, 11)
        
    def test_lbtim_access(self):
        self.assertEqual(self.pp.lbtim[0], 1)
        self.assertEqual(self.pp.lbtim.ic, 1)
        
    def test_lbtim_setter(self):
        self.pp.lbtim[4] = 4
        self.pp.lbtim[0] = 4
        self.assertEqual(self.pp.lbtim[0], 4)
        self.assertEqual(self.pp.lbtim.ic, 4)
        
        self.pp.lbtim.ib = 9
        self.assertEqual(self.pp.lbtim.ib, 9)
        self.assertEqual(self.pp.lbtim[1], 9)
        
    def test_lbproc_access(self):
        # lbproc == 65539
        self.assertEqual(self.pp.lbproc[0], 9)
        self.assertEqual(self.pp.lbproc[19], 0)
        self.assertEqual(self.pp.lbproc.flag1, 1)
        self.assertEqual(self.pp.lbproc.flag65536, 1)
        self.assertEqual(self.pp.lbproc.flag131072, 0)
    
    def test_set_lbuser(self):
        self.pp.stash = 'm02s12i003'
        self.assertEqual(self.pp.stash, pp.STASH(2, 12, 3))
        self.pp.lbuser[6] = 5
        self.assertEqual(self.pp.stash, pp.STASH(5, 12, 3))
        self.pp.lbuser[3] = 4321
        self.assertEqual(self.pp.stash, pp.STASH(5, 4, 321))
    
    def test_set_stash(self):
        self.pp.stash = 'm02s12i003'
        self.assertEqual(self.pp.stash, pp.STASH(2, 12, 3))

        self.pp.stash = pp.STASH(3, 13, 4)
        self.assertEqual(self.pp.stash, pp.STASH(3, 13, 4))
        self.assertEqual(self.pp.lbuser[3], self.pp.stash.lbuser3())
        self.assertEqual(self.pp.lbuser[6], self.pp.stash.lbuser6())
        
        with self.assertRaises(ValueError):
            self.pp.stash = (4, 15, 5)
        
    def test_lbproc_bad_access(self):
        try:
            print self.pp.lbproc.flag65537
        except AttributeError:
            pass
        except Exception, err:
            self.fail("Should return a better error: " + str(err))


@tests.skip_data
class TestPPField_GlobalTemperature(IrisPPTest):
    def setUp(self):
        self.original_pp_filepath = tests.get_data_path(('PP', 'aPPglob1', 'global.pp'))
        self.r = list(pp.load(self.original_pp_filepath))

    def test_full_file(self):
        self.check_pp(self.r[0:10], ('PP', 'global_test.pp.txt'))   
            
    def test_lbtim_access(self):
        self.assertEqual(self.r[0].lbtim[0], 2)
        self.assertEqual(self.r[0].lbtim.ic, 2)
    
    def test_lbproc_access(self):
        self.assertEqual(self.r[0].lbproc[0], 8)
        self.assertEqual(self.r[0].lbproc[19], 0)
        self.assertEqual(self.r[0].lbproc.flag1, 0)
        self.assertEqual(self.r[0].lbproc.flag65536, 0)
        self.assertEqual(self.r[0].lbproc.flag131072, 0)

    def test_t1_t2_access(self):
        self.assertEqual(self.r[0].t1.timetuple(), netcdftime.datetime(1994, 12, 1, 0, 0).timetuple())

    def test_save_single(self):
        temp_filename = iris.util.create_temp_filename(".pp")
        self.r[0].save(open(temp_filename, 'wb'))
        self.assertEqual(self.file_checksum(temp_filename), self.file_checksum(self.original_pp_filepath))
        os.remove(temp_filename)
           
    def test_save_api(self):
        filepath = self.original_pp_filepath
        
        f = pp.load(filepath).next()

        temp_filename = iris.util.create_temp_filename(".pp")
        
        f.save(open(temp_filename, 'wb'))
        self.assertEqual(self.file_checksum(temp_filename), self.file_checksum(filepath))
        
        os.remove(temp_filename)
    

@tests.skip_data
class TestPackedPP(IrisPPTest):
    def test_wgdos(self):
        r = pp.load(tests.get_data_path(('PP', 'wgdos_packed', 'nae.20100104-06_0001.pp')))
        
        # Check that the result is a generator and convert to a list so that we can index and get the first one
        self.assertEqual( type(r), GeneratorType)
        r = list(r)
        
        self.check_pp(r, ('PP', 'nae_unpacked.pp.txt'))
        
        # check that trying to save this field again raises an error (we cannot currently write WGDOS packed fields)
        temp_filename = iris.util.create_temp_filename(".pp")
        self.assertRaises(NotImplementedError, r[0].save, open(temp_filename, 'wb'))
        os.remove(temp_filename)
        
    def test_rle(self):
        r = pp.load(tests.get_data_path(('PP', 'ocean_rle', 'ocean_rle.pp')))

        # Check that the result is a generator and convert to a list so that we can index and get the first one
        self.assertEqual( type(r), GeneratorType)
        r = list(r)

        self.check_pp(r, ('PP', 'rle_unpacked.pp.txt'))

        # check that trying to save this field again raises an error
        # (we cannot currently write RLE packed fields)
        with self.temp_filename('.pp') as temp_filename:
            with self.assertRaises(NotImplementedError):
                r[0].save(open(temp_filename, 'wb'))


@tests.skip_data
class TestPPFile(IrisPPTest):
    def test_lots_of_extra_data(self):
        r = pp.load(tests.get_data_path(('PP', 'cf_processing', 'HadCM2_ts_SAT_ann_18602100.b.pp')))
        r = list(r)
        self.assertEqual(r[0].lbcode.ix, 13)
        self.assertEqual(r[0].lbcode.iy, 23)
        self.assertEqual(len(r[0].lbcode), 5)
        self.check_pp(r, ('PP', 'extra_data_time_series.pp.txt'))
        

@tests.skip_data
class TestPPFileExtraXData(IrisPPTest):
    def setUp(self):
        self.original_pp_filepath = tests.get_data_path(('PP', 'ukV1', 'ukVpmslont.pp'))
        self.r = list(pp.load(self.original_pp_filepath))[0:5]
        
    def test_full_file(self):
        self.check_pp(self.r, ('PP', 'extra_x_data.pp.txt'))

    def test_save_single(self):
        filepath = tests.get_data_path(('PP', 'ukV1', 'ukVpmslont_first_field.pp'))
        f = pp.load(filepath).next()

        temp_filename = iris.util.create_temp_filename(".pp")
        f.save(open(temp_filename, 'wb'))
        
        s = pp.load(temp_filename).next()
        
        # force the data to be loaded (this was done for f when save was run)
        s.data
        self._assert_str_same(str(s)+'\n', str(f)+'\n', '', type_comparison_name='PP files')
        
        self.assertEqual(self.file_checksum(temp_filename), self.file_checksum(filepath))
        os.remove(temp_filename)
    

@tests.skip_data
class TestPPFileWithExtraCharacterData(IrisPPTest):
    def setUp(self):
        self.original_pp_filepath = tests.get_data_path(('PP', 'model_comp', 'dec_subset.pp'))
        self.r = pp.load(self.original_pp_filepath)
        self.r_loaded_data = pp.load(self.original_pp_filepath, read_data=True)
        
        # Check that the result is a generator and convert to a list so that we can index and get the first one
        self.assertEqual( type(self.r), GeneratorType)
        self.r = list(self.r)
        
        self.assertEqual( type(self.r_loaded_data), GeneratorType)
        self.r_loaded_data = list(self.r_loaded_data)
        
            
    def test_extra_field_title(self):
        self.assertEqual(self.r[0].field_title, 'AJHQA Time mean  !C Atmos u compnt of wind after timestep at 9.998 metres !C 01/12/2007 00:00 -> 01/01/2008 00:00')    

    def test_full_file(self):
        self.check_pp(self.r[0:10], ('PP', 'extra_char_data.pp.txt'))
        self.check_pp(self.r_loaded_data[0:10], ('PP', 'extra_char_data.w_data_loaded.pp.txt'))   
    
    def test_save_single(self):
        filepath = tests.get_data_path(('PP', 'model_comp', 'dec_first_field.pp'))
        f = pp.load(filepath).next()

        temp_filename = iris.util.create_temp_filename(".pp")
        f.save(open(temp_filename, 'wb'))
        
        s = pp.load(temp_filename).next()
        
        # force the data to be loaded (this was done for f when save was run)
        s.data
        self._assert_str_same(str(s)+'\n', str(f)+'\n', '', type_comparison_name='PP files')
        
        self.assertEqual(self.file_checksum(temp_filename), self.file_checksum(filepath))
        os.remove(temp_filename)
    

class TestBitwiseInt(unittest.TestCase):

    def test_3(self):
        t = pp.BitwiseInt(3)
        self.assertEqual(t[0], 3)
        self.assertTrue(t.flag1)
        self.assertTrue(t.flag2)
        self.assertRaises(AttributeError, getattr, t, "flag1024")
        
    def test_setting_flags(self):
        t = pp.BitwiseInt(3)
        self.assertEqual(t._value, 3)

        t.flag1 = False
        self.assertEqual(t._value, 2)
        t.flag2 = False
        self.assertEqual(t._value, 0)
        
        t.flag1 = True
        self.assertEqual(t._value, 1)
        t.flag2 = True
        self.assertEqual(t._value, 3)
        
        self.assertRaises(AttributeError, setattr, t, "flag1024", True)
        self.assertRaises(TypeError, setattr, t, "flag2", 1)

        t = pp.BitwiseInt(3, num_bits=11)
        t.flag1024 = True
        self.assertEqual(t._value, 1027)

    def test_standard_operators(self):
        t = pp.BitwiseInt(323)
        
        self.assertTrue(t == 323)
        self.assertFalse(t == 324)
        
        self.assertFalse(t != 323)
        self.assertTrue(t != 324)
        
        self.assertTrue(t >= 323)
        self.assertFalse(t >= 324)
        
        self.assertFalse(t > 323)
        self.assertTrue(t > 322)
        
        self.assertTrue(t <= 323)
        self.assertFalse(t <= 322)
        
        self.assertFalse(t < 323)
        self.assertTrue(t < 324)

        self.assertTrue(t in [323])
        self.assertFalse(t in [324])

    def test_323(self):
        t = pp.BitwiseInt(323)
        self.assertRaises(AttributeError, getattr, t, 'flag0')
        
        self.assertEqual(t.flag1, 1)
        self.assertEqual(t.flag2, 1)
        self.assertEqual(t.flag4, 0)
        self.assertEqual(t.flag8, 0)
        self.assertEqual(t.flag16, 0)
        self.assertEqual(t.flag32, 0)
        self.assertEqual(t.flag64, 1)
        self.assertEqual(t.flag128, 0)
        self.assertEqual(t.flag256, 1)


    def test_33214(self):
        t = pp.BitwiseInt(33214)
        self.assertEqual(t[0], 4)
        self.assertEqual(t.flag1, 0)
        self.assertEqual(t.flag2, 1)

    def test_negative_number(self):
        try:
            _ = pp.BitwiseInt(-5)
        except ValueError, err:
            self.assertEqual(str(err), 'Negative numbers not supported with splittable integers object')

    def test_128(self):
        t = pp.BitwiseInt(128)
        self.assertEqual(t.flag1, 0)
        self.assertEqual(t.flag2, 0)
        self.assertEqual(t.flag4, 0)
        self.assertEqual(t.flag8, 0)
        self.assertEqual(t.flag16, 0)
        self.assertEqual(t.flag32, 0)
        self.assertEqual(t.flag64, 0)
        self.assertEqual(t.flag128, 1)
        

class TestSplittableInt(unittest.TestCase):

    def test_3(self):
        t = pp.SplittableInt(3)
        self.assertEqual(t[0], 3)
        
    def test_grow_str_list(self):
        t = pp.SplittableInt(3)
        t[1] = 3
        self.assertEqual(t[1], 3)
        
        t[5] = 4
        
        self.assertEqual(t[5], 4)
        
        self.assertEqual( int(t), 400033)
        
        self.assertEqual(t, 400033)
        self.assertNotEqual(t, 33)
        
        self.assertTrue(t >= 400033)
        self.assertFalse(t >= 400034)
        
        self.assertTrue(t <= 400033)
        self.assertFalse(t <= 400032)
        
        self.assertTrue(t > 400032)
        self.assertFalse(t > 400034)
        
        self.assertTrue(t < 400034)
        self.assertFalse(t < 400032)        

    def test_name_mapping(self):
        t = pp.SplittableInt(33214, {'ones':0, 'tens':1, 'hundreds':2})
        self.assertEqual(t.ones, 4)
        self.assertEqual(t.tens, 1)
        self.assertEqual(t.hundreds, 2)
        
        t.ones = 9
        t.tens = 4
        t.hundreds = 0
        
        self.assertEqual(t.ones, 9)
        self.assertEqual(t.tens, 4)
        self.assertEqual(t.hundreds, 0)
        
    def test_name_mapping_multi_index(self):
        t = pp.SplittableInt(33214, {'weird_number':slice(None, None, 2), 
                                     'last_few':slice(-2, -5, -2), 
                                     'backwards':slice(None, None, -1)})
        self.assertEqual(t.weird_number, 324)
        self.assertEqual(t.last_few, 13)
        self.assertRaises(ValueError, setattr, t, 'backwards', 1)
        self.assertRaises(ValueError, setattr, t, 'last_few', 1)
        self.assertEqual(t.backwards, 41233)
        self.assertEqual(t, 33214)
        
        t.weird_number = 99
        # notice that this will zero the 5th number
        
        self.assertEqual(t, 3919)
        t.weird_number = 7899
        self.assertEqual(t, 7083919)
        t.foo = 1
        
        t = pp.SplittableInt(33214, {'ix':slice(None, 2), 'iy':slice(2, 4)})
        self.assertEqual(t.ix, 14)
        self.assertEqual(t.iy, 32)
        
        t.ix = 21
        self.assertEqual(t, 33221)
        
        t = pp.SplittableInt(33214, {'ix':slice(-1, 2)})
        self.assertEqual(t.ix, 0)

        t = pp.SplittableInt(4, {'ix':slice(None, 2), 'iy':slice(2, 4)})
        self.assertEqual(t.ix, 4)
        self.assertEqual(t.iy, 0)
        
    def test_33214(self):
        t = pp.SplittableInt(33214)
        self.assertEqual(t[4], 3)
        self.assertEqual(t[3], 3)
        self.assertEqual(t[2], 2)
        self.assertEqual(t[1], 1)
        self.assertEqual(t[0], 4)
        
        # The rest should be zero
        for i in range(5, 100):
            self.assertEqual(t[i], 0)

    def test_negative_number(self):
        self.assertRaises(ValueError, pp.SplittableInt, -5)
        try:
            _ = pp.SplittableInt(-5)
        except ValueError, err:
            self.assertEqual(str(err), 'Negative numbers not supported with splittable integers object')

            
class TestSplittableIntEquality(unittest.TestCase):
    def test_not_implemented(self):
        class Terry(object): pass
        sin = pp.SplittableInt(0)
        self.assertIs(sin.__eq__(Terry()), NotImplemented)
        self.assertIs(sin.__ne__(Terry()), NotImplemented)


class TestPPDataProxyEquality(unittest.TestCase):
    def test_not_implemented(self):
        class Terry(object): pass
        pox = pp.PPDataProxy("john", "michael", "eric", "graham", "brian",
                             "spam", "beans", "eggs")
        self.assertIs(pox.__eq__(Terry()), NotImplemented)
        self.assertIs(pox.__ne__(Terry()), NotImplemented)


class TestPPFieldEquality(unittest.TestCase):
    def test_not_implemented(self):
        class Terry(object): pass
        pox = pp.PPField3()
        self.assertIs(pox.__eq__(Terry()), NotImplemented)
        self.assertIs(pox.__ne__(Terry()), NotImplemented)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pp_stash
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import iris
import iris.fileformats.pp
import iris.io
import iris.util
import iris.tests.stock


class TestPPStash(tests.IrisTest):

    @tests.skip_data
    def test_cube_attributes(self):
        cube = tests.stock.simple_pp()
        self.assertEqual('m01s16i203', cube.attributes['STASH'])
        self.assertNotEqual('m01s16i999', cube.attributes['STASH'])
        self.assertEqual(cube.attributes['STASH'], 'm01s16i203')
        self.assertNotEqual(cube.attributes['STASH'], 'm01s16i999')

    @tests.skip_data
    def test_ppfield(self):
        data_path = tests.get_data_path(('PP', 'simple_pp', 'global.pp'))
        pps = iris.fileformats.pp.load(data_path)
        for pp in pps:
            self.assertEqual('m01s16i203', pp.stash)
            self.assertNotEqual('m01s16i999', pp.stash)
            self.assertEqual(pp.stash, 'm01s16i203')
            self.assertNotEqual(pp.stash, 'm01s16i999')

    def test_stash_against_stash(self):
        self.assertEqual(iris.fileformats.pp.STASH(1,2,3), iris.fileformats.pp.STASH(1,2,3))
        self.assertNotEqual(iris.fileformats.pp.STASH(1,2,3), iris.fileformats.pp.STASH(2,3,4))

    def test_stash_against_str(self):
        self.assertEqual(iris.fileformats.pp.STASH(1,2,3), 'm01s02i003')
        self.assertEqual('m01s02i003', iris.fileformats.pp.STASH(1,2,3))
        self.assertNotEqual(iris.fileformats.pp.STASH(1,2,3), 'm02s03i004')
        self.assertNotEqual('m02s03i004', iris.fileformats.pp.STASH(1,2,3))

    def test_irregular_stash_str(self):
        self.assertEqual(iris.fileformats.pp.STASH(1,2,3), 'm01s02i0000000003')
        self.assertEqual(iris.fileformats.pp.STASH(1,2,3), 'm01s02i3')
        self.assertEqual(iris.fileformats.pp.STASH(1,2,3), 'm01s2i3')
        self.assertEqual(iris.fileformats.pp.STASH(1,2,3), 'm1s2i3')

        self.assertEqual('m01s02i0000000003', iris.fileformats.pp.STASH(1,2,3))
        self.assertEqual('m01s02i3', iris.fileformats.pp.STASH(1,2,3))
        self.assertEqual('m01s2i3', iris.fileformats.pp.STASH(1,2,3))
        self.assertEqual('m1s2i3', iris.fileformats.pp.STASH(1,2,3))

        self.assertNotEqual(iris.fileformats.pp.STASH(2,3,4), 'm01s02i0000000003')
        self.assertNotEqual(iris.fileformats.pp.STASH(2,3,4), 'm01s02i3')
        self.assertNotEqual(iris.fileformats.pp.STASH(2,3,4), 'm01s2i3')
        self.assertNotEqual(iris.fileformats.pp.STASH(2,3,4), 'm1s2i3')

        self.assertNotEqual('m01s02i0000000003', iris.fileformats.pp.STASH(2,3,4))
        self.assertNotEqual('m01s02i3', iris.fileformats.pp.STASH(2,3,4))
        self.assertNotEqual('m01s2i3', iris.fileformats.pp.STASH(2,3,4))
        self.assertNotEqual('m1s2i3', iris.fileformats.pp.STASH(2,3,4))

        self.assertEqual(iris.fileformats.pp.STASH.from_msi('M01s02i003'), 'm01s02i003')
        self.assertEqual('m01s02i003', iris.fileformats.pp.STASH.from_msi('M01s02i003'))

    def test_illegal_stash_str_range(self):
        self.assertEqual(iris.fileformats.pp.STASH(0,2,3), 'm??s02i003')
        self.assertNotEqual(iris.fileformats.pp.STASH(0,2,3), 'm01s02i003')
        self.assertEqual('m??s02i003', iris.fileformats.pp.STASH(0,2,3))
        self.assertNotEqual('m01s02i003', iris.fileformats.pp.STASH(0,2,3))

        self.assertEqual(iris.fileformats.pp.STASH(0,2,3), 'm??s02i003')
        self.assertEqual(iris.fileformats.pp.STASH(0,2,3), 'm00s02i003')
        self.assertEqual('m??s02i003', iris.fileformats.pp.STASH(0,2,3))
        self.assertEqual('m00s02i003', iris.fileformats.pp.STASH(0,2,3))

        self.assertEqual(iris.fileformats.pp.STASH(100,2,3), 'm??s02i003')
        self.assertEqual(iris.fileformats.pp.STASH(100,2,3), 'm100s02i003')
        self.assertEqual('m??s02i003', iris.fileformats.pp.STASH(100,2,3))
        self.assertEqual('m100s02i003', iris.fileformats.pp.STASH(100,2,3))

    def test_illegal_stash_stash_range(self):
        self.assertEqual(iris.fileformats.pp.STASH(0,2,3), iris.fileformats.pp.STASH(0,2,3))
        self.assertEqual(iris.fileformats.pp.STASH(100,2,3), iris.fileformats.pp.STASH(100,2,3))
        self.assertEqual(iris.fileformats.pp.STASH(100,2,3), iris.fileformats.pp.STASH(999,2,3))

    def test_illegal_stash_format(self):
        with self.assertRaises(ValueError):
            self.assertEqual(iris.fileformats.pp.STASH(1,2,3), 'abc')
        with self.assertRaises(ValueError):
            self.assertEqual('abc', iris.fileformats.pp.STASH(1,2,3))

        with self.assertRaises(ValueError):
            self.assertEqual(iris.fileformats.pp.STASH(1,2,3), 'm01s02003')
        with self.assertRaises(ValueError):
            self.assertEqual('m01s02003', iris.fileformats.pp.STASH(1,2,3))

    def test_illegal_stash_type(self):
        with self.assertRaises(TypeError):
            self.assertEqual(iris.fileformats.pp.STASH.from_msi(0102003), 'm01s02i003')

        with self.assertRaises(TypeError):
            self.assertEqual('m01s02i003', iris.fileformats.pp.STASH.from_msi(0102003))

        with self.assertRaises(TypeError):
            self.assertEqual(iris.fileformats.pp.STASH.from_msi(['m01s02i003']), 'm01s02i003')

        with self.assertRaises(TypeError):
            self.assertEqual('m01s02i003', iris.fileformats.pp.STASH.from_msi(['m01s02i003']))

    def test_stash_lbuser(self):
        stash = iris.fileformats.pp.STASH(2, 32, 456)
        self.assertEqual(stash.lbuser6(), 2)
        self.assertEqual(stash.lbuser3(), 32456)

if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pp_to_cube
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import os

import iris
import iris.fileformats.pp
import iris.fileformats.pp_rules
import iris.fileformats.rules
import iris.io
import iris.util
import iris.tests.stock


@tests.skip_data
class TestPPLoadCustom(tests.IrisTest):
    def setUp(self):
        self.subcubes = iris.cube.CubeList()
        filename = tests.get_data_path(('PP', 'aPPglob1', 'global.pp'))
        self.template = iris.fileformats.pp.load(filename).next()

    def _field_to_cube(self, field):
        cube, _, _ = iris.fileformats.rules._make_cube(
            field, iris.fileformats.pp_rules.convert)
        return cube

    def test_lbtim_2(self):
        for delta in range(10):
            field = self.template.copy()
            field.lbtim = 2
            field.lbdat += delta
            cube = self._field_to_cube(field)
            self.subcubes.append(cube)
        cube = self.subcubes.merge()[0]
        self.assertCML(cube, ('pp_rules', 'lbtim_2.cml'))

    def _ocean_depth(self, bounded=False):
        lbuser = list(self.template.lbuser)
        lbuser[6] = 2
        lbuser[3] = 101
        lbuser = tuple(lbuser)
        for level_and_depth in enumerate([5.0, 15.0, 25.0, 35.0, 45.0]):
            field = self.template.copy()
            field.lbuser = lbuser
            field.lbvc = 2
            field.lbfc = 601
            field.lblev, field.blev = level_and_depth
            if bounded:
                brsvd = list(field.brsvd)
                brsvd[0] = field.blev - 1
                field.brsvd = tuple(brsvd)
                field.brlev = field.blev + 1
            cube = self._field_to_cube(field)
            self.subcubes.append(cube)

    def test_ocean_depth(self):
        self._ocean_depth()
        cube = self.subcubes.merge()[0]
        self.assertCML(cube, ('pp_rules', 'ocean_depth.cml'))

    def test_ocean_depth_bounded(self):
        self._ocean_depth(bounded=True)
        cube = self.subcubes.merge()[0]
        self.assertCML(cube, ('pp_rules', 'ocean_depth_bounded.cml'))

    def test_invalid_units(self):
        # UM to CF rules are mapped to the invalid unit "1e3 psu @0.035"
        # for the STASH code m02s00i102.
        lbuser = list(self.template.lbuser)
        lbuser[6] = 2
        lbuser[3] = 102
        self.template.lbuser = tuple(lbuser)
        cube = self._field_to_cube(self.template)
        self.assertCML(cube, ('pp_rules', 'invalid_units.cml'))


class TestReferences(tests.IrisTest):
    def setUp(self):
        target = iris.tests.stock.simple_2d()
        target.data = target.data.astype('f4')
        self.target = target
        self.ref = target.copy()

    def test_regrid_missing_coord(self):
        # If the target cube is missing one of the source dimension
        # coords, ensure the re-grid fails nicely - i.e. returns None.
        self.target.remove_coord('bar')
        new_ref = iris.fileformats.rules._ensure_aligned({}, self.ref,
                                                         self.target)
        self.assertIsNone(new_ref)

    def test_regrid_codimension(self):
        # If the target cube has two of the source dimension coords
        # sharing the same dimension (e.g. a trajectory) then ensure
        # the re-grid fails nicely - i.e. returns None.
        self.target.remove_coord('foo')
        new_foo = self.target.coord('bar').copy()
        new_foo.rename('foo')
        self.target.add_aux_coord(new_foo, 0)
        new_ref = iris.fileformats.rules._ensure_aligned({}, self.ref,
                                                         self.target)
        self.assertIsNone(new_ref)

    def test_regrid_identity(self):
        new_ref = iris.fileformats.rules._ensure_aligned({}, self.ref,
                                                         self.target)
        # Bounds don't make it through the re-grid process
        self.ref.coord('bar').bounds = None
        self.ref.coord('foo').bounds = None
        self.assertEqual(new_ref, self.ref)


@tests.skip_data
class TestPPLoading(tests.IrisTest):
    def test_simple(self):
        cube = iris.tests.stock.simple_pp()
        self.assertCML(cube, ('cube_io', 'pp', 'load', 'global.cml'))


@tests.skip_data
class TestPPLoadRules(tests.IrisTest):
    def test_pp_load_rules(self):
        # Test PP loading and rule evaluation.

        cube = iris.tests.stock.simple_pp()
        self.assertCML(cube, ('pp_rules', 'global.cml'))

        data_path = tests.get_data_path(('PP', 'rotated_uk', 'rotated_uk.pp'))
        cube = iris.load(data_path)[0]
        self.assertCML(cube, ('pp_rules', 'rotated_uk.cml'))

    def test_lbproc(self):
        data_path = tests.get_data_path(('PP', 'meanMaxMin', '200806081200__qwpb.T24.pp'))
        # Set up standard name and T+24 constraint
        constraint = iris.Constraint('air_temperature', forecast_period=24)
        cubes = iris.load(data_path, constraint)
        cubes = iris.cube.CubeList([cubes[0], cubes[3], cubes[1], cubes[2], cubes[4]])
        self.assertCML(cubes, ('pp_rules', 'lbproc_mean_max_min.cml'))

    def test_custom_rules(self):
        # Test custom rule evaluation.
        # Default behaviour
        data_path = tests.get_data_path(('PP', 'aPPglob1', 'global.pp'))
        cube = iris.load_cube(data_path)
        self.assertEqual(cube.standard_name, 'air_temperature')

        # Custom behaviour
        temp_path = iris.util.create_temp_filename()
        f = open(temp_path, 'w')
        f.write('\n'.join((
            'IF',
            'f.lbuser[3] == 16203',
            'THEN',
            'CMAttribute("standard_name", None)',
            'CMAttribute("long_name", "customised")')))
        f.close()
        iris.fileformats.pp.add_load_rules(temp_path)
        cube = iris.load_cube(data_path)
        self.assertEqual(cube.name(), 'customised')
        os.remove(temp_path)

        # Back to default
        iris.fileformats.pp.reset_load_rules()
        cube = iris.load_cube(data_path)
        self.assertEqual(cube.standard_name, 'air_temperature')

    def test_cell_methods(self):
        # Test cell methods are created for correct values of lbproc
        orig_file = tests.get_data_path(('PP', 'aPPglob1', 'global.pp'))

        # Values that result in cell methods being created
        cell_method_values = {128 : "mean", 4096 : "minimum", 8192 : "maximum"}

        # Make test values as list of single bit values and some multiple bit values
        single_bit_values = list(iris.fileformats.pp.LBPROC_PAIRS)
        multiple_bit_values = [(128 + 64, ""), (4096 + 2096, ""), (8192 + 1024, "")]
        test_values = list(single_bit_values) + multiple_bit_values

        for value, _ in test_values:
            f = iris.fileformats.pp.load(orig_file).next()
            f.lbproc = value # set value

            # Write out pp file
            temp_filename = iris.util.create_temp_filename(".pp")
            f.save(open(temp_filename, 'wb'))

            # Load pp file
            cube = iris.load_cube(temp_filename)

            if value in cell_method_values:
                # Check for cell method on cube
                self.assertEqual(cube.cell_methods[0].method, cell_method_values[value])
            else:
                # Check no cell method was created for values other than 128, 4096, 8192
                self.assertEqual(len(cube.cell_methods), 0)

            os.remove(temp_filename)


    def test_process_flags(self):
        # Test that process flags are created for correct values of lbproc
        orig_file = tests.get_data_path(('PP', 'aPPglob1', 'global.pp'))

        # Values that result in process flags attribute NOT being created
        omit_process_flags_values = (128, 4096, 8192)

        # Test single flag values
        for value, _ in iris.fileformats.pp.LBPROC_PAIRS:
            f = iris.fileformats.pp.load(orig_file).next()
            f.lbproc = value # set value

            # Write out pp file
            temp_filename = iris.util.create_temp_filename(".pp")
            f.save(open(temp_filename, 'wb'))

            # Load pp file
            cube = iris.load_cube(temp_filename)

            if value in omit_process_flags_values:
                # Check ukmo__process_flags attribute not created
                self.assertEqual(cube.attributes.get("ukmo__process_flags", None), None)
            else:
                # Check ukmo__process_flags attribute contains correct values
                self.assertIn(iris.fileformats.pp.lbproc_map[value], cube.attributes["ukmo__process_flags"])

            os.remove(temp_filename)

        # Test multiple flag values
        multiple_bit_values = ((128, 64), (4096, 1024), (8192, 1024))

        # Maps lbproc value to the process flags that should be created
        multiple_map = {sum(x) : [iris.fileformats.pp.lbproc_map[y] for y in x] for x in multiple_bit_values}

        for bit_values in multiple_bit_values:
            f = iris.fileformats.pp.load(orig_file).next()
            f.lbproc = sum(bit_values) # set value

            # Write out pp file
            temp_filename = iris.util.create_temp_filename(".pp")
            f.save(open(temp_filename, 'wb'))

            # Load pp file
            cube = iris.load_cube(temp_filename)

            # Check the process flags created
            self.assertEquals(set(cube.attributes["ukmo__process_flags"]), set(multiple_map[sum(bit_values)]), "Mismatch between expected and actual process flags.")

            os.remove(temp_filename)


@tests.skip_data
class TestStdName(tests.IrisTest):
    def test_no_std_name(self):
        fname = tests.get_data_path(['PP', 'simple_pp', 'bad_global.pp'])
        cube = iris.load_cube(fname)
        self.assertCML([cube], ['cube_io', 'pp', 'no_std_name.cml'])


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_quickplot
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Tests the high-level plotting interface.

"""
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests
import iris.tests.test_plot as test_plot

import iris

# Run tests in no graphics mode if matplotlib is not available.
if tests.MPL_AVAILABLE:
    import matplotlib.pyplot as plt
    import iris.plot as iplt
    import iris.quickplot as qplt
    import iris.tests.test_mapping as test_mapping


# Caches _load_theta so subsequent calls are faster
def cache(fn, cache={}):
    def inner(*args, **kwargs):
        key = "result"
        if not cache:
            cache[key] =  fn(*args, **kwargs)
        return cache[key]
    return inner


@cache
def _load_theta():
    path = tests.get_data_path(('PP', 'COLPEX', 'theta_and_orog_subset.pp'))
    theta = iris.load_cube(path, 'air_potential_temperature')

    # Improve the unit
    theta.units = 'K'

    return theta


@tests.skip_data
@tests.skip_plot
class TestQuickplotCoordinatesGiven(test_plot.TestPlotCoordinatesGiven):
    def setUp(self):
        filename = tests.get_data_path(('PP', 'COLPEX', 'theta_and_orog_subset.pp'))
        self.cube = test_plot.load_cube_once(filename, 'air_potential_temperature')

        self.draw_module = iris.quickplot
        self.contourf = test_plot.LambdaStr('iris.quickplot.contourf', lambda cube, *args, **kwargs:
                                                  iris.quickplot.contourf(cube, *args, **kwargs))
        self.contour = test_plot.LambdaStr('iris.quickplot.contour', lambda cube, *args, **kwargs:
                                                  iris.quickplot.contour(cube, *args, **kwargs))
        self.points = test_plot.LambdaStr('iris.quickplot.points', lambda cube, *args, **kwargs:
                                                  iris.quickplot.points(cube, c=cube.data, *args, **kwargs))
        self.plot = test_plot.LambdaStr('iris.quickplot.plot', lambda cube, *args, **kwargs:
                                                  iris.quickplot.plot(cube, *args, **kwargs))

        self.results = {'yx': (
                           [self.contourf, ['grid_latitude', 'grid_longitude']],
                           [self.contourf, ['grid_longitude', 'grid_latitude']],
                           [self.contour, ['grid_latitude', 'grid_longitude']],
                           [self.contour, ['grid_longitude', 'grid_latitude']],
                           [self.points, ['grid_latitude', 'grid_longitude']],
                           [self.points, ['grid_longitude', 'grid_latitude']],
                           ),
                       'zx': (
                           [self.contourf, ['model_level_number', 'grid_longitude']],
                           [self.contourf, ['grid_longitude', 'model_level_number']],
                           [self.contour, ['model_level_number', 'grid_longitude']],
                           [self.contour, ['grid_longitude', 'model_level_number']],
                           [self.points, ['model_level_number', 'grid_longitude']],
                           [self.points, ['grid_longitude', 'model_level_number']],
                           ),
                        'tx': (
                           [self.contourf, ['time', 'grid_longitude']],
                           [self.contourf, ['grid_longitude', 'time']],
                           [self.contour, ['time', 'grid_longitude']],
                           [self.contour, ['grid_longitude', 'time']],
                           [self.points, ['time', 'grid_longitude']],
                           [self.points, ['grid_longitude', 'time']],
                           ),
                        'x': (
                           [self.plot, ['grid_longitude']],
                           ),
                        'y': (
                           [self.plot, ['grid_latitude']],
                           ),
                       }


@tests.skip_data
@tests.skip_plot
class TestLabels(tests.GraphicsTest):
    def setUp(self):
        self.theta = _load_theta()

    def _slice(self, coords):
        """Returns the first cube containing the requested coordinates."""
        for cube in self.theta.slices(coords):
            break
        return cube

    def _small(self):
        # Use a restricted size so we can make out the detail
        cube = self._slice(['model_level_number', 'grid_longitude'])
        return cube[:5, :5]

    def test_contour(self):
        qplt.contour(self._small())
        self.check_graphic()

        qplt.contourf(self._small(), coords=['model_level_number', 'grid_longitude'])
        self.check_graphic()

    def test_contourf(self):
        qplt.contourf(self._small())

        cube = self._small()
        iplt.orography_at_points(cube)

        self.check_graphic()

        qplt.contourf(self._small(), coords=['model_level_number', 'grid_longitude'])
        self.check_graphic()

        qplt.contourf(self._small(), coords=['grid_longitude', 'model_level_number'])
        self.check_graphic()

    def test_contourf_nameless(self):
        cube = self._small()
        cube.standard_name = None
        qplt.contourf(cube, coords=['grid_longitude', 'model_level_number'])
        self.check_graphic()

    def test_pcolor(self):
        qplt.pcolor(self._small())
        self.check_graphic()

    def test_pcolormesh(self):
        qplt.pcolormesh(self._small())

        #cube = self._small()
        #iplt.orography_at_bounds(cube)

        self.check_graphic()

    def test_map(self):
        cube = self._slice(['grid_latitude', 'grid_longitude'])
        qplt.contour(cube)
        self.check_graphic()

        # check that the result of adding 360 to the data is *almost* identically the same result
        lon = cube.coord('grid_longitude')
        lon.points = lon.points + 360
        qplt.contour(cube)
        self.check_graphic()

    def test_alignment(self):
        cube = self._small()
        qplt.contourf(cube)
        #qplt.outline(cube)
        qplt.points(cube)
        self.check_graphic()


@tests.skip_data
@tests.skip_plot
class TestTimeReferenceUnitsLabels(tests.GraphicsTest):

    def setUp(self):
        path = tests.get_data_path(('PP', 'aPProt1', 'rotatedMHtimecube.pp'))
        self.cube = iris.load_cube(path)[:, 0, 0]

    def test_reference_time_units(self):
        # units should not be displayed for a reference time
        qplt.plot(self.cube.coord('time'), self.cube)
        plt.gcf().autofmt_xdate()
        self.check_graphic()

    def test_not_reference_time_units(self):
        # units should be displayed for other time coordinates
        qplt.plot(self.cube.coord('forecast_period'), self.cube)
        self.check_graphic()


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_regrid
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import numpy as np

import iris
from iris import load_cube
from iris.analysis.interpolate import regrid_to_max_resolution
from iris.cube import Cube
from iris.coords import DimCoord
from iris.coord_systems import GeogCS


@tests.skip_data
class TestRegrid(tests.IrisTest):
    @staticmethod
    def patch_data(cube):
        # Workaround until regrid can handle factories
        for factory in cube.aux_factories:
            cube.remove_aux_factory(factory)

        # Remove coords that share lat/lon dimensions
        dim = cube.coord_dims(cube.coord('grid_longitude'))[0]
        for coord in cube.coords(contains_dimension=dim, dim_coords=False):
            cube.remove_coord(coord)
        dim = cube.coord_dims(cube.coord('grid_latitude'))[0]
        for coord in cube.coords(contains_dimension=dim, dim_coords=False):
            cube.remove_coord(coord)

    def setUp(self):
        self.theta_p_alt_path = tests.get_data_path(
            ('PP', 'COLPEX', 'small_colpex_theta_p_alt.pp'))
        self.theta_constraint = iris.Constraint('air_potential_temperature')
        self.airpress_constraint = iris.Constraint('air_pressure')
        self.level_constraint = iris.Constraint(model_level_number=1)
        self.multi_level_constraint = iris.Constraint(
            model_level_number=lambda c: 1 <= c < 6)
        self.forecast_constraint = iris.Constraint(
            forecast_period=lambda dt: 0.49 < dt < 0.51)

    def test_regrid_low_dimensional(self):
        theta = load_cube(
            self.theta_p_alt_path,
            (self.theta_constraint
             & self.level_constraint
             & self.forecast_constraint)
        )
        airpress = load_cube(
            self.theta_p_alt_path,
            (self.airpress_constraint
             & self.level_constraint
             & self.forecast_constraint)
        )
        TestRegrid.patch_data(theta)
        TestRegrid.patch_data(airpress)

        # 0-dimensional
        theta_0 = theta[0, 0]
        airpress_0 = airpress[0, 0]
        theta0_regridded = theta_0.regridded(airpress_0, mode='nearest')
        airpress0_regridded = airpress_0.regridded(theta_0, mode='nearest')
        self.assertEqual(theta_0, theta0_regridded)
        self.assertEqual(airpress_0, airpress0_regridded)
        self.assertCMLApproxData(
            theta0_regridded,
            ('regrid', 'theta_on_airpress_0d.cml'))
        self.assertCMLApproxData(
            airpress0_regridded,
            ('regrid', 'airpress_on_theta_0d.cml'))

        # 1-dimensional
        theta_1 = theta[0, 1:4]
        airpress_1 = airpress[0, 0:4]
        self.assertCMLApproxData(
            theta_1.regridded(airpress_1, mode='nearest'),
            ('regrid', 'theta_on_airpress_1d.cml'))
        self.assertCMLApproxData(
            airpress_1.regridded(theta_1, mode='nearest'),
            ('regrid', 'airpress_on_theta_1d.cml'))

        # 2-dimensional
        theta_2 = theta[1:3, 1:4]
        airpress_2 = airpress[0:4, 0:4]
        self.assertCMLApproxData(
            theta_2.regridded(airpress_2, mode='nearest'),
            ('regrid', 'theta_on_airpress_2d.cml'))
        self.assertCMLApproxData(
            airpress_2.regridded(theta_2, mode='nearest'),
            ('regrid', 'airpress_on_theta_2d.cml'))

    def test_regrid_3d(self):
        theta = load_cube(
            self.theta_p_alt_path,
            (self.theta_constraint
             & self.multi_level_constraint
             & self.forecast_constraint)
        )
        airpress = load_cube(
            self.theta_p_alt_path,
            (self.airpress_constraint
             & self.multi_level_constraint
             & self.forecast_constraint)
        )
        TestRegrid.patch_data(theta)
        TestRegrid.patch_data(airpress)

        theta_part = theta[:, 1:3, 1:4]
        airpress_part = airpress[:, 0:4, 0:4]
        self.assertCMLApproxData(
            theta_part.regridded(airpress_part, mode='nearest'),
            ('regrid', 'theta_on_airpress_3d.cml'))
        self.assertCMLApproxData(
            airpress_part.regridded(theta_part, mode='nearest'),
            ('regrid', 'airpress_on_theta_3d.cml'))

    def test_regrid_max_resolution(self):
        low = Cube(np.arange(12).reshape((3, 4)))
        cs = GeogCS(6371229)
        low.add_dim_coord(DimCoord(np.array([-1, 0, 1], dtype=np.int32), 'latitude', units='degrees', coord_system=cs), 0)
        low.add_dim_coord(DimCoord(np.array([-1, 0, 1, 2], dtype=np.int32), 'longitude', units='degrees', coord_system=cs), 1)

        med = Cube(np.arange(20).reshape((4, 5)))
        cs = GeogCS(6371229)
        med.add_dim_coord(DimCoord(np.array([-1, 0, 1, 2], dtype=np.int32), 'latitude', units='degrees', coord_system=cs), 0)
        med.add_dim_coord(DimCoord(np.array([-2, -1, 0, 1, 2], dtype=np.int32), 'longitude', units='degrees', coord_system=cs), 1)

        high = Cube(np.arange(30).reshape((5, 6)))
        cs = GeogCS(6371229)
        high.add_dim_coord(DimCoord(np.array([-2, -1, 0, 1, 2], dtype=np.int32), 'latitude', units='degrees', coord_system=cs), 0)
        high.add_dim_coord(DimCoord(np.array([-2, -1, 0, 1, 2, 3], dtype=np.int32), 'longitude', units='degrees', coord_system=cs), 1)

        cubes = regrid_to_max_resolution([low, med, high], mode='nearest')
        self.assertCMLApproxData(cubes, ('regrid', 'low_med_high.cml'))


class TestRegridBilinear(tests.IrisTest):
    def setUp(self):
        self.cs = GeogCS(6371229)

        # Source cube candidate for regridding.
        cube = Cube(np.arange(12, dtype=np.float32).reshape(3, 4), long_name='unknown')
        cube.units = '1'
        cube.add_dim_coord(DimCoord(np.array([1, 2, 3]), 'latitude', units='degrees', coord_system=self.cs), 0)
        cube.add_dim_coord(DimCoord(np.array([1, 2, 3, 4]), 'longitude', units='degrees', coord_system=self.cs), 1)
        self.source = cube

        # Cube with a smaller grid in latitude and longitude than the source grid by taking the coordinate mid-points.
        cube = Cube(np.arange(6, dtype=np.float).reshape(2, 3))
        cube.units = '1'
        cube.add_dim_coord(DimCoord(np.array([1.5, 2.5]), 'latitude', units='degrees', coord_system=self.cs), 0)
        cube.add_dim_coord(DimCoord(np.array([1.5, 2.5, 3.5]), 'longitude', units='degrees', coord_system=self.cs), 1)
        self.smaller = cube

        # Cube with a larger grid in latitude and longitude than the source grid by taking the coordinate mid-points and extrapolating at extremes.
        cube = Cube(np.arange(20, dtype=np.float).reshape(4, 5))
        cube.units = '1'
        cube.add_dim_coord(DimCoord(np.array([0.5, 1.5, 2.5, 3.5]), 'latitude', units='degrees', coord_system=self.cs), 0)
        cube.add_dim_coord(DimCoord(np.array([0.5, 1.5, 2.5, 3.5, 4.5]), 'longitude', units='degrees', coord_system=self.cs), 1)
        self.larger = cube

    def test_bilinear_smaller_lon_left(self):
        # Anchor smaller grid from the first point in longitude and perform mid-point linear interpolation in latitude.
        self.smaller.coord('longitude').points = self.smaller.coord('longitude').points - 0.5
        self.assertCMLApproxData(self.source.regridded(self.smaller), ('regrid', 'bilinear_smaller_lon_align_left.cml'))

    def test_bilinear_smaller(self):
        # Perform mid-point bilinear interpolation over both latitude and longitude.
        self.assertCMLApproxData(self.source.regridded(self.smaller), ('regrid', 'bilinear_smaller.cml'))

    def test_bilinear_smaller_lon_right(self):
        # Anchor smaller grid from the last point in longitude and perform mid-point linear interpolation in latitude.
        self.smaller.coord('longitude').points = self.smaller.coord('longitude').points + 0.5
        self.assertCMLApproxData(self.source.regridded(self.smaller), ('regrid', 'bilinear_smaller_lon_align_right.cml'))

    def test_bilinear_larger_lon_left(self):
        # Extrapolate first point of longitude with others aligned to source grid, and perform linear interpolation with extrapolation over latitude.
        coord = iris.coords.DimCoord(np.array([0.5, 1, 2, 3, 4]), 'longitude', units='degrees', coord_system=self.cs)
        self.larger.remove_coord('longitude')
        self.larger.add_dim_coord(coord, 1)
        self.assertCMLApproxData(self.source.regridded(self.larger), ('regrid', 'bilinear_larger_lon_extrapolate_left.cml'))

    def test_bilinear_larger(self):
        # Perform mid-point bi-linear interpolation with extrapolation over latitude and longitude.
        self.assertCMLApproxData(self.source.regridded(self.larger), ('regrid', 'bilinear_larger.cml'))

    def test_bilinear_larger_lon_right(self):
        # Extrapolate last point of longitude with others aligned to source grid, and perform linear interpolation with extrapolation over latitude.
        coord = iris.coords.DimCoord(np.array([1, 2, 3, 4, 4.5]), 'longitude', units='degrees', coord_system=self.cs)
        self.larger.remove_coord('longitude')
        self.larger.add_dim_coord(coord, 1)
        self.assertCMLApproxData(self.source.regridded(self.larger), ('regrid', 'bilinear_larger_lon_extrapolate_right.cml'))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_rules
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test metadata translation rules.

"""
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import types

from iris.aux_factory import HybridHeightFactory
from iris.cube import Cube
from iris.fileformats.rules import ConcreteReferenceTarget, Factory, Loader, \
                                   Reference, ReferenceTarget, RuleResult, \
                                   load_cubes
import iris.tests.stock as stock


class Mock(object):
    def __repr__(self):
        return '<Mock {!r}>'.format(self.__dict__)


class TestConcreteReferenceTarget(tests.IrisTest):
    def test_attributes(self):
        with self.assertRaises(TypeError):
            target = ConcreteReferenceTarget()

        target = ConcreteReferenceTarget('foo')
        self.assertEqual(target.name, 'foo')
        self.assertIsNone(target.transform)

        transform = lambda _: _
        target = ConcreteReferenceTarget('foo', transform)
        self.assertEqual(target.name, 'foo')
        self.assertIs(target.transform, transform)

    def test_single_cube_no_transform(self):
        target = ConcreteReferenceTarget('foo')
        src = stock.simple_2d()
        target.add_cube(src)
        self.assertIs(target.as_cube(), src)

    def test_single_cube_with_transform(self):
        transform = lambda cube: {'long_name': 'wibble'}
        target = ConcreteReferenceTarget('foo', transform)
        src = stock.simple_2d()
        target.add_cube(src)
        dest = target.as_cube()
        self.assertEqual(dest.long_name, 'wibble')
        self.assertNotEqual(dest, src)
        dest.long_name = src.long_name
        self.assertEqual(dest, src)

    def test_multiple_cubes_no_transform(self):
        target = ConcreteReferenceTarget('foo')
        src = stock.realistic_4d()
        for i in range(src.shape[0]):
            target.add_cube(src[i])
        dest = target.as_cube()
        self.assertIsNot(dest, src)
        self.assertEqual(dest, src)

    def test_multiple_cubes_with_transform(self):
        transform = lambda cube: {'long_name': 'wibble'}
        target = ConcreteReferenceTarget('foo', transform)
        src = stock.realistic_4d()
        for i in range(src.shape[0]):
            target.add_cube(src[i])
        dest = target.as_cube()
        self.assertEqual(dest.long_name, 'wibble')
        self.assertNotEqual(dest, src)
        dest.long_name = src.long_name
        self.assertEqual(dest, src)


class TestLoadCubes(tests.IrisTest):
    def test_simple_factory(self):
        # Test the creation process for a factory definition which only
        # uses simple dict arguments.

        # The fake PPField which will be supplied to our converter.
        field = Mock()
        field.data = None
        field_generator = lambda filename: [field]
        # A fake conversion function returning:
        #   1) A parameter cube needing a simple factory construction.
        aux_factory = Mock()
        factory = Mock()
        factory.args = [{'name': 'foo'}]
        factory.factory_class = lambda *args: \
            setattr(aux_factory, 'fake_args', args) or aux_factory
        def converter(field):
            return ([factory], [], '', '', '', {}, [], [], [])
        # Finish by making a fake Loader
        fake_loader = Loader(field_generator, {}, converter, None)
        cubes = load_cubes(['fake_filename'], None, fake_loader)

        # Check the result is a generator with a single entry.
        self.assertIsInstance(cubes, types.GeneratorType)
        try:
            # Suppress the normal Cube.coord() and Cube.add_aux_factory()
            # methods.
            coord_method = Cube.coord
            add_aux_factory_method = Cube.add_aux_factory
            Cube.coord = lambda self, **args: args
            Cube.add_aux_factory = lambda self, aux_factory: \
                setattr(self, 'fake_aux_factory', aux_factory)

            cubes = list(cubes)
        finally:
            Cube.coord = coord_method
            Cube.add_aux_factory = add_aux_factory_method
        self.assertEqual(len(cubes), 1)
        # Check the "cube" has an "aux_factory" added, which itself
        # must have been created with the correct arguments.
        self.assertTrue(hasattr(cubes[0], 'fake_aux_factory'))
        self.assertIs(cubes[0].fake_aux_factory, aux_factory)
        self.assertTrue(hasattr(aux_factory, 'fake_args'))
        self.assertEqual(aux_factory.fake_args, ({'name': 'foo'},))

    def test_cross_reference(self):
        # Test the creation process for a factory definition which uses
        # a cross-reference.

        param_cube = stock.realistic_4d_no_derived()
        orog_coord = param_cube.coord('surface_altitude')
        param_cube.remove_coord(orog_coord)

        orog_cube = param_cube[0, 0, :, :]
        orog_cube.data = orog_coord.points
        orog_cube.rename('surface_altitude')
        orog_cube.units = orog_coord.units
        orog_cube.attributes = orog_coord.attributes

        # We're going to test for the presence of the hybrid height
        # stuff later, so let's make sure it's not already there!
        assert len(param_cube.aux_factories) == 0
        assert not param_cube.coords('surface_altitude')

        # The fake PPFields which will be supplied to our converter.
        press_field = Mock()
        press_field.data = param_cube.data
        orog_field = Mock()
        orog_field.data = orog_cube.data
        field_generator = lambda filename: [press_field, orog_field]
        # A fake rule set returning:
        #   1) A parameter cube needing an "orography" reference
        #   2) An "orography" cube
        def converter(field):
            if field is press_field:
                src = param_cube
                factories = [Factory(HybridHeightFactory,
                                     [Reference('orography')])]
                references = []
            else:
                src = orog_cube
                factories = []
                references = [ReferenceTarget('orography', None)]
            dim_coords_and_dims = [(coord, src.coord_dims(coord)[0])
                                   for coord in src.dim_coords]
            aux_coords_and_dims = [(coord, src.coord_dims(coord))
                                   for coord in src.aux_coords]
            return (factories, references, src.standard_name, src.long_name,
                    src.units, src.attributes, src.cell_methods,
                    dim_coords_and_dims, aux_coords_and_dims)
        # Finish by making a fake Loader
        fake_loader = Loader(field_generator, {}, converter, None)
        cubes = load_cubes(['fake_filename'], None, fake_loader)

        # Check the result is a generator containing two Cubes.
        self.assertIsInstance(cubes, types.GeneratorType)
        cubes = list(cubes)
        self.assertEqual(len(cubes), 2)
        # Check the "cube" has an "aux_factory" added, which itself
        # must have been created with the correct arguments.
        self.assertEqual(len(cubes[1].aux_factories), 1)
        self.assertEqual(len(cubes[1].coords('surface_altitude')), 1)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_std_names
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

import unittest

from iris.std_names import STD_NAMES


class TestStandardNames(unittest.TestCase):
    """
    standard_names.py is a machine generated file which contains a single dictionary
    called STD_NAMES
    """

    longMessage = True

    def test_standard_names(self):
        # Check we have a dict
        self.assertIsInstance(STD_NAMES, dict)

        keyset = set(STD_NAMES)

        # Check for some known standard names
        valid_nameset = set(["air_density", "northward_wind", "wind_speed"])
        self.assertTrue(valid_nameset.issubset(keyset), "Known standard name missing from STD_NAMES")

        # Check for some invalid standard names
        invalid_nameset = set(["invalid_air_density", "invalid_northward_wind",
                               "invalid_wind_speed",
                               "stratiform_snowfall_rate"])
        self.assertSetEqual(invalid_nameset - keyset, invalid_nameset,
                            "\nInvalid standard name(s) present in STD_NAMES")


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_trajectory
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import biggus
import numpy as np

import iris.analysis.trajectory
import iris.tests.stock

# Run tests in no graphics mode if matplotlib is not available.
if tests.MPL_AVAILABLE:
    import matplotlib.pyplot as plt


class TestSimple(tests.IrisTest):
    def test_invalid_coord(self):
        cube = iris.tests.stock.realistic_4d()
        sample_points = [('altitude', [0, 10, 50])]
        with self.assertRaises(ValueError):
            iris.analysis.trajectory.interpolate(cube, sample_points, 'nearest')


class TestTrajectory(tests.IrisTest):
    def test_trajectory_definition(self):
        # basic 2-seg line along x
        waypoints = [ {'lat':0, 'lon':0}, {'lat':0, 'lon':1}, {'lat':0, 'lon':2} ]
        trajectory = iris.analysis.trajectory.Trajectory(waypoints, sample_count=21)

        self.assertEqual(trajectory.length, 2.0)
        self.assertEqual(trajectory.sampled_points[19], {'lat': 0.0, 'lon': 1.9000000000000001})

        # 4-seg m-shape
        waypoints = [ {'lat':0, 'lon':0}, {'lat':1, 'lon':1}, {'lat':0, 'lon':2}, {'lat':1, 'lon':3}, {'lat':0, 'lon':4} ]
        trajectory = iris.analysis.trajectory.Trajectory(waypoints, sample_count=33)

        self.assertEqual(trajectory.length, 5.6568542494923806)
        self.assertEqual(trajectory.sampled_points[31], {'lat': 0.12499999999999989, 'lon': 3.875})

    @tests.skip_data
    @tests.skip_plot
    def test_trajectory_extraction(self):

        # Load the COLPEX data => TZYX
        path = tests.get_data_path(['PP', 'COLPEX', 'theta_and_orog_subset.pp'])
        cube = iris.load_cube(path, 'air_potential_temperature')
        cube.coord('grid_latitude').bounds = None
        cube.coord('grid_longitude').bounds = None
        # TODO: Workaround until regrid can handle factories
        cube.remove_aux_factory(cube.aux_factories[0])
        cube.remove_coord('surface_altitude')
        self.assertCML(cube, ('trajectory', 'big_cube.cml'))

        # Pull out a single point - no interpolation required
        single_point = iris.analysis.trajectory.interpolate(
            cube, [('grid_latitude', [-0.1188]),
                   ('grid_longitude', [359.57958984])])
        expected = cube[..., 10, 0].data

        self.assertArrayAllClose(single_point[..., 0].data, expected, rtol=2.0e-7)
        self.assertCML(single_point, ('trajectory', 'single_point.cml'),
                       checksum=False)

        # Pull out another point and test against a manually calculated result.
        single_point = [['grid_latitude', [-0.1188]], ['grid_longitude', [359.584090412]]]
        scube = cube[0, 0, 10:11, 4:6]
        x0 = scube.coord('grid_longitude')[0].points
        x1 = scube.coord('grid_longitude')[1].points
        y0 = scube.data[0, 0]
        y1 = scube.data[0, 1]
        expected = y0 + ((y1 - y0) * ((359.584090412 - x0)/(x1 - x0)))
        trajectory_cube = iris.analysis.trajectory.interpolate(scube,
                                                               single_point) 
        self.assertArrayAllClose(trajectory_cube.data, expected, rtol=2.0e-7)

        # Extract a simple, axis-aligned trajectory that is similar to an indexing operation.
        # (It's not exactly the same because the source cube doesn't have regular spacing.)
        waypoints = [
            {'grid_latitude': -0.1188, 'grid_longitude': 359.57958984},
            {'grid_latitude': -0.1188, 'grid_longitude': 359.66870117}
        ]
        trajectory = iris.analysis.trajectory.Trajectory(waypoints, sample_count=100)
        def traj_to_sample_points(trajectory):
            sample_points = []
            src_points = trajectory.sampled_points
            for name in src_points[0].iterkeys():
                values = [point[name] for point in src_points]
                sample_points.append((name, values))
            return sample_points
        sample_points = traj_to_sample_points(trajectory)
        trajectory_cube = iris.analysis.trajectory.interpolate(cube,
                                                               sample_points)
        self.assertCML(trajectory_cube, ('trajectory',
                                         'constant_latitude.cml'))

        # Sanity check the results against a simple slice
        plt.plot(cube[0, 0, 10, :].data)
        plt.plot(trajectory_cube[0, 0, :].data)
        self.check_graphic()

        # Extract a zig-zag trajectory
        waypoints = [
            {'grid_latitude': -0.1188, 'grid_longitude': 359.5886},
            {'grid_latitude': -0.0828, 'grid_longitude': 359.6606},
            {'grid_latitude': -0.0468, 'grid_longitude': 359.6246},
        ]
        trajectory = iris.analysis.trajectory.Trajectory(waypoints, sample_count=20)
        sample_points = traj_to_sample_points(trajectory)
        trajectory_cube = iris.analysis.trajectory.interpolate(
            cube[0, 0], sample_points)
        expected = np.array([287.95953369, 287.9190979, 287.95550537,
                             287.93240356, 287.83850098, 287.87869263,
                             287.90942383, 287.9463501, 287.74365234,
                             287.68856812, 287.75588989, 287.54611206,
                             287.48522949, 287.53356934, 287.60217285,
                             287.43795776, 287.59701538, 287.52468872,
                             287.45025635, 287.52716064], dtype=np.float32)

        self.assertCML(trajectory_cube, ('trajectory', 'zigzag.cml'), checksum=False)
        self.assertArrayAllClose(trajectory_cube.data, expected, rtol=2.0e-7)

        # Sanity check the results against a simple slice
        x = cube.coord('grid_longitude').points
        y = cube.coord('grid_latitude').points
        plt.pcolormesh(x, y, cube[0, 0, :, :].data)
        x = trajectory_cube.coord('grid_longitude').points
        y = trajectory_cube.coord('grid_latitude').points
        plt.scatter(x, y, c=trajectory_cube.data)
        self.check_graphic()

    @tests.skip_data
    @tests.skip_plot
    def test_tri_polar(self):
        # load data
        cubes = iris.load(tests.get_data_path(['NetCDF', 'ORCA2', 'votemper.nc']))
        cube = cubes[0]
        # The netCDF file has different data types for the points and
        # bounds of 'depth'. This wasn't previously supported, so we
        # emulate that old behaviour.
        cube.coord('depth').bounds = cube.coord('depth').bounds.astype(np.float32)

        # define a latitude trajectory (put coords in a different order to the cube, just to be awkward)
        latitudes = range(-90, 90, 2)
        longitudes = [-90]*len(latitudes)
        sample_points = [('longitude', longitudes), ('latitude', latitudes)]

        # extract
        sampled_cube = iris.analysis.trajectory.interpolate(cube, sample_points)
        self.assertCML(sampled_cube, ('trajectory', 'tri_polar_latitude_slice.cml'))

        # turn it upside down for the visualisation
        plot_cube = sampled_cube[0]
        plot_cube = plot_cube[::-1, :]

        plt.clf()
        plt.pcolormesh(plot_cube.data, vmin=cube.data.min(), vmax=cube.data.max())
        plt.colorbar()
        self.check_graphic()

        # Try to request linear interpolation.
        # Not allowed, as we have multi-dimensional coords.
        self.assertRaises(iris.exceptions.CoordinateMultiDimError, iris.analysis.trajectory.interpolate, cube, sample_points, method="linear")

        # Try to request unknown interpolation.
        self.assertRaises(ValueError, iris.analysis.trajectory.interpolate, cube, sample_points, method="linekar")

    def test_hybrid_height(self):
        cube = tests.stock.simple_4d_with_hybrid_height()
        # Put a biggus array on the cube so we can test deferred loading.
        cube.lazy_data(biggus.NumpyArrayAdapter(cube.data))

        traj = (('grid_latitude',[20.5, 21.5, 22.5, 23.5]),
                ('grid_longitude',[31, 32, 33, 34]))
        xsec = iris.analysis.trajectory.interpolate(cube, traj, method='nearest')

        # Check that creating the trajectory hasn't led to the original
        # data being loaded.
        self.assertTrue(cube.has_lazy_data())
        self.assertCML([cube, xsec], ('trajectory', 'hybrid_height.cml'))


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_unit
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test Unit the wrapper class for Unidata udunits2.

"""
# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import copy
import datetime as datetime
import operator

import numpy as np

import iris.unit as unit


Unit = unit.Unit


class TestUnit(tests.IrisTest):
    def setUp(self):
        unit._handler(unit._ut_ignore)

    def tearDown(self):
        unit._handler(unit._default_handler)


class TestCreation(TestUnit):
    #
    # test: unit creation
    #
    def test_unit_fail_0(self):
        self.assertRaises(ValueError, Unit, 'wibble')

    def test_unit_pass_0(self):
        u = Unit('    meter')
        self.assertTrue(u.name, 'meter')

    def test_unit_pass_1(self):
        u = Unit('meter   ')
        self.assertTrue(u.name, 'meter')

    def test_unit_pass_2(self):
        u = Unit('   meter   ')
        self.assertTrue(u.name, 'meter')

    def test_calendar(self):
        calendar = unit.CALENDAR_365_DAY
        u = Unit('hours since 1970-01-01 00:00:00', calendar=calendar)
        self.assertEqual(u.calendar, calendar)

    def test_no_calendar(self):
        u = Unit('hours since 1970-01-01 00:00:00')
        self.assertEqual(u.calendar, unit.CALENDAR_GREGORIAN)

    def test_unknown_calendar(self):
        with self.assertRaises(ValueError):
            u = Unit('hours since 1970-01-01 00:00:00', calendar='wibble')


class TestModulus(TestUnit):
    #
    # test: modulus property
    #
    def test_modulus_pass_0(self):
        u = Unit("degrees")
        self.assertEqual(u.modulus, 360.0)

    def test_modulus_pass_1(self):
        u = Unit("radians")
        self.assertEqual(u.modulus, np.pi*2)

    def test_modulus_pass_2(self):
        u = Unit("meter")
        self.assertEqual(u.modulus, None)


class TestConvertible(TestUnit):
    #
    # test: convertible method
    #
    def test_convertible_fail_0(self):
        u = Unit("meter")
        v = Unit("newton")
        self.assertFalse(u.is_convertible(v))

    def test_convertible_pass_0(self):
        u = Unit("meter")
        v = Unit("mile")
        self.assertTrue(u.is_convertible(v))

    def test_convertible_fail_1(self):
        u = Unit('meter')
        v = Unit('unknown')
        self.assertFalse(u.is_convertible(v))
        self.assertFalse(v.is_convertible(u))

    def test_convertible_fail_2(self):
        u = Unit('meter')
        v = Unit('no unit')
        self.assertFalse(u.is_convertible(v))
        self.assertFalse(v.is_convertible(u))

    def test_convertible_fail_3(self):
        u = Unit('unknown')
        v = Unit('no unit')
        self.assertFalse(u.is_convertible(v))
        self.assertFalse(v.is_convertible(u))


class TestDimensionless(TestUnit):
    #
    # test: dimensionless property
    #
    def test_dimensionless_fail_0(self):
        u = Unit("meter")
        self.assertFalse(u.is_dimensionless())

    def test_dimensionless_pass_0(self):
        u = Unit("1")
        self.assertTrue(u.is_dimensionless())

    def test_dimensionless_fail_1(self):
        u = Unit('unknown')
        self.assertFalse(u.is_dimensionless())

    def test_dimensionless_fail_2(self):
        u = Unit('no unit')
        self.assertFalse(u.is_dimensionless())


class TestFormat(TestUnit):
    #
    # test: format method
    #
    def test_format_pass_0(self):
        u = Unit("watt")
        self.assertEqual(u.format(), "W")

    def test_format_pass_1(self):
        u = Unit("watt")
        self.assertEqual(u.format(unit.UT_ASCII), "W")

    def test_format_pass_2(self):
        u = Unit("watt")
        self.assertEqual(u.format(unit.UT_NAMES), "watt")

    def test_format_pass_3(self):
        u = Unit("watt")
        self.assertEqual(u.format(unit.UT_DEFINITION), "m2.kg.s-3")

    def test_format_pass_4(self):
        u = Unit('?')
        self.assertEqual(u.format(), 'unknown')

    def test_format_pass_5(self):
        u = Unit('nounit')
        self.assertEqual(u.format(), 'no_unit')


class TestName(TestUnit):
    #
    # test: name property
    #
    def test_name_pass_0(self):
        u = Unit("newton")
        self.assertEqual(u.name, 'newton')

    def test_name_pass_1(self):
        u = Unit('unknown')
        self.assertEqual(u.name, 'unknown')

    def test_name_pass_2(self):
        u = Unit('no unit')
        self.assertEqual(u.name, 'no_unit')


class TestSymbol(TestUnit):
    #
    # test: symbol property
    #
    def test_symbol_pass_0(self):
        u = Unit("joule")
        self.assertEqual(u.symbol, 'J')

    def test_symbol_pass_1(self):
        u = Unit('unknown')
        self.assertEqual(u.symbol, unit._UNKNOWN_UNIT_SYMBOL)

    def test_symbol_pass_2(self):
        u = Unit('no unit')
        self.assertEqual(u.symbol, unit._NO_UNIT_SYMBOL)


class TestDefinition(TestUnit):
    #
    # test: definition property
    #
    def test_definition_pass_0(self):
        u = Unit("joule")
        self.assertEqual(u.definition, 'm2.kg.s-2')

    def test_definition_pass_1(self):
        u = Unit('unknown')
        self.assertEqual(u.definition, unit._UNKNOWN_UNIT_SYMBOL)

    def test_definition_pass_2(self):
        u = Unit('no unit')
        self.assertEqual(u.definition, unit._NO_UNIT_SYMBOL)


class TestOffset(TestUnit):
    #
    # test: offset method
    #
    def test_offset_fail_0(self):
        u = Unit("meter")
        self.assertRaises(TypeError, operator.add, u, "naughty")

    def test_offset_fail_1(self):
        u = Unit('unknown')
        self.assertEqual(u + 10, 'unknown')

    def test_offset_fail_2(self):
        u = Unit('no unit')
        self.assertRaises(ValueError, operator.add, u, 10)

    def test_offset_pass_0(self):
        u = Unit("meter")
        self.assertEqual(u + 10, "m @ 10")

    def test_offset_pass_1(self):
        u = Unit("meter")
        self.assertEqual(u + 100.0, "m @ 100")

    def test_offset_pass_2(self):
        u = Unit("meter")
        self.assertEqual(u + 1000L, "m @ 1000")


class TestOffsetByTime(TestUnit):
    #
    # test: offset_by_time method
    #
    def test_offset_by_time_fail_0(self):
        u = Unit("hour")
        self.assertRaises(TypeError, u.offset_by_time, "naughty")

    def test_offset_by_time_fail_1(self):
        u = Unit("mile")
        self.assertRaises(ValueError, u.offset_by_time, 10)

    def test_offset_by_time_fail_2(self):
        u = Unit('unknown')
        self.assertRaises(ValueError, u.offset_by_time, unit.encode_time(1970, 1, 1, 0, 0, 0))

    def test_offset_by_time_fail_3(self):
        u = Unit('no unit')
        self.assertRaises(ValueError, u.offset_by_time, unit.encode_time(1970, 1, 1, 0, 0, 0))

    def test_offset_by_time_pass_0(self):
        u = Unit("hour")
        v = u.offset_by_time(unit.encode_time(2007, 1, 15, 12, 6, 0))
        self.assertEqual(v, "(3600 s) @ 20070115T120600.00000000 UTC")


class TestInvert(TestUnit):
    #
    # test: invert method
    #
    def test_invert_fail_0(self):
        u = Unit('unknown')
        self.assertEqual(u.invert(), u)

    def test_invert_fail_1(self):
        u = Unit('no unit')
        self.assertRaises(ValueError, u.invert)

    def test_invert_pass_0(self):
        u = Unit("newton")
        self.assertEqual(u.invert(), "m-1.kg-1.s2")
        self.assertEqual(u.invert().invert(), "N")


class TestRoot(TestUnit):
    #
    # test: root method
    #
    def test_root_fail_0(self):
        u = Unit("volt")
        self.assertRaises(TypeError, u.root, "naughty")

    def test_root_fail_1(self):
        u = Unit("volt")
        self.assertRaises(TypeError, u.root, 1.2)

    def test_root_fail_2(self):
        u = Unit("volt")
        self.assertRaises(ValueError, u.root, 2)

    def test_root_fail_3(self):
        u = Unit('unknown')
        self.assertEqual(u.root(2), u)

    def test_root_fail_4(self):
        u = Unit('no unit')
        self.assertRaises(ValueError, u.root, 2)

    def test_root_pass_0(self):
        u = Unit("volt^2")
        self.assertEqual(u.root(2), "V")


class TestLog(TestUnit):
    #
    # test: log method
    #
    def test_log_fail_0(self):
        u = Unit("hPa")
        self.assertRaises(TypeError, u.log, "naughty")

    def test_log_fail_1(self):
        u = Unit('unknown')
        self.assertEqual(u.log(10), u)

    def test_log_fail_2(self):
        u = Unit('no unit')
        self.assertRaises(ValueError, u.log, 10)

    def test_log_pass_0(self):
        u = Unit("hPa")
        self.assertEqual(u.log(10), "lg(re 100 Pa)")


class TestMultiply(TestUnit):
    def test_multiply_fail_0(self):
        u = Unit("amp")
        self.assertRaises(ValueError, operator.mul, u, "naughty")

    def test_multiply_fail_1(self):
        u = Unit('unknown')
        v = Unit('meters')
        self.assertTrue((u * v).is_unknown())
        self.assertTrue((v * u).is_unknown())

    def test_multiply_fail_3(self):
        u = Unit('unknown')
        v = Unit('no unit')
        self.assertRaises(ValueError, operator.mul, u, v)
        self.assertRaises(ValueError, operator.mul, v, u)

    def test_multiply_fail_5(self):
        u = Unit('meters')
        v = Unit('no unit')
        self.assertRaises(ValueError, operator.mul, u, v)
        self.assertRaises(ValueError, operator.mul, v, u)

    def test_multiply_pass_0(self):
        u = Unit("amp")
        self.assertEqual((u * 10).format(), "10 A")

    def test_multiply_pass_1(self):
        u = Unit("amp")
        self.assertEqual((u * 100.0).format(), "100 A")

    def test_multiply_pass_2(self):
        u = Unit("amp")
        self.assertEqual((u * 1000L).format(), "1000 A")

    def test_multiply_pass_3(self):
        u = Unit("amp")
        v = Unit("volt")
        self.assertEqual((u * v).format(), "W")


class TestDivide(TestUnit):
    def test_divide_fail_0(self):
        u = Unit("watts")
        self.assertRaises(ValueError, operator.div, u, "naughty")

    def test_divide_fail_1(self):
        u = Unit('unknown')
        v = Unit('meters')
        self.assertTrue((u / v).is_unknown())
        self.assertTrue((v / u).is_unknown())

    def test_divide_fail_3(self):
        u = Unit('unknown')
        v = Unit('no unit')
        self.assertRaises(ValueError, operator.div, u, v)
        self.assertRaises(ValueError, operator.div, v, u)

    def test_divide_fail_5(self):
        u = Unit('meters')
        v = Unit('no unit')
        self.assertRaises(ValueError, operator.div, u, v)
        self.assertRaises(ValueError, operator.div, v, u)

    def test_divide_pass_0(self):
        u = Unit("watts")
        self.assertEqual((u / 10).format(), "0.1 W")

    def test_divide_pass_1(self):
        u = Unit("watts")
        self.assertEqual((u / 100.0).format(), "0.01 W")

    def test_divide_pass_2(self):
        u = Unit("watts")
        self.assertEqual((u / 1000L).format(), "0.001 W")

    def test_divide_pass_3(self):
        u = Unit("watts")
        v = Unit("volts")
        self.assertEqual((u / v).format(), "A")


class TestPower(TestUnit):
    def test_power(self):
        u = Unit("amp")
        self.assertRaises(TypeError, operator.pow, u, "naughty")
        self.assertRaises(TypeError, operator.pow, u, Unit('m'))
        self.assertRaises(TypeError, operator.pow, u, Unit('unknown'))
        self.assertRaises(TypeError, operator.pow, u, Unit('no unit'))
        self.assertEqual(u ** 2, Unit('A^2'))
        self.assertEqual(u ** 3.0, Unit('A^3'))
        self.assertEqual(u ** 4L, Unit('A^4'))
        self.assertRaises(ValueError, operator.pow, u, 2.4)

        u = Unit("m^2")
        self.assertEqual(u ** 0.5, Unit('m'))
        self.assertRaises(ValueError, operator.pow, u, 0.4)

    def test_power_unknown(self):
        u = Unit('unknown')
        self.assertRaises(TypeError, operator.pow, u, "naughty")
        self.assertRaises(TypeError, operator.pow, u, Unit('m'))
        self.assertEqual(u ** 2, Unit('unknown'))
        self.assertEqual(u ** 3.0, Unit('unknown'))
        self.assertEqual(u ** 4L, Unit('unknown'))

    def test_power_nounit(self):
        u = Unit('no unit')
        self.assertRaises(TypeError, operator.pow, u, "naughty")
        self.assertRaises(TypeError, operator.pow, u, Unit('m'))
        self.assertRaises(ValueError, operator.pow, u, 2)


class TestCopy(TestUnit):
    #
    # test: copy method
    #
    def test_copy_pass_0(self):
        u = Unit("joule")
        self.assertEqual(copy.copy(u) == u, True)

    def test_copy_pass_1(self):
        u = Unit('unknown')
        self.assertTrue(copy.copy(u).is_unknown())

    def test_copy_pass_2(self):
        u = Unit('no unit')
        self.assertTrue(copy.copy(u).is_no_unit())


class TestStringify(TestUnit):
    #
    # test: __str__ method
    #
    def test_str_pass_0(self):
        u = Unit("meter")
        self.assertEqual(str(u), "meter")

    #
    # test: __repr__ method
    #
    def test_repr_pass_0(self):
        u = Unit("meter")
        self.assertEqual(repr(u), "Unit('meter')")

    def test_repr_pass_1(self):
        u = Unit("hours since 2007-01-15 12:06:00", calendar=unit.CALENDAR_STANDARD)
        #self.assertEqual(repr(u), "Unit('hour since 2007-01-15 12:06:00.00000000 UTC', calendar='standard')")
        self.assertEqual(repr(u), "Unit('hours since 2007-01-15 12:06:00', calendar='standard')")


class TestRichComparison(TestUnit):
    #
    # test: __eq__ method
    #
    def test_eq_pass_0(self):
        u = Unit("meter")
        v = Unit("amp")
        self.assertEqual(u == v, False)

    def test_eq_pass_1(self):
        u = Unit("meter")
        v = Unit("m.s-1")
        w = Unit("hertz")
        self.assertEqual(u == (v / w), True)

    def test_eq_pass_2(self):
        u = Unit("meter")
        self.assertEqual(u == "meter", True)

    def test_eq_cross_category(self):
        m = Unit("meter")
        u = Unit('unknown')
        n = Unit('no_unit')
        self.assertFalse(m == u)
        self.assertFalse(m == n)
        self.assertFalse(u == n)

    #
    # test: __ne__ method
    #
    def test_neq_pass_0(self):
        u = Unit("meter")
        v = Unit("amp")
        self.assertEqual(u != v, True)

    def test_neq_pass_1(self):
        u = Unit("meter")
        self.assertEqual(u != 'meter', False)

    def test_ne_cross_category(self):
        m = Unit("meter")
        u = Unit('unknown')
        n = Unit('no_unit')
        self.assertTrue(m != u)
        self.assertTrue(m != n)
        self.assertTrue(u != n)


class TestOrdering(TestUnit):
    def test_order(self):
        m = Unit("meter")
        u = Unit('unknown')
        n = Unit('no_unit')
        start = [m, u, n]
        self.assertEqual(sorted(start), [m, n, u])


class TestTimeEncoding(TestUnit):
    #
    # test: encode_time module function
    #
    def test_encode_time_pass_0(self):
        result = unit.encode_time(2006, 1, 15, 12, 6, 0)
        self.assertEqual(result, 159019560.0)

    #
    # test: encode_date module function
    #
    def test_encode_date_pass_0(self):
        result = unit.encode_date(2006, 1, 15)
        self.assertEqual(result, 158976000.0)

    #
    # test: encode_clock module function
    #
    def test_encode_clock_pass_0(self):
        result = unit.encode_clock(12, 6, 0)
        self.assertEqual(result, 43560.0)

    #
    # test: decode_time module function
    #
    def test_decode_time_pass_0(self):
        (year, month, day, hour, min, sec, res) = unit.decode_time(158976000.0+43560.0)
        self.assertEqual((year, month, day, hour, min, sec), (2006, 1, 15, 12, 6, 0))


class TestConvert(TestUnit):
    #
    # test: convert method
    #
    def test_convert_float_pass_0(self):
        u = Unit("meter")
        v = Unit("mile")
        self.assertEqual(u.convert(1609.344, v), 1.0)

    def test_convert_float_pass_1(self):
        u = Unit("meter")
        v = Unit("mile")
        a = (np.arange(2, dtype=np.float32) + 1) * 1609.344
        res = u.convert(a, v)
        e = np.arange(2, dtype=np.float32) + 1
        self.assertEqual(res[0], e[0])
        self.assertEqual(res[1], e[1])

    def test_convert_np_float(self):
        u = Unit("mile")
        v = Unit("meter")
        self.assertEqual(u.convert(np.float(1.0), v), 1609.344)
        self.assertEqual(u.convert(np.float16(1.0), v), 1609.344)
        self.assertEqual(u.convert(np.float32(1.0), v), 1609.344)
        self.assertEqual(u.convert(np.float64(1.0), v), 1609.344)

    def test_convert_double_pass_0(self):
        u = Unit("meter")
        v = Unit("mile")
        self.assertEqual(u.convert(1609.344, v, unit.FLOAT64), 1.0)

    def test_convert_double_pass_1(self):
        u = Unit("meter")
        v = Unit("mile")
        a = (np.arange(2, dtype=np.float64) + 1) * 1609.344
        res = u.convert(a, v, unit.FLOAT64)
        e = np.arange(2, dtype=np.float64) + 1
        self.assertEqual(res[0], e[0])
        self.assertEqual(res[1], e[1])

    def test_convert_int(self):
        u = Unit("mile")
        v = Unit("meter")
        self.assertEqual(u.convert(1, v), 1609.344)

    def test_convert_int_array(self):
        u = Unit("mile")
        v = Unit("meter")
        a = np.arange(2, dtype=np.int) + 1
        res = u.convert(a, v)
        e = (np.arange(2, dtype=np.float64) + 1) * 1609.344
        self.assertArrayAlmostEqual(res, e)

    def test_convert_int_array_ctypearg(self):
        u = Unit("mile")
        v = Unit("meter")
        a = np.arange(2, dtype=np.int) + 1

        res = u.convert(a, v, unit.FLOAT32)
        e = (np.arange(2, dtype=np.float32) + 1) * 1609.344
        self.assertEqual(res.dtype, e.dtype)
        self.assertArrayAlmostEqual(res, e)

        res = u.convert(a, v, unit.FLOAT64)
        e = (np.arange(2, dtype=np.float64) + 1) * 1609.344
        self.assertEqual(res.dtype, e.dtype)
        self.assertArrayAlmostEqual(res, e)

    def test_convert_np_int(self):
        u = Unit("mile")
        v = Unit("meter")
        self.assertEqual(u.convert(np.int(1), v), 1609.344)
        self.assertEqual(u.convert(np.int8(1), v), 1609.344)
        self.assertEqual(u.convert(np.int16(1), v), 1609.344)
        self.assertEqual(u.convert(np.int32(1), v), 1609.344)
        self.assertEqual(u.convert(np.int64(1), v), 1609.344)

    def test_convert_fail_0(self):
        u = Unit('unknown')
        v = Unit('no unit')
        w = Unit('meters')
        x = Unit('kg')
        a = np.arange(10)

        # unknown and/or no-unit
        self.assertRaises(ValueError, u.convert, a, v)
        self.assertRaises(ValueError, v.convert, a, u)
        self.assertRaises(ValueError, w.convert, a, u)
        self.assertRaises(ValueError, w.convert, a, v)
        self.assertRaises(ValueError, u.convert, a, w)
        self.assertRaises(ValueError, v.convert, a, w)

        # Incompatible units
        self.assertRaises(ValueError, w.convert, a, x)

    def test_convert_time_ref_0(self):
        # Test converting from one reference time to another on non-standard
        # calendar.
        u1 = Unit('seconds since 1978-09-01 00:00:00', calendar='360_day')
        u2 = Unit('seconds since 1979-04-01 00:00:00', calendar='360_day')
        u1point = np.array([ 54432000.], dtype=np.float32)
        u2point = np.array([ 36288000.], dtype=np.float32)
        res = u1.convert(u1point, u2)
        self.assertArrayAlmostEqual(res, u2point)

    def test_convert_time_fail_0(self):
        # Test converting from one reference time to another between different
        # calendars raises an error.
        u1 = Unit('seconds since 1978-09-01 00:00:00', calendar='360_day')
        u2 = Unit('seconds since 1979-04-01 00:00:00', calendar='gregorian')
        u1point = np.array([ 54432000.], dtype=np.float32)
        with self.assertRaises(ValueError):
            u1.convert(u1point, u2)


class TestNumsAndDates(TestUnit):
    #
    # test: num2date method
    #
    def test_num2date_pass_0(self):
        u = Unit("hours since 2010-11-02 12:00:00", calendar=unit.CALENDAR_STANDARD)
        self.assertEqual(str(u.num2date(1)), "2010-11-02 13:00:00")

    #
    # test: date2num method
    #
    def test_date2num_pass_0(self):
        u = Unit("hours since 2010-11-02 12:00:00", calendar=unit.CALENDAR_STANDARD)
        d = datetime.datetime(2010, 11, 2, 13, 0, 0)
        self.assertEqual(str(u.num2date(u.date2num(d))), "2010-11-02 13:00:00")


class TestUnknown(TestUnit):
    #
    # test: unknown units
    #
    def test_unknown_unit_pass_0(self):
        u = Unit("?")
        self.assertTrue(u.is_unknown())

    def test_unknown_unit_pass_1(self):
        u = Unit("???")
        self.assertTrue(u.is_unknown())

    def test_unknown_unit_pass_2(self):
        u = Unit("unknown")
        self.assertTrue(u.is_unknown())

    def test_unknown_unit_fail_0(self):
        u = Unit('no unit')
        self.assertFalse(u.is_unknown())

    def test_unknown_unit_fail_2(self):
        u = Unit('meters')
        self.assertFalse(u.is_unknown())


class TestNoUnit(TestUnit):
    #
    # test: no unit
    #
    def test_no_unit_pass_0(self):
        u = Unit('no_unit')
        self.assertTrue(u.is_no_unit())

    def test_no_unit_pass_1(self):
        u = Unit('no unit')
        self.assertTrue(u.is_no_unit())

    def test_no_unit_pass_2(self):
        u = Unit('no-unit')
        self.assertTrue(u.is_no_unit())

    def test_no_unit_pass_3(self):
        u = Unit('nounit')
        self.assertTrue(u.is_no_unit())


class TestTimeReference(TestUnit):
    #
    # test: time reference
    #
    def test_time_reference_pass_0(self):
        u = Unit('hours since epoch')
        self.assertTrue(u.is_time_reference())

    def test_time_reference_fail_0(self):
        u = Unit('hours')
        self.assertFalse(u.is_time_reference())


class TestTitle(TestUnit):
    #
    # test: title
    #
    def test_title_pass_0(self):
        u = Unit('meter')
        self.assertEqual(u.title(10), '10 meter')

    def test_title_pass_1(self):
        u = Unit('hours since epoch', calendar=unit.CALENDAR_STANDARD)
        self.assertEqual(u.title(10), '1970-01-01 10:00:00')


class TestImmutable(TestUnit):
    def _set_attr(self, unit, name):
        setattr(unit, name, -999)
        raise ValueError("'Unit' attribute '%s' is mutable!" % name)

    def test_immutable(self):
        u = Unit('m')
        for name in dir(u):
            self.assertRaises(AttributeError, self._set_attr, u, name)

    def test_hash(self):
        u1 = Unit('m')
        u2 = Unit('meter')
        u3 = copy.deepcopy(u1)
        h = set()
        for u in (u1, u2, u3):
            h.add(hash(u))
        self.assertEqual(len(h), 1)

        v1 = Unit('V')
        v2 = Unit('volt')
        for u in (v1, v2):
            h.add(hash(u))
        self.assertEqual(len(h), 2)


class TestInPlace(TestUnit):

    def test1(self):
        # Check conversions do not change original object
        c = unit.Unit('deg_c')
        f = unit.Unit('deg_f')

        orig = np.arange(3, dtype=np.float32)
        converted = c.convert(orig, f)

        with self.assertRaises(AssertionError):
            np.testing.assert_array_equal(orig, converted)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_uri_callback
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import os

import iris.coords


@tests.skip_data
class TestCallbacks(tests.IrisTest):
    def test_grib_callback(self):
        def grib_thing_getter(cube, field, filename):
            cube.add_aux_coord(iris.coords.AuxCoord(field.extra_keys['_periodStartDateTime'], long_name='random element', units='no_unit'))

        iris.fileformats.grib.hindcast_workaround = True
        fname = tests.get_data_path(('GRIB', 'global_t', 'global.grib2'))
        cube = iris.load_cube(fname, callback=grib_thing_getter)
        try:
            self.assertCML(cube, ['uri_callback', 'grib_global.cml'])
        finally:
            iris.fileformats.grib.hindcast_workaround = False

    def test_pp_callback(self):
        def pp_callback(cube, field, filename):
            cube.attributes['filename'] = os.path.basename(filename)
            cube.attributes['lbyr'] = field.lbyr
        fname = tests.get_data_path(('PP', 'aPPglob1', 'global.pp'))
        cube = iris.load_cube(fname, callback=pp_callback)
        self.assertCML(cube, ['uri_callback', 'pp_global.cml'])


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_util
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test iris.util

"""
# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import inspect
import os
import StringIO
import unittest

import numpy as np

import iris.analysis
import iris.coords
import iris.tests.stock as stock
import iris.util


class TestMonotonic(unittest.TestCase):
    def assertMonotonic(self, array, direction=None, **kwargs):
        if direction is not None:
            mono, dir = iris.util.monotonic(array, return_direction=True, **kwargs)
            if not mono:
                self.fail('Array was not monotonic:/n %r' % array)
            if dir != np.sign(direction):
                self.fail('Array was monotonic but not in the direction expected:'
                          '/n  + requested direction: %s/n  + resultant direction: %s' % (direction, dir))
        else:
            mono = iris.util.monotonic(array, **kwargs)
            if not mono:
                self.fail('Array was not monotonic:/n %r' % array)

    def assertNotMonotonic(self, array, **kwargs):
        mono = iris.util.monotonic(array, **kwargs)
        if mono:
            self.fail("Array was monotonic when it shouldn't be:/n %r" % array)

    def test_monotonic_pve(self):
        a = np.array([3, 4, 5.3])
        self.assertMonotonic(a)
        self.assertMonotonic(a, direction=1)

        # test the reverse for negative monotonic.
        a = a[::-1]
        self.assertMonotonic(a)
        self.assertMonotonic(a, direction=-1)

    def test_not_monotonic(self):
        b = np.array([3, 5.3, 4])
        self.assertNotMonotonic(b)

    def test_monotonic_strict(self):
        b = np.array([3, 5.3, 4])
        self.assertNotMonotonic(b, strict=True)
        self.assertNotMonotonic(b)

        b = np.array([3, 5.3, 5.3])
        self.assertNotMonotonic(b, strict=True)
        self.assertMonotonic(b, direction=1)
        
        b = b[::-1]
        self.assertNotMonotonic(b, strict=True)
        self.assertMonotonic(b, direction=-1)

        b = np.array([0.0])
        self.assertRaises(ValueError, iris.util.monotonic, b)
        self.assertRaises(ValueError, iris.util.monotonic, b, strict=True)

        b = np.array([0.0, 0.0])
        self.assertNotMonotonic(b, strict=True)
        self.assertMonotonic(b)


class TestReverse(unittest.TestCase):
    def test_simple(self):
        a = np.arange(12).reshape(3, 4)
        np.testing.assert_array_equal(a[::-1], iris.util.reverse(a, 0))
        np.testing.assert_array_equal(a[::-1, ::-1], iris.util.reverse(a, [0, 1]))
        np.testing.assert_array_equal(a[:, ::-1], iris.util.reverse(a, 1))
        np.testing.assert_array_equal(a[:, ::-1], iris.util.reverse(a, [1]))
        self.assertRaises(ValueError, iris.util.reverse, a, [])
        self.assertRaises(ValueError, iris.util.reverse, a, -1)
        self.assertRaises(ValueError, iris.util.reverse, a, 10)
        self.assertRaises(ValueError, iris.util.reverse, a, [-1])
        self.assertRaises(ValueError, iris.util.reverse, a, [0, -1])

    def test_single(self):
        a = np.arange(36).reshape(3, 4, 3)
        np.testing.assert_array_equal(a[::-1], iris.util.reverse(a, 0))
        np.testing.assert_array_equal(a[::-1, ::-1], iris.util.reverse(a, [0, 1]))
        np.testing.assert_array_equal(a[:, ::-1, ::-1], iris.util.reverse(a, [1, 2]))
        np.testing.assert_array_equal(a[..., ::-1], iris.util.reverse(a, 2))
        self.assertRaises(ValueError, iris.util.reverse, a, -1)
        self.assertRaises(ValueError, iris.util.reverse, a, 10)
        self.assertRaises(ValueError, iris.util.reverse, a, [-1])
        self.assertRaises(ValueError, iris.util.reverse, a, [0, -1])


class TestClipString(unittest.TestCase):
    def setUp(self):
        self.test_string = "Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."
        self.rider = "**^^**$$..--__" # A good chance at being unique and not in the string to be tested!

    def test_oversize_string(self):
        # Test with a clip length that means the string will be clipped

        clip_length = 109
        result = iris.util.clip_string(self.test_string, clip_length, self.rider)

        # Check the length is between what we requested ( + rider length) and the length of the original string
        self.assertTrue(clip_length + len(self.rider) <= len(result) < len(self.test_string), "String was not clipped.")

        # Also test the rider was added
        self.assertTrue(self.rider in result, "Rider was not added to the string when it should have been.")

    def test_undersize_string(self):
        # Test with a clip length that is longer than the string

        clip_length = 10999
        result = iris.util.clip_string(self.test_string, clip_length, self.rider)
        self.assertEqual(len(result), len(self.test_string), "String was clipped when it should not have been.")

        # Also test that no rider was added on the end if the string was not clipped
        self.assertFalse(self.rider in result, "Rider was adding to the string when it should not have been.")

    def test_invalid_clip_lengths(self):
        # Clip values less than or equal to zero are not valid
        for clip_length in [0, -100]:
            result = iris.util.clip_string(self.test_string, clip_length, self.rider)
            self.assertEqual(len(result), len(self.test_string), "String was clipped when it should not have been.")

    def test_default_values(self):
        # Get the default values specified in the function
        argspec = inspect.getargspec(iris.util.clip_string)
        arg_dict = dict(zip(argspec.args[-2:], argspec.defaults))

        result = iris.util.clip_string(self.test_string, arg_dict["clip_length"], arg_dict["rider"])

        self.assertLess(len(result), len(self.test_string), "String was not clipped.")

        rider_returned = result[-len(arg_dict["rider"]):]
        self.assertEquals(rider_returned, arg_dict["rider"], "Default rider was not applied.")

    def test_trim_string_with_no_spaces(self):

        clip_length = 200
        no_space_string = "a" * 500

        # Since this string has no spaces, clip_string will not be able to gracefully clip it
        # but will instead clip it exactly where the user specified
        result = iris.util.clip_string(no_space_string, clip_length, self.rider)

        expected_length = clip_length + len(self.rider)

        # Check the length of the returned string is equal to clip length + length of rider
        self.assertEquals(len(result), expected_length, "Mismatch in expected length of clipped string. Length was %s, expected value is %s" % (len(result), expected_length))
        

class TestDescribeDiff(iris.tests.IrisTest):
    def test_identical(self):
        test_cube_a = stock.realistic_4d()
        test_cube_b = stock.realistic_4d()

        return_str_IO = StringIO.StringIO()
        iris.util.describe_diff(test_cube_a, test_cube_b, output_file=return_str_IO)
        return_str = return_str_IO.getvalue()

        self.assertString(return_str, 'compatible_cubes.str.txt')

    def test_different(self):
        return_str_IO = StringIO.StringIO()
        
        # test incompatible attributes
        test_cube_a = stock.realistic_4d()
        test_cube_b = stock.realistic_4d()
        
        test_cube_a.attributes['Conventions'] = 'CF-1.5'
        test_cube_b.attributes['Conventions'] = 'CF-1.6'
        
        iris.util.describe_diff(test_cube_a, test_cube_b, output_file=return_str_IO)
        return_str = return_str_IO.getvalue()
        
        self.assertString(return_str, 'incompatible_attr.str.txt')
        
        # test incompatible names
        test_cube_a = stock.realistic_4d()
        test_cube_b = stock.realistic_4d()

        test_cube_a.standard_name = "relative_humidity"

        return_str_IO.truncate(0)
        iris.util.describe_diff(test_cube_a, test_cube_b, output_file=return_str_IO)
        return_str = return_str_IO.getvalue()

        self.assertString(return_str, 'incompatible_name.str.txt')

        # test incompatible unit
        test_cube_a = stock.realistic_4d()
        test_cube_b = stock.realistic_4d()
        
        test_cube_a.units = iris.unit.Unit('m')

        return_str_IO.truncate(0)
        iris.util.describe_diff(test_cube_a, test_cube_b, output_file=return_str_IO)
        return_str = return_str_IO.getvalue()
        
        self.assertString(return_str, 'incompatible_unit.str.txt')
        
        # test incompatible methods
        test_cube_a = stock.realistic_4d()
        test_cube_b = stock.realistic_4d().collapsed('model_level_number', iris.analysis.MEAN)

        return_str_IO.truncate(0)
        iris.util.describe_diff(test_cube_a, test_cube_b, output_file=return_str_IO)
        return_str = return_str_IO.getvalue()

        self.assertString(return_str, 'incompatible_meth.str.txt')

    def test_output_file(self):
        # test incompatible attributes
        test_cube_a = stock.realistic_4d()
        test_cube_b = stock.realistic_4d().collapsed('model_level_number', iris.analysis.MEAN)

        test_cube_a.attributes['Conventions'] = 'CF-1.5'
        test_cube_b.attributes['Conventions'] = 'CF-1.6'
        test_cube_a.standard_name = "relative_humidity"
        test_cube_a.units = iris.unit.Unit('m')

        with self.temp_filename() as filename:
            with open(filename, 'w') as f:
                iris.util.describe_diff(test_cube_a, test_cube_b, output_file=f)
                f.close()

            self.assertFilesEqual(filename,
                              'incompatible_cubes.str.txt')


class TestAsCompatibleShape(tests.IrisTest):
    def test_slice(self):
        cube = tests.stock.realistic_4d()
        sliced = cube[1, :, 2, :-2]
        expected = cube[1:2, :, 2:3, :-2]
        res = iris.util.as_compatible_shape(sliced, cube)
        self.assertEqual(res, expected)

    def test_transpose(self):
        cube = tests.stock.realistic_4d()
        transposed = cube.copy()
        transposed.transpose()
        expected = cube
        res = iris.util.as_compatible_shape(transposed, cube)
        self.assertEqual(res, expected)

    def test_slice_and_transpose(self):
        cube = tests.stock.realistic_4d()
        sliced_and_transposed = cube[1, :, 2, :-2]
        sliced_and_transposed.transpose()
        expected = cube[1:2, :, 2:3, :-2]
        res = iris.util.as_compatible_shape(sliced_and_transposed, cube)
        self.assertEqual(res, expected)

    def test_collapsed(self):
        cube = tests.stock.realistic_4d()
        collapsed = cube.collapsed('model_level_number', iris.analysis.MEAN)
        expected_shape = list(cube.shape)
        expected_shape[1] = 1
        expected_data = collapsed.data.reshape(expected_shape)
        res = iris.util.as_compatible_shape(collapsed, cube)
        self.assertCML(res, ('util', 'as_compatible_shape_collapsed.cml'),
                       checksum=False)
        self.assertArrayEqual(expected_data, res.data)
        self.assertArrayEqual(expected_data.mask, res.data.mask)

    def test_reduce_dimensionality(self):
        # Test that as_compatible_shape() can demote
        # length one dimensions to scalars.
        cube = tests.stock.realistic_4d()
        src = cube[:, 2:3]
        expected = reduced = cube[:, 2]
        res = iris.util.as_compatible_shape(src, reduced)
        self.assertEqual(res, expected)

    def test_anonymous_dims(self):
        cube = tests.stock.realistic_4d()
        # Move all coords from dim_coords to aux_coords.
        for coord in cube.dim_coords:
            dim = cube.coord_dims(coord)
            cube.remove_coord(coord)
            cube.add_aux_coord(coord, dim)

        sliced = cube[1, :, 2, :-2]
        expected = cube[1:2, :, 2:3, :-2]
        res = iris.util.as_compatible_shape(sliced, cube)
        self.assertEqual(res, expected)

    def test_scalar_auxcoord(self):
        def dim_to_aux(cube, coord_name):
            """Convert coordinate on cube from DimCoord to AuxCoord."""
            coord = cube.coord(coord_name)
            coord = iris.coords.AuxCoord.from_coord(coord)
            cube.replace_coord(coord)

        cube = tests.stock.realistic_4d()
        src = cube[:, :, 3]
        dim_to_aux(src, 'grid_latitude')
        expected = cube[:, :, 3:4]
        dim_to_aux(expected, 'grid_latitude')
        res = iris.util.as_compatible_shape(src, cube)
        self.assertEqual(res, expected)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_verbose_logging
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# import iris tests first so that some things can be initialised before importing anything else
import iris.tests as tests

import os

import iris
import iris.fileformats.pp
import iris.config as config
import iris.fileformats.rules as rules


@tests.skip_data
class TestVerboseLogging(tests.IrisTest):
    def test_verbose_logging(self):
        # check that verbose logging no longer breaks in pp.save()
        # load some data, enable logging, and save a cube to PP.
        data_path = tests.get_data_path(('PP', 'simple_pp', 'global.pp'))
        cube = iris.load_cube(data_path)
        OLD_RULE_LOG_DIR = config.RULE_LOG_DIR
        config.RULE_LOG_DIR = '/var/tmp'
        old_log = rules.log
        rules.log = rules._prepare_rule_logger(verbose=True)

        temp_filename1 = iris.util.create_temp_filename(suffix='.pp')

        # Test writing to a file handle to test that the logger uses the handle name
        with open(temp_filename1, "wb") as mysavefile:
            try:
                iris.save(cube, mysavefile)
            finally:
                # Restore old logging config
                config.RULE_LOG_DIR = OLD_RULE_LOG_DIR
                rules.log = old_log
                os.unlink(temp_filename1)

if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_project
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :func:`iris.analysis.cartography.project` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock
import numpy as np

import cartopy.crs as ccrs
import iris
from iris.coords import DimCoord
from iris.analysis.cartography import project


class TestAll(tests.IrisTest):
    def setUp(self):
        cs = iris.coord_systems.GeogCS(654321)
        self.cube = iris.cube.Cube(np.zeros(25).reshape(5, 5))
        self.cube.add_dim_coord(
            DimCoord(range(5), standard_name="latitude", units='degrees',
                     coord_system=cs), 0)
        self.cube.add_dim_coord(
            DimCoord(range(5), standard_name="longitude", units='degrees',
                     coord_system=cs), 1)

        self.tcs = iris.coord_systems.GeogCS(600000)

    def test_is_iris_coord_system(self):
        res, _ = project(self.cube, self.tcs)
        self.assertEqual(res.coord('projection_y_coordinate').coord_system,
                         self.tcs)
        self.assertEqual(res.coord('projection_x_coordinate').coord_system,
                         self.tcs)

        self.assertIsNot(res.coord('projection_y_coordinate').coord_system,
                         self.tcs)
        self.assertIsNot(res.coord('projection_x_coordinate').coord_system,
                         self.tcs)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_linear
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :func:`iris.analysis.interpolate.linear` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

import numpy as np
import iris

from iris.analysis.interpolate import linear
import iris.tests.stock as stock


class Test(tests.IrisTest):
    def setUp(self):
        self.cube = stock.simple_2d()
        self.extrapolation = 'extrapolation_mode'
        self.scheme = mock.Mock(name='linear scheme')

    @mock.patch('iris.analysis.interpolate.Linear', name='linear_patch')
    @mock.patch('iris.cube.Cube.interpolate', name='cube_interp_patch')
    def _assert_expected_call(self, sample_points, sample_points_call,
                              cinterp_patch, linear_patch):
        linear_patch.return_value = self.scheme
        linear(self.cube, sample_points, self.extrapolation)

        linear_patch.assert_called_once_with(self.extrapolation)

        cinterp_patch.assert_called_once_with(self.scheme, sample_points_call)

    def test_sample_point_dict(self):
        # Passing sample_points in the form of a dictionary.
        sample_points = {'foo': 0.5, 'bar': 0.5}
        sample_points_call = [('foo', 0.5), ('bar', 0.5)]
        self._assert_expected_call(sample_points, sample_points_call)

    def test_sample_point_iterable(self):
        # Passing an interable sample_points object.
        sample_points = (('foo', 0.5), ('bar', 0.5))
        sample_points_call = sample_points
        self._assert_expected_call(sample_points, sample_points_call)


class Test_masks(tests.IrisTest):
    def test_mask_retention(self):
        cube = stock.realistic_4d_w_missing_data()
        interp_cube = linear(cube, [('pressure', [850, 950])])
        self.assertIsInstance(interp_cube.data, np.ma.MaskedArray)

        # this value is masked in the input
        self.assertTrue(cube.data.mask[0, 2, 2, 0])
        # and is still masked in the output
        self.assertTrue(interp_cube.data.mask[0, 1, 2, 0])


class TestNDCoords(tests.IrisTest):
    def setUp(self):
        cube = stock.simple_3d_w_multidim_coords()
        cube.add_dim_coord(iris.coords.DimCoord(range(3), 'longitude'), 1)
        cube.add_dim_coord(iris.coords.DimCoord(range(4), 'latitude'), 2)
        cube.data = cube.data.astype(np.float32)
        self.cube = cube

    def test_multi(self):
        # Testing interpolation on specified points on cube with
        # multidimensional coordinates.
        interp_cube = linear(self.cube, {'latitude': 1.5, 'longitude': 1.5})
        self.assertCMLApproxData(interp_cube, ('experimental', 'analysis',
                                               'interpolate',
                                               'linear_nd_2_coords.cml'))

    def test_single_extrapolation(self):
        # Interpolation on the 1d coordinate with extrapolation.
        interp_cube = linear(self.cube, {'wibble': np.float32(1.5)})
        expected = ('experimental', 'analysis', 'interpolate',
                    'linear_nd_with_extrapolation.cml')
        self.assertCMLApproxData(interp_cube, expected)

    def test_single(self):
        # Interpolation on the 1d coordinate.
        interp_cube = linear(self.cube, {'wibble': 20})
        self.assertArrayEqual(np.mean(self.cube.data, axis=0),
                              interp_cube.data)
        self.assertCMLApproxData(interp_cube, ('experimental', 'analysis',
                                               'interpolate', 'linear_nd.cml'))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_LinearInterpolator
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for :class:`iris.analysis._interpolator.LinearInterpolator`."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

from nose.tools import assert_raises_regexp
import numpy as np

import iris
import iris.exceptions
import iris.tests.stock as stock
from iris.analysis._interpolator import LinearInterpolator


class ThreeDimCube(tests.IrisTest):
    def setUp(self):
        cube = stock.simple_3d_w_multidim_coords()
        cube.add_aux_coord(iris.coords.DimCoord(range(2), 'height'), 0)
        cube.add_dim_coord(iris.coords.DimCoord(range(3), 'latitude'), 1)
        cube.add_dim_coord(iris.coords.DimCoord(range(4), 'longitude'), 2)
        self.data = np.arange(24).reshape(2, 3, 4).astype(np.float32)
        cube.data = self.data
        self.cube = cube


class Test___init__(ThreeDimCube):
    def test_properties(self):
        interpolator = LinearInterpolator(self.cube, ['latitude'])

        # Default extrapolation mode.
        self.assertEqual(interpolator.extrapolation_mode, 'linear')

        # Access to cube property of the LinearInterpolator instance.
        self.assertEqual(interpolator.cube, self.cube)

        # Access to the resulting coordinate which we are interpolating over.
        self.assertEqual(interpolator.coords,
                         [self.cube.coord('latitude')])


class Test___init____circular(tests.IrisTest):
    # Capture the behaviour of the interpolator initialisation based on the
    # circular flag.
    def setUp(self):
        # Load some data from a file so that we have some deferred data.
        self.cube = stock.simple_pp()

    def test_not_circular(self):
        self.cube.coord('longitude').circular = False
        interpolator = LinearInterpolator(self.cube, ['longitude'])
        self.assertTrue(interpolator.cube.has_lazy_data())

    def test_circular(self):
        self.cube.coord('longitude').circular = True
        interpolator = LinearInterpolator(self.cube, ['longitude'])
        self.assertTrue(interpolator.cube.has_lazy_data())


class Test___init____validation(ThreeDimCube):
    def test_interpolator_overspecified(self):
        # Over specification by means of interpolating over two coordinates
        # mapped to the same dimension.
        msg = 'Coordinates repeat a data dimension - '\
            'the interpolation would be over-specified'
        with self.assertRaisesRegexp(ValueError, msg):
            LinearInterpolator(self.cube, ['wibble', 'height'])

    def test_interpolator_overspecified_scalar(self):
        # Over specification by means of interpolating over one dimension
        # coordinate and a scalar coordinate (not mapped to a dimension).
        self.cube.add_aux_coord(iris.coords.AuxCoord(
            1, long_name='scalar'), None)

        msg = 'Coordinates repeat a data dimension - '\
            'the interpolation would be over-specified'
        with self.assertRaisesRegexp(ValueError, msg):
            LinearInterpolator(self.cube, ['wibble', 'scalar'])

    def test_interpolate__decreasing(self):
        def check_expected():
            # Check a simple case is equivalent to extracting the first row.
            self.interpolator = LinearInterpolator(self.cube, ['latitude'])
            expected = self.data[:, 0:1, :]
            result = self.interpolator([[0]])
            self.assertArrayEqual(result.data, expected)

        # Check with normal cube.
        check_expected()
        # Check same result from a cube inverted in the latitude dimension.
        self.cube = self.cube[:, ::-1]
        check_expected()

    def test_interpolate_non_monotonic(self):
        self.cube.add_aux_coord(iris.coords.AuxCoord(
            [0, 3, 2], long_name='non-monotonic'), 1)
        msg = ('Cannot interpolate over the non-monotonic coordinate '
               'non-monotonic.')
        with self.assertRaisesRegexp(ValueError, msg):
            LinearInterpolator(self.cube, ['non-monotonic'])


class Test___call____1D(ThreeDimCube):
    def setUp(self):
        ThreeDimCube.setUp(self)
        self.interpolator = LinearInterpolator(self.cube, ['latitude'])

    def test_interpolate_bad_coord_name(self):
        with self.assertRaises(iris.exceptions.CoordinateNotFoundError):
            LinearInterpolator(self.cube, ['doesnt exist'])

    def test_interpolate_data_single(self):
        # Single sample point.
        result = self.interpolator([[1.5]])
        expected = self.data[:, 1:, :].mean(axis=1).reshape(2, 1, 4)
        self.assertArrayEqual(result.data, expected)

        foo_res = result.coord('foo').points
        bar_res = result.coord('bar').points
        expected_foo = self.cube[:, 1:, :].coord('foo').points.mean(
            axis=0).reshape(1, 4)
        expected_bar = self.cube[:, 1:, :].coord('bar').points.mean(
            axis=0).reshape(1, 4)

        self.assertArrayEqual(foo_res, expected_foo)
        self.assertArrayEqual(bar_res, expected_bar)

    def test_interpolate_data_multiple(self):
        # Multiple sample points for a single coordinate (these points are not
        # interpolated).
        result = self.interpolator([[1, 2]])
        self.assertArrayEqual(result.data, self.data[:, 1:3, :])

        foo_res = result.coord('foo').points
        bar_res = result.coord('bar').points
        expected_foo = self.cube[:, 1:, :].coord('foo').points
        expected_bar = self.cube[:, 1:, :].coord('bar').points

        self.assertArrayEqual(foo_res, expected_foo)
        self.assertArrayEqual(bar_res, expected_bar)

    def _interpolate_data_linear_extrapolation(self, result):
        expected = self.data[:, 0:1] - (self.data[:, 1:2] - self.data[:, 0:1])
        self.assertArrayEqual(result.data, expected)

    def test_interpolate_data_linear_extrapolation(self):
        # Sample point outside the coordinate range.
        result = self.interpolator([[-1]])
        self._interpolate_data_linear_extrapolation(result)

    def test_interpolate_data_default_extrapolation(self):
        # Sample point outside the coordinate range.
        interpolator = LinearInterpolator(self.cube, ['latitude'],
                                          extrapolation_mode='linear')
        result = interpolator([[-1]])
        self._interpolate_data_linear_extrapolation(result)

    def _extrapolation_dtype(self, dtype):
        interpolator = LinearInterpolator(self.cube, ['latitude'],
                                          extrapolation_mode='nan')
        result = interpolator([[-1]])
        self.assertTrue(np.all(np.isnan(result.data)))

    def test_extrapolation_nan_float32(self):
        # Ensure np.nan in a float32 array results.
        self._extrapolation_dtype(np.float32)

    def test_extrapolation_nan_float64(self):
        # Ensure np.nan in a float64 array results.
        self._extrapolation_dtype(np.float64)

    def test_interpolate_data_error_on_extrapolation(self):
        msg = 'One of the requested xi is out of bounds in dimension 0'
        interpolator = LinearInterpolator(self.cube, ['latitude'],
                                          extrapolation_mode='error')
        with assert_raises_regexp(ValueError, msg):
            interpolator([[-1]])

    def test_interpolate_data_unsupported_extrapolation(self):
        msg = "Extrapolation mode 'unsupported' not supported"
        with assert_raises_regexp(ValueError, msg):
            LinearInterpolator(self.cube, ['latitude'],
                               extrapolation_mode='unsupported')

    def test_multi_points_array(self):
        # Providing a multidimensional sample points for a 1D interpolation.
        # i.e. points given for two coordinates where there are only one
        # specified.
        msg = 'Expected sample points for 1 coordinates, got 2.'
        with assert_raises_regexp(ValueError, msg):
            self.interpolator([[1, 2], [1]])

    def test_interpolate_data_dtype_casting(self):
        data = self.data.astype(int)
        self.cube.data = data
        self.interpolator = LinearInterpolator(self.cube, ['latitude'])
        result = self.interpolator([[0.125]])
        self.assertEqual(result.data.dtype, np.float64)

    def test_default_collapse_scalar(self):
        interpolator = LinearInterpolator(self.cube, ['wibble'])
        result = interpolator([0])
        self.assertEqual(result.shape, (3, 4))

    def test_collapse_scalar(self):
        interpolator = LinearInterpolator(self.cube, ['wibble'])
        result = interpolator([0], collapse_scalar=True)
        self.assertEqual(result.shape, (3, 4))

    def test_no_collapse_scalar(self):
        interpolator = LinearInterpolator(self.cube, ['wibble'])
        result = interpolator([0], collapse_scalar=False)
        self.assertEqual(result.shape, (1, 3, 4))

    def test_unsorted_datadim_mapping(self):
        # Currently unsorted data dimension mapping is not supported as the
        # indexing is not yet clever enough to remap the interpolated
        # coordinates.
        self.cube.transpose((0, 2, 1))
        interpolator = LinearInterpolator(self.cube, ['latitude'])
        msg = 'Currently only increasing data_dims is supported.'
        with self.assertRaisesRegexp(NotImplementedError, msg):
            interpolator([0])


class Test___call____1D_circular(ThreeDimCube):
    # Note: all these test data interpolation.
    def setUp(self):
        ThreeDimCube.setUp(self)
        self.cube.coord('longitude')._points = np.linspace(0, 360, 4,
                                                           endpoint=False)
        self.cube.coord('longitude').circular = True
        self.cube.coord('longitude').units = 'degrees'
        self.interpolator = LinearInterpolator(self.cube, ['longitude'],
                                               extrapolation_mode='nan')
        self.cube_reverselons = self.cube[:, :, ::-1]
        self.interpolator_reverselons = LinearInterpolator(
            self.cube_reverselons, ['longitude'], extrapolation_mode='nan')

        self.testpoints_fully_wrapped = ([[180, 270]], [[-180, -90]])
        self.testpoints_partially_wrapped = ([[180, 90]], [[-180, 90]])
        self.testpoints_fully_wrapped_twice = (
            [np.linspace(-360, 360, 100)],
            [(np.linspace(-360, 360, 100) + 360) % 360])

    def test_fully_wrapped(self):
        points, points_wrapped = self.testpoints_fully_wrapped
        expected = self.interpolator(points)
        result = self.interpolator(points_wrapped)
        self.assertArrayEqual(expected.data, result.data)

    def test_fully_wrapped_reversed_mainpoints(self):
        points, _ = self.testpoints_fully_wrapped
        expected = self.interpolator(points)
        result = self.interpolator_reverselons(points)
        self.assertArrayEqual(expected.data, result.data)

    def test_fully_wrapped_reversed_testpoints(self):
        _, points = self.testpoints_fully_wrapped
        expected = self.interpolator(points)
        result = self.interpolator_reverselons(points)
        self.assertArrayEqual(expected.data, result.data)

    def test_partially_wrapped(self):
        points, points_wrapped = self.testpoints_partially_wrapped
        expected = self.interpolator(points)
        result = self.interpolator(points_wrapped)

    def test_partially_wrapped_reversed_mainpoints(self):
        points, _ = self.testpoints_partially_wrapped
        expected = self.interpolator(points)
        result = self.interpolator_reverselons(points)
        self.assertArrayEqual(expected.data, result.data)

    def test_partially_wrapped_reversed_testpoints(self):
        points, _ = self.testpoints_partially_wrapped
        expected = self.interpolator(points)
        result = self.interpolator_reverselons(points)
        self.assertArrayEqual(expected.data, result.data)

    def test_fully_wrapped_twice(self):
        xs, xs_not_wrapped = self.testpoints_fully_wrapped_twice
        expected = self.interpolator(xs)
        result = self.interpolator(xs_not_wrapped)
        self.assertArrayEqual(expected.data, result.data)

    def test_fully_wrapped_twice_reversed_mainpoints(self):
        points. _ = self.testpoints_fully_wrapped_twice
        expected = self.interpolator(points)
        result = self.interpolator_reverselons(points)
        self.assertArrayEqual(expected.data, result.data)

    def test_fully_wrapped_twice_reversed_mainpoints(self):
        _, points = self.testpoints_fully_wrapped_twice
        expected = self.interpolator(points)
        result = self.interpolator_reverselons(points)
        self.assertArrayEqual(expected.data, result.data)

    def test_fully_wrapped_not_circular(self):
        cube = stock.lat_lon_cube()
        new_long = cube.coord('longitude').copy(
            cube.coord('longitude').points + 710)
        cube.remove_coord('longitude')
        cube.add_dim_coord(new_long, 1)

        interpolator = LinearInterpolator(cube, ['longitude'])
        res = interpolator([-10])
        self.assertArrayEqual(res.data, cube[:, 1].data)


class Test___call____1D_singlelendim(ThreeDimCube):
    def setUp(self):
        """
        thingness / (1)                     (wibble: 2; latitude: 1)
             Dimension coordinates:
                  wibble                           x            -
                  latitude                         -            x
             Auxiliary coordinates:
                  height                           x            -
                  bar                              -            x
                  foo                              -            x
             Scalar coordinates:
                  longitude: 0
        """
        ThreeDimCube.setUp(self)
        self.cube = self.cube[:, 0:1, 0]
        self.interpolator = LinearInterpolator(self.cube, ['latitude'])

    def test_interpolate_data_linear_extrapolation(self):
        # Linear extrapolation of a single valued element.
        result = self.interpolator([[1001]])
        self.assertArrayEqual(result.data, self.cube.data)

    def test_interpolate_data_nan_extrapolation(self):
        interpolator = LinearInterpolator(self.cube, ['latitude'],
                                          extrapolation_mode='nan')
        result = interpolator([[1001]])
        self.assertTrue(np.all(np.isnan(result.data)))

    def test_interpolate_data_nan_extrapolation_not_needed(self):
        # No extrapolation for a single length dimension.
        interpolator = LinearInterpolator(self.cube, ['latitude'],
                                          extrapolation_mode='nan')
        result = interpolator([[0]])
        self.assertArrayEqual(result.data, self.cube.data)


class Test___call____masked(tests.IrisTest):
    def setUp(self):
        self.cube = stock.simple_4d_with_hybrid_height()
        mask = np.isnan(self.cube.data)
        mask[::3, ::3] = True
        self.cube.data = np.ma.masked_array(self.cube.data,
                                            mask=mask)

    def test_orthogonal_cube(self):
        interpolator = LinearInterpolator(self.cube, ['grid_latitude'])
        result_cube = interpolator([1])

        # Explicit mask comparison to ensure mask retention.
        # Masked value input
        self.assertTrue(self.cube.data.mask[0, 0, 0, 0])
        # Mask retention on output
        self.assertTrue(result_cube.data.mask[0, 0, 0])

        self.assertCML(result_cube, ('experimental', 'analysis',
                                     'interpolate', 'LinearInterpolator',
                                     'orthogonal_cube_with_factory.cml'))


class Test___call____2D(ThreeDimCube):
    def setUp(self):
        ThreeDimCube.setUp(self)
        self.interpolator = LinearInterpolator(self.cube,
                                               ['latitude', 'longitude'])

    def test_interpolate_data(self):
        result = self.interpolator([[1, 2], [2]])
        expected = self.data[:, 1:3, 2:3]
        self.assertArrayEqual(result.data, expected)

        index = (slice(None), slice(1, 3, 1), slice(2, 3, 1))
        for coord in self.cube.coords():
            coord_res = result.coord(coord).points
            coord_expected = self.cube[index].coord(coord).points

            self.assertArrayEqual(coord_res, coord_expected)

    def test_orthogonal_points(self):
        result = self.interpolator([[1, 2], [1, 2]])
        expected = self.data[:, 1:3, 1:3]
        self.assertArrayEqual(result.data, expected)

        index = (slice(None), slice(1, 3, 1), slice(1, 3, 1))
        for coord in self.cube.coords():
            coord_res = result.coord(coord).points
            coord_expected = self.cube[index].coord(coord).points

            self.assertArrayEqual(coord_res, coord_expected)

    def test_multi_dim_coord_interpolation(self):
        msg = 'Interpolation coords must be 1-d for rectilinear interpolation.'
        with self.assertRaisesRegexp(ValueError, msg):
            interpolator = LinearInterpolator(self.cube, ['foo', 'bar'])
            interpolator([[15], [10]])


class Test___call____2D_non_contiguous(ThreeDimCube):
    def setUp(self):
        ThreeDimCube.setUp(self)
        coords = ['height', 'longitude']
        self.interpolator = LinearInterpolator(self.cube, coords)

    def test_interpolate_data_multiple(self):
        result = self.interpolator([[1], [1, 2]])
        expected = self.data[1:2, :, 1:3]
        self.assertArrayEqual(result.data, expected)

        index = (slice(1, 2), slice(None), slice(1, 3, 1))
        for coord in self.cube.coords():
            coord_res = result.coord(coord).points
            coord_expected = self.cube[index].coord(coord).points

            self.assertArrayEqual(coord_res, coord_expected)

    def test_orthogonal_cube(self):
        result_cube = self.interpolator([np.int64([0, 1, 1]),
                                         np.int32([0, 1])])
        result_path = ('experimental', 'analysis', 'interpolate',
                       'LinearInterpolator', 'basic_orthogonal_cube.cml')
        self.assertCMLApproxData(result_cube, result_path)
        self.assertEqual(result_cube.coord('longitude').dtype, np.int32)
        self.assertEqual(result_cube.coord('height').dtype, np.int64)

    def test_orthogonal_cube_squash(self):
        result_cube = self.interpolator([np.int64(0),
                                         np.int32([0, 1])])
        result_path = ('experimental', 'analysis', 'interpolate',
                       'LinearInterpolator', 'orthogonal_cube_1d_squashed.cml')
        self.assertCMLApproxData(result_cube, result_path)
        self.assertEqual(result_cube.coord('longitude').dtype, np.int32)
        self.assertEqual(result_cube.coord('height').dtype, np.int64)

        non_collapsed_cube = self.interpolator([[np.int64(0)],
                                                np.int32([0, 1])],
                                               collapse_scalar=False)
        result_path = ('experimental', 'analysis', 'interpolate',
                       'LinearInterpolator',
                       'orthogonal_cube_1d_squashed_2.cml')
        self.assertCML(non_collapsed_cube[0, ...], result_path)
        self.assertCML(result_cube, result_path)
        self.assertEqual(result_cube, non_collapsed_cube[0, ...])


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_add
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :func:`iris.analysis.maths.add` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import operator

from iris.analysis.maths import add
from iris.tests.unit.analysis.maths import \
    CubeArithmeticBroadcastingTestMixin, CubeArithmeticMaskingTestMixin


class TestBroadcasting(tests.IrisTest, CubeArithmeticBroadcastingTestMixin):
    @property
    def data_op(self):
        return operator.add

    @property
    def cube_func(self):
        return add


class TestMasking(tests.IrisTest, CubeArithmeticMaskingTestMixin):
    @property
    def data_op(self):
        return operator.add

    @property
    def cube_func(self):
        return add


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_divide
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :func:`iris.analysis.maths.divide` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import numpy as np
import operator

from iris.analysis.maths import divide
from iris.cube import Cube
from iris.tests.unit.analysis.maths import \
    CubeArithmeticBroadcastingTestMixin, CubeArithmeticMaskingTestMixin


class TestBroadcasting(tests.IrisTest, CubeArithmeticBroadcastingTestMixin):
    @property
    def data_op(self):
        return operator.div

    @property
    def cube_func(self):
        return divide


class TestMasking(tests.IrisTest, CubeArithmeticMaskingTestMixin):
    @property
    def data_op(self):
        return operator.div

    @property
    def cube_func(self):
        return divide

    def test_unmasked_div_zero(self):
        # Ensure cube behaviour matches numpy operator behaviour for the
        # handling of arrays containing 0.
        dat_a = np.array([0., 0., 0., 0.])
        dat_b = np.array([2., 2., 2., 2.])

        cube_a = Cube(dat_a)
        cube_b = Cube(dat_b)

        com = self.data_op(dat_b, dat_a)
        res = self.cube_func(cube_b, cube_a).data

        self.assertArrayEqual(com, res)

    def test_masked_div_zero(self):
        # Ensure cube behaviour matches numpy operator behaviour for the
        # handling of arrays containing 0.
        dat_a = np.ma.array([0., 0., 0., 0.], mask=False)
        dat_b = np.ma.array([2., 2., 2., 2.], mask=False)

        cube_a = Cube(dat_a)
        cube_b = Cube(dat_b)

        com = self.data_op(dat_b, dat_a)
        res = self.cube_func(cube_b, cube_a).data

        self.assertMaskedArrayEqual(com, res, strict=True)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_multiply
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :func:`iris.analysis.maths.multiply` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import operator

from iris.analysis.maths import multiply
from iris.tests.unit.analysis.maths import \
    CubeArithmeticBroadcastingTestMixin, CubeArithmeticMaskingTestMixin


class TestBroadcasting(tests.IrisTest, CubeArithmeticBroadcastingTestMixin):
    @property
    def data_op(self):
        return operator.mul

    @property
    def cube_func(self):
        return multiply


class TestMasking(tests.IrisTest, CubeArithmeticMaskingTestMixin):
    @property
    def data_op(self):
        return operator.mul

    @property
    def cube_func(self):
        return multiply


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_subtract
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :func:`iris.analysis.maths.subtract` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import operator

from iris.analysis.maths import subtract
from iris.tests.unit.analysis.maths import \
    CubeArithmeticBroadcastingTestMixin, CubeArithmeticMaskingTestMixin


class TestBroadcasting(tests.IrisTest, CubeArithmeticBroadcastingTestMixin):
    @property
    def data_op(self):
        return operator.sub

    @property
    def cube_func(self):
        return subtract


class TestMasking(tests.IrisTest, CubeArithmeticMaskingTestMixin):
    @property
    def data_op(self):
        return operator.sub

    @property
    def cube_func(self):
        return subtract


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_Aggregator
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :class:`iris.analysis.Aggregator` class instance."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

from mock import patch, sentinel, Mock
import numpy as np
import numpy.ma as ma

from iris.analysis import Aggregator


class Test_aggregate(tests.IrisTest):
    # These unit tests don't call a data aggregation function, they call a
    # mocked one i.e. the return values of the mocked data aggregation
    # function don't matter, only how these are dealt with by the aggregate
    # method.
    def setUp(self):
        self.TEST = Aggregator('test', None)
        self.array = ma.array([[1, 2, 3],
                               [4, 5, 6]],
                              mask=[[False, True, False],
                                    [True, False, False]],
                              dtype=np.float64)
        self.expected_result_axis0 = ma.array([1, 2, 3], mask=None)
        self.expected_result_axis1 = ma.array([4, 5], mask=None)

    def test_masked_notol(self):
        # Providing masked array with no tolerance keyword (mdtol) provided.
        axis = 0
        mock_return = self.expected_result_axis0.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(self.array, axis)
            self.assertMaskedArrayEqual(result, self.expected_result_axis0)
        mock_method.assert_called_once_with(self.array, axis=axis)

        axis = 1
        mock_return = self.expected_result_axis1.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(self.array, axis)
            self.assertMaskedArrayEqual(result, self.expected_result_axis1)
        mock_method.assert_called_once_with(self.array, axis=axis)

    def test_masked_above_tol(self):
        # Providing masked array with a high tolerance (mdtol) provided.
        axis = 0
        mock_return = self.expected_result_axis0.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(self.array, axis, mdtol=.55)
            self.assertMaskedArrayEqual(result, self.expected_result_axis0)
        mock_method.assert_called_once_with(self.array, axis=axis)

        axis = 1
        mock_return = self.expected_result_axis1.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(self.array, axis, mdtol=.55)
            self.assertMaskedArrayEqual(result, self.expected_result_axis1)
        mock_method.assert_called_once_with(self.array, axis=axis)

    def test_masked_below_tol(self):
        # Providing masked array with a tolerance on missing values, low
        # enough to modify the resulting mask for axis 0.
        axis = 0
        result_axis_0 = self.expected_result_axis0.copy()
        result_axis_0.mask = np.array([True, True, False])
        mock_return = ma.array([1, 2, 3], mask=None)
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(self.array, axis, mdtol=.45)
            self.assertMaskedArrayAlmostEqual(result, result_axis_0)
        mock_method.assert_called_once_with(self.array, axis=axis)

        axis = 1
        mock_return = self.expected_result_axis1.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(self.array, axis, mdtol=.45)
            self.assertMaskedArrayEqual(result, self.expected_result_axis1)
        mock_method.assert_called_once_with(self.array, axis=axis)

    def test_masked_below_tol_alt(self):
        # Providing masked array with a tolerance on missing values, low
        # enough to modify the resulting mask for axis 1.
        axis = 1
        result_axis_1 = self.expected_result_axis1.copy()
        result_axis_1.mask = np.array([True, True])
        mock_return = self.expected_result_axis1.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(self.array, axis, mdtol=.1)
            self.assertMaskedArrayAlmostEqual(result, result_axis_1)
        mock_method.assert_called_once_with(self.array, axis=axis)

    def test_unmasked_with_mdtol(self):
        # Providing aggregator with an unmasked array and tolerance specified
        # for missing data - ensure that result is unaffected.
        data = self.array.data

        axis = 0
        mock_return = self.expected_result_axis0.data.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(data, axis, mdtol=0.5)
            self.assertArrayAlmostEqual(result, mock_return.copy())
        mock_method.assert_called_once_with(data, axis=axis)

        axis = 1
        mock_return = self.expected_result_axis1.data.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(data, axis, mdtol=0.5)
            self.assertArrayAlmostEqual(result, mock_return.copy())
        mock_method.assert_called_once_with(data, axis=axis)

    def test_unmasked(self):
        # Providing aggregator with an unmasked array and no additional keyword
        # arguments ensure that result is unaffected.
        data = self.array.data

        axis = 0
        mock_return = self.expected_result_axis0.data.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(data, axis)
            self.assertArrayAlmostEqual(result, mock_return.copy())
        mock_method.assert_called_once_with(data, axis=axis)

        axis = 1
        mock_return = self.expected_result_axis1.data.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(data, axis)
            self.assertArrayAlmostEqual(result, mock_return.copy())
        mock_method.assert_called_once_with(data, axis=axis)

    def test_returning_array_len_one_mdtol(self):
        # Test the case when the data aggregation function returns a scalar and
        # turns it into a masked array.
        axis = -1
        data = self.array.flatten()
        mock_return = 2
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(data, axis, mdtol=1)
            self.assertMaskedArrayEqual(result, ma.array([2], mask=[False]))
        mock_method.assert_called_once_with(data, axis=axis)

    def test_returning_array_len_one_mdtol_alt(self):
        # Test the case when the data aggregation function returns a scalar
        # with no tolerance for missing data values and turns it into a masked
        # array of length one.
        axis = -1
        data = self.array.flatten()
        mock_return = 2
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(data, axis, mdtol=0)
            self.assertMaskedArrayEqual(result, ma.array([2], mask=[True]))
        mock_method.assert_called_once_with(data, axis=axis)

    def test_returning_non_masked_array_from_masked_array(self):
        # Providing a masked array, call_func returning a non-masked array,
        # resulting in a masked array output.
        axis = 0
        mock_return = self.expected_result_axis0.data.copy()
        result_axis_0 = ma.array(mock_return, mask=[True, True, False])
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(self.array, axis, mdtol=.45)
            self.assertMaskedArrayAlmostEqual(result, result_axis_0)
        mock_method.assert_called_once_with(self.array, axis=axis)

        axis = 1
        mock_return = self.expected_result_axis1.data.copy()
        with patch.object(self.TEST, 'call_func',
                          return_value=mock_return) as mock_method:
            result = self.TEST.aggregate(self.array, axis, mdtol=.45)
            self.assertMaskedArrayEqual(result, self.expected_result_axis1)
        mock_method.assert_called_once_with(self.array, axis=axis)


class Test_update_metadata(tests.IrisTest):
    def test_no_units_change(self):
        # If the Aggregator has no units_func then the units should be
        # left unchanged.
        aggregator = Aggregator('', None)
        cube = Mock(units=sentinel.units)
        aggregator.update_metadata(cube, [])
        self.assertIs(cube.units, sentinel.units)

    def test_units_change(self):
        # If the Aggregator has a units_func then the new units should
        # be defined by its return value.
        units_func = Mock(return_value=sentinel.new_units)
        aggregator = Aggregator('', None, units_func)
        cube = Mock(units=sentinel.units)
        aggregator.update_metadata(cube, [])
        units_func.assert_called_once_with(sentinel.units)
        self.assertEqual(cube.units, sentinel.new_units)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_COUNT
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :data:`iris.analysis.COUNT` aggregator."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import biggus
import numpy.ma as ma

from iris.analysis import COUNT
import iris.cube
from iris.coords import DimCoord
import iris.exceptions


class Test_units_func(tests.IrisTest):
    def test(self):
        self.assertIsNotNone(COUNT.units_func)
        new_units = COUNT.units_func(None)
        self.assertEqual(new_units, 1)


class Test_masked(tests.IrisTest):
    def setUp(self):
        self.cube = iris.cube.Cube(ma.masked_equal([1, 2, 3, 4, 5], 3))
        self.cube.add_dim_coord(DimCoord([6, 7, 8, 9, 10], long_name='foo'), 0)
        self.func = lambda x: x >= 3

    def test_ma(self):
        cube = self.cube.collapsed("foo", COUNT, function=self.func)
        self.assertArrayEqual(cube.data, [2])

    def test_biggus(self):
        self.cube.lazy_data(array=biggus.NumpyArrayAdapter(self.cube.data))
        with self.assertRaises(iris.exceptions.LazyAggregatorError):
            cube = self.cube.collapsed("foo", COUNT, lazy=True)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_geometry_area_weights
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for the :func:`iris.analysis.geometry.geometry_area_weights`
function.

"""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

from mock import Mock
import numpy as np
from shapely.geometry import Polygon

from iris.analysis.geometry import geometry_area_weights
from iris.coords import DimCoord
from iris.cube import Cube


class Test(tests.IrisTest):
    def setUp(self):
        x_coord = DimCoord([0.5, 1.5], 'longitude', bounds=[[0, 2], [2, 4]])
        y_coord = DimCoord([0.5, 1.5], 'latitude', bounds=[[0, 2], [2, 4]])
        self.data = np.empty((4, 2, 2))
        dim_coords_and_dims = [(y_coord, (1,)), (x_coord, (2,))]
        self.cube = Cube(self.data, dim_coords_and_dims=dim_coords_and_dims)
        self.geometry = Polygon([(3, 3), (3, 50), (50, 50), (50, 3)])

    def test_no_overlap(self):
        self.geometry = Polygon([(4, 4), (4, 6), (6, 6), (6, 4)])
        weights = geometry_area_weights(self.cube, self.geometry)
        self.assertEqual(np.sum(weights), 0)

    def test_overlap(self):
        weights = geometry_area_weights(self.cube, self.geometry)
        expected = np.repeat([[[0., 0.], [0., 1.]]], self.data.shape[0],
                             axis=0)
        self.assertArrayEqual(weights, expected)

    def test_overlap_normalize(self):
        weights = geometry_area_weights(self.cube, self.geometry,
                                        normalize=True)
        expected = np.repeat([[[0., 0.], [0., 0.25]]], self.data.shape[0],
                             axis=0)
        self.assertArrayEqual(weights, expected)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_PROPORTION
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :data:`iris.analysis.PROPORTION` aggregator."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import biggus
import numpy.ma as ma

from iris.analysis import PROPORTION
import iris.cube
from iris.coords import DimCoord
import iris.exceptions


class Test_units_func(tests.IrisTest):
    def test(self):
        self.assertIsNotNone(PROPORTION.units_func)
        new_units = PROPORTION.units_func(None)
        self.assertEqual(new_units, 1)


class Test_masked(tests.IrisTest):
    def setUp(self):
        self.cube = iris.cube.Cube(ma.masked_equal([1, 2, 3, 4, 5], 3))
        self.cube.add_dim_coord(DimCoord([6, 7, 8, 9, 10], long_name='foo'), 0)
        self.func = lambda x: x >= 3

    def test_ma(self):
        cube = self.cube.collapsed("foo", PROPORTION, function=self.func)
        self.assertArrayEqual(cube.data, [0.5])

    def test_biggus(self):
        self.cube.lazy_data(array=biggus.NumpyArrayAdapter(self.cube.data))
        with self.assertRaises(iris.exceptions.LazyAggregatorError):
            cube = self.cube.collapsed("foo", PROPORTION, lazy=True)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_RMS
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :data:`iris.analysis.RMS` aggregator."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import numpy as np
import numpy.ma as ma

from iris.analysis import RMS


class Test_aggregate(tests.IrisTest):

    def test_1d(self):
        # 1-dimensional input
        data = np.array([5, 2, 6, 4], dtype=np.float64)
        rms = RMS.aggregate(data, 0)
        expected_rms = 4.5
        self.assertAlmostEqual(rms, expected_rms)

    def test_2d(self):
        # 2-dimensional input
        data = np.array([[5, 2, 6, 4], [12, 4, 10, 8]], dtype=np.float64)
        expected_rms = np.array([4.5, 9.0], dtype=np.float64)
        rms = RMS.aggregate(data, 1)
        self.assertArrayAlmostEqual(rms, expected_rms)

    def test_1d_weighted(self):
        # 1-dimensional input with weights
        data = np.array([4, 7, 10, 8], dtype=np.float64)
        weights = np.array([1, 4, 3, 2], dtype=np.float64)
        expected_rms = 8.0
        rms = RMS.aggregate(data, 0, weights=weights)
        self.assertAlmostEqual(rms, expected_rms)

    def test_2d_weighted(self):
        # 2-dimensional input with weights
        data = np.array([[4, 7, 10, 8], [14, 16, 20, 8]], dtype=np.float64)
        weights = np.array([[1, 4, 3, 2], [2, 1, 1.5, 0.5]], dtype=np.float64)
        expected_rms = np.array([8.0, 16.0], dtype=np.float64)
        rms = RMS.aggregate(data, 1, weights=weights)
        self.assertArrayAlmostEqual(rms, expected_rms)

    def test_unit_weighted(self):
        # unit weights should be the same as no weights
        data = np.array([5, 2, 6, 4], dtype=np.float64)
        weights = np.ones_like(data)
        rms = RMS.aggregate(data, 0, weights=weights)
        expected_rms = 4.5
        self.assertAlmostEqual(rms, expected_rms)

    def test_masked(self):
        # masked entries should be completely ignored
        data = ma.array([5, 10, 2, 11, 6, 4],
                        mask=[False, True, False, True, False, False],
                        dtype=np.float64)
        expected_rms = 4.5
        rms = RMS.aggregate(data, 0)
        self.assertAlmostEqual(rms, expected_rms)

    def test_masked_weighted(self):
        # weights should work properly with masked arrays
        data = ma.array([4, 7, 18, 10, 11, 8],
                        mask=[False, False, True, False, True, False],
                        dtype=np.float64)
        weights = np.array([1, 4, 5, 3, 8, 2], dtype=np.float64)
        expected_rms = 8.0
        rms = RMS.aggregate(data, 0, weights=weights)
        self.assertAlmostEqual(rms, expected_rms)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_VARIANCE
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :data:`iris.analysis.VARIANCE` aggregator."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

import biggus
import numpy.ma as ma

from iris.analysis import VARIANCE
import iris.cube
from iris.coords import DimCoord
import iris.exceptions


class Test_units_func(tests.IrisTest):
    def test(self):
        self.assertIsNotNone(VARIANCE.units_func)
        mul = mock.Mock(return_value=mock.sentinel.new_unit)
        units = mock.Mock(__mul__=mul)
        new_units = VARIANCE.units_func(units)
        # Make sure the VARIANCE units_func tries to square the units.
        mul.assert_called_once_with(units)
        self.assertEqual(new_units, mock.sentinel.new_unit)


class Test_masked(tests.IrisTest):
    def setUp(self):
        self.cube = iris.cube.Cube(ma.masked_equal([1, 2, 3, 4, 5], 3))
        self.cube.add_dim_coord(DimCoord([6, 7, 8, 9, 10], long_name='foo'), 0)

    def test_ma(self):
        # Note: iris.analysis.VARIANCE adds ddof=1
        cube = self.cube.collapsed("foo", VARIANCE)
        self.assertArrayAlmostEqual(cube.data, [3.333333])

    def test_ma_ddof0(self):
        cube = self.cube.collapsed("foo", VARIANCE, ddof=0)
        self.assertArrayEqual(cube.data, [2.5])

    # Pending #1004.
#     def test_biggus(self):
#         self.cube.lazy_data(array=biggus.NumpyArrayAdapter(self.cube.data))
#         cube = self.cube.collapsed("foo", VARIANCE, lazy=True)
#         self.assertArrayAlmostEqual(cube.lazy_data().masked_array(),
#                                     [3.333333])


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_HybridPressureFactory
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for the
`iris.aux_factory.HybridPressureFactory` class.

"""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

import numpy as np

import iris
from iris.aux_factory import HybridPressureFactory


class Test___init__(tests.IrisTest):
    def setUp(self):
        self.delta = mock.Mock(units=iris.unit.Unit('Pa'), nbounds=0)
        self.sigma = mock.Mock(units=iris.unit.Unit('1'), nbounds=0)
        self.surface_air_pressure = mock.Mock(units=iris.unit.Unit('Pa'),
                                              nbounds=0)

    def test_insufficient_coords(self):
        with self.assertRaises(ValueError):
            HybridPressureFactory()
        with self.assertRaises(ValueError):
            HybridPressureFactory(
                delta=None, sigma=self.sigma,
                surface_air_pressure=None)
        with self.assertRaises(ValueError):
            HybridPressureFactory(
                delta=None, sigma=None,
                surface_air_pressure=self.surface_air_pressure)

    def test_incompatible_delta_units(self):
        self.delta.units = iris.unit.Unit('m')
        with self.assertRaises(ValueError):
            HybridPressureFactory(
                delta=self.delta, sigma=self.sigma,
                surface_air_pressure=self.surface_air_pressure)

    def test_incompatible_sigma_units(self):
        self.sigma.units = iris.unit.Unit('Pa')
        with self.assertRaises(ValueError):
            HybridPressureFactory(
                delta=self.delta, sigma=self.sigma,
                surface_air_pressure=self.surface_air_pressure)

    def test_incompatible_surface_air_pressure_units(self):
        self.surface_air_pressure.units = iris.unit.Unit('unknown')
        with self.assertRaises(ValueError):
            HybridPressureFactory(
                delta=self.delta, sigma=self.sigma,
                surface_air_pressure=self.surface_air_pressure)

    def test_different_pressure_units(self):
        self.delta.units = iris.unit.Unit('hPa')
        self.surface_air_pressure.units = iris.unit.Unit('Pa')
        with self.assertRaises(ValueError):
            HybridPressureFactory(
                delta=self.delta, sigma=self.sigma,
                surface_air_pressure=self.surface_air_pressure)

    def test_too_many_delta_bounds(self):
        self.delta.nbounds = 4
        with self.assertRaises(ValueError):
            HybridPressureFactory(
                delta=self.delta, sigma=self.sigma,
                surface_air_pressure=self.surface_air_pressure)

    def test_too_many_sigma_bounds(self):
        self.sigma.nbounds = 4
        with self.assertRaises(ValueError):
            HybridPressureFactory(
                delta=self.delta, sigma=self.sigma,
                surface_air_pressure=self.surface_air_pressure)

    def test_factory_metadata(self):
        factory = HybridPressureFactory(
            delta=self.delta, sigma=self.sigma,
            surface_air_pressure=self.surface_air_pressure)
        self.assertEqual(factory.standard_name, 'air_pressure')
        self.assertIsNone(factory.long_name)
        self.assertIsNone(factory.var_name)
        self.assertEqual(factory.units, self.delta.units)
        self.assertEqual(factory.units, self.surface_air_pressure.units)
        self.assertIsNone(factory.coord_system)
        self.assertEqual(factory.attributes, {})


class Test_dependencies(tests.IrisTest):
    def setUp(self):
        self.delta = mock.Mock(units=iris.unit.Unit('Pa'), nbounds=0)
        self.sigma = mock.Mock(units=iris.unit.Unit('1'), nbounds=0)
        self.surface_air_pressure = mock.Mock(units=iris.unit.Unit('Pa'),
                                              nbounds=0)

    def test_value(self):
        kwargs = dict(delta=self.delta,
                      sigma=self.sigma,
                      surface_air_pressure=self.surface_air_pressure)
        factory = HybridPressureFactory(**kwargs)
        self.assertEqual(factory.dependencies, kwargs)


class Test_make_coord(tests.IrisTest):
    @staticmethod
    def coords_dims_func(coord):
        mapping = dict(level_pressure=(0,), sigma=(0,),
                       surface_air_pressure=(1, 2))
        return mapping[coord.name()]

    def setUp(self):
        self.delta = iris.coords.DimCoord(
            [0.0, 1.0, 2.0], long_name='level_pressure', units='Pa')
        self.sigma = iris.coords.DimCoord(
            [1.0, 0.9, 0.8], long_name='sigma')
        self.surface_air_pressure = iris.coords.AuxCoord(
            np.arange(4).reshape(2, 2), 'surface_air_pressure',
            units='Pa')

    def test_points_only(self):
        # Determine expected coord by manually broadcasting coord points
        # knowing the dimension mapping.
        delta_pts = self.delta.points[..., np.newaxis, np.newaxis]
        sigma_pts = self.sigma.points[..., np.newaxis, np.newaxis]
        surf_pts = self.surface_air_pressure.points[np.newaxis, ...]
        expected_points = delta_pts + sigma_pts * surf_pts
        expected_coord = iris.coords.AuxCoord(expected_points,
                                              standard_name='air_pressure',
                                              units='Pa')
        factory = HybridPressureFactory(
            delta=self.delta, sigma=self.sigma,
            surface_air_pressure=self.surface_air_pressure)
        derived_coord = factory.make_coord(self.coords_dims_func)
        self.assertEqual(expected_coord, derived_coord)

    def test_none_delta(self):
        delta_pts = 0
        sigma_pts = self.sigma.points[..., np.newaxis, np.newaxis]
        surf_pts = self.surface_air_pressure.points[np.newaxis, ...]
        expected_points = delta_pts + sigma_pts * surf_pts
        expected_coord = iris.coords.AuxCoord(expected_points,
                                              standard_name='air_pressure',
                                              units='Pa')
        factory = HybridPressureFactory(
            sigma=self.sigma, surface_air_pressure=self.surface_air_pressure)
        derived_coord = factory.make_coord(self.coords_dims_func)
        self.assertEqual(expected_coord, derived_coord)

    def test_none_sigma(self):
        delta_pts = self.delta.points[..., np.newaxis, np.newaxis]
        sigma_pts = 0
        surf_pts = self.surface_air_pressure.points[np.newaxis, ...]
        expected_points = delta_pts + sigma_pts * surf_pts
        expected_coord = iris.coords.AuxCoord(expected_points,
                                              standard_name='air_pressure',
                                              units='Pa')
        factory = HybridPressureFactory(
            delta=self.delta, surface_air_pressure=self.surface_air_pressure)
        derived_coord = factory.make_coord(self.coords_dims_func)
        self.assertEqual(expected_coord, derived_coord)

    def test_none_surface_air_pressure(self):
        # Note absence of broadcasting as multidimensional coord
        # is not present.
        expected_points = self.delta.points
        expected_coord = iris.coords.AuxCoord(expected_points,
                                              standard_name='air_pressure',
                                              units='Pa')
        factory = HybridPressureFactory(delta=self.delta, sigma=self.sigma)
        derived_coord = factory.make_coord(self.coords_dims_func)
        self.assertEqual(expected_coord, derived_coord)

    def test_with_bounds(self):
        self.delta.guess_bounds(0)
        self.sigma.guess_bounds(0.5)
        # Determine expected coord by manually broadcasting coord points
        # and bounds based on the dimension mapping.
        delta_pts = self.delta.points[..., np.newaxis, np.newaxis]
        sigma_pts = self.sigma.points[..., np.newaxis, np.newaxis]
        surf_pts = self.surface_air_pressure.points[np.newaxis, ...]
        expected_points = delta_pts + sigma_pts * surf_pts
        delta_vals = self.delta.bounds.reshape(3, 1, 1, 2)
        sigma_vals = self.sigma.bounds.reshape(3, 1, 1, 2)
        surf_vals = self.surface_air_pressure.points.reshape(1, 2, 2, 1)
        expected_bounds = delta_vals + sigma_vals * surf_vals
        expected_coord = iris.coords.AuxCoord(expected_points,
                                              standard_name='air_pressure',
                                              units='Pa',
                                              bounds=expected_bounds)
        factory = HybridPressureFactory(
            delta=self.delta, sigma=self.sigma,
            surface_air_pressure=self.surface_air_pressure)
        derived_coord = factory.make_coord(self.coords_dims_func)
        self.assertEqual(expected_coord, derived_coord)


class Test_update(tests.IrisTest):
    def setUp(self):
        self.delta = mock.Mock(units=iris.unit.Unit('Pa'), nbounds=0)
        self.sigma = mock.Mock(units=iris.unit.Unit('1'), nbounds=0)
        self.surface_air_pressure = mock.Mock(units=iris.unit.Unit('Pa'),
                                              nbounds=0)

        self.factory = HybridPressureFactory(
            delta=self.delta, sigma=self.sigma,
            surface_air_pressure=self.surface_air_pressure)

    def test_good_delta(self):
        new_delta_coord = mock.Mock(units=iris.unit.Unit('Pa'), nbounds=0)
        self.factory.update(self.delta, new_delta_coord)
        self.assertIs(self.factory.delta, new_delta_coord)

    def test_bad_delta(self):
        new_delta_coord = mock.Mock(units=iris.unit.Unit('1'), nbounds=0)
        with self.assertRaises(ValueError):
            self.factory.update(self.delta, new_delta_coord)

    def test_alternative_bad_delta(self):
        new_delta_coord = mock.Mock(units=iris.unit.Unit('Pa'), nbounds=4)
        with self.assertRaises(ValueError):
            self.factory.update(self.delta, new_delta_coord)

    def test_good_surface_air_pressure(self):
        new_surface_p_coord = mock.Mock(units=iris.unit.Unit('Pa'), nbounds=0)
        self.factory.update(self.surface_air_pressure, new_surface_p_coord)
        self.assertIs(self.factory.surface_air_pressure, new_surface_p_coord)

    def test_bad_surface_air_pressure(self):
        new_surface_p_coord = mock.Mock(units=iris.unit.Unit('km'), nbounds=0)
        with self.assertRaises(ValueError):
            self.factory.update(self.surface_air_pressure, new_surface_p_coord)

    def test_non_dependency(self):
        old_coord = mock.Mock()
        new_coord = mock.Mock()
        orig_dependencies = self.factory.dependencies
        self.factory.update(old_coord, new_coord)
        self.assertEqual(orig_dependencies, self.factory.dependencies)

    def test_none_delta(self):
        self.factory.update(self.delta, None)
        self.assertIsNone(self.factory.delta)

    def test_none_sigma(self):
        self.factory.update(self.sigma, None)
        self.assertIsNone(self.factory.sigma)

    def test_insufficient_coords(self):
        self.factory.update(self.delta, None)
        with self.assertRaises(ValueError):
            self.factory.update(self.surface_air_pressure, None)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_OceanSigmaZFactory
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for the
`iris.aux_factory.OceanSigmaZFactory` class.

"""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

import numpy as np

import iris
from iris.aux_factory import OceanSigmaZFactory
from iris.coords import AuxCoord, DimCoord
from iris.unit import Unit


class Test___init__(tests.IrisTest):
    def setUp(self):
        self.sigma = mock.Mock(units=Unit('1'), nbounds=0)
        self.eta = mock.Mock(units=Unit('m'), nbounds=0)
        self.depth = mock.Mock(units=Unit('m'), nbounds=0)
        self.depth_c = mock.Mock(units=Unit('m'), nbounds=0, shape=(1,))
        self.nsigma = mock.Mock(units=Unit('1'), nbounds=0, shape=(1,))
        self.zlev = mock.Mock(units=Unit('m'), nbounds=0)
        self.kwargs = dict(sigma=self.sigma, eta=self.eta,
                           depth=self.depth, depth_c=self.depth_c,
                           nsigma=self.nsigma, zlev=self.zlev)

    def test_insufficient_coordinates(self):
        with self.assertRaises(ValueError):
            OceanSigmaZFactory()
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(sigma=self.sigma, eta=self.eta,
                               depth=self.depth, depth_c=self.depth_c,
                               nsigma=self.nsigma, zlev=None)
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(sigma=None, eta=None, depth=self.depth,
                               depth_c=self.depth_c, nsigma=self.nsigma,
                               zlev=self.zlev)
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(sigma=self.sigma, eta=None, depth=None,
                               depth_c=self.depth_c, nsigma=self.nsigma,
                               zlev=self.zlev)
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(sigma=self.sigma, eta=None, depth=self.depth,
                               depth_c=None, nsigma=self.nsigma,
                               zlev=self.zlev)
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(sigma=self.sigma, eta=self.eta,
                               depth=self.depth, depth_c=self.depth_c,
                               nsigma=None, zlev=self.zlev)

    def test_sigma_too_many_bounds(self):
        self.sigma.nbounds = 4
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(**self.kwargs)

    def test_zlev_too_many_bounds(self):
        self.zlev.nbounds = 4
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(**self.kwargs)

    def test_sigma_zlev_same_boundedness(self):
        self.zlev.nbounds = 2
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(**self.kwargs)

    def test_depth_c_non_scalar(self):
        self.depth_c.shape = (2,)
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(**self.kwargs)

    def test_nsigma_non_scalar(self):
        self.nsigma.shape = (4,)
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(**self.kwargs)

    def test_zlev_incompatible_units(self):
        self.zlev.units = Unit('Pa')
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(**self.kwargs)

    def test_sigma_incompatible_units(self):
        self.sigma.units = Unit('km')
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(**self.kwargs)

    def test_eta_incompatible_units(self):
        self.eta.units = Unit('km')
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(**self.kwargs)

    def test_depth_c_incompatible_units(self):
        self.depth_c.units = Unit('km')
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(**self.kwargs)

    def test_depth_incompatible_units(self):
        self.depth.units = Unit('km')
        with self.assertRaises(ValueError):
            OceanSigmaZFactory(**self.kwargs)


class Test_dependencies(tests.IrisTest):
    def setUp(self):
        self.sigma = mock.Mock(units=Unit('1'), nbounds=0)
        self.eta = mock.Mock(units=Unit('m'), nbounds=0)
        self.depth = mock.Mock(units=Unit('m'), nbounds=0)
        self.depth_c = mock.Mock(units=Unit('m'), nbounds=0, shape=(1,))
        self.nsigma = mock.Mock(units=Unit('1'), nbounds=0, shape=(1,))
        self.zlev = mock.Mock(units=Unit('m'), nbounds=0)
        self.kwargs = dict(sigma=self.sigma, eta=self.eta,
                           depth=self.depth, depth_c=self.depth_c,
                           nsigma=self.nsigma, zlev=self.zlev)

    def test_values(self):
        factory = OceanSigmaZFactory(**self.kwargs)
        self.assertEqual(factory.dependencies, self.kwargs)


class Test_make_coord(tests.IrisTest):
    @staticmethod
    def coord_dims(coord):
        mapping = dict(sigma=(0,), eta=(1, 2), depth=(1, 2),
                       depth_c=(), nsigma=(), zlev=(0,))
        return mapping[coord.name()]

    @staticmethod
    def derive(sigma, eta, depth, depth_c, nsigma, zlev, coord=True):
        nsigma_slice = slice(0, int(nsigma))
        temp = eta + sigma * (np.minimum(depth_c, depth) + eta)
        shape = temp.shape
        result = np.ones(shape, dtype=temp.dtype) * zlev
        result[nsigma_slice] = temp[nsigma_slice]
        if coord:
            name = 'sea_surface_height_above_reference_ellipsoid'
            result = AuxCoord(result,
                              standard_name=name,
                              units='m',
                              attributes=dict(positive='up'))
        return result

    def setUp(self):
        self.sigma = DimCoord(np.arange(5, dtype=np.float) * 10,
                              long_name='sigma', units='1')
        self.eta = AuxCoord(np.arange(4, dtype=np.float).reshape(2, 2),
                            long_name='eta', units='m')
        self.depth = AuxCoord(np.arange(4, dtype=np.float).reshape(2, 2) * 10,
                              long_name='depth', units='m')
        self.depth_c = AuxCoord([15], long_name='depth_c', units='m')
        self.nsigma = AuxCoord([3], long_name='nsigma')
        self.zlev = DimCoord(np.arange(5, dtype=np.float) * 10,
                             long_name='zlev', units='m')
        self.kwargs = dict(sigma=self.sigma, eta=self.eta, depth=self.depth,
                           depth_c=self.depth_c, nsigma=self.nsigma,
                           zlev=self.zlev)

    def test_derived_points(self):
        # Broadcast expected points given the known dimensional mapping.
        sigma = self.sigma.points[..., np.newaxis, np.newaxis]
        eta = self.eta.points[np.newaxis, ...]
        depth = self.depth.points[np.newaxis, ...]
        depth_c = self.depth_c.points
        nsigma = self.nsigma.points
        zlev = self.zlev.points[..., np.newaxis, np.newaxis]
        # Calculate the expected result.
        expected_coord = self.derive(sigma, eta, depth, depth_c, nsigma, zlev)
        # Calculate the actual result.
        factory = OceanSigmaZFactory(**self.kwargs)
        coord = factory.make_coord(self.coord_dims)
        self.assertEqual(expected_coord, coord)

    def test_derived_points_with_bounds(self):
        self.sigma.guess_bounds()
        self.zlev.guess_bounds()
        # Broadcast expected points given the known dimensional mapping.
        sigma = self.sigma.points[..., np.newaxis, np.newaxis]
        eta = self.eta.points[np.newaxis, ...]
        depth = self.depth.points[np.newaxis, ...]
        depth_c = self.depth_c.points
        nsigma = self.nsigma.points
        zlev = self.zlev.points[..., np.newaxis, np.newaxis]
        # Calculate the expected coordinate with points.
        expected_coord = self.derive(sigma, eta, depth, depth_c, nsigma, zlev)
        # Broadcast expected bounds given the known dimensional mapping.
        sigma = self.sigma.bounds.reshape(sigma.shape + (2,))
        eta = self.eta.points.reshape(eta.shape + (1,))
        depth = self.depth.points.reshape(depth.shape + (1,))
        depth_c = self.depth_c.points.reshape(depth_c.shape + (1,))
        nsigma = self.nsigma.points.reshape(nsigma.shape + (1,))
        zlev = self.zlev.bounds.reshape(zlev.shape + (2,))
        # Calculate the expected bounds.
        bounds = self.derive(sigma, eta, depth, depth_c, nsigma, zlev,
                             coord=False)
        expected_coord.bounds = bounds
        # Calculate the actual result.
        factory = OceanSigmaZFactory(**self.kwargs)
        coord = factory.make_coord(self.coord_dims)
        self.assertEqual(expected_coord, coord)

    def test_no_eta(self):
        # Broadcast expected points given the known dimensional mapping.
        sigma = self.sigma.points[..., np.newaxis, np.newaxis]
        eta = 0
        depth = self.depth.points[np.newaxis, ...]
        depth_c = self.depth_c.points
        nsigma = self.nsigma.points
        zlev = self.zlev.points[..., np.newaxis, np.newaxis]
        # Calculate the expected result.
        expected_coord = self.derive(sigma, eta, depth, depth_c, nsigma, zlev)
        # Calculate the actual result.
        self.kwargs['eta'] = None
        factory = OceanSigmaZFactory(**self.kwargs)
        coord = factory.make_coord(self.coord_dims)
        self.assertEqual(expected_coord, coord)

    def test_no_sigma(self):
        # Broadcast expected points given the known dimensional mapping.
        sigma = 0
        eta = self.eta.points[np.newaxis, ...]
        depth = self.depth.points[np.newaxis, ...]
        depth_c = self.depth_c.points
        nsigma = self.nsigma.points
        zlev = self.zlev.points[..., np.newaxis, np.newaxis]
        # Calculate the expected result.
        expected_coord = self.derive(sigma, eta, depth, depth_c, nsigma, zlev)
        # Calculate the actual result.
        self.kwargs['sigma'] = None
        factory = OceanSigmaZFactory(**self.kwargs)
        coord = factory.make_coord(self.coord_dims)
        self.assertEqual(expected_coord, coord)

    def test_no_depth_c(self):
        # Broadcast expected points given the known dimensional mapping.
        sigma = self.sigma.points[..., np.newaxis, np.newaxis]
        eta = self.eta.points[np.newaxis, ...]
        depth = self.depth.points[np.newaxis, ...]
        depth_c = 0
        nsigma = self.nsigma.points
        zlev = self.zlev.points[..., np.newaxis, np.newaxis]
        # Calculate the expected result.
        expected_coord = self.derive(sigma, eta, depth, depth_c, nsigma, zlev)
        # Calculate the actual result.
        self.kwargs['depth_c'] = None
        factory = OceanSigmaZFactory(**self.kwargs)
        coord = factory.make_coord(self.coord_dims)
        self.assertEqual(expected_coord, coord)

    def test_no_depth(self):
        # Broadcast expected points given the known dimensional mapping.
        sigma = self.sigma.points[..., np.newaxis, np.newaxis]
        eta = self.eta.points[np.newaxis, ...]
        depth = 0
        depth_c = self.depth_c.points
        nsigma = self.nsigma.points
        zlev = self.zlev.points[..., np.newaxis, np.newaxis]
        # Calculate the expected result.
        expected_coord = self.derive(sigma, eta, depth, depth_c, nsigma, zlev)
        # Calculate the actual result.
        self.kwargs['depth'] = None
        factory = OceanSigmaZFactory(**self.kwargs)
        coord = factory.make_coord(self.coord_dims)
        self.assertEqual(expected_coord, coord)


class Test_update(tests.IrisTest):
    def setUp(self):
        self.sigma = mock.Mock(units=Unit('1'), nbounds=0)
        self.eta = mock.Mock(units=Unit('m'), nbounds=0)
        self.depth = mock.Mock(units=Unit('m'), nbounds=0)
        self.depth_c = mock.Mock(units=Unit('m'), nbounds=0, shape=(1,))
        self.nsigma = mock.Mock(units=Unit('1'), nbounds=0, shape=(1,))
        self.zlev = mock.Mock(units=Unit('m'), nbounds=0)
        self.kwargs = dict(sigma=self.sigma, eta=self.eta,
                           depth=self.depth, depth_c=self.depth_c,
                           nsigma=self.nsigma, zlev=self.zlev)
        self.factory = OceanSigmaZFactory(**self.kwargs)

    def test_sigma(self):
        new_sigma = mock.Mock(units=Unit('1'), nbounds=0)
        self.factory.update(self.sigma, new_sigma)
        self.assertIs(self.factory.sigma, new_sigma)

    def test_sigma_too_many_bounds(self):
        new_sigma = mock.Mock(units=Unit('1'), nbounds=4)
        with self.assertRaises(ValueError):
            self.factory.update(self.sigma, new_sigma)

    def test_sigma_zlev_same_boundedness(self):
        new_sigma = mock.Mock(units=Unit('1'), nbounds=2)
        with self.assertRaises(ValueError):
            self.factory.update(self.sigma, new_sigma)

    def test_sigma_incompatible_units(self):
        new_sigma = mock.Mock(units=Unit('Pa'), nbounds=0)
        with self.assertRaises(ValueError):
            self.factory.update(self.sigma, new_sigma)

    def test_eta(self):
        new_eta = mock.Mock(units=Unit('m'), nbounds=0)
        self.factory.update(self.eta, new_eta)
        self.assertIs(self.factory.eta, new_eta)

    def test_eta_incompatible_units(self):
        new_eta = mock.Mock(units=Unit('Pa'), nbounds=0)
        with self.assertRaises(ValueError):
            self.factory.update(self.eta, new_eta)

    def test_depth(self):
        new_depth = mock.Mock(units=Unit('m'), nbounds=0)
        self.factory.update(self.depth, new_depth)
        self.assertIs(self.factory.depth, new_depth)

    def test_depth_incompatible_units(self):
        new_depth = mock.Mock(units=Unit('Pa'), nbounds=0)
        with self.assertRaises(ValueError):
            self.factory.update(self.depth, new_depth)

    def test_depth_c(self):
        new_depth_c = mock.Mock(units=Unit('m'), nbounds=0, shape=(1,))
        self.factory.update(self.depth_c, new_depth_c)
        self.assertIs(self.factory.depth_c, new_depth_c)

    def test_depth_c_non_scalar(self):
        new_depth_c = mock.Mock(units=Unit('m'), nbounds=0, shape=(10,))
        with self.assertRaises(ValueError):
            self.factory.update(self.depth_c, new_depth_c)

    def test_depth_c_incompatible_units(self):
        new_depth_c = mock.Mock(units=Unit('Pa'), nbounds=0, shape=(1,))
        with self.assertRaises(ValueError):
            self.factory.update(self.depth_c, new_depth_c)

    def test_nsigma(self):
        new_nsigma = mock.Mock(units=Unit('1'), nbounds=0, shape=(1,))
        self.factory.update(self.nsigma, new_nsigma)
        self.assertIs(self.factory.nsigma, new_nsigma)

    def test_nsigma_missing(self):
        with self.assertRaises(ValueError):
            self.factory.update(self.nsigma, None)

    def test_nsigma_non_scalar(self):
        new_nsigma = mock.Mock(units=Unit('1'), nbounds=0, shape=(10,))
        with self.assertRaises(ValueError):
            self.factory.update(self.nsigma, new_nsigma)

    def test_zlev(self):
        new_zlev = mock.Mock(units=Unit('m'), nbounds=0)
        self.factory.update(self.zlev, new_zlev)
        self.assertIs(self.factory.zlev, new_zlev)

    def test_zlev_missing(self):
        with self.assertRaises(ValueError):
            self.factory.update(self.zlev, None)

    def test_zlev_too_many_bounds(self):
        new_zlev = mock.Mock(units=Unit('m'), nbounds=4)
        with self.assertRaises(ValueError):
            self.factory.update(self.zlev, new_zlev)

    def test_zlev_same_boundedness(self):
        new_zlev = mock.Mock(units=Unit('m'), nbounds=2)
        with self.assertRaises(ValueError):
            self.factory.update(self.zlev, new_zlev)

    def test_zlev_incompatible_units(self):
        new_zlev = new_zlev = mock.Mock(units=Unit('Pa'), nbounds=0)
        with self.assertRaises(ValueError):
            self.factory.update(self.zlev, new_zlev)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_concatenate
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Test function :func:`iris._concatenate.concatenate.py`."""

# import iris tests first so that some things can be initialised
# before importing anything else.
import iris.tests as tests

import numpy as np

import iris.coords
from iris._concatenate import concatenate
import iris.cube
import iris.unit


class Test_concatenate__epoch(tests.IrisTest):
    def simple_1d_time_cubes(self, reftimes, coords_points):
        cubes = []
        data_points = [273, 275, 278, 277, 274]
        for reftime, coord_points in zip(reftimes, coords_points):
            cube = iris.cube.Cube(np.array(data_points, dtype=np.float32),
                                  standard_name='air_temperature',
                                  units='K')
            unit = iris.unit.Unit(reftime, calendar='gregorian')
            coord = iris.coords.DimCoord(points=np.array(coord_points,
                                                         dtype=np.float32),
                                         standard_name='time',
                                         units=unit)
            cube.add_dim_coord(coord, 0)
            cubes.append(cube)
        return cubes

    def test_concat_1d_with_same_time_units(self):
        reftimes = ['hours since 1970-01-01 00:00:00',
                    'hours since 1970-01-01 00:00:00']
        coords_points = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]
        cubes = self.simple_1d_time_cubes(reftimes, coords_points)
        result = concatenate(cubes)
        self.assertEqual(len(result), 1)
        self.assertEqual(result[0].shape, (10,))


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_Cell
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :class:`iris.coords.Cell` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import datetime

import mock
import netcdftime
import numpy as np

from iris.coords import Cell
from iris.time import PartialDateTime


class Test___common_cmp__(tests.IrisTest):
    def assert_raises_on_comparison(self, cell, other, exception_type, regexp):
        with self.assertRaisesRegexp(exception_type, regexp):
            cell < other
        with self.assertRaisesRegexp(exception_type, regexp):
            cell <= other
        with self.assertRaisesRegexp(exception_type, regexp):
            cell > other
        with self.assertRaisesRegexp(exception_type, regexp):
            cell >= other

    def test_netcdftime_cell(self):
        # Check that cell comparison when the cell contains
        # netcdftime.datetime objects raises an exception otherwise
        # this will fall back to id comparison producing unreliable
        # results.
        cell = Cell(netcdftime.datetime(2010, 3, 21))
        dt = mock.Mock(timetuple=mock.Mock())
        self.assert_raises_on_comparison(cell, dt, TypeError,
                                         'determine the order of netcdftime')
        self.assert_raises_on_comparison(cell, 23, TypeError,
                                         'determine the order of netcdftime')
        self.assert_raises_on_comparison(cell, 'hello', TypeError,
                                         'Unexpected type.*str')

    def test_netcdftime_other(self):
        # Check that cell comparison to a netcdftime.datetime object
        # raises an exception otherwise this will fall back to id comparison
        # producing unreliable results.
        dt = netcdftime.datetime(2010, 3, 21)
        cell = Cell(mock.Mock(timetuple=mock.Mock()))
        self.assert_raises_on_comparison(cell, dt, TypeError,
                                         'determine the order of netcdftime')

    def test_PartialDateTime_bounded_cell(self):
        # Check that bounded comparisions to a PartialDateTime
        # raise an exception. These are not supported as they
        # depend on the calendar.
        dt = PartialDateTime(month=6)
        cell = Cell(datetime.datetime(2010, 1, 1),
                    bound=[datetime.datetime(2010, 1, 1),
                           datetime.datetime(2011, 1, 1)])
        self.assert_raises_on_comparison(cell, dt, TypeError,
                                         'bounded region for datetime')

    def test_PartialDateTime_unbounded_cell(self):
        # Check that cell comparison works with PartialDateTimes.
        dt = PartialDateTime(month=6)
        cell = Cell(netcdftime.datetime(2010, 3, 1))
        self.assertLess(cell, dt)
        self.assertGreater(dt, cell)
        self.assertLessEqual(cell, dt)
        self.assertGreaterEqual(dt, cell)

    def test_datetime_unbounded_cell(self):
        # Check that cell comparison works with datetimes.
        dt = datetime.datetime(2000, 6, 15)
        cell = Cell(datetime.datetime(2000, 1, 1))
        # Note the absence of the inverse of these
        # e.g. self.assertGreater(dt, cell).
        # See http://bugs.python.org/issue8005
        self.assertLess(cell, dt)
        self.assertLessEqual(cell, dt)


class Test___eq__(tests.IrisTest):
    def test_datetimelike(self):
        # Check that cell equality works with objects with a "timetuple".
        dt = mock.Mock(timetuple=mock.Mock())
        cell = mock.MagicMock(spec=Cell, point=datetime.datetime(2010, 3, 21),
                              bound=None)
        _ = cell == dt
        cell.__eq__.assert_called_once_with(dt)

    def test_datetimelike_bounded_cell(self):
        # Check that equality with a datetime-like bounded cell
        # raises an error. This is not supported as it
        # depends on the calendar which is not always known from
        # the datetime-like bound objects.
        other = mock.Mock(timetuple=mock.Mock())
        cell = Cell(point=object(),
                    bound=[mock.Mock(timetuple=mock.Mock()),
                           mock.Mock(timetuple=mock.Mock())])
        with self.assertRaisesRegexp(TypeError, 'bounded region for datetime'):
            cell == other

    def test_PartialDateTime_other(self):
        cell = Cell(datetime.datetime(2010, 3, 2))
        # A few simple cases.
        self.assertEqual(cell, PartialDateTime(month=3))
        self.assertNotEqual(cell, PartialDateTime(month=3, hour=12))
        self.assertNotEqual(cell, PartialDateTime(month=4))


class Test_contains_point(tests.IrisTest):
    def test_datetimelike_bounded_cell(self):
        point = object()
        cell = Cell(point=object(),
                    bound=[mock.Mock(timetuple=mock.Mock()),
                           mock.Mock(timetuple=mock.Mock())])
        with self.assertRaisesRegexp(TypeError, 'bounded region for datetime'):
            cell.contains_point(point)

    def test_datetimelike_point(self):
        point = mock.Mock(timetuple=mock.Mock())
        cell = Cell(point=object(), bound=[object(), object()])
        with self.assertRaisesRegexp(TypeError, 'bounded region for datetime'):
            cell.contains_point(point)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_Coord
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :class:`iris.coords.Coord` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import datetime
import collections

import mock
import numpy as np

from iris.coords import DimCoord, AuxCoord, Coord


Pair = collections.namedtuple('Pair', 'points bounds')


class Test_cell(tests.IrisTest):
    def _mock_coord(self):
        coord = mock.Mock(spec=Coord, ndim=1,
                          points=np.array([mock.sentinel.time]),
                          bounds=np.array([[mock.sentinel.lower,
                                            mock.sentinel.upper]]))
        return coord

    def test_time_as_number(self):
        # Make sure Coord.cell() normally returns the values straight
        # out of the Coord's points/bounds arrays.
        coord = self._mock_coord()
        cell = Coord.cell(coord, 0)
        self.assertIs(cell.point, mock.sentinel.time)
        self.assertEquals(cell.bound,
                          (mock.sentinel.lower, mock.sentinel.upper))

    def test_time_as_object(self):
        # When iris.FUTURE.cell_datetime_objects is True, ensure
        # Coord.cell() converts the point/bound values to "datetime"
        # objects.
        coord = self._mock_coord()
        coord.units.num2date = mock.Mock(
            side_effect=[mock.sentinel.datetime,
                         (mock.sentinel.datetime_lower,
                          mock.sentinel.datetime_upper)])
        with mock.patch('iris.FUTURE', cell_datetime_objects=True):
            cell = Coord.cell(coord, 0)
        self.assertIs(cell.point, mock.sentinel.datetime)
        self.assertEquals(cell.bound,
                          (mock.sentinel.datetime_lower,
                           mock.sentinel.datetime_upper))
        self.assertEqual(coord.units.num2date.call_args_list,
                         [mock.call((mock.sentinel.time,)),
                          mock.call((mock.sentinel.lower,
                                     mock.sentinel.upper))])


class Test_collapsed(tests.IrisTest):

    def test_serialize(self):
        # Collapse a string AuxCoord, causing it to be serialised.
        string = Pair(np.array(['two', 'four', 'six', 'eight']),
                      np.array([['one', 'three'],
                                ['three', 'five'],
                                ['five', 'seven'],
                                ['seven', 'nine']]))
        string_multi = Pair(np.array(['three', 'six', 'nine']),
                            np.array([['one', 'two', 'four', 'five'],
                                      ['four', 'five', 'seven', 'eight'],
                                      ['seven', 'eight', 'ten', 'eleven']]))

        def _serialize(data):
            return '|'.join(str(item) for item in data.flatten())

        for units in ['unknown', 'no_unit']:
            for points, bounds in [string, string_multi]:
                coord = AuxCoord(points=points, bounds=bounds, units=units)
                collapsed_coord = coord.collapsed()
                self.assertArrayEqual(collapsed_coord.points,
                                      _serialize(points))
                for index in np.ndindex(bounds.shape[1:]):
                    index_slice = (slice(None),) + tuple(index)
                    self.assertArrayEqual(collapsed_coord.bounds[index_slice],
                                          _serialize(bounds[index_slice]))

    def test_dim_1d(self):
        # Numeric coords should not be serialised.
        coord = DimCoord(points=np.array([2, 4, 6, 8]),
                         bounds=np.array([[1, 3], [3, 5], [5, 7], [7, 9]]))
        for units in ['unknown', 'no_unit', 1, 'K']:
            coord.units = units
            collapsed_coord = coord.collapsed()
            self.assertArrayEqual(collapsed_coord.points,
                                  np.mean(coord.points))
            self.assertArrayEqual(collapsed_coord.bounds,
                                  [[coord.bounds.min(), coord.bounds.max()]])

    def test_numeric_nd(self):
        # Contiguous only defined for 2d bounds.
        coord = AuxCoord(points=np.array([3, 6, 9]),
                         bounds=np.array([[1, 2, 4, 5],
                                          [4, 5, 7, 8],
                                          [7, 8, 10, 11]]))
        with self.assertRaises(ValueError):
            collapsed_coord = coord.collapsed()


class Test_is_compatible(tests.IrisTest):
    def setUp(self):
        self.test_coord = AuxCoord([1.])
        self.other_coord = self.test_coord.copy()

    def test_noncommon_array_attrs_compatible(self):
        # Non-common array attributes should be ok.
        self.test_coord.attributes['array_test'] = np.array([1.0, 2, 3])
        self.assertTrue(self.test_coord.is_compatible(self.other_coord))

    def test_matching_array_attrs_compatible(self):
        # Matching array attributes should be ok.
        self.test_coord.attributes['array_test'] = np.array([1.0, 2, 3])
        self.other_coord.attributes['array_test'] = np.array([1.0, 2, 3])
        self.assertTrue(self.test_coord.is_compatible(self.other_coord))

    def test_different_array_attrs_incompatible(self):
        # Differing array attributes should make coords incompatible.
        self.test_coord.attributes['array_test'] = np.array([1.0, 2, 3])
        self.other_coord.attributes['array_test'] = np.array([1.0, 2, 777.7])
        self.assertFalse(self.test_coord.is_compatible(self.other_coord))


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_add_categorised_coord
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Test function :func:`iris.coord_categorisation.add_categorised_coord`."""


# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import mock
import numpy as np

from iris.coord_categorisation import add_categorised_coord
from iris.coord_categorisation import add_day_of_year
from iris.cube import Cube
from iris.coords import DimCoord
from iris.unit import CALENDARS as calendars
from iris.unit import Unit


class Test_add_categorised_coord(tests.IrisTest):

    def setUp(self):
        # Factor out common variables and objects.
        self.cube = mock.Mock(name='cube', coords=mock.Mock(return_value=[]))
        self.coord = mock.Mock(name='coord',
                               points=np.arange(12).reshape(3, 4))
        self.units = 'units'
        self.vectorised = mock.Mock(name='vectorized_result')

    def test_vectorise_call(self):
        # Check that the function being passed through gets called with
        # numpy.vectorize, before being applied to the points array.
        # The reason we use numpy.vectorize is to support multi-dimensional
        # coordinate points.
        fn = lambda coord, v: v**2

        with mock.patch('numpy.vectorize',
                        return_value=self.vectorised) as vectorise_patch:
            with mock.patch('iris.coords.AuxCoord') as aux_coord_constructor:
                add_categorised_coord(self.cube, 'foobar', self.coord, fn,
                                      units=self.units)

        # Check the constructor of AuxCoord gets called with the
        # appropriate arguments.
        # Start with the vectorised function.
        vectorise_patch.assert_called_once_with(fn)
        # Check the vectorize wrapper gets called with the appropriate args.
        self.vectorised.assert_called_once_with(self.coord, self.coord.points)
        # Check the AuxCoord constructor itself.
        aux_coord_constructor.assert_called_once_with(
            self.vectorised(self.coord, self.coord.points),
            units=self.units,
            attributes=self.coord.attributes.copy())
        # And check adding the aux coord to the cube mock.
        self.cube.add_aux_coord.assert_called_once_with(
            aux_coord_constructor(), self.cube.coord_dims(self.coord))

    def test_string_vectorised(self):
        # Check that special case handling of a vectorized string returning
        # function is taking place.
        fn = lambda coord, v: '0123456789'[:v]

        with mock.patch('numpy.vectorize',
                        return_value=self.vectorised) as vectorise_patch:
            with mock.patch('iris.coords.AuxCoord') as aux_coord_constructor:
                add_categorised_coord(self.cube, 'foobar', self.coord, fn,
                                      units=self.units)

        self.assertEqual(
            aux_coord_constructor.call_args[0][0],
            vectorise_patch(fn, otypes=[object])(self.coord, self.coord.points)
            .astype('|S64'))


class Test_add_day_of_year(tests.IrisTest):
    def setUp(self):
        self.expected = {
            'standard': np.array(range(360, 367) + range(1, 4)),
            'gregorian': np.array(range(360, 367) + range(1, 4)),
            'proleptic_gregorian': np.array(range(360, 367) + range(1, 4)),
            'noleap': np.array(range(359, 366) + range(1, 4)),
            'julian': np.array(range(360, 367) + range(1, 4)),
            'all_leap': np.array(range(360, 367) + range(1, 4)),
            '365_day': np.array(range(359, 366) + range(1, 4)),
            '366_day': np.array(range(360, 367) + range(1, 4)),
            '360_day': np.array(range(355, 361) + range(1, 5))}

    def make_cube(self, calendar):
        n_times = 10
        cube = Cube(np.arange(n_times))
        time_coord = DimCoord(np.arange(n_times), standard_name='time',
                              units=Unit('days since 1980-12-25',
                                         calendar=calendar))
        cube.add_dim_coord(time_coord, 0)
        return cube

    def test_calendars(self):
        for calendar in calendars:
            cube = self.make_cube(calendar)
            add_day_of_year(cube, 'time')
            points = cube.coord('day_of_year').points
            expected_points = self.expected[calendar]
            msg = 'Test failed for the following calendar: {}.'
            self.assertArrayEqual(points, expected_points,
                                  err_msg=msg.format(calendar))


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_Orthographic
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :class:`iris.coord_systems.Orthographic` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import cartopy.crs
from iris.coord_systems import GeogCS, Orthographic


class Test_as_cartopy_crs(tests.IrisTest):
    def setUp(self):
        self.latitude_of_projection_origin = 0.0
        self.longitude_of_projection_origin = 0.0
        self.semi_major_axis = 6377563.396
        self.semi_minor_axis = 6356256.909
        self.ellipsoid = GeogCS(self.semi_major_axis, self.semi_minor_axis)
        self.ortho_cs = Orthographic(self.latitude_of_projection_origin,
                                     self.longitude_of_projection_origin,
                                     ellipsoid=self.ellipsoid)

    def test_crs_creation(self):
        res = self.ortho_cs.as_cartopy_crs()
        globe = cartopy.crs.Globe(semimajor_axis=self.semi_major_axis,
                                  semiminor_axis=self.semi_minor_axis,
                                  ellipse=None)
        expected = cartopy.crs.Orthographic(
            self.latitude_of_projection_origin,
            self.longitude_of_projection_origin,
            globe=globe)
        self.assertEqual(res, expected)


class Test_as_cartopy_projection(tests.IrisTest):
    def setUp(self):
        self.latitude_of_projection_origin = 0.0
        self.longitude_of_projection_origin = 0.0
        self.semi_major_axis = 6377563.396
        self.semi_minor_axis = 6356256.909
        self.ellipsoid = GeogCS(self.semi_major_axis, self.semi_minor_axis)
        self.ortho_cs = Orthographic(self.latitude_of_projection_origin,
                                     self.longitude_of_projection_origin,
                                     ellipsoid=self.ellipsoid)

    def test_projection_creation(self):
        res = self.ortho_cs.as_cartopy_projection()
        globe = cartopy.crs.Globe(semimajor_axis=self.semi_major_axis,
                                  semiminor_axis=self.semi_minor_axis,
                                  ellipse=None)
        expected = cartopy.crs.Orthographic(
            self.latitude_of_projection_origin,
            self.longitude_of_projection_origin,
            globe=globe)
        self.assertEqual(res, expected)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_VerticalPerspective
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :class:`iris.coord_systems.VerticalPerspective` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import cartopy.crs
from iris.coord_systems import GeogCS, VerticalPerspective


class Test_cartopy_crs(tests.IrisTest):
    def setUp(self):
        self.latitude_of_projection_origin = 0.0
        self.longitude_of_projection_origin = 0.0
        self.semi_major_axis = 6377563.396
        self.semi_minor_axis = 6356256.909
        self.perspective_point_height = 38204820000.0
        self.ellipsoid = GeogCS(self.semi_major_axis, self.semi_minor_axis)
        self.vp_cs = VerticalPerspective(self.latitude_of_projection_origin,
                                         self.longitude_of_projection_origin,
                                         self.perspective_point_height,
                                         ellipsoid=self.ellipsoid)

    def test_crs_creation(self):
        res = self.vp_cs.as_cartopy_crs()
        globe = cartopy.crs.Globe(semimajor_axis=self.semi_major_axis,
                                  semiminor_axis=self.semi_minor_axis,
                                  ellipse=None)
        expected = cartopy.crs.Geostationary(
            self.longitude_of_projection_origin,
            self.perspective_point_height,
            globe=globe)
        self.assertEqual(res, expected)


class Test_cartopy_projection(tests.IrisTest):
    def setUp(self):
        self.latitude_of_projection_origin = 0.0
        self.longitude_of_projection_origin = 0.0
        self.semi_major_axis = 6377563.396
        self.semi_minor_axis = 6356256.909
        self.perspective_point_height = 38204820000.0
        self.ellipsoid = GeogCS(self.semi_major_axis, self.semi_minor_axis)
        self.vp_cs = VerticalPerspective(self.latitude_of_projection_origin,
                                         self.longitude_of_projection_origin,
                                         self.perspective_point_height,
                                         ellipsoid=self.ellipsoid)

    def test_projection_creation(self):
        res = self.vp_cs.as_cartopy_projection()
        globe = cartopy.crs.Globe(semimajor_axis=self.semi_major_axis,
                                  semiminor_axis=self.semi_minor_axis,
                                  ellipse=None)
        expected = cartopy.crs.Geostationary(
            self.longitude_of_projection_origin,
            self.perspective_point_height,
            globe=globe)
        self.assertEqual(res, expected)


class Test_non_zero_lat(tests.IrisTest):
    def setUp(self):
        self.latitude_of_projection_origin = 22.0
        self.longitude_of_projection_origin = 11.0
        self.semi_major_axis = 6377563.396
        self.semi_minor_axis = 6356256.909
        self.perspective_point_height = 38204820000.0
        self.ellipsoid = GeogCS(self.semi_major_axis, self.semi_minor_axis)

    def test_lat(self):
        with self.assertRaises(ValueError):
            res = VerticalPerspective(self.latitude_of_projection_origin,
                                      self.longitude_of_projection_origin,
                                      self.perspective_point_height,
                                      ellipsoid=self.ellipsoid)

if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_Cube
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.cube.Cube` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import biggus
import mock
import numpy as np

import iris.aux_factory
import iris.coords
import iris.exceptions
from iris import FUTURE
from iris.analysis import WeightedAggregator, Aggregator
from iris.analysis import MEAN
from iris.cube import Cube
from iris.coords import AuxCoord, DimCoord
from iris.exceptions import LazyAggregatorError
import iris.tests.stock as stock


class Test___init___data(tests.IrisTest):
    def test_ndarray(self):
        # np.ndarray should be allowed through
        data = np.arange(12).reshape(3, 4)
        cube = Cube(data)
        self.assertEqual(type(cube.data), np.ndarray)
        self.assertArrayEqual(cube.data, data)

    def test_masked(self):
        # np.ma.MaskedArray should be allowed through
        data = np.ma.masked_greater(np.arange(12).reshape(3, 4), 1)
        cube = Cube(data)
        self.assertEqual(type(cube.data), np.ma.MaskedArray)
        self.assertMaskedArrayEqual(cube.data, data)

    def test_matrix(self):
        # Subclasses of np.ndarray should be coerced back to np.ndarray.
        # (Except for np.ma.MaskedArray.)
        data = np.matrix([[1, 2, 3], [4, 5, 6]])
        cube = Cube(data)
        self.assertEqual(type(cube.data), np.ndarray)
        self.assertArrayEqual(cube.data, data)


class Test_xml(tests.IrisTest):
    def test_checksum_ignores_masked_values(self):
        # Mask out an single element.
        data = np.ma.arange(12).reshape(3, 4)
        data[1, 2] = np.ma.masked
        cube = Cube(data)
        self.assertCML(cube)

        # If we change the underlying value before masking it, the
        # checksum should be unaffected.
        data = np.ma.arange(12).reshape(3, 4)
        data[1, 2] = 42
        data[1, 2] = np.ma.masked
        cube = Cube(data)
        self.assertCML(cube)

    def test_byteorder_default(self):
        cube = Cube(np.arange(3))
        self.assertIn('byteorder', cube.xml())

    def test_byteorder_false(self):
        cube = Cube(np.arange(3))
        self.assertNotIn('byteorder', cube.xml(byteorder=False))

    def test_byteorder_true(self):
        cube = Cube(np.arange(3))
        self.assertIn('byteorder', cube.xml(byteorder=True))


class Test_collapsed__lazy(tests.IrisTest):
    def setUp(self):
        self.data = np.arange(6.0).reshape((2, 3))
        self.lazydata = biggus.NumpyArrayAdapter(self.data)
        cube = Cube(self.lazydata)
        for i_dim, name in enumerate(('y', 'x')):
            npts = cube.shape[i_dim]
            coord = DimCoord(np.arange(npts), long_name=name)
            cube.add_dim_coord(coord, i_dim)
        self.cube = cube

    def test_dim0_lazy(self):
        cube_collapsed = self.cube.collapsed('y', MEAN, lazy=True)
        self.assertTrue(cube_collapsed.has_lazy_data())
        self.assertArrayAlmostEqual(cube_collapsed.data, [1.5, 2.5, 3.5])
        self.assertFalse(cube_collapsed.has_lazy_data())

    def test_dim1_lazy(self):
        cube_collapsed = self.cube.collapsed('x', MEAN, lazy=True)
        self.assertTrue(cube_collapsed.has_lazy_data())
        self.assertArrayAlmostEqual(cube_collapsed.data, [1.0, 4.0])
        self.assertFalse(cube_collapsed.has_lazy_data())

    def test_fail_multidims(self):
        # Check that MEAN produces a suitable error message for multiple dims.
        # N.B. non-lazy op can do this
        with self.assertRaises(AssertionError) as err:
            cube_collapsed = self.cube.collapsed(('x', 'y'), MEAN, lazy=True)

    def test_fail_no_lazy(self):
        dummy_agg = Aggregator('custom_op', lambda x: 1)
        with self.assertRaises(LazyAggregatorError) as err:
            cube_collapsed = self.cube.collapsed('x', dummy_agg, lazy=True)
        msg = err.exception.message
        self.assertIn('custom_op', msg)
        self.assertIn('lazy', msg)
        self.assertIn('not support', msg)


class Test_collapsed__warning(tests.IrisTest):
    def setUp(self):
        self.cube = Cube([[1, 2], [1, 2]])
        lat = DimCoord([1, 2], standard_name='latitude')
        lon = DimCoord([1, 2], standard_name='longitude')
        grid_lat = AuxCoord([1, 2], standard_name='grid_latitude')
        grid_lon = AuxCoord([1, 2], standard_name='grid_longitude')
        wibble = AuxCoord([1, 2], long_name='wibble')

        self.cube.add_dim_coord(lat, 0)
        self.cube.add_dim_coord(lon, 1)
        self.cube.add_aux_coord(grid_lat, 0)
        self.cube.add_aux_coord(grid_lon, 1)
        self.cube.add_aux_coord(wibble, 1)

    def _aggregator(self, uses_weighting):
        # Returns a mock aggregator with a mocked method (uses_weighting)
        # which returns the given True/False condition.
        aggregator = mock.Mock(spec=WeightedAggregator)
        aggregator.cell_method = None
        aggregator.uses_weighting = mock.Mock(return_value=uses_weighting)

        return aggregator

    def _assert_warn_collapse_without_weight(self, coords, warn):
        # Ensure that warning is raised.
        msg = "Collapsing spatial coordinate {!r} without weighting"
        for coord in coords:
            self.assertIn(mock.call(msg.format(coord)), warn.call_args_list)

    def _assert_nowarn_collapse_without_weight(self, coords, warn):
        # Ensure that warning is not rised.
        msg = "Collapsing spatial coordinate {!r} without weighting"
        for coord in coords:
            self.assertNotIn(mock.call(msg.format(coord)), warn.call_args_list)

    def test_lat_lon_noweighted_aggregator(self):
        # Collapse latitude coordinate with unweighted aggregator.
        aggregator = mock.Mock(spec=Aggregator)
        aggregator.cell_method = None
        coords = ['latitude', 'longitude']

        with mock.patch('warnings.warn') as warn:
            self.cube.collapsed(coords, aggregator, somekeyword='bla')

        self._assert_nowarn_collapse_without_weight(coords, warn)

    def test_lat_lon_weighted_aggregator(self):
        # Collapse latitude coordinate with weighted aggregator without
        # providing weights.
        aggregator = self._aggregator(False)
        coords = ['latitude', 'longitude']

        with mock.patch('warnings.warn') as warn:
            self.cube.collapsed(coords, aggregator)

        coords = filter(lambda coord: 'latitude' in coord, coords)
        self._assert_warn_collapse_without_weight(coords, warn)

    def test_lat_lon_weighted_aggregator_with_weights(self):
        # Collapse latitude coordinate with a weighted aggregators and
        # providing suitable weights.
        weights = np.array([[0.1, 0.5], [0.3, 0.2]])
        aggregator = self._aggregator(True)
        coords = ['latitude', 'longitude']

        with mock.patch('warnings.warn') as warn:
            self.cube.collapsed(coords, aggregator, weights=weights)

        self._assert_nowarn_collapse_without_weight(coords, warn)

    def test_lat_lon_weighted_aggregator_alt(self):
        # Collapse grid_latitude coordinate with weighted aggregator without
        # providing weights.  Tests coordinate matching logic.
        aggregator = self._aggregator(False)
        coords = ['grid_latitude', 'grid_longitude']

        with mock.patch('warnings.warn') as warn:
            self.cube.collapsed(coords, aggregator)

        coords = filter(lambda coord: 'latitude' in coord, coords)
        self._assert_warn_collapse_without_weight(coords, warn)

    def test_no_lat_weighted_aggregator_mixed(self):
        # Collapse grid_latitude and an unmatched coordinate (not lat/lon)
        # with weighted aggregator without providing weights.
        # Tests coordinate matching logic.
        aggregator = self._aggregator(False)
        coords = ['wibble']

        with mock.patch('warnings.warn') as warn:
            self.cube.collapsed(coords, aggregator)

        self._assert_nowarn_collapse_without_weight(coords, warn)


class Test_summary(tests.IrisTest):
    def test_cell_datetime_objects(self):
        # Check the scalar coordinate summary still works even when
        # iris.FUTURE.cell_datetime_objects is True.
        cube = Cube(0)
        cube.add_aux_coord(AuxCoord(42, units='hours since epoch'))
        with FUTURE.context(cell_datetime_objects=True):
            summary = cube.summary()
        self.assertIn('1970-01-02 18:00:00', summary)


class Test_is_compatible(tests.IrisTest):
    def setUp(self):
        self.test_cube = Cube([1.])
        self.other_cube = self.test_cube.copy()

    def test_noncommon_array_attrs_compatible(self):
        # Non-common array attributes should be ok.
        self.test_cube.attributes['array_test'] = np.array([1.0, 2, 3])
        self.assertTrue(self.test_cube.is_compatible(self.other_cube))

    def test_matching_array_attrs_compatible(self):
        # Matching array attributes should be ok.
        self.test_cube.attributes['array_test'] = np.array([1.0, 2, 3])
        self.other_cube.attributes['array_test'] = np.array([1.0, 2, 3])
        self.assertTrue(self.test_cube.is_compatible(self.other_cube))

    def test_different_array_attrs_incompatible(self):
        # Differing array attributes should make the cubes incompatible.
        self.test_cube.attributes['array_test'] = np.array([1.0, 2, 3])
        self.other_cube.attributes['array_test'] = np.array([1.0, 2, 777.7])
        self.assertFalse(self.test_cube.is_compatible(self.other_cube))


class Test_aggregated_by(tests.IrisTest):
    def setUp(self):
        self.cube = Cube(np.arange(11))
        val_coord = AuxCoord([0, 0, 0, 1, 1, 2, 0, 0, 2, 0, 1],
                             long_name="val")
        label_coord = AuxCoord(['alpha', 'alpha', 'beta',
                                'beta', 'alpha', 'gamma',
                                'alpha', 'alpha', 'alpha',
                                'gamma', 'beta'],
                               long_name='label', units='no_unit')
        self.cube.add_aux_coord(val_coord, 0)
        self.cube.add_aux_coord(label_coord, 0)
        self.mock_agg = mock.Mock(spec=Aggregator)
        self.mock_agg.aggregate = mock.Mock(
            return_value=mock.Mock(dtype='object'))

    def test_string_coord_agg_by_label(self):
        # Aggregate a cube on a string coordinate label where label
        # and val entries are not in step; the resulting cube has a val
        # coord of bounded cells and a label coord of single string entries.
        res_cube = self.cube.aggregated_by('label', self.mock_agg)
        val_coord = AuxCoord(np.array([1., 0.5, 1.]),
                             bounds=np.array([[0, 2], [0, 1], [2, 0]]),
                             long_name='val')
        label_coord = AuxCoord(np.array(['alpha', 'beta', 'gamma']),
                               long_name='label', units='no_unit')
        self.assertEqual(res_cube.coord('val'), val_coord)
        self.assertEqual(res_cube.coord('label'), label_coord)

    def test_string_coord_agg_by_val(self):
        # Aggregate a cube on a numeric coordinate val where label
        # and val entries are not in step; the resulting cube has a label
        # coord with serialised labels from the aggregated cells.
        res_cube = self.cube.aggregated_by('val', self.mock_agg)
        val_coord = AuxCoord(np.array([0,  1,  2]), long_name='val')
        exp0 = 'alpha|alpha|beta|alpha|alpha|gamma'
        exp1 = 'beta|alpha|beta'
        exp2 = 'gamma|alpha'
        label_coord = AuxCoord(np.array((exp0, exp1, exp2)),
                               long_name='label', units='no_unit')
        self.assertEqual(res_cube.coord('val'), val_coord)
        self.assertEqual(res_cube.coord('label'), label_coord)


class Test_rolling_window(tests.IrisTest):

    def setUp(self):
        self.cube = Cube(np.arange(6))
        val_coord = DimCoord([0, 1, 2, 3, 4, 5], long_name="val")
        month_coord = AuxCoord(['jan', 'feb', 'mar', 'apr', 'may', 'jun'],
                               long_name='month')
        self.cube.add_dim_coord(val_coord, 0)
        self.cube.add_aux_coord(month_coord, 0)
        self.mock_agg = mock.Mock(spec=Aggregator)
        self.mock_agg.aggregate = mock.Mock(
            return_value=np.empty([4]))

    def test_string_coord(self):
        # Rolling window on a cube that contains a string coordinate.
        res_cube = self.cube.rolling_window('val', self.mock_agg, 3)
        val_coord = DimCoord(np.array([1, 2, 3, 4]),
                             bounds=np.array([[0, 2], [1, 3], [2, 4], [3, 5]]),
                             long_name='val')
        month_coord = AuxCoord(
            np.array(['jan|feb|mar', 'feb|mar|apr', 'mar|apr|may',
                      'apr|may|jun']),
            bounds=np.array([['jan', 'mar'], ['feb', 'apr'],
                             ['mar', 'may'], ['apr', 'jun']]),
            long_name='month')
        self.assertEqual(res_cube.coord('val'), val_coord)
        self.assertEqual(res_cube.coord('month'), month_coord)


def create_cube(lon_min, lon_max, bounds=False):
    n_lons = max(lon_min, lon_max) - min(lon_max, lon_min)
    data = np.arange(4 * 3 * n_lons, dtype='f4').reshape(4, 3, n_lons)
    data = biggus.NumpyArrayAdapter(data)
    cube = Cube(data, standard_name='x_wind', units='ms-1')
    cube.add_dim_coord(iris.coords.DimCoord([0, 20, 40, 80],
                                            long_name='level_height',
                                            units='m'), 0)
    cube.add_aux_coord(iris.coords.AuxCoord([1.0, 0.9, 0.8, 0.6],
                                            long_name='sigma'), 0)
    cube.add_dim_coord(iris.coords.DimCoord([-45, 0, 45], 'latitude',
                                            units='degrees'), 1)
    step = 1 if lon_max > lon_min else -1
    cube.add_dim_coord(iris.coords.DimCoord(np.arange(lon_min, lon_max, step),
                                            'longitude', units='degrees'), 2)
    if bounds:
        cube.coord('longitude').guess_bounds()
    cube.add_aux_coord(iris.coords.AuxCoord(
        np.arange(3 * n_lons).reshape(3, n_lons) * 10, 'surface_altitude',
        units='m'), [1, 2])
    cube.add_aux_factory(iris.aux_factory.HybridHeightFactory(
        cube.coord('level_height'), cube.coord('sigma'),
        cube.coord('surface_altitude')))
    return cube


# Ensure all the other coordinates and factories are correctly preserved.
class Test_intersection__Metadata(tests.IrisTest):
    def test_metadata(self):
        cube = create_cube(0, 360)
        result = cube.intersection(longitude=(170, 190))
        self.assertCMLApproxData(result)

    def test_metadata_wrapped(self):
        cube = create_cube(-180, 180)
        result = cube.intersection(longitude=(170, 190))
        self.assertCMLApproxData(result)


# Check the various error conditions.
class Test_intersection__Invalid(tests.IrisTest):
    def test_reversed_min_max(self):
        cube = create_cube(0, 360)
        with self.assertRaises(ValueError):
            cube.intersection(longitude=(30, 10))

    def test_dest_too_large(self):
        cube = create_cube(0, 360)
        with self.assertRaises(ValueError):
            cube.intersection(longitude=(30, 500))

    def test_src_too_large(self):
        cube = create_cube(0, 400)
        with self.assertRaises(ValueError):
            cube.intersection(longitude=(10, 30))

    def test_missing_coord(self):
        cube = create_cube(0, 360)
        with self.assertRaises(iris.exceptions.CoordinateNotFoundError):
            cube.intersection(parrots=(10, 30))

    def test_multi_dim_coord(self):
        cube = create_cube(0, 360)
        with self.assertRaises(iris.exceptions.CoordinateMultiDimError):
            cube.intersection(surface_altitude=(10, 30))

    def test_null_region(self):
        # 10 <= v < 10
        cube = create_cube(0, 360)
        with self.assertRaises(IndexError):
            cube.intersection(longitude=(10, 10, False, False))


class Test_intersection__Lazy(tests.IrisTest):
    def test_real_data(self):
        cube = create_cube(0, 360)
        cube.data
        result = cube.intersection(longitude=(170, 190))
        self.assertFalse(result.has_lazy_data())
        self.assertArrayEqual(result.coord('longitude').points,
                              range(170, 191))
        self.assertEqual(result.data[0, 0, 0], 170)
        self.assertEqual(result.data[0, 0, -1], 190)

    def test_real_data_wrapped(self):
        cube = create_cube(-180, 180)
        cube.data
        result = cube.intersection(longitude=(170, 190))
        self.assertFalse(result.has_lazy_data())
        self.assertArrayEqual(result.coord('longitude').points,
                              range(170, 191))
        self.assertEqual(result.data[0, 0, 0], 350)
        self.assertEqual(result.data[0, 0, -1], 10)

    def test_lazy_data(self):
        cube = create_cube(0, 360)
        result = cube.intersection(longitude=(170, 190))
        self.assertTrue(result.has_lazy_data())
        self.assertArrayEqual(result.coord('longitude').points,
                              range(170, 191))
        self.assertEqual(result.data[0, 0, 0], 170)
        self.assertEqual(result.data[0, 0, -1], 190)

    def test_lazy_data_wrapped(self):
        cube = create_cube(-180, 180)
        result = cube.intersection(longitude=(170, 190))
        self.assertTrue(result.has_lazy_data())
        self.assertArrayEqual(result.coord('longitude').points,
                              range(170, 191))
        self.assertEqual(result.data[0, 0, 0], 350)
        self.assertEqual(result.data[0, 0, -1], 10)


# Check what happens with a regional, points-only circular intersection
# coordinate.
class Test_intersection__RegionalSrcModulus(tests.IrisTest):
    def test_request_subset(self):
        cube = create_cube(40, 60)
        result = cube.intersection(longitude=(45, 50))
        self.assertArrayEqual(result.coord('longitude').points, range(45, 51))
        self.assertArrayEqual(result.data[0, 0], range(5, 11))

    def test_request_left(self):
        cube = create_cube(40, 60)
        result = cube.intersection(longitude=(35, 45))
        self.assertArrayEqual(result.coord('longitude').points, range(40, 46))
        self.assertArrayEqual(result.data[0, 0], range(0, 6))

    def test_request_right(self):
        cube = create_cube(40, 60)
        result = cube.intersection(longitude=(55, 65))
        self.assertArrayEqual(result.coord('longitude').points, range(55, 60))
        self.assertArrayEqual(result.data[0, 0], range(15, 20))

    def test_request_superset(self):
        cube = create_cube(40, 60)
        result = cube.intersection(longitude=(35, 65))
        self.assertArrayEqual(result.coord('longitude').points, range(40, 60))
        self.assertArrayEqual(result.data[0, 0], range(0, 20))

    def test_request_subset_modulus(self):
        cube = create_cube(40, 60)
        result = cube.intersection(longitude=(45 + 360, 50 + 360))
        self.assertArrayEqual(result.coord('longitude').points,
                              range(45 + 360, 51 + 360))
        self.assertArrayEqual(result.data[0, 0], range(5, 11))

    def test_request_left_modulus(self):
        cube = create_cube(40, 60)
        result = cube.intersection(longitude=(35 + 360, 45 + 360))
        self.assertArrayEqual(result.coord('longitude').points,
                              range(40 + 360, 46 + 360))
        self.assertArrayEqual(result.data[0, 0], range(0, 6))

    def test_request_right_modulus(self):
        cube = create_cube(40, 60)
        result = cube.intersection(longitude=(55 + 360, 65 + 360))
        self.assertArrayEqual(result.coord('longitude').points,
                              range(55 + 360, 60 + 360))
        self.assertArrayEqual(result.data[0, 0], range(15, 20))

    def test_request_superset_modulus(self):
        cube = create_cube(40, 60)
        result = cube.intersection(longitude=(35 + 360, 65 + 360))
        self.assertArrayEqual(result.coord('longitude').points,
                              range(40 + 360, 60 + 360))
        self.assertArrayEqual(result.data[0, 0], range(0, 20))

    def test_tolerance_f4(self):
        cube = create_cube(0, 5)
        cube.coord('longitude').points = np.array([0., 3.74999905, 7.49999809,
                                                   11.24999714, 14.99999619],
                                                  dtype='f4')
        result = cube.intersection(longitude=(0, 5))

    def test_tolerance_f8(self):
        cube = create_cube(0, 5)
        cube.coord('longitude').points = np.array([0., 3.74999905, 7.49999809,
                                                   11.24999714, 14.99999619],
                                                  dtype='f8')
        result = cube.intersection(longitude=(0, 5))


# Check what happens with a global, points-only circular intersection
# coordinate.
class Test_intersection__GlobalSrcModulus(tests.IrisTest):
    def test_global(self):
        cube = create_cube(0, 360)
        result = cube.intersection(longitude=(0, 360))
        self.assertEqual(result.coord('longitude').points[0], 0)
        self.assertEqual(result.coord('longitude').points[-1], 359)
        self.assertEqual(result.data[0, 0, 0], 0)
        self.assertEqual(result.data[0, 0, -1], 359)

    def test_global_wrapped(self):
        cube = create_cube(0, 360)
        result = cube.intersection(longitude=(-180, 180))
        self.assertEqual(result.coord('longitude').points[0], -180)
        self.assertEqual(result.coord('longitude').points[-1], 179)
        self.assertEqual(result.data[0, 0, 0], 180)
        self.assertEqual(result.data[0, 0, -1], 179)

    def test_aux_coord(self):
        cube = create_cube(0, 360)
        cube.replace_coord(iris.coords.AuxCoord.from_coord(
            cube.coord('longitude')))
        result = cube.intersection(longitude=(0, 360))
        self.assertEqual(result.coord('longitude').points[0], 0)
        self.assertEqual(result.coord('longitude').points[-1], 359)
        self.assertEqual(result.data[0, 0, 0], 0)
        self.assertEqual(result.data[0, 0, -1], 359)

    def test_aux_coord_wrapped(self):
        cube = create_cube(0, 360)
        cube.replace_coord(iris.coords.AuxCoord.from_coord(
            cube.coord('longitude')))
        result = cube.intersection(longitude=(-180, 180))
        self.assertEqual(result.coord('longitude').points[0], 0)
        self.assertEqual(result.coord('longitude').points[-1], -1)
        self.assertEqual(result.data[0, 0, 0], 0)
        self.assertEqual(result.data[0, 0, -1], 359)

    def test_aux_coord_non_contiguous_wrapped(self):
        cube = create_cube(0, 360)
        coord = iris.coords.AuxCoord.from_coord(cube.coord('longitude'))
        coord.points = (coord.points * 1.5) % 360
        cube.replace_coord(coord)
        result = cube.intersection(longitude=(-90, 90))
        self.assertEqual(result.coord('longitude').points[0], 0)
        self.assertEqual(result.coord('longitude').points[-1], 90)
        self.assertEqual(result.data[0, 0, 0], 0)
        self.assertEqual(result.data[0, 0, -1], 300)

    def test_decrementing(self):
        cube = create_cube(360, 0)
        result = cube.intersection(longitude=(40, 60))
        self.assertEqual(result.coord('longitude').points[0], 60)
        self.assertEqual(result.coord('longitude').points[-1], 40)
        self.assertEqual(result.data[0, 0, 0], 300)
        self.assertEqual(result.data[0, 0, -1], 320)

    def test_decrementing_wrapped(self):
        cube = create_cube(360, 0)
        result = cube.intersection(longitude=(-10, 10))
        self.assertEqual(result.coord('longitude').points[0], 10)
        self.assertEqual(result.coord('longitude').points[-1], -10)
        self.assertEqual(result.data[0, 0, 0], 350)
        self.assertEqual(result.data[0, 0, -1], 10)

    def test_no_wrap_after_modulus(self):
        cube = create_cube(0, 360)
        result = cube.intersection(longitude=(170 + 360, 190 + 360))
        self.assertEqual(result.coord('longitude').points[0], 170 + 360)
        self.assertEqual(result.coord('longitude').points[-1], 190 + 360)
        self.assertEqual(result.data[0, 0, 0], 170)
        self.assertEqual(result.data[0, 0, -1], 190)

    def test_wrap_after_modulus(self):
        cube = create_cube(-180, 180)
        result = cube.intersection(longitude=(170 + 360, 190 + 360))
        self.assertEqual(result.coord('longitude').points[0], 170 + 360)
        self.assertEqual(result.coord('longitude').points[-1], 190 + 360)
        self.assertEqual(result.data[0, 0, 0], 350)
        self.assertEqual(result.data[0, 0, -1], 10)

    def test_select_by_coord(self):
        cube = create_cube(0, 360)
        coord = iris.coords.DimCoord(0, 'longitude', units='degrees')
        result = cube.intersection(iris.coords.CoordExtent(coord, 10, 30))
        self.assertEqual(result.coord('longitude').points[0], 10)
        self.assertEqual(result.coord('longitude').points[-1], 30)
        self.assertEqual(result.data[0, 0, 0], 10)
        self.assertEqual(result.data[0, 0, -1], 30)

    def test_inclusive_exclusive(self):
        cube = create_cube(0, 360)
        result = cube.intersection(longitude=(170, 190, True, False))
        self.assertEqual(result.coord('longitude').points[0], 170)
        self.assertEqual(result.coord('longitude').points[-1], 189)
        self.assertEqual(result.data[0, 0, 0], 170)
        self.assertEqual(result.data[0, 0, -1], 189)

    def test_exclusive_inclusive(self):
        cube = create_cube(0, 360)
        result = cube.intersection(longitude=(170, 190, False))
        self.assertEqual(result.coord('longitude').points[0], 171)
        self.assertEqual(result.coord('longitude').points[-1], 190)
        self.assertEqual(result.data[0, 0, 0], 171)
        self.assertEqual(result.data[0, 0, -1], 190)

    def test_exclusive_exclusive(self):
        cube = create_cube(0, 360)
        result = cube.intersection(longitude=(170, 190, False, False))
        self.assertEqual(result.coord('longitude').points[0], 171)
        self.assertEqual(result.coord('longitude').points[-1], 189)
        self.assertEqual(result.data[0, 0, 0], 171)
        self.assertEqual(result.data[0, 0, -1], 189)

    def test_single_point(self):
        # 10 <= v <= 10
        cube = create_cube(0, 360)
        result = cube.intersection(longitude=(10, 10))
        self.assertEqual(result.coord('longitude').points[0], 10)
        self.assertEqual(result.coord('longitude').points[-1], 10)
        self.assertEqual(result.data[0, 0, 0], 10)
        self.assertEqual(result.data[0, 0, -1], 10)

    def test_wrap_radians(self):
        cube = create_cube(0, 360)
        cube.coord('longitude').convert_units('radians')
        result = cube.intersection(longitude=(-1, 0.5))
        self.assertEqual(result.coord('longitude').points[0],
                         -0.99483767363676634)
        self.assertEqual(result.coord('longitude').points[-1],
                         0.48869219055841207)
        self.assertEqual(result.data[0, 0, 0], 303)
        self.assertEqual(result.data[0, 0, -1], 28)


# Check what happens with a global, points-and-bounds circular
# intersection coordinate.
class Test_intersection__ModulusBounds(tests.IrisTest):
    def test_misaligned_points_inside(self):
        cube = create_cube(0, 360, bounds=True)
        result = cube.intersection(longitude=(169.75, 190.25))
        self.assertArrayEqual(result.coord('longitude').bounds[0],
                              [169.5, 170.5])
        self.assertArrayEqual(result.coord('longitude').bounds[-1],
                              [189.5, 190.5])
        self.assertEqual(result.data[0, 0, 0], 170)
        self.assertEqual(result.data[0, 0, -1], 190)

    def test_misaligned_points_outside(self):
        cube = create_cube(0, 360, bounds=True)
        result = cube.intersection(longitude=(170.25, 189.75))
        self.assertArrayEqual(result.coord('longitude').bounds[0],
                              [169.5, 170.5])
        self.assertArrayEqual(result.coord('longitude').bounds[-1],
                              [189.5, 190.5])
        self.assertEqual(result.data[0, 0, 0], 170)
        self.assertEqual(result.data[0, 0, -1], 190)

    def test_aligned_inclusive(self):
        cube = create_cube(0, 360, bounds=True)
        result = cube.intersection(longitude=(170.5, 189.5))
        self.assertArrayEqual(result.coord('longitude').bounds[0],
                              [169.5, 170.5])
        self.assertArrayEqual(result.coord('longitude').bounds[-1],
                              [189.5, 190.5])
        self.assertEqual(result.data[0, 0, 0], 170)
        self.assertEqual(result.data[0, 0, -1], 190)

    def test_aligned_exclusive(self):
        cube = create_cube(0, 360, bounds=True)
        result = cube.intersection(longitude=(170.5, 189.5, False, False))
        self.assertArrayEqual(result.coord('longitude').bounds[0],
                              [170.5, 171.5])
        self.assertArrayEqual(result.coord('longitude').bounds[-1],
                              [188.5, 189.5])
        self.assertEqual(result.data[0, 0, 0], 171)
        self.assertEqual(result.data[0, 0, -1], 189)

    def test_negative_misaligned_points_inside(self):
        cube = create_cube(0, 360, bounds=True)
        result = cube.intersection(longitude=(-10.25, 10.25))
        self.assertArrayEqual(result.coord('longitude').bounds[0],
                              [-10.5, -9.5])
        self.assertArrayEqual(result.coord('longitude').bounds[-1],
                              [9.5, 10.5])
        self.assertEqual(result.data[0, 0, 0], 350)
        self.assertEqual(result.data[0, 0, -1], 10)

    def test_negative_misaligned_points_outside(self):
        cube = create_cube(0, 360, bounds=True)
        result = cube.intersection(longitude=(-9.75, 9.75))
        self.assertArrayEqual(result.coord('longitude').bounds[0],
                              [-10.5, -9.5])
        self.assertArrayEqual(result.coord('longitude').bounds[-1],
                              [9.5, 10.5])
        self.assertEqual(result.data[0, 0, 0], 350)
        self.assertEqual(result.data[0, 0, -1], 10)

    def test_negative_aligned_inclusive(self):
        cube = create_cube(0, 360, bounds=True)
        result = cube.intersection(longitude=(-10.5, 10.5))
        self.assertArrayEqual(result.coord('longitude').bounds[0],
                              [-11.5, -10.5])
        self.assertArrayEqual(result.coord('longitude').bounds[-1],
                              [10.5, 11.5])
        self.assertEqual(result.data[0, 0, 0], 349)
        self.assertEqual(result.data[0, 0, -1], 11)

    def test_negative_aligned_exclusive(self):
        cube = create_cube(0, 360, bounds=True)
        result = cube.intersection(longitude=(-10.5, 10.5, False, False))
        self.assertArrayEqual(result.coord('longitude').bounds[0],
                              [-10.5, -9.5])
        self.assertArrayEqual(result.coord('longitude').bounds[-1],
                              [9.5, 10.5])
        self.assertEqual(result.data[0, 0, 0], 350)
        self.assertEqual(result.data[0, 0, -1], 10)


def unrolled_cube():
    data = np.arange(5, dtype='f4')
    cube = Cube(data)
    cube.add_aux_coord(iris.coords.AuxCoord([5.0, 10.0, 8.0, 5.0, 3.0],
                                            'longitude', units='degrees'), 0)
    cube.add_aux_coord(iris.coords.AuxCoord([1.0, 3.0, -2.0, -1.0, -4.0],
                                            'latitude'), 0)
    return cube


# Check what happens with a "unrolled" scatter-point data with a circular
# intersection coordinate.
class Test_intersection__ScatterModulus(tests.IrisTest):
    def test_subset(self):
        cube = unrolled_cube()
        result = cube.intersection(longitude=(5, 8))
        self.assertArrayEqual(result.coord('longitude').points, [5, 8, 5])
        self.assertArrayEqual(result.data, [0, 2, 3])

    def test_subset_wrapped(self):
        cube = unrolled_cube()
        result = cube.intersection(longitude=(5 + 360, 8 + 360))
        self.assertArrayEqual(result.coord('longitude').points,
                              [365, 368, 365])
        self.assertArrayEqual(result.data, [0, 2, 3])

    def test_superset(self):
        cube = unrolled_cube()
        result = cube.intersection(longitude=(0, 15))
        self.assertArrayEqual(result.coord('longitude').points,
                              [5, 10, 8, 5, 3])
        self.assertArrayEqual(result.data, range(5))


# Test the API of the cube interpolation method.
class Test_interpolate(tests.IrisTest):
    def setUp(self):
        self.cube = stock.simple_2d()

        self.scheme = mock.Mock(name='interpolation scheme')
        self.interpolator = mock.Mock(name='interpolator')
        self.interpolator.return_value = mock.sentinel.RESULT
        self.scheme.interpolator.return_value = self.interpolator
        self.collapse_coord = True

    def test_api(self):
        sample_points = (('foo', 0.5), ('bar', 0.6))
        result = self.cube.interpolate(self.scheme, sample_points,
                                       self.collapse_coord)
        self.scheme.interpolator.assert_called_once_with(
            self.cube, ('foo', 'bar'))
        self.interpolator.assert_called_once_with(
            (0.5, 0.6), collapse_scalar=self.collapse_coord)
        self.assertIs(result, mock.sentinel.RESULT)

if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_CubeList
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.cube.CubeList` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import numpy as np

from iris.cube import Cube, CubeList
from iris.coords import AuxCoord, DimCoord
import iris.exceptions


class Test_merge_cube(tests.IrisTest):
    def setUp(self):
        self.cube1 = Cube([1, 2, 3], "air_temperature", units="K")
        self.cube1.add_aux_coord(AuxCoord([0], "height", units="m"))

    def test_pass(self):
        cube2 = self.cube1.copy()
        cube2.coord("height").points = [1]
        result = CubeList([self.cube1, cube2]).merge_cube()
        self.assertIsInstance(result, Cube)

    def test_fail(self):
        cube2 = self.cube1.copy()
        cube2.rename("not air temperature")
        with self.assertRaises(iris.exceptions.MergeError):
            CubeList([self.cube1, cube2]).merge_cube()

    def test_empty(self):
        with self.assertRaises(ValueError):
            CubeList([]).merge_cube()

    def test_single_cube(self):
        result = CubeList([self.cube1]).merge_cube()
        self.assertEqual(result, self.cube1)
        self.assertIsNot(result, self.cube1)

    def test_repeated_cube(self):
        with self.assertRaises(iris.exceptions.MergeError):
            CubeList([self.cube1, self.cube1]).merge_cube()


class Test_merge__time_triple(tests.IrisTest):
    @staticmethod
    def _make_cube(fp, rt, t, realization=None):
        cube = Cube(np.arange(20).reshape(4, 5))
        cube.add_dim_coord(DimCoord(np.arange(5), long_name='x'), 1)
        cube.add_dim_coord(DimCoord(np.arange(4), long_name='y'), 0)
        cube.add_aux_coord(DimCoord(fp, standard_name='forecast_period'))
        cube.add_aux_coord(DimCoord(rt,
                                    standard_name='forecast_reference_time'))
        cube.add_aux_coord(DimCoord(t, standard_name='time'))
        if realization is not None:
            cube.add_aux_coord(DimCoord(realization,
                                        standard_name='realization'))
        return cube

    def test_orthogonal_with_realization(self):
        # => fp: 2; rt: 2; t: 2; realization: 2
        triples = ((0, 10, 1),
                   (0, 10, 2),
                   (0, 11, 1),
                   (0, 11, 2),
                   (1, 10, 1),
                   (1, 10, 2),
                   (1, 11, 1),
                   (1, 11, 2))
        en1_cubes = [self._make_cube(*triple, realization=1) for
                     triple in triples]
        en2_cubes = [self._make_cube(*triple, realization=2) for
                     triple in triples]
        cubes = CubeList(en1_cubes) + CubeList(en2_cubes)
        cube, = cubes.merge()
        self.assertCML(cube, checksum=False)

    def test_combination_with_realization(self):
        # => fp, rt, t: 8; realization: 2
        triples = ((0, 10, 1),
                   (0, 10, 2),
                   (0, 11, 1),
                   (0, 11, 3),  # This '3' breaks the pattern.
                   (1, 10, 1),
                   (1, 10, 2),
                   (1, 11, 1),
                   (1, 11, 2))
        en1_cubes = [self._make_cube(*triple, realization=1) for
                     triple in triples]
        en2_cubes = [self._make_cube(*triple, realization=2) for
                     triple in triples]
        cubes = CubeList(en1_cubes) + CubeList(en2_cubes)
        cube, = cubes.merge()
        self.assertCML(cube, checksum=False)

    def test_combination_with_extra_realization(self):
        # => fp, rt, t, realization: 17
        triples = ((0, 10, 1),
                   (0, 10, 2),
                   (0, 11, 1),
                   (0, 11, 2),
                   (1, 10, 1),
                   (1, 10, 2),
                   (1, 11, 1),
                   (1, 11, 2))
        en1_cubes = [self._make_cube(*triple, realization=1) for
                     triple in triples]
        en2_cubes = [self._make_cube(*triple, realization=2) for
                     triple in triples]
        # Add extra that is a duplicate of one of the time triples
        # but with a different realisation.
        en3_cubes = [self._make_cube(0, 10, 2, realization=3)]
        cubes = CubeList(en1_cubes) + CubeList(en2_cubes) + CubeList(en3_cubes)
        cube, = cubes.merge()
        self.assertCML(cube, checksum=False)

    def test_combination_with_extra_triple(self):
        # => fp, rt, t, realization: 17
        triples = ((0, 10, 1),
                   (0, 10, 2),
                   (0, 11, 1),
                   (0, 11, 2),
                   (1, 10, 1),
                   (1, 10, 2),
                   (1, 11, 1),
                   (1, 11, 2))
        en1_cubes = [self._make_cube(*triple, realization=1) for
                     triple in triples]
        # Add extra time triple on the end.
        en2_cubes = [self._make_cube(*triple, realization=2) for
                     triple in triples + ((1, 11, 3),)]
        cubes = CubeList(en1_cubes) + CubeList(en2_cubes)
        cube, = cubes.merge()
        self.assertCML(cube, checksum=False)


class Test_xml(tests.IrisTest):
    def setUp(self):
        self.cubes = CubeList([Cube(np.arange(3)),
                               Cube(np.arange(3))])

    def test_byteorder_default(self):
        self.assertIn('byteorder', self.cubes.xml())

    def test_byteorder_false(self):
        self.assertNotIn('byteorder', self.cubes.xml(byteorder=False))

    def test_byteorder_true(self):
        self.assertIn('byteorder', self.cubes.xml(byteorder=True))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_equalise_attributes
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for the :func:`iris.experimental.equalise_cubes.equalise_attributes`
function.

"""

# import iris tests first so that some things can be initialised
# before importing anything else.
import iris.tests as tests

import numpy as np

from iris.cube import Cube
from iris.experimental.equalise_cubes import equalise_attributes
import iris.tests.stock


class TestEqualiseAttributes(tests.IrisTest):
    def setUp(self):
        empty = Cube([])

        self.cube_no_attrs = empty.copy()

        self.cube_a1 = empty.copy()
        self.cube_a1.attributes.update({'a': 1})

        self.cube_a2 = empty.copy()
        self.cube_a2.attributes.update({'a': 2})

        self.cube_a1b5 = empty.copy()
        self.cube_a1b5.attributes.update({'a': 1, 'b': 5})

        self.cube_a1b6 = empty.copy()
        self.cube_a1b6.attributes.update({'a': 1, 'b': 6})

        self.cube_a2b6 = empty.copy()
        self.cube_a2b6.attributes.update({'a': 2, 'b': 6})

        self.cube_b5 = empty.copy()
        self.cube_b5.attributes.update({'b': 5})

        # Array attribute values
        v1 = np.array([11, 12, 13])
        v2 = np.array([11, 9999, 13])
        self.v1 = v1

        self.cube_a1b5v1 = empty.copy()
        self.cube_a1b5v1.attributes.update({'a': 1, 'b': 5, 'v': v1})

        self.cube_a1b6v1 = empty.copy()
        self.cube_a1b6v1.attributes.update({'a': 1, 'b': 6, 'v': v1})

        self.cube_a1b6v2 = empty.copy()
        self.cube_a1b6v2.attributes.update({'a': 1, 'b': 6, 'v': v2})

    def _test(self, cubes, expect_attributes):
        """Test."""
        working_cubes = [cube.copy() for cube in cubes]
        original_working_list = [cube for cube in working_cubes]
        # Exercise basic operation
        equalise_attributes(working_cubes)
        # Check they are the same cubes
        self.assertEqual(working_cubes, original_working_list)
        # Check resulting attributes all match the expected set
        for cube in working_cubes:
            self.assertEqual(cube.attributes, expect_attributes)
        # Check everything else remains the same
        for new_cube, old_cube in zip(working_cubes, cubes):
            cube_before_noatts = old_cube.copy()
            cube_before_noatts.attributes.clear()
            cube_after_noatts = new_cube.copy()
            cube_after_noatts.attributes.clear()
            self.assertEqual(cube_after_noatts, cube_before_noatts)

    def test_no_attrs(self):
        cubes = [self.cube_no_attrs]
        self._test(cubes, {})

    def test_single(self):
        cubes = [self.cube_a1]
        self._test(cubes, {'a': 1})

    def test_identical(self):
        cubes = [self.cube_a1, self.cube_a1.copy()]
        self._test(cubes, {'a': 1})

    def test_one_extra(self):
        cubes = [self.cube_a1, self.cube_a1b5.copy()]
        self._test(cubes, {'a': 1})

    def test_one_different(self):
        cubes = [self.cube_a1b5, self.cube_a1b6]
        self._test(cubes, {'a': 1})

    def test_common_no_diffs(self):
        cubes = [self.cube_a1b5, self.cube_a1b5.copy()]
        self._test(cubes, {'a': 1, 'b': 5})

    def test_common_all_diffs(self):
        cubes = [self.cube_a1b5, self.cube_a2b6]
        self._test(cubes, {})

    def test_none_common(self):
        cubes = [self.cube_a1, self.cube_b5]
        self._test(cubes, {})

    def test_array_extra(self):
        cubes = [self.cube_a1b6, self.cube_a1b6v1]
        self._test(cubes, {'a': 1, 'b': 6})

    def test_array_different(self):
        cubes = [self.cube_a1b5v1, self.cube_a1b6v2]
        self._test(cubes, {'a': 1})

    def test_array_same(self):
        cubes = [self.cube_a1b5v1, self.cube_a1b6v1]
        self._test(cubes, {'a': 1, 'v': self.v1})

    def test_complex_nonecommon(self):
        # Example with cell methods and factories, but no common attributes.
        cubes = [iris.tests.stock.global_pp(),
                 iris.tests.stock.hybrid_height()]
        self._test(cubes, {})

    def test_complex_somecommon(self):
        # Example with cell methods and factories, plus some common attributes.
        cubes = [iris.tests.stock.global_pp(), iris.tests.stock.simple_pp()]
        self._test(
            cubes,
            {'STASH': iris.fileformats.pp.STASH(model=1, section=16, item=203),
             'source': 'Data from Met Office Unified Model'})


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_export_geotiff
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.experimental.raster.export_geotiff` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import numpy as np
from osgeo import gdal

from iris.coord_systems import GeogCS
from iris.coords import DimCoord
from iris.cube import Cube
from iris.experimental.raster import export_geotiff


class TestDtypeAndValues(tests.IrisTest):
    def _cube(self, dtype):
        data = np.arange(12).reshape(3, 4).astype(dtype) + 20
        cube = Cube(data, 'air_pressure_anomaly')
        coord = DimCoord(range(3), 'latitude', units='degrees')
        coord.guess_bounds()
        cube.add_dim_coord(coord, 0)
        coord = DimCoord(range(4), 'longitude', units='degrees')
        coord.guess_bounds()
        cube.add_dim_coord(coord, 1)
        return cube

    def _check_dtype(self, dtype, gdal_dtype):
        cube = self._cube(dtype)
        with self.temp_filename('.tif') as temp_filename:
            export_geotiff(cube, temp_filename)
            dataset = gdal.Open(temp_filename, gdal.GA_ReadOnly)
            band = dataset.GetRasterBand(1)
            self.assertEqual(band.DataType, gdal_dtype)
            self.assertEqual(band.ComputeRasterMinMax(1), (20, 31))

    def test_int16(self):
        self._check_dtype('<i2', gdal.GDT_Int16)

    def test_int16_big_endian(self):
        self._check_dtype('>i2', gdal.GDT_Int16)

    def test_int32(self):
        self._check_dtype('<i4', gdal.GDT_Int32)

    def test_int32_big_endian(self):
        self._check_dtype('>i4', gdal.GDT_Int32)

    def test_uint8(self):
        self._check_dtype('u1', gdal.GDT_Byte)

    def test_uint16(self):
        self._check_dtype('<u2', gdal.GDT_UInt16)

    def test_uint16_big_endian(self):
        self._check_dtype('>u2', gdal.GDT_UInt16)

    def test_uint32(self):
        self._check_dtype('<u4', gdal.GDT_UInt32)

    def test_uint32_big_endian(self):
        self._check_dtype('>u4', gdal.GDT_UInt32)

    def test_float32(self):
        self._check_dtype('<f4', gdal.GDT_Float32)

    def test_float32_big_endian(self):
        self._check_dtype('>f4', gdal.GDT_Float32)

    def test_float64(self):
        self._check_dtype('<f8', gdal.GDT_Float64)

    def test_float64_big_endian(self):
        self._check_dtype('>f8', gdal.GDT_Float64)

    def test_invalid(self):
        cube = self._cube('i1')
        with self.assertRaises(ValueError):
            with self.temp_filename('.tif') as temp_filename:
                export_geotiff(cube, temp_filename)


class TestProjection(tests.IrisTest):
    def _cube(self, ellipsoid=None):
        data = np.arange(12).reshape(3, 4).astype('u1')
        cube = Cube(data, 'air_pressure_anomaly')
        coord = DimCoord(range(3), 'latitude', units='degrees',
                         coord_system=ellipsoid)
        coord.guess_bounds()
        cube.add_dim_coord(coord, 0)
        coord = DimCoord(range(4), 'longitude', units='degrees',
                         coord_system=ellipsoid)
        coord.guess_bounds()
        cube.add_dim_coord(coord, 1)
        return cube

    def test_no_ellipsoid(self):
        cube = self._cube()
        with self.temp_filename('.tif') as temp_filename:
            export_geotiff(cube, temp_filename)
            dataset = gdal.Open(temp_filename, gdal.GA_ReadOnly)
            self.assertEqual(dataset.GetProjection(), '')

    def test_sphere(self):
        cube = self._cube(GeogCS(6377000))
        with self.temp_filename('.tif') as temp_filename:
            export_geotiff(cube, temp_filename)
            dataset = gdal.Open(temp_filename, gdal.GA_ReadOnly)
            self.assertEqual(
                dataset.GetProjection(),
                'GEOGCS["unnamed ellipse",DATUM["unknown",'
                'SPHEROID["unnamed",6377000,0]],PRIMEM["Greenwich",0],'
                'UNIT["degree",0.0174532925199433]]')

    def test_ellipsoid(self):
        cube = self._cube(GeogCS(6377000, 6360000))
        with self.temp_filename('.tif') as temp_filename:
            export_geotiff(cube, temp_filename)
            dataset = gdal.Open(temp_filename, gdal.GA_ReadOnly)
            self.assertEqual(
                dataset.GetProjection(),
                'GEOGCS["unnamed ellipse",DATUM["unknown",'
                'SPHEROID["unnamed",6377000,375.117647058816]],'
                'PRIMEM["Greenwich",0],UNIT["degree",0.0174532925199433]]')


class TestGeoTransform(tests.IrisTest):
    def test_(self):
        data = np.arange(12).reshape(3, 4).astype(np.uint8)
        cube = Cube(data, 'air_pressure_anomaly')
        coord = DimCoord([30, 40, 50], 'latitude', units='degrees')
        coord.guess_bounds()
        cube.add_dim_coord(coord, 0)
        coord = DimCoord([-10, -5, 0, 5], 'longitude', units='degrees')
        coord.guess_bounds()
        cube.add_dim_coord(coord, 1)
        with self.temp_filename('.tif') as temp_filename:
            export_geotiff(cube, temp_filename)
            dataset = gdal.Open(temp_filename, gdal.GA_ReadOnly)
            self.assertEqual(dataset.GetGeoTransform(),
                             (-12.5, 5, 0, 55, 0, -10))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_regrid_bilinear_rectilinear_src_and_grid
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for
:func:`iris.experimental.regrid.regrid_bilinear_rectilinear_src_and_grid`.

"""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np

from iris.coord_systems import GeogCS, OSGB
from iris.coords import AuxCoord, DimCoord
from iris.cube import Cube
from iris.experimental.regrid import \
    regrid_bilinear_rectilinear_src_and_grid as regrid
from iris.tests.stock import lat_lon_cube


class TestInvalidTypes(tests.IrisTest):
    def test_src_as_array(self):
        with self.assertRaises(TypeError):
            regrid(np.zeros((3, 4)), Cube())

    def test_grid_as_array(self):
        with self.assertRaises(TypeError):
            regrid(Cube(), np.zeros((3, 4)))

    def test_src_as_int(self):
        with self.assertRaises(TypeError):
            regrid(42, Cube())

    def test_grid_as_int(self):
        with self.assertRaises(TypeError):
            regrid(Cube(), 42)


class TestMissingCoords(tests.IrisTest):
    def ok_bad(self, coord_names):
        # Deletes the named coords from `bad`.
        ok = lat_lon_cube()
        bad = lat_lon_cube()
        for name in coord_names:
            bad.remove_coord(name)
        return ok, bad

    def test_src_missing_lat(self):
        ok, bad = self.ok_bad(['latitude'])
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_missing_lat(self):
        ok, bad = self.ok_bad(['latitude'])
        with self.assertRaises(ValueError):
            regrid(ok, bad)

    def test_src_missing_lon(self):
        ok, bad = self.ok_bad(['longitude'])
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_missing_lon(self):
        ok, bad = self.ok_bad(['longitude'])
        with self.assertRaises(ValueError):
            regrid(ok, bad)

    def test_src_missing_lat_lon(self):
        ok, bad = self.ok_bad(['latitude', 'longitude'])
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_missing_lat_lon(self):
        ok, bad = self.ok_bad(['latitude', 'longitude'])
        with self.assertRaises(ValueError):
            regrid(ok, bad)


class TestNotDimCoord(tests.IrisTest):
    def ok_bad(self, coord_name):
        # Demotes the named DimCoord on `bad` to an AuxCoord.
        ok = lat_lon_cube()
        bad = lat_lon_cube()
        coord = bad.coord(coord_name)
        dims = bad.coord_dims(coord)
        bad.remove_coord(coord_name)
        aux_coord = AuxCoord.from_coord(coord)
        bad.add_aux_coord(aux_coord, dims)
        return ok, bad

    def test_src_with_aux_lat(self):
        ok, bad = self.ok_bad('latitude')
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_with_aux_lat(self):
        ok, bad = self.ok_bad('latitude')
        with self.assertRaises(ValueError):
            regrid(ok, bad)

    def test_src_with_aux_lon(self):
        ok, bad = self.ok_bad('longitude')
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_with_aux_lon(self):
        ok, bad = self.ok_bad('longitude')
        with self.assertRaises(ValueError):
            regrid(ok, bad)


class TestNotDimCoord(tests.IrisTest):
    def ok_bad(self):
        # Make lat/lon share a single dimension on `bad`.
        ok = lat_lon_cube()
        bad = lat_lon_cube()
        lat = bad.coord('latitude')
        bad = bad[0, :lat.shape[0]]
        bad.remove_coord('latitude')
        bad.add_aux_coord(lat, 0)
        return ok, bad

    def test_src_shares_dim(self):
        ok, bad = self.ok_bad()
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_shares_dim(self):
        ok, bad = self.ok_bad()
        with self.assertRaises(ValueError):
            regrid(ok, bad)


class TestBadGeoreference(tests.IrisTest):
    def ok_bad(self, lat_cs, lon_cs):
        # Updates `bad` to use the given coordinate systems.
        ok = lat_lon_cube()
        bad = lat_lon_cube()
        bad.coord('latitude').coord_system = lat_cs
        bad.coord('longitude').coord_system = lon_cs
        return ok, bad

    def test_src_no_cs(self):
        ok, bad = self.ok_bad(None, None)
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_no_cs(self):
        ok, bad = self.ok_bad(None, None)
        with self.assertRaises(ValueError):
            regrid(ok, bad)

    def test_src_one_cs(self):
        ok, bad = self.ok_bad(None, GeogCS(6371000))
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_one_cs(self):
        ok, bad = self.ok_bad(None, GeogCS(6371000))
        with self.assertRaises(ValueError):
            regrid(ok, bad)

    def test_src_inconsistent_cs(self):
        ok, bad = self.ok_bad(GeogCS(6370000), GeogCS(6371000))
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_inconsistent_cs(self):
        ok, bad = self.ok_bad(GeogCS(6370000), GeogCS(6371000))
        with self.assertRaises(ValueError):
            regrid(ok, bad)


class TestBadAngularUnits(tests.IrisTest):
    def ok_bad(self):
        # Changes the longitude coord to radians on `bad`.
        ok = lat_lon_cube()
        bad = lat_lon_cube()
        bad.coord('longitude').units = 'radians'
        return ok, bad

    def test_src_radians(self):
        ok, bad = self.ok_bad()
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_radians(self):
        ok, bad = self.ok_bad()
        with self.assertRaises(ValueError):
            regrid(ok, bad)


class TestBadLinearUnits(tests.IrisTest):
    def ok_bad(self):
        # Defines `bad` with an x coordinate in km.
        ok = lat_lon_cube()
        bad = Cube(np.arange(12, dtype=np.float32).reshape(3, 4))
        cs = OSGB()
        y_coord = DimCoord(range(3), 'projection_y_coordinate', units='m',
                           coord_system=cs)
        x_coord = DimCoord(range(4), 'projection_x_coordinate', units='km',
                           coord_system=cs)
        bad.add_dim_coord(y_coord, 0)
        bad.add_dim_coord(x_coord, 1)
        return ok, bad

    def test_src_km(self):
        ok, bad = self.ok_bad()
        with self.assertRaises(ValueError):
            regrid(bad, ok)

    def test_grid_km(self):
        ok, bad = self.ok_bad()
        with self.assertRaises(ValueError):
            regrid(ok, bad)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_regrid_weighted_curvilinear_to_rectilinear
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test function
:func:`iris.experimental.regrid.regrid_weighted_curvilinear_to_rectilinear`.

"""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import copy

import numpy as np
import numpy.ma as ma

import iris
import iris.coords
import iris.cube
from iris.experimental.regrid \
    import regrid_weighted_curvilinear_to_rectilinear as regrid


class Test(tests.IrisTest):
    def setUp(self):
        # Source cube.
        data = ma.arange(1, 13, dtype=np.float).reshape(3, 4)
        attributes = dict(wibble='wobble')
        bibble = iris.coords.DimCoord([1], long_name='bibble')
        self.src = iris.cube.Cube(data,
                                  standard_name='air_temperature',
                                  units='K',
                                  aux_coords_and_dims=[(bibble, None)],
                                  attributes=attributes)

        # Source cube x-coordinates.
        points = np.array([[010, 020, 200, 220],
                           [110, 120, 180, 185],
                           [190, 203, 211, 220]])
        self.src_x_positive = iris.coords.AuxCoord(points,
                                                   standard_name='longitude',
                                                   units='degrees')
        self.src_x_transpose = iris.coords.AuxCoord(points.T,
                                                    standard_name='longitude',
                                                    units='degrees')
        points = np.array([[-180, -176, -170, -150],
                           [-180, -179, -178, -177],
                           [-170, -168, -159, -140]])
        self.src_x_negative = iris.coords.AuxCoord(points,
                                                   standard_name='longitude',
                                                   units='degrees')

        # Source cube y-coordinates.
        points = np.array([[00, 04, 03, 01],
                           [05, 07, 10, 06],
                           [12, 20, 15, 30]])
        self.src_y = iris.coords.AuxCoord(points,
                                          standard_name='latitude',
                                          units='degrees')
        self.src_y_transpose = iris.coords.AuxCoord(points.T,
                                                    standard_name='latitude',
                                                    units='degrees')

        # Weights.
        self.weight_factor = 10
        self.weights = np.asarray(data) * self.weight_factor

        # Target grid cube.
        self.grid = iris.cube.Cube(np.zeros((2, 2)))

        # Target grid cube x-coordinates.
        self.grid_x_inc = iris.coords.DimCoord([187, 200],
                                               standard_name='longitude',
                                               units='degrees',
                                               bounds=[[180, 190],
                                                       [190, 220]])
        self.grid_x_dec = iris.coords.DimCoord([200, 187],
                                               standard_name='longitude',
                                               units='degrees',
                                               bounds=[[220, 190],
                                                       [190, 180]])

        # Target grid cube y-coordinates.
        self.grid_y_inc = iris.coords.DimCoord([2, 10],
                                               standard_name='latitude',
                                               units='degrees',
                                               bounds=[[0, 5],
                                                       [5, 30]])
        self.grid_y_dec = iris.coords.DimCoord([10, 2],
                                               standard_name='latitude',
                                               units='degrees',
                                               bounds=[[30, 5],
                                                       [5, 0]])

    def _weighted_mean(self, points):
        points = np.asarray(points, dtype=np.float)
        weights = points * self.weight_factor
        numerator = denominator = 0
        for point, weight in zip(points, weights):
            numerator += point * weight
            denominator += weight
        return numerator / denominator

    def _expected_cube(self, data):
        cube = iris.cube.Cube(data)
        cube.metadata = copy.deepcopy(self.src)
        grid_x = self.grid.coord('longitude')
        grid_y = self.grid.coord('latitude')
        cube.add_dim_coord(grid_x.copy(), self.grid.coord_dims(grid_x))
        cube.add_dim_coord(grid_y.copy(), self.grid.coord_dims(grid_y))
        src_x = self.src.coord('longitude')
        src_y = self.src.coord('latitude')
        for coord in self.src.aux_coords:
            if coord is not src_x and coord is not src_y:
                if not self.src.coord_dims(coord):
                    cube.add_aux_coord(coord)
        return cube

    def test_aligned_src_x(self):
        self.src.add_aux_coord(self.src_y, (0, 1))
        self.src.add_aux_coord(self.src_x_positive, (0, 1))
        self.grid.add_dim_coord(self.grid_y_inc, 0)
        self.grid.add_dim_coord(self.grid_x_inc, 1)
        result = regrid(self.src, self.weights, self.grid)
        data = np.array([0,
                         self._weighted_mean([3]),
                         self._weighted_mean([7, 8]),
                         self._weighted_mean([9, 10, 11])]).reshape(2, 2)
        expected = self._expected_cube(data)
        self.assertEqual(result, expected)
        mask = np.array([[True, False], [False, False]])
        self.assertArrayEqual(result.data.mask, mask)

    def test_aligned_src_x_mask(self):
        self.src.add_aux_coord(self.src_y, (0, 1))
        self.src.add_aux_coord(self.src_x_positive, (0, 1))
        self.src.data[([1, 2, 2], [3, 0, 2])] = ma.masked
        self.grid.add_dim_coord(self.grid_y_inc, 0)
        self.grid.add_dim_coord(self.grid_x_inc, 1)
        result = regrid(self.src, self.weights, self.grid)
        data = np.array([0,
                         self._weighted_mean([3]),
                         self._weighted_mean([7]),
                         self._weighted_mean([10])]).reshape(2, 2)
        expected = self._expected_cube(data)
        self.assertEqual(result, expected)
        mask = np.array([[True, False], [False, False]])
        self.assertArrayEqual(result.data.mask, mask)

    def test_aligned_src_x_zero_weights(self):
        self.src.add_aux_coord(self.src_y, (0, 1))
        self.src.add_aux_coord(self.src_x_positive, (0, 1))
        self.grid.add_dim_coord(self.grid_y_inc, 0)
        self.grid.add_dim_coord(self.grid_x_inc, 1)
        self.weights[:, 2] = 0
        self.weights[1, :] = 0
        result = regrid(self.src, self.weights, self.grid)
        data = np.array([0, 0, 0, self._weighted_mean([9, 10])]).reshape(2, 2)
        expected = self._expected_cube(data)
        self.assertEqual(result, expected)
        mask = np.array([[True, True], [True, False]])
        self.assertArrayEqual(result.data.mask, mask)

    def test_aligned_src_x_transpose(self):
        self.src.add_aux_coord(self.src_y, (0, 1))
        self.src.add_aux_coord(self.src_x_transpose, (1, 0))
        self.grid.add_dim_coord(self.grid_y_inc, 0)
        self.grid.add_dim_coord(self.grid_x_inc, 1)
        result = regrid(self.src, self.weights, self.grid)
        data = np.array([0,
                         self._weighted_mean([3]),
                         self._weighted_mean([7, 8]),
                         self._weighted_mean([9, 10, 11])]).reshape(2, 2)
        expected = self._expected_cube(data)
        self.assertEqual(result, expected)
        mask = np.array([[True, False], [False, False]])
        self.assertArrayEqual(result.data.mask, mask)

    def test_aligned_src_y_transpose(self):
        self.src.add_aux_coord(self.src_y_transpose, (1, 0))
        self.src.add_aux_coord(self.src_x_positive, (0, 1))
        self.grid.add_dim_coord(self.grid_y_inc, 0)
        self.grid.add_dim_coord(self.grid_x_inc, 1)
        result = regrid(self.src, self.weights, self.grid)
        data = np.array([0,
                         self._weighted_mean([3]),
                         self._weighted_mean([7, 8]),
                         self._weighted_mean([9, 10, 11])]).reshape(2, 2)
        expected = self._expected_cube(data)
        self.assertEqual(result, expected)
        mask = np.array([[True, False], [False, False]])
        self.assertArrayEqual(result.data.mask, mask)

    def test_aligned_tgt_dec(self):
        self.src.add_aux_coord(self.src_y, (0, 1))
        self.src.add_aux_coord(self.src_x_positive, (0, 1))
        self.grid.add_dim_coord(self.grid_y_dec, 0)
        self.grid.add_dim_coord(self.grid_x_dec, 1)
        result = regrid(self.src, self.weights, self.grid)
        data = np.array([self._weighted_mean([10, 11, 12]),
                         self._weighted_mean([8, 9]),
                         self._weighted_mean([3, 4]),
                         0]).reshape(2, 2)
        expected = self._expected_cube(data)
        self.assertEqual(result, expected)
        mask = np.array([[False, False], [False, True]])
        self.assertArrayEqual(result.data.mask, mask)

    def test_misaligned_src_x_negative(self):
        self.src.add_aux_coord(self.src_y, (0, 1))
        self.src.add_aux_coord(self.src_x_negative, (0, 1))
        self.grid.add_dim_coord(self.grid_y_inc, 0)
        self.grid.add_dim_coord(self.grid_x_inc, 1)
        result = regrid(self.src, self.weights, self.grid)
        data = np.array([self._weighted_mean([1, 2]),
                         self._weighted_mean([3, 4]),
                         self._weighted_mean([5, 6, 7, 8]),
                         self._weighted_mean([9, 10, 11])]).reshape(2, 2)
        expected = self._expected_cube(data)
        self.assertEqual(result, expected)
        mask = np.array([[False, False], [False, False]])
        self.assertArrayEqual(result.data.mask, mask)

    def test_misaligned_src_x_negative_mask(self):
        self.src.add_aux_coord(self.src_y, (0, 1))
        self.src.add_aux_coord(self.src_x_negative, (0, 1))
        self.src.data[([0, 0, 1, 1, 2, 2],
                       [1, 3, 1, 3, 1, 3])] = ma.masked
        self.grid.add_dim_coord(self.grid_y_inc, 0)
        self.grid.add_dim_coord(self.grid_x_inc, 1)
        result = regrid(self.src, self.weights, self.grid)
        data = np.array([self._weighted_mean([1]),
                         self._weighted_mean([3]),
                         self._weighted_mean([5, 7]),
                         self._weighted_mean([9, 11])]).reshape(2, 2)
        expected = self._expected_cube(data)
        self.assertEqual(result, expected)
        mask = np.array([[False, False], [False, False]])
        self.assertArrayEqual(result.data.mask, mask)

    def test_misaligned_tgt_dec(self):
        self.src.add_aux_coord(self.src_y, (0, 1))
        self.src.add_aux_coord(self.src_x_negative, (0, 1))
        self.grid.add_dim_coord(self.grid_y_dec, 0)
        self.grid.add_dim_coord(self.grid_x_dec, 1)
        result = regrid(self.src, self.weights, self.grid)
        data = np.array([self._weighted_mean([10, 11, 12]),
                         self._weighted_mean([6, 7, 8, 9]),
                         self._weighted_mean([4]),
                         self._weighted_mean([2, 3])]).reshape(2, 2)
        expected = self._expected_cube(data)
        self.assertEqual(result, expected)
        mask = np.array([[False, False], [False, False]])
        self.assertArrayEqual(result.data.mask, mask)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test__regrid_bilinear_array
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test function
:func:`iris.experimental.regrid._regrid_bilinear_array`.

"""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np

import iris
from iris.experimental.regrid import _regrid_bilinear_array


class Test(tests.IrisTest):

    def setUp(self):
        self.x = iris.coords.DimCoord(np.linspace(-2, 57, 60))
        self.y = iris.coords.DimCoord(np.linspace(0, 49, 50))
        self.xs, self.ys = np.meshgrid(self.x.points, self.y.points)

        transformation = lambda x, y: x + y ** 2
        # Construct a function which adds dimensions to the 2D data array
        # so that we can test higher dimensional functionality.
        dim_extender = lambda arr: (arr[np.newaxis, ..., np.newaxis] * [1, 2])

        self.data = dim_extender(transformation(self.xs, self.ys))

        target_x = np.linspace(-3, 60, 4)
        target_y = np.linspace(0.5, 51, 3)
        self.target_x, self.target_y = np.meshgrid(target_x, target_y)

        #: Expected values, which not quite the analytical value, but
        #: representative of the bilinear interpolation scheme.
        self.expected = np.array([[[[np.nan, np.nan],
                                    [18.5, 37.],
                                    [39.5, 79.],
                                    [np.nan, np.nan]],
                                   [[np.nan, np.nan],
                                    [681.25, 1362.5],
                                    [702.25, 1404.5],
                                    [np.nan, np.nan]],
                                   [[np.nan, np.nan],
                                    [np.nan, np.nan],
                                    [np.nan, np.nan],
                                    [np.nan, np.nan]]]])

        self.x_dim = 2
        self.y_dim = 1

    def assert_values(self, values):
        # values is a list of [x, y, [val1, val2]]
        xs, ys, expecteds = zip(*values)
        expecteds = np.array(expecteds)[None, None, ...]
        result = _regrid_bilinear_array(self.data, self.x_dim, self.y_dim,
                                        self.x, self.y,
                                        np.array([xs]), np.array([ys]))
        self.assertArrayAllClose(result, expecteds, rtol=1e-04)

        # Check that transposing the input data results in the same values
        ndim = self.data.ndim
        result2 = _regrid_bilinear_array(self.data.T, ndim - self.x_dim - 1,
                                         ndim - self.y_dim - 1,
                                         self.x, self.y,
                                         np.array([xs]), np.array([ys]))
        self.assertArrayEqual(result.T, result2)

    def test_single_values(self):
        # Check that the values are sensible e.g. (3 + 4**2 == 19)
        self.assert_values([[3, 4, [19, 38]],
                            [-2, 0, [-2, -4]],
                            [-2.01, 0, [np.nan, np.nan]],
                            [2, -0.01, [np.nan, np.nan]],
                            [57, 0, [57, 114]],
                            [57.01, 0, [np.nan, np.nan]],
                            [57, 49, [2458, 4916]],
                            [57, 49.01, [np.nan, np.nan]]])

    def test_simple_result(self):
        result = _regrid_bilinear_array(self.data, self.x_dim, self.y_dim,
                                        self.x, self.y,
                                        self.target_x, self.target_y)
        self.assertArrayEqual(result, self.expected)

    def test_simple_masked(self):
        data = np.ma.MaskedArray(self.data, mask=True)
        data.mask[:, 1:30, 1:30] = False
        result = _regrid_bilinear_array(data, self.x_dim, self.y_dim,
                                        self.x, self.y,
                                        self.target_x, self.target_y)
        expected_mask = np.array([[[[True, True], [True, True],
                                    [True, True], [True, True]],
                                   [[True, True], [False, False],
                                    [True, True], [True, True]],
                                   [[True, True], [True, True],
                                    [True, True], [True, True]]]], dtype=bool)
        expected = np.ma.MaskedArray(self.expected,
                                     mask=expected_mask)
        self.assertMaskedArrayEqual(result, expected)

    def test_simple_masked_no_mask(self):
        data = np.ma.MaskedArray(self.data, mask=False)
        result = _regrid_bilinear_array(data, self.x_dim, self.y_dim,
                                        self.x, self.y,
                                        self.target_x, self.target_y)
        self.assertIsInstance(result, np.ma.MaskedArray)

    def test_result_transpose_shape(self):
        ndim = self.data.ndim
        result = _regrid_bilinear_array(self.data.T, ndim - self.x_dim - 1,
                                        ndim - self.y_dim - 1, self.x, self.y,
                                        self.target_x, self.target_y)
        self.assertArrayEqual(result, self.expected.T)

    def test_reverse_x_coord(self):
        index = [slice(None)] * self.data.ndim
        index[self.x_dim] = slice(None, None, -1)
        result = _regrid_bilinear_array(self.data[index], self.x_dim,
                                        self.y_dim, self.x[::-1], self.y,
                                        self.target_x, self.target_y)
        self.assertArrayEqual(result, self.expected)

    def test_circular_x_coord(self):
        # Check that interpolation of a circular src coordinate doesn't result
        # in an out of bounds value.
        self.x.circular = True
        self.x.units = 'degree'
        result = _regrid_bilinear_array(self.data, self.x_dim, self.y_dim,
                                        self.x, self.y, np.array([[58]]),
                                        np.array([[0]]))
        self.assertArrayAlmostEqual(result,
                                    np.array([56.80398671, 113.60797342],
                                             ndmin=self.data.ndim))


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_ABFField
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.abf.ABFField` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock
import numpy as np

from iris.fileformats.abf import ABFField


class MethodCounter(object):
    def __init__(self, method_name):
        self.method_name = method_name
        self.count = 0

    def __enter__(self):
        self.orig_method = getattr(ABFField, self.method_name)

        def new_method(*args, **kwargs):
            self.count += 1
            self.orig_method(*args, **kwargs)

        setattr(ABFField, self.method_name, new_method)
        return self

    def __exit__(self, exc_type, exc_value, traceback):
        setattr(ABFField, self.method_name, self.orig_method)
        return False


class Test_data(tests.IrisTest):
    def test_single_read(self):
        path = '0000000000000000jan00000'
        field = ABFField(path)

        with mock.patch('iris.fileformats.abf.np.fromfile') as fromfile:
            with MethodCounter('__getattr__') as getattr:
                with MethodCounter('_read') as read:
                    field.data

        fromfile.assert_called_once_with(path, dtype='>u1')
        self.assertEqual(getattr.count, 1)
        self.assertEqual(read.count, 1)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_CFReader
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for the `iris.fileformats.cf.CFReader` class.

"""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock
import warnings

import numpy as np

import iris
from iris.fileformats.cf import CFReader


def netcdf_variable(name, dimensions, dtype, ancillary_variables=None,
                    coordinates=None, bounds=None, climatology=None,
                    formula_terms=None, grid_mapping=None,
                    cell_measures=None):
    """Return a mock NetCDF4 variable."""
    ndim = 0
    if dimensions is not None:
        dimensions = dimensions.split()
        ndim = len(dimensions)
    else:
        dimensions = []
    ncvar = mock.Mock(name=name, dimensions=dimensions,
                      ncattrs=mock.Mock(return_value=[]),
                      ndim=ndim, dtype=dtype,
                      ancillary_variables=ancillary_variables,
                      coordinates=coordinates,
                      bounds=bounds, climatology=climatology,
                      formula_terms=formula_terms,
                      grid_mapping=grid_mapping, cell_measures=cell_measures)
    return ncvar


class Test_translate__formula_terms(tests.IrisTest):
    def setUp(self):
        self.delta = netcdf_variable('delta', 'height', np.float,
                                     bounds='delta_bnds')
        self.delta_bnds = netcdf_variable('delta_bnds', 'height bnds',
                                          np.float)
        self.sigma = netcdf_variable('sigma', 'height', np.float,
                                     bounds='sigma_bnds')
        self.sigma_bnds = netcdf_variable('sigma_bnds', 'height bnds',
                                          np.float)
        self.orography = netcdf_variable('orography', 'lat lon', np.float)
        formula_terms = 'a: delta b: sigma orog: orography'
        self.height = netcdf_variable('height', 'height', np.float,
                                      formula_terms=formula_terms,
                                      bounds='height_bnds')
        # Over-specify the formula terms on the bounds variable,
        # which will be ignored by the cf loader.
        formula_terms = 'a: delta_bnds b: sigma_bnds orog: orography'
        self.height_bnds = netcdf_variable('height_bnds', 'height bnds',
                                           np.float,
                                           formula_terms=formula_terms)
        self.lat = netcdf_variable('lat', 'lat', np.float)
        self.lon = netcdf_variable('lon', 'lon', np.float)
        # Note that, only lat and lon are explicitly associated as coordinates.
        self.temp = netcdf_variable('temp', 'height lat lon', np.float,
                                    coordinates='lat lon')

        self.variables = dict(delta=self.delta, sigma=self.sigma,
                              orography=self.orography, height=self.height,
                              lat=self.lat, lon=self.lon, temp=self.temp,
                              delta_bnds=self.delta_bnds,
                              sigma_bnds=self.sigma_bnds,
                              height_bnds=self.height_bnds)
        ncattrs = mock.Mock(return_value=[])
        self.dataset = mock.Mock(file_format='NetCDF4',
                                 variables=self.variables,
                                 ncattrs=ncattrs)
        # Restrict the CFReader functionality to only performing translations.
        build_patch = mock.patch(
            'iris.fileformats.cf.CFReader._build_cf_groups')
        reset_patch = mock.patch('iris.fileformats.cf.CFReader._reset')
        build_patch.start()
        reset_patch.start()
        self.addCleanup(build_patch.stop)
        self.addCleanup(reset_patch.stop)

    def test_create_formula_terms(self):
        with mock.patch('netCDF4.Dataset', return_value=self.dataset):
            cf_group = CFReader('dummy').cf_group
            self.assertEqual(len(cf_group), len(self.variables))
            # Check there is a singular data variable.
            group = cf_group.data_variables
            self.assertEqual(len(group), 1)
            self.assertEqual(group.keys(), ['temp'])
            self.assertIs(group['temp'].cf_data, self.temp)
            # Check there are three coordinates.
            group = cf_group.coordinates
            self.assertEqual(len(group), 3)
            coordinates = ['height', 'lat', 'lon']
            self.assertEqual(group.viewkeys(), set(coordinates))
            for name in coordinates:
                self.assertIs(group[name].cf_data, getattr(self, name))
            # Check there are three auxiliary coordinates.
            group = cf_group.auxiliary_coordinates
            self.assertEqual(len(group), 3)
            aux_coordinates = ['delta', 'sigma', 'orography']
            self.assertEqual(group.viewkeys(), set(aux_coordinates))
            for name in aux_coordinates:
                self.assertIs(group[name].cf_data, getattr(self, name))
            # Check all the auxiliary coordinates are formula terms.
            formula_terms = cf_group.formula_terms
            self.assertEqual(group.viewitems(), formula_terms.viewitems())
            # Check there are three bounds.
            group = cf_group.bounds
            self.assertEqual(len(group), 3)
            bounds = ['height_bnds', 'delta_bnds', 'sigma_bnds']
            self.assertEqual(group.viewkeys(), set(bounds))
            for name in bounds:
                self.assertEqual(group[name].cf_data, getattr(self, name))


class Test_build_cf_groups__formula_terms(tests.IrisTest):
    def setUp(self):
        self.delta = netcdf_variable('delta', 'height', np.float,
                                     bounds='delta_bnds')
        self.delta_bnds = netcdf_variable('delta_bnds', 'height bnds',
                                          np.float)
        self.sigma = netcdf_variable('sigma', 'height', np.float,
                                     bounds='sigma_bnds')
        self.sigma_bnds = netcdf_variable('sigma_bnds', 'height bnds',
                                          np.float)
        self.orography = netcdf_variable('orography', 'lat lon', np.float)
        formula_terms = 'a: delta b: sigma orog: orography'
        self.height = netcdf_variable('height', 'height', np.float,
                                      formula_terms=formula_terms,
                                      bounds='height_bnds')
        # Over-specify the formula terms on the bounds variable,
        # which will be ignored by the cf loader.
        formula_terms = 'a: delta_bnds b: sigma_bnds orog: orography'
        self.height_bnds = netcdf_variable('height_bnds', 'height bnds',
                                           np.float,
                                           formula_terms=formula_terms)
        self.lat = netcdf_variable('lat', 'lat', np.float)
        self.lon = netcdf_variable('lon', 'lon', np.float)
        # Note that, only lat and lon are explicitly associated as coordinates.
        self.temp = netcdf_variable('temp', 'height lat lon', np.float,
                                    coordinates='lat lon')

        self.variables = dict(delta=self.delta, sigma=self.sigma,
                              orography=self.orography, height=self.height,
                              lat=self.lat, lon=self.lon, temp=self.temp,
                              delta_bnds=self.delta_bnds,
                              sigma_bnds=self.sigma_bnds,
                              height_bnds=self.height_bnds)
        ncattrs = mock.Mock(return_value=[])
        self.dataset = mock.Mock(file_format='NetCDF4',
                                 variables=self.variables,
                                 ncattrs=ncattrs)
        # Restrict the CFReader functionality to only performing translations
        # and building first level cf-groups for variables.
        patcher = mock.patch('iris.fileformats.cf.CFReader._reset')
        patcher.start()
        self.addCleanup(patcher.stop)

    def test_associate_formula_terms_with_data_variable(self):
        with mock.patch('netCDF4.Dataset', return_value=self.dataset):
            cf_group = CFReader('dummy').cf_group
            self.assertEqual(len(cf_group), len(self.variables))
            # Check the cf-group associated with the data variable.
            temp_cf_group = cf_group['temp'].cf_group
            # Check the data variable is associated with six variables.
            self.assertEqual(len(temp_cf_group), 6)
            # Check there are three coordinates.
            group = temp_cf_group.coordinates
            self.assertEqual(len(group), 3)
            coordinates = ['height', 'lat', 'lon']
            self.assertEqual(group.viewkeys(), set(coordinates))
            for name in coordinates:
                self.assertIs(group[name].cf_data, getattr(self, name))
            # Check the height coordinate is bounded.
            group = group['height'].cf_group
            self.assertEqual(len(group.bounds), 1)
            self.assertIn('height_bnds', group.bounds)
            self.assertIs(group['height_bnds'].cf_data, self.height_bnds)
            # Check there are three auxiliary coordinates.
            group = temp_cf_group.auxiliary_coordinates
            self.assertEqual(len(group), 3)
            aux_coordinates = ['delta', 'sigma', 'orography']
            self.assertEqual(group.viewkeys(), set(aux_coordinates))
            for name in aux_coordinates:
                self.assertIs(group[name].cf_data, getattr(self, name))
            # Check all the auxiliary coordinates are formula terms.
            formula_terms = cf_group.formula_terms
            self.assertEqual(group.viewitems(), formula_terms.viewitems())
            # Check the terms by root.
            for name, term in zip(aux_coordinates, ['a', 'b', 'orog']):
                self.assertEqual(formula_terms[name].cf_terms_by_root,
                                 dict(height=term))
            # Check the bounded auxiliary coordinates.
            for name, name_bnds in zip(['delta', 'sigma'],
                                       ['delta_bnds', 'sigma_bnds']):
                aux_coord_group = group[name].cf_group
                self.assertEqual(len(aux_coord_group.bounds), 1)
                self.assertIn(name_bnds, aux_coord_group.bounds)
                self.assertIs(aux_coord_group[name_bnds].cf_data,
                              getattr(self, name_bnds))

    def test_formula_terms_dimension_mismatch(self):
        self.orography.dimensions = 'lat wibble'
        with mock.patch('netCDF4.Dataset', return_value=self.dataset):
            with warnings.catch_warnings(record=True) as warn:
                warnings.simplefilter('always')
                CFReader('dummy')
                self.assertEqual(len(warn), 1)
                self.assertIn('dimension mis-match', warn[0].message.message)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_ArakawaC
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for :class:`iris.fileformat.ff.ArakawaC`."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock
import numpy as np

from iris.fileformats.ff import ArakawaC


class Test__x_vectors(tests.IrisTest):
    def _test(self, column, horiz_grid_type, xp, xu):
        reals = np.arange(6) + 100
        grid = ArakawaC(column, None, reals, horiz_grid_type)
        result_xp, result_xu = grid._x_vectors()
        self.assertArrayEqual(result_xp, xp)
        self.assertArrayEqual(result_xu, xu)

    def test_none(self):
        self._test(column=None, horiz_grid_type=None, xp=None, xu=None)

    def test_1d(self):
        self._test(column=np.array([[0], [1], [2], [3]]),
                   horiz_grid_type=None,
                   xp=np.array([0, 1, 2, 3]), xu=None)

    def test_2d_no_wrap(self):
        self._test(column=np.array([[0, 0], [1, 10], [2, 20], [3, 30]]),
                   horiz_grid_type=1,
                   xp=np.array([0, 1, 2, 3]),
                   xu=np.array([0, 10, 20, 30]))

    def test_2d_with_wrap(self):
        self._test(column=np.array([[0, 0], [1, 10], [2, 20], [3, 30]]),
                   horiz_grid_type=0,
                   xp=np.array([0, 1, 2, 3]),
                   xu=np.array([0, 10, 20]))


class Test_regular_x(tests.IrisTest):
    def _test(self, subgrid, bzx, bdx):
        grid = ArakawaC(None, None, [4.0, None, None, -5.0, None, None], None)
        result_bzx, result_bdx = grid.regular_x(subgrid)
        self.assertEqual(result_bzx, bzx)
        self.assertEqual(result_bdx, bdx)

    def test_theta_subgrid(self):
        self._test(1, -9.0, 4.0)

    def test_u_subgrid(self):
        self._test(11, -7.0, 4.0)


class Test_regular_y(tests.IrisTest):
    def _test(self, v_offset, subgrid, bzy, bdy):
        grid = ArakawaC(None, None, [None, 4.0, 45.0, None, None, None], None)
        grid._v_offset = v_offset
        result_bzy, result_bdy = grid.regular_y(subgrid)
        self.assertEqual(result_bzy, bzy)
        self.assertEqual(result_bdy, bdy)

    def test_theta_subgrid_NewDynamics(self):
        self._test(0.5, 1, 41.0, 4.0)

    def test_v_subgrid_NewDynamics(self):
        self._test(0.5, 11, 43.0, 4.0)

    def test_theta_subgrid_ENDGame(self):
        self._test(-0.5, 1, 41.0, 4.0)

    def test_v_subgrid_ENDGame(self):
        self._test(-0.5, 11, 39.0, 4.0)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_ENDGame
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for :class:`iris.fileformat.ff.ENDGame`."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock
import numpy as np

from iris.fileformats.ff import ENDGame


class Test(tests.IrisTest):
    def test_class_attributes(self):
        reals = np.arange(6) + 100
        grid = ENDGame(None, None, reals, None)
        self.assertEqual(grid._v_offset, -0.5)


class Test__y_vectors(tests.IrisTest):
    def _test(self, row, yp, yv):
        reals = np.arange(6) + 100
        grid = ENDGame(None, row, reals, None)
        result_yp, result_yv = grid._y_vectors()
        self.assertArrayEqual(result_yp, yp)
        self.assertArrayEqual(result_yv, yv)

    def test_none(self):
        self._test(row=None, yp=None, yv=None)

    def test_1d(self):
        self._test(row=np.array([[0], [1], [2], [3]]),
                   yp=np.array([0, 1, 2]), yv=None)

    def test_2d(self):
        self._test(row=np.array([[0, 0], [1, 10], [2, 20], [3, 30]]),
                   yp=np.array([0, 1, 2]), yv=np.array([0, 10, 20, 30]))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_FF2PP
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :class:`iris.fileformat.ff.FF2PP` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import contextlib

import mock
import numpy as np

from iris.fileformats.ff import FF2PP
import iris.fileformats.ff as ff
import iris.fileformats.pp as pp


class Test____iter__(tests.IrisTest):
    @mock.patch('iris.fileformats.ff.FFHeader')
    def test_call_structure(self, _FFHeader):
        # Check that the iter method calls the two necessary utility
        # functions
        extract_result = mock.Mock()
        interpret_patch = mock.patch('iris.fileformats.pp._interpret_fields',
                                     autospec=True, return_value=iter([]))
        extract_patch = mock.patch('iris.fileformats.ff.FF2PP._extract_field',
                                   autospec=True, return_value=extract_result)

        FF2PP_instance = ff.FF2PP('mock')
        with interpret_patch as interpret, extract_patch as extract:
            list(iter(FF2PP_instance))

        interpret.assert_called_once_with(extract_result)
        extract.assert_called_once_with(FF2PP_instance)


class Test__extract_field__LBC_format(tests.IrisTest):
    @contextlib.contextmanager
    def mock_for_extract_field(self, fields, x=None, y=None):
        """
        A context manager to ensure FF2PP._extract_field gets a field
        instance looking like the next one in the "fields" iterable from
        the "make_pp_field" call.

        """
        with mock.patch('iris.fileformats.ff.FFHeader'):
            ff2pp = ff.FF2PP('mock')
        ff2pp._ff_header.lookup_table = [0, 0, len(fields)]
        grid = mock.Mock()
        grid.vectors = mock.Mock(return_value=(x, y))
        ff2pp._ff_header.grid = mock.Mock(return_value=grid)

        with mock.patch('numpy.fromfile', return_value=[0]), \
                mock.patch('__builtin__.open'), \
                mock.patch('struct.unpack_from', return_value=[4]), \
                mock.patch('iris.fileformats.pp.make_pp_field',
                           side_effect=fields), \
                mock.patch('iris.fileformats.ff.FF2PP._payload',
                           return_value=(0, 0)):
            yield ff2pp

    def test_LBC_header(self):
        bzx, bzy = -10, 15
        # stash m01s00i001
        lbuser = [None, None, 121416, 1, None, None, 1]
        field = mock.Mock(lbegin=0,
                          lbrow=10, lbnpt=12, bdx=1, bdy=1, bzx=bzx, bzy=bzy,
                          lbuser=lbuser)
        with self.mock_for_extract_field([field]) as ff2pp:
            ff2pp._ff_header.dataset_type = 5
            result = list(ff2pp._extract_field())

        self.assertEqual([field], result)
        self.assertEqual(field.lbrow, 10 + 14 * 2)
        self.assertEqual(field.lbnpt, 12 + 16 * 2)

        name_mapping_dict = dict(rim_width=slice(4, 6), y_halo=slice(2, 4),
                                 x_halo=slice(0, 2))
        boundary_packing = pp.SplittableInt(121416, name_mapping_dict)

        self.assertEqual(field.lbpack.boundary_packing, boundary_packing)
        self.assertEqual(field.bzy, bzy - boundary_packing.y_halo * field.bdy)
        self.assertEqual(field.bzx, bzx - boundary_packing.x_halo * field.bdx)

    def check_non_trivial_coordinate_warning(self, field):
        field.lbegin = 0
        field.lbrow = 10
        field.lbnpt = 12
        # stash m01s31i020
        field.lbuser = [None, None, 121416, 20, None, None, 1]
        orig_bdx, orig_bdy = field.bdx, field.bdy

        x = np.array([1, 2, 6])
        y = np.array([1, 2, 6])
        with self.mock_for_extract_field([field], x, y) as ff2pp:
            ff2pp._ff_header.dataset_type = 5
            with mock.patch('warnings.warn') as warn:
                list(ff2pp._extract_field())

        # Check the values are unchanged.
        self.assertEqual(field.bdy, orig_bdy)
        self.assertEqual(field.bdx, orig_bdx)

        # Check a warning was raised with a suitable message.
        warn_error_tmplt = 'Unexpected warning message: {}'
        non_trivial_coord_warn_msg = warn.call_args[0][0]
        msg = ('The x or y coordinates of your boundary condition field may '
               'be incorrect, not having taken into account the boundary '
               'size.')
        self.assertTrue(non_trivial_coord_warn_msg.startswith(msg),
                        warn_error_tmplt.format(non_trivial_coord_warn_msg))

    def test_LBC_header_non_trivial_coords_both(self):
        # Check a warning is raised when both bdx and bdy are bad.
        field = mock.Mock(bdx=0, bdy=0, bzx=10, bzy=10)
        self.check_non_trivial_coordinate_warning(field)

        field.bdy = field.bdx = field.bmdi
        self.check_non_trivial_coordinate_warning(field)

    def test_LBC_header_non_trivial_coords_x(self):
        # Check a warning is raised when bdx is bad.
        field = mock.Mock(bdx=0, bdy=10, bzx=10, bzy=10)
        self.check_non_trivial_coordinate_warning(field)

        field.bdx = field.bmdi
        self.check_non_trivial_coordinate_warning(field)

    def test_LBC_header_non_trivial_coords_y(self):
        # Check a warning is raised when bdy is bad.
        field = mock.Mock(bdx=10, bdy=0, bzx=10, bzy=10)
        self.check_non_trivial_coordinate_warning(field)

        field.bdy = field.bmdi
        self.check_non_trivial_coordinate_warning(field)

    def test_negative_bdy(self):
        # Check a warning is raised when bdy is negative,
        # we don't yet know what "north" means in this case.
        field = mock.Mock(bdx=10, bdy=-10, bzx=10, bzy=10, lbegin=0,
                          lbuser=[0, 0, 121416, 0, None, None, 0],
                          lbrow=10, lbnpt=12)
        with self.mock_for_extract_field([field]) as ff2pp:
            ff2pp._ff_header.dataset_type = 5
            with mock.patch('warnings.warn') as warn:
                list(ff2pp._extract_field())
        msg = 'The LBC has a bdy less than 0.'
        self.assertTrue(warn.call_args[0][0].startswith(msg),
                        'Northwards bdy warning not correctly raised.')


class Test__det_border(tests.IrisTest):
    def setUp(self):
        _FFH_patch = mock.patch('iris.fileformats.ff.FFHeader')
        _FFH_patch.start()
        self.addCleanup(_FFH_patch.stop)

    def test_unequal_spacing_eitherside(self):
        # Ensure that we do not interpret the case where there is not the same
        # spacing on the lower edge as the upper edge.
        ff2pp = FF2PP('dummy')
        field_x = np.array([1, 2, 10])

        msg = ('The x or y coordinates of your boundary condition field may '
               'be incorrect, not having taken into account the boundary '
               'size.')

        with mock.patch('warnings.warn') as warn:
            result = ff2pp._det_border(field_x, None)
        warn.assert_called_with(msg)
        self.assertIs(result, field_x)

    def test_increasing_field_values(self):
        # Field where its values a increasing.
        ff2pp = FF2PP('dummy')
        field_x = np.array([1, 2, 3])
        com = np.array([0, 1, 2, 3, 4])
        result = ff2pp._det_border(field_x, 1)
        self.assertArrayEqual(result, com)

    def test_decreasing_field_values(self):
        # Field where its values a decreasing.
        ff2pp = FF2PP('dummy')
        field_x = np.array([3, 2, 1])
        com = np.array([4, 3, 2, 1, 0])
        result = ff2pp._det_border(field_x, 1)
        self.assertArrayEqual(result, com)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_FFHeader
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for :class:`iris.fileformat.ff.FFHeader`."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import collections

import mock

from iris.fileformats.ff import FFHeader


MyGrid = collections.namedtuple('MyGrid', 'column row real horiz_grid_type')


class Test_grid(tests.IrisTest):
    def _header(self, grid_staggering):
        with mock.patch.object(FFHeader, '__init__',
                               mock.Mock(return_value=None)):
            header = FFHeader()
        header.grid_staggering = grid_staggering
        header.column_dependent_constants = mock.sentinel.column
        header.row_dependent_constants = mock.sentinel.row
        header.real_constants = mock.sentinel.real
        header.horiz_grid_type = mock.sentinel.horiz_grid_type
        return header

    def _test_grid_staggering(self, grid_staggering):
        header = self._header(grid_staggering)
        with mock.patch.dict(FFHeader.GRID_STAGGERING_CLASS,
                             {grid_staggering: MyGrid}):
            grid = header.grid()
        self.assertIsInstance(grid, MyGrid)
        self.assertIs(grid.column, mock.sentinel.column)
        self.assertIs(grid.row, mock.sentinel.row)
        self.assertIs(grid.real, mock.sentinel.real)
        self.assertIs(grid.horiz_grid_type, mock.sentinel.horiz_grid_type)

    def test_new_dynamics(self):
        self._test_grid_staggering(3)

    def test_end_game(self):
        self._test_grid_staggering(6)

    def test_unknown(self):
        header = self._header(0)
        with mock.patch('iris.fileformats.ff.NewDynamics',
                        mock.Mock(return_value=mock.sentinel.grid)):
            with mock.patch('warnings.warn') as warn:
                grid = header.grid()
        warn.assert_called_with('Staggered grid type: 0 not currently'
                                ' interpreted, assuming standard C-grid')
        self.assertIs(grid, mock.sentinel.grid)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_Grid
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for :class:`iris.fileformat.ff.Grid`."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock
import numpy as np

from iris.fileformats.ff import Grid


class Test___init__(tests.IrisTest):
    def test_attributes(self):
        # Ensure the constructor initialises all the grid's attributes
        # correctly, including unpacking values from the REAL constants.
        reals = (mock.sentinel.ew, mock.sentinel.ns,
                 mock.sentinel.first_lat, mock.sentinel.first_lon,
                 mock.sentinel.pole_lat, mock.sentinel.pole_lon)
        grid = Grid(mock.sentinel.column, mock.sentinel.row, reals,
                    mock.sentinel.horiz_grid_type)
        self.assertIs(grid.column_dependent_constants, mock.sentinel.column)
        self.assertIs(grid.row_dependent_constants, mock.sentinel.row)
        self.assertIs(grid.ew_spacing, mock.sentinel.ew)
        self.assertIs(grid.ns_spacing, mock.sentinel.ns)
        self.assertIs(grid.first_lat, mock.sentinel.first_lat)
        self.assertIs(grid.first_lon, mock.sentinel.first_lon)
        self.assertIs(grid.pole_lat, mock.sentinel.pole_lat)
        self.assertIs(grid.pole_lon, mock.sentinel.pole_lon)
        self.assertIs(grid.horiz_grid_type, mock.sentinel.horiz_grid_type)


class Test_vectors(tests.IrisTest):
    def setUp(self):
        self.xp = mock.sentinel.xp
        self.xu = mock.sentinel.xu
        self.yp = mock.sentinel.yp
        self.yv = mock.sentinel.yv

    def _test_subgrid_vectors(self, subgrid, expected):
        grid = Grid(None, None, (None,) * 6, None)
        dummy_vectors = (self.xp, self.yp, self.xu, self.yv)
        grid._x_vectors = mock.Mock(return_value=(self.xp, self.xu))
        grid._y_vectors = mock.Mock(return_value=(self.yp, self.yv))
        result = grid.vectors(subgrid)
        self.assertEqual(result, expected)

    def test_1(self):
        # Data on atmospheric theta points.
        self._test_subgrid_vectors(1, (self.xp, self.yp))

    def test_2(self):
        # Data on atmospheric theta points, values over land only.
        self._test_subgrid_vectors(2, (self.xp, self.yp))

    def test_3(self):
        # Data on atmospheric theta points, values over sea only.
        self._test_subgrid_vectors(3, (self.xp, self.yp))

    def test_4(self):
        # Data on atmospheric zonal theta points.
        self._test_subgrid_vectors(4, (self.xp, self.yp))

    def test_5(self):
        # Data on atmospheric meridional theta points.
        self._test_subgrid_vectors(5, (self.xp, self.yp))

    def test_11(self):
        # Data on atmospheric uv points.
        self._test_subgrid_vectors(11, (self.xu, self.yv))

    def test_18(self):
        # Data on atmospheric u points on the 'c' grid.
        self._test_subgrid_vectors(18, (self.xu, self.yp))

    def test_19(self):
        # Data on atmospheric v points on the 'c' grid.
        self._test_subgrid_vectors(19, (self.xp, self.yv))

    def test_26(self):
        # Lateral boundary data at atmospheric theta points.
        self._test_subgrid_vectors(26, (self.xp, self.yp))

    def test_27(self):
        # Lateral boundary data at atmospheric u points.
        self._test_subgrid_vectors(27, (self.xu, self.yp))

    def test_28(self):
        # Lateral boundary data at atmospheric v points.
        self._test_subgrid_vectors(28, (self.xp, self.yv))

    def test_29(self):
        # Orography field for atmospheric LBCs.
        self._test_subgrid_vectors(29, (self.xp, self.yp))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_NewDynamics
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for :class:`iris.fileformat.ff.NewDynamics`."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock
import numpy as np

from iris.fileformats.ff import NewDynamics


class Test(tests.IrisTest):
    def test_class_attributes(self):
        reals = np.arange(6) + 100
        grid = NewDynamics(None, None, reals, None)
        self.assertEqual(grid._v_offset, 0.5)


class Test__y_vectors(tests.IrisTest):
    def _test(self, row, yp, yv):
        reals = np.arange(6) + 100
        grid = NewDynamics(None, row, reals, None)
        result_yp, result_yv = grid._y_vectors()
        self.assertArrayEqual(result_yp, yp)
        self.assertArrayEqual(result_yv, yv)

    def test_none(self):
        self._test(row=None, yp=None, yv=None)

    def test_1d(self):
        self._test(row=np.array([[0], [1], [2], [3]]),
                   yp=np.array([0, 1, 2, 3]), yv=None)

    def test_2d(self):
        self._test(row=np.array([[0, 0], [1, 10], [2, 20], [3, 30]]),
                   yp=np.array([0, 1, 2, 3]), yv=np.array([0, 10, 20]))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_data
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for :func:`iris.fileformats.grib.grib_save_rules.data`."""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import mock
import numpy as np

import iris.cube

from iris.fileformats.grib.grib_save_rules import data


GRIB_API = 'iris.fileformats.grib.grib_save_rules.gribapi'
GRIB_MESSAGE = mock.sentinel.GRIB_MESSAGE


class TestMDI(tests.IrisTest):
    def assertBitmapOff(self, grib_api):
        # Check the use of a mask has been turned off via:
        #   gribapi.grib_set(grib_message, 'bitmapPresent', 0)
        grib_api.grib_set.assert_called_once_with(GRIB_MESSAGE,
                                                  'bitmapPresent', 0)

    def assertBitmapOn(self, grib_api, fill_value):
        # Check the use of a mask has been turned on via:
        #   gribapi.grib_set(grib_message, 'bitmapPresent', 1)
        #   gribapi.grib_set_double(grib_message, 'missingValue', fill_value)
        grib_api.grib_set.assert_called_once_with(GRIB_MESSAGE,
                                                  'bitmapPresent', 1)
        grib_api.grib_set_double.assert_called_once_with(GRIB_MESSAGE,
                                                         'missingValue',
                                                         fill_value)

    def assertBitmapRange(self, grib_api, min_data, max_data):
        # Check the use of a mask has been turned on via:
        #   gribapi.grib_set(grib_message, 'bitmapPresent', 1)
        #   gribapi.grib_set_double(grib_message, 'missingValue', ...)
        # and that a suitable fill value has been chosen.
        grib_api.grib_set.assert_called_once_with(GRIB_MESSAGE,
                                                  'bitmapPresent', 1)
        args, = grib_api.grib_set_double.call_args_list
        (message, key, fill_value), kwargs = args
        self.assertIs(message, GRIB_MESSAGE)
        self.assertEqual(key, 'missingValue')
        self.assertTrue(fill_value < min_data or fill_value > max_data,
                        'Fill value {} is not outside data range '
                        '{} to {}.'.format(fill_value, min_data, max_data))
        return fill_value

    def assertValues(self, grib_api, values):
        # Check the correct data values have been set via:
        #   gribapi.grib_set_double_array(grib_message, 'values', ...)
        args, = grib_api.grib_set_double_array.call_args_list
        (message, key, values), kwargs = args
        self.assertIs(message, GRIB_MESSAGE)
        self.assertEqual(key, 'values')
        self.assertArrayEqual(values, values)
        self.assertEqual(kwargs, {})

    def test_simple(self):
        # Check the simple case of non-masked data with no scaling.
        cube = iris.cube.Cube(np.arange(5))
        grib_message = mock.sentinel.GRIB_MESSAGE
        with mock.patch(GRIB_API) as grib_api:
            data(cube, grib_message)
        # Check the use of a mask has been turned off.
        self.assertBitmapOff(grib_api)
        # Check the correct data values have been set.
        self.assertValues(grib_api, np.arange(5))

    def test_masked_with_finite_fill_value(self):
        cube = iris.cube.Cube(np.ma.MaskedArray([1.0, 2.0, 3.0, 1.0, 2.0, 3.0],
                                                mask=[0, 0, 0, 1, 1, 1],
                                                fill_value=2000))
        grib_message = mock.sentinel.GRIB_MESSAGE
        with mock.patch(GRIB_API) as grib_api:
            data(cube, grib_message)
        # Check the use of a mask has been turned on.
        FILL = 2000
        self.assertBitmapOn(grib_api, FILL)
        # Check the correct data values have been set.
        self.assertValues(grib_api, [1, 2, 3, FILL, FILL, FILL])

    def test_masked_with_nan_fill_value(self):
        cube = iris.cube.Cube(np.ma.MaskedArray([1.0, 2.0, 3.0, 1.0, 2.0, 3.0],
                                                mask=[0, 0, 0, 1, 1, 1],
                                                fill_value=np.nan))
        grib_message = mock.sentinel.GRIB_MESSAGE
        with mock.patch(GRIB_API) as grib_api:
            data(cube, grib_message)
        # Check the use of a mask has been turned on and a suitable fill
        # value has been chosen.
        FILL = self.assertBitmapRange(grib_api, 1, 3)
        # Check the correct data values have been set.
        self.assertValues(grib_api, [1, 2, 3, FILL, FILL, FILL])

    def test_scaled(self):
        # If the Cube's units don't match the units required by GRIB
        # ensure the data values are scaled correctly.
        cube = iris.cube.Cube(np.arange(5),
                              standard_name='geopotential_height', units='km')
        grib_message = mock.sentinel.GRIB_MESSAGE
        with mock.patch(GRIB_API) as grib_api:
            data(cube, grib_message)
        # Check the use of a mask has been turned off.
        self.assertBitmapOff(grib_api)
        # Check the correct data values have been set.
        self.assertValues(grib_api, np.arange(5) * 1000)

    def test_scaled_with_finite_fill_value(self):
        # When re-scaling masked data with a finite fill value, ensure
        # the fill value and any filled values are also re-scaled.
        cube = iris.cube.Cube(np.ma.MaskedArray([1.0, 2.0, 3.0, 1.0, 2.0, 3.0],
                                                mask=[0, 0, 0, 1, 1, 1],
                                                fill_value=2000),
                              standard_name='geopotential_height', units='km')
        grib_message = mock.sentinel.GRIB_MESSAGE
        with mock.patch(GRIB_API) as grib_api:
            data(cube, grib_message)
        # Check the use of a mask has been turned on.
        FILL = 2000 * 1000
        self.assertBitmapOn(grib_api, FILL)
        # Check the correct data values have been set.
        self.assertValues(grib_api, [1000, 2000, 3000, FILL, FILL, FILL])

    def test_scaled_with_nan_fill_value(self):
        # When re-scaling masked data with a NaN fill value, ensure
        # a fill value is chosen which allows for the scaling, and any
        # filled values match the chosen fill value.
        cube = iris.cube.Cube(np.ma.MaskedArray([-1.0, 2.0, -1.0, 2.0],
                                                mask=[0, 0, 1, 1],
                                                fill_value=np.nan),
                              standard_name='geopotential_height', units='km')
        grib_message = mock.sentinel.GRIB_MESSAGE
        with mock.patch(GRIB_API) as grib_api:
            data(cube, grib_message)
        # Check the use of a mask has been turned on and a suitable fill
        # value has been chosen.
        FILL = self.assertBitmapRange(grib_api, -1000, 2000)
        # Check the correct data values have been set.
        self.assertValues(grib_api, [-1000, 2000, FILL, FILL])


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_non_hybrid_surfaces
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for module-level functions."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import gribapi
import numpy as np

import iris
import iris.cube
import iris.coords
import iris.fileformats.grib.grib_save_rules as grib_save_rules


class Test_non_hybrid_surfaces(tests.IrisTest):
    def test_bounded_altitude_feet(self):
        cube = iris.cube.Cube([0])
        cube.add_aux_coord(iris.coords.AuxCoord(
            1500.0, long_name='altitude', units='ft',
            bounds=np.array([1000.0, 2000.0])))
        grib = gribapi.grib_new_from_samples("GRIB2")
        grib_save_rules.non_hybrid_surfaces(cube, grib)
        self.assertEqual(
            gribapi.grib_get_double(grib, "scaledValueOfFirstFixedSurface"),
            304.0)
        self.assertEqual(
            gribapi.grib_get_double(grib, "scaledValueOfSecondFixedSurface"),
            609.0)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_reference_time
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for `iris.fileformats.grib.grib_save_rules.reference_time`."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import gribapi
import mock

import iris.fileformats.grib
from iris.fileformats.grib.grib_save_rules import reference_time
import iris.tests.stock as stock
from iris.tests.test_grib_load import TestGribSimple


class Test(TestGribSimple):
    @tests.skip_data
    def test_forecast_period(self):
        # The stock cube has a non-compliant forecast_period.
        iris.fileformats.grib.hindcast_workaround = True
        cube = stock.global_grib2()

        grib = mock.Mock()
        mock_gribapi = mock.Mock(spec=gribapi)
        with mock.patch('iris.fileformats.grib.grib_save_rules.gribapi',
                        mock_gribapi):
            reference_time(cube, grib)

        mock_gribapi.assert_has_calls(
            [mock.call.grib_set_long(grib, "significanceOfReferenceTime", 1),
             mock.call.grib_set_long(grib, "dataDate", '19980306'),
             mock.call.grib_set_long(grib, "dataTime", '0300')])

    @tests.skip_data
    def test_no_forecast_period(self):
        # The stock cube has a non-compliant forecast_period.
        iris.fileformats.grib.hindcast_workaround = True
        cube = stock.global_grib2()
        cube.remove_coord("forecast_period")

        grib = mock.Mock()
        mock_gribapi = mock.Mock(spec=gribapi)
        with mock.patch('iris.fileformats.grib.grib_save_rules.gribapi',
                        mock_gribapi):
            reference_time(cube, grib)

        mock_gribapi.assert_has_calls(
            [mock.call.grib_set_long(grib, "significanceOfReferenceTime", 2),
             mock.call.grib_set_long(grib, "dataDate", '19941201'),
             mock.call.grib_set_long(grib, "dataTime", '0000')])


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_type_of_statistical_processing
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for module-level functions."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import gribapi
import mock
import numpy as np

from iris.fileformats.grib.grib_save_rules \
    import type_of_statistical_processing

from iris.tests.test_grib_load import TestGribSimple


class Test(TestGribSimple):
    def test_sum(self):
        cube = mock.Mock()
        cube.cell_methods = [mock.Mock(method='sum', coord_names=['ni'])]

        coord = mock.Mock()
        coord.name = mock.Mock(return_value='ni')

        grib = mock.Mock()
        mock_gribapi = mock.Mock(spec=gribapi)
        with mock.patch('iris.fileformats.grib.grib_save_rules.gribapi',
                        mock_gribapi):
            type_of_statistical_processing(cube, grib, coord)

        mock_gribapi.assert_has_calls(mock.call.grib_set_long(
            grib, "typeOfStatisticalProcessing", 1))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test__missing_forecast_period
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for module-level functions."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import gribapi
import mock
import numpy as np

from iris.fileformats.grib.grib_save_rules import _missing_forecast_period
from iris.tests.test_grib_load import TestGribSimple


class Test(TestGribSimple):
    def test_point(self):
        t_coord = mock.Mock()
        t_coord.has_bounds = mock.Mock(return_value=False)
        t_coord.points = [15]

        cube = mock.Mock()
        cube.coord = mock.Mock(return_value=t_coord)
        rt, rt_meaning, fp, fp_meaning = _missing_forecast_period(cube)

        t_coord.units.assert_has_call(mock.call.num2date(15))
        self.assertEqual((rt_meaning, fp, fp_meaning), (2, 0, 1))

    def test_bounds(self):
        t_coord = mock.Mock()
        t_coord.has_bounds = mock.Mock(return_value=True)
        t_coord.points = [15]
        t_coord.bounds = np.array([[10, 20]])

        cube = mock.Mock()
        cube.coord = mock.Mock(return_value=t_coord)
        rt, rt_meaning, fp, fp_meaning = _missing_forecast_period(cube)

        t_coord.units.assert_has_call(mock.call.num2date(10))
        self.assertEqual((rt_meaning, fp, fp_meaning), (2, 0, 1))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test__non_missing_forecast_period
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for module-level functions."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import gribapi
import mock
import numpy as np

from iris.fileformats.grib.grib_save_rules import _non_missing_forecast_period
import iris.unit


class Test(tests.IrisTest):
    def _cube(self, t_bounds=False):
        time_coord = iris.coords.DimCoord(15, standard_name='time',
                                          units='hours since epoch')
        fp_coord = iris.coords.DimCoord(10, standard_name='forecast_period',
                                        units='hours')
        if t_bounds:
            time_coord.bounds = [[8, 100]]
            fp_coord.bounds = [[3, 95]]
        cube = iris.cube.Cube([23])
        cube.add_dim_coord(time_coord, 0)
        cube.add_aux_coord(fp_coord, 0)
        return cube

    def test_time_point(self):
        cube = self._cube()
        rt, rt_meaning, fp, fp_meaning = _non_missing_forecast_period(cube)
        self.assertEqual((rt_meaning, fp, fp_meaning), (1, 10, 1))

    def test_time_bounds(self):
        cube = self._cube(t_bounds=True)
        rt, rt_meaning, fp, fp_meaning = _non_missing_forecast_period(cube)
        self.assertEqual((rt_meaning, fp, fp_meaning), (1, 3, 1))

    def test_time_bounds_in_minutes(self):
        cube = self._cube(t_bounds=True)
        cube.coord('forecast_period').convert_units('minutes')
        rt, rt_meaning, fp, fp_meaning = _non_missing_forecast_period(cube)
        self.assertEqual((rt_meaning, fp, fp_meaning), (1, 180, 0))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_convert
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for :func:`iris.fileformats.grib.load_rules.convert`."""

# Import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import gribapi
import mock

import iris
from iris.fileformats.rules import Reference
from iris.tests.test_grib_load import TestGribSimple
from iris.tests.unit.fileformats import TestField
import iris.unit

from iris.fileformats.grib import GribWrapper
from iris.fileformats.grib.load_rules import convert


class Test_GribLevels_Mock(TestGribSimple):
    # Unit test levels with mocking.
    def test_grib2_height(self):
        grib = self.mock_grib()
        grib.edition = 2
        grib.typeOfFirstFixedSurface = 103
        grib.scaledValueOfFirstFixedSurface = 12345
        grib.scaleFactorOfFirstFixedSurface = 0
        grib.typeOfSecondFixedSurface = 255
        cube = self.cube_from_message(grib)
        self.assertEqual(
            cube.coord('height'),
            iris.coords.DimCoord(12345, standard_name="height", units="m"))

    def test_grib2_bounded_height(self):
        grib = self.mock_grib()
        grib.edition = 2
        grib.typeOfFirstFixedSurface = 103
        grib.scaledValueOfFirstFixedSurface = 12345
        grib.scaleFactorOfFirstFixedSurface = 0
        grib.typeOfSecondFixedSurface = 103
        grib.scaledValueOfSecondFixedSurface = 54321
        grib.scaleFactorOfSecondFixedSurface = 0
        cube = self.cube_from_message(grib)
        self.assertEqual(
            cube.coord('height'),
            iris.coords.DimCoord(33333, standard_name="height", units="m",
                                 bounds=[[12345, 54321]]))

    def test_grib2_diff_bound_types(self):
        grib = self.mock_grib()
        grib.edition = 2
        grib.typeOfFirstFixedSurface = 103
        grib.scaledValueOfFirstFixedSurface = 12345
        grib.scaleFactorOfFirstFixedSurface = 0
        grib.typeOfSecondFixedSurface = 102
        grib.scaledValueOfSecondFixedSurface = 54321
        grib.scaleFactorOfSecondFixedSurface = 0
        with mock.patch('warnings.warn') as warn:
            cube = self.cube_from_message(grib)
        warn.assert_called_with(
            "Different vertical bound types not yet handled.")


class TestBoundedTime(TestField):
    @staticmethod
    def is_forecast_period(coord):
        return (coord.standard_name == 'forecast_period' and
                coord.units == 'hours')

    @staticmethod
    def is_time(coord):
        return (coord.standard_name == 'time' and
                coord.units == 'hours since epoch')

    def assert_bounded_message(self, **kwargs):
        attributes = {'productDefinitionTemplateNumber': 0,
                      'edition': 1, '_forecastTime': 15,
                      '_forecastTimeUnit': 'hours',
                      'phenomenon_bounds': lambda u: (80, 120),
                      '_phenomenonDateTime': -1}
        attributes.update(kwargs)
        message = mock.Mock(**attributes)
        self._test_for_coord(message, convert, self.is_forecast_period,
                             expected_points=[35],
                             expected_bounds=[[15, 55]])
        self._test_for_coord(message, convert, self.is_time,
                             expected_points=[100],
                             expected_bounds=[[80, 120]])

    def test_time_range_indicator_3(self):
        self.assert_bounded_message(timeRangeIndicator=3)

    def test_time_range_indicator_4(self):
        self.assert_bounded_message(timeRangeIndicator=4)

    def test_time_range_indicator_5(self):
        self.assert_bounded_message(timeRangeIndicator=5)

    def test_time_range_indicator_51(self):
        self.assert_bounded_message(timeRangeIndicator=51)

    def test_time_range_indicator_113(self):
        self.assert_bounded_message(timeRangeIndicator=113)

    def test_time_range_indicator_114(self):
        self.assert_bounded_message(timeRangeIndicator=114)

    def test_time_range_indicator_115(self):
        self.assert_bounded_message(timeRangeIndicator=115)

    def test_time_range_indicator_116(self):
        self.assert_bounded_message(timeRangeIndicator=116)

    def test_time_range_indicator_117(self):
        self.assert_bounded_message(timeRangeIndicator=117)

    def test_time_range_indicator_118(self):
        self.assert_bounded_message(timeRangeIndicator=118)

    def test_time_range_indicator_123(self):
        self.assert_bounded_message(timeRangeIndicator=123)

    def test_time_range_indicator_124(self):
        self.assert_bounded_message(timeRangeIndicator=124)

    def test_time_range_indicator_125(self):
        self.assert_bounded_message(timeRangeIndicator=125)

    def test_product_template_8(self):
        self.assert_bounded_message(edition=2,
                                    productDefinitionTemplateNumber=8)

    def test_product_template_9(self):
        self.assert_bounded_message(edition=2,
                                    productDefinitionTemplateNumber=9)


class Test_GribLevels(tests.IrisTest):
    def test_grib1_hybrid_height(self):
        gm = gribapi.grib_new_from_samples('regular_gg_ml_grib1')
        gw = GribWrapper(gm)
        results = convert(gw)

        factory, = results[0]
        self.assertEqual(factory.factory_class,
                         iris.aux_factory.HybridPressureFactory)
        delta, sigma, ref = factory.args
        self.assertEqual(delta, {'long_name': 'level_pressure'})
        self.assertEqual(sigma, {'long_name': 'sigma'})
        self.assertEqual(ref, Reference(name='surface_pressure'))

        ml_ref = iris.coords.CoordDefn('model_level_number', None, None,
                                       iris.unit.Unit('1'),
                                       {'positive': 'up'}, None)
        lp_ref = iris.coords.CoordDefn(None, 'level_pressure', None,
                                       iris.unit.Unit('Pa'),
                                       {}, None)
        s_ref = iris.coords.CoordDefn(None, 'sigma', None,
                                      iris.unit.Unit('1'),
                                      {}, None)

        aux_coord_defns = [coord._as_defn() for coord, dim in results[8]]
        self.assertIn(ml_ref, aux_coord_defns)
        self.assertIn(lp_ref, aux_coord_defns)
        self.assertIn(s_ref, aux_coord_defns)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_load_cubes
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.grib.load_cubes` function."""
import iris.tests as tests

from iris.fileformats.grib import load_cubes


@tests.skip_data
class Test_load_cubes(tests.IrisTest):

    def test_reduced_raw(self):
        # Loading a GRIB message defined on a reduced grid without
        # interpolating to a regular grid.
        gribfile = tests.get_data_path(
            ("GRIB", "reduced", "reduced_gg.grib2"))
        grib_generator = load_cubes(gribfile, auto_regularise=False)
        self.assertCML(next(grib_generator))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_generate_cubes
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for :func:`iris.analysis.name_loaders._generate_cubes`.

"""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

import iris.cube
import iris.fileformats.name_loaders
from iris.fileformats.name_loaders import _generate_cubes


class TestCellMethods(tests.IrisTest):
    def test_cell_methods(self):
        header = mock.MagicMock()
        column_headings = {'Species': [1, 2, 3], 'Quantity': [4, 5, 6],
                           "Unit": ['m', 'm', 'm'], 'Z': [1, 2, 3]}
        coords = mock.MagicMock()
        data_arrays = [mock.Mock(), mock.Mock()]
        cell_methods = ["cell_method_1", "cell_method_2"]

        with mock.patch('iris.fileformats.name_loaders._cf_height_from_name'):
            with mock.patch('iris.cube.Cube'):
                cubes = list(_generate_cubes(header, column_headings, coords,
                                             data_arrays, cell_methods))

        cubes[0].assert_has_call(mock.call.add_cell_method('cell_method_1'))
        cubes[1].assert_has_call(mock.call.add_cell_method('cell_method_2'))


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test__build_cell_methods
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for :func:`iris.fileformats.name_loaders._build_cell_methods`.

"""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

from iris.fileformats.name_loaders import _build_cell_methods


class Tests(tests.IrisTest):
    def setUp(self):
        patch = mock.patch('iris.coords.CellMethod')
        self.mock_CellMethod = patch.start()
        self.addCleanup(patch.stop)

    def test_nameII_average(self):
        av_or_int = ['something average ob bla'] * 3
        coord_name = 'foo'
        res = _build_cell_methods(av_or_int, coord_name)
        self.mock_CellMethod.assert_called('average', coord_name)

    def test_nameIII_averaged(self):
        av_or_int = ['something averaged ob bla'] * 3
        coord_name = 'bar'
        res = _build_cell_methods(av_or_int, coord_name)
        self.mock_CellMethod.assert_called('average', coord_name)

    def test_nameII_integral(self):
        av_or_int = ['something integral ob bla'] * 3
        coord_name = 'ensemble'
        res = _build_cell_methods(av_or_int, coord_name)
        self.mock_CellMethod.assert_called('sum', coord_name)

    def test_nameIII_integrated(self):
        av_or_int = ['something integrated ob bla'] * 3
        coord_name = 'time'
        res = _build_cell_methods(av_or_int, coord_name)
        self.mock_CellMethod.assert_called('sum', coord_name)

    def test_no_averaging(self):
        av_or_int = ['No foo averaging',
                     'No bar averaging',
                     'no',
                     '',
                     'no averaging',
                     'no anything at all averaging']
        coord_name = 'time'
        res = _build_cell_methods(av_or_int, coord_name)
        self.assertEqual(res, [None] * len(av_or_int))

    def test_unrecognised(self):
        unrecognised_heading = 'bla else'
        av_or_int = ['something average',
                     unrecognised_heading,
                     'something integral']
        coord_name = 'foo'
        with mock.patch('warnings.warn') as warn:
            res = _build_cell_methods(av_or_int, coord_name)
        expected_msg = 'Unknown {} statistic: {!r}. Unable to ' \
                       'create cell method.'.format(coord_name,
                                                    unrecognised_heading)
        warn.assert_called_with(expected_msg)

    def test_unrecognised_similar_to_no_averaging(self):
        unrecognised_headings = ['not averaging',
                                 'this is not a valid no',
                                 'nope',
                                 'no daveraging',
                                 'no averagingg',
                                 'no something',
                                 'noaveraging']
        for unrecognised_heading in unrecognised_headings:
            av_or_int = ['something average',
                         unrecognised_heading,
                         'something integral']
            coord_name = 'foo'
            with mock.patch('warnings.warn') as warn:
                res = _build_cell_methods(av_or_int, coord_name)
            expected_msg = 'Unknown {} statistic: {!r}. Unable to ' \
                           'create cell method.'.format(coord_name,
                                                        unrecognised_heading)
            warn.assert_called_with(expected_msg)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test__cf_height_from_name
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for the :class:`iris.analysis.name_loaders._cf_height_from_name`
function.

"""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import numpy as np

from iris.coords import AuxCoord
from iris.fileformats.name_loaders import _cf_height_from_name


class TestAll(tests.IrisTest):
    def _default_coord(self, data):
        # This private method returns a coordinate with values expected
        # when no interpretation is made of the field header string.
        return AuxCoord(
            units='no-unit', points=data, bounds=None, standard_name=None,
            long_name='z')


class TestAll_NAMEII(TestAll):
    # NAMEII formats are defined by bounds, not points
    def test_bounded_height_above_ground(self):
        data = 'From     0 -   100m agl'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='m', points=50.0, bounds=np.array([0., 100.]),
            standard_name='height', long_name='height above ground level')
        self.assertEqual(com, res)

    def test_bounded_flight_level(self):
        data = 'From FL0 - FL100'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='unknown', points=50.0, bounds=np.array([0., 100.]),
            standard_name=None, long_name='flight_level')
        self.assertEqual(com, res)

    def test_bounded_height_above_sea_level(self):
        data = 'From     0 -   100m asl'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='m', points=50.0, bounds=np.array([0., 100.]),
            standard_name='altitude', long_name='altitude above sea level')
        self.assertEqual(com, res)

    def test_malformed_height_above_ground(self):
        # Parse height above ground level with additional stuff on the end of
        # the string (agl).
        data = 'From     0 -   100m agl and stuff'
        res = _cf_height_from_name(data)
        com = self._default_coord(data)
        self.assertEqual(com, res)

    def test_malformed_height_above_sea_level(self):
        # Parse height above ground level with additional stuff on the end of
        # the string (agl).
        data = 'From     0 -   100m asl and stuff'
        res = _cf_height_from_name(data)
        com = self._default_coord(data)
        self.assertEqual(com, res)

    def test_malformed_flight_level(self):
        # Parse height above ground level with additional stuff on the end of
        # the string (agl).
        data = 'From FL0 - FL100 and stuff'
        res = _cf_height_from_name(data)
        com = self._default_coord(data)
        self.assertEqual(com, res)

    def test_float_bounded_height_above_ground(self):
        # Parse height above ground level when its a float.
        data = 'From     0.0 -   100.0m agl'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='m', points=50.0, bounds=np.array([0., 100.]),
            standard_name='height', long_name='height above ground level')
        self.assertEqual(com, res)

    def test_float_bounded_height_flight_level(self):
        # Parse height above ground level, as a float (agl).
        data = 'From FL0.0 - FL100.0'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='unknown', points=50.0, bounds=np.array([0., 100.]),
            standard_name=None, long_name='flight_level')
        self.assertEqual(com, res)

    def test_float_bounded_height_above_sea_level(self):
        # Parse height above ground level as a float (agl).
        data = 'From     0.0 -   100.0m asl'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='m', points=50.0, bounds=np.array([0., 100.]),
            standard_name='altitude', long_name='altitude above sea level')
        self.assertEqual(com, res)

    def test_no_match(self):
        # Parse height information when there is no match.
        # No interpretation, just returns default values.
        data = 'Vertical integral'
        res = _cf_height_from_name(data)
        com = self._default_coord(data)
        self.assertEqual(com, res)

    def test_pressure(self):
        # Parse air_pressure string.
        data = 'From     0 -   100 Pa'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='Pa', points=50.0, bounds=np.array([0., 100.]),
            standard_name='air_pressure', long_name=None)
        self.assertEqual(com, res)


class TestAll_NAMEIII(TestAll):
    # NAMEIII formats are defined by points, not bounds.
    def test_height_above_ground(self):
        data = 'Z = 50.00000 m agl'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='m', points=50.0, bounds=None,
            standard_name='height', long_name='height above ground level')
        self.assertEqual(com, res)

    def test_height_flight_level(self):
        data = 'Z = 50.00000 FL'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='unknown', points=50.0, bounds=None,
            standard_name=None, long_name='flight_level')
        self.assertEqual(com, res)

    def test_height_above_sea_level(self):
        data = 'Z = 50.00000 m asl'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='m', points=50.0, bounds=None,
            standard_name='altitude', long_name='altitude above sea level')
        self.assertEqual(com, res)

    def test_malformed_height_above_ground(self):
        # Parse height above ground level, with additonal stuff at the string
        # end (agl).
        data = 'Z = 50.00000 m agl and stuff'
        res = _cf_height_from_name(data)
        com = self._default_coord(data)
        self.assertEqual(com, res)

    def test_malformed_height_above_sea_level(self):
        # Parse height above ground level, with additional stuff at string
        # end (agl).
        data = 'Z = 50.00000 m asl and stuff'
        res = _cf_height_from_name(data)
        com = self._default_coord(data)
        self.assertEqual(com, res)

    def test_malformed_flight_level(self):
        # Parse height above ground level (agl), with additional stuff at
        # string end.
        data = 'Z = 50.00000 FL and stuff'
        res = _cf_height_from_name(data)
        com = self._default_coord(data)
        self.assertEqual(com, res)

    def test_integer_height_above_ground(self):
        # Parse height above ground level when its an integer.
        data = 'Z = 50 m agl'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='m', points=50.0, bounds=None,
            standard_name='height', long_name='height above ground level')
        self.assertEqual(com, res)

    def test_integer_height_flight_level(self):
        # Parse flight level when its an integer.
        data = 'Z = 50 FL'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='unknown', points=50.0, bounds=None,
            standard_name=None, long_name='flight_level')
        self.assertEqual(com, res)

    def test_integer_height_above_sea_level(self):
        # Parse height above sea level (agl) when its an integer.
        data = 'Z = 50 m asl'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='m', points=50.0, bounds=None,
            standard_name='altitude', long_name='altitude above sea level')
        self.assertEqual(com, res)

    def test_pressure(self):
        # Parse pressure.
        data = 'Z = 50.00000 Pa'
        res = _cf_height_from_name(data)
        com = AuxCoord(
            units='Pa', points=50.0, bounds=None,
            standard_name='air_pressure', long_name=None)
        self.assertEqual(com, res)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_Saver
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.netcdf.Saver` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock
import netCDF4 as nc
import numpy as np

import iris
from iris.coord_systems import GeogCS, TransverseMercator, RotatedGeogCS
from iris.coords import DimCoord
from iris.cube import Cube
from iris.fileformats.netcdf import Saver
import iris.tests.stock as stock


class Test_write(tests.IrisTest):
    def _transverse_mercator_cube(self, ellipsoid=None):
        data = np.arange(12).reshape(3, 4)
        cube = Cube(data, 'air_pressure_anomaly')
        trans_merc = TransverseMercator(49.0, -2.0, -400000.0, 100000.0,
                                        0.9996012717, ellipsoid)
        coord = DimCoord(range(3), 'projection_y_coordinate', units='m',
                         coord_system=trans_merc)
        cube.add_dim_coord(coord, 0)
        coord = DimCoord(range(4), 'projection_x_coordinate', units='m',
                         coord_system=trans_merc)
        cube.add_dim_coord(coord, 1)
        return cube

    def test_transverse_mercator(self):
        # Create a Cube with a transverse Mercator coordinate system.
        ellipsoid = GeogCS(6377563.396, 6356256.909)
        cube = self._transverse_mercator_cube(ellipsoid)
        with self.temp_filename('.nc') as nc_path:
            with Saver(nc_path, 'NETCDF4') as saver:
                saver.write(cube)
            self.assertCDL(nc_path)

    def test_transverse_mercator_no_ellipsoid(self):
        # Create a Cube with a transverse Mercator coordinate system.
        cube = self._transverse_mercator_cube()
        with self.temp_filename('.nc') as nc_path:
            with Saver(nc_path, 'NETCDF4') as saver:
                saver.write(cube)
            self.assertCDL(nc_path)

    def _simple_cube(self, dtype):
        data = np.arange(12, dtype=dtype).reshape(3, 4)
        points = np.arange(3, dtype=dtype)
        bounds = np.arange(6, dtype=dtype).reshape(3, 2)
        cube = Cube(data, 'air_pressure_anomaly')
        coord = DimCoord(points, bounds=bounds)
        cube.add_dim_coord(coord, 0)
        return cube

    def test_little_endian(self):
        # Create a Cube with little-endian data.
        cube = self._simple_cube('<f4')
        with self.temp_filename('.nc') as nc_path:
            with Saver(nc_path, 'NETCDF4') as saver:
                saver.write(cube)
            self.assertCDL(nc_path, basename='endian', flags='')

    def test_big_endian(self):
        # Create a Cube with big-endian data.
        cube = self._simple_cube('>f4')
        with self.temp_filename('.nc') as nc_path:
            with Saver(nc_path, 'NETCDF4') as saver:
                saver.write(cube)
            self.assertCDL(nc_path, basename='endian', flags='')

    def test_zlib(self):
        cube = self._simple_cube('>f4')
        with mock.patch('iris.fileformats.netcdf.netCDF4') as api:
            with Saver('/dummy/path', 'NETCDF4') as saver:
                saver.write(cube, zlib=True)
        dataset = api.Dataset.return_value
        create_var_calls = mock.call.createVariable(
            'air_pressure_anomaly', np.dtype('float32'), ['dim0', 'dim1'],
            fill_value=None, shuffle=True, least_significant_digit=None,
            contiguous=False, zlib=True, fletcher32=False,
            endian='native', complevel=4, chunksizes=None).call_list()
        dataset.assert_has_calls(create_var_calls)

    def test_least_significant_digit(self):
        cube = Cube(np.array([1.23, 4.56, 7.89]),
                    standard_name='surface_temperature', long_name=None,
                    var_name='temp', units='K')
        with self.temp_filename('.nc') as nc_path:
            with Saver(nc_path, 'NETCDF4') as saver:
                saver.write(cube, least_significant_digit=1)
            cube_saved = iris.load_cube(nc_path)
            self.assertEquals(
                cube_saved.attributes['least_significant_digit'], 1)
            self.assertFalse(np.all(cube.data == cube_saved.data))
            self.assertArrayAllClose(cube.data, cube_saved.data, 0.1)

    def test_default_unlimited_dimensions(self):
        cube = self._simple_cube('>f4')
        with self.temp_filename('.nc') as nc_path:
            with Saver(nc_path, 'NETCDF4') as saver:
                saver.write(cube)
            ds = nc.Dataset(nc_path)
            self.assertTrue(ds.dimensions['dim0'].isunlimited())
            self.assertFalse(ds.dimensions['dim1'].isunlimited())
            ds.close()

    def test_no_unlimited_dimensions(self):
        cube = self._simple_cube('>f4')
        with self.temp_filename('.nc') as nc_path:
            with Saver(nc_path, 'NETCDF4') as saver:
                saver.write(cube, unlimited_dimensions=[])
            ds = nc.Dataset(nc_path)
            for dim in ds.dimensions.itervalues():
                self.assertFalse(dim.isunlimited())
            ds.close()

    def test_invalid_unlimited_dimensions(self):
        cube = self._simple_cube('>f4')
        with self.temp_filename('.nc') as nc_path:
            with Saver(nc_path, 'NETCDF4') as saver:
                # should not raise an exception
                saver.write(cube, unlimited_dimensions=['not_found'])

    def test_custom_unlimited_dimensions(self):
        cube = self._transverse_mercator_cube()
        unlimited_dimensions = ['projection_y_coordinate',
                                'projection_x_coordinate']
        # test coordinates by name
        with self.temp_filename('.nc') as nc_path:
            with Saver(nc_path, 'NETCDF4') as saver:
                saver.write(cube, unlimited_dimensions=unlimited_dimensions)
            ds = nc.Dataset(nc_path)
            for dim in unlimited_dimensions:
                self.assertTrue(ds.dimensions[dim].isunlimited())
            ds.close()
        # test coordinate arguments
        with self.temp_filename('.nc') as nc_path:
            coords = [cube.coord(dim) for dim in unlimited_dimensions]
            with Saver(nc_path, 'NETCDF4') as saver:
                saver.write(cube, unlimited_dimensions=coords)
            ds = nc.Dataset(nc_path)
            for dim in unlimited_dimensions:
                self.assertTrue(ds.dimensions[dim].isunlimited())
            ds.close()


class TestCoordSystems(tests.IrisTest):
    def cube_with_cs(self, coord_system,
                     names=['grid_longitude', 'grid_latitude']):
        cube = stock.lat_lon_cube()
        x, y = cube.coord('longitude'), cube.coord('latitude')
        x.coord_system = y.coord_system = coord_system
        for coord, name in zip([x, y], names):
            coord.rename(name)
        return cube

    def construct_cf_grid_mapping_variable(self, cube):
        # Calls the actual NetCDF saver with appropriate mocking, returning
        # the grid variable that gets created.
        grid_variable = mock.Mock(name='NetCDFVariable')
        create_var_fn = mock.Mock(side_effect=[grid_variable])
        dataset = mock.Mock(variables=[],
                            createVariable=create_var_fn)
        saver = mock.Mock(spec=Saver, _coord_systems=[],
                          _dataset=dataset)
        variable = mock.Mock()

        Saver._create_cf_grid_mapping(saver, cube, variable)
        self.assertEqual(create_var_fn.call_count, 1)
        self.assertEqual(variable.grid_mapping,
                         grid_variable.grid_mapping_name)
        return grid_variable

    def variable_attributes(self, mocked_variable):
        """Get the attributes dictionary from a mocked NetCDF variable."""
        # Get the attributes defined on the mock object.
        attributes = filter(lambda name: not name.startswith('_'),
                            sorted(mocked_variable.__dict__.keys()))
        attributes.remove('method_calls')
        return {key: getattr(mocked_variable, key) for key in attributes}

    def test_rotated_geog_cs(self):
        coord_system = RotatedGeogCS(37.5, 177.5, ellipsoid=GeogCS(6371229.0))
        cube = self.cube_with_cs(coord_system)
        expected = {'grid_mapping_name': 'rotated_latitude_longitude',
                    'north_pole_grid_longitude': 0.0,
                    'grid_north_pole_longitude': 177.5,
                    'grid_north_pole_latitude': 37.5,
                    'longitude_of_prime_meridian': 0.0,
                    'earth_radius': 6371229.0,
                    }

        grid_variable = self.construct_cf_grid_mapping_variable(cube)
        actual = self.variable_attributes(grid_variable)

        # To see obvious differences, check that they keys are the same.
        self.assertEqual(sorted(actual.keys()), sorted(expected.keys()))
        # Now check that the values are equivalent.
        self.assertEqual(actual, expected)

    def test_spherical_geog_cs(self):
        coord_system = GeogCS(6371229.0)
        cube = self.cube_with_cs(coord_system)
        expected = {'grid_mapping_name': 'latitude_longitude',
                    'longitude_of_prime_meridian': 0.0,
                    'earth_radius': 6371229.0
                    }

        grid_variable = self.construct_cf_grid_mapping_variable(cube)
        actual = self.variable_attributes(grid_variable)

        # To see obvious differences, check that they keys are the same.
        self.assertEqual(sorted(actual.keys()), sorted(expected.keys()))
        # Now check that the values are equivalent.
        self.assertEqual(actual, expected)

    def test_elliptic_geog_cs(self):
        coord_system = GeogCS(637, 600)
        cube = self.cube_with_cs(coord_system)
        expected = {'grid_mapping_name': 'latitude_longitude',
                    'longitude_of_prime_meridian': 0.0,
                    'semi_minor_axis': 600.0,
                    'semi_major_axis': 637.0,
                    }

        grid_variable = self.construct_cf_grid_mapping_variable(cube)
        actual = self.variable_attributes(grid_variable)

        # To see obvious differences, check that they keys are the same.
        self.assertEqual(sorted(actual.keys()), sorted(expected.keys()))
        # Now check that the values are equivalent.
        self.assertEqual(actual, expected)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test__load_aux_factory
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.netcdf._load_aux_factory` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock
import numpy as np
import warnings

from iris.coords import DimCoord
from iris.cube import Cube
from iris.aux_factory import HybridPressureFactory
from iris.fileformats.netcdf import _load_aux_factory


class TestAtmosphereHybridSigmaPressureCoordinate(tests.IrisTest):
    def setUp(self):
        standard_name = 'atmosphere_hybrid_sigma_pressure_coordinate'
        self.requires = dict(formula_type=standard_name)
        coordinates = [(mock.sentinel.b, 'b'), (mock.sentinel.ps, 'ps')]
        self.provides = dict(coordinates=coordinates)
        self.engine = mock.Mock(requires=self.requires, provides=self.provides)
        self.cube = mock.create_autospec(Cube, spec_set=True, instance=True)
        # Patch out the check_dependencies functionality.
        func = 'iris.aux_factory.HybridPressureFactory._check_dependencies'
        patcher = mock.patch(func)
        patcher.start()
        self.addCleanup(patcher.stop)

    def test_formula_terms_ap(self):
        self.provides['coordinates'].append((mock.sentinel.ap, 'ap'))
        self.requires['formula_terms'] = dict(ap='ap', b='b', ps='ps')
        _load_aux_factory(self.engine, self.cube)
        # Check cube.add_aux_coord method.
        self.assertEqual(self.cube.add_aux_coord.call_count, 0)
        # Check cube.add_aux_factory method.
        self.assertEqual(self.cube.add_aux_factory.call_count, 1)
        args, _ = self.cube.add_aux_factory.call_args
        self.assertEqual(len(args), 1)
        factory = args[0]
        self.assertEqual(factory.delta, mock.sentinel.ap)
        self.assertEqual(factory.sigma, mock.sentinel.b)
        self.assertEqual(factory.surface_air_pressure, mock.sentinel.ps)

    def test_formula_terms_a_p0(self):
        coord_a = DimCoord(range(5), units='Pa')
        coord_p0 = DimCoord(10, units='1')
        coord_expected = DimCoord(np.arange(5) * 10, units='Pa',
                                  long_name='vertical pressure', var_name='ap')
        self.provides['coordinates'].extend([(coord_a, 'a'), (coord_p0, 'p0')])
        self.requires['formula_terms'] = dict(a='a', b='b', ps='ps', p0='p0')
        _load_aux_factory(self.engine, self.cube)
        # Check cube.coord_dims method.
        self.assertEqual(self.cube.coord_dims.call_count, 1)
        args, _ = self.cube.coord_dims.call_args
        self.assertEqual(len(args), 1)
        self.assertIs(args[0], coord_a)
        # Check cube.add_aux_coord method.
        self.assertEqual(self.cube.add_aux_coord.call_count, 1)
        args, _ = self.cube.add_aux_coord.call_args
        self.assertEqual(len(args), 2)
        self.assertEqual(args[0], coord_expected)
        self.assertIsInstance(args[1], mock.Mock)
        # Check cube.add_aux_factory method.
        self.assertEqual(self.cube.add_aux_factory.call_count, 1)
        args, _ = self.cube.add_aux_factory.call_args
        self.assertEqual(len(args), 1)
        factory = args[0]
        self.assertEqual(factory.delta, coord_expected)
        self.assertEqual(factory.sigma, mock.sentinel.b)
        self.assertEqual(factory.surface_air_pressure, mock.sentinel.ps)

    def test_formula_terms_p0_non_scalar(self):
        coord_p0 = DimCoord(range(5))
        self.provides['coordinates'].append((coord_p0, 'p0'))
        self.requires['formula_terms'] = dict(p0='p0')
        with self.assertRaises(ValueError):
            _load_aux_factory(self.engine, self.cube)

    def test_formula_terms_p0_bounded(self):
        coord_a = DimCoord(range(5))
        coord_p0 = DimCoord(1, bounds=[0, 2], var_name='p0')
        self.provides['coordinates'].extend([(coord_a, 'a'), (coord_p0, 'p0')])
        self.requires['formula_terms'] = dict(a='a', b='b', ps='ps', p0='p0')
        with warnings.catch_warnings(record=True) as warn:
            warnings.simplefilter('always')
            _load_aux_factory(self.engine, self.cube)
            self.assertEqual(len(warn), 1)
            msg = 'Ignoring atmosphere hybrid sigma pressure scalar ' \
                'coordinate {!r} bounds.'.format(coord_p0.name())
            self.assertEqual(msg, warn[0].message.message)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test__load_cube
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.netcdf._load_cube` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import iris.fileformats.cf
import mock
import netCDF4
import numpy as np

from iris.fileformats.netcdf import _load_cube


class TestFillValue(tests.IrisTest):
    def setUp(self):
        name = 'iris.fileformats.netcdf._assert_case_specific_facts'
        patch = mock.patch(name)
        patch.start()
        self.addCleanup(patch.stop)

        self.engine = mock.Mock()
        self.cf = None
        self.filename = 'DUMMY'

    def _make_cf_var(self, dtype):
        variable = mock.Mock(spec=netCDF4.Variable, dtype=dtype)
        cf_var = mock.MagicMock(spec=iris.fileformats.cf.CFVariable,
                                cf_data=variable, cf_name='DUMMY_VAR',
                                cf_group=mock.Mock(), dtype=dtype,
                                shape=mock.MagicMock())
        return cf_var

    def _test(self, cf_var, expected_fill_value):
        cube = _load_cube(self.engine, self.cf, cf_var, self.filename)
        self.assertEqual(cube._my_data.fill_value, expected_fill_value)

    def test_from_attribute_dtype_f4(self):
        # A _FillValue attribute on the netCDF variable should end up as
        # the fill_value for the cube.
        dtype = np.dtype('f4')
        cf_var = self._make_cf_var(dtype)
        cf_var.cf_data._FillValue = mock.sentinel.FILL_VALUE
        self._test(cf_var, mock.sentinel.FILL_VALUE)

    def test_from_default_dtype_f4(self):
        # Without an explicit _FillValue attribute on the netCDF
        # variable, the fill value should be selected from the default
        # netCDF fill values.
        dtype = np.dtype('f4')
        cf_var = self._make_cf_var(dtype)
        self._test(cf_var, netCDF4.default_fillvals['f4'])

    def test_from_attribute_dtype_i4(self):
        # A _FillValue attribute on the netCDF variable should end up as
        # the fill_value for the cube.
        dtype = np.dtype('i4')
        cf_var = self._make_cf_var(dtype)
        cf_var.cf_data._FillValue = mock.sentinel.FILL_VALUE
        self._test(cf_var, mock.sentinel.FILL_VALUE)

    def test_from_default_dtype_i4(self):
        # Without an explicit _FillValue attribute on the netCDF
        # variable, the fill value should be selected from the default
        # netCDF fill values.
        dtype = np.dtype('i4')
        cf_var = self._make_cf_var(dtype)
        self._test(cf_var, netCDF4.default_fillvals['i4'])

    def test_from_attribute_with_scale_offset(self):
        # The _FillValue attribute still takes priority even when an
        # offset/scale transformation takes place on the data.
        dtype = np.dtype('i2')
        cf_var = self._make_cf_var(dtype)
        cf_var.scale_factor = np.float64(1.5)
        cf_var.cf_data._FillValue = mock.sentinel.FILL_VALUE
        self._test(cf_var, mock.sentinel.FILL_VALUE)

    def test_from_default_with_scale_offset(self):
        # The fill value should be related to the *non-scaled* dtype.
        dtype = np.dtype('i2')
        cf_var = self._make_cf_var(dtype)
        cf_var.scale_factor = np.float64(1.5)
        self._test(cf_var, netCDF4.default_fillvals['i2'])


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_tm_meridian_scaling
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for the `iris.fileformats.nimrod_load_rules.tm_meridian_scaling`
function.

"""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

from iris.fileformats.nimrod_load_rules import (tm_meridian_scaling,
                                                NIMROD_DEFAULT,
                                                MERIDIAN_SCALING_BNG)
from iris.fileformats.nimrod import NimrodField


class Test(tests.IrisTest):
    def setUp(self):
        self.field = mock.Mock(tm_meridian_scaling=NIMROD_DEFAULT,
                               spec=NimrodField,
                               float32_mdi=-123)
        self.cube = mock.Mock()

    def _call_tm_meridian_scaling(self, scaling_value):
        self.field.tm_meridian_scaling = scaling_value
        tm_meridian_scaling(self.cube, self.field)

    def test_unhandled(self):
        with mock.patch('warnings.warn') as warn:
            self._call_tm_meridian_scaling(1)
        warn.assert_called_once()

    @tests.no_warnings
    def test_british_national_grid(self):
        # A value is not returned in this rule currently.
        self.assertEqual(None,
                         self._call_tm_meridian_scaling(MERIDIAN_SCALING_BNG))

    def test_null(self):
        with mock.patch('warnings.warn') as warn:
            self._call_tm_meridian_scaling(NIMROD_DEFAULT)
            self._call_tm_meridian_scaling(self.field.float32_mdi)
        self.assertEqual(warn.call_count, 0)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_vertical_coord
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for the `iris.fileformats.nimrod_load_rules.vertical_coord`
function.

"""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

from iris.fileformats.nimrod_load_rules import (vertical_coord,
                                                NIMROD_DEFAULT,
                                                TranslationWarning)
from iris.fileformats.nimrod import NimrodField


class Test(tests.IrisTest):
    NIMROD_LOCATION = 'iris.fileformats.nimrod_load_rules'

    def setUp(self):
        self.field = mock.Mock(vertical_coord_type=NIMROD_DEFAULT,
                               int_mdi=mock.sentinel.int_mdi,
                               field_code=mock.sentinel.field_code,
                               spec=NimrodField)
        self.cube = mock.Mock()

    def _call_vertical_coord(self, vertical_coord_type):
        self.field.vertical_coord_type = vertical_coord_type
        vertical_coord(self.cube, self.field)

    def test_unhandled(self):
        with mock.patch('warnings.warn') as warn:
            self._call_vertical_coord(-1)
        warn.assert_called_once_with("Vertical coord -1 not yet handled",
                                     TranslationWarning)

    def test_orography(self):
        name = 'orography_vertical_coord'
        with mock.patch(self.NIMROD_LOCATION + '.' + name) as orog:
            self.field.field_code = 73
            self._call_vertical_coord(None)
        orog.assert_called_once_with(self.cube, self.field)

    def test_height(self):
        name = 'height_vertical_coord'
        with mock.patch(self.NIMROD_LOCATION + '.' + name) as height:
            self._call_vertical_coord(0)
        height.assert_called_once_with(self.cube, self.field)

    def test_null(self):
        with mock.patch('warnings.warn') as warn:
            self._call_vertical_coord(NIMROD_DEFAULT)
            self._call_vertical_coord(self.field.int_mdi)
        self.assertEqual(warn.call_count, 0)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_load
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.pp.load` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

import iris.fileformats.pp as pp


class Test_load(tests.IrisTest):
    def test_call_structure(self):
        # Check that the load function calls the two necessary utility
        # functions.
        extract_result = mock.Mock()
        interpret_patch = mock.patch('iris.fileformats.pp._interpret_fields',
                                     autospec=True, return_value=iter([]))
        field_gen_patch = mock.patch('iris.fileformats.pp._field_gen',
                                     autospec=True,
                                     return_value=extract_result)
        with interpret_patch as interpret, field_gen_patch as field_gen:
            pp.load('mock', read_data=True)

        interpret.assert_called_once_with(extract_result)
        field_gen.assert_called_once_with('mock', read_data_bytes=True)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_PPDataProxy
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.pp.PPDataProxy` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

from iris.fileformats.pp import PPDataProxy, SplittableInt


class Test_lbpack(tests.IrisTest):
    def test_lbpack_SplittableInt(self):
        lbpack = mock.Mock(spec_set=SplittableInt)
        proxy = PPDataProxy(None, None, None, None,
                            None, lbpack, None, None)
        self.assertEqual(proxy.lbpack, lbpack)
        self.assertIs(proxy.lbpack, lbpack)

    def test_lnpack_raw(self):
        lbpack = 4321
        proxy = PPDataProxy(None, None, None, None,
                            None, lbpack, None, None)
        self.assertEqual(proxy.lbpack, lbpack)
        self.assertIsNot(proxy.lbpack, lbpack)
        self.assertIsInstance(proxy.lbpack, SplittableInt)
        self.assertEqual(proxy.lbpack.n1, lbpack % 10)
        self.assertEqual(proxy.lbpack.n2, lbpack / 10 % 10)
        self.assertEqual(proxy.lbpack.n3, lbpack / 100 % 10)
        self.assertEqual(proxy.lbpack.n4, lbpack / 1000 % 10)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_PPField
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.pp.PPField` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import warnings

import mock
import numpy as np

import iris.fileformats.pp as pp
from iris.fileformats.pp import PPField
from iris.fileformats.pp import SplittableInt

# The PPField class is abstract, so to test we define a minimal,
# concrete subclass with the `t1` and `t2` properties.
#
# NB. We define dummy header items to allow us to zero the unused header
# items when written to disk and get consistent results.


DUMMY_HEADER = [('dummy1', (0, 13)),
                ('lbtim', (12,)),
                ('lblrec', (14,)),
                ('dummy2', (15, 18)),
                ('lbrow', (17,)),
                ('lbext',  (19,)),
                ('lbpack', (20,)),
                ('dummy3', (21, 37)),
                ('lbuser', (38, 39, 40, 41, 42, 43, 44,)),
                ('brsvd', (45, 46, 47, 48)),
                ('bdatum', (49,)),
                ('dummy4', (45, 63)),
                ]


class TestPPField(PPField):

    HEADER_DEFN = DUMMY_HEADER
    HEADER_DICT = dict(DUMMY_HEADER)

    @property
    def t1(self):
        return netcdftime.datetime(2013, 10, 14, 10, 4)

    @property
    def t2(self):
        return netcdftime.datetime(2013, 10, 14, 10, 5)


class Test_save(tests.IrisTest):
    def test_float64(self):
        # Tests down-casting of >f8 data to >f4.

        def field_checksum(data):
            field = TestPPField()
            field.dummy1 = 0
            field.dummy2 = 0
            field.dummy3 = 0
            field.dummy4 = 0
            field.lbtim = 0
            field.lblrec = 0
            field.lbrow = 0
            field.lbext = 0
            field.lbpack = 0
            field.lbuser = 0
            field.brsvd = 0
            field.bdatum = 0
            field.data = data
            with self.temp_filename('.pp') as temp_filename:
                with open(temp_filename, 'wb') as pp_file:
                    field.save(pp_file)
                checksum = self.file_checksum(temp_filename)
            return checksum

        data_64 = np.linspace(0, 1, num=10, endpoint=False).reshape(2, 5)
        checksum_32 = field_checksum(data_64.astype('>f4'))
        with mock.patch('warnings.warn') as warn:
            checksum_64 = field_checksum(data_64.astype('>f8'))

        self.assertEquals(checksum_32, checksum_64)
        warn.assert_called()


class Test_calendar(tests.IrisTest):
    def test_greg(self):
        field = TestPPField()
        field.lbtim = SplittableInt(1, {'ia': 2, 'ib': 1, 'ic': 0})
        self.assertEqual(field.calendar, 'gregorian')

    def test_360(self):
        field = TestPPField()
        field.lbtim = SplittableInt(2, {'ia': 2, 'ib': 1, 'ic': 0})
        self.assertEqual(field.calendar, '360_day')

    def test_365(self):
        field = TestPPField()
        field.lbtim = SplittableInt(4, {'ia': 2, 'ib': 1, 'ic': 0})
        self.assertEqual(field.calendar, '365_day')


class Test__init__(tests.IrisTest):
    def setUp(self):
        header_longs = np.zeros(pp.NUM_LONG_HEADERS, dtype=np.int)
        header_floats = np.zeros(pp.NUM_FLOAT_HEADERS, dtype=np.float)
        self.header = list(header_longs) + list(header_floats)

    def test_no_headers(self):
        field = TestPPField()
        self.assertIsNone(field._raw_header)
        self.assertIsNone(field.raw_lbtim)
        self.assertIsNone(field.raw_lbpack)

    def test_lbtim_lookup(self):
        self.assertEqual(TestPPField.HEADER_DICT['lbtim'], (12,))

    def test_lbpack_lookup(self):
        self.assertEqual(TestPPField.HEADER_DICT['lbpack'], (20,))

    def test_raw_lbtim(self):
        raw_lbtim = 4321
        loc, = TestPPField.HEADER_DICT['lbtim']
        self.header[loc] = raw_lbtim
        field = TestPPField(header=self.header)
        self.assertEqual(field.raw_lbtim, raw_lbtim)

    def test_raw_lbpack(self):
        raw_lbpack = 4321
        loc, = TestPPField.HEADER_DICT['lbpack']
        self.header[loc] = raw_lbpack
        field = TestPPField(header=self.header)
        self.assertEqual(field.raw_lbpack, raw_lbpack)


class Test__getattr__(tests.IrisTest):
    def setUp(self):
        header_longs = np.zeros(pp.NUM_LONG_HEADERS, dtype=np.int)
        header_floats = np.zeros(pp.NUM_FLOAT_HEADERS, dtype=np.float)
        self.header = list(header_longs) + list(header_floats)

    def test_attr_singular_long(self):
        lbrow = 1234
        loc, = TestPPField.HEADER_DICT['lbrow']
        self.header[loc] = lbrow
        field = TestPPField(header=self.header)
        self.assertEqual(field.lbrow, lbrow)

    def test_attr_multi_long(self):
        lbuser = (100, 101, 102, 103, 104, 105, 106)
        loc = TestPPField.HEADER_DICT['lbuser']
        self.header[loc[0]:loc[-1] + 1] = lbuser
        field = TestPPField(header=self.header)
        self.assertEqual(field.lbuser, lbuser)

    def test_attr_singular_float(self):
        bdatum = 1234
        loc, = TestPPField.HEADER_DICT['bdatum']
        self.header[loc] = bdatum
        field = TestPPField(header=self.header)
        self.assertEqual(field.bdatum, bdatum)

    def test_attr_multi_float(self):
        brsvd = (100, 101, 102, 103)
        loc = TestPPField.HEADER_DICT['brsvd']
        start = loc[0]
        stop = loc[-1] + 1
        self.header[start:stop] = brsvd
        field = TestPPField(header=self.header)
        self.assertEqual(field.brsvd, brsvd)

    def test_attr_lbtim(self):
        raw_lbtim = 4321
        loc, = TestPPField.HEADER_DICT['lbtim']
        self.header[loc] = raw_lbtim
        field = TestPPField(header=self.header)
        result = field.lbtim
        self.assertEqual(result, raw_lbtim)
        self.assertIsInstance(result, SplittableInt)
        result = field._lbtim
        self.assertEqual(result, raw_lbtim)
        self.assertIsInstance(result, SplittableInt)

    def test_attr_lbpack(self):
        raw_lbpack = 4321
        loc, = TestPPField.HEADER_DICT['lbpack']
        self.header[loc] = raw_lbpack
        field = TestPPField(header=self.header)
        result = field.lbpack
        self.assertEqual(result, raw_lbpack)
        self.assertIsInstance(result, SplittableInt)
        result = field._lbpack
        self.assertEqual(result, raw_lbpack)
        self.assertIsInstance(result, SplittableInt)

    def test_attr_raw_lbtim_assign(self):
        field = TestPPField(header=self.header)
        self.assertEqual(field.raw_lbpack, 0)
        self.assertEqual(field.lbtim, 0)
        raw_lbtim = 4321
        field.lbtim = raw_lbtim
        self.assertEqual(field.raw_lbtim, raw_lbtim)
        self.assertNotIsInstance(field.raw_lbtim, SplittableInt)

    def test_attr_raw_lbpack_assign(self):
        field = TestPPField(header=self.header)
        self.assertEqual(field.raw_lbpack, 0)
        self.assertEqual(field.lbpack, 0)
        raw_lbpack = 4321
        field.lbpack = raw_lbpack
        self.assertEqual(field.raw_lbpack, raw_lbpack)
        self.assertNotIsInstance(field.raw_lbpack, SplittableInt)

    def test_attr_unknown(self):
        with self.assertRaises(AttributeError):
            TestPPField().x


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_save
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.pp.save` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

from iris.fileformats._ff_cross_references import STASH_TRANS
import iris.fileformats.pp as pp
import iris.tests.stock as stock


def _pp_save_ppfield_values(cube):
    """
    Emulate saving a cube as PP, and capture the resulting PP field values.

    """
    # Create a test object to stand in for a real PPField.
    pp_field = mock.MagicMock(spec=pp.PPField3)
    # Add minimal content required by the pp.save operation.
    pp_field.HEADER_DEFN = pp.PPField3.HEADER_DEFN
    # Save cube to a dummy file, mocking the internally created PPField
    with mock.patch('iris.fileformats.pp.PPField3',
                    return_value=pp_field):
        target_filelike = mock.Mock(name='target')
        target_filelike.mode = ('b')
        pp.save(cube, target_filelike)
    # Return pp-field mock with all the written properties
    return pp_field


class TestLbfcProduction(tests.IrisTest):
    def setUp(self):
        self.cube = stock.lat_lon_cube()

    def check_cube_stash_yields_lbfc(self, stash, lbfc_expected):
        if stash:
            self.cube.attributes['STASH'] = stash
        lbfc_produced = _pp_save_ppfield_values(self.cube).lbfc
        self.assertEqual(lbfc_produced, lbfc_expected)

    def test_known_stash(self):
        stashcode_str = 'm04s07i002'
        self.assertIn(stashcode_str, STASH_TRANS)
        self.check_cube_stash_yields_lbfc(stashcode_str, 359)

    def test_unknown_stash(self):
        stashcode_str = 'm99s99i999'
        self.assertNotIn(stashcode_str, STASH_TRANS)
        self.check_cube_stash_yields_lbfc(stashcode_str, 0)

    def test_no_stash(self):
        self.assertNotIn('STASH', self.cube.attributes)
        self.check_cube_stash_yields_lbfc(None, 0)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test__create_field_data
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.pp._create_field_data` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import biggus
import mock
import numpy as np

import iris.fileformats.pp as pp


class Test__create_field_data(tests.IrisTest):
    def test_loaded_bytes(self):
        # Check that a field with LoadedArrayBytes in _data gets the
        # result of a suitable call to _data_bytes_to_shaped_array().
        mock_loaded_bytes = mock.Mock(spec=pp.LoadedArrayBytes)
        field = mock.Mock(_data=mock_loaded_bytes)
        data_shape = mock.Mock()
        land_mask = mock.Mock()
        with mock.patch('iris.fileformats.pp._data_bytes_to_shaped_array') as \
                convert_bytes:
            convert_bytes.return_value = mock.sentinel.array
            pp._create_field_data(field, data_shape, land_mask)

        self.assertIs(field._data, mock.sentinel.array)
        convert_bytes.assert_called_once_with(mock_loaded_bytes.bytes,
                                              field.lbpack, data_shape,
                                              mock_loaded_bytes.dtype,
                                              field.bmdi, land_mask)

    def test_deferred_bytes(self):
        # Check that a field with deferred array bytes in _data gets a
        # biggus array.
        fname = mock.sentinel.fname
        position = mock.sentinel.position
        n_bytes = mock.sentinel.n_bytes
        newbyteorder = mock.Mock(return_value=mock.sentinel.dtype)
        dtype = mock.Mock(newbyteorder=newbyteorder)
        deferred_bytes = (fname, position, n_bytes, dtype)
        field = mock.Mock(_data=deferred_bytes)
        data_shape = (mock.sentinel.lat, mock.sentinel.lon)
        land_mask = mock.Mock()
        proxy = mock.Mock(dtype=mock.sentinel.dtype, shape=data_shape)
        # We can't directly inspect the concrete data source underlying
        # the biggus array (it's a private attribute), so instead we
        # patch the proxy creation and check it's being created and
        # invoked correctly.
        with mock.patch('iris.fileformats.pp.PPDataProxy') as PPDataProxy:
            PPDataProxy.return_value = proxy
            pp._create_field_data(field, data_shape, land_mask)
        # Does the biggus array look OK from the outside?
        self.assertIsInstance(field._data, biggus.Array)
        self.assertEqual(field._data.shape, data_shape)
        self.assertEqual(field._data.dtype, mock.sentinel.dtype)
        # Is it making use of a correctly configured proxy?
        # NB. We know it's *using* the result of this call because
        # that's where the dtype came from above.
        PPDataProxy.assert_called_once_with((data_shape), dtype,
                                            fname, position,
                                            n_bytes,
                                            field.raw_lbpack, field.bmdi,
                                            land_mask)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test__data_bytes_to_shaped_array
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Unit tests for the `iris.fileformats.pp._data_bytes_to_shaped_array` function.

"""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import io

import mock
import numpy as np

import iris.fileformats.pp as pp


class Test__data_bytes_to_shaped_array__lateral_boundary_compression(
        tests.IrisTest):
    def setUp(self):
        self.data_shape = 30, 40
        y_halo, x_halo, rim = 2, 3, 4

        data_len = np.prod(self.data_shape)
        decompressed = np.arange(data_len).reshape(*self.data_shape)
        decompressed *= np.arange(self.data_shape[1]) % 3 + 1

        decompressed_mask = np.zeros(self.data_shape, np.bool)
        decompressed_mask[y_halo+rim:-(y_halo+rim),
                          x_halo+rim:-(x_halo+rim)] = True

        self.decompressed = np.ma.masked_array(decompressed,
                                               mask=decompressed_mask)

        self.north = decompressed[-(y_halo+rim):, :]
        self.east = decompressed[y_halo+rim:-(y_halo+rim), -(x_halo+rim):]
        self.south = decompressed[:y_halo+rim, :]
        self.west = decompressed[y_halo+rim:-(y_halo+rim), :x_halo+rim]

        # Get the bytes of the north, east, south, west arrays combined.
        buf = io.BytesIO()
        buf.write(self.north.copy())
        buf.write(self.east.copy())
        buf.write(self.south.copy())
        buf.write(self.west.copy())
        buf.seek(0)
        self.data_payload_bytes = buf.read()

    def test_boundary_decompression(self):
        boundary_packing = mock.Mock(rim_width=4, x_halo=3, y_halo=2)
        lbpack = mock.Mock(n1=0, boundary_packing=boundary_packing)
        r = pp._data_bytes_to_shaped_array(self.data_payload_bytes, lbpack,
                                           self.data_shape,
                                           self.decompressed.dtype, -99)
        self.assertMaskedArrayEqual(r, self.decompressed)


class Test__data_bytes_to_shaped_array__land_packed(tests.IrisTest):
    def setUp(self):
        # Sets up some useful arrays for use with the land/sea mask
        # decompression.
        self.land = np.array([[0, 1, 0, 0],
                              [1, 0, 0, 0],
                              [0, 0, 0, 1]], dtype=np.float64)
        sea = ~self.land.astype(np.bool)
        self.land_masked_data = np.array([1, 3, 4.5])
        self.sea_masked_data = np.array([1, 3, 4.5, -4, 5, 0, 1, 2, 3])

        # Compute the decompressed land mask data.
        self.decomp_land_data = np.ma.masked_array([[0, 1, 0, 0],
                                                    [3, 0, 0, 0],
                                                    [0, 0, 0, 4.5]],
                                                   mask=sea,
                                                   dtype=np.float64)
        # Compute the decompressed sea mask data.
        self.decomp_sea_data = np.ma.masked_array([[1, -10, 3, 4.5],
                                                   [-10, -4, 5, 0],
                                                   [1, 2, 3, -10]],
                                                  mask=self.land,
                                                  dtype=np.float64)

        self.land_mask = mock.Mock(data=self.land,
                                   lbrow=self.land.shape[0],
                                   lbnpt=self.land.shape[1])

    def create_lbpack(self, value):
        name_mapping = dict(n5=slice(4, None), n4=3, n3=2, n2=1, n1=0)
        return pp.SplittableInt(value, name_mapping)

    def test_no_land_mask(self):
        with mock.patch('numpy.frombuffer',
                        return_value=np.arange(3)):
            with self.assertRaises(ValueError) as err:
                pp._data_bytes_to_shaped_array(mock.Mock(),
                                               self.create_lbpack(120),
                                               (3, 4), np.dtype('>f4'),
                                               -999, mask=None)
            self.assertEqual(str(err.exception),
                             ('No mask was found to unpack the data. '
                              'Could not load.'))

    def test_land_mask(self):
        # Check basic land unpacking.
        field_data = self.land_masked_data
        result = self.check_read_data(field_data, 120, self.land_mask)
        self.assertMaskedArrayEqual(result, self.decomp_land_data)

    def test_land_masked_data_too_long(self):
        # Check land unpacking with field data that is larger than the mask.
        field_data = np.tile(self.land_masked_data, 2)
        result = self.check_read_data(field_data, 120, self.land_mask)
        self.assertMaskedArrayEqual(result, self.decomp_land_data)

    def test_sea_mask(self):
        # Check basic land unpacking.
        field_data = self.sea_masked_data
        result = self.check_read_data(field_data, 220, self.land_mask)
        self.assertMaskedArrayEqual(result, self.decomp_sea_data)

    def test_sea_masked_data_too_long(self):
        # Check sea unpacking with field data that is larger than the mask.
        field_data = np.tile(self.sea_masked_data, 2)
        result = self.check_read_data(field_data, 220, self.land_mask)
        self.assertMaskedArrayEqual(result, self.decomp_sea_data)

    def test_bad_lbpack(self):
        # Check basic land unpacking.
        field_data = self.sea_masked_data
        with self.assertRaises(ValueError):
            self.check_read_data(field_data, 320, self.land_mask)

    def check_read_data(self, field_data, lbpack, mask):
        # Calls pp._data_bytes_to_shaped_array with the necessary mocked
        # items, an lbpack instance, the correct data shape and mask instance.
        with mock.patch('numpy.frombuffer', return_value=field_data):
            return pp._data_bytes_to_shaped_array(mock.Mock(),
                                                  self.create_lbpack(lbpack),
                                                  mask.shape, np.dtype('>f4'),
                                                  -999, mask=mask)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test__field_gen
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.pp._field_gen` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import contextlib

import mock
import numpy as np

import iris.fileformats.pp as pp


class Test__field_gen(tests.IrisTest):
    @contextlib.contextmanager
    def mock_for_field_gen(self, fields):
        side_effect_fields = list(fields)[:]

        def make_pp_field_override(*args):
            # Iterates over the fields passed to this context manager,
            # until there are no more, upon which the np.fromfile
            # returns an empty list and the while loop in load() is
            # broken.
            result = side_effect_fields.pop(0)
            if not side_effect_fields:
                np.fromfile.return_value = []
            return result

        with mock.patch('numpy.fromfile', return_value=[0]), \
                mock.patch('__builtin__.open'), \
                mock.patch('struct.unpack_from', return_value=[4]), \
                mock.patch('iris.fileformats.pp.make_pp_field',
                           side_effect=make_pp_field_override):
            yield

    def gen_fields(self, fields):
        with self.mock_for_field_gen(fields):
            return list(pp._field_gen('mocked', 'mocked'))

    def test_lblrec_invalid(self):
        pp_field = mock.Mock(lblrec=2,
                             lbext=0)
        with self.assertRaises(ValueError) as err:
            self.gen_fields([pp_field])
        self.assertEqual(str(err.exception),
                         ('LBLREC has a different value to the integer '
                          'recorded after the header in the file (8 '
                          'and 4).'))

    def test_read_headers_call(self):
        # Checks that the two calls to np.fromfile are called in the
        # expected way.
        pp_field = mock.Mock(lblrec=1,
                             lbext=0,
                             lbuser=[0])
        with self.mock_for_field_gen([pp_field]):
            open_fh = mock.Mock()
            open.return_value = open_fh
            next(pp._field_gen('mocked', read_data_bytes=False))
            calls = [mock.call(open_fh, count=45, dtype='>i4'),
                     mock.call(open_fh, count=19, dtype='>f4')]
            np.fromfile.assert_has_calls(calls)
        expected_deferred_bytes = ('mocked', open_fh.tell(),
                                   4, np.dtype('>f4'))
        self.assertEqual(pp_field._data, expected_deferred_bytes)

    def test_read_data_call(self):
        # Checks that data is read if read_data is True.
        pp_field = mock.Mock(lblrec=1,
                             lbext=0,
                             lbuser=[0])
        with self.mock_for_field_gen([pp_field]):
            open_fh = mock.Mock()
            open.return_value = open_fh
            next(pp._field_gen('mocked', read_data_bytes=True))
        expected_loaded_bytes = pp.LoadedArrayBytes(open_fh.read(),
                                                    np.dtype('>f4'))
        self.assertEqual(pp_field._data, expected_loaded_bytes)

if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test__interpret_field
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.fileformats.pp._interpret_field` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

from copy import deepcopy

import mock
import numpy as np

import iris.fileformats.pp as pp


class Test__interpret_fields__land_packed_fields(tests.IrisTest):
    def setUp(self):
        # A field packed using a land/sea mask.
        self.pp_field = mock.Mock(lblrec=1, lbext=0, lbuser=[0] * 7,
                                  lbrow=0, lbnpt=0,
                                  raw_lbpack=20,
                                  _data=('dummy', 0, 0, 0))
        # The field specifying the land/seamask.
        lbuser = [None, None, None, 30, None, None, 1]  # m01s00i030
        self.land_mask_field = mock.Mock(lblrec=1, lbext=0, lbuser=lbuser,
                                         lbrow=3, lbnpt=4,
                                         raw_lbpack=0,
                                         _data=('dummy', 0, 0, 0))

    def test_non_deferred_fix_lbrow_lbnpt(self):
        # Checks the fix_lbrow_lbnpt is applied to fields which are not
        # deferred.
        f1, mask = self.pp_field, self.land_mask_field
        self.assertEqual(f1.lbrow, 0)
        self.assertEqual(f1.lbnpt, 0)
        list(pp._interpret_fields([mask, f1]))
        self.assertEqual(f1.lbrow, 3)
        self.assertEqual(f1.lbnpt, 4)
        # Check the data's shape has been updated too.
        self.assertEqual(f1._data.shape, (3, 4))

    def test_fix_lbrow_lbnpt_no_mask_available(self):
        # Check a warning is issued when loading a land masked field
        # without a land mask.
        with mock.patch('warnings.warn') as warn:
            list(pp._interpret_fields([self.pp_field]))
        self.assertEqual(warn.call_count, 1)
        warn_msg = warn.call_args[0][0]
        self.assertTrue(warn_msg.startswith('Landmask compressed fields '
                                            'existed without a landmask'),
                        'Unexpected warning message: {!r}'.format(warn_msg))

    def test_deferred_mask_field(self):
        # Check that the order of the load is yielded last if the mask
        # hasn't yet been seen.
        result = list(pp._interpret_fields([self.pp_field,
                                            self.land_mask_field]))
        self.assertEqual(result, [self.land_mask_field, self.pp_field])

    def test_not_deferred_mask_field(self):
        # Check that the order of the load is unchanged if a land mask
        # has already been seen.
        f1, mask = self.pp_field, self.land_mask_field
        mask2 = deepcopy(mask)
        result = list(pp._interpret_fields([mask, f1, mask2]))
        self.assertEqual(result, [mask, f1, mask2])

    def test_deferred_fix_lbrow_lbnpt(self):
        # Check the fix is also applied to fields which are deferred.
        f1, mask = self.pp_field, self.land_mask_field
        list(pp._interpret_fields([f1, mask]))
        self.assertEqual(f1.lbrow, 3)
        self.assertEqual(f1.lbnpt, 4)

    def test_shared_land_mask_field(self):
        # Check that multiple land masked fields share the
        # land mask field instance.
        f1 = deepcopy(self.pp_field)
        f2 = deepcopy(self.pp_field)
        self.assertIsNot(f1, f2)
        with mock.patch('iris.fileformats.pp.PPDataProxy') as PPDataProxy:
            PPDataProxy.return_value = mock.MagicMock()
            list(pp._interpret_fields([f1, self.land_mask_field, f2]))
        for call in PPDataProxy.call_args_list:
            positional_args = call[0]
            self.assertIs(positional_args[7], self.land_mask_field)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_convert
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for :func:`iris.fileformats.pp_rules.convert`."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import types

import mock
import numpy as np

from iris.fileformats.pp_rules import convert
from iris.util import guess_coord_axis
from iris.fileformats.pp import SplittableInt
from iris.fileformats.pp import PPField3
import iris.tests.unit.fileformats
import iris.unit


class TestLBCODE(iris.tests.unit.fileformats.TestField):
    @staticmethod
    def _is_cross_section_height_coord(coord):
        return (coord.standard_name == 'height' and
                coord.units == 'km' and
                coord.attributes['positive'] == 'up')

    def test_cross_section_height_bdy_zero(self):
        lbcode = SplittableInt(19902, {'iy': slice(0, 2), 'ix': slice(2, 4)})
        points = np.array([10, 20, 30, 40])
        bounds = np.array([[0, 15], [15, 25], [25, 35], [35, 45]])
        field = mock.MagicMock(lbcode=lbcode, bdy=0, y=points, y_bounds=bounds)
        self._test_for_coord(field, convert,
                             TestLBCODE._is_cross_section_height_coord,
                             expected_points=points,
                             expected_bounds=bounds)

    def test_cross_section_height_bdy_bmdi(self):
        lbcode = SplittableInt(19902, {'iy': slice(0, 2), 'ix': slice(2, 4)})
        points = np.array([10, 20, 30, 40])
        bounds = np.array([[0, 15], [15, 25], [25, 35], [35, 45]])
        bmdi = -1.07374e+09
        field = mock.MagicMock(lbcode=lbcode, bdy=bmdi, bmdi=bmdi,
                               y=points, y_bounds=bounds)
        self._test_for_coord(field, convert,
                             TestLBCODE._is_cross_section_height_coord,
                             expected_points=points,
                             expected_bounds=bounds)


class TestLBVC(iris.tests.unit.fileformats.TestField):
    @staticmethod
    def _is_potm_level_coord(coord):
        return (coord.standard_name == 'air_potential_temperature' and
                coord.attributes['positive'] == 'up')

    @staticmethod
    def _is_model_level_number_coord(coord):
        return (coord.standard_name == 'model_level_number' and
                coord.units.is_dimensionless() and
                coord.attributes['positive'] == 'up')

    @staticmethod
    def _is_level_pressure_coord(coord):
        return (coord.name() == 'level_pressure' and
                coord.units == 'Pa')

    @staticmethod
    def _is_sigma_coord(coord):
        return (coord.name() == 'sigma' and
                coord.units.is_dimensionless())

    @staticmethod
    def _is_soil_model_level_number_coord(coord):
        return (coord.long_name == 'soil_model_level_number' and
                coord.units.is_dimensionless() and
                coord.attributes['positive'] == 'down')

    def test_soil_levels(self):
        level = 1234
        field = mock.MagicMock(lbvc=6, lblev=level)
        self._test_for_coord(field, convert,
                             TestLBVC._is_soil_model_level_number_coord,
                             expected_points=[level],
                             expected_bounds=None)

    def test_hybrid_pressure_model_level_number(self):
        level = 5678
        field = mock.MagicMock(lbvc=9, lblev=level,
                               blev=20, brlev=23, bhlev=42,
                               bhrlev=45, brsvd=[17, 40])
        self._test_for_coord(field, convert,
                             TestLBVC._is_model_level_number_coord,
                             expected_points=[level],
                             expected_bounds=None)

    def test_hybrid_pressure_delta(self):
        delta_point = 12.0
        delta_lower_bound = 11.0
        delta_upper_bound = 13.0
        field = mock.MagicMock(lbvc=9, lblev=5678,
                               blev=20, brlev=23, bhlev=delta_point,
                               bhrlev=delta_lower_bound,
                               brsvd=[17, delta_upper_bound])
        self._test_for_coord(field, convert,
                             TestLBVC._is_level_pressure_coord,
                             expected_points=[delta_point],
                             expected_bounds=[[delta_lower_bound,
                                               delta_upper_bound]])

    def test_hybrid_pressure_sigma(self):
        sigma_point = 0.5
        sigma_lower_bound = 0.6
        sigma_upper_bound = 0.4
        field = mock.MagicMock(lbvc=9, lblev=5678,
                               blev=sigma_point, brlev=sigma_lower_bound,
                               bhlev=12, bhrlev=11,
                               brsvd=[sigma_upper_bound, 13])
        self._test_for_coord(field, convert, TestLBVC._is_sigma_coord,
                             expected_points=[sigma_point],
                             expected_bounds=[[sigma_lower_bound,
                                               sigma_upper_bound]])

    def test_potential_temperature_levels(self):
        potm_value = 27.32
        field = mock.MagicMock(lbvc=19, blev=potm_value)
        self._test_for_coord(field, convert, TestLBVC._is_potm_level_coord,
                             expected_points=np.array([potm_value]),
                             expected_bounds=None)


class TestLBTIM(iris.tests.unit.fileformats.TestField):
    def test_365_calendar(self):
        f = mock.MagicMock(lbtim=SplittableInt(4, {'ia': 2, 'ib': 1, 'ic': 0}),
                           lbyr=2013, lbmon=1, lbdat=1, lbhr=12, lbmin=0,
                           lbsec=0,
                           spec=PPField3)
        f.time_unit = types.MethodType(PPField3.time_unit, f)
        f.calendar = iris.unit.CALENDAR_365_DAY
        (factories, references, standard_name, long_name, units,
         attributes, cell_methods, dim_coords_and_dims,
         aux_coords_and_dims) = convert(f)

        def is_t_coord(coord_and_dims):
            coord, dims = coord_and_dims
            return coord.standard_name == 'time'

        coords_and_dims = filter(is_t_coord, aux_coords_and_dims)
        self.assertEqual(len(coords_and_dims), 1)
        coord, dims = coords_and_dims[0]
        self.assertEqual(guess_coord_axis(coord), 'T')
        self.assertEqual(coord.units.calendar, '365_day')

    def base_field(self):
        field = PPField3()
        field.lbfc = 0
        field.bdx = 1
        field.bdy = 1
        field.bmdi = 999
        field.lbproc = 0
        field.lbvc = 0
        field.lbuser = [0] * 7
        field.lbrsvd = [0] * 4
        field.lbsrce = 0
        field.lbcode = 0
        return field

    @staticmethod
    def is_forecast_period(coord):
        return (coord.standard_name == 'forecast_period' and
                coord.units == 'hours')

    @staticmethod
    def is_time(coord):
        return (coord.standard_name == 'time' and
                coord.units == 'hours since epoch')

    def test_time_mean_ib2(self):
        field = self.base_field()
        field.lbtim = 21
        # Implicit reference time: 1970-01-02 06:00
        field.lbft = 9
        # t1
        field.lbyr, field.lbmon, field.lbdat = 1970, 1, 2
        field.lbhr, field.lbmin, field.lbsec = 12, 0, 0
        # t2
        field.lbyrd, field.lbmond, field.lbdatd = 1970, 1, 2
        field.lbhrd, field.lbmind, field.lbsecd = 15, 0, 0

        self._test_for_coord(field, convert, self.is_forecast_period,
                             expected_points=[7.5],
                             expected_bounds=[[6, 9]])

        self._test_for_coord(field, convert, self.is_time,
                             expected_points=[24 + 13.5],
                             expected_bounds=[[36, 39]])

    def test_time_mean_ib3(self):
        field = self.base_field()
        field.lbtim = 31
        # Implicit reference time: 1970-01-02 06:00
        field.lbft = lbft = ((365 + 1) * 24 + 15) - (24 + 6)
        # t1
        field.lbyr, field.lbmon, field.lbdat = 1970, 1, 2
        field.lbhr, field.lbmin, field.lbsec = 12, 0, 0
        # t2
        field.lbyrd, field.lbmond, field.lbdatd = 1971, 1, 2
        field.lbhrd, field.lbmind, field.lbsecd = 15, 0, 0

        self._test_for_coord(field, convert, self.is_forecast_period,
                             expected_points=[lbft],
                             expected_bounds=[[36 - 30, lbft]])

        self._test_for_coord(field, convert, self.is_time,
                             expected_points=[lbft + 30],
                             expected_bounds=[[36, lbft + 30]])


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test__model_level_number
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for :func:`iris.fileformats.pp_rules._model_level_number`."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock

from iris.fileformats.pp_rules import _model_level_number


class Test_9999(tests.IrisTest):
    def test(self):
        field = mock.Mock(lblev=9999)
        self.assertEqual(_model_level_number(field), 0)


class Test_lblev(tests.IrisTest):
    def test(self):
        for val in xrange(9999):
            field = mock.Mock(lblev=val)
            self.assertEqual(_model_level_number(field), val)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_build_auxiliary_coordinate
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test function :func:`iris.fileformats._pyke_rules.compiled_krb.\
fc_rules_cf_fc.build_auxilliary_coordinate`.

"""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np
import mock

from iris.coords import AuxCoord
from iris.fileformats._pyke_rules.compiled_krb.fc_rules_cf_fc import \
    build_auxiliary_coordinate


class TestBoundsVertexDim(tests.IrisTest):
    def setUp(self):
        # Create coordinate cf variables and pyke engine.
        points = np.arange(6).reshape(2, 3)
        self.cf_coord_var = mock.Mock(
            dimensions=('foo', 'bar'),
            cf_name='wibble',
            standard_name=None,
            long_name='wibble',
            units='m',
            shape=points.shape,
            dtype=points.dtype,
            __getitem__=lambda self, key: points[key])

        self.engine = mock.Mock(
            cube=mock.Mock(),
            cf_var=mock.Mock(dimensions=('foo', 'bar')),
            filename='DUMMY',
            provides=dict(coordinates=[]))

        # Create patch for deferred loading that prevents attempted
        # file access. This assumes that self.cf_bounds_var is
        # defined in the test case.
        def deferred_load(filename, var_name):
            for var in (self.cf_coord_var, self.cf_bounds_var):
                if var_name == var.cf_name:
                    return var[:]

        self.deferred_load_patch = mock.patch(
            'iris.fileformats._pyke_rules.compiled_krb.'
            'fc_rules_cf_fc.deferred_load', new=deferred_load)

    def test_slowest_varying_vertex_dim(self):
        # Create the bounds cf variable.
        bounds = np.arange(24).reshape(4, 2, 3)
        self.cf_bounds_var = mock.Mock(
            dimensions=('nv', 'foo', 'bar'),
            cf_name='wibble_bnds',
            shape=bounds.shape,
            __getitem__=lambda self, key: bounds[key])

        # Expected bounds on the resulting coordinate should be rolled so that
        # the vertex dimension is at the end.
        expected_bounds = np.rollaxis(bounds, 0, bounds.ndim)
        expected_coord = AuxCoord(
            self.cf_coord_var[:],
            long_name=self.cf_coord_var.long_name,
            var_name=self.cf_coord_var.cf_name,
            units=self.cf_coord_var.units,
            bounds=expected_bounds)

        # Patch the helper function that retrieves the bounds cf variable.
        # This avoids the need for setting up further mocking of cf objects.
        get_cf_bounds_var_patch = mock.patch(
            'iris.fileformats._pyke_rules.compiled_krb.'
            'fc_rules_cf_fc.get_cf_bounds_var',
            return_value=self.cf_bounds_var)

        # Asserts must lie within context manager because of deferred loading.
        with self.deferred_load_patch, get_cf_bounds_var_patch:
            build_auxiliary_coordinate(self.engine, self.cf_coord_var)

            # Test that expected coord is built and added to cube.
            self.engine.cube.add_aux_coord.assert_called_with(
                expected_coord, [0, 1])

            # Test that engine.provides container is correctly populated.
            expected_list = [(expected_coord, self.cf_coord_var.cf_name)]
            self.assertEqual(self.engine.provides['coordinates'],
                             expected_list)

    def test_fastest_varying_vertex_dim(self):
        bounds = np.arange(24).reshape(2, 3, 4)
        self.cf_bounds_var = mock.Mock(
            dimensions=('foo', 'bar', 'nv'),
            cf_name='wibble_bnds',
            shape=bounds.shape,
            __getitem__=lambda self, key: bounds[key])

        expected_coord = AuxCoord(
            self.cf_coord_var[:],
            long_name=self.cf_coord_var.long_name,
            var_name=self.cf_coord_var.cf_name,
            units=self.cf_coord_var.units,
            bounds=bounds)

        get_cf_bounds_var_patch = mock.patch(
            'iris.fileformats._pyke_rules.compiled_krb.'
            'fc_rules_cf_fc.get_cf_bounds_var',
            return_value=self.cf_bounds_var)

        # Asserts must lie within context manager because of deferred loading.
        with self.deferred_load_patch, get_cf_bounds_var_patch:
            build_auxiliary_coordinate(self.engine, self.cf_coord_var)

            # Test that expected coord is built and added to cube.
            self.engine.cube.add_aux_coord.assert_called_with(
                expected_coord, [0, 1])

            # Test that engine.provides container is correctly populated.
            expected_list = [(expected_coord, self.cf_coord_var.cf_name)]
            self.assertEqual(self.engine.provides['coordinates'],
                             expected_list)

    def test_fastest_with_different_dim_names(self):
        # Despite the dimension names ('x', and 'y') differing from the coord's
        # which are 'foo' and 'bar' (as permitted by the cf spec),
        # this should still work because the vertex dim is the fastest varying.
        bounds = np.arange(24).reshape(2, 3, 4)
        self.cf_bounds_var = mock.Mock(
            dimensions=('x', 'y', 'nv'),
            cf_name='wibble_bnds',
            shape=bounds.shape,
            __getitem__=lambda self, key: bounds[key])

        expected_coord = AuxCoord(
            self.cf_coord_var[:],
            long_name=self.cf_coord_var.long_name,
            var_name=self.cf_coord_var.cf_name,
            units=self.cf_coord_var.units,
            bounds=bounds)

        get_cf_bounds_var_patch = mock.patch(
            'iris.fileformats._pyke_rules.compiled_krb.'
            'fc_rules_cf_fc.get_cf_bounds_var',
            return_value=self.cf_bounds_var)

        # Asserts must lie within context manager because of deferred loading.
        with self.deferred_load_patch, get_cf_bounds_var_patch:
            build_auxiliary_coordinate(self.engine, self.cf_coord_var)

            # Test that expected coord is built and added to cube.
            self.engine.cube.add_aux_coord.assert_called_with(
                expected_coord, [0, 1])

            # Test that engine.provides container is correctly populated.
            expected_list = [(expected_coord, self.cf_coord_var.cf_name)]
            self.assertEqual(self.engine.provides['coordinates'],
                             expected_list)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_build_dimension_coordinate
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test function :func:`iris.fileformats._pyke_rules.compiled_krb.\
fc_rules_cf_fc.build_dimension_coordinate`.

"""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np
import mock

from iris.coords import DimCoord
from iris.fileformats._pyke_rules.compiled_krb.fc_rules_cf_fc import \
    build_dimension_coordinate


class TestBoundsVertexDim(tests.IrisTest):
    def setUp(self):
        # Create coordinate cf variables and pyke engine.
        points = np.arange(6)
        self.cf_coord_var = mock.Mock(
            dimensions=('foo',),
            cf_name='wibble',
            standard_name=None,
            long_name='wibble',
            units='m',
            shape=points.shape,
            dtype=points.dtype,
            __getitem__=lambda self, key: points[key])

        self.engine = mock.Mock(
            cube=mock.Mock(),
            cf_var=mock.Mock(dimensions=('foo', 'bar')),
            filename='DUMMY',
            provides=dict(coordinates=[]))

        # Create patch for deferred loading that prevents attempted
        # file access. This assumes that self.cf_bounds_var is
        # defined in the test case.
        def deferred_load(filename, var_name):
            for var in (self.cf_coord_var, self.cf_bounds_var):
                if var_name == var.cf_name:
                    return var[:]

        self.deferred_load_patch = mock.patch(
            'iris.fileformats._pyke_rules.compiled_krb.'
            'fc_rules_cf_fc.deferred_load', new=deferred_load)

    def test_slowest_varying_vertex_dim(self):
        # Create the bounds cf variable.
        bounds = np.arange(12).reshape(2, 6)
        self.cf_bounds_var = mock.Mock(
            dimensions=('nv', 'foo'),
            cf_name='wibble_bnds',
            shape=bounds.shape,
            __getitem__=lambda self, key: bounds[key])

        # Expected bounds on the resulting coordinate should be rolled so that
        # the vertex dimension is at the end.
        expected_bounds = bounds.transpose()
        expected_coord = DimCoord(
            self.cf_coord_var[:],
            long_name=self.cf_coord_var.long_name,
            var_name=self.cf_coord_var.cf_name,
            units=self.cf_coord_var.units,
            bounds=expected_bounds)

        # Patch the helper function that retrieves the bounds cf variable.
        # This avoids the need for setting up further mocking of cf objects.
        get_cf_bounds_var_patch = mock.patch(
            'iris.fileformats._pyke_rules.compiled_krb.'
            'fc_rules_cf_fc.get_cf_bounds_var',
            return_value=self.cf_bounds_var)

        # Asserts must lie within context manager because of deferred loading.
        with self.deferred_load_patch, get_cf_bounds_var_patch:
            build_dimension_coordinate(self.engine, self.cf_coord_var)

            # Test that expected coord is built and added to cube.
            self.engine.cube.add_dim_coord.assert_called_with(
                expected_coord, [0])

            # Test that engine.provides container is correctly populated.
            expected_list = [(expected_coord, self.cf_coord_var.cf_name)]
            self.assertEqual(self.engine.provides['coordinates'],
                             expected_list)

    def test_fastest_varying_vertex_dim(self):
        bounds = np.arange(12).reshape(6, 2)
        self.cf_bounds_var = mock.Mock(
            dimensions=('foo', 'nv'),
            cf_name='wibble_bnds',
            shape=bounds.shape,
            __getitem__=lambda self, key: bounds[key])

        expected_coord = DimCoord(
            self.cf_coord_var[:],
            long_name=self.cf_coord_var.long_name,
            var_name=self.cf_coord_var.cf_name,
            units=self.cf_coord_var.units,
            bounds=bounds)

        get_cf_bounds_var_patch = mock.patch(
            'iris.fileformats._pyke_rules.compiled_krb.'
            'fc_rules_cf_fc.get_cf_bounds_var',
            return_value=self.cf_bounds_var)

        # Asserts must lie within context manager because of deferred loading.
        with self.deferred_load_patch, get_cf_bounds_var_patch:
            build_dimension_coordinate(self.engine, self.cf_coord_var)

            # Test that expected coord is built and added to cube.
            self.engine.cube.add_dim_coord.assert_called_with(
                expected_coord, [0])

            # Test that engine.provides container is correctly populated.
            expected_list = [(expected_coord, self.cf_coord_var.cf_name)]
            self.assertEqual(self.engine.provides['coordinates'],
                             expected_list)

    def test_fastest_with_different_dim_names(self):
        # Despite the dimension names 'x' differing from the coord's
        # which is 'foo' (as permitted by the cf spec),
        # this should still work because the vertex dim is the fastest varying.
        bounds = np.arange(12).reshape(6, 2)
        self.cf_bounds_var = mock.Mock(
            dimensions=('x', 'nv'),
            cf_name='wibble_bnds',
            shape=bounds.shape,
            __getitem__=lambda self, key: bounds[key])

        expected_coord = DimCoord(
            self.cf_coord_var[:],
            long_name=self.cf_coord_var.long_name,
            var_name=self.cf_coord_var.cf_name,
            units=self.cf_coord_var.units,
            bounds=bounds)

        get_cf_bounds_var_patch = mock.patch(
            'iris.fileformats._pyke_rules.compiled_krb.'
            'fc_rules_cf_fc.get_cf_bounds_var',
            return_value=self.cf_bounds_var)

        # Asserts must lie within context manager because of deferred loading.
        with self.deferred_load_patch, get_cf_bounds_var_patch:
            build_dimension_coordinate(self.engine, self.cf_coord_var)

            # Test that expected coord is built and added to cube.
            self.engine.cube.add_dim_coord.assert_called_with(
                expected_coord, [0])

            # Test that engine.provides container is correctly populated.
            expected_list = [(expected_coord, self.cf_coord_var.cf_name)]
            self.assertEqual(self.engine.provides['coordinates'],
                             expected_list)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_reorder_bounds_data
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test function :func:`iris.fileformats._pyke_rules.compiled_krb.\
fc_rules_cf_fc.reorder_bounds_data`.

"""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np
import mock

from iris.aux_factory import LazyArray
from iris.fileformats._pyke_rules.compiled_krb.fc_rules_cf_fc import \
    reorder_bounds_data


class Test(tests.IrisTest):
    def test_fastest_varying(self):
        bounds_data = np.arange(24).reshape(2, 3, 4)
        cf_bounds_var = mock.Mock(dimensions=('foo', 'bar', 'nv'),
                                  cf_name='wibble_bnds')
        cf_coord_var = mock.Mock(dimensions=('foo', 'bar'))

        res = reorder_bounds_data(bounds_data, cf_bounds_var, cf_coord_var)
        # Vertex dimension (nv) is already at the end.
        self.assertArrayEqual(res, bounds_data)

    def test_slowest_varying(self):
        bounds_data = np.arange(24).reshape(4, 2, 3)
        cf_bounds_var = mock.Mock(dimensions=('nv', 'foo', 'bar'))
        cf_coord_var = mock.Mock(dimensions=('foo', 'bar'))

        res = reorder_bounds_data(bounds_data, cf_bounds_var, cf_coord_var)
        # Move zeroth dimension (nv) to the end.
        expected = np.rollaxis(bounds_data, 0, bounds_data.ndim)
        self.assertArrayEqual(res, expected)

    def test_slowest_varying_lazy(self):
        bounds_data = np.arange(24).reshape(4, 2, 3)
        func = lambda: bounds_data
        lazy_bounds_data = LazyArray(bounds_data.shape, func)
        cf_bounds_var = mock.Mock(dimensions=('nv', 'foo', 'bar'))
        cf_coord_var = mock.Mock(dimensions=('foo', 'bar'))

        res = reorder_bounds_data(lazy_bounds_data, cf_bounds_var,
                                  cf_coord_var)
        # Move zeroth dimension (nv) to the end.
        expected = np.rollaxis(bounds_data, 0, bounds_data.ndim)
        self.assertArrayEqual(res, expected)

    def test_different_dim_names(self):
        bounds_data = np.arange(24).reshape(2, 3, 4)
        cf_bounds_var = mock.Mock(dimensions=('foo', 'bar', 'nv'),
                                  cf_name='wibble_bnds')
        cf_coord_var = mock.Mock(dimensions=('x', 'y'), cf_name='wibble')
        with self.assertRaisesRegexp(ValueError, 'dimension names'):
            reorder_bounds_data(bounds_data, cf_bounds_var, cf_coord_var)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_run_callback
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.io.run_callback` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import mock
import iris.exceptions
import iris.io


class Test_run_callback(tests.IrisTest):
    def setUp(self):
        tests.IrisTest.setUp(self)
        self.cube = mock.sentinel.cube

    def test_no_callback(self):
        # No callback results in the cube being returned.
        self.assertEqual(iris.io.run_callback(None, self.cube, None, None),
                         self.cube)

    def test_ignore_cube(self):
        # Ignore cube should result in None being returned.
        def callback(cube, field, fname):
            raise iris.exceptions.IgnoreCubeException()
        cube = self.cube
        self.assertEqual(iris.io.run_callback(callback, cube, None, None),
                         None)

    def test_callback_no_return(self):
        # Check that a callback not returning anything still results in the
        # cube being passed back from "run_callback".
        def callback(cube, field, fname):
            pass

        cube = self.cube
        self.assertEqual(iris.io.run_callback(callback, cube, None, None),
                         cube)

    def test_bad_callback_return_type(self):
        # Check that a TypeError is raised with a bad callback return value.
        def callback(cube, field, fname):
            return iris.cube.CubeList()
        with self.assertRaisesRegexp(TypeError,
                                     'Callback function returned an '
                                     'unhandled data type.'):
            iris.io.run_callback(callback, None, None, None)

    def test_bad_signature(self):
        # Check that a TypeError is raised with a bad callback function
        # signature.
        def callback(cube):
            pass
        with self.assertRaisesRegexp(TypeError,
                                     'takes exactly 1 argument '):
            iris.io.run_callback(callback, None, None, None)

    def test_callback_args(self):
        # Check that the appropriate args are passed through to the callback.
        self.field = mock.sentinel.field
        self.fname = mock.sentinel.fname

        def callback(cube, field, fname):
            self.assertEqual(cube, self.cube)
            self.assertEqual(field, self.field)
            self.assertEqual(fname, self.fname)

        iris.io.run_callback(callback, self.cube, self.field, self.fname)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_ProtoCube
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris._merge.ProtoCube` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import abc

import mock
import numpy as np
import numpy.ma as ma

import iris
from iris._merge import ProtoCube
from iris.aux_factory import HybridHeightFactory, HybridPressureFactory
from iris.coords import DimCoord, AuxCoord
from iris.exceptions import MergeError
from iris.unit import Unit


def example_cube():
    return iris.cube.Cube(np.array([1, 2, 3], dtype='i4'),
                          standard_name='air_temperature',
                          long_name='screen_air_temp', var_name='airtemp',
                          units='K', attributes={'mint': 'thin'})


class Mixin_register(object):
    __metaclass__ = abc.ABCMeta

    @property
    def cube1(self):
        return example_cube()

    @abc.abstractproperty
    def cube2():
        pass

    @abc.abstractproperty
    def fragments(self):
        pass

    def test_default(self):
        # Test what happens when we call:
        #   ProtoCube.register(cube)
        proto_cube = ProtoCube(self.cube1)
        result = proto_cube.register(self.cube2)
        self.assertEqual(result, not self.fragments)

    def test_no_error(self):
        # Test what happens when we call:
        #   ProtoCube.register(cube, error_on_mismatch=False)
        proto_cube = ProtoCube(self.cube1)
        result = proto_cube.register(self.cube2, error_on_mismatch=False)
        self.assertEqual(result, not self.fragments)

    def test_error(self):
        # Test what happens when we call:
        #   ProtoCube.register(cube, error_on_mismatch=True)
        proto_cube = ProtoCube(self.cube1)
        if self.fragments:
            with self.assertRaises(iris.exceptions.MergeError) as cm:
                proto_cube.register(self.cube2, error_on_mismatch=True)
            error_message = str(cm.exception)
            for substr in self.fragments:
                self.assertIn(substr, error_message)
        else:
            result = proto_cube.register(self.cube2, error_on_mismatch=True)
            self.assertTrue(result)


class Test_register__match(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return []

    @property
    def cube2(self):
        return example_cube()


class Test_register__standard_name(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube.standard_name', 'air_temperature', 'air_density']

    @property
    def cube2(self):
        cube = example_cube()
        cube.standard_name = 'air_density'
        return cube


class Test_register__long_name(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube.long_name', 'screen_air_temp', 'Belling']

    @property
    def cube2(self):
        cube = example_cube()
        cube.long_name = 'Belling'
        return cube


class Test_register__var_name(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube.var_name', "'airtemp'", "'airtemp2'"]

    @property
    def cube2(self):
        cube = example_cube()
        cube.var_name = 'airtemp2'
        return cube


class Test_register__units(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube.units', "'K'", "'C'"]

    @property
    def cube2(self):
        cube = example_cube()
        cube.units = 'C'
        return cube


class Test_register__attributes_unequal(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube.attributes', "'mint'"]

    @property
    def cube2(self):
        cube = example_cube()
        cube.attributes['mint'] = 'waffer-thin'
        return cube


class Test_register__attributes_unequal_array(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube.attributes', "'mint'"]

    @property
    def cube1(self):
        cube = example_cube()
        cube.attributes['mint'] = np.arange(3)
        return cube

    @property
    def cube2(self):
        cube = example_cube()
        cube.attributes['mint'] = np.arange(3) + 1
        return cube


class Test_register__attributes_superset(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube.attributes', "'stuffed'"]

    @property
    def cube2(self):
        cube = example_cube()
        cube.attributes['stuffed'] = 'yes'
        return cube


class Test_register__attributes_multi_diff(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube.attributes', "'sam'", "'mint'"]

    @property
    def cube1(self):
        cube = example_cube()
        cube.attributes['ralph'] = 1
        cube.attributes['sam'] = 2
        cube.attributes['tom'] = 3
        return cube

    @property
    def cube2(self):
        cube = example_cube()
        cube.attributes['ralph'] = 1
        cube.attributes['sam'] = 'mug'
        cube.attributes['tom'] = 3
        cube.attributes['mint'] = 'humbug'
        return cube


class Test_register__cell_method(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube.cell_methods']

    @property
    def cube2(self):
        cube = example_cube()
        cube.add_cell_method(iris.coords.CellMethod('monty', ('python',)))
        return cube


class Test_register__data_shape(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube.shape', '(2,)', '(3,)']

    @property
    def cube2(self):
        cube = example_cube()
        cube = cube[1:]
        return cube


class Test_register__data_dtype(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube data dtype', 'int32', 'int8']

    @property
    def cube2(self):
        cube = example_cube()
        cube.data = cube.data.astype(np.int8)
        return cube


class Test_register__fill_value(Mixin_register, tests.IrisTest):
    @property
    def fragments(self):
        return ['cube data fill_value', '654', '12345']

    @property
    def cube1(self):
        cube = example_cube()
        cube.data = ma.array(cube.data, fill_value=654)
        return cube

    @property
    def cube2(self):
        cube = example_cube()
        cube.data = ma.array(cube.data, fill_value=12345)
        return cube


class _MergeTest(object):
    # A mixin test class for common test methods implementation.

    # used by check routine: inheritors must implement it
    _mergetest_type = NotImplementedError

    def check_merge_fails_with_message(self):
        proto_cube = iris._merge.ProtoCube(self.cube1)
        with self.assertRaises(MergeError) as arc:
            proto_cube.register(self.cube2, error_on_mismatch=True)
        return str(arc.exception)

    def check_fail(self, *substrs):
        if isinstance(substrs, basestring):
            substrs = [substrs]
        msg = self.check_merge_fails_with_message()
        for substr in substrs:
            self.assertIn(substr, msg)


class Test_register__CubeSig(_MergeTest, tests.IrisTest):
    # Test potential registration failures.

    _mergetest_type = 'cube'

    def setUp(self):
        self.cube1 = iris.cube.Cube([1, 2, 3], standard_name="air_temperature",
                                    units='K', attributes={"mint": "thin"})
        self.cube2 = self.cube1.copy()

    def test_noise(self):
        # Test a massive set of all defn diffs to make sure it's not noise.
        self.cube1.var_name = "Arthur"
        cube2 = self.cube1[1:]
        cube2.data = cube2.data.astype(np.int8)
        cube2.data = ma.array(cube2.data)
        cube2.data.fill_value = 12345
        cube2.standard_name = "air_pressure"
        cube2.var_name = "Nudge"
        cube2.attributes['stuffed'] = 'yes'
        cube2.attributes['mint'] = 'waffer-thin'
        cube2.add_cell_method(iris.coords.CellMethod('monty', ('python',)))

        # Check the actual message, so we've got a readable reference text.
        self.cube2 = cube2
        msg = self.check_merge_fails_with_message()
        self.assertString(msg, self.result_path(ext='txt'))


class Test_register__CoordSig_general(_MergeTest, tests.IrisTest):

    _mergetest_type = 'coord'

    def setUp(self):
        self.cube1 = iris.cube.Cube(np.zeros((3, 3, 3)))
        self.cube2 = self.cube1.copy()

    def test_scalar_defns_one_extra(self):
        self.cube2.add_aux_coord(DimCoord([1], standard_name="latitude"))
        self.check_fail('aux_coords (scalar)', 'latitude')

    def test_scalar_defns_both_extra(self):
        self.cube2.add_aux_coord(DimCoord([1], standard_name="latitude"))
        self.cube1.add_aux_coord(DimCoord([1], standard_name="longitude"))
        self.check_fail('aux_coords (scalar)', 'latitude', 'longitude')

    def test_vector_dim_coords_and_dims_one_extra(self):
        self.cube2.add_dim_coord(
            DimCoord([1, 2, 3], standard_name="latitude"), 0)
        self.check_fail('dim_coords', 'latitude')

    def test_vector_dim_coords_and_dims_both_extra(self):
        self.cube2.add_dim_coord(
            DimCoord([1, 2, 3], standard_name="latitude"), 0)
        self.cube1.add_dim_coord(
            DimCoord([1, 2, 3], standard_name="longitude"), 0)
        self.check_fail('dim_coords', 'latitude', 'longitude')

    def test_vector_aux_coords_and_dims_one_extra(self):
        self.cube2.add_aux_coord(
            DimCoord([1, 2, 3], standard_name="latitude"), 0)
        self.check_fail('aux_coords (non-scalar)', 'latitude')

    def test_vector_aux_coords_and_dims_both_extra(self):
        self.cube2.add_aux_coord(
            DimCoord([1, 2, 3], standard_name="latitude"), 0)
        self.cube1.add_aux_coord(
            DimCoord([1, 2, 3], standard_name="longitude"), 0)
        self.check_fail('aux_coords (non-scalar)', 'latitude', 'longitude')

    def test_factory_defns_one_extra(self):
        self.cube2.add_aux_factory(mock.MagicMock(spec=HybridHeightFactory))
        self.check_fail("cube.aux_factories", "differ")

    def test_factory_defns_both_extra(self):
        self.cube2.add_aux_factory(mock.MagicMock(spec=HybridHeightFactory))
        self.cube1.add_aux_factory(mock.MagicMock(spec=HybridPressureFactory))
        self.check_fail("cube.aux_factories", "differ")

    def test_noise(self):
        cube2 = self.cube2

        # scalar
        cube2.add_aux_coord(DimCoord([1], long_name="liff"))
        cube2.add_aux_coord(DimCoord([1], long_name="life"))
        cube2.add_aux_coord(DimCoord([1], long_name="like"))

        self.cube1.add_aux_coord(DimCoord([1], var_name="ming"))
        self.cube1.add_aux_coord(DimCoord([1], var_name="mong"))
        self.cube1.add_aux_coord(DimCoord([1], var_name="moog"))

        # aux
        cube2.add_dim_coord(DimCoord([1, 2, 3], standard_name="latitude"), 0)
        cube2.add_dim_coord(DimCoord([1, 2, 3], standard_name="longitude"), 1)
        cube2.add_dim_coord(DimCoord([1, 2, 3], standard_name="altitude"), 2)

        self.cube1.add_dim_coord(
            DimCoord([1, 2, 3], long_name="equinimity"), 0)
        self.cube1.add_dim_coord(
            DimCoord([1, 2, 3], long_name="equinomity"), 1)
        self.cube1.add_dim_coord(
            DimCoord([1, 2, 3], long_name="equinumity"), 2)

        # dim
        cube2.add_aux_coord(DimCoord([1, 2, 3], var_name="one"), 0)
        cube2.add_aux_coord(DimCoord([1, 2, 3], var_name="two"), 1)
        cube2.add_aux_coord(DimCoord([1, 2, 3], var_name="three"), 2)

        self.cube1.add_aux_coord(DimCoord([1, 2, 3], long_name="ay"), 0)
        self.cube1.add_aux_coord(DimCoord([1, 2, 3], long_name="bee"), 1)
        self.cube1.add_aux_coord(DimCoord([1, 2, 3], long_name="cee"), 2)

        # factory
        cube2.add_aux_factory(mock.MagicMock(spec=HybridHeightFactory))
        self.cube1.add_aux_factory(mock.MagicMock(spec=HybridPressureFactory))

        # Check the actual message, so we've got a readable reference text.
        self.cube2 = cube2
        msg = self.check_merge_fails_with_message()
        self.assertString(msg, self.result_path(ext='txt'))


class _MergeTest_coordprops(_MergeTest):
    # A mixin test class for common coordinate properties tests.

    # This must be implemented by inheritors.
    _mergetest_type = NotImplementedError

    def test_nochange(self):
        # This should simply succeed.
        proto_cube = iris._merge.ProtoCube(self.cube1)
        proto_cube.register(self.cube2, error_on_mismatch=True)

    def _props_fail(self, *terms):
        self.check_fail(self._mergetest_type, self.coord_to_change.name(),
                        *terms)

    def test_standard_name(self):
        self.coord_to_change.standard_name = 'soil_temperature'
        self._props_fail('air_temperature', 'soil_temperature')

    def test_long_name(self):
        self.coord_to_change.long_name = 'alternate_name'
        self._props_fail('air_temperature')

    def test_var_name(self):
        self.coord_to_change.var_name = 'alternate_name'
        self._props_fail('air_temperature')

    def test_units(self):
        self.coord_to_change.units = 'm'
        self._props_fail('air_temperature')

    def test_attrs_unequal(self):
        self.coord_to_change.attributes['att_a'] = 99
        self._props_fail('air_temperature')

    def test_attrs_set(self):
        self.coord_to_change.attributes['att_extra'] = 101
        self._props_fail('air_temperature')

    def test_coord_system(self):
        self.coord_to_change.coord_system = mock.Mock()
        self._props_fail('air_temperature')


class Test_register__CoordSig_scalar(_MergeTest_coordprops, tests.IrisTest):

    _mergetest_type = 'aux_coords (scalar)'

    def setUp(self):
        self.cube1 = iris.cube.Cube(np.zeros((3, 3, 3)))
        self.cube1.add_aux_coord(DimCoord(
            [1],
            standard_name="air_temperature",
            long_name='eg_scalar',
            var_name='t1',
            units='K',
            attributes={'att_a': 1, 'att_b': 2},
            coord_system=None))
        self.coord_to_change = self.cube1.coord('air_temperature')
        self.cube2 = self.cube1.copy()


class _MergeTest_coordprops_vect(_MergeTest_coordprops):
    # A derived mixin test class.
    # Adds extra props test for aux+dim coords (test points, bounds + dims)
    _mergetest_type = NotImplementedError
    _coord_typename = NotImplementedError

    def test_points(self):
        self.coord_to_change.points = self.coord_to_change.points + 1.0
        self.check_fail(self._mergetest_type, 'air_temperature')

    def test_bounds(self):
        self.coord_to_change.bounds = self.coord_to_change.bounds + 1.0
        self.check_fail(self._mergetest_type, 'air_temperature')

    def test_dims(self):
        self.cube2.remove_coord(self.coord_to_change)
        cube2_add_method = getattr(self.cube2, 'add_'+self._coord_typename)
        cube2_add_method(self.coord_to_change, (1,))
        self.check_fail(self._mergetest_type, 'mapping')


class Test_register__CoordSig_dim(_MergeTest_coordprops_vect, tests.IrisTest):

    _mergetest_type = 'dim_coords'
    _coord_typename = 'dim_coord'

    def setUp(self):
        self.cube1 = iris.cube.Cube(np.zeros((3, 3)))
        self.cube1.add_dim_coord(DimCoord(
            [15, 25, 35],
            bounds=[[10, 20], [20, 30], [30, 40]],
            standard_name="air_temperature",
            long_name='eg_scalar',
            var_name='t1',
            units='K',
            attributes={'att_a': 1, 'att_b': 2},
            coord_system=None),
            (0,))
        self.coord_to_change = self.cube1.coord('air_temperature')
        self.cube2 = self.cube1.copy()

    def test_circular(self):
        # Extra failure mode that only applies to dim coords
        self.coord_to_change.circular = True
        self.check_fail(self._mergetest_type, 'air_temperature')


class Test_register__CoordSig_aux(_MergeTest_coordprops_vect, tests.IrisTest):

    _mergetest_type = 'aux_coords (non-scalar)'
    _coord_typename = 'aux_coord'

    def setUp(self):
        self.cube1 = iris.cube.Cube(np.zeros((3, 3)))
        self.cube1.add_aux_coord(AuxCoord(
            [65, 45, 85],
            bounds=[[60, 70], [40, 50], [80, 90]],
            standard_name="air_temperature",
            long_name='eg_scalar',
            var_name='t1',
            units='K',
            attributes={'att_a': 1, 'att_b': 2},
            coord_system=None),
            (0,))
        self.coord_to_change = self.cube1.coord('air_temperature')
        self.cube2 = self.cube1.copy()


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_contour
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.plot.contour` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.plot as iplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        iplt.contour(self.cube, coords=('bar', 'str_coord'))
        self.assertPointsTickLabels('yaxis')

    def test_xaxis_labels(self):
        iplt.contour(self.cube, coords=('str_coord', 'bar'))
        self.assertPointsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_contourf
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.plot.contourf` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.plot as iplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        iplt.contourf(self.cube, coords=('bar', 'str_coord'))
        self.assertPointsTickLabels('yaxis')

    def test_xaxis_labels(self):
        iplt.contourf(self.cube, coords=('str_coord', 'bar'))
        self.assertPointsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_outline
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.plot.outline` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.plot as iplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        iplt.outline(self.cube, coords=('bar', 'str_coord'))
        self.assertBoundsTickLabels('yaxis')

    def test_xaxis_labels(self):
        iplt.outline(self.cube, coords=('str_coord', 'bar'))
        self.assertBoundsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pcolor
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.plot.pcolor` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.plot as iplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        iplt.pcolor(self.cube, coords=('bar', 'str_coord'))
        self.assertBoundsTickLabels('yaxis')

    def test_xaxis_labels(self):
        iplt.pcolor(self.cube, coords=('str_coord', 'bar'))
        self.assertBoundsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pcolormesh
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.plot.pcolormesh` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.plot as iplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        iplt.pcolormesh(self.cube, coords=('bar', 'str_coord'))
        self.assertBoundsTickLabels('yaxis')

    def test_xaxis_labels(self):
        iplt.pcolormesh(self.cube, coords=('str_coord', 'bar'))
        self.assertBoundsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_plot
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.plot.plot` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.plot as iplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def setUp(self):
        super(TestStringCoordPlot, self).setUp()
        self.cube = self.cube[0, :]

    def test_yaxis_labels(self):
        iplt.plot(self.cube, self.cube.coord('str_coord'))
        self.assertPointsTickLabels('yaxis')

    def test_xaxis_labels(self):
        iplt.plot(self.cube.coord('str_coord'), self.cube)
        self.assertPointsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_points
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.plot.points` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.plot as iplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        iplt.points(self.cube, coords=('bar', 'str_coord'))
        self.assertBoundsTickLabels('yaxis')

    def test_xaxis_labels(self):
        iplt.points(self.cube, coords=('str_coord', 'bar'))
        self.assertBoundsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_scatter
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.plot.scatter` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.plot as iplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def setUp(self):
        super(TestStringCoordPlot, self).setUp()
        self.cube = self.cube[0, :]

    def test_xaxis_labels(self):
        iplt.scatter(self.cube.coord('str_coord'), self.cube)
        self.assertBoundsTickLabels('xaxis')

    def test_yaxis_labels(self):
        iplt.scatter(self.cube, self.cube.coord('str_coord'))
        self.assertBoundsTickLabels('yaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_contour
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.quickplot.contour` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.quickplot as qplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        qplt.contour(self.cube, coords=('bar', 'str_coord'))
        self.assertPointsTickLabels('yaxis')

    def test_xaxis_labels(self):
        qplt.contour(self.cube, coords=('str_coord', 'bar'))
        self.assertPointsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_contourf
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.quickplot.contourf` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.quickplot as qplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        qplt.contourf(self.cube, coords=('bar', 'str_coord'))
        self.assertPointsTickLabels('yaxis')

    def test_xaxis_labels(self):
        qplt.contourf(self.cube, coords=('str_coord', 'bar'))
        self.assertPointsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_outline
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.quickplot.outline` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.quickplot as qplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        qplt.outline(self.cube, coords=('bar', 'str_coord'))
        self.assertBoundsTickLabels('yaxis')

    def test_xaxis_labels(self):
        qplt.outline(self.cube, coords=('str_coord', 'bar'))
        self.assertBoundsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pcolor
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.quickplot.pcolor` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.quickplot as qplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        qplt.pcolor(self.cube, coords=('bar', 'str_coord'))
        self.assertBoundsTickLabels('yaxis')

    def test_xaxis_labels(self):
        qplt.pcolor(self.cube, coords=('str_coord', 'bar'))
        self.assertBoundsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_pcolormesh
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.quickplot.pcolormesh` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.quickplot as qplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        qplt.pcolormesh(self.cube, coords=('bar', 'str_coord'))
        self.assertBoundsTickLabels('yaxis')

    def test_xaxis_labels(self):
        qplt.pcolormesh(self.cube, coords=('str_coord', 'bar'))
        self.assertBoundsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_plot
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.quickplot.plot` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.quickplot as qplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def setUp(self):
        super(TestStringCoordPlot, self).setUp()
        self.cube = self.cube[0, :]

    def test_yaxis_labels(self):
        qplt.plot(self.cube, self.cube.coord('str_coord'))
        self.assertPointsTickLabels('yaxis')

    def test_xaxis_labels(self):
        qplt.plot(self.cube.coord('str_coord'), self.cube)
        self.assertPointsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_points
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.quickplot.points` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.quickplot as qplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def test_yaxis_labels(self):
        qplt.points(self.cube, coords=('bar', 'str_coord'))
        self.assertBoundsTickLabels('yaxis')

    def test_xaxis_labels(self):
        qplt.points(self.cube, coords=('str_coord', 'bar'))
        self.assertBoundsTickLabels('xaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_scatter
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.quickplot.scatter` function."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests
from iris.tests.unit.plot import TestGraphicStringCoord

if tests.MPL_AVAILABLE:
    import iris.quickplot as qplt


@tests.skip_plot
class TestStringCoordPlot(TestGraphicStringCoord):
    def setUp(self):
        super(TestStringCoordPlot, self).setUp()
        self.cube = self.cube[0, :]

    def test_xaxis_labels(self):
        qplt.scatter(self.cube.coord('str_coord'), self.cube)
        self.assertBoundsTickLabels('xaxis')

    def test_yaxis_labels(self):
        qplt.scatter(self.cube, self.cube.coord('str_coord'))
        self.assertBoundsTickLabels('yaxis')


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_IrisTest
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the :mod:`iris.tests.IrisTest` class."""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

from abc import ABCMeta, abstractproperty

import numpy as np


class _MaskedArrayEquality(object):
    __metaclass__ = ABCMeta

    def setUp(self):
        self.arr1 = np.ma.array([1, 2, 3, 4], mask=[False, True, True, False])
        self.arr2 = np.ma.array([1, 3, 2, 4], mask=[False, True, True, False])

    @abstractproperty
    def _func(self):
        pass

    def test_strict_comparison(self):
        # Comparing both mask and data array completely.
        with self.assertRaises(AssertionError):
            self._func(self.arr1, self.arr2, strict=True)

    def test_non_strict_comparison(self):
        # Checking masked array equality and all unmasked array data values.
        self._func(self.arr1, self.arr2, strict=False)

    def test_default_strict_arg_comparison(self):
        self._func(self.arr1, self.arr2)

    def test_nomask(self):
        # Test that an assertion is raised when comparing a masked array
        # containing masked and unmasked values with a masked array with
        # 'nomask'.
        arr1 = np.ma.array([1, 2, 3, 4])
        with self.assertRaises(AssertionError):
            self._func(arr1, self.arr2, strict=False)

    def test_nomask_unmasked(self):
        # Ensure that a masked array with 'nomask' can compare with an entirely
        # unmasked array.
        arr1 = np.ma.array([1, 2, 3, 4])
        arr2 = np.ma.array([1, 2, 3, 4], mask=False)
        self._func(arr1, arr2, strict=False)

    def test_different_mask_strict(self):
        # Differing masks, equal data
        arr2 = self.arr1.copy()
        arr2[0] = np.ma.masked
        with self.assertRaises(AssertionError):
            self._func(self.arr1, arr2, strict=True)

    def test_different_mask_nonstrict(self):
        # Differing masks, equal data
        arr2 = self.arr1.copy()
        arr2[0] = np.ma.masked
        with self.assertRaises(AssertionError):
            self._func(self.arr1, arr2, strict=False)


class Test_assertMaskedArrayEqual(_MaskedArrayEquality, tests.IrisTest):
    @property
    def _func(self):
        return self.assertMaskedArrayEqual


class Test_assertMaskedArrayAlmostEqual(_MaskedArrayEquality, tests.IrisTest):
    @property
    def _func(self):
        return self.assertMaskedArrayAlmostEqual


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_Future
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.Future` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

from iris import Future


class Test___setattr__(tests.IrisTest):
    def test_valid_attribute(self):
        future = Future()
        new_value = not future.cell_datetime_objects
        future.cell_datetime_objects = new_value
        self.assertEqual(future.cell_datetime_objects, new_value)

    def test_invalid_attribute(self):
        future = Future()
        with self.assertRaises(AttributeError):
            future.numberwang = 7


class Test_context(tests.IrisTest):
    def test_no_args(self):
        future = Future(cell_datetime_objects=False)
        self.assertFalse(future.cell_datetime_objects)
        with future.context():
            self.assertFalse(future.cell_datetime_objects)
            future.cell_datetime_objects = True
            self.assertTrue(future.cell_datetime_objects)
        self.assertFalse(future.cell_datetime_objects)

    def test_with_arg(self):
        future = Future(cell_datetime_objects=False)
        self.assertFalse(future.cell_datetime_objects)
        with future.context(cell_datetime_objects=True):
            self.assertTrue(future.cell_datetime_objects)
        self.assertFalse(future.cell_datetime_objects)

    def test_invalid_arg(self):
        future = Future()
        with self.assertRaises(AttributeError):
            with future.context(this_does_not_exist=True):
                # Don't need to do anything here... the context manager
                # will (assuming it's working!) have already raised the
                # exception we're looking for.
                pass

    def test_exception(self):
        # Check that an interrupted context block restores the initial state.
        class LocalTestException(Exception):
            pass

        future = Future(cell_datetime_objects=False)
        try:
            with future.context(cell_datetime_objects=True):
                raise LocalTestException()
        except LocalTestException:
            pass
        self.assertEqual(future.cell_datetime_objects, False)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_PartialDateTime
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.time.PartialDateTime` class."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import datetime
import operator

import mock
import netcdftime

from iris.time import PartialDateTime


class Test___init__(tests.IrisTest):
    def test_positional(self):
        # Test that we can define PartialDateTimes with positional arguments.
        pd = PartialDateTime(1066, None, 10)
        self.assertEqual(pd.year, 1066)
        self.assertEqual(pd.month, None)
        self.assertEqual(pd.day, 10)

    def test_keyword_args(self):
        # Test that we can define PartialDateTimes with keyword arguments.
        pd = PartialDateTime(microsecond=10)
        self.assertEqual(pd.year, None)
        self.assertEqual(pd.microsecond, 10)


class Test___repr__(tests.IrisTest):
    def test_full(self):
        pd = PartialDateTime(*range(7))
        result = repr(pd)
        self.assertEqual(result, 'PartialDateTime(year=0, month=1, day=2,'
                                 ' hour=3, minute=4, second=5,'
                                 ' microsecond=6)')

    def test_partial(self):
        pd = PartialDateTime(month=2, day=30)
        result = repr(pd)
        self.assertEqual(result, 'PartialDateTime(month=2, day=30)')

    def test_empty(self):
        pd = PartialDateTime()
        result = repr(pd)
        self.assertEqual(result, 'PartialDateTime()')


class Test_timetuple(tests.IrisTest):
    def test_exists(self):
        # Check that the PartialDateTime class implements a timetuple (needed
        # because of http://bugs.python.org/issue8005).
        pd = PartialDateTime(*range(7))
        self.assertTrue(hasattr(pd, 'timetuple'))


class _Test_operator(object):
    def test_invalid_type(self):
        pdt = PartialDateTime()
        with self.assertRaises(TypeError):
            self.op(pdt, 1)

    def _test(self, pdt, other, name):
        expected = self.expected_value[name]
        if isinstance(expected, type):
            with self.assertRaises(expected):
                result = self.op(pdt, other)
        else:
            result = self.op(pdt, other)
            self.assertIs(result, expected)

    def _test_dt(self, pdt, name):
        other = mock.Mock(name='datetime', spec=datetime.datetime,
                          year=2013, month=3, day=20, second=2)
        self._test(pdt, other, name)

    def test_no_difference(self):
        self._test_dt(PartialDateTime(year=2013, month=3, day=20, second=2),
                      'no_difference')

    def test_null(self):
        self._test_dt(PartialDateTime(), 'null')

    def test_item1_lo(self):
        self._test_dt(PartialDateTime(year=2011, month=3, second=2),
                      'item1_lo')

    def test_item1_hi(self):
        self._test_dt(PartialDateTime(year=2015, month=3, day=24), 'item1_hi')

    def test_item2_lo(self):
        self._test_dt(PartialDateTime(year=2013, month=1, second=2),
                      'item2_lo')

    def test_item2_hi(self):
        self._test_dt(PartialDateTime(year=2013, month=5, day=24), 'item2_hi')

    def test_item3_lo(self):
        self._test_dt(PartialDateTime(year=2013, month=3, second=1),
                      'item3_lo')

    def test_item3_hi(self):
        self._test_dt(PartialDateTime(year=2013, month=3, second=42),
                      'item3_hi')

    def test_mix_hi_lo(self):
        self._test_dt(PartialDateTime(year=2015, month=1, day=24), 'mix_hi_lo')

    def test_mix_lo_hi(self):
        self._test_dt(PartialDateTime(year=2011, month=5, day=24), 'mix_lo_hi')

    def _test_pdt(self, other, name):
        pdt = PartialDateTime(year=2013, day=24)
        self._test(pdt, other, name)

    def test_pdt_same(self):
        self._test_pdt(PartialDateTime(year=2013, day=24), 'pdt_same')

    def test_pdt_diff(self):
        self._test_pdt(PartialDateTime(year=2013, day=25), 'pdt_diff')

    def test_pdt_diff_fewer_fields(self):
        self._test_pdt(PartialDateTime(year=2013), 'pdt_diff_fewer')

    def test_pdt_diff_more_fields(self):
        self._test_pdt(PartialDateTime(year=2013, day=24, hour=12),
                       'pdt_diff_more')

    def test_pdt_diff_no_fields(self):
        pdt1 = PartialDateTime()
        pdt2 = PartialDateTime(month=3, day=24)
        self._test(pdt1, pdt2, 'pdt_empty')


def negate_expectations(expectations):
    def negate(expected):
        if not isinstance(expected, type):
            expected = not expected
        return expected

    return {name: negate(value) for name, value in expectations.iteritems()}


EQ_EXPECTATIONS = {'no_difference': True, 'item1_lo': False, 'item1_hi': False,
                   'item2_lo': False, 'item2_hi': False, 'item3_lo': False,
                   'item3_hi': False, 'mix_hi_lo': False, 'mix_lo_hi': False,
                   'null': True, 'pdt_same': True, 'pdt_diff': False,
                   'pdt_diff_fewer': False, 'pdt_diff_more': False,
                   'pdt_empty': False}

GT_EXPECTATIONS = {'no_difference': False, 'item1_lo': False, 'item1_hi': True,
                   'item2_lo': False, 'item2_hi': True, 'item3_lo': False,
                   'item3_hi': True, 'mix_hi_lo': True, 'mix_lo_hi': False,
                   'null': False, 'pdt_same': TypeError, 'pdt_diff': TypeError,
                   'pdt_diff_fewer': TypeError, 'pdt_diff_more': TypeError,
                   'pdt_empty': TypeError}

LT_EXPECTATIONS = {'no_difference': False, 'item1_lo': True, 'item1_hi': False,
                   'item2_lo': True, 'item2_hi': False, 'item3_lo': True,
                   'item3_hi': False, 'mix_hi_lo': False, 'mix_lo_hi': True,
                   'null': False, 'pdt_same': TypeError, 'pdt_diff': TypeError,
                   'pdt_diff_fewer': TypeError, 'pdt_diff_more': TypeError,
                   'pdt_empty': TypeError}


class Test___eq__(tests.IrisTest, _Test_operator):
    def setUp(self):
        self.op = operator.eq
        self.expected_value = EQ_EXPECTATIONS

    def test_netcdftime_equal(self):
        pdt = PartialDateTime(month=3, microsecond=2)
        other = netcdftime.datetime(year=2013, month=3, day=20, second=2)
        self.assertTrue(pdt == other)

    def test_netcdftime_not_equal(self):
        pdt = PartialDateTime(month=3, microsecond=2)
        other = netcdftime.datetime(year=2013, month=4, day=20, second=2)
        self.assertFalse(pdt == other)


class Test___ne__(tests.IrisTest, _Test_operator):
    def setUp(self):
        self.op = operator.ne
        self.expected_value = negate_expectations(EQ_EXPECTATIONS)


class Test___gt__(tests.IrisTest, _Test_operator):
    def setUp(self):
        self.op = operator.gt
        self.expected_value = GT_EXPECTATIONS

    def test_netcdftime_greater(self):
        pdt = PartialDateTime(month=3, microsecond=2)
        other = netcdftime.datetime(year=2013, month=2, day=20, second=3)
        self.assertTrue(pdt > other)

    def test_netcdftime_not_greater(self):
        pdt = PartialDateTime(month=3, microsecond=2)
        other = netcdftime.datetime(year=2013, month=3, day=20, second=3)
        self.assertFalse(pdt > other)


class Test___le__(tests.IrisTest, _Test_operator):
    def setUp(self):
        self.op = operator.le
        self.expected_value = negate_expectations(GT_EXPECTATIONS)


class Test___lt__(tests.IrisTest, _Test_operator):
    def setUp(self):
        self.op = operator.lt
        self.expected_value = LT_EXPECTATIONS


class Test___ge__(tests.IrisTest, _Test_operator):
    def setUp(self):
        self.op = operator.ge
        self.expected_value = negate_expectations(LT_EXPECTATIONS)


class Test___le__(tests.IrisTest, _Test_operator):
    def setUp(self):
        self.op = operator.le
        self.expected_value = negate_expectations(GT_EXPECTATIONS)


if __name__ == "__main__":
    tests.main()

########NEW FILE########
__FILENAME__ = test_Unit
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Unit tests for the `iris.unit.Unit` class."""


# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np

import iris.unit
from iris.unit import Unit


class Test___init__(tests.IrisTest):

    def test_capitalised_calendar(self):
        calendar = 'GrEgoRian'
        expected = iris.unit.CALENDAR_GREGORIAN
        u = Unit('hours since 1970-01-01 00:00:00', calendar=calendar)
        self.assertEqual(u.calendar, expected)

    def test_not_basestring_calendar(self):
        with self.assertRaises(TypeError):
            u = Unit('hours since 1970-01-01 00:00:00', calendar=5)


class Test_convert(tests.IrisTest):

    class MyStr(str):
        pass

    def test_gregorian_calendar_conversion_setup(self):
        # Reproduces a situation where a unit's gregorian calendar would not
        # match (using the `is` operator) to the literal string 'gregorian',
        # causing an `is not` test to return a false negative.
        cal_str = iris.unit.CALENDAR_GREGORIAN
        calendar = self.MyStr(cal_str)
        self.assertIsNot(calendar, cal_str)
        u1 = Unit('hours since 1970-01-01 00:00:00', calendar=calendar)
        u2 = Unit('hours since 1969-11-30 00:00:00', calendar=calendar)
        u1point = np.array([8.], dtype=np.float32)
        expected = np.array([776.], dtype=np.float32)
        result = u1.convert(u1point, u2)
        return expected, result

    def test_gregorian_calendar_conversion_array(self):
        expected, result = self.test_gregorian_calendar_conversion_setup()
        self.assertArrayEqual(expected, result)

    def test_gregorian_calendar_conversion_dtype(self):
        expected, result = self.test_gregorian_calendar_conversion_setup()
        self.assertEqual(expected.dtype, result.dtype)

    def test_gregorian_calendar_conversion_shape(self):
        expected, result = self.test_gregorian_calendar_conversion_setup()
        self.assertEqual(expected.shape, result.shape)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_array_equal
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Test function :func:`iris.util.array_equal`."""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np
import numpy.ma as ma

from iris.util import array_equal


class Test(tests.IrisTest):
    def test_0d(self):
        array_a = np.array(23)
        array_b = np.array(23)
        array_c = np.array(7)
        self.assertTrue(array_equal(array_a, array_b))
        self.assertFalse(array_equal(array_a, array_c))

    def test_0d_and_scalar(self):
        array_a = np.array(23)
        self.assertTrue(array_equal(array_a, 23))
        self.assertFalse(array_equal(array_a, 45))

    def test_1d_and_sequences(self):
        for sequence_type in (list, tuple):
            seq_a = sequence_type([1, 2, 3])
            array_a = np.array(seq_a)
            self.assertTrue(array_equal(array_a, seq_a))
            self.assertFalse(array_equal(array_a, seq_a[:-1]))
            array_a[1] = 45
            self.assertFalse(array_equal(array_a, seq_a))

    def test_nd(self):
        array_a = np.array(np.arange(24).reshape(2, 3, 4))
        array_b = np.array(np.arange(24).reshape(2, 3, 4))
        array_c = np.array(np.arange(24).reshape(2, 3, 4))
        array_c[0, 1, 2] = 100
        self.assertTrue(array_equal(array_a, array_b))
        self.assertFalse(array_equal(array_a, array_c))

    def test_masked_is_ignored(self):
        array_a = np.ma.masked_array([1, 2, 3], mask=[1, 0, 1])
        array_b = np.ma.masked_array([2, 2, 2], mask=[1, 0, 1])
        self.assertFalse(array_equal(array_a, array_b))

    def test_fully_masked_arrays(self):
        array_a = np.ma.masked_array(np.arange(24).reshape(2, 3, 4), mask=True)
        array_b = np.ma.masked_array(np.arange(24).reshape(2, 3, 4), mask=True)
        self.assertTrue(array_equal(array_a, array_b))

    def test_fully_masked_0d_arrays(self):
        array_a = np.ma.masked_array(3, mask=True)
        array_b = np.ma.masked_array(3, mask=True)
        self.assertTrue(array_equal(array_a, array_b))

    def test_fully_masked_string_arrays(self):
        array_a = np.ma.masked_array(['a', 'b', 'c'], mask=True)
        array_b = np.ma.masked_array(['a', 'b', 'c'], mask=[1, 1, 1])
        self.assertTrue(array_equal(array_a, array_b))

    def test_partially_masked_string_arrays(self):
        array_a = np.ma.masked_array(['a', 'b', 'c'], mask=[1, 0, 1])
        array_b = np.ma.masked_array(['a', 'b', 'c'], mask=[1, 0, 1])
        self.assertTrue(array_equal(array_a, array_b))

    def test_string_arrays_equal(self):
        array_a = np.array(['abc', 'def', 'efg'])
        array_b = np.array(['abc', 'def', 'efg'])
        self.assertTrue(array_equal(array_a, array_b))

    def test_string_arrays_different_contents(self):
        array_a = np.array(['abc', 'def', 'efg'])
        array_b = np.array(['abc', 'de', 'efg'])
        self.assertFalse(array_equal(array_a, array_b))

    def test_string_arrays_subset(self):
        array_a = np.array(['abc', 'def', 'efg'])
        array_b = np.array(['abc', 'def'])
        self.assertFalse(array_equal(array_a, array_b))
        self.assertFalse(array_equal(array_b, array_a))

    def test_string_arrays_unequal_dimensionality(self):
        array_a = np.array('abc')
        array_b = np.array(['abc'])
        array_c = np.array([['abc']])
        self.assertFalse(array_equal(array_a, array_b))
        self.assertFalse(array_equal(array_b, array_a))
        self.assertFalse(array_equal(array_a, array_c))
        self.assertFalse(array_equal(array_b, array_c))

    def test_string_arrays_0d_and_scalar(self):
        array_a = np.array('foobar')
        self.assertTrue(array_equal(array_a, 'foobar'))
        self.assertFalse(array_equal(array_a, 'foo'))
        self.assertFalse(array_equal(array_a, 'foobar.'))


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_broadcast_to_shape
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Test function :func:`iris.util.broadcast_to_shape`."""


# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np
import numpy.ma as ma

from iris.util import broadcast_to_shape


class Test_broadcast_to_shape(tests.IrisTest):

    def test_same_shape(self):
        # broadcast to current shape should result in no change
        a = np.random.random([2, 3])
        b = broadcast_to_shape(a, a.shape, (0, 1))
        self.assertArrayEqual(b, a)

    def test_added_dimensions(self):
        # adding two dimensions, on at the front and one in the middle of
        # the existing dimensions
        a = np.random.random([2, 3])
        b = broadcast_to_shape(a, (5, 2, 4, 3), (1, 3))
        for i in xrange(5):
            for j in xrange(4):
                self.assertArrayEqual(b[i, :, j, :], a)

    def test_added_dimensions_transpose(self):
        # adding dimensions and having the dimensions of the input
        # transposed
        a = np.random.random([2, 3])
        b = broadcast_to_shape(a, (5, 3, 4, 2), (3, 1))
        for i in xrange(5):
            for j in xrange(4):
                self.assertArrayEqual(b[i, :, j, :].T, a)

    def test_masked(self):
        # masked arrays are also accepted
        a = np.random.random([2, 3])
        m = ma.array(a, mask=[[0, 1, 0], [0, 1, 1]])
        b = broadcast_to_shape(m, (5, 3, 4, 2), (3, 1))
        for i in xrange(5):
            for j in xrange(4):
                self.assertMaskedArrayEqual(b[i, :, j, :].T, m)

    def test_masked_degenerate(self):
        # masked arrays can have degenerate masks too
        a = np.random.random([2, 3])
        m = ma.array(a)
        b = broadcast_to_shape(m, (5, 3, 4, 2), (3, 1))
        for i in xrange(5):
            for j in xrange(4):
                self.assertMaskedArrayEqual(b[i, :, j, :].T, m)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_describe_diff
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Test function :func:`iris.util.describe_diff`."""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np
import StringIO

import iris.cube
from iris.util import describe_diff


class Test(iris.tests.IrisTest):
    def setUp(self):
        self.cube_a = iris.cube.Cube([])
        self.cube_b = self.cube_a.copy()

    def _compare_result(self, cube_a, cube_b):
        result_strIO = StringIO.StringIO()
        describe_diff(cube_a, cube_b, output_file=result_strIO)
        return result_strIO.getvalue()

    def test_noncommon_array_attributes(self):
        # test non-common array attribute
        self.cube_a.attributes['test_array'] = np.array([1, 2, 3])
        return_str = self._compare_result(self.cube_a, self.cube_b)
        self.assertString(return_str, ['compatible_cubes.str.txt'])

    def test_same_array_attributes(self):
        # test matching array attribute
        self.cube_a.attributes['test_array'] = np.array([1, 2, 3])
        self.cube_b.attributes['test_array'] = np.array([1, 2, 3])
        return_str = self._compare_result(self.cube_a, self.cube_b)
        self.assertString(return_str, ['compatible_cubes.str.txt'])

    def test_different_array_attributes(self):
        # test non-matching array attribute
        self.cube_a.attributes['test_array'] = np.array([1, 2, 3])
        self.cube_b.attributes['test_array'] = np.array([1, 7, 3])
        return_str = self._compare_result(self.cube_a, self.cube_b)
        self.assertString(
            return_str,
            ['unit', 'util', 'describe_diff',
             'incompatible_array_attrs.str.txt'])


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_file_is_newer_than
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Test function :func:`iris.util.test_file_is_newer`.

"""
# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import os
import os.path
import shutil
import tempfile

from iris.util import file_is_newer_than


class TestFileIsNewer(tests.IrisTest):
    """Test the :func:`iris.util.file_is_newer_than` function."""

    def _name2path(self, filename):
        """Add the temporary dirpath to a filename to make a full path."""
        return os.path.join(self.temp_dir, filename)

    def setUp(self):
        # make a temporary directory with testfiles of known timestamp order.
        self.temp_dir = tempfile.mkdtemp('_testfiles_tempdir')
        # define the names of some files to create
        create_file_names = ['older_source_1', 'older_source_2',
                             'example_result',
                             'newer_source_1', 'newer_source_2']
        # create testfiles + ensure distinct 'mtime's in the required order.
        for i_file, file_name in enumerate(create_file_names):
            file_path = self._name2path(file_name)
            with open(file_path, 'w') as test_file:
                test_file.write('..content..')
            # Ensure 'mtime's are adequately separated and after create times.
            mtime = os.stat(file_path).st_mtime
            mtime += 5.0 + 10.0 * i_file
            os.utime(file_path, (mtime, mtime))

    def tearDown(self):
        # destroy whole contents of temporary directory
        shutil.rmtree(self.temp_dir)

    def _test(self, boolean_result, result_name, source_names):
        """Test expected result of executing with given args."""
        # Make args into full paths
        result_path = self._name2path(result_name)
        if isinstance(source_names, basestring):
            source_paths = self._name2path(source_names)
        else:
            source_paths = [self._name2path(name)
                            for name in source_names]
        # Check result is as expected.
        self.assertEqual(
            boolean_result,
            file_is_newer_than(result_path, source_paths))

    def test_no_sources(self):
        self._test(True, 'example_result', [])

    def test_string_ok(self):
        self._test(True, 'example_result', 'older_source_1')

    def test_string_fail(self):
        self._test(False, 'example_result', 'newer_source_1')

    def test_self_result(self):
        # This fails, because same-timestamp is *not* acceptable.
        self._test(False, 'example_result', 'example_result')

    def test_single_ok(self):
        self._test(True, 'example_result', ['older_source_2'])

    def test_single_fail(self):
        self._test(False, 'example_result', ['newer_source_2'])

    def test_multiple_ok(self):
        self._test(True, 'example_result', ['older_source_1',
                                            'older_source_2'])

    def test_multiple_fail(self):
        self._test(False, 'example_result', ['older_source_1',
                                             'older_source_2',
                                             'newer_source_1'])

    def test_wild_ok(self):
        self._test(True, 'example_result', ['older_sour*_*'])

    def test_wild_fail(self):
        self._test(False, 'example_result', ['older_sour*', 'newer_sour*'])

    def test_error_missing_result(self):
        with self.assertRaises(OSError) as error_trap:
            self._test(False, 'non_exist', ['older_sour*'])
        error = error_trap.exception
        self.assertEqual(error.strerror, 'No such file or directory')
        self.assertEqual(error.filename, self._name2path('non_exist'))

    def test_error_missing_source(self):
        with self.assertRaises(IOError) as error_trap:
            self._test(False, 'example_result', ['older_sour*', 'non_exist'])
        self.assertTrue(error_trap.exception.message.startswith(
            'One or more of the files specified did not exist'))

    def test_error_missing_wild(self):
        with self.assertRaises(IOError) as error_trap:
            self._test(False, 'example_result', ['older_sour*', 'unknown_*'])
        self.assertTrue(error_trap.exception.message.startswith(
            'One or more of the files specified did not exist'))


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_new_axis
# (C) British Crown Copyright 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Test function :func:`iris.util.new_axis`."""

# Import iris.tests first so that some things can be initialised before
# importing anything else.
import iris.tests as tests

import copy
import numpy as np
import unittest

import iris
from iris.util import new_axis


class Test(tests.IrisTest):
    def setUp(self):
        self.data = np.array([[1, 2], [1, 2]])
        self.cube = iris.cube.Cube(self.data)
        lat = iris.coords.DimCoord([1, 2], standard_name='latitude')
        lon = iris.coords.DimCoord([1, 2], standard_name='longitude')

        time = iris.coords.DimCoord([1], standard_name='time')
        wibble = iris.coords.AuxCoord([1], long_name='wibble')

        self.cube.add_dim_coord(lat, 0)
        self.cube.add_dim_coord(lon, 1)
        self.cube.add_aux_coord(time, None)
        self.cube.add_aux_coord(wibble, None)

        self.coords = {'lat': lat, 'lon': lon, 'time': time, 'wibble': wibble}

    def _assert_cube_notis(self, cube_a, cube_b):
        for coord_a, coord_b in zip(cube_a.coords(), cube_b.coords()):
            self.assertIsNot(coord_a, coord_b)

        self.assertIsNot(cube_a.metadata, cube_b.metadata)

        for factory_a, factory_b in zip(
                cube_a.aux_factories, cube_b.aux_factories):
            self.assertIsNot(factory_a, factory_b)

    def test_no_coord(self):
        # Providing no coordinate to promote.
        res = new_axis(self.cube)
        com = iris.cube.Cube(self.data[None])
        com.add_dim_coord(self.coords['lat'].copy(), 1)
        com.add_dim_coord(self.coords['lon'].copy(), 2)
        com.add_aux_coord(self.coords['time'].copy(), None)
        com.add_aux_coord(self.coords['wibble'].copy(), None)

        self.assertEqual(res, com)
        self._assert_cube_notis(res, self.cube)

    def test_scalar_dimcoord(self):
        # Providing a scalar coordinate to promote.
        res = new_axis(self.cube, 'time')
        com = iris.cube.Cube(self.data[None])
        com.add_dim_coord(self.coords['lat'].copy(), 1)
        com.add_dim_coord(self.coords['lon'].copy(), 2)
        com.add_aux_coord(self.coords['time'].copy(), 0)
        com.add_aux_coord(self.coords['wibble'].copy(), None)

        self.assertEqual(res, com)
        self._assert_cube_notis(res, self.cube)

    def test_scalar_auxcoord(self):
        # Providing a scalar coordinate to promote.
        res = new_axis(self.cube, 'wibble')
        com = iris.cube.Cube(self.data[None])
        com.add_dim_coord(self.coords['lat'].copy(), 1)
        com.add_dim_coord(self.coords['lon'].copy(), 2)
        com.add_aux_coord(self.coords['time'].copy(), None)
        com.add_aux_coord(self.coords['wibble'].copy(), 0)

        self.assertEqual(res, com)
        self._assert_cube_notis(res, self.cube)

    def test_maint_factory(self):
        # Ensure that aux factory persists.
        data = np.arange(12, dtype='i8').reshape((3, 4))

        orography = iris.coords.AuxCoord(
            [10, 25, 50, 5], standard_name='surface_altitude', units='m')

        model_level = iris.coords.AuxCoord(
            [2, 1, 0], standard_name='model_level_number')

        level_height = iris.coords.DimCoord(
            [100, 50, 10], long_name='level_height', units='m',
            attributes={'positive': 'up'},
            bounds=[[150, 75], [75, 20], [20, 0]])

        sigma = iris.coords.AuxCoord(
            [0.8, 0.9, 0.95], long_name='sigma',
            bounds=[[0.7, 0.85], [0.85, 0.97], [0.97, 1.0]])

        hybrid_height = iris.aux_factory.HybridHeightFactory(
            level_height, sigma, orography)

        cube = iris.cube.Cube(
            data, standard_name='air_temperature', units='K',
            dim_coords_and_dims=[(level_height, 0)],
            aux_coords_and_dims=[(orography, 1), (model_level, 0), (sigma, 0)],
            aux_factories=[hybrid_height])

        com = iris.cube.Cube(
            data[None], standard_name='air_temperature', units='K',
            dim_coords_and_dims=[(copy.copy(level_height), 1)],
            aux_coords_and_dims=[(copy.copy(orography), 2),
                                 (copy.copy(model_level), 1),
                                 (copy.copy(sigma), 1)],
            aux_factories=[copy.copy(hybrid_height)])
        res = new_axis(cube)

        self.assertEqual(res, com)
        self._assert_cube_notis(res, cube)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_rolling_window
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Test function :func:`iris.util.rolling_window`."""


# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import numpy as np
import numpy.ma as ma

from iris.util import rolling_window


class Test_rolling_window(tests.IrisTest):

    def test_1d(self):
        # 1-d array input
        a = np.array([0, 1, 2, 3, 4], dtype=np.int32)
        expected_result = np.array([[0, 1], [1, 2], [2, 3], [3, 4]],
                                   dtype=np.int32)
        result = rolling_window(a, window=2)
        self.assertArrayEqual(result, expected_result)

    def test_2d(self):
        # 2-d array input
        a = np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]], dtype=np.int32)
        expected_result = np.array([[[0, 1, 2], [1, 2, 3], [2, 3, 4]],
                                    [[5, 6, 7], [6, 7, 8], [7, 8, 9]]],
                                   dtype=np.int32)
        result = rolling_window(a, window=3, axis=1)
        self.assertArrayEqual(result, expected_result)

    def test_1d_masked(self):
        # 1-d masked array input
        a = ma.array([0, 1, 2, 3, 4], mask=[0, 0, 1, 0, 0],
                     dtype=np.int32)
        expected_result = ma.array([[0, 1], [1, 2], [2, 3], [3, 4]],
                                   mask=[[0, 0], [0, 1], [1, 0], [0, 0]],
                                   dtype=np.int32)
        result = rolling_window(a, window=2)
        self.assertMaskedArrayEqual(result, expected_result)

    def test_2d_masked(self):
        # 2-d masked array input
        a = ma.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]],
                     mask=[[0, 0, 1, 0, 0], [1, 0, 1, 0, 0]],
                     dtype=np.int32)
        expected_result = ma.array([[[0, 1, 2], [1, 2, 3], [2, 3, 4]],
                                    [[5, 6, 7], [6, 7, 8], [7, 8, 9]]],
                                   mask=[[[0, 0, 1], [0, 1, 0], [1, 0, 0]],
                                         [[1, 0, 1], [0, 1, 0], [1, 0, 0]]],
                                   dtype=np.int32)
        result = rolling_window(a, window=3, axis=1)
        self.assertMaskedArrayEqual(result, expected_result)

    def test_degenerate_mask(self):
        a = ma.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]], dtype=np.int32)
        expected_result = ma.array([[[0, 1, 2], [1, 2, 3], [2, 3, 4]],
                                    [[5, 6, 7], [6, 7, 8], [7, 8, 9]]],
                                   mask=[[[0, 0, 0], [0, 0, 0], [0, 0, 0]],
                                         [[0, 0, 0], [0, 0, 0], [0, 0, 0]]],
                                   dtype=np.int32)
        result = rolling_window(a, window=3, axis=1)
        self.assertMaskedArrayEqual(result, expected_result)

    def test_step(self):
        # step should control how far apart consecutive windows are
        a = np.array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]], dtype=np.int32)
        expected_result = np.array([[[0, 1, 2], [2, 3, 4]],
                                    [[5, 6, 7], [7, 8, 9]]],
                                   dtype=np.int32)
        result = rolling_window(a, window=3, step=2, axis=1)
        self.assertArrayEqual(result, expected_result)

    def test_window_too_short(self):
        # raise an error if the window length is less than 1
        a = np.empty([5])
        with self.assertRaises(ValueError):
            rolling_window(a, window=0)

    def test_window_too_long(self):
        # raise an error if the window length is longer than the
        # corresponding array dimension
        a = np.empty([7, 5])
        with self.assertRaises(ValueError):
            rolling_window(a, window=6, axis=1)

    def test_invalid_step(self):
        # raise an error if the step between windows is less than 1
        a = np.empty([5])
        with self.assertRaises(ValueError):
            rolling_window(a, step=0)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = test_unify_time_units
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""Test function :func:`iris.util.array_equal`."""

# import iris tests first so that some things can be initialised before
# importing anything else
import iris.tests as tests

import copy

import numpy as np

import iris
import iris.tests.stock as stock
from iris.util import unify_time_units


class Test(tests.IrisTest):
    def simple_1d_time_cubes(self, calendar='gregorian'):
        coord_points = [1, 2, 3, 4, 5]
        data_points = [273, 275, 278, 277, 274]
        reftimes = ['hours since 1970-01-01 00:00:00',
                    'hours since 1970-01-02 00:00:00']
        list_of_cubes = []
        for reftime in reftimes:
            cube = iris.cube.Cube(np.array(data_points, dtype=np.float32),
                                  standard_name='air_temperature',
                                  units='K')
            unit = iris.unit.Unit(reftime, calendar=calendar)
            coord = iris.coords.DimCoord(points=np.array(coord_points,
                                                         dtype=np.float32),
                                         standard_name='time',
                                         units=unit)
            cube.add_dim_coord(coord, 0)
            list_of_cubes.append(cube)
        return list_of_cubes

    def _common(self, expected, result, coord_name='time'):
        # This tests time-like coords only.
        for cube in result:
            try:
                epoch = cube.coord(coord_name).units.origin
            except iris.exceptions.CoordinateNotFoundError:
                pass
            else:
                self.assertEqual(expected, epoch)

    def test_cubelist_with_time_coords(self):
        # Tests an :class:`iris.cube.CubeList` containing cubes with time
        # coords against a time string and a time coord.
        cubelist = iris.cube.CubeList(self.simple_1d_time_cubes())
        expected = 'hours since 1970-01-01 00:00:00'
        unify_time_units(cubelist)
        self._common(expected, cubelist)

    def test_list_of_cubes_with_time_coords(self):
        # Tests an iterable containing cubes with time coords against a time
        # string and a time coord.
        list_of_cubes = self.simple_1d_time_cubes()
        expected = 'hours since 1970-01-01 00:00:00'
        unify_time_units(list_of_cubes)
        self._common(expected, list_of_cubes)

    @tests.skip_data
    def test_no_time_coord_in_cubes(self):
        path0 = tests.get_data_path(('PP', 'aPPglob1', 'global.pp'))
        path1 = tests.get_data_path(('PP', 'aPPglob1', 'global_t_forecast.pp'))
        cube0 = iris.load_cube(path0)
        cube1 = iris.load_cube(path1)
        cubes = iris.cube.CubeList([cube0, cube1])
        result = copy.copy(cubes)
        unify_time_units(result)
        self.assertEqual(cubes, result)

    def test_time_coord_only_in_some_cubes(self):
        list_of_cubes = self.simple_1d_time_cubes()
        cube = stock.simple_2d()
        list_of_cubes.append(cube)
        expected = 'hours since 1970-01-01 00:00:00'
        unify_time_units(list_of_cubes)
        self._common(expected, list_of_cubes)

    def test_multiple_time_coords_in_cube(self):
        cube0, cube1 = self.simple_1d_time_cubes()
        units = iris.unit.Unit('days since 1980-05-02 00:00:00',
                               calendar='gregorian')
        aux_coord = iris.coords.AuxCoord(
            72, standard_name='forecast_reference_time', units=units)
        cube1.add_aux_coord(aux_coord)
        cubelist = iris.cube.CubeList([cube0, cube1])
        expected = 'hours since 1970-01-01 00:00:00'
        unify_time_units(cubelist)
        self._common(expected, cubelist)
        self._common(expected, cubelist, coord_name='forecast_reference_time')

    def test_multiple_calendars(self):
        cube0, cube1 = self.simple_1d_time_cubes()
        cube2, cube3 = self.simple_1d_time_cubes(calendar='360_day')
        cubelist = iris.cube.CubeList([cube0, cube1, cube2, cube3])
        expected = 'hours since 1970-01-01 00:00:00'
        unify_time_units(cubelist)
        self._common(expected, cubelist)


if __name__ == '__main__':
    tests.main()

########NEW FILE########
__FILENAME__ = time
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Time handling.

"""
import collections
import datetime
import functools
import operator

import netcdftime


@functools.total_ordering
class PartialDateTime(object):
    """
    A :class:`PartialDateTime` object specifies values for some subset of
    the calendar/time fields (year, month, hour, etc.) for comparing
    with :class:`datetime.datetime`-like instances.

    Comparisons are defined against any other class with all of the
    attributes: year, month, day, hour, minute, and second.
    Notably, this includes :class:`datetime.datetime` and
    :class:`netcdftime.datetime`. Comparison also extends to the
    microsecond attribute for classes, such as
    :class:`datetime.datetime`, which define it.

    A :class:`PartialDateTime` object is not limited to any particular
    calendar, so no restriction is placed on the range of values
    allowed in its component fields. Thus, it is perfectly legitimate to
    create an instance as: `PartialDateTime(month=2, day=30)`.

    """

    __slots__ = ('year', 'month', 'day', 'hour', 'minute', 'second',
                 'microsecond')

    #: A dummy value provided as a workaround to allow comparisons with
    #: :class:`datetime.datetime`.
    #: See http://bugs.python.org/issue8005.
    # NB. It doesn't even matter what this value is.
    timetuple = None

    def __init__(self, year=None, month=None, day=None, hour=None,
                 minute=None, second=None, microsecond=None):
        """
        Allows partial comparisons against datetime-like objects.

        Args:

        * year (int):
        * month (int):
        * day (int):
        * hour (int):
        * minute (int):
        * second (int):
        * microsecond (int):

        For example, to select any days of the year after the 3rd of April:

        >>> from iris.time import PartialDateTime
        >>> import datetime
        >>> pdt = PartialDateTime(month=4, day=3)
        >>> datetime.datetime(2014, 4, 1) > pdt
        False
        >>> datetime.datetime(2014, 4, 5) > pdt
        True
        >>> datetime.datetime(2014, 5, 1) > pdt
        True
        >>> datetime.datetime(2015, 2, 1) > pdt
        False

        """

        #: The year number as an integer, or None.
        self.year = year
        #: The month number as an integer, or None.
        self.month = month
        #: The day number as an integer, or None.
        self.day = day
        #: The hour number as an integer, or None.
        self.hour = hour
        #: The minute number as an integer, or None.
        self.minute = minute
        #: The second number as an integer, or None.
        self.second = second
        #: The microsecond number as an integer, or None.
        self.microsecond = microsecond

    def __repr__(self):
        attr_pieces = ['{}={}'.format(name, getattr(self, name))
                       for name in self.__slots__
                       if getattr(self, name) is not None]
        result = '{}({})'.format(type(self).__name__, ', '.join(attr_pieces))
        return result

    def __gt__(self, other):
        if isinstance(other, type(self)):
            raise TypeError('Cannot order PartialDateTime instances.')
        result = False
        try:
            # Everything except 'microsecond' is mandatory
            for attr_name in self.__slots__[:-1]:
                attr = getattr(self, attr_name)
                other_attr = getattr(other, attr_name)
                if attr is not None and attr != other_attr:
                    result = attr > other_attr
                    break
            # 'microsecond' is optional
            if result and hasattr(other, 'microsecond'):
                attr = self.microsecond
                other_attr = other.microsecond
                if attr is not None and attr != other_attr:
                    result = attr > other_attr
        except AttributeError:
            result = NotImplemented
        return result

    def __eq__(self, other):
        if isinstance(other, type(self)):
            slots = self.__slots__
            self_tuple = tuple(getattr(self, name) for name in slots)
            other_tuple = tuple(getattr(other, name) for name in slots)
            result = self_tuple == other_tuple
        else:
            result = True
            try:
                # Everything except 'microsecond' is mandatory
                for attr_name in self.__slots__[:-1]:
                    attr = getattr(self, attr_name)
                    other_attr = getattr(other, attr_name)
                    if attr is not None and attr != other_attr:
                        result = False
                        break
                # 'microsecond' is optional
                if result and hasattr(other, 'microsecond'):
                    attr = self.microsecond
                    other_attr = other.microsecond
                    if attr is not None and attr != other_attr:
                        result = False
            except AttributeError:
                result = NotImplemented
        return result

    def __ne__(self, other):
        result = self.__eq__(other)
        if result is not NotImplemented:
            result = not result
        return result

    def __cmp__(self, other):
        # Since we've defined all the rich comparison operators (via
        # functools.total_ordering), we can only reach this point if
        # neither this class nor the other class had a rich comparison
        # that could handle the type combination.
        # We don't want Python to fall back to the default `object`
        # behaviour (which compares using object IDs), so we raise an
        # exception here instead.
        fmt = 'unable to compare PartialDateTime with {}'
        raise TypeError(fmt.format(type(other)))

########NEW FILE########
__FILENAME__ = unit
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Units of measure.

Provision of a wrapper class to support Unidata/UCAR UDUNITS-2, and the
netcdftime calendar functionality.

See also: `UDUNITS-2
<http://www.unidata.ucar.edu/software/udunits/udunits-2/udunits2.html>`_.

"""

from __future__ import division

import copy
import ctypes
import ctypes.util
import os.path
import sys
import warnings

import netcdftime
import numpy as np

import iris.config
import iris.util


__all__ = ['Unit', 'date2num', 'decode_time', 'encode_clock', 'encode_date',
           'encode_time', 'num2date']


########################################################################
#
# module level constants
#
########################################################################

#
# default constants
#
IRIS_EPOCH = '1970-01-01 00:00:00'
_STRING_BUFFER_DEPTH = 128
_UNKNOWN_UNIT_STRING = 'unknown'
_UNKNOWN_UNIT_SYMBOL = '?'
_UNKNOWN_UNIT = [_UNKNOWN_UNIT_STRING, _UNKNOWN_UNIT_SYMBOL, '???', '']
_NO_UNIT_STRING = 'no_unit'
_NO_UNIT_SYMBOL = '-'
_NO_UNIT = [_NO_UNIT_STRING, _NO_UNIT_SYMBOL, 'no unit', 'no-unit', 'nounit']
_UNIT_DIMENSIONLESS = '1'
_OP_SINCE = ' since '
_CATEGORY_UNKNOWN, _CATEGORY_NO_UNIT, _CATEGORY_UDUNIT = range(3)


#
# libudunits2 constants
#
# ut_status enumerations
_UT_STATUS = ['UT_SUCCESS', 'UT_BAD_ARG', 'UT_EXISTS', 'UT_NO_UNIT',
              'UT_OS', 'UT_NOT_SAME_NAME', 'UT_MEANINGLESS', 'UT_NO_SECOND',
              'UT_VISIT_ERROR', 'UT_CANT_FORMAT', 'UT_SYNTAX', 'UT_UNKNOWN',
              'UT_OPEN_ARG', 'UT_OPEN_ENV', 'UT_OPEN_DEFAULT', 'UT_PARSE']

# explicit function names
_UT_HANDLER = 'ut_set_error_message_handler'
_UT_IGNORE = 'ut_ignore'

# ut_encoding enumerations
UT_ASCII = 0
UT_ISO_8859_1 = 1
UT_LATIN1 = 1
UT_UTF8 = 2
UT_NAMES = 4
UT_DEFINITION = 8

UT_FORMATS = [UT_ASCII, UT_ISO_8859_1, UT_LATIN1, UT_UTF8, UT_NAMES,
              UT_DEFINITION]

#
# netcdftime constants
#
CALENDAR_STANDARD = 'standard'
CALENDAR_GREGORIAN = 'gregorian'
CALENDAR_PROLEPTIC_GREGORIAN = 'proleptic_gregorian'
CALENDAR_NO_LEAP = 'noleap'
CALENDAR_JULIAN = 'julian'
CALENDAR_ALL_LEAP = 'all_leap'
CALENDAR_365_DAY = '365_day'
CALENDAR_366_DAY = '366_day'
CALENDAR_360_DAY = '360_day'

CALENDARS = [CALENDAR_STANDARD, CALENDAR_GREGORIAN,
             CALENDAR_PROLEPTIC_GREGORIAN, CALENDAR_NO_LEAP, CALENDAR_JULIAN,
             CALENDAR_ALL_LEAP, CALENDAR_365_DAY, CALENDAR_366_DAY,
             CALENDAR_360_DAY]

#
# ctypes types
#
FLOAT32 = ctypes.c_float
FLOAT64 = ctypes.c_double

########################################################################
#
# module level variables
#
########################################################################

# cache for ctypes foreign shared library handles
_lib_c = None
_lib_ud = None
_ud_system = None

# cache for libc shared library functions
_strerror = None

# class cache for libudunits2 shared library functions
_cv_convert_float = None
_cv_convert_floats = None
_cv_convert_double = None
_cv_convert_doubles = None
_cv_free = None
_ut_are_convertible = None
_ut_clone = None
_ut_compare = None
_ut_decode_time = None
_ut_divide = None
_ut_encode_clock = None
_ut_encode_date = None
_ut_encode_time = None
_ut_format = None
_ut_free = None
_ut_get_converter = None
_ut_get_status = None
_ut_get_unit_by_name = None
_ut_ignore = None
_ut_invert = None
_ut_is_dimensionless = None
_ut_log = None
_ut_multiply = None
_ut_offset = None
_ut_offset_by_time = None
_ut_parse = None
_ut_raise = None
_ut_read_xml = None
_ut_root = None
_ut_scale = None
_ut_set_error_message_handler = None

########################################################################
#
# module level statements
#
########################################################################

#
# load the libc shared library
#
if _lib_c is None:
    _lib_c = ctypes.CDLL(ctypes.util.find_library('libc'))

    #
    # cache common shared library functions
    #
    _strerror = _lib_c.strerror
    _strerror.restype = ctypes.c_char_p

#
# load the libudunits2 shared library
#
if _lib_ud is None:
    _lib_ud = iris.config.get_option(
        'System', 'udunits2_path',
        default=ctypes.util.find_library('udunits2'))
    _lib_ud = ctypes.CDLL(_lib_ud, use_errno=True)

    #
    # cache common shared library functions
    #
    _cv_convert_float = _lib_ud.cv_convert_float
    _cv_convert_float.argtypes = [ctypes.c_void_p, ctypes.c_float]
    _cv_convert_float.restype = ctypes.c_float

    _cv_convert_floats = _lib_ud.cv_convert_floats
    _cv_convert_floats.argtypes = [ctypes.c_void_p, ctypes.c_void_p,
                                   ctypes.c_ulong, ctypes.c_void_p]
    _cv_convert_floats.restype = ctypes.c_void_p

    _cv_convert_double = _lib_ud.cv_convert_double
    _cv_convert_double.argtypes = [ctypes.c_void_p, ctypes.c_double]
    _cv_convert_double.restype = ctypes.c_double

    _cv_convert_doubles = _lib_ud.cv_convert_doubles
    _cv_convert_doubles.argtypes = [ctypes.c_void_p, ctypes.c_void_p,
                                    ctypes.c_ulong, ctypes.c_void_p]
    _cv_convert_doubles.restype = ctypes.c_void_p

    _cv_free = _lib_ud.cv_free
    _cv_free.argtypes = [ctypes.c_void_p]

    _ut_are_convertible = _lib_ud.ut_are_convertible
    _ut_are_convertible.argtypes = [ctypes.c_void_p, ctypes.c_void_p]

    _ut_clone = _lib_ud.ut_clone
    _ut_clone.argtypes = [ctypes.c_void_p]
    _ut_clone.restype = ctypes.c_void_p

    _ut_compare = _lib_ud.ut_compare
    _ut_compare.argtypes = [ctypes.c_void_p, ctypes.c_void_p]
    _ut_compare.restype = ctypes.c_int

    _ut_decode_time = _lib_ud.ut_decode_time
    _ut_decode_time.restype = None

    _ut_divide = _lib_ud.ut_divide
    _ut_divide.argtypes = [ctypes.c_void_p, ctypes.c_void_p]
    _ut_divide.restype = ctypes.c_void_p

    _ut_encode_clock = _lib_ud.ut_encode_clock
    _ut_encode_clock.restype = ctypes.c_double

    _ut_encode_date = _lib_ud.ut_encode_date
    _ut_encode_date.restype = ctypes.c_double

    _ut_encode_time = _lib_ud.ut_encode_time
    _ut_encode_time.restype = ctypes.c_double

    _ut_format = _lib_ud.ut_format
    _ut_format.argtypes = [ctypes.c_void_p, ctypes.c_char_p,
                           ctypes.c_ulong, ctypes.c_uint]

    _ut_free = _lib_ud.ut_free
    _ut_free.argtypes = [ctypes.c_void_p]
    _ut_free.restype = None

    _ut_get_converter = _lib_ud.ut_get_converter
    _ut_get_converter.argtypes = [ctypes.c_void_p, ctypes.c_void_p]
    _ut_get_converter.restype = ctypes.c_void_p

    _ut_get_status = _lib_ud.ut_get_status

    _ut_get_unit_by_name = _lib_ud.ut_get_unit_by_name
    _ut_get_unit_by_name.argtypes = [ctypes.c_void_p, ctypes.c_char_p]
    _ut_get_unit_by_name.restype = ctypes.c_void_p

    _ut_invert = _lib_ud.ut_invert
    _ut_invert.argtypes = [ctypes.c_void_p]
    _ut_invert.restype = ctypes.c_void_p

    _ut_is_dimensionless = _lib_ud.ut_is_dimensionless
    _ut_is_dimensionless.argtypes = [ctypes.c_void_p]

    _ut_log = _lib_ud.ut_log
    _ut_log.argtypes = [ctypes.c_double, ctypes.c_void_p]
    _ut_log.restype = ctypes.c_void_p

    _ut_multiply = _lib_ud.ut_multiply
    _ut_multiply.argtypes = [ctypes.c_void_p, ctypes.c_void_p]
    _ut_multiply.restype = ctypes.c_void_p

    _ut_offset = _lib_ud.ut_offset
    _ut_offset.argtypes = [ctypes.c_void_p, ctypes.c_double]
    _ut_offset.restype = ctypes.c_void_p

    _ut_offset_by_time = _lib_ud.ut_offset_by_time
    _ut_offset_by_time.argtypes = [ctypes.c_void_p, ctypes.c_double]
    _ut_offset_by_time.restype = ctypes.c_void_p

    _ut_parse = _lib_ud.ut_parse
    _ut_parse.argtypes = [ctypes.c_void_p, ctypes.c_char_p, ctypes.c_int]
    _ut_parse.restype = ctypes.c_void_p

    _ut_raise = _lib_ud.ut_raise
    _ut_raise.argtypes = [ctypes.c_void_p, ctypes.c_int]
    _ut_raise.restype = ctypes.c_void_p

    _ut_read_xml = _lib_ud.ut_read_xml
    _ut_read_xml.argtypes = [ctypes.c_char_p]
    _ut_read_xml.restype = ctypes.c_void_p

    _ut_root = _lib_ud.ut_root
    _ut_root.argtypes = [ctypes.c_void_p, ctypes.c_int]
    _ut_root.restype = ctypes.c_void_p

    _ut_scale = _lib_ud.ut_scale
    _ut_scale.argtypes = [ctypes.c_double, ctypes.c_void_p]
    _ut_scale.restype = ctypes.c_void_p

    # convenience dictionary for the Unit convert method
    _cv_convert_scalar = {FLOAT32: _cv_convert_float,
                          FLOAT64: _cv_convert_double}
    _cv_convert_array = {FLOAT32: _cv_convert_floats,
                         FLOAT64: _cv_convert_doubles}
    _numpy2ctypes = {np.float32: FLOAT32, np.float64: FLOAT64}
    _ctypes2numpy = {v: k for k, v in _numpy2ctypes.iteritems()}
#
# load the UDUNITS-2 xml-formatted unit-database
#
if not _ud_system:
    _func_type = ctypes.CFUNCTYPE(ctypes.c_int, ctypes.c_char_p,
                                  use_errno=True)
    _set_handler_type = ctypes.CFUNCTYPE(_func_type, _func_type)
    _ut_set_error_message_handler = _set_handler_type((_UT_HANDLER, _lib_ud))
    _ut_ignore = _func_type((_UT_IGNORE, _lib_ud))
    # ignore standard UDUNITS-2 start-up preamble redirected to stderr stream
    _default_handler = _ut_set_error_message_handler(_ut_ignore)
    # Load the unit-database from the default location (modified via
    # the UDUNITS2_XML_PATH environment variable) and if that fails look
    # relative to sys.prefix to support environments such as conda.
    _ud_system = _ut_read_xml(None)
    if _ud_system is None:
        _alt_xml_path = os.path.join(sys.prefix, 'share',
                                     'udunits', 'udunits2.xml')
        _ud_system = _ut_read_xml(_alt_xml_path)
    # reinstate old error handler
    _ut_set_error_message_handler(_default_handler)
    del _func_type
    if not _ud_system:
        _status_msg = 'UNKNOWN'
        _error_msg = ''
        _status = _ut_get_status()
        try:
            _status_msg = _UT_STATUS[_status]
        except IndexError:
            pass
        _errno = ctypes.get_errno()
        if _errno != 0:
            _error_msg = ': "%s"' % _strerror(_errno)
            ctypes.set_errno(0)
        raise OSError('[%s] Failed to open UDUNITS-2 XML unit database %s' % (
            _status_msg, _error_msg))


########################################################################
#
# module level function definitions
#
########################################################################

def encode_time(year, month, day, hour, minute, second):
    """
    Return date/clock time encoded as a double precision value.

    Encoding performed using UDUNITS-2 hybrid Gregorian/Julian calendar.
    Dates on or after 1582-10-15 are assumed to be Gregorian dates;
    dates before that are assumed to be Julian dates. In particular, the
    year 1 BCE is immediately followed by the year 1 CE.

    Args:

    * year (int):
        Year value to be encoded.
    * month (int):
        Month value to be encoded.
    * day (int):
        Day value to be encoded.
    * hour (int):
        Hour value to be encoded.
    * minute (int):
        Minute value to be encoded.
    * second (int):
        Second value to be encoded.

    Returns:
        float.

    For example:

        >>> import iris.unit as unit
        >>> unit.encode_time(1970, 1, 1, 0, 0, 0)
        -978307200.0

    """

    return _ut_encode_time(ctypes.c_int(year), ctypes.c_int(month),
                           ctypes.c_int(day), ctypes.c_int(hour),
                           ctypes.c_int(minute), ctypes.c_double(second))


def encode_date(year, month, day):
    """
    Return date encoded as a double precision value.

    Encoding performed using UDUNITS-2 hybrid Gergorian/Julian calendar.
    Dates on or after 1582-10-15 are assumed to be Gregorian dates;
    dates before that are assumed to be Julian dates. In particular, the
    year 1 BCE is immediately followed by the year 1 CE.

    Args:

    * year (int):
        Year value to be encoded.
    * month (int):
        Month value to be encoded.
    * day (int):
        Day value to be encoded.

    Returns:
        float.

    For example:

        >>> import iris.unit as unit
        >>> unit.encode_date(1970, 1, 1)
        -978307200.0

    """

    return _ut_encode_date(ctypes.c_int(year), ctypes.c_int(month),
                           ctypes.c_int(day))


def encode_clock(hour, minute, second):
    """
    Return clock time encoded as a double precision value.

    Args:

    * hour (int):
        Hour value to be encoded.
    * minute (int):
        Minute value to be encoded.
    * second (int):
        Second value to be encoded.

    Returns:
        float.

    For example:

        >>> import iris.unit as unit
        >>> unit.encode_clock(0, 0, 0)
        0.0

    """

    return _ut_encode_clock(ctypes.c_int(hour), ctypes.c_int(minute),
                            ctypes.c_double(second))


def decode_time(time):
    """
    Decode a double precision date/clock time value into its component
    parts and return as tuple.

    Decode time into it's year, month, day, hour, minute, second, and
    resolution component parts. Where resolution is the uncertainty of
    the time in seconds.

    Args:

    * time (float): Date/clock time encoded as a double precision value.

    Returns:
        tuple of (year, month, day, hour, minute, second, resolution).

    For example:

        >>> import iris.unit as unit
        >>> unit.decode_time(unit.encode_time(1970, 1, 1, 0, 0, 0))
        (1970, 1, 1, 0, 0, 0.0, 1.086139178596568e-07)

    """

    year = ctypes.c_int()
    month = ctypes.c_int()
    day = ctypes.c_int()
    hour = ctypes.c_int()
    minute = ctypes.c_int()
    second = ctypes.c_double()
    resolution = ctypes.c_double()
    _ut_decode_time(ctypes.c_double(time), ctypes.pointer(year),
                    ctypes.pointer(month), ctypes.pointer(day),
                    ctypes.pointer(hour), ctypes.pointer(minute),
                    ctypes.pointer(second), ctypes.pointer(resolution))
    return (year.value, month.value, day.value, hour.value, minute.value,
            second.value, resolution.value)


def julian_day2date(julian_day, calendar):
    """
    Return a netcdftime datetime-like object representing the Julian day.

    If calendar is 'standard' or 'gregorian', Julian day follows
    Julian calendar on and before 1582-10-5, Gregorian calendar after
    1582-10-15.
    If calendar is 'proleptic_gregorian', Julian Day follows Gregorian
    calendar.
    If calendar is 'julian', Julian Day follows Julian calendar.

    The datetime object is a 'real' datetime object if the date falls in
    the Gregorian calendar (i.e. calendar is 'proleptic_gregorian', or
    calendar is 'standard'/'gregorian' and the date is after 1582-10-15).
    Otherwise, it's a 'phony' datetime object which is actually an instance
    of netcdftime.datetime.

    Algorithm:
        Meeus, Jean (1998) Astronomical Algorithms (2nd Edition).
        Willmann-Bell, Virginia. p. 63.

    Args:

    * julian_day (float):
        Julian day with a resolution of 1 second.
    * calendar (string):
        Name of the calendar, see iris.unit.CALENDARS.

    Returns:
        datetime or netcdftime.datetime.

    For example:

        >>> import iris.unit as unit
        >>> import datetime
        >>> unit.julian_day2date(
        ...     unit.date2julian_day(datetime.datetime(1970, 1, 1, 0, 0, 0),
        ...                          unit.CALENDAR_STANDARD),
        ...     unit.CALENDAR_STANDARD)
        datetime.datetime(1970, 1, 1, 0, 0)

    """

    return netcdftime.DateFromJulianDay(julian_day, calendar)


def date2julian_day(date, calendar):
    """
    Return the Julian day (resolution of 1 second) from a netcdftime
    datetime-like object.

    If calendar is 'standard' or 'gregorian', Julian day follows Julian
    calendar on and before 1582-10-5, Gregorian calendar after 1582-10-15.
    If calendar is 'proleptic_gregorian', Julian day follows Gregorian
    calendar.
    If calendar is 'julian', Julian day follows Julian calendar.

    Algorithm:
        Meeus, Jean (1998) Astronomical Algorithms (2nd Edition).
        Willmann-Bell, Virginia. p. 63.

    Args:

    * date (netcdftime.date):
        Date and time representation.
    * calendar (string):
        Name of the calendar, see iris.unit.CALENDARS.

    Returns:
        float.

    For example:

        >>> import iris.unit as unit
        >>> import datetime
        >>> unit.date2julian_day(datetime.datetime(1970, 1, 1, 0, 0, 0),
        ...                      unit.CALENDAR_STANDARD)
        2440587.5

    """

    return netcdftime.JulianDayFromDate(date, calendar)


def date2num(date, unit, calendar):
    """
    Return numeric time value (resolution of 1 second) encoding of
    datetime object.

    The units of the numeric time values are described by the unit and
    calendar arguments. The datetime objects must be in UTC with no
    time-zone offset. If there is a time-zone offset in unit, it will be
    applied to the returned numeric values.

    Like the :func:`matplotlib.dates.date2num` function, except that it allows
    for different units and calendars.  Behaves the same as if
    unit = 'days since 0001-01-01 00:00:00' and
    calendar = 'proleptic_gregorian'.

    Args:

    * date (datetime):
        A datetime object or a sequence of datetime objects.
        The datetime objects should not include a time-zone offset.
    * unit (string):
        A string of the form '<time-unit> since <time-origin>' describing
        the time units. The <time-unit> can be days, hours, minutes or seconds.
        The <time-origin> is a date/time reference point. A valid choice
        would be unit='hours since 1800-01-01 00:00:00 -6:00'.
    * calendar (string):
        Name of the calendar, see iris.unit.CALENDARS.

    Returns:
        float, or numpy.ndarray of float.

    For example:

        >>> import iris.unit as unit
        >>> import datetime
        >>> dt1 = datetime.datetime(1970, 1, 1, 6, 0, 0)
        >>> dt2 = datetime.datetime(1970, 1, 1, 7, 0, 0)
        >>> unit.date2num(dt1, 'hours since 1970-01-01 00:00:00',
        ...               unit.CALENDAR_STANDARD)
        6.0
        >>> unit.date2num([dt1, dt2], 'hours since 1970-01-01 00:00:00',
        ...               unit.CALENDAR_STANDARD)
        array([ 6.,  7.])

    """

    #
    # ensure to strip out any 'UTC' postfix which is generated by
    # UDUNITS-2 formatted output and causes the netcdftime parser
    # to choke
    #
    unit_string = unit.rstrip(" UTC")
    if unit_string.endswith(" since epoch"):
        unit_string = unit_string.replace("epoch", IRIS_EPOCH)
    return netcdftime.date2num(date, unit_string, calendar)


def num2date(time_value, unit, calendar):
    """
    Return datetime encoding of numeric time value (resolution of 1 second).

    The units of the numeric time value are described by the unit and
    calendar arguments. The returned datetime object represent UTC with
    no time-zone offset, even if the specified unit contain a time-zone
    offset.

    Like the :func:`matplotlib.dates.num2date` function, except that it allows
    for different units and calendars.  Behaves the same if
    unit = 'days since 001-01-01 00:00:00'}
    calendar = 'proleptic_gregorian'.

    The datetime instances returned are 'real' python datetime
    objects if the date falls in the Gregorian calendar (i.e.
    calendar='proleptic_gregorian', or calendar = 'standard' or 'gregorian'
    and the date is after 1582-10-15). Otherwise, they are 'phony' datetime
    objects which support some but not all the methods of 'real' python
    datetime objects.  This is because the python datetime module cannot
    use the 'proleptic_gregorian' calendar, even before the switch
    occured from the Julian calendar in 1582. The datetime instances
    do not contain a time-zone offset, even if the specified unit
    contains one.

    Args:

    * time_value (float):
        Numeric time value/s. Maximum resolution is 1 second.
    * unit (sting):
        A string of the form '<time-unit> since <time-origin>'
        describing the time units. The <time-unit> can be days, hours,
        minutes or seconds. The <time-origin> is the date/time reference
        point. A valid choice would be
        unit='hours since 1800-01-01 00:00:00 -6:00'.
    * calendar (string):
        Name of the calendar, see iris.unit.CALENDARS.

    Returns:
        datetime, or numpy.ndarray of datetime object.

    For example:

        >>> import iris.unit as unit
        >>> import datetime
        >>> unit.num2date(6, 'hours since 1970-01-01 00:00:00',
        ...               unit.CALENDAR_STANDARD)
        datetime.datetime(1970, 1, 1, 6, 0)
        >>> unit.num2date([6, 7], 'hours since 1970-01-01 00:00:00',
        ...               unit.CALENDAR_STANDARD)
        array([datetime.datetime(1970, 1, 1, 6, 0),
               datetime.datetime(1970, 1, 1, 7, 0)], dtype=object)

    """

    #
    # ensure to strip out any 'UTC' postfix which is generated by
    # UDUNITS-2 formatted output and causes the netcdftime parser
    # to choke
    #
    unit_string = unit.rstrip(" UTC")
    if unit_string.endswith(" since epoch"):
        unit_string = unit_string.replace("epoch", IRIS_EPOCH)
    return netcdftime.num2date(time_value, unit_string, calendar)


def _handler(func):
    """Set the error message handler."""

    _ut_set_error_message_handler(func)


########################################################################
#
# unit wrapper class for unidata/ucar UDUNITS-2
#
########################################################################

def _Unit(category, ut_unit, calendar=None, origin=None):
    unit = iris.util._OrderedHashable.__new__(Unit)
    unit._init(category, ut_unit, calendar, origin)
    return unit


_CACHE = {}


def as_unit(unit):
    """
    Returns a Unit corresponding to the given unit.

    .. note::

        If the given unit is already a Unit it will be returned unchanged.

    """
    if isinstance(unit, Unit):
        result = unit
    else:
        result = None
        use_cache = isinstance(unit, basestring) or unit is None
        if use_cache:
            result = _CACHE.get(unit)
        if result is None:
            result = Unit(unit)
            if use_cache:
                _CACHE[unit] = result
    return result


def is_time(unit):
    """
    Determine whether the unit is a related SI Unit of time.

    Args:

    * unit (string/Unit): Unit to be compared.

    Returns:
        Boolean.

    For example:

        >>> import iris.unit as unit
        >>> unit.is_time('hours')
        True
        >>> unit.is_time('meters')
        False

    """
    return as_unit(unit).is_time()


def is_vertical(unit):
    """
    Determine whether the unit is a related SI Unit of pressure or distance.

    Args:

    * unit (string/Unit): Unit to be compared.

    Returns:
        Boolean.

    For example:

        >>> import iris.unit as unit
        >>> unit.is_vertical('millibar')
        True
        >>> unit.is_vertical('km')
        True

    """
    return as_unit(unit).is_vertical()


class Unit(iris.util._OrderedHashable):
    """
    A class to represent S.I. units and support common operations to
    manipulate such units in a consistent manner as per UDUNITS-2.

    These operations include scaling the unit, offsetting the unit by a
    constant or time, inverting the unit, raising the unit by a power,
    taking a root of the unit, taking a log of the unit, multiplying the
    unit by a constant or another unit, dividing the unit by a constant
    or another unit, comparing units, copying units and converting unit
    data to single precision or double precision floating point numbers.

    This class also supports time and calendar defintion and manipulation.

    """
    # Declare the attribute names relevant to the _OrderedHashable behaviour.
    _names = ('category', 'ut_unit', 'calendar', 'origin')

    category = None
    'Is this an unknown unit, a no-unit, or a UDUNITS-2 unit.'

    ut_unit = None
    'Reference to the ctypes quantity defining the UDUNITS-2 unit.'

    calendar = None
    'Represents the unit calendar name, see iris.unit.CALENDARS'

    origin = None
    'The original string used to create this unit.'

    __slots__ = ()

    def __init__(self, unit, calendar=None):
        """
        Create a wrapper instance for UDUNITS-2.

        An optional calendar may be provided for a unit which defines a
        time reference of the form '<time-unit> since <time-origin>'
        i.e. unit='days since 1970-01-01 00:00:00'. For a unit that is a
        time reference, the default calendar is 'standard'.

        Accepted calendars are as follows,

        * 'standard' or 'gregorian' - Mixed Gregorian/Julian calendar as
          defined by udunits.
        * 'proleptic_gregorian' - A Gregorian calendar extended to dates
          before 1582-10-15. A year is a leap year if either,
            1. It is divisible by 4 but not by 100, or
            2. It is divisible by 400.
        * 'noleap' or '365_day' - A Gregorian calendar without leap
          years i.e. all years are 365 days long.
        * 'all_leap' or '366_day' - A Gregorian calendar with every year
          being a leap year i.e. all years are 366 days long.
        * '360_day' - All years are 360 days divided into 30 day months.
        * 'julian' - Proleptic Julian calendar, extended to dates after
          1582-10-5. A year is a leap year if it is divisible by 4.

        Args:

        * unit:
            Specify the unit as defined by UDUNITS-2.
        * calendar (string):
            Describes the calendar used in time calculations. The
            default is 'standard' or 'gregorian' for a time reference
            unit.

        Returns:

            Unit object.

        Units should be set to "no_unit" for values which are strings.
        Units can also be set to "unknown" (or None).
        For example:

            >>> from iris.unit import Unit
            >>> volts = Unit('volts')
            >>> no_unit = Unit('no_unit')
            >>> unknown = Unit('unknown')
            >>> unknown = Unit(None)

        """
        ut_unit = None
        calendar_ = None

        if unit is None:
            unit = ''
        else:
            unit = str(unit).strip()

        if unit.lower().endswith(' utc'):
            unit = unit[:unit.lower().rfind(' utc')]

        if unit.endswith(" since epoch"):
            unit = unit.replace("epoch", IRIS_EPOCH)

        if unit.lower() in _UNKNOWN_UNIT:
            # TODO - removing the option of an unknown unit. Currently
            # the auto generated MOSIG rules are missing units on a
            # number of phenomena which would lead to errors.
            # Will be addressed by work on metadata translation.
            category = _CATEGORY_UNKNOWN
            unit = _UNKNOWN_UNIT_STRING
        elif unit.lower() in _NO_UNIT:
            category = _CATEGORY_NO_UNIT
            unit = _NO_UNIT_STRING
        else:
            category = _CATEGORY_UDUNIT
            ut_unit = _ut_parse(_ud_system, unit, UT_ASCII)
            # _ut_parse returns 0 on failure
            if ut_unit is None:
                self._raise_error('Failed to parse unit "%s"' % unit)
            if _OP_SINCE in unit.lower():
                if calendar is None:
                    calendar_ = CALENDAR_GREGORIAN
                elif isinstance(calendar, basestring):
                    if calendar.lower() in CALENDARS:
                        calendar_ = calendar.lower()
                    else:
                        msg = '{!r} is an unsupported calendar.'
                        raise ValueError(msg.format(calendar))
                else:
                    msg = 'Expected string-like calendar argument, got {!r}.'
                    raise TypeError(msg.format(type(calendar)))

        self._init(category, ut_unit, calendar_, unit)

    def _raise_error(self, msg):
        """
        Retrieve the UDUNITS-2 ut_status, the implementation-defined string
        corresponding to UDUNITS-2 errno and raise generic exception.

        """
        status_msg = 'UNKNOWN'
        error_msg = ''
        if _lib_ud:
            status = _ut_get_status()
            try:
                status_msg = _UT_STATUS[status]
            except IndexError:
                pass
            errno = ctypes.get_errno()
            if errno != 0:
                error_msg = ': "%s"' % _strerror(errno)
                ctypes.set_errno(0)

        raise ValueError('[%s] %s %s' % (status_msg, msg, error_msg))

    # NOTE:
    # "__getstate__" and "__setstate__" functions are defined here to
    # provide a custom interface for Pickle
    #  : Pickle "normal" behaviour is just to save/reinstate the object
    #    dictionary
    #  : that won't work here, because the "ut_unit" attribute is an
    #    object handle
    #    - the corresponding udunits object only exists in the original
    #      invocation
    def __getstate__(self):
        # state capture method for Pickle.dump()
        #  - return the instance data needed to reconstruct a Unit value
        return {'unit_text': self.origin, 'calendar': self.calendar}

    def __setstate__(self, state):
        # object reconstruction method for Pickle.load()
        # intercept the Pickle.load() operation and call own __init__ again
        #  - this is to ensure a valid ut_unit attribute (as these
        #    handles aren't persistent)
        self.__init__(state['unit_text'], calendar=state['calendar'])

    def __del__(self):
        # NB. If Python is terminating then the module global "_ut_free"
        # may have already been deleted ... so we check before using it.
        if _ut_free:
            _ut_free(self.ut_unit)

    def __copy__(self):
        return self

    def __deepcopy__(self, memo):
        return self

    def is_time(self):
        """
        Determine whether this unit is a related SI Unit of time.

        Returns:
            Boolean.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('hours')
            >>> u.is_time()
            True
            >>> v = unit.Unit('meter')
            >>> v.is_time()
            False

        """
        if self.is_unknown() or self.is_no_unit():
            result = False
        else:
            day = _ut_get_unit_by_name(_ud_system, 'day')
            result = _ut_are_convertible(self.ut_unit, day) != 0
        return result

    def is_vertical(self):
        """
        Determine whether the unit is a related SI Unit of pressure or
        distance.

        Returns:
            Boolean.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('millibar')
            >>> u.is_vertical()
            True
            >>> v = unit.Unit('km')
            >>> v.is_vertical()
            True

        """
        if self.is_unknown() or self.is_no_unit():
            result = False
        else:
            bar = _ut_get_unit_by_name(_ud_system, 'bar')
            result = _ut_are_convertible(self.ut_unit, bar) != 0
            if not result:
                meter = _ut_get_unit_by_name(_ud_system, 'meter')
                result = _ut_are_convertible(self.ut_unit, meter) != 0
        return result

    def is_udunits(self):
        """Return whether the unit is a vaild unit of UDUNITS."""
        return self.ut_unit is not None

    def is_time_reference(self):
        """
        Return whether the unit is a time reference unit of the form
        '<time-unit> since <time-origin>'
        i.e. unit='days since 1970-01-01 00:00:00'

        Returns:
            Boolean.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('days since epoch')
            >>> u.is_time_reference()
            True

        """
        return self.calendar is not None

    def title(self, value):
        """
        Return the unit value as a title string.

        Args:

        * value (float): Unit value to be incorporated into title string.

        Returns:
            string.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('hours since epoch',
            ...               calendar=unit.CALENDAR_STANDARD)
            >>> u.title(10)
            '1970-01-01 10:00:00'

        """
        if self.is_time_reference():
            dt = self.num2date(value)
            result = dt.strftime('%Y-%m-%d %H:%M:%S')
        else:
            result = '%s %s' % (str(value), self)
        return result

    @property
    def modulus(self):
        """
        *(read-only)* Return the modulus value of the unit.

        Convenience method that returns the unit modulus value as follows,
            * 'radians' - pi*2
            * 'degrees' - 360.0
            * Otherwise None.

        Returns:
            float.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('degrees')
            >>> u.modulus
            360.0

        """

        if self == 'radians':
            result = np.pi * 2
        elif self == 'degrees':
            result = 360.0
        else:
            result = None
        return result

    def is_convertible(self, other):
        """
        Return whether two units are convertible.

        Args:

        * other (Unit): Unit to be compared.

        Returns:
            Boolean.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('meters')
            >>> v = unit.Unit('kilometers')
            >>> u.is_convertible(v)
            True

        """
        other = as_unit(other)
        if self.is_unknown() or self.is_no_unit() or other.is_unknown() or \
           other.is_no_unit():
            result = False
        else:
            result = (self.calendar == other.calendar and
                      _ut_are_convertible(self.ut_unit, other.ut_unit) != 0)
        return result

    def is_dimensionless(self):
        """
        Return whether the unit is dimensionless.

        Returns:
            Boolean.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('meters')
            >>> u.is_dimensionless()
            False
            >>> u = unit.Unit('1')
            >>> u.is_dimensionless()
            True

        """
        return (self.category == _CATEGORY_UDUNIT and
                bool(_ut_is_dimensionless(self.ut_unit)))

    def is_unknown(self):
        """
        Return whether the unit is defined to be an *unknown* unit.

        Returns:
            Boolean.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('unknown')
            >>> u.is_unknown()
            True
            >>> u = unit.Unit('meters')
            >>> u.is_unknown()
            False

        """
        return self.category == _CATEGORY_UNKNOWN

    def is_no_unit(self):
        """
        Return whether the unit is defined to be a *no_unit* unit.

        Typically, a quantity such as a string, will have no associated
        unit to describe it. Such a class of quantity may be defined
        using the *no_unit* unit.

        Returns:
            Boolean.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('no unit')
            >>> u.is_no_unit()
            True
            >>> u = unit.Unit('meters')
            >>> u.is_no_unit()
            False

        """
        return self.category == _CATEGORY_NO_UNIT

    def format(self, option=None):
        """
        Return a formatted string representation of the binary unit.

        Args:

        * option (iris.unit.UT_FORMATS):
            Set the encoding option of the formatted string representation.
            Valid encoding options may be one of the following enumerations:

            * Unit.UT_ASCII
            * Unit.UT_ISO_8859_1
            * Unit.UT_LATIN1
            * Unit.UT_UTF8
            * Unit.UT_NAMES
            * Unit.UT_DEFINITION

            Multiple options may be combined within a list. The default
            option is iris.unit.UT_ASCII.

        Returns:
            string.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('meters')
            >>> u.format()
            'm'
            >>> u.format(unit.UT_NAMES)
            'meter'
            >>> u.format(unit.UT_DEFINITION)
            'm'

        """
        if self.is_unknown():
            return _UNKNOWN_UNIT_STRING
        elif self.is_no_unit():
            return _NO_UNIT_STRING
        else:
            bitmask = UT_ASCII
            if option is not None:
                if not isinstance(option, list):
                    option = [option]
                for i in option:
                    bitmask |= i
            string_buffer = ctypes.create_string_buffer(_STRING_BUFFER_DEPTH)
            depth = _ut_format(self.ut_unit, string_buffer,
                               ctypes.sizeof(string_buffer), bitmask)
            if depth < 0:
                self._raise_error('Failed to format %r' % self)
        return string_buffer.value

    @property
    def name(self):
        """
        *(read-only)* The full name of the unit.

        Formats the binary unit into a string representation using
        method :func:`iris.unit.Unit.format` with keyword argument
        option=iris.unit.UT_NAMES.

        Returns:
            string.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('watts')
            >>> u.name
            'watt'

        """
        return self.format(UT_NAMES)

    @property
    def symbol(self):
        """
        *(read-only)* The symbolic representation of the unit.

        Formats the binary unit into a string representation using
        method :func:`iris.unit.Unit.format`.

        Returns:
            string.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('watts')
            >>> u.symbol
            'W'

        """
        if self.is_unknown():
            result = _UNKNOWN_UNIT_SYMBOL
        elif self.is_no_unit():
            result = _NO_UNIT_SYMBOL
        else:
            result = self.format()
        return result

    @property
    def definition(self):
        """
        *(read-only)* The symbolic decomposition of the unit.

        Formats the binary unit into a string representation using
        method :func:`iris.unit.Unit.format` with keyword argument
        option=iris.unit.UT_DEFINITION.

        Returns:
            string.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('watts')
            >>> u.definition
            'm2.kg.s-3'

        """
        if self.is_unknown():
            result = _UNKNOWN_UNIT_SYMBOL
        elif self.is_no_unit():
            result = _NO_UNIT_SYMBOL
        else:
            result = self.format(UT_DEFINITION)
        return result

    def offset_by_time(self, origin):
        """
        Returns the time unit offset with respect to the time origin.

        Args:

        * origin (float): Time origin as returned by the
          :func:`iris.unit.encode_time` method.

        Returns:
            None.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('hours')
            >>> u.offset_by_time(unit.encode_time(1970, 1, 1, 0, 0, 0))
            Unit('hour since 1970-01-01 00:00:00.0000000 UTC')

        """

        if not isinstance(origin, (int, float, long)):
            raise TypeError('a numeric type for the origin argument is'
                            ' required')
        ut_unit = _ut_offset_by_time(self.ut_unit, ctypes.c_double(origin))
        if not ut_unit:
            self._raise_error('Failed to offset %r' % self)
        calendar = None
        return _Unit(_CATEGORY_UDUNIT, ut_unit, calendar)

    def invert(self):
        """
        Invert the unit i.e. find the reciprocal of the unit, and return
        the Unit result.

        Returns:
            Unit.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('meters')
            >>> u.invert()
            Unit('meter^-1')

        """
        if self.is_unknown():
            result = self
        elif self.is_no_unit():
            raise ValueError("Cannot invert a 'no-unit'.")
        else:
            ut_unit = _ut_invert(self.ut_unit)
            if not ut_unit:
                self._raise_error('Failed to invert %r' % self)
            calendar = None
            result = _Unit(_CATEGORY_UDUNIT, ut_unit, calendar)
        return result

    def root(self, root):
        """
        Returns the given root of the unit.

        Args:

        * root (int/long): Value by which the unit root is taken.

        Returns:
            None.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('meters^2')
            >>> u.root(2)
            Unit('meter')

        .. note::

            Taking a fractional root of a unit is not supported.

        """
        try:
            root = ctypes.c_int(root)
        except TypeError:
            raise TypeError('An int or long type for the root argument'
                            ' is required')

        if self.is_unknown():
            result = self
        elif self.is_no_unit():
            raise ValueError("Cannot take the logarithm of a 'no-unit'.")
        else:
            # only update the unit if it is not scalar
            if self == Unit('1'):
                result = self
            else:
                ut_unit = _ut_root(self.ut_unit, root)
                if not ut_unit:
                    self._raise_error('Failed to take the root of %r' % self)
                calendar = None
                result = _Unit(_CATEGORY_UDUNIT, ut_unit, calendar)
        return result

    def log(self, base):
        """
        Returns the logorithmic unit corresponding to the given
        logorithmic base.

        Args:

        * base (int/float/long): Value of the logorithmic base.

        Returns:
            None.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('meters')
            >>> u.log(2)
            Unit('lb(re 1 meter)')

        """
        try:
            base = ctypes.c_double(base)
        except TypeError:
            raise TypeError('A numeric type for the base argument is required')

        if self.is_unknown():
            result = self
        elif self.is_no_unit():
            raise ValueError("Cannot take the logarithm of a 'no-unit'.")
        else:
            ut_unit = _ut_log(base, self.ut_unit)
            if not ut_unit:
                msg = 'Failed to calculate logorithmic base of %r' % self
                self._raise_error(msg)
            calendar = None
            result = _Unit(_CATEGORY_UDUNIT, ut_unit, calendar)
        return result

    def __str__(self):
        """
        Returns a simple string representation of the unit.

        Returns:
            string.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('meters')
            >>> str(u)
            'meters'

        """
        return self.origin or self.name

    def __repr__(self):
        """
        Returns a string representation of the unit object.

        Returns:
            string.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('meters')
            >>> repr(u)
            "Unit('meters')"

        """

        if self.calendar is None:
            result = "%s('%s')" % (self.__class__.__name__, self)
        else:
            result = "%s('%s', calendar='%s')" % (self.__class__.__name__,
                                                  self, self.calendar)
        return result

    def _offset_common(self, offset):
        try:
            offset = ctypes.c_double(offset)
        except TypeError:
            result = NotImplemented
        else:
            if self.is_unknown():
                result = self
            elif self.is_no_unit():
                raise ValueError("Cannot offset a 'no-unit'.")
            else:
                ut_unit = _ut_offset(self.ut_unit, offset)
                if not ut_unit:
                    self._raise_error('Failed to offset %r' % self)
                calendar = None
                result = _Unit(_CATEGORY_UDUNIT, ut_unit, calendar)
        return result

    def __add__(self, other):
        return self._offset_common(other)

    def __sub__(self, other):
        try:
            other = -other
        except TypeError:
            result = NotImplemented
        else:
            result = self._offset_common(-other)
        return result

    def _op_common(self, other, op_func):
        # Convienience method to create a new unit from an operation between
        # the units 'self' and 'other'.

        op_label = op_func.__name__.split('_')[1]

        other = as_unit(other)

        if self.is_no_unit() or other.is_no_unit():
            raise ValueError("Cannot %s a 'no-unit'." % op_label)

        if self.is_unknown() or other.is_unknown():
            result = _Unit(_CATEGORY_UNKNOWN, None)
        else:
            ut_unit = op_func(self.ut_unit, other.ut_unit)
            if not ut_unit:
                msg = 'Failed to %s %r by %r' % (op_label, self, other)
                self._raise_error(msg)
            calendar = None
            result = _Unit(_CATEGORY_UDUNIT, ut_unit, calendar)
        return result

    def __rmul__(self, other):
        # NB. Because we've subclassed a tuple, we need to define this to
        # prevent the default tuple-repetition behaviour.
        # ie. 2 * ('a', 'b') -> ('a', 'b', 'a', 'b')
        return self * other

    def __mul__(self, other):
        """
        Multiply the self unit by the other scale factor or unit and
        return the Unit result.

        Note that, multiplication involving an 'unknown' unit will always
        result in an 'unknown' unit.

        Args:

        * other (int/float/long/string/Unit): Multiplication scale
          factor or unit.

        Returns:
            Unit.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('meters')
            >>> v = unit.Unit('hertz')
            >>> u*v
            Unit('meter-second^-1')

        """
        return self._op_common(other, _ut_multiply)

    def __div__(self, other):
        """
        Divide the self unit by the other scale factor or unit and
        return the Unit result.

        Note that, division involving an 'unknown' unit will always
        result in an 'unknown' unit.

        Args:

        * other (int/float/long/string/Unit): Division scale factor or unit.

        Returns:
            Unit.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('m.s-1')
            >>> v = unit.Unit('hertz')
            >>> u/v
            Unit('meter')

        """
        return self._op_common(other, _ut_divide)

    def __truediv__(self, other):
        """
        Divide the self unit by the other scale factor or unit and
        return the Unit result.

        Note that, division involving an 'unknown' unit will always
        result in an 'unknown' unit.

        Args:

        * other (int/float/long/string/Unit): Division scale factor or unit.

        Returns:
            Unit.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('m.s-1')
            >>> v = unit.Unit('hertz')
            >>> u/v
            Unit('meter')

        """
        return self.__div__(other)

    def __pow__(self, power):
        """
        Raise the unit by the given power and return the Unit result.

        Note that, UDUNITS-2 does not support raising a
        non-dimensionless unit by a fractional power.
        Approximate floating point power behaviour has been implemented
        specifically for Iris.

        Args:

        * power (int/float/long): Value by which the unit power is raised.

        Returns:
            Unit.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('meters')
            >>> u**2
            Unit('meter^2')

        """
        try:
            power = float(power)
        except ValueError:
            raise TypeError('A numeric value is required for the power'
                            ' argument.')

        if self.is_unknown():
            result = self
        elif self.is_no_unit():
            raise ValueError("Cannot raise the power of a 'no-unit'.")
        elif self == Unit('1'):
            # 1 ** N -> 1
            result = self
        else:
            # UDUNITS-2 does not support floating point raise/root.
            # But if the power is of the form 1/N, where N is an integer
            # (within a certain acceptable accuracy) then we can find the Nth
            # root.
            if not iris.util.approx_equal(power, 0.0) and abs(power) < 1:
                if not iris.util.approx_equal(1 / power, round(1 / power)):
                    raise ValueError('Cannot raise a unit by a decimal.')
                root = int(round(1 / power))
                result = self.root(root)
            else:
                # Failing that, check for powers which are (very nearly) simple
                # integer values.
                if not iris.util.approx_equal(power, round(power)):
                    msg = 'Cannot raise a unit by a decimal (got %s).' % power
                    raise ValueError(msg)
                power = int(round(power))

                ut_unit = _ut_raise(self.ut_unit, ctypes.c_int(power))
                if not ut_unit:
                    self._raise_error('Failed to raise the power of %r' % self)
                result = _Unit(_CATEGORY_UDUNIT, ut_unit)
        return result

    def _identity(self):
        # Redefine the comparison/hash/ordering identity as used by
        # iris.util._OrderedHashable.
        return (self.name, self.calendar)

    def __eq__(self, other):
        """
        Compare the two units for equality and return the boolean result.

        Args:

        * other (string/Unit): Unit to be compared.

        Returns:
            Boolean.

        For example:

            >>> from iris.unit import Unit
            >>> Unit('meters') == Unit('millimeters')
            False
            >>> Unit('meters') == 'm'
            True

        """
        other = as_unit(other)

        # Compare category (i.e. unknown, no_unit, etc.).
        if self.category != other.category:
            return False

        # Compare calendar as UDUNITS cannot handle calendars.
        if self.calendar != other.calendar:
            return False

        # Compare UDUNITS.
        res = _ut_compare(self.ut_unit, other.ut_unit)
        return res == 0

    def __ne__(self, other):
        """
        Compare the two units for inequality and return the boolean result.

        Args:

        * other (string/Unit): Unit to be compared.

        Returns:
            Boolean.

        For example:

            >>> from iris.unit import Unit
            >>> Unit('meters') != Unit('millimeters')
            True
            >>> Unit('meters') != 'm'
            False

        """
        return not self == other

    def convert(self, value, other, ctype=FLOAT64):
        """
        Converts a single value or numpy array of values from the current unit
        to the other target unit.

        If the units are not convertible, then no conversion will take place.

        Args:

        * value (int/float/long/numpy.ndarray):
            Value/s to be converted.
        * other (string/Unit):
            Target unit to convert to.
        * ctype (ctypes.c_float/ctypes.c_double):
            Floating point 32-bit single-precision (iris.unit.FLOAT32) or
            64-bit double-precision (iris.unit.FLOAT64) of conversion. The
            default is 64-bit double-precision conversion.

        Returns:
            float or numpy.ndarray of appropriate float type.

        For example:

            >>> import iris.unit as unit
            >>> import numpy as np
            >>> c = unit.Unit('deg_c')
            >>> f = unit.Unit('deg_f')
            >>> print c.convert(0, f)
            32.0
            >>> c.convert(0, f, unit.FLOAT32)
            32.0
            >>> a64 = np.arange(10, dtype=np.float64)
            >>> c.convert(a64, f)
            array([ 32. ,  33.8,  35.6,  37.4,  39.2,  41. ,  42.8,  44.6, \
 46.4,  48.2])
            >>> a32 = np.arange(10, dtype=np.float32)
            >>> c.convert(a32, f)
            array([ 32.        ,  33.79999924,  35.59999847,  37.40000153,
                    39.20000076,  41.        ,  42.79999924,  44.59999847,
                    46.40000153,  48.20000076], dtype=float32)

        .. note::

            Conversion is done *in-place* for numpy arrays. Also note that,
            conversion between unit calendars is not permitted.

        """
        result = None
        other = as_unit(other)
        value_copy = copy.deepcopy(value)

        if self == other:
            return value

        if self.is_convertible(other):
            # Use utime for converting reference times that are not using a
            # gregorian calendar as it handles these and udunits does not.
            if self.is_time_reference() \
                    and self.calendar != CALENDAR_GREGORIAN:
                ut1 = self.utime()
                ut2 = other.utime()
                result = ut2.date2num(ut1.num2date(value_copy))
            else:
                ut_converter = _ut_get_converter(self.ut_unit, other.ut_unit)
                if ut_converter:
                    if isinstance(value_copy, np.ndarray):
                        # Can only handle array of np.float32 or np.float64 so
                        # cast array of ints to array of floats of requested
                        # precision.
                        if issubclass(value_copy.dtype.type, np.integer):
                            value_copy = value_copy.astype(
                                _ctypes2numpy[ctype])
                        # strict type check of numpy array
                        if value_copy.dtype.type not in _numpy2ctypes.keys():
                            raise TypeError(
                                "Expect a numpy array of '%s' or '%s'" %
                                tuple(sorted(_numpy2ctypes.keys())))
                        ctype = _numpy2ctypes[value_copy.dtype.type]
                        pointer = value_copy.ctypes.data_as(
                            ctypes.POINTER(ctype))
                        # Utilise global convenience dictionary
                        # _cv_convert_array
                        _cv_convert_array[ctype](ut_converter, pointer,
                                                 value_copy.size, pointer)
                        result = value_copy
                    else:
                        if ctype not in _cv_convert_scalar.keys():
                            raise ValueError('Invalid target type. Can only '
                                             'convert to float or double.')
                        # Utilise global convenience dictionary
                        # _cv_convert_scalar
                        result = _cv_convert_scalar[ctype](ut_converter,
                                                           ctype(value_copy))
                    _cv_free(ut_converter)
                else:
                    self._raise_error('Failed to convert %r to %r' %
                                      (self, other))
        else:
            raise ValueError("Unable to convert from '%r' to '%r'." %
                             (self, other))
        return result

    def utime(self):
        """
        Returns a netcdftime.utime object which performs conversions of
        numeric time values to/from datetime objects given the current
        calendar and unit time reference.

        The current unit time reference must be of the form:
        '<time-unit> since <time-origin>'
        i.e. 'hours since 1970-01-01 00:00:00'

        Returns:
            netcdftime.utime.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('hours since 1970-01-01 00:00:00',
            ...               calendar=unit.CALENDAR_STANDARD)
            >>> ut = u.utime()
            >>> ut.num2date(2)
            datetime.datetime(1970, 1, 1, 2, 0)

        """

        #
        # ensure to strip out non-parsable 'UTC' postfix which
        # is generated by UDUNITS-2 formatted output
        #
        if self.calendar is None:
            raise ValueError('Unit has undefined calendar')
        return netcdftime.utime(str(self).rstrip(" UTC"), self.calendar)

    def date2num(self, date):
        """
        Returns the numeric time value calculated from the datetime
        object using the current calendar and unit time reference.

        The current unit time reference must be of the form:
        '<time-unit> since <time-origin>'
        i.e. 'hours since 1970-01-01 00:00:00'

        Works for scalars, sequences and numpy arrays. Returns a scalar
        if input is a scalar, else returns a numpy array.

        Args:

        * date (datetime):
            A datetime object or a sequence of datetime objects.
            The datetime objects should not include a time-zone offset.

        Returns:
            float or numpy.ndarray of float.

        For example:

            >>> import iris.unit as unit
            >>> import datetime
            >>> u = unit.Unit('hours since 1970-01-01 00:00:00',
            ...               calendar=unit.CALENDAR_STANDARD)
            >>> u.date2num(datetime.datetime(1970, 1, 1, 5))
            5.00000000372529
            >>> u.date2num([datetime.datetime(1970, 1, 1, 5),
            ...             datetime.datetime(1970, 1, 1, 6)])
            array([ 5.,  6.])

        """

        cdf_utime = self.utime()
        return cdf_utime.date2num(date)

    def num2date(self, time_value):
        """
        Returns a datetime-like object calculated from the numeric time
        value using the current calendar and the unit time reference.

        The current unit time reference must be of the form:
        '<time-unit> since <time-origin>'
        i.e. 'hours since 1970-01-01 00:00:00'

        The datetime objects returned are 'real' Python datetime objects
        if the date falls in the Gregorian calendar (i.e. the calendar
        is 'standard', 'gregorian', or 'proleptic_gregorian' and the
        date is after 1582-10-15). Otherwise a 'phoney' datetime-like
        object (netcdftime.datetime) is returned which can handle dates
        that don't exist in the Proleptic Gregorian calendar.

        Works for scalars, sequences and numpy arrays. Returns a scalar
        if input is a scalar, else returns a numpy array.

        Args:

        * time_value (float): Numeric time value/s. Maximum resolution
          is 1 second.

        Returns:
            datetime, or numpy.ndarray of datetime object.

        For example:

            >>> import iris.unit as unit
            >>> u = unit.Unit('hours since 1970-01-01 00:00:00',
            ...               calendar=unit.CALENDAR_STANDARD)
            >>> u.num2date(6)
            datetime.datetime(1970, 1, 1, 6, 0)
            >>> u.num2date([6, 7])
            array([datetime.datetime(1970, 1, 1, 6, 0),
                   datetime.datetime(1970, 1, 1, 7, 0)], dtype=object)

        """
        cdf_utime = self.utime()
        return cdf_utime.num2date(time_value)

########NEW FILE########
__FILENAME__ = util
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Miscellaneous utility functions.

"""

import abc
import collections
import copy
import inspect
import os
import os.path
import sys
import tempfile
import time
import warnings

import numpy as np
import numpy.ma as ma

import iris
import iris.exceptions


def broadcast_weights(weights, array, dims):
    """
    Broadcast a weights array to the shape of another array.

    Each dimension of the weights array must correspond to a dimension
    of the other array.

    .. deprecated:: 1.6

       Please use :func:`~iris.util.broadcast_to_shape()`.

    Args:

    * weights (:class:`numpy.ndarray`-like):
        An array of weights to broadcast.

    * array (:class:`numpy.ndarray`-like):
        An array whose shape is the target shape for *weights*.

    * dims (:class:`list` :class:`tuple` etc.):
        A sequence of dimension indices, specifying which dimensions of
        *array* are represented in *weights*. The order the dimensions
        are given in is not important, but the order of the dimensions
        in *weights* should be the same as the relative ordering of the
        corresponding dimensions in *array*. For example, if *array* is
        4d with dimensions (ntime, nlev, nlat, nlon) and *weights*
        provides latitude-longitude grid weightings then *dims* could be
        set to [2, 3] or [3, 2] but *weights* must have shape
        (nlat, nlon) since the latitude dimension comes before the
        longitude dimension in *array*.

    """
    warnings.warn('broadcast_weights() is deprecated and will be removed '
                  'in a future release. Consider converting existing code '
                  'to use broadcast_to_shape() as a replacement.',
                  stacklevel=2)
    # Create a shape array, which *weights* can be re-shaped to, allowing
    # them to be broadcast with *array*.
    weights_shape = np.ones(array.ndim)
    for dim in dims:
        if dim is not None:
            weights_shape[dim] = array.shape[dim]
    # Broadcast the arrays together.
    return np.broadcast_arrays(weights.reshape(weights_shape), array)[0]


def broadcast_to_shape(array, shape, dim_map):
    """
    Broadcast an array to a given shape.

    Each dimension of the array must correspond to a dimension in the
    given shape. Striding is used to repeat the array until it matches
    the desired shape, returning repeated views on the original array.
    If you need to write to the resulting array, make a copy first.

    Args:

    * array (:class:`numpy.ndarray`-like)
        An array to broadcast.

    * shape (:class:`list`, :class:`tuple` etc.):
        The shape the array should be broadcast to.

    * dim_map (:class:`list`, :class:`tuple` etc.):
        A mapping of the dimensions of *array* to their corresponding
        element in *shape*. *dim_map* must be the same length as the
        number of dimensions in *array*. Each element of *dim_map*
        corresponds to a dimension of *array* and its value provides
        the index in *shape* which the dimension of *array* corresponds
        to, so the first element of *dim_map* gives the index of *shape*
        that corresponds to the first dimension of *array* etc.

    Examples:

    Broadcasting an array of shape (2, 3) to the shape (5, 2, 6, 3)
    where the first dimension of the array corresponds to the second
    element of the desired shape and the second dimension of the array
    corresponds to the fourth element of the desired shape::

        a = np.array([[1, 2, 3], [4, 5, 6]])
        b = broadcast_to_shape(a, (5, 2, 6, 3), (1, 3))

    Broadcasting an array of shape (48, 96) to the shape (96, 48, 12)::

        # a is an array of shape (48, 96)
        result = broadcast_to_shape(a, (96, 48, 12), (1, 0))

    """
    if len(dim_map) != array.ndim:
        # We must check for this condition here because we cannot rely on
        # getting an error from numpy if the dim_map argument is not the
        # correct length, we might just get a segfault.
        raise ValueError('dim_map must have an entry for every '
                         'dimension of the input array')

    def _broadcast_helper(a):
        strides = [0] * len(shape)
        for idim, dim in enumerate(dim_map):
            if shape[dim] != a.shape[idim]:
                # We'll get garbage values if the dimensions of array are not
                # those indicated by shape.
                raise ValueError('shape and array are not compatible')
            strides[dim] = a.strides[idim]
        return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)

    array_view = _broadcast_helper(array)
    if ma.isMaskedArray(array):
        if array.mask is ma.nomask:
            # Degenerate masks can be applied as-is.
            mask_view = array.mask
        else:
            # Mask arrays need to be handled in the same way as the data array.
            mask_view = _broadcast_helper(array.mask)
        array_view = ma.array(array_view, mask=mask_view)
    return array_view


def delta(ndarray, dimension, circular=False):
    """
    Calculates the difference between values along a given dimension.

    Args:

    * ndarray:
        The array over which to do the difference.

    * dimension:
        The dimension over which to do the difference on ndarray.

    * circular:
        If not False then return n results in the requested dimension
        with the delta between the last and first element included in
        the result otherwise the result will be of length n-1 (where n
        is the length of ndarray in the given dimension's direction)

        If circular is numeric then the value of circular will be added
        to the last element of the given dimension if the last element
        is negative, otherwise the value of circular will be subtracted
        from the last element.

        The example below illustrates the process::

            original array              -180, -90,  0,    90
            delta (with circular=360):    90,  90, 90, -270+360

    .. note::

        The difference algorithm implemented is forward difference:

            >>> import numpy as np
            >>> import iris.util
            >>> original = np.array([-180, -90, 0, 90])
            >>> iris.util.delta(original, 0)
            array([90, 90, 90])
            >>> iris.util.delta(original, 0, circular=360)
            array([90, 90, 90, 90])

    """
    if circular is not False:
        _delta = np.roll(ndarray, -1, axis=dimension)
        last_element = [slice(None, None)] * ndarray.ndim
        last_element[dimension] = slice(-1, None)

        if not isinstance(circular, bool):
            result = np.where(ndarray[last_element] >= _delta[last_element])[0]
            _delta[last_element] -= circular
            _delta[last_element][result] += 2*circular

        np.subtract(_delta, ndarray, _delta)
    else:
        _delta = np.diff(ndarray, axis=dimension)

    return _delta


def describe_diff(cube_a, cube_b, output_file=None):
    """
    Prints the differences that prevent compatibility between two cubes, as
    defined by :meth:`iris.cube.Cube.is_compatible()`.

    Args:

    * cube_a:
        An instance of :class:`iris.cube.Cube` or
        :class:`iris.cube.CubeMetadata`.

    * cube_b:
        An instance of :class:`iris.cube.Cube` or
        :class:`iris.cube.CubeMetadata`.

    * output_file:
        A :class:`file` or file-like object to receive output. Defaults to
        sys.stdout.

    .. seealso::

        :meth:`iris.cube.Cube.is_compatible()`

    .. note::

        Compatibility does not guarantee that two cubes can be merged.
        Instead, this function is designed to provide a verbose description
        of the differences in metadata between two cubes. Determining whether
        two cubes will merge requires additional logic that is beyond the
        scope of this function.

    """

    if output_file is None:
        output_file = sys.stdout

    if cube_a.is_compatible(cube_b):
        output_file.write('Cubes are compatible\n')
    else:
        common_keys = set(cube_a.attributes).intersection(cube_b.attributes)
        for key in common_keys:
            if np.any(cube_a.attributes[key] != cube_b.attributes[key]):
                output_file.write('"%s" cube_a attribute value "%s" is not '
                                  'compatible with cube_b '
                                  'attribute value "%s"\n'
                                  % (key,
                                     cube_a.attributes[key],
                                     cube_b.attributes[key]))

        if cube_a.name() != cube_b.name():
            output_file.write('cube_a name "%s" is not compatible '
                              'with cube_b name "%s"\n'
                              % (cube_a.name(), cube_b.name()))

        if cube_a.units != cube_b.units:
            output_file.write(
                'cube_a units "%s" are not compatible with cube_b units "%s"\n'
                % (cube_a.units, cube_b.units))

        if cube_a.cell_methods != cube_b.cell_methods:
            output_file.write('Cell methods\n%s\nand\n%s\nare not compatible\n'
                              % (cube_a.cell_methods, cube_b.cell_methods))


def guess_coord_axis(coord):
    """
    Returns a "best guess" axis name of the coordinate.

    Heuristic categoration of the coordinate into either label
    'T', 'Z', 'Y', 'X' or None.

    Args:

    * coord:
        The :class:`iris.coords.Coord`.

    Returns:
        'T', 'Z', 'Y', 'X', or None.

    """
    axis = None

    if coord.standard_name in ('longitude', 'grid_longitude',
                               'projection_x_coordinate'):
        axis = 'X'
    elif coord.standard_name in ('latitude', 'grid_latitude',
                                 'projection_y_coordinate'):
        axis = 'Y'
    elif (coord.units.is_convertible('hPa')
          or coord.attributes.get('positive') in ('up', 'down')):
        axis = 'Z'
    elif coord.units.is_time_reference():
        axis = 'T'

    return axis


def rolling_window(a, window=1, step=1, axis=-1):
    """
    Make an ndarray with a rolling window of the last dimension

    Args:

    * a : array_like
        Array to add rolling window to

    Kwargs:

    * window : int
        Size of rolling window
    * step : int
        Size of step between rolling windows
    * axis : int
        Axis to take the rolling window over

    Returns:

        Array that is a view of the original array with an added dimension
        of the size of the given window at axis + 1.

    Examples::

        >>> x=np.arange(10).reshape((2,5))
        >>> rolling_window(x, 3)
        array([[[0, 1, 2], [1, 2, 3], [2, 3, 4]],
               [[5, 6, 7], [6, 7, 8], [7, 8, 9]]])

    Calculate rolling mean of last dimension::

        >>> np.mean(rolling_window(x, 3), -1)
        array([[ 1.,  2.,  3.],
               [ 6.,  7.,  8.]])

    """
    # NOTE: The implementation of this function originates from
    # https://github.com/numpy/numpy/pull/31#issuecomment-1304851 04/08/2011
    if window < 1:
        raise ValueError("`window` must be at least 1.")
    if window > a.shape[axis]:
        raise ValueError("`window` is too long.")
    if step < 1:
        raise ValueError("`step` must be at least 1.")
    axis = axis % a.ndim
    num_windows = (a.shape[axis] - window + step) / step
    shape = a.shape[:axis] + (num_windows, window) + a.shape[axis + 1:]
    strides = (a.strides[:axis] + (step * a.strides[axis], a.strides[axis]) +
               a.strides[axis + 1:])
    rw = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)
    if ma.isMaskedArray(a):
        mask = ma.getmaskarray(a)
        strides = (mask.strides[:axis] +
                   (step * mask.strides[axis], mask.strides[axis]) +
                   mask.strides[axis + 1:])
        rw = ma.array(rw, mask=np.lib.stride_tricks.as_strided(
            mask, shape=shape, strides=strides))
    return rw


def array_equal(array1, array2):
    """
    Returns whether two arrays have the same shape and elements.

    This provides the same functionality as :func:`numpy.array_equal` but with
    additional support for arrays of strings.

    """
    array1, array2 = np.asarray(array1), np.asarray(array2)
    if array1.shape != array2.shape:
        eq = False
    else:
        eq = bool(np.asarray(array1 == array2).all())

    return eq


def approx_equal(a, b, max_absolute_error=1e-10, max_relative_error=1e-10):
    """
    Returns whether two numbers are almost equal, allowing for the
    finite precision of floating point numbers.

    """
    # Deal with numbers close to zero
    if abs(a - b) < max_absolute_error:
        return True
    # Ensure we get consistent results if "a" and "b" are supplied in the
    # opposite order.
    max_ab = max([a, b], key=abs)
    relative_error = abs(a - b) / max_ab
    return relative_error < max_relative_error


def between(lh, rh, lh_inclusive=True, rh_inclusive=True):
    """
    Provides a convenient way of defining a 3 element inequality such as
    ``a < number < b``.

    Arguments:

    * lh
        The left hand element of the inequality
    * rh
        The right hand element of the inequality

    Keywords:

    * lh_inclusive - boolean
        Affects the left hand comparison operator to use in the inequality.
        True for ``<=`` false for ``<``. Defaults to True.
    * rh_inclusive - boolean
        Same as lh_inclusive but for right hand operator.


    For example::

        between_3_and_6 = between(3, 6)
        for i in range(10):
           print i, between_3_and_6(i)


        between_3_and_6 = between(3, 6, rh_inclusive=False)
        for i in range(10):
           print i, between_3_and_6(i)

    """
    if lh_inclusive and rh_inclusive:
        return lambda c: lh <= c <= rh
    elif lh_inclusive and not rh_inclusive:
        return lambda c: lh <= c < rh
    elif not lh_inclusive and rh_inclusive:
        return lambda c: lh < c <= rh
    else:
        return lambda c: lh < c < rh


def reverse(array, axes):
    """
    Reverse the array along the given axes.

    Args:

    * array
        The array to reverse
    * axes
        A single value or array of values of axes to reverse

    ::

        >>> import numpy as np
        >>> a = np.arange(24).reshape(2, 3, 4)
        >>> print a
        [[[ 0  1  2  3]
          [ 4  5  6  7]
          [ 8  9 10 11]]
        <BLANKLINE>
         [[12 13 14 15]
          [16 17 18 19]
          [20 21 22 23]]]
        >>> print reverse(a, 1)
        [[[ 8  9 10 11]
          [ 4  5  6  7]
          [ 0  1  2  3]]
        <BLANKLINE>
         [[20 21 22 23]
          [16 17 18 19]
          [12 13 14 15]]]
        >>> print reverse(a, [1, 2])
        [[[11 10  9  8]
          [ 7  6  5  4]
          [ 3  2  1  0]]
        <BLANKLINE>
         [[23 22 21 20]
          [19 18 17 16]
          [15 14 13 12]]]

    """
    index = [slice(None, None)] * array.ndim
    axes = np.array(axes, ndmin=1)
    if axes.ndim != 1:
        raise ValueError('Reverse was expecting a single axis or a 1d array '
                         'of axes, got %r' % axes)
    if np.min(axes) < 0 or np.max(axes) > array.ndim-1:
        raise ValueError('An axis value out of range for the number of '
                         'dimensions from the given array (%s) was received. '
                         'Got: %r' % (array.ndim, axes))

    for axis in axes:
        index[axis] = slice(None, None, -1)

    return array[tuple(index)]


def monotonic(array, strict=False, return_direction=False):
    """
    Return whether the given 1d array is monotonic.

    Note that, the array must not contain missing data.

    Kwargs:

    * strict (boolean)
        Flag to enable strict monotonic checking
    * return_direction (boolean)
        Flag to change return behaviour to return
        (monotonic_status, direction). Direction will be 1 for positive
        or -1 for negative. The direction is meaningless if the array is
        not monotonic.

    Returns:

    * monotonic_status (boolean)
        Whether the array was monotonic.

        If the return_direction flag was given then the returned value
        will be:
            ``(monotonic_status, direction)``

    """
    if array.ndim != 1 or len(array) <= 1:
        raise ValueError('The array to check must be 1 dimensional and have '
                         'more than 1 element.')

    if ma.isMaskedArray(array) and ma.count_masked(array) != 0:
        raise ValueError('The array to check contains missing data.')

    # Identify the directions of the largest/most-positive and
    # smallest/most-negative steps.
    d = np.diff(array)

    sign_max_d = np.sign(np.max(d))
    sign_min_d = np.sign(np.min(d))

    if strict:
        monotonic = sign_max_d == sign_min_d and sign_max_d != 0
    else:
        monotonic = (sign_min_d < 0 and sign_max_d <= 0) or \
                    (sign_max_d > 0 and sign_min_d >= 0) or \
                    (sign_min_d == sign_max_d == 0)

    if return_direction:
        if sign_max_d == 0:
            direction = sign_min_d
        else:
            direction = sign_max_d

        return monotonic, direction

    return monotonic


def column_slices_generator(full_slice, ndims):
    """
    Given a full slice full of tuples, return a dictionary mapping old
    data dimensions to new and a generator which gives the successive
    slices needed to index correctly (across columns).

    This routine deals with the special functionality for tuple based
    indexing e.g. [0, (3, 5), :, (1, 6, 8)] by first providing a slice
    which takes the non tuple slices out first i.e. [0, :, :, :] then
    subsequently iterates through each of the tuples taking out the
    appropriate slices i.e. [(3, 5), :, :] followed by [:, :, (1, 6, 8)]

    This method was developed as numpy does not support the direct
    approach of [(3, 5), : , (1, 6, 8)] for column based indexing.

    """
    list_of_slices = []

    # Map current dimensions to new dimensions, or None
    dimension_mapping = {None: None}
    _count_current_dim = 0
    for i, i_key in enumerate(full_slice):
        if isinstance(i_key, int):
            dimension_mapping[i] = None
        else:
            dimension_mapping[i] = _count_current_dim
            _count_current_dim += 1

    # Get all of the dimensions for which a tuple of indices were provided
    # (numpy.ndarrays are treated in the same way tuples in this case)
    is_tuple_style_index = lambda key: isinstance(key, tuple) or \
        (isinstance(key, np.ndarray) and key.ndim == 1)
    tuple_indices = [i for i, key in enumerate(full_slice)
                     if is_tuple_style_index(key)]

    # stg1: Take a copy of the full_slice specification, turning all tuples
    # into a full slice
    if tuple_indices != range(len(full_slice)):
        first_slice = list(full_slice)
        for tuple_index in tuple_indices:
            first_slice[tuple_index] = slice(None, None)
        # turn first_slice back into a tuple ready for indexing
        first_slice = tuple(first_slice)

        list_of_slices.append(first_slice)

    data_ndims = max(dimension_mapping.values())
    if data_ndims is not None:
        data_ndims += 1

    # stg2 iterate over each of the tuples
    for tuple_index in tuple_indices:
        # Create a list with the indices to span the whole data array that we
        # currently have
        spanning_slice_with_tuple = [slice(None, None)] * data_ndims
        # Replace the slice(None, None) with our current tuple
        spanning_slice_with_tuple[dimension_mapping[tuple_index]] = \
            full_slice[tuple_index]

        # if we just have [(0, 1)] turn it into [(0, 1), ...] as this is
        # Numpy's syntax.
        if len(spanning_slice_with_tuple) == 1:
            spanning_slice_with_tuple.append(Ellipsis)

        spanning_slice_with_tuple = tuple(spanning_slice_with_tuple)

        list_of_slices.append(spanning_slice_with_tuple)

    # return the dimension mapping and a generator of slices
    return dimension_mapping, iter(list_of_slices)


def _build_full_slice_given_keys(keys, ndim):
    """
    Given the keys passed to a __getitem__ call, build an equivalent
    tuple of keys which span ndims.

    """
    # Ensure that we always have a tuple of keys
    if not isinstance(keys, tuple):
        keys = tuple([keys])

    # catch the case where an extra Ellipsis has been provided which can be
    # discarded iff len(keys)-1 == ndim
    if len(keys)-1 == ndim and \
            Ellipsis in filter(lambda obj:
                               not isinstance(obj, np.ndarray), keys):
        keys = list(keys)
        is_ellipsis = [key is Ellipsis for key in keys]
        keys.pop(is_ellipsis.index(True))
        keys = tuple(keys)

    # for ndim >= 1 appending a ":" to the slice specification is allowable,
    # remove this now
    if len(keys) > ndim and ndim != 0 and keys[-1] == slice(None, None):
        keys = keys[:-1]

    if len(keys) > ndim:
        raise IndexError('More slices requested than dimensions. Requested '
                         '%r, but there were only %s dimensions.' %
                         (keys, ndim))

    # For each dimension get the slice which has been requested.
    # If no slice provided, then default to the whole dimension
    full_slice = [slice(None, None)] * ndim

    for i, key in enumerate(keys):
        if key is Ellipsis:

            # replace any subsequent Ellipsis objects in keys with
            # slice(None, None) as per Numpy
            keys = keys[:i] + tuple([slice(None, None) if key is Ellipsis
                                    else key for key in keys[i:]])

            # iterate over the remaining keys in reverse to fill in
            # the gaps from the right hand side
            for j, key in enumerate(keys[:i:-1]):
                full_slice[-j-1] = key

            # we've finished with i now so stop the iteration
            break
        else:
            full_slice[i] = key

    # remove any tuples on dimensions, turning them into numpy array's for
    # consistent behaviour
    full_slice = tuple([np.array(key, ndmin=1) if isinstance(key, tuple)
                        else key for key in full_slice])
    return full_slice


def _wrap_function_for_method(function, docstring=None):
    """
    Returns a wrapper function modified to be suitable for use as a
    method.

    The wrapper function renames the first argument as "self" and allows
    an alternative docstring, thus allowing the built-in help(...)
    routine to display appropriate output.

    """
    # Generate the Python source for the wrapper function.
    # NB. The first argument is replaced with "self".
    args, varargs, varkw, defaults = inspect.getargspec(function)
    if defaults is None:
        basic_args = ['self'] + args[1:]
        default_args = []
        simple_default_args = []
    else:
        cutoff = -len(defaults)
        basic_args = ['self'] + args[1:cutoff]
        default_args = ['%s=%r' % pair
                        for pair in zip(args[cutoff:], defaults)]
        simple_default_args = args[cutoff:]
    var_arg = [] if varargs is None else ['*' + varargs]
    var_kw = [] if varkw is None else ['**' + varkw]
    arg_source = ', '.join(basic_args + default_args + var_arg + var_kw)
    simple_arg_source = ', '.join(basic_args + simple_default_args +
                                  var_arg + var_kw)
    source = ('def %s(%s):\n    return function(%s)' %
              (function.func_name, arg_source, simple_arg_source))

    # Compile the wrapper function
    # NB. There's an outstanding bug with "exec" where the locals and globals
    # dictionaries must be the same if we're to get closure behaviour.
    my_locals = {'function': function}
    exec source in my_locals, my_locals

    # Update the docstring if required, and return the modified function
    wrapper = my_locals[function.func_name]
    if docstring is None:
        wrapper.__doc__ = function.__doc__
    else:
        wrapper.__doc__ = docstring
    return wrapper


class _MetaOrderedHashable(abc.ABCMeta):
    """
    A metaclass that ensures that non-abstract subclasses of _OrderedHashable
    without an explicit __init__ method are given a default __init__ method
    with the appropriate method signature.

    Also, an _init method is provided to allow subclasses with their own
    __init__ constructors to initialise their values via an explicit method
    signature.

    NB. This metaclass is used to construct the _OrderedHashable class as well
    as all its subclasses.

    """

    def __new__(cls, name, bases, namespace):
        # We only want to modify concrete classes that have defined the
        # "_names" property.
        if '_names' in namespace and \
                not isinstance(namespace['_names'], abc.abstractproperty):
            args = ', '.join(namespace['_names'])

            # Ensure the class has a constructor with explicit arguments.
            if '__init__' not in namespace:
                # Create a default __init__ method for the class
                method_source = ('def __init__(self, %s):\n '
                                 'self._init_from_tuple((%s,))' % (args, args))
                exec method_source in namespace

            # Ensure the class has a "helper constructor" with explicit
            # arguments.
            if '_init' not in namespace:
                # Create a default _init method for the class
                method_source = ('def _init(self, %s):\n '
                                 'self._init_from_tuple((%s,))' % (args, args))
                exec method_source in namespace

        return super(_MetaOrderedHashable, cls).__new__(
            cls, name, bases, namespace)


class _OrderedHashable(collections.Hashable):
    """
    Convenience class for creating "immutable", hashable, and ordered classes.

    Instance identity is defined by the specific list of attribute names
    declared in the abstract attribute "_names". Subclasses must declare the
    attribute "_names" as an iterable containing the names of all the
    attributes relevant to equality/hash-value/ordering.

    Initial values should be set by using ::
        self._init(self, value1, value2, ..)

    .. note::

        It's the responsibility of the subclass to ensure that the values of
        its attributes are themselves hashable.

    """

    # The metaclass adds default __init__ methods when appropriate.
    __metaclass__ = _MetaOrderedHashable

    @abc.abstractproperty
    def _names(self):
        """
        Override this attribute to declare the names of all the attributes
        relevant to the hash/comparison semantics.

        """
        pass

    def _init_from_tuple(self, values):
        for name, value in zip(self._names, values):
            object.__setattr__(self, name, value)

    def __repr__(self):
        class_name = type(self).__name__
        attributes = ', '.join('%s=%r' % (name, value)
                               for (name, value)
                               in zip(self._names, self._as_tuple()))
        return '%s(%s)' % (class_name, attributes)

    def _as_tuple(self):
        return tuple(getattr(self, name) for name in self._names)

    # Prevent attribute updates

    def __setattr__(self, name, value):
        raise AttributeError('Instances of %s are immutable' %
                             type(self).__name__)

    def __delattr__(self, name):
        raise AttributeError('Instances of %s are immutable' %
                             type(self).__name__)

    # Provide hash semantics

    def _identity(self):
        return self._as_tuple()

    def __hash__(self):
        return hash(self._identity())

    def __eq__(self, other):
        return (isinstance(other, type(self)) and
                self._identity() == other._identity())

    def __ne__(self, other):
        # Since we've defined __eq__ we should also define __ne__.
        return not self == other

    # Provide default ordering semantics

    def __cmp__(self, other):
        if isinstance(other, _OrderedHashable):
            result = cmp(self._identity(), other._identity())
        else:
            result = NotImplemented
        return result


def create_temp_filename(suffix=''):
    """Return a temporary file name.

    Args:

        * suffix  -  Optional filename extension.

    """
    temp_file = tempfile.mkstemp(suffix)
    os.close(temp_file[0])
    return temp_file[1]


def clip_string(the_str, clip_length=70, rider="..."):
    """
    Returns a clipped version of the string based on the specified clip
    length and whether or not any graceful clip points can be found.

    If the string to be clipped is shorter than the specified clip
    length, the original string is returned.

    If the string is longer than the clip length, a graceful point (a
    space character) after the clip length is searched for. If a
    graceful point is found the string is clipped at this point and the
    rider is added. If no graceful point can be found, then the string
    is clipped exactly where the user requested and the rider is added.

    Args:

    * the_str
        The string to be clipped
    * clip_length
        The length in characters that the input string should be clipped
        to. Defaults to a preconfigured value if not specified.
    * rider
        A series of characters appended at the end of the returned
        string to show it has been clipped. Defaults to a preconfigured
        value if not specified.

    Returns:
        The string clipped to the required length with a rider appended.
        If the clip length was greater than the orignal string, the
        original string is returned unaltered.

    """

    if clip_length >= len(the_str) or clip_length <= 0:
        return the_str
    else:
        if the_str[clip_length].isspace():
            return the_str[:clip_length] + rider
        else:
            first_part = the_str[:clip_length]
            remainder = the_str[clip_length:]

            # Try to find a graceful point at which to trim i.e. a space
            # If no graceful point can be found, then just trim where the user
            # specified by adding an empty slice of the remainder ( [:0] )
            termination_point = remainder.find(" ")
            if termination_point == -1:
                termination_point = 0

            return first_part + remainder[:termination_point] + rider


def ensure_array(a):
    if not isinstance(a, (np.ndarray, ma.core.MaskedArray)):
        a = np.array([a])
    return a


class _Timers(object):
    # See help for timers, below.

    def __init__(self):
        self.timers = {}

    def start(self, name, step_name):
        self.stop(name)
        timer = self.timers.setdefault(name, {})
        timer[step_name] = time.time()
        timer["active_timer_step"] = step_name

    def restart(self, name, step_name):
        self.stop(name)
        timer = self.timers.setdefault(name, {})
        timer[step_name] = time.time() - timer.get(step_name, 0)
        timer["active_timer_step"] = step_name

    def stop(self, name):
        if name in self.timers and "active_timer_step" in self.timers[name]:
            timer = self.timers[name]
            active = timer["active_timer_step"]
            start = timer[active]
            timer[active] = time.time() - start
        return self.get(name)

    def get(self, name):
        result = (name, [])
        if name in self.timers:
            result = (name, ", ".join(["'%s':%8.5f" % (k, v)
                                       for k, v in self.timers[name].items()
                                       if k != "active_timer_step"]))
        return result

    def reset(self, name):
        self.timers[name] = {}


timers = _Timers()
"""
Provides multiple named timers, each composed of multiple named steps.

Only one step is active at a time, so calling start(timer_name, step_name)
will stop the current step and start the new one.

Example Usage:

    from iris.util import timers

    def little_func(param):

        timers.restart("little func", "init")
        init()

        timers.restart("little func", "main")
        main(param)

        timers.restart("little func", "cleanup")
        cleanup()

        timers.stop("little func")

    def my_big_func():

        timers.start("big func", "input")
        input()

        timers.start("big func", "processing")
        little_func(123)
        little_func(456)

        timers.start("big func", "output")
        output()

        print timers.stop("big func")

        print timers.get("little func")

"""


def format_array(arr):
    """
    Returns the given array as a string, using the python builtin str
    function on a piecewise basis.

    Useful for xml representation of arrays.

    For customisations, use the :mod:`numpy.core.arrayprint` directly.

    """
    if arr.size > 85:
        summary_insert = "..., "
    else:
        summary_insert = ""
    ffunc = str
    return np.core.arrayprint._formatArray(arr, ffunc, len(arr.shape),
                                           max_line_len=50,
                                           next_line_prefix='\t\t',
                                           separator=', ', edge_items=3,
                                           summary_insert=summary_insert)[:-1]


def new_axis(src_cube, scalar_coord=None):
    """
    Create a new axis as the leading dimension of the cube, promoting a scalar
    coordinate if specified.

    Args:

    * src_cube (:class:`iris.cube.Cube`)
        Source cube on which to generate a new axis.

    Kwargs:

    * scalar_coord (:class:`iris.coord.Coord` or 'string')
        Scalar coordinate to promote to a dimension coordinate.

    Returns:
        A new :class:`iris.cube.Cube` instance with one extra leading dimension
        (length 1).

    For example::

        >>> cube.shape
        (360, 360)
        >>> ncube = iris.util.new_axis(cube, 'time')
        >>> ncube.shape
        (1, 360, 360)

    .. warning::

        Calling this method will trigger any deferred loading, causing the
        data array of the cube to be loaded into memory.

    """
    if scalar_coord is not None:
        scalar_coord = src_cube.coord(scalar_coord)

    # Indexing numpy arrays requires loading deferred data here returning a
    # copy of the data with a new leading dimension.
    new_cube = iris.cube.Cube(src_cube.data[None])
    new_cube.metadata = src_cube.metadata

    for coord in src_cube.aux_coords:
        if scalar_coord and scalar_coord == coord:
            dim_coord = iris.coords.DimCoord.from_coord(coord)
            new_cube.add_dim_coord(dim_coord, 0)
        else:
            dims = np.array(src_cube.coord_dims(coord)) + 1
            new_cube.add_aux_coord(coord.copy(), dims)

    for coord in src_cube.dim_coords:
        coord_dims = np.array(src_cube.coord_dims(coord)) + 1
        new_cube.add_dim_coord(coord.copy(), coord_dims)

    for factory in src_cube.aux_factories:
        new_cube.add_aux_factory(copy.deepcopy(factory))

    return new_cube


def as_compatible_shape(src_cube, target_cube):
    """
    Return a cube with added length one dimensions to match the dimensionality
    and dimension ordering of `target_cube`.

    This function can be used to add the dimensions that have been collapsed,
    aggregated or sliced out, promoting scalar coordinates to length one
    dimension coordinates where necessary. It operates by matching coordinate
    metadata to infer the dimensions that need modifying, so the provided
    cubes must have coordinates with the same metadata
    (see :class:`iris.coords.CoordDefn`).

    .. note:: This function will load and copy the data payload of `src_cube`.

    Args:

    * src_cube:
        An instance of :class:`iris.cube.Cube` with missing dimensions.

    * target_cube:
        An instance of :class:`iris.cube.Cube` with the desired dimensionality.

    Returns:
        A instance of :class:`iris.cube.Cube` with the same dimensionality as
        `target_cube` but with the data and coordinates from `src_cube`
        suitably reshaped to fit.

    """
    dim_mapping = {}
    for coord in target_cube.aux_coords + target_cube.dim_coords:
        dims = target_cube.coord_dims(coord)
        try:
            collapsed_dims = src_cube.coord_dims(coord)
        except iris.exceptions.CoordinateNotFoundError:
            continue
        if collapsed_dims:
            if len(collapsed_dims) == len(dims):
                for dim_from, dim_to in zip(dims, collapsed_dims):
                    dim_mapping[dim_from] = dim_to
        elif dims:
            for dim_from in dims:
                dim_mapping[dim_from] = None

    if len(dim_mapping) != target_cube.ndim:
        raise ValueError('Insufficient or conflicting coordinate '
                         'metadata. Cannot infer dimension mapping '
                         'to restore cube dimensions.')

    new_shape = [1] * target_cube.ndim
    for dim_from, dim_to in dim_mapping.iteritems():
        if dim_to is not None:
            new_shape[dim_from] = src_cube.shape[dim_to]

    new_data = src_cube.data.copy()

    # Transpose the data (if necessary) to prevent assignment of
    # new_shape doing anything except adding length one dims.
    order = [v for k, v in sorted(dim_mapping.items()) if v is not None]
    if order != sorted(order):
        new_order = [order.index(i) for i in range(len(order))]
        new_data = np.transpose(new_data, new_order).copy()

    new_cube = iris.cube.Cube(new_data.reshape(new_shape))
    new_cube.metadata = copy.deepcopy(src_cube.metadata)

    # Record a mapping from old coordinate IDs to new coordinates,
    # for subsequent use in creating updated aux_factories.
    coord_mapping = {}

    def add_coord(coord):
        """Closure used to add a suitably reshaped coord to new_cube."""
        dims = target_cube.coord_dims(coord)
        shape = [new_cube.shape[dim] for dim in dims]
        if not shape:
            shape = [1]
        points = coord.points.reshape(shape)
        bounds = None
        if coord.has_bounds():
            bounds = coord.bounds.reshape(shape + [coord.nbounds])
        new_coord = coord.copy(points=points, bounds=bounds)
        # If originally in dim_coords, add to dim_coords, otherwise add to
        # aux_coords.
        if target_cube.coords(coord, dim_coords=True):
            try:
                new_cube.add_dim_coord(new_coord, dims)
            except ValueError:
                # Catch cases where the coord is an AuxCoord and therefore
                # cannot be added to dim_coords.
                new_cube.add_aux_coord(new_coord, dims)
        else:
            new_cube.add_aux_coord(new_coord, dims)
        coord_mapping[id(coord)] = new_coord

    for coord in src_cube.aux_coords + src_cube.dim_coords:
        add_coord(coord)
    for factory in src_cube.aux_factories:
        new_cube.add_aux_factory(factory.updated(coord_mapping))

    return new_cube


def file_is_newer_than(result_path, source_paths):
    """
    Return whether the 'result' file has a later modification time than all of
    the 'source' files.

    If a stored result depends entirely on known 'sources', it need only be
    re-built when one of them changes.  This function can be used to test that
    by comparing file timestamps.

    Args:

    * result_path (string):
        The filepath of a file containing some derived result data.
    * source_paths (string or iterable of strings):
        The path(s) to the original datafiles used to make the result.  May
        include wildcards and '~' expansions (like Iris load paths), but not
        URIs.

    Returns:
        True if all the sources are older than the result, else False.

        If any of the file paths describes no existing files, an exception will
        be raised.

    .. note::
        There are obvious caveats to using file timestamps for this, as correct
        usage depends on how the sources might change.  For example, a file
        could be replaced by one of the same name, but an older timestamp.

        If wildcards and '~' expansions are used, this introduces even more
        uncertainty, as then you cannot even be sure that the resulting list of
        file names is the same as the originals.  For example, some files may
        have been deleted or others added.

    .. note::
        The result file may often be a :mod:`pickle` file.  In that case, it
        also depends on the relevant module sources, so extra caution is
        required.  Ideally, an additional check on iris.__version__ is advised.

    """
    # Accept a string as a single source path
    if isinstance(source_paths, basestring):
        source_paths = [source_paths]
    # Fix our chosen timestamp function
    file_date = os.path.getmtime
    # Get the 'result file' time
    result_timestamp = file_date(result_path)
    # Get all source filepaths, with normal Iris.io load helper function
    source_file_paths = iris.io.expand_filespecs(source_paths)
    # Compare each filetime, for each spec, with the 'result time'
    for path in source_file_paths:
        source_timestamp = file_date(path)
        if source_timestamp >= result_timestamp:
            return False
    return True


def is_regular(coord):
    """Determine if the given coord is regular."""
    try:
        regular_step(coord)
    except iris.exceptions.CoordinateNotRegularError:
        return False
    except (TypeError, ValueError):
        return False
    return True


def regular_step(coord):
    """Return the regular step from a coord or fail."""
    if coord.ndim != 1:
        raise iris.exceptions.CoordinateMultiDimError("Expected 1D coord")
    if coord.shape[0] < 2:
        raise ValueError("Expected a non-scalar coord")

    diffs = coord.points[1:] - coord.points[:-1]
    avdiff = np.mean(diffs)
    if not np.allclose(diffs, avdiff, rtol=0.001):
        # TODO: This value is set for test_analysis to pass...
        msg = "Coord %s is not regular" % coord.name()
        raise iris.exceptions.CoordinateNotRegularError(msg)
    return avdiff.astype(coord.points.dtype)


def unify_time_units(cubes):
    """
    Performs an in-place conversion of the time units of all time coords in the
    cubes in a given iterable. One common epoch is defined for each calendar
    found in the cubes to prevent units being defined with inconsistencies
    between epoch and calendar.

    Each epoch is defined from the first suitable time coordinate found in the
    input cubes.

    Arg:

    * cubes:
        An iterable containing :class:`iris.cube.Cube` instances.

    """
    epochs = {}

    for cube in cubes:
        for time_coord in cube.coords():
            if time_coord.units.is_time_reference():
                epoch = epochs.setdefault(time_coord.units.calendar,
                                          time_coord.units.origin)
                new_unit = iris.unit.Unit(epoch, time_coord.units.calendar)
                time_coord.convert_units(new_unit)

########NEW FILE########
__FILENAME__ = _concatenate
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Automatic concatenation of multiple cubes over one or more existing dimensions.

.. warning::

    Currently, the :func:`concatenate` routine will load the data payload
    of all cubes passed to it.

    This restriction will be relaxed in a future release.

"""

from collections import defaultdict, namedtuple

import numpy as np
import numpy.ma as ma

import iris.coords
import iris.cube
from iris.util import guess_coord_axis, array_equal, unify_time_units


#
# TODO:
#
#   * Deal with scalar coordinate promotion to a new dimension
#     e.g. promote scalar z coordinate in 2D cube (y:m, x:n) to
#     give the similar 3D cube (z:1, y:m, x:n). These two types
#     of cubes are one and the same, and as such should concatenate
#     together.
#
#   * Cope with auxiliary coordinate factories.
#
#   * Don't load the cube data payload.
#
#   * Deal with anonymous dimensions.
#
#   * Allow concatentation over a user specified dimension.
#


# Restrict the names imported from this namespace.
__all__ = ['concatenate']

# Direction of dimension coordinate value order.
_CONSTANT = 0
_DECREASING = -1
_INCREASING = 1


class _CoordAndDims(namedtuple('CoordAndDims',
                               ['coord', 'dims'])):
    """
    Container for a coordinate and the associated data dimension(s)
    spanned over a :class:`iris.cube.Cube`.

    Args:

    * coord:
        A :class:`iris.coords.DimCoord` or :class:`iris.coords.AuxCoord`
        coordinate instance.

    * dims:
        A tuple of the data dimension(s) spanned by the coordinate.

    """


class _CoordMetaData(namedtuple('CoordMetaData',
                                ['defn', 'dims', 'points_dtype',
                                 'bounds_dtype', 'kwargs'])):
    """
    Container for the metadata that defines a dimension or auxiliary
    coordinate.

    Args:

    * defn:
        The :class:`iris.coords.CoordDefn` metadata that represents a
        coordinate.

    * dims:
        The dimension(s) associated with the coordinate.

    * points_dtype:
        The points data :class:`np.dtype` of an associated coordinate.

    * bounds_dtype:
        The bounds data :class:`np.dtype` of an associated coordinate.

    * kwargs:
        A dictionary of key/value pairs required to define a coordinate.

    """
    def __new__(cls, coord, dims):
        """
        Create a new :class:`_CoordMetaData` instance.

        Args:

        * coord:
            The :class:`iris.coord.DimCoord` or :class:`iris.coord.AuxCoord`.

        * dims:
            The dimension(s) associated with the coordinate.

        Returns:
            The new class instance.

        """
        defn = coord._as_defn()
        points_dtype = coord.points.dtype
        bounds_dtype = coord.bounds.dtype if coord.bounds is not None \
            else None
        kwargs = {}
        # Add circular flag metadata for dimensional coordinates.
        if hasattr(coord, 'circular'):
            kwargs['circular'] = coord.circular
        if isinstance(coord, iris.coords.DimCoord):
            # Mix the monotonic ordering into the metadata.
            if coord.points[0] == coord.points[-1]:
                order = _CONSTANT
            elif coord.points[-1] > coord.points[0]:
                order = _INCREASING
            else:
                order = _DECREASING
            kwargs['order'] = order
        metadata = super(_CoordMetaData, cls).__new__(cls, defn, dims,
                                                      points_dtype,
                                                      bounds_dtype,
                                                      kwargs)
        return metadata


class _SkeletonCube(namedtuple('SkeletonCube',
                               ['signature', 'data'])):
    """
    Basis of a source-cube, containing the associated coordinate metadata,
    coordinates and cube data payload.

    Args:

    * signature:
        The :class:`_CoordSignature` of an associated source-cube.

    * data:
        The data payload of an associated :class:`iris.cube.Cube` source-cube.

    """


class _Extent(namedtuple('Extent',
                         ['min', 'max'])):
    """
    Container representing the limits of a one-dimensional extent/range.

    Args:

    * min:
        The minimum value of the extent.

    * max:
        The maximum value of the extent.

    """


class _CoordExtent(namedtuple('CoordExtent',
                              ['points', 'bounds'])):
    """
    Container representing the points and bounds extent of a one dimensional
    coordinate.

    Args:

    * points:
        The :class:`_Extent` of the coordinate point values.

    * bounds:
        A list containing the :class:`_Extent` of the coordinate lower
        bound and the upper bound. Defaults to None if no associated
        bounds exist for the coordinate.

    """


def concatenate(cubes):
    """
    Concatenate the provided cubes over common existing dimensions.

    Args:

    * cubes:
        An iterable containing one or more :class:`iris.cube.Cube` instances
        to be concatenated together.

    Returns:
        A :class:`iris.cube.CubeList` of concatenated :class:`iris.cube.Cube`
        instances.

    .. warning::

        This routine will load your data payload!

    """
    proto_cubes_by_name = defaultdict(list)
    # Initialise the nominated axis (dimension) of concatenation
    # which requires to be negotiated.
    axis = None

    # Register each cube with its appropriate proto-cube.
    for cube in cubes:
        # TODO: Remove this when new deferred data mechanism is available.
        # Avoid deferred data/data manager issues, and load the cube data!
        cube.data

        name = cube.standard_name or cube.long_name
        proto_cubes = proto_cubes_by_name[name]
        registered = False

        # Register cube with an existing proto-cube.
        for proto_cube in proto_cubes:
            registered = proto_cube.register(cube, axis)
            if registered:
                axis = proto_cube.axis
                break

        # Create a new proto-cube for an unregistered cube.
        if not registered:
            proto_cubes.append(_ProtoCube(cube))

    # Construct a concatenated cube from each of the proto-cubes.
    concatenated_cubes = iris.cube.CubeList()

    for name in sorted(proto_cubes_by_name):
        for proto_cube in proto_cubes_by_name[name]:
            # Construct the concatenated cube.
            concatenated_cubes.append(proto_cube.concatenate())

    # Perform concatenation until we've reached an equilibrium.
    count = len(concatenated_cubes)
    if count != 1 and count != len(cubes):
        concatenated_cubes = concatenate(concatenated_cubes)

    return concatenated_cubes


class _CubeSignature(object):
    """
    Template for identifying a specific type of :class:`iris.cube.Cube` based
    on its metadata and coordinates.

    """
    def __init__(self, cube):
        """
        Represents the cube metadata and associated coordinate metadata that
        allows suitable cubes for concatenation to be identified.

        Args:

        * cube:
            The :class:`iris.cube.Cube` source-cube.

        """
        self.aux_coords_and_dims = []
        self.aux_metadata = []
        self.dim_coords = cube.dim_coords
        self.dim_metadata = []
        self.ndim = cube.ndim
        self.scalar_coords = []

        # Determine whether there are any anonymous cube dimensions.
        covered = set(cube.coord_dims(coord)[0] for coord in self.dim_coords)
        self.anonymous = covered != set(range(self.ndim))

        self.defn = cube.metadata
        self.data_type = cube.data.dtype

        #
        # Collate the dimension coordinate metadata.
        #
        for coord in self.dim_coords:
            metadata = _CoordMetaData(coord, cube.coord_dims(coord))
            self.dim_metadata.append(metadata)

        #
        # Collate the auxiliary coordinate metadata and scalar coordinates.
        #
        axes = dict(T=0, Z=1, Y=2, X=3)
        # Coordinate sort function - by guessed coordinate axis, then
        # by coordinate definition, then by dimensions, in ascending order.
        key_func = lambda coord: (axes.get(guess_coord_axis(coord),
                                           len(axes) + 1),
                                  coord._as_defn(),
                                  cube.coord_dims(coord))

        for coord in sorted(cube.aux_coords, key=key_func):
            dims = cube.coord_dims(coord)
            if dims:
                metadata = _CoordMetaData(coord, dims)
                self.aux_metadata.append(metadata)
                coord_and_dims = _CoordAndDims(coord, tuple(dims))
                self.aux_coords_and_dims.append(coord_and_dims)
            else:
                self.scalar_coords.append(coord)

    def __eq__(self, other):
        result = NotImplemented

        if isinstance(other, _CubeSignature):
            # Only concatenate with fully described cubes.
            if self.anonymous or other.anonymous:
                result = False
            else:
                result = self.aux_metadata == other.aux_metadata and \
                    self.data_type == other.data_type and \
                    self.defn == other.defn and \
                    self.dim_metadata == other.dim_metadata and \
                    self.ndim == other.ndim and \
                    self.scalar_coords == other.scalar_coords

        return result

    def __ne__(self, other):
        result = self.__eq__(other)
        if result is not NotImplemented:
            result = not result
        return result


class _CoordSignature(object):
    """
    Template for identifying a specific type of :class:`iris.cube.Cube` based
    on its coordinates.

    """
    def __init__(self, cube_signature):
        """
        Represents the coordinate metadata required to identify suitable
        non-overlapping :class:`iris.cube.Cube` source-cubes for
        concatenation over a common single dimension.

        Args:

        * cube_signature:
            The :class:`_CubeSignature` that defines the source-cube.

        """
        self.aux_coords_and_dims = cube_signature.aux_coords_and_dims
        self.dim_coords = cube_signature.dim_coords
        self.dim_extents = []
        self.dim_order = [metadata.kwargs['order']
                          for metadata in cube_signature.dim_metadata]

        # Calculate the extents for each dimensional coordinate.
        self._calculate_extents()

    @staticmethod
    def _cmp(coord, other):
        """
        Compare the coordinates for concatenation compatibility.

        Returns:
            A boolean tuple pair of whether the coordinates are compatible,
            and whether they represent a candidate axis of concatenation.

        """
        # A candidate axis must have non-identical coordinate points.
        candidate_axis = not array_equal(coord.points, other.points)

        if candidate_axis:
            # Ensure both have equal availability of bounds.
            result = (coord.bounds is None) == (other.bounds is None)
        else:
            if coord.bounds is not None and other.bounds is not None:
                # Ensure equality of bounds.
                result = array_equal(coord.bounds, other.bounds)
            else:
                # Ensure both have equal availability of bounds.
                result = coord.bounds is None and other.bounds is None

        return result, candidate_axis

    def candidate_axis(self, other):
        """
        Determine the candidate axis of concatenation with the
        given coordinate signature.

        If a candidate axis is found, then the coordinate
        signatures are compatible.

        Args:

        * other:
            The :class:`_CoordSignature`

        Returns:
            None if no single candidate axis exists, otherwise
            the candidate axis of concatenation.

        """
        result = False
        candidate_axes = []

        # Compare dimension coordinates.
        for dim, coord in enumerate(self.dim_coords):
            result, candidate_axis = self._cmp(coord,
                                               other.dim_coords[dim])
            if not result:
                break
            if candidate_axis:
                candidate_axes.append(dim)

        # Only permit one degree of dimensional freedom when
        # determining the candidate axis of concatenation.
        if result and len(candidate_axes) == 1:
            result = candidate_axes[0]
        else:
            result = None

        return result

    def _calculate_extents(self):
        """
        Calculate the extent over each dimension coordinates points and bounds.

        """
        self.dim_extents = []
        for coord, order in zip(self.dim_coords, self.dim_order):
            if order == _CONSTANT or order == _INCREASING:
                points = _Extent(coord.points[0], coord.points[-1])
                if coord.bounds is not None:
                    bounds = (_Extent(coord.bounds[0, 0], coord.bounds[-1, 0]),
                              _Extent(coord.bounds[0, 1], coord.bounds[-1, 1]))
                else:
                    bounds = None
            else:
                # The order must be decreasing ...
                points = _Extent(coord.points[-1], coord.points[0])
                if coord.bounds is not None:
                    bounds = (_Extent(coord.bounds[-1, 0], coord.bounds[0, 0]),
                              _Extent(coord.bounds[-1, 1], coord.bounds[0, 1]))
                else:
                    bounds = None

            self.dim_extents.append(_CoordExtent(points, bounds))


class _ProtoCube(object):
    """
    Framework for concatenating multiple source-cubes over one
    common dimension.

    """
    def __init__(self, cube):
        """
        Create a new _ProtoCube from the given cube and record the cube
        as a source-cube.

        Args:

        * cube:
            Source :class:`iris.cube.Cube` of the :class:`_ProtoCube`.

        """
        # Cache the source-cube of this proto-cube.
        self._cube = cube

        # The cube signature is a combination of cube and coordinate
        # metadata that defines this proto-cube.
        self._cube_signature = _CubeSignature(cube)
        self._data_is_masked = ma.isMaskedArray(cube.data)

        # The coordinate signature allows suitable non-overlapping
        # source-cubes to be identified.
        self._coord_signature = _CoordSignature(self._cube_signature)

        # The list of source-cubes relevant to this proto-cube.
        self._skeletons = []
        self._add_skeleton(self._coord_signature, cube.data)

        # The nominated axis of concatenation.
        self._axis = None

    @property
    def axis(self):
        """Return the nominated dimension of concatenation."""

        return self._axis

    def concatenate(self):
        """
        Concatenates all the source-cubes registered with the
        :class:`_ProtoCube` over the nominated common dimension.

        Returns:
            The concatenated :class:`iris.cube.Cube`.

        """
        if len(self._skeletons) > 1:
            skeletons = self._skeletons
            order = self._coord_signature.dim_order[self.axis]
            cube_signature = self._cube_signature

            # Sequence the skeleton segments into the correct order
            # pending concatenation.
            key_func = lambda skeleton: skeleton.signature.dim_extents
            skeletons.sort(key=key_func,
                           reverse=(order == _DECREASING))

            # Concatenate the new dimension coordinate.
            dim_coords_and_dims = self._build_dim_coordinates()

            # Concatenate the new auxiliary coordinates.
            aux_coords_and_dims = self._build_aux_coordinates()

            # Concatenate the new data payload.
            data = self._build_data()

            # Build the new cube.
            kwargs = cube_signature.defn._asdict()
            cube = iris.cube.Cube(data,
                                  dim_coords_and_dims=dim_coords_and_dims,
                                  aux_coords_and_dims=aux_coords_and_dims,
                                  **kwargs)
        else:
            # There are no other source-cubes to concatenate
            # with this proto-cube.
            cube = self._cube

        return cube

    def register(self, cube, axis=None):
        """
        Determine whether the given source-cube is suitable for concatenation
        with this :class:`_ProtoCube`.

        Args:

        * cube:
            The :class:`iris.cube.Cube` source-cube candidate for
            concatenation.

        Kwargs:

        * axis:
            Seed the dimension of concatenation for the :class:`_ProtoCube`
            rather than rely on negotiation with source-cubes.

        Returns:
            Boolean.

        """
        # Verify and assert the nominated axis.
        if axis is not None and self.axis is not None and self.axis != axis:
            msg = 'Nominated axis [{}] is not equal ' \
                'to negotiated axis [{}]'.format(axis, self.axis)
            raise ValueError(msg)

        # Check for compatible cube signatures.
        cube_signature = _CubeSignature(cube)
        match = self._cube_signature == cube_signature

        # Check for compatible coordinate signatures.
        if match:
            coord_signature = _CoordSignature(cube_signature)
            candidate_axis = self._coord_signature.candidate_axis(
                coord_signature)
            match = candidate_axis is not None and \
                (candidate_axis == axis or axis is None)

        # Check for compatible coordinate extents.
        if match:
            match = self._sequence(coord_signature.dim_extents[candidate_axis],
                                   candidate_axis)

        if match:
            # Register the cube as a source-cube for this proto-cube.
            self._add_skeleton(coord_signature, cube.data)
            self._data_is_masked |= ma.isMaskedArray(cube.data)
            # Declare the nominated axis of concatenation.
            self._axis = candidate_axis

        return match

    def _add_skeleton(self, coord_signature, data):
        """
        Create and add the source-cube skeleton to the
        :class:`_ProtoCube`.

        Args:

        * coord_signature:
            The :class:`_CoordSignature` of the associated
            given source-cube.

        * data:
            The data payload of an associated :class:`iris.cube.Cube`
            source-cube.

        """
        skeleton = _SkeletonCube(coord_signature, data)
        self._skeletons.append(skeleton)

    def _build_aux_coordinates(self):
        """
        Generate the auxiliary coordinates with associated dimension(s)
        mapping for the new concatenated cube.

        Returns:
            A list of auxiliary coordinates and dimension(s) tuple pairs.

        """
        # Setup convenience hooks.
        skeletons = self._skeletons
        cube_signature = self._cube_signature

        aux_coords_and_dims = []

        # Generate all the auxiliary coordinates for the new concatenated cube.
        for i, (coord, dims) in enumerate(cube_signature.aux_coords_and_dims):
            # Check whether the coordinate spans the nominated
            # dimension of concatenation.
            if self.axis in dims:
                # Concatenate the points together.
                dim = dims.index(self.axis)
                points = [skton.signature.aux_coords_and_dims[i].coord.points
                          for skton in skeletons]
                points = np.concatenate(tuple(points), axis=dim)

                # Concatenate the bounds together.
                bnds = None
                if coord.has_bounds():
                    bnds = [skton.signature.aux_coords_and_dims[i].coord.bounds
                            for skton in skeletons]
                    bnds = np.concatenate(tuple(bnds), axis=dim)

                # Generate the associated coordinate metadata.
                kwargs = cube_signature.aux_metadata[i].defn._asdict()

                # Build the concatenated coordinate.
                if isinstance(coord, iris.coords.AuxCoord):
                    coord = iris.coords.AuxCoord(points, bounds=bnds, **kwargs)
                else:
                    # Attempt to create a DimCoord, otherwise default to
                    # an AuxCoord on failure.
                    try:
                        coord = iris.coords.DimCoord(points, bounds=bnds,
                                                     **kwargs)
                    except ValueError:
                        coord = iris.coords.AuxCoord(points, bounds=bnds,
                                                     **kwargs)

            aux_coords_and_dims.append((coord.copy(), dims))

        # Generate all the scalar coordinates for the new concatenated cube.
        for coord in cube_signature.scalar_coords:
            aux_coords_and_dims.append((coord.copy(), ()))

        return aux_coords_and_dims

    def _build_data(self):
        """
        Generate the data payload for the new concatenated cube.

        Returns:
            The concatenated :class:`iris.cube.Cube` data payload.

        """
        skeletons = self._skeletons
        data = [skeleton.data for skeleton in skeletons]

        if self._data_is_masked:
            data = ma.concatenate(tuple(data), axis=self.axis)
        else:
            data = np.concatenate(tuple(data), axis=self.axis)

        return data

    def _build_dim_coordinates(self):
        """
        Generate the dimension coordinates with associated dimension
        mapping for the new concatenated cube.

        Return:
            A list of dimension coordinate and dimension tuple pairs.

        """
        # Setup convenience hooks.
        skeletons = self._skeletons
        axis = self.axis
        defn = self._cube_signature.dim_metadata[axis].defn
        circular = self._cube_signature.dim_metadata[axis].kwargs['circular']

        # Concatenate the points together for the nominated dimension.
        points = [skeleton.signature.dim_coords[axis].points
                  for skeleton in skeletons]
        points = np.concatenate(tuple(points))

        # Concatenate the bounds together for the nominated dimension.
        bounds = None
        if self._cube_signature.dim_coords[axis].has_bounds():
            bounds = [skeleton.signature.dim_coords[axis].bounds
                      for skeleton in skeletons]
            bounds = np.concatenate(tuple(bounds))

        # Populate the new dimension coordinate with the concatenated
        # points, bounds and associated metadata.
        kwargs = defn._asdict()
        kwargs['circular'] = circular
        dim_coord = iris.coords.DimCoord(points, bounds=bounds, **kwargs)

        # Generate all the dimension coordinates for the new concatenated cube.
        dim_coords_and_dims = []
        for dim, coord in enumerate(self._cube_signature.dim_coords):
            if dim == axis:
                dim_coords_and_dims.append((dim_coord, dim))
            else:
                dim_coords_and_dims.append((coord.copy(), dim))

        return dim_coords_and_dims

    def _sequence(self, extent, axis):
        """
        Determine whether the given extent can be sequenced along with
        all the extents of the source-cubes already registered with
        this :class:`_ProtoCube` into non-overlapping segments for the
        given axis.

        Args:

        * extent:
            The :class:`_CoordExtent` of the candidate source-cube.

        * axis:
            The candidate axis of concatenation.

        Returns:
            Boolean.

        """
        result = True

        # Add the new extent to the current extents collection.
        dim_extents = [skeleton.signature.dim_extents[axis]
                       for skeleton in self._skeletons]
        dim_extents.append(extent)

        # Sort into the appropriate dimension order.
        order = self._coord_signature.dim_order[axis]
        dim_extents.sort(reverse=(order == _DECREASING))

        # Ensure that the extents don't overlap.
        if len(dim_extents) > 1:
            for i, extent in enumerate(dim_extents[1:]):
                # Check the points - must be strictly monotonic.
                if order == _DECREASING:
                    left = dim_extents[i].points.min
                    right = extent.points.max
                else:
                    left = dim_extents[i].points.max
                    right = extent.points.min

                if left >= right:
                    result = False
                    break

                # Check the bounds - must be strictly monotonic.
                if extent.bounds is not None:
                    if order == _DECREASING:
                        left_0 = dim_extents[i].bounds[0].min
                        left_1 = dim_extents[1].bounds[1].min
                        right_0 = extent.bounds[0].max
                        right_1 = extent.bounds[1].max
                    else:
                        left_0 = dim_extents[i].bounds[0].max
                        left_1 = dim_extents[i].bounds[1].max
                        right_0 = extent.bounds[0].min
                        right_1 = extent.bounds[1].min

                    lower_bound_fail = left_0 >= right_0
                    upper_bound_fail = left_1 >= right_1

                    if lower_bound_fail or upper_bound_fail:
                        result = False
                        break

        return result

########NEW FILE########
__FILENAME__ = _constraints
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Provides objects for building up expressions useful for pattern matching.

"""
import collections
import operator
import warnings

import numpy as np

import iris.coords
import iris.exceptions


class Constraint(object):
    """
    Constraints are the mechanism by which cubes can be pattern matched and
    filtered according to specific criteria.

    Once a constraint has been defined, it can be applied to cubes using the
    :meth:`Constraint.extract` method.

    """
    def __init__(self, name=None, cube_func=None, coord_values=None, **kwargs):
        """
        Creates a new instance of a Constraint which can be used for filtering
        cube loading or cube list extraction.

        Args:

        * name:   string or None
            If a string, it is used as the name to match against Cube.name().
        * cube_func:   callable or None
            If a callable, it must accept a Cube as its first and only argument
            and return either True or False.
        * coord_values:   dict or None
            If a dict, it must map coordinate name to the condition on the
            associated coordinate.
        * `**kwargs`:
            The remaining keyword arguments are converted to coordinate
            constraints. The name of the argument gives the name of a
            coordinate, and the value of the argument is the condition to meet
            on that coordinate::

                Constraint(model_level_number=10)

            Coordinate level constraints can be of several types:

            * **string, int or float** - the value of the coordinate to match.
              e.g. ``model_level_number=10``

            * **list of values** - the possible values that the coordinate may
              have to match. e.g. ``model_level_number=[10, 12]``

            * **callable** - a function which accepts a
              :class:`iris.coords.Cell` instance as its first and only argument
              returning True or False if the value of the Cell is desired.
              e.g. ``model_level_number=lambda cell: 5 < cell < 10``

        The :ref:`user guide <loading_iris_cubes>` covers cube much of
        constraining in detail, however an example which uses all of the
        features of this class is given here for completeness::

            Constraint(name='air_potential_temperature',
                       cube_func=lambda cube: cube.units == 'kelvin',
                       coord_values={'latitude':lambda cell: 0 < cell < 90},
                       model_level_number=[10, 12])
                       & Constraint(ensemble_member=2)

        Constraint filtering is performed at the cell level.
        For further details on how cell comparisons are performed see
        :class:`iris.coords.Cell`.

        """
        if not (name is None or isinstance(name, basestring)):
            raise TypeError('name must be None or string, got %r' % name)
        if not (cube_func is None or callable(cube_func)):
            raise TypeError('cube_func must be None or callable, got %r'
                            % cube_func)
        if not (coord_values is None or isinstance(coord_values,
                                                   collections.Mapping)):
            raise TypeError('coord_values must be None or a '
                            'collections.Mapping, got %r' % coord_values)

        coord_values = coord_values or {}
        duplicate_keys = coord_values.viewkeys() & kwargs.viewkeys()
        if duplicate_keys:
            raise ValueError('Duplicate coordinate conditions specified for: '
                             '%s' % list(duplicate_keys))

        self._name = name
        self._cube_func = cube_func

        self._coord_values = coord_values.copy()
        self._coord_values.update(kwargs)

        self._coord_constraints = []
        for coord_name, coord_thing in self._coord_values.items():
            self._coord_constraints.append(_CoordConstraint(coord_name,
                                                            coord_thing))

    def __repr__(self):
        args = []
        if self._name:
            args.append(('name', self._name))
        if self._cube_func:
            args.append(('cube_func', self._cube_func))
        if self._coord_values:
            args.append(('coord_values', self._coord_values))
        return 'Constraint(%s)' % ', '.join('%s=%r' % (k, v) for k, v in args)

    def _coordless_match(self, cube):
        """
        Return whether this constraint matches the given cube when not
        taking coordinates into account.

        """
        match = True
        if self._name:
            match = self._name == cube.name()
        if match and self._cube_func:
            match = self._cube_func(cube)
        return match

    def extract(self, cube):
        """
        Return the subset of the given cube which matches this constraint,
        else return None.

        """
        resultant_CIM = self._CIM_extract(cube)
        slice_tuple = resultant_CIM.as_slice()
        result = None
        if slice_tuple is not None:
            # Slicing the cube is an expensive operation.
            if all([item == slice(None) for item in slice_tuple]):
                # Don't perform a full slice, just return the cube.
                result = cube
            else:
                # Performing the partial slice.
                result = cube[slice_tuple]
        return result

    def _CIM_extract(self, cube):
        # Returns _ColumnIndexManager
        resultant_CIM = _ColumnIndexManager(len(cube.shape))

        if not self._coordless_match(cube):
            resultant_CIM.all_false()
        else:
            for coord_constraint in self._coord_constraints:
                resultant_CIM = resultant_CIM & coord_constraint.extract(cube)

        return resultant_CIM

    def __and__(self, other):
        return ConstraintCombination(self, other, operator.__and__)

    def __rand__(self, other):
        return ConstraintCombination(other, self, operator.__and__)


class ConstraintCombination(Constraint):
    """Represents the binary combination of two Constraint instances."""
    def __init__(self, lhs, rhs, operator):
        """
        A ConstraintCombination instance is created by providing two
        Constraint instances and the appropriate :mod:`operator`.

        """
        try:
            lhs_constraint = as_constraint(lhs)
            rhs_constraint = as_constraint(rhs)
        except TypeError:
            raise TypeError('Can only combine Constraint instances, '
                            'got: %s and %s' % (type(lhs), type(rhs)))
        self.lhs = lhs_constraint
        self.rhs = rhs_constraint
        self.operator = operator

    def _coordless_match(self, cube):
        return self.operator(self.lhs._coordless_match(cube),
                             self.rhs._coordless_match(cube))

    def __repr__(self):
        return 'ConstraintCombination(%r, %r, %r)' % (self.lhs, self.rhs,
                                                      self.operator)

    def _CIM_extract(self, cube):
        return self.operator(self.lhs._CIM_extract(cube),
                             self.rhs._CIM_extract(cube))


class _CoordConstraint(object):
    """Represents the atomic elements which might build up a Constraint."""
    def __init__(self, coord_name, coord_thing):
        """
        Create a coordinate constraint given the coordinate name and a
        thing to compare it with.

        Arguments:

        * coord_name  -  string
            The name of the coordinate to constrain
        * coord_thing
            The object to compare

        """
        self.coord_name = coord_name
        self._coord_thing = coord_thing

    def __repr__(self):
        return '_CoordConstraint(%r, %r)' % (self.coord_name,
                                             self._coord_thing)

    def extract(self, cube):
        """
        Returns the the column based indices of the given cube which
        match the constraint.

        """
        cube_cim = _ColumnIndexManager(len(cube.shape))
        try:
            coord = cube.coord(self.coord_name)
        except iris.exceptions.CoordinateNotFoundError:
            cube_cim.all_false()
            return cube_cim
        dims = cube.coord_dims(coord)
        if len(dims) > 1:
            msg = 'Cannot apply constraints to multidimensional coordinates'
            raise iris.exceptions.CoordinateMultiDimError(msg)

        try_quick = False
        if callable(self._coord_thing):
            call_func = self._coord_thing
        elif (isinstance(self._coord_thing, collections.Iterable) and
                not isinstance(self._coord_thing,
                               (basestring, iris.coords.Cell))):
            call_func = lambda cell: cell in list(self._coord_thing)
        else:
            call_func = lambda c: c == self._coord_thing
            try_quick = (isinstance(coord, iris.coords.DimCoord) and
                         not isinstance(self._coord_thing, iris.coords.Cell))

        # Simple, yet dramatic, optimisation for the monotonic case.
        if try_quick:
            try:
                i = coord.nearest_neighbour_index(self._coord_thing)
            except TypeError:
                try_quick = False
        if try_quick:
            r = np.zeros(coord.shape, dtype=np.bool)
            if coord.cell(i) == self._coord_thing:
                r[i] = True
        else:
            r = np.array([call_func(cell) for cell in coord.cells()])
        if dims:
            cube_cim[dims[0]] = r
        elif not all(r):
            cube_cim.all_false()
        return cube_cim


class _ColumnIndexManager(object):
    """
    A class to represent column aligned slices which can be operated on
    using ``&``, ``|`` or ``^``.

    ::

        # 4 Dimensional slices
        import numpy as np
        cim = _ColumnIndexManager(4)
        cim[1] = np.array([3, 4, 5]) > 3
        print cim.as_slice()

    """
    def __init__(self, ndims):
        """
        A _ColumnIndexManager is always created to span the given
        number of dimensions.

        """
        self._column_arrays = [True] * ndims
        self.ndims = ndims

    def __and__(self, other):
        return self._bitwise_operator(other, operator.__and__)

    def __or__(self, other):
        return self._bitwise_operator(other, operator.__or__)

    def __xor__(self, other):
        return self._bitwise_operator(other, operator.__xor__)

    def _bitwise_operator(self, other, operator):
        if not isinstance(other, _ColumnIndexManager):
            return NotImplemented

        if self.ndims != other.ndims:
            raise ValueError('Cannot do %s for %r and %r as they have a '
                             'different number of dimensions.' % operator)
        r = _ColumnIndexManager(self.ndims)
        # iterate over each dimension an combine appropriately
        for i, (lhs, rhs) in enumerate(zip(self, other)):
            r[i] = operator(lhs, rhs)
        return r

    def all_false(self):
        """Turn all slices into False."""
        for i in range(self.ndims):
            self[i] = False

    def __getitem__(self, key):
        return self._column_arrays[key]

    def __setitem__(self, key, value):
        is_vector = isinstance(value, np.ndarray) and value.ndim == 1
        if is_vector or isinstance(value, bool):
            self._column_arrays[key] = value
        else:
            raise TypeError('Expecting value to be a 1 dimensional numpy array'
                            ', or a boolean. Got %s' % (type(value)))

    def as_slice(self):
        """
        Turns a _ColumnIndexManager into a tuple which can be used in an
        indexing operation.

        If no index is possible, None will be returned.
        """
        result = [None] * self.ndims

        for dim, dimension_array in enumerate(self):
            # If dimension_array has not been set, span the entire dimension
            if isinstance(dimension_array, np.ndarray):
                where_true = np.where(dimension_array)[0]
                # If the array had no True values in it, then the dimension
                # is equivalent to False
                if len(where_true) == 0:
                    result = None
                    break

                # If there was exactly one match, the key should be an integer
                if where_true.shape == (1,):
                    result[dim] = where_true[0]
                else:
                    # Finally, we can either provide a slice if possible,
                    # or a tuple of indices which match. In order to determine
                    # if we can provide a slice, calculate the deltas between
                    # the indices and check if they are the same.
                    delta = np.diff(where_true, axis=0)
                    # if the diff is consistent we can create a slice object
                    if all(delta[0] == delta):
                        result[dim] = slice(where_true[0], where_true[-1] + 1,
                                            delta[0])
                    else:
                        # otherwise, key is a tuple
                        result[dim] = tuple(where_true)

            # Handle the case where dimension_array is a boolean
            elif dimension_array:
                result[dim] = slice(None, None)
            else:
                result = None
                break

        if result is None:
            return result
        else:
            return tuple(result)


def list_of_constraints(constraints):
    """
    Turns the given constraints into a list of valid constraints
    using :func:`as_constraint`.

    """
    if not isinstance(constraints, (list, tuple)):
        constraints = [constraints]

    return [as_constraint(constraint) for constraint in constraints]


def as_constraint(thing):
    """
    Casts an object into a cube constraint where possible, otherwise
    a TypeError will be raised.

    If the given object is already a valid constraint then the given object
    will be returned, else a TypeError will be raised.

    """
    if isinstance(thing, Constraint):
        return thing
    elif thing is None:
        return Constraint()
    elif isinstance(thing, basestring):
        return Constraint(thing)
    else:
        raise TypeError('%r cannot be cast to a constraint.' % thing)


class AttributeConstraint(Constraint):
    """Provides a simple Cube-attribute based :class:`Constraint`."""
    def __init__(self, **attributes):
        """
        Example usage::

            iris.AttributeConstraint(STASH='m01s16i004')

            iris.AttributeConstraint(
                STASH=lambda stash: stash.endswith('i005'))

        .. note:: Attribute constraint names are case sensitive.

        """
        self._attributes = attributes
        Constraint.__init__(self, cube_func=self._cube_func)

    def _cube_func(self, cube):
        match = True
        for name, value in self._attributes.iteritems():
            if name in cube.attributes:
                cube_attr = cube.attributes.get(name)
                # if we have a callable, then call it with the value,
                # otherwise, assert equality
                if callable(value):
                    if not value(cube_attr):
                        match = False
                        break
                else:
                    if cube_attr != value:
                        match = False
                        break
            else:
                match = False
                break
        return match

    def __repr__(self):
        return 'AttributeConstraint(%r)' % self._attributes

########NEW FILE########
__FILENAME__ = _cube_coord_common
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.


# TODO: Is this a mixin or a base class?

import string

import iris.std_names
import iris.unit


class LimitedAttributeDict(dict):
    _forbidden_keys = ('standard_name', 'long_name', 'units', 'bounds', 'axis',
                       'calendar', 'leap_month', 'leap_year', 'month_lengths',
                       'coordinates', 'grid_mapping', 'climatology',
                       'cell_methods', 'formula_terms', 'compress',
                       'missing_value', 'add_offset', 'scale_factor',
                       'valid_max', 'valid_min', 'valid_range', '_FillValue')

    def __init__(self, *args, **kwargs):
        dict.__init__(self, *args, **kwargs)
        # Check validity of keys
        for key in self.iterkeys():
            if key in self._forbidden_keys:
                raise ValueError('%r is not a permitted attribute' % key)

    def __eq__(self, other):
        # Extend equality to allow for NumPy arrays.
        match = self.viewkeys() == other.viewkeys()
        if match:
            for key, value in self.iteritems():
                match = value == other[key]
                try:
                    match = bool(match)
                except ValueError:
                    match = match.all()
                if not match:
                    break
        return match

    def __ne__(self, other):
        return not self == other

    def __setitem__(self, key, value):
        if key in self._forbidden_keys:
            raise ValueError('%r is not a permitted attribute' % key)
        dict.__setitem__(self, key, value)

    def update(self, other, **kwargs):
        # Gather incoming keys
        keys = []
        if hasattr(other, "keys"):
            keys += other.keys()
        else:
            keys += [k for k, v in other]

        keys += kwargs.keys()

        # Check validity of keys
        for key in keys:
            if key in self._forbidden_keys:
                raise ValueError('%r is not a permitted attribute' % key)

        dict.update(self, other, **kwargs)


class CFVariableMixin(object):
    def name(self, default='unknown'):
        """
        Returns a human-readable name.

        First it tries :attr:`standard_name`, then 'long_name', then 'var_name'
        before falling back to the value of `default` (which itself defaults to
        'unknown').

        """
        return self.standard_name or self.long_name or self.var_name or default

    def rename(self, name):
        """
        Changes the human-readable name.

        If 'name' is a valid standard name it will assign it to
        :attr:`standard_name`, otherwise it will assign it to
        :attr:`long_name`.

        """
        try:
            self.standard_name = name
            self.long_name = None
        except ValueError:
            self.standard_name = None
            self.long_name = unicode(name)

        # Always clear var_name when renaming.
        self.var_name = None

    @property
    def standard_name(self):
        """The standard name for the Cube's data."""
        return self._standard_name

    @standard_name.setter
    def standard_name(self, name):
        if name is None or name in iris.std_names.STD_NAMES:
            self._standard_name = name
        else:
            raise ValueError('%r is not a valid standard_name' % name)

    @property
    def units(self):
        """The :mod:`~iris.unit.Unit` instance of the object."""
        return self._units

    @units.setter
    def units(self, unit):
        self._units = iris.unit.as_unit(unit)

    @property
    def var_name(self):
        """The CF variable name for the object."""
        return self._var_name

    @var_name.setter
    def var_name(self, name):
        if name is not None:
            if not name:
                raise ValueError('An empty string is not a valid CF variable '
                                 'name.')
            elif set(name).intersection(string.whitespace):
                raise ValueError('{!r} is not a valid CF variable name because'
                                 ' it contains whitespace.'.format(name))
        self._var_name = name

    @property
    def attributes(self):
        return self._attributes

    @attributes.setter
    def attributes(self, attributes):
        self._attributes = LimitedAttributeDict(attributes or {})

########NEW FILE########
__FILENAME__ = _merge
# (C) British Crown Copyright 2010 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Automatic collation of cubes into higher-dimensional cubes.

Typically the cube merge process is handled by
:method:`iris.cube.CubeList.merge`.

"""
from collections import namedtuple, OrderedDict
from copy import deepcopy

import biggus
import numpy as np
import numpy.ma as ma

import iris.cube
import iris.coords
import iris.exceptions
import iris.unit
import iris.util


#
# Private namedtuple wrapper classes.
#
class _Template(namedtuple('Template',
                           ['dims', 'points', 'bounds', 'kwargs'])):
    """
    Common framework from which to build a dimension or auxiliary coordinate.

    Args:

    * dims:
        Tuple of the associated :class:`iris.cube.Cube` data dimension/s
        spanned by this coordinate template.

    * points:
        A :mod:`numpy` array representing the coordinate point data. No
        points data is represented by None.

    * bounds:
        A :mod:`numpy` array representing the coordinate bounds data. No
        bounds data is represented by None.

    * kwargs:
        A dictionary of key/value pairs required to create a coordinate.

    """


class _CoordMetaData(namedtuple('CoordMetaData',
                                ['points_dtype', 'bounds_dtype', 'kwargs'])):
    """
    Bespoke metadata required to build a dimension or auxiliary coordinate.

    Args:

    * points_dtype:
        The points data :class:`numpy.dtype` of an associated coordinate.
        None otherwise.

    * bounds_dtype:
        The bounds data :class:`numpy.dtype` of an associated coordinate.
        None otherwise.

    * kwargs:
        A dictionary of key/value pairs required to create a coordinate.

    """


class _CoordAndDims(namedtuple('CoordAndDims',
                               ['coord', 'dims'])):
    """
    Container for a coordinate and the associated data dimension/s
    spanned over a :class:`iris.cube.Cube`.

    Args:

    * coord:
        A :class:`iris.coords.DimCoord` or :class:`iris.coords.AuxCoord`
        coordinate instance.

    * dims:
        A tuple of the data dimension/s spanned by the coordinate.

    """


class _ScalarCoordPayload(namedtuple('ScalarCoordPayload',
                                     ['defns', 'values', 'metadata'])):
    """
    Container for all scalar coordinate data and metadata represented
    within a :class:`iris.cube.Cube`.

    All scalar coordinate related data is sorted into ascending order
    of the associated coordinate definition.

    Args:

    * defns:
        A list of scalar coordinate definitions :class:`iris.coords.CoordDefn`
        belonging to a :class:`iris.cube.Cube`.

    * values:
        A list of scalar coordinate values belonging to a
        :class:`iris.cube.Cube`.  Each scalar coordinate value is
        typically an :class:`iris.coords.Cell`.

    * metadata:
        A list of :class:`_CoordMetaData` instances belonging to a
        :class:`iris.cube.Cube`.

    """


class _VectorCoordPayload(namedtuple('VectorCoordPayload',
                                     ['dim_coords_and_dims',
                                      'aux_coords_and_dims'])):
    """
    Container for all vector coordinate data and metadata represented
    within a :class:`iris.cube.Cube`.

    Args:

    * dim_coords_and_dims:
        A list of :class:`_CoordAndDim` instances containing non-scalar
        (i.e. multi-valued) :class:`iris.coords.DimCoord` instances and
        the associated data dimension spanned by them for a
        :class:`iris.cube.Cube`.

    * aux_coords_and_dims:
        A list of :class:`_CoordAndDim` instances containing non-scalar
        (i.e. multi-valued) :class:`iris.coords.DimCoord` and/or
        :class:`iris.coords.AuxCoord` instances and the associated data
        dimension/s spanned by them for a :class:`iris.cube.Cube`.

    """


class _CoordPayload(namedtuple('CoordPayload',
                               ['scalar', 'vector', 'factory_defns'])):
    """
    Container for all the scalar and vector coordinate data and
    metadata, and auxiliary coordinate factories represented within a
    :class:`iris.cube.Cube`.

    All scalar coordinate and factory related data is sorted into
    ascending order of the associated coordinate definition.

    Args:

    * scalar:
        A :class:`_ScalarCoordPayload` instance.

    * vector:
        A :class:`_VectorCoordPayload` instance.

    * factory_defns:
        A list of :class:`_FactoryDefn` instances.

    """
    def as_signature(self):
        """Construct and return a :class:`_CoordSignature` from the payload."""

        return _CoordSignature(self.scalar.defns,
                               self.vector.dim_coords_and_dims,
                               self.vector.aux_coords_and_dims,
                               self.factory_defns)

    @staticmethod
    def _coords_msgs(msgs, coord_group, defns_a, defns_b):
        if defns_a != defns_b:
            # Get a new list so we can modify it
            defns_b = list(defns_b)
            diff_defns = []
            for defn_a in defns_a:
                try:
                    defns_b.remove(defn_a)
                except ValueError:
                    diff_defns.append(defn_a)
            diff_defns.extend(defns_b)
            if diff_defns:
                names = set(defn.name() for defn in diff_defns)
                msgs.append('Coordinates in {} differ: {}.'.format(
                    coord_group, ', '.join(names)))
            else:
                msgs.append('Coordinates in {} differ by dtype or class'
                            ' (i.e. DimCoord vs AuxCoord).'.format(
                                coord_group))

    def match_signature(self, signature, error_on_mismatch):
        """
        Return whether this _CoordPayload matches the corresponding
        aspects of a _CoordSignature.

        Args:

        * signature (_CoordSignature):
            The _CoordSignature to compare against.

        * error_on_mismatch (bool):
            If True, raise an Exception with detailed explanation.

        Returns:
           Boolean. True if and only if this _CoordPayload matches
           the corresponding aspects `other`.

        """
        def unzip(coords_and_dims):
            if coords_and_dims:
                coords, dims = zip(*coords_and_dims)
            else:
                coords, dims = [], []
            return coords, dims

        def dims_msgs(msgs, coord_group, dimlists_a, dimlists_b):
            if dimlists_a != dimlists_b:
                msgs.append(
                    'Coordinate-to-dimension mapping differs for {}.'.format(
                        coord_group))

        msgs = []
        self._coords_msgs(msgs, 'cube.aux_coords (scalar)', self.scalar.defns,
                          signature.scalar_defns)

        coord_group = 'cube.dim_coords'
        self_coords, self_dims = unzip(self.vector.dim_coords_and_dims)
        other_coords, other_dims = unzip(signature.vector_dim_coords_and_dims)
        self._coords_msgs(msgs, coord_group, self_coords, other_coords)
        dims_msgs(msgs, coord_group, self_dims, other_dims)

        coord_group = 'cube.aux_coords (non-scalar)'
        self_coords, self_dims = unzip(self.vector.aux_coords_and_dims)
        other_coords, other_dims = unzip(signature.vector_aux_coords_and_dims)
        self._coords_msgs(msgs, coord_group, self_coords, other_coords)
        dims_msgs(msgs, coord_group, self_dims, other_dims)

        if self.factory_defns != signature.factory_defns:
            msgs.append('cube.aux_factories() differ')

        match = not bool(msgs)
        if error_on_mismatch and not match:
            raise iris.exceptions.MergeError(msgs)
        return match


class _CoordSignature(namedtuple('CoordSignature',
                                 ['scalar_defns',
                                  'vector_dim_coords_and_dims',
                                  'vector_aux_coords_and_dims',
                                  'factory_defns'])):
    """
    Criterion for identifying a specific type of :class:`iris.cube.Cube`
    based on its scalar and vector coorinate data and metadata, and
    auxiliary coordinate factories.

    Args:

    * scalar_defns:
        A list of scalar coordinate definitions sorted into ascending order.

    * vector_dim_coords_and_dims:
        A list of :class:`_CoordAndDim` instances containing non-scalar
        (i.e. multi-valued) :class:`iris.coords.DimCoord` instances and
        the associated data dimension spanned by them for a
        :class:`iris.cube.Cube`.

    * vector_aux_coords_and_dims:
        A list of :class:`_CoordAndDim` instances containing non-scalar
        (i.e. multi-valued) :class:`iris.coords.DimCoord` and/or
        :class:`iris.coords.AuxCoord` instances and the associated data
        dimension/s spanned by them for a :class:`iris.cube.Cube`.

    * factory_defns:
        A list of :class:`_FactoryDefn` instances.

    """


class _CubeSignature(namedtuple('CubeSignature',
                                ['defn', 'data_shape', 'data_type',
                                 'fill_value'])):
    """
    Criterion for identifying a specific type of :class:`iris.cube.Cube`
    based on its metadata.

    Args:

    * defn:
        A cube definition tuple.

    * data_shape:
        The data payload shape of a :class:`iris.cube.Cube`.

    * data_type:
        The data payload :class:`numpy.dtype` of a :class:`iris.cube.Cube`.

    * fill_value:
        The value to be used to mark missing data in the data payload,
        or None if no such value exists.

    """
    def _defn_msgs(self, other_defn):
        msgs = []
        self_defn = self.defn
        if self_defn.standard_name != other_defn.standard_name:
            msgs.append('cube.standard_name differs: {!r} != {!r}'.format(
                self_defn.standard_name, other_defn.standard_name))
        if self_defn.long_name != other_defn.long_name:
            msgs.append('cube.long_name differs: {!r} != {!r}'.format(
                self_defn.long_name, other_defn.long_name))
        if self_defn.var_name != other_defn.var_name:
            msgs.append('cube.var_name differs: {!r} != {!r}'.format(
                self_defn.var_name, other_defn.var_name))
        if self_defn.units != other_defn.units:
            msgs.append('cube.units differs: {!r} != {!r}'.format(
                self_defn.units, other_defn.units))
        if self_defn.attributes != other_defn.attributes:
            diff_keys = (self_defn.attributes.viewkeys() ^
                         other_defn.attributes.viewkeys())
            if diff_keys:
                msgs.append('cube.attributes keys differ: ' +
                            ', '.join(repr(key) for key in diff_keys))
            else:
                diff_attrs = [repr(key) for key in self_defn.attributes
                              if np.all(self_defn.attributes[key] !=
                                        other_defn.attributes[key])]
                diff_attrs = ', '.join(diff_attrs)
                msgs.append(
                    'cube.attributes values differ for keys: {}'.format(
                        diff_attrs))
        if self_defn.cell_methods != other_defn.cell_methods:
            msgs.append('cube.cell_methods differ')
        return msgs

    def match(self, other, error_on_mismatch):
        """
        Return whether this _CubeSignature equals another.

        This is the first step to determine if two "cubes" (either a
        real Cube or a ProtoCube) can be merged, by considering:
            - standard_name, long_name, var_name
            - units
            - attributes
            - cell_methods
            - shape, dtype, fill_value

        Args:

        * other (_CubeSignature):
            The _CubeSignature to compare against.

        * error_on_mismatch (bool):
            If True, raise a :class:`~iris.exceptions.MergeException`
            with a detailed explanation if the two do not match.

        Returns:
           Boolean. True if and only if this _CubeSignature matches `other`.

        """
        msgs = self._defn_msgs(other.defn)
        if self.data_shape != other.data_shape:
            msgs.append('cube.shape differs: {} != {}'.format(
                self.data_shape, other.data_shape))
        if self.data_type != other.data_type:
            msgs.append('cube data dtype differs: {} != {}'.format(
                self.data_type, other.data_type))
        if self.fill_value != other.fill_value:
            msgs.append('cube data fill_value differs: {!r} != {!r}'.format(
                self.fill_value, other.fill_value))
        match = not bool(msgs)
        if error_on_mismatch and not match:
            raise iris.exceptions.MergeError(msgs)
        return match


class _Skeleton(namedtuple('Skeleton',
                           ['scalar_values', 'data'])):
    """
    Basis of a source-cube, containing the associated scalar coordinate values
    and data payload of a :class:`iris.cube.Cube`.

    Args:

    * scalar_values:
        A list of scalar coordinate values belonging to a
        :class:`iris.cube.Cube` sorted into ascending order of the
        associated coordinate definition. Each scalar coordinate value
        is typically an :class:`iris.coords.Cell`.

    * data:
        The data payload of a :class:`iris.cube.Cube`.

    """


class _FactoryDefn(namedtuple('_FactoryDefn',
                              ['class_', 'dependency_defns'])):
    """
    The information required to identify and rebuild a single AuxCoordFactory.

    Args:

    * class_:
        The class of the AuxCoordFactory.

    * dependency_defns:
        A list of pairs, where each pair contains a dependency key and its
        corresponding coordinate definition. Sorted on dependency key.

    """


class _Relation(namedtuple('Relation',
                           ['separable', 'inseparable'])):
    """
    Categorisation of the candidate dimensions belonging to a
    :class:`ProtoCube` into separable 'independent' dimensions, and
    inseparable dependent dimensions.

    Args:

    * separable:
        A set of independent candidate dimension names.

    * inseperable:
        A set of dependent candidate dimension names.

    """


_COMBINATION_JOIN = '-'


def _is_combination(name):
    """
    Determine whether the candidate dimension is an 'invented' combination
    of candidate dimensions.

    Args:

    * name:
        The candidate dimension.

    Returns:
        Boolean.

    """
    return _COMBINATION_JOIN in str(name)


def build_indexes(positions):
    """
    Construct a mapping for each candidate dimension that maps for each
    of its scalar values the set of values for each of the other candidate
    dimensions.

    For example:

        >>> from iris._merge import build_indexes
        >>> positions = [{'a': 0, 'b': 10, 'c': 100},
        ...              {'a': 1, 'b': 10, 'c': 200},
        ...              {'a': 2, 'b': 20, 'c': 300}]
        ...
        >>> indexes = build_indexes(positions)
        >>> for k in sorted(indexes):
        ...     print '%r:' % k
        ...     for kk in sorted(indexes[k]):
        ...         print '\t%r:' % kk,
        ...         for kkk in sorted(indexes[k][kk]):
        ...             print '%r: %r' % (kkk, indexes[k][kk][kkk]),
        ...         print
        ...
        'a':
             0: 'b': set([10]) 'c': set([100])
             1: 'b': set([10]) 'c': set([200])
             2: 'b': set([20]) 'c': set([300])
        'b':
             10: 'a': set([0, 1]) 'c': set([200, 100])
             20: 'a': set([2]) 'c': set([300])
        'c':
             100: 'a': set([0]) 'b': set([10])
             200: 'a': set([1]) 'b': set([10])
             300: 'a': set([2]) 'b': set([20])

    Args:

    * positions:
        A list containing a dictionary of candidate dimension key to
        scalar value pairs for each source-cube.

    Returns:
        The cross-reference dictionary for each candidate dimension.

    """
    names = positions[0].keys()
    scalar_index_by_name = {name: {} for name in names}

    for position in positions:
        for name, value in position.iteritems():
            name_index_by_scalar = scalar_index_by_name[name]

            if value in name_index_by_scalar:
                value_index_by_name = name_index_by_scalar[value]
                for other_name in names:
                    if other_name != name:
                        value_index_by_name[other_name].add(
                            position[other_name])
            else:
                name_index_by_scalar[value] = {
                    other_name: set((position[other_name],))
                    for other_name in names if other_name != name}

    return scalar_index_by_name


def _separable_pair(name, index):
    """
    Determine whether the candidate dimension is separable.

    A candidate dimension X and Y are separable if each scalar
    value of X maps to the same set of scalar values of Y.

    Args:

    * name1:
        The first candidate dimension to be compared.

    * name2:
        The second candidate dimension to be compared.

    * index:
        The cross-reference dictionary for the first candidate
        dimension.

    Returns:
        Boolean.

    """
    items = index.itervalues()
    reference = next(items)[name]

    return all([item[name] == reference for item in items])


def _separable(name, indexes):
    """
    Determine the candidate dimensions that are separable and
    inseparable relative to the provided candidate dimension.

    A candidate dimension X and Y are separable if each scalar
    value of X maps to the same set of scalar values of Y.

    Args:

    * name:
        The candidate dimension that requires its separable and
        inseparable relationship to be determined.

    * indexes:
        The cross-reference dictionary for each candidate dimension.

    Returns:
        A tuple containing the set of separable and inseparable
        candidate dimensions.

    """
    separable = set()
    inseparable = set()

    for target_name in indexes:
        if name != target_name:
            if _separable_pair(target_name, indexes[name]):
                separable.add(target_name)
            else:
                inseparable.add(target_name)

    return _Relation(separable, inseparable)


def derive_relation_matrix(indexes):
    """
    Construct a mapping for each candidate dimension that specifies
    which of the other candidate dimensions are separable or inseparable.

    A candidate dimension X and Y are separable if each scalar value of
    X maps to the same set of scalar values of Y.

    Also see :func:`iris._merge.build_indexes`.

    For example:

        >>> from iris._merge import build_indexes, derive_relation_matrix
        >>> positions = [{'a': 0, 'b': 10, 'c': 100},
        ...              {'a': 1, 'b': 10, 'c': 200},
        ...              {'a': 2, 'b': 20, 'c': 300}]
        ...
        >>> indexes = build_indexes(positions)
        >>> matrix = derive_relation_matrix(indexes)
        >>> for k, v in matrix.iteritems():
        ...     print '%r: %r' % (k, v)
        ...
        'a': Relation(separable=set([]), inseparable=set(['c', 'b']))
        'c': Relation(separable=set([]), inseparable=set(['a', 'b']))
        'b': Relation(separable=set([]), inseparable=set(['a', 'c']))

    Args:

    * indexes:
        The cross-reference dictionary for each candidate dimension.

    Returns:
        The relation dictionary for each candidate dimension.

    """
    # TODO: This takes twice as long as it could do because it doesn't
    # account for the symmetric nature of the relationship.
    relation_matrix = {name: _separable(name, indexes) for name in indexes}

    return relation_matrix


def derive_groups(relation_matrix):
    """
    Determine all related (chained) groups of inseparable candidate dimensions.

    If candidate dimension A is inseparable for B and C, and B is inseparable
    from D, and E is inseparable from F. Then the groups are ABCD and EF.

    Args:

    * relation_matrix:
        The relation dictionary for each candidate dimension.

    Returns:
        A list of all related (chained) inseparable candidate dimensions.

    """
    names = set(relation_matrix)
    groups = []

    while names:
        name = names.pop()
        group = set([name])
        to_follow = set([name])

        while to_follow:
            name = to_follow.pop()
            new = relation_matrix[name].inseparable - group
            group.update(new)
            to_follow.update(new)
            names -= new

        groups.append(group)

    return groups


def _derive_separable_group(relation_matrix, group):
    """
    Determine which candidate dimensions in the group are separable.

    Args:

    * relation_matrix:
        The relation dictionary for each candidate dimension.

    * group:
        A set of related (chained) inseparable candidate dimensions.

    Returns:
        The set of candidate dimensions within the group that are
        separable.

    """
    result = set()

    for name in group:
        if relation_matrix[name].separable & group:
            result.add(name)

    return result


def _is_dependent(dependent, independent, positions, function_mapping=None):
    """
    Determine whether there exists a one-to-one functional relationship
    between the independent candidate dimension/s and the dependent
    candidate dimension.

    Args:

    * dependent:
        A candidate dimension that requires to be functionally
        dependent on all the independent candidate dimensions.

    * independent:
        A list of candidate dimension/s that require to act as the independent
        variables in a functional relationship.

    * positions:
        A list containing a dictionary of candidate dimension key to
        scalar value pairs for each source-cube.

    Kwargs:

    * function_mapping:
        A dictionary that enumerates a valid functional relationship
        between the dependent candidate dimension and the independent
        candidate dimension/s.

    Returns:
        Boolean.

    """
    valid = True
    relation = {}

    if isinstance(function_mapping, dict):
        relation = function_mapping

    for position in positions:
        item = tuple([position[name] for name in independent])

        if item in relation:
            if position[dependent] != relation[item]:
                valid = False
                break
        else:
            relation[item] = position[dependent]

    return valid


def _derive_consistent_groups(relation_matrix, separable_group):
    """
    Determine the largest combinations of candidate dimensions within the
    separable group that are self consistently separable from one another.

    If the candidate dimension A is separable from the candidate dimensions
    B and C. Then the candidate dimension group ABC is a separable consistent
    group if B is separable from A and C, and C is separable from A and B.

    Args:

    * relation_matrix:
        The relation dictionary for each candidate dimension.

    * separable_group:
        The set of candidate dimensions that are separable.

    Returns:
        A list of candidate dimension groups that are consistently separable.

    """
    result = []

    for name in separable_group:
        name_separable_group = relation_matrix[name].separable & \
            separable_group
        candidate = list(name_separable_group) + [name]
        valid = True

        for _ in range(len(name_separable_group)):
            candidate_separable_group = set(candidate[1:])

            if candidate_separable_group & \
                    (relation_matrix[candidate[0]].separable &
                     separable_group) != candidate_separable_group:
                valid = False
                break

            candidate.append(candidate.pop(0))

        if valid:
            result.append(candidate)

    return result


def _build_separable_group(space, group, separable_consistent_groups,
                           positions, function_matrix):
    """
    Update the space with the first separable consistent group that
    satisfies a valid functional relationship with all other candidate
    dimensions in the group.

    For example, the group ABCD and separable consistent group CD,
    if A = f(C, D) and B = f(C, D) then update the space with
    "A: (C, D), B: (C, D), C: None, D: None". Where "A: (C, D)" means
    that candidate dimension A is dependent on candidate dimensions C
    and D, and "C: None" means that this candidate dimension is
    independent.

    Args:

    * space:
        A dictionary defining for each candidate dimension its
        dependency on any other candidate dimensions within the space.

    * group:
        A set of related (chained) inseparable candidate dimensions.

    * separable_consistent_groups:
        A list of candidate dimension groups that are consistently separable.

    * positions:
        A list containing a dictionary of candidate dimension key to
        scalar value pairs for each source-cube.

    * function_matrix:
        The function mapping dictionary for each candidate dimension that
        participates in a functional relationship.

    Returns:
        Boolean.

    """
    valid = False

    for independent in sorted(separable_consistent_groups):
        dependent = list(group - set(independent))
        dependent_function_matrix = {}

        for name in dependent:
            function_mapping = {}
            valid = _is_dependent(name, independent, positions,
                                  function_mapping)

            if not valid:
                break

            dependent_function_matrix[name] = function_mapping

        if valid:
            break

    if function_matrix is not None:
        function_matrix.update(dependent_function_matrix)

    if valid:
        space.update({name: None for name in independent})
        space.update({name: tuple(independent) for name in dependent})

    return valid


def _build_inseparable_group(space, group, positions, function_matrix):
    """
    Update the space with the first valid scalar functional relationship
    between a candidate dimension within the group and all other
    candidate dimensions.

    For example, with the inseparable group ABCD, a valid scalar
    relationship B = f(A), C = f(A) and D = f(A) results in a space with
    "A: None, B: (A,), C: (A,), D: (A,)".
    Where "A: None" means that this candidate dimension is independent,
    and "B: (A,)" means that candidate dimension B is dependent on
    candidate dimension A.

    The scalar relationship must exist between one candidate dimension
    and all others in the group, as the group is considered inseparable
    in this context.

    Args:

    * space:
        A dictionary defining for each candidate dimension its dependency on
        any other candidate dimensions within the space.

    * group:
        A set of related (chained) inseparable candidate dimensions.

    * positions:
        A list containing a dictionary of candidate dimension key to
        scalar value pairs for each source-cube.

    * function_matrix:
        The function mapping dictionary for each candidate dimension that
        participates in a functional relationship.

    Returns:
        Boolean.

    """
    scalar = False

    for name in sorted(group):
        independent = set([name])
        dependent = set(group) - independent
        valid = False
        dependent_function_matrix = {}

        for name in dependent:
            function_mapping = {}
            valid = _is_dependent(name, independent, positions,
                                  function_mapping)

            if not valid:
                break

            dependent_function_matrix[name] = function_mapping

        if valid:
            scalar = True

            if function_matrix is not None:
                function_matrix.update(dependent_function_matrix)

            space.update({name: None for name in independent})
            space.update({name: tuple(independent) for name in dependent})
            break

    return scalar


def _build_combination_group(space, group, positions, function_matrix):
    """
    Update the space with the new combined or invented dimension
    that each member of this inseparable group depends on.

    As no functional relationship between members of the group can be
    determined, the new combination dimension will not have a dimension
    coordinate associated with it. Rather, it is simply an enumeration
    of the group members for each of the positions (source-cubes).

    Args:

    * space:
        A dictionary defining for each candidate dimension its dependency on
        any other candidate dimensions within the space.

    * group:
        A set of related (chained) inseparable candidate dimensions.

    * positions:
        A list containing a dictionary of candidate dimension key to
        scalar value pairs for each source-cube.

    * function_matrix:
        The function mapping dictionary for each candidate dimension that
        participates in a functional relationship.

    Returns:
        None.

    """
    combination = _COMBINATION_JOIN.join(sorted(map(str, group)))
    space.update({name: None for name in (combination,)})
    space.update({name: (combination,) for name in group})
    members = combination.split(_COMBINATION_JOIN)

    # Populate the function matrix for each member of the group.
    for name in group:
        function_matrix[name] = {}

    for position in positions:
        # Note, the cell double-tuple! This ensures that the cell value for
        # each member of the group is kept bound together as one key.
        cell = (tuple([position[int(member) if member.isdigit() else member]
                       for member in members]),)
        for name in group:
            function_matrix[name][cell] = position[name]


def derive_space(groups, relation_matrix, positions, function_matrix=None):
    """
    Determine the relationship between all the candidate dimensions.

    Args:
      * groups:
          A list of all related (chained) inseparable candidate dimensions.

      * relation_matrix:
          The relation dictionary for each candidate dimension.

      * positions:
          A list containing a dictionary of candidate dimension key to
          scalar value pairs for each source-cube.

    Kwargs:
      * function_matrix:
          The function mapping dictionary for each candidate dimension that
          participates in a functional relationship.

    Returns:
        A space dictionary describing the relationship between each
        candidate dimension.

    """
    space = {}

    for group in groups:
        separable_group = _derive_separable_group(relation_matrix, group)

        if len(group) == 1 and not separable_group:
            # This single candidate dimension is separable from all other
            # candidate dimensions in the group, therefore it is a genuine
            # dimension of the space.
            space.update({name: None for name in group})
        elif separable_group:
            # Determine the largest combination of the candidate dimensions
            # in the separable group that are consistently separable.
            consistent_groups = _derive_consistent_groups(relation_matrix,
                                                          separable_group)
            if not _build_separable_group(space, group, consistent_groups,
                                          positions, function_matrix):
                # There is no relationship between any of the candidate
                # dimensions in the separable group, so merge them together
                # into a new combined dimension of the space.
                _build_combination_group(space, group,
                                         positions, function_matrix)
        else:
            # Determine whether there is a scalar relationship between one of
            # the candidate dimensions and each of the other candidate
            # dimensions in this inseparable group.
            if not _build_inseparable_group(space, group,
                                            positions, function_matrix):
                # There is no relationship between any of the candidate
                # dimensions in this inseparable group, so merge them together
                # into a new combined dimension of the space.
                _build_combination_group(space, group,
                                         positions, function_matrix)

    return space


class ProtoCube(object):
    """
    Framework for merging source-cubes into one or more higher
    dimensional cubes.

    """

    def __init__(self, cube):
        """
        Create a new ProtoCube from the given cube and record the cube
        as a source-cube.

        """

        # Default hint ordering for candidate dimension coordinates.
        self._hints = ['time', 'forecast_reference_time', 'forecast_period',
                       'model_level_number']

        # The cube signature is metadata that defines this ProtoCube.
        self._cube_signature = self._build_signature(cube)

        # Extract the scalar and vector coordinate data and metadata
        # from the cube.
        coord_payload = self._extract_coord_payload(cube)

        # The coordinate signature defines the scalar and vector
        # coordinates of this ProtoCube.
        self._coord_signature = coord_payload.as_signature()
        self._coord_metadata = coord_payload.scalar.metadata

        # The list of stripped-down source-cubes relevant to this ProtoCube.
        self._skeletons = []
        self._add_cube(cube, coord_payload)

        # Proto-coordinates constructed from merged scalars.
        self._dim_templates = []
        self._aux_templates = []

        # During the merge this will contain the complete, merged shape
        # of a result cube.
        # E.g. Merging three (72, 96) cubes would give:
        #      self._shape = (3, 72, 96).
        self._shape = []
        # During the merge this will contain the shape of the "stack"
        # of cubes used to create a single result cube.
        # E.g. Merging three (72, 96) cubes would give:
        #      self._stack_shape = (3,)
        self._stack_shape = []

        self._nd_names = []
        self._cache_by_name = {}
        self._dim_coords_and_dims = []
        self._aux_coords_and_dims = []

        # Dims offset by merged space higher dimensionality.
        self._vector_dim_coords_dims = []
        self._vector_aux_coords_dims = []

    def _report_duplicate(self, nd_indexes, group_by_nd_index):
        # Find the first offending source-cube with duplicate metadata.
        index = [group_by_nd_index[nd_index][1]
                 for nd_index in nd_indexes
                 if len(group_by_nd_index[nd_index]) > 1][0]
        name = self._cube_signature.defn.name()
        scalars = []
        for defn, value in zip(self._coord_signature.scalar_defns,
                               self._skeletons[index].scalar_values):
            scalars.append('%s=%r' % (defn.name(), value))
        msg = 'Duplicate %r cube, with scalar coordinates %s'
        msg = msg % (name, ', '.join(scalars))
        raise iris.exceptions.DuplicateDataError(msg)

    def merge(self, unique=True):
        """
        Returns the list of cubes resulting from merging the registered
        source-cubes.

        Kwargs:

        * unique:
            If True, raises `iris.exceptions.DuplicateDataError` if
            duplicate cubes are detected.

        Returns:
            A :class:`iris.cube.CubeList` of merged cubes.

        """
        positions = [{i: v for i, v in enumerate(skeleton.scalar_values)}
                     for skeleton in self._skeletons]
        indexes = build_indexes(positions)
        relation_matrix = derive_relation_matrix(indexes)
        groups = derive_groups(relation_matrix)

        function_matrix = {}
        space = derive_space(groups, relation_matrix, positions,
                             function_matrix=function_matrix)
        self._define_space(space, positions, indexes, function_matrix)
        self._build_coordinates()

        # All the final, merged cubes will end up here.
        merged_cubes = iris.cube.CubeList()

        # Collate source-cubes by the nd-index.
        group_by_nd_index = {}
        for index, position in enumerate(positions):
            group = group_by_nd_index.setdefault(self._nd_index(position), [])
            group.append(index)

        # Determine the largest group of source-cubes that want to occupy
        # the same nd-index in the final merged cube.
        group_depth = max([len(group) for group in group_by_nd_index.values()])
        nd_indexes = group_by_nd_index.keys()
        nd_indexes.sort()

        # Check for unique data.
        if unique and group_depth > 1:
            self._report_duplicate(nd_indexes, group_by_nd_index)

        # Generate group-depth merged cubes from the source-cubes.
        for level in xrange(group_depth):
            # Stack up all the data from all of the relevant source
            # cubes in a single biggus ArrayStack.
            # If it turns out that all the source cubes already had
            # their data loaded then at the end we can convert the
            # ArrayStack back to a numpy array.
            stack = np.empty(self._stack_shape, 'object')
            all_have_data = True
            for nd_index in nd_indexes:
                # Get the data of the current existing or last known
                # good source-cube
                group = group_by_nd_index[nd_index]
                offset = min(level, len(group) - 1)
                data = self._skeletons[group[offset]].data
                # Ensure the data is represented as a biggus.Array and
                # slot that Array into the stack.
                if isinstance(data, biggus.Array):
                    all_have_data = False
                else:
                    data = biggus.NumpyArrayAdapter(data)
                stack[nd_index] = data

            merged_data = biggus.ArrayStack(stack)
            if all_have_data:
                merged_data = merged_data.masked_array()
                # Unmask the array only if it is filled.
                if (ma.isMaskedArray(merged_data) and
                        ma.count_masked(merged_data) == 0):
                    merged_data = merged_data.data
            merged_cube = self._get_cube(merged_data)
            merged_cubes.append(merged_cube)

        return merged_cubes

    def register(self, cube, error_on_mismatch=False):
        """
        Add a compatible :class:`iris.cube.Cube` as a source-cube for
        merging under this :class:`ProtoCube`.

        A cube will be deemed compatible based on the signature of the
        cube and the signature of its scalar coordinates and vector
        coordinates being identical to that of the ProtoCube.

        Args:

        * cube:
            Candidate :class:`iris.cube.Cube` to be associated with
            this :class:`ProtoCube`.

        Kwargs:

        * error_on_mismatch:
            If True, raise an informative
            :class:`~iris.exceptions.MergeError` if registration fails.

        Returns:
            True iff the :class:`iris.cube.Cube` is compatible with
            this :class:`ProtoCube`.

        """
        match = self._cube_signature.match(self._build_signature(cube),
                                           error_on_mismatch)
        if match:
            coord_payload = self._extract_coord_payload(cube)
            match = coord_payload.match_signature(self._coord_signature,
                                                  error_on_mismatch)
        if match:
            # Register the cube as a source-cube for this ProtoCube.
            self._add_cube(cube, coord_payload)
        return match

    def _guess_axis(self, name):
        """
        Returns a "best guess" axis name of the candidate dimension.

        Heuristic categoration of the candidate dimension
        (i.e. scalar_defn index) into either label 'T', 'Z', 'Y', 'X'
        or None.

        Based on the associated scalar coordinate definition rather than the
        scalar coordinate itself.

        Args:

        * name:
            The candidate dimension.

        Returns:
            'T', 'Z', 'Y', 'X', or None.

        """
        axis = None

        if not _is_combination(name):
            defn = self._coord_signature.scalar_defns[name]
            axis = iris.util.guess_coord_axis(defn)

        return axis

    def _define_space(self, space, positions, indexes, function_matrix):
        """
        Given the derived :class:`ProtoCube` space, define this space in
        terms of its dimensionality, shape, coordinates and associated
        coordinate to space dimension mappings.

        Args:

        * space:
            A dictionary defining for each candidate dimension its
            dependency on any other candidate dimensions within the space.

        * positions:
            A list containing a dictionary of candidate dimension key to
            scalar value pairs for each source-cube.

        * indexes:
            A cross-reference dictionary for each candidate dimension.

        * function_matrix:
            The function mapping dictionary for each candidate dimension that
            participates in a functional relationship.

        """
        # Heuristic reordering of coordinate defintion indexes into
        # preferred dimension order.
        def axis_and_name(name):
            axis_dict = {'T': 1, 'Z': 2, 'Y': 3, 'X': 4}
            axis_index = axis_dict.get(self._guess_axis(name), 0)
            return (axis_index, name)
        names = sorted(space, key=axis_and_name)
        dim_by_name = {}

        metadata = self._coord_metadata
        coord_signature = self._coord_signature
        defns = coord_signature.scalar_defns
        vector_dim_coords_and_dims = coord_signature.vector_dim_coords_and_dims
        vector_aux_coords_and_dims = coord_signature.vector_aux_coords_and_dims
        signature = self._cube_signature

        # First pass - Build the dimension coordinate templates for the space.
        for name in names:
            if space[name] is None:
                if _is_combination(name):
                    members = name.split(_COMBINATION_JOIN)
                    # Create list of unique tuples from all combinations of
                    # scalars for each source cube. The keys of an OrderedDict
                    # are used to retain the ordering of source cubes but to
                    # remove any duplicate tuples.
                    cells = OrderedDict(
                        (tuple(position[int(member) if member.isdigit() else
                                        member] for member in members), None)
                        for position in positions).keys()
                    dim_by_name[name] = len(self._shape)
                    self._nd_names.append(name)
                    self._shape.append(len(cells))
                    self._stack_shape.append(len(cells))
                    self._cache_by_name[name] = {cell: index for index, cell
                                                 in enumerate(cells)}
                else:
                    # TODO: Consider appropriate sort order (ascending,
                    # decending) i.e. use CF positive attribute.
                    cells = sorted(indexes[name])
                    points = np.array([cell.point for cell in cells],
                                      dtype=metadata[name].points_dtype)
                    if cells[0].bound is not None:
                        bounds = np.array([cell.bound for cell in cells],
                                          dtype=metadata[name].bounds_dtype)
                    else:
                        bounds = None
                    kwargs = dict(zip(iris.coords.CoordDefn._fields,
                                      defns[name]))
                    kwargs.update(metadata[name].kwargs)

                    def name_in_independents():
                        return any(name in independents
                                   for independents in space.itervalues()
                                   if independents is not None)
                    if len(cells) == 1 and not name_in_independents():
                        # A scalar coordinate not participating in a
                        # function dependency.
                        self._aux_templates.append(
                            _Template((), points, bounds, kwargs))
                    else:
                        # Dimension coordinate (or aux if the data is
                        # string like).
                        dim_by_name[name] = dim = len(self._shape)
                        self._nd_names.append(name)
                        if metadata[name].points_dtype.kind == 'S':
                            self._aux_templates.append(
                                _Template(dim, points, bounds, kwargs))
                        else:
                            self._dim_templates.append(
                                _Template(dim, points, bounds, kwargs))
                        self._shape.append(len(cells))
                        self._stack_shape.append(len(cells))
                        self._cache_by_name[name] = {cell: index
                                                     for index, cell
                                                     in enumerate(cells)}

        # Second pass - Build the auxiliary coordinate templates for the space.
        for name in names:
            name_independents = space[name]

            # Determine if there is a function dependency.
            if name_independents is not None:
                # Calculate the auxiliary coordinate shape.
                dims = tuple([dim_by_name[independent]
                             for independent in name_independents])
                aux_shape = [self._shape[dim] for dim in dims]
                # Create empty points and bounds in preparation to be filled.
                points = np.empty(aux_shape, dtype=metadata[name].points_dtype)
                if positions[0][name].bound is not None:
                    shape = aux_shape + [len(positions[0][name].bound)]
                    bounds = np.empty(shape, dtype=metadata[name].bounds_dtype)
                else:
                    bounds = None

                # Populate the points and bounds based on the appropriate
                # function mapping.
                temp = function_matrix[name].iteritems()
                for function_independents, name_value in temp:
                    # Build the index (and cache it) for the auxiliary
                    # coordinate based on the associated independent
                    # dimension coordinate/s.
                    index = []

                    name_function_pairs = zip(name_independents,
                                              function_independents)
                    for independent, independent_value in name_function_pairs:
                        cache = self._cache_by_name[independent]
                        index.append(cache[independent_value])

                    index = tuple(index)
                    if points is not None:
                        points[index] = name_value.point

                    if bounds is not None:
                        bounds[index] = name_value.bound

                kwargs = dict(zip(iris.coords.CoordDefn._fields, defns[name]))
                self._aux_templates.append(_Template(dims, points, bounds,
                                                     kwargs))

        # Calculate the dimension mapping for each vector within the space.
        offset = len(self._shape)
        self._vector_dim_coords_dims = \
            [tuple([dim + offset for dim in item.dims])
             for item in vector_dim_coords_and_dims]
        self._vector_aux_coords_dims = \
            [tuple([dim + offset for dim in item.dims])
             for item in vector_aux_coords_and_dims]

        # Now factor in the vector payload shape. Note that, for
        # deferred loading, this does NOT change the shape.
        self._shape.extend(signature.data_shape)

    def _get_cube(self, data):
        """
        Return a fully constructed cube for the given data, containing
        all its coordinates and metadata.

        """
        signature = self._cube_signature
        dim_coords_and_dims = [(deepcopy(coord), dim)
                               for coord, dim in self._dim_coords_and_dims]
        aux_coords_and_dims = [(deepcopy(coord), dims)
                               for coord, dims in self._aux_coords_and_dims]
        kwargs = dict(zip(iris.cube.CubeMetadata._fields, signature.defn))

        cube = iris.cube.Cube(data,
                              dim_coords_and_dims=dim_coords_and_dims,
                              aux_coords_and_dims=aux_coords_and_dims,
                              **kwargs)

        # Add on any aux coord factories.
        for factory_defn in self._coord_signature.factory_defns:
            args = {}
            for key, defn in factory_defn.dependency_defns:
                coord = cube.coord(defn)
                args[key] = coord
            factory = factory_defn.class_(**args)
            cube.add_aux_factory(factory)

        return cube

    def _nd_index(self, position):
        """
        Returns the n-dimensional index of this source-cube (position),
        within the merged cube.

        """

        index = []

        # Determine the index of the source-cube cell for each dimension.
        for name in self._nd_names:
            if _is_combination(name):
                members = name.split(_COMBINATION_JOIN)
                cell = tuple(
                    position[int(member) if member.isdigit() else member]
                    for member in members)
                index.append(self._cache_by_name[name][cell])
            else:
                index.append(self._cache_by_name[name][position[name]])

        return tuple(index)

    def _build_coordinates(self):
        """
        Build the dimension and auxiliary coordinates for the final
        merged cube given that the final dimensionality of the target
        merged cube is known and the associated dimension/s that each
        coordinate maps onto in that merged cube.

        The associated vector coordinate/s of the ProtoCube are also
        created with the correct space dimensionality and dimension/s
        mappings.

        """
        # Containers for the newly created coordinates and associated
        # dimension mappings.
        dim_coords_and_dims = self._dim_coords_and_dims
        aux_coords_and_dims = self._aux_coords_and_dims

        # Build the dimension coordinates.
        for template in self._dim_templates:
            # Sometimes it's not possible to build a dim coordinate e.g.
            # the bounds are not monotonic, so try building the coordinate,
            # and if it fails make the coordinate into an auxiliary coordinate.
            # This will ultimately make an anonymous dimension.
            try:
                coord = iris.coords.DimCoord(template.points,
                                             bounds=template.bounds,
                                             **template.kwargs)
                dim_coords_and_dims.append(_CoordAndDims(coord, template.dims))
            except ValueError:
                self._aux_templates.append(template)

        # There is the potential that there are still anonymous dimensions.
        # Get a list of the dimensions which are not anonymous at this stage.
        covered_dims = [dim_coord_and_dim.dims
                        for dim_coord_and_dim in dim_coords_and_dims]

        # Build the auxiliary coordinates.
        for template in self._aux_templates:
            # Attempt to build a DimCoord and add it to the cube. If this
            # fails e.g it's non-monontic or multi-dimensional or non-numeric,
            # then build an AuxCoord.
            try:
                coord = iris.coords.DimCoord(template.points,
                                             bounds=template.bounds,
                                             **template.kwargs)
                if len(template.dims) == 1 and \
                        template.dims[0] not in covered_dims:
                    dim_coords_and_dims.append(
                        _CoordAndDims(coord, template.dims))
                    covered_dims.append(template.dims[0])
                else:
                    aux_coords_and_dims.append(
                        _CoordAndDims(coord, template.dims))
            except ValueError:
                # kwarg not applicable to AuxCoord.
                template.kwargs.pop('circular', None)
                coord = iris.coords.AuxCoord(template.points,
                                             bounds=template.bounds,
                                             **template.kwargs)
                aux_coords_and_dims.append(_CoordAndDims(coord, template.dims))

        # Mix in the vector coordinates.
        for item, dims in zip(self._coord_signature.vector_dim_coords_and_dims,
                              self._vector_dim_coords_dims):
            dim_coords_and_dims.append(_CoordAndDims(item.coord, dims))

        for item, dims in zip(self._coord_signature.vector_aux_coords_and_dims,
                              self._vector_aux_coords_dims):
            aux_coords_and_dims.append(_CoordAndDims(item.coord, dims))

    def _build_signature(self, cube):
        """Generate the signature that defines this cube."""
        array = cube.lazy_data()
        return _CubeSignature(cube.metadata, cube.shape, array.dtype,
                              array.fill_value)

    def _add_cube(self, cube, coord_payload):
        """Create and add the source-cube skeleton to the ProtoCube."""
        if cube.has_lazy_data():
            data = cube.lazy_data()
        else:
            data = cube.data
        skeleton = _Skeleton(coord_payload.scalar.values, data)
        # Attempt to do something sensible with mixed scalar dtypes.
        for i, metadata in enumerate(coord_payload.scalar.metadata):
            if metadata.points_dtype > self._coord_metadata[i].points_dtype:
                self._coord_metadata[i] = metadata
        self._skeletons.append(skeleton)

    def _extract_coord_payload(self, cube):
        """
        Extract all relevant coordinate data and metadata from the cube.

        In particular, for each scalar coordinate determine its definition,
        its cell (point and bound) value and all other scalar coordinate
        metadata that allows us to fully reconstruct that scalar
        coordinate. Note that all scalar data is sorted in order of the
        scalar coordinate definition.

        The coordinate payload of the cube also includes any associated vector
        coordinates that describe that cube, and descriptions of any auxiliary
        coordinate factories.

        """
        scalar_defns = []
        scalar_values = []
        scalar_metadata = []
        vector_dim_coords_and_dims = []
        vector_aux_coords_and_dims = []

        cube_aux_coords = cube.aux_coords
        coords = cube.dim_coords + cube_aux_coords
        cube_aux_coord_ids = {id(coord) for coord in cube_aux_coords}

        # Coordinate hint ordering dictionary - from most preferred to least.
        # Copes with duplicate hint entries, where the most preferred is king.
        hint_dict = {name: i for i, name in zip(range(len(self._hints), 0, -1),
                                                self._hints[::-1])}
        # Coordinate axis ordering dictionary.
        axis_dict = {'T': 0, 'Z': 1, 'Y': 2, 'X': 3}

        # Coordinate sort function.
        # NB. This makes use of two properties which don't end up in
        # the CoordDefn used by scalar_defns: `coord.points.dtype` and
        # `type(coord)`.
        def key_func(coord):
            return (not np.issubdtype(coord.points.dtype, np.number),
                    not isinstance(coord, iris.coords.DimCoord),
                    hint_dict.get(coord.name(), len(hint_dict) + 1),
                    axis_dict.get(iris.util.guess_coord_axis(coord),
                                  len(axis_dict) + 1),
                    coord._as_defn())

        # Order the coordinates by hints, axis, and definition.
        for coord in sorted(coords, key=key_func):
            if not cube.coord_dims(coord) and coord.shape == (1,):
                # Extract the scalar coordinate data and metadata.
                scalar_defns.append(coord._as_defn())
                # Because we know there's a single Cell in the
                # coordinate, it's quicker to roll our own than use
                # Coord.cell().
                points = coord.points
                bounds = coord.bounds
                points_dtype = points.dtype
                if bounds is not None:
                    bounds_dtype = bounds.dtype
                    bounds = bounds[0]
                else:
                    bounds_dtype = None
                scalar_values.append(iris.coords.Cell(points[0], bounds))
                kwargs = {}
                if isinstance(coord, iris.coords.DimCoord):
                    kwargs['circular'] = coord.circular
                scalar_metadata.append(_CoordMetaData(points_dtype,
                                                      bounds_dtype, kwargs))
            else:
                # Extract the vector coordinate and metadata.
                if id(coord) in cube_aux_coord_ids:
                    vector_aux_coords_and_dims.append(
                        _CoordAndDims(coord, tuple(cube.coord_dims(coord))))
                else:
                    vector_dim_coords_and_dims.append(
                        _CoordAndDims(coord, tuple(cube.coord_dims(coord))))

        factory_defns = []
        for factory in sorted(cube.aux_factories,
                              key=lambda factory: factory._as_defn()):
            dependency_defns = []
            dependencies = factory.dependencies
            for key in sorted(dependencies):
                coord = dependencies[key]
                dependency_defns.append((key, coord._as_defn()))
            factory_defn = _FactoryDefn(type(factory), dependency_defns)
            factory_defns.append(factory_defn)

        scalar = _ScalarCoordPayload(scalar_defns, scalar_values,
                                     scalar_metadata)
        vector = _VectorCoordPayload(vector_dim_coords_and_dims,
                                     vector_aux_coords_and_dims)

        return _CoordPayload(scalar, vector, factory_defns)

########NEW FILE########
__FILENAME__ = generate_std_names
# (C) British Crown Copyright 2010 - 2013, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
A script to convert the standard names information from the provided XML
file into a Python dictionary format.

Takes two arguments: the first is the XML file to process and the second
is the name of the file to write the Python dictionary file into.

By default, Iris will use the source XML file:
    etc/cf-standard-name-table.xml
as obtained from:
    http://cf-pcmdi.llnl.gov/documents/cf-standard-names

"""

import argparse
import pprint
import xml.etree.ElementTree as ET


STD_VALUES_FILE_TEMPLATE = '''
# (C) British Crown Copyright 2010 - 2012, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
This file contains a dictionary of standard value names that are mapped
to another dictionary of other standard name attributes. Currently only
the `canonical_unit` exists in these attribute dictionaries.

This file is automatically generated. Do not edit this file by hand.

The file will be generated during a standard build/installation:
    python setup.py build
    python setup.py install

Also, the file can be re-generated in the source distribution via:
    python setup.py std_names

Or for more control (e.g. to use an alternative XML file) via:
    python tools/generate_std_names.py XML_FILE MODULE_FILE

"""


STD_NAMES = '''.lstrip()


def process_name_table(tree, element_name, *child_elements):
    """
    Yields a series of dictionaries with the key being the id of the entry element and the value containing
    another dictionary mapping other attributes of the standard name to their values, e.g. units, description, grib value etc.
    """
    for elem in tree.iterfind(element_name):
        sub_section = {}

        for child_elem in child_elements:
            found_elem = elem.find(child_elem)
            sub_section[child_elem] = found_elem.text if found_elem is not None else None

        yield {elem.get("id") : sub_section}


def to_dict(infile, outfile):
    values = {}
    aliases = {}

    tree = ET.parse(infile)

    for section in process_name_table(tree, 'entry', 'canonical_units'):
        values.update(section)

    for section in process_name_table(tree, 'alias', 'entry_id'):
        aliases.update(section)

    for key, valued in aliases.iteritems():
        values.update({
                key : {'canonical_units' : values.get(valued['entry_id']).get('canonical_units')}
            })

    outfile.write(STD_VALUES_FILE_TEMPLATE + pprint.pformat(values))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description='Create Python code from CF standard name XML.')
    parser.add_argument('input', type=argparse.FileType(),
                        metavar='INPUT',
                        help='Path to CF standard name XML')
    parser.add_argument('output', type=argparse.FileType('w'),
                        metavar='OUTPUT',
                        help='Path to resulting Python code')
    args = parser.parse_args()
    to_dict(args.input, args.output)

########NEW FILE########
__FILENAME__ = gen_helpers
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

from datetime import datetime
import os
import os.path

HEADER = \
    '''# (C) British Crown Copyright 2013 - {}, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
#
# DO NOT EDIT: AUTO-GENERATED'''


def absolute_path(path):
    return os.path.abspath(os.path.join(os.path.dirname(__file__), path))


def prep_module_file(module_path):
    """
    prepare a module file, creating directory if needed and writing the
    header into that file

    """
    module_path = absolute_path(module_path)
    module_dir = os.path.dirname(module_path)
    if not os.path.isdir(module_dir):
        os.makedirs(module_dir)
    with open(module_path, 'w') as module_file:
        module_file.write(HEADER.format(datetime.utcnow().year))

########NEW FILE########
__FILENAME__ = gen_stash_refs
# (C) British Crown Copyright 2013 - 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.

import json
import urllib
import urllib2

from iris.fileformats.pp import STASH

import gen_helpers


HEADER = '''
"""
Auto-generated from iris/tools/gen_stash_refs.py
Relates grid code and field code to the stash code.

"""
'''


CODE_PREAMBLE = ("from collections import namedtuple\n\n\n"
                 "Stash = namedtuple('Stash', 'grid_code field_code')\n\n\n")


def write_cross_reference_module(module_path, xrefs):
    gen_helpers.prep_module_file(module_path)
    with open(module_path, 'a') as module_file:
        module_file.write(HEADER)
        module_file.write(CODE_PREAMBLE)
        module_file.write('STASH_TRANS = {\n')
        for xref in xrefs:
            stash = xref.get('stash')
            try:
                STASH.from_msi(stash.replace('"', ''))
            except ValueError:
                msg = ('stash code is not of a recognised'
                       '"m??s??i???" form: {}'.format(stash))
                print msg
            grid = xref.get('grid')
            if grid is not None:
                try:
                    int(grid)
                except ValueError:
                    msg = ('grid code retrieved from STASH lookup'
                           'is not an interger: {}'.format(grid))
                    print msg
            else:
                grid = 0
            lbfc = xref.get('lbfcn')
            try:
                int(lbfc)
            except (ValueError, TypeError):
                lbfc = 0
            module_file.write(
                '    "{}": Stash({}, {}),\n'.format(stash, grid, lbfc))
        module_file.write('}\n')


def stash_grid_retrieve():
    """return a dictionary of stash codes and rel;ated information from
    the Met Office Reference Registry
    """
    baseurl = 'http://reference.metoffice.gov.uk/system/query?query='
    query = '''prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>
prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>
prefix skos: <http://www.w3.org/2004/02/skos/core#>

SELECT ?stash ?grid ?lbfcn
WHERE {
  ?stashcode rdf:type <http://reference.metoffice.gov.uk/um/c4/stash/Stash> ;
  skos:notation ?stash ;
  <http://reference.metoffice.gov.uk/um/c4/stash/grid> ?gridcode .
OPTIONAL { ?gridcode skos:notation ?grid .}
OPTIONAL {?stashcode <http://reference.metoffice.gov.uk/um/c4/stash/ppfc> ?lbfc .
         ?lbfc skos:notation ?lbfcn .}
}
order by ?stash'''
    
    encquery = urllib.quote_plus(query)
    out_format = '&output=json'
    url = baseurl + encquery + out_format

    response = urllib2.urlopen(url)
    stash = json.loads(response.read())

    ## heads will be of the form [u'stash', u'grid', u'lbfcn']
    ## as defined in the query string
    heads = stash['head']['vars']

    stashcodes = []

    for result in stash['results']['bindings']:
        res = {}
        for head in heads:
            if result.has_key(head):
                res[head] = result[head]['value']
        stashcodes.append(res)
    return stashcodes


if __name__ == '__main__':
    xrefs = stash_grid_retrieve()
    outfile = '../lib/iris/fileformats/_ff_cross_references.py'
    write_cross_reference_module(outfile, xrefs)

########NEW FILE########
__FILENAME__ = gen_translations
# (C) British Crown Copyright 2014, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
"""
Processing of metarelate metOcean content to provide Iris encodings of
metOcean mapping translations.

"""

from datetime import datetime
import os.path

from metarelate.fuseki import FusekiServer

from translator import *


HEADER = """# (C) British Crown Copyright 2013 - {year}, Met Office
#
# This file is part of Iris.
#
# Iris is free software: you can redistribute it and/or modify it under
# the terms of the GNU Lesser General Public License as published by the
# Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Iris is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public License
# along with Iris.  If not, see <http://www.gnu.org/licenses/>.
#
# DO NOT EDIT: AUTO-GENERATED
{doc_string}
from collections import namedtuple


CFName = namedtuple('CFName', 'standard_name long_name units')
"""

HEADER_GRIB = """
DimensionCoordinate = namedtuple('DimensionCoordinate',
                                 'standard_name units points')

G1LocalParam = namedtuple('G1LocalParam', 'edition t2version centre iParam')
G2Param = namedtuple('G2Param', 'edition discipline category number')
"""

DOC_STRING_GRIB = r'''"""
Provides GRIB/CF phenomenon translations.

"""'''

DOC_STRING_UM = r'''"""
Provides UM/CF phenomenon translations.

"""'''

DIR_BASE = '../lib/iris/fileformats'
FILE_UM_CF = os.path.join(DIR_BASE, 'um_cf_map.py')
FILE_GRIB_CF = os.path.join(DIR_BASE, 'grib', '_grib_cf_map.py')
YEAR = datetime.utcnow().year


def _retrieve_mappings(fuseki, source, target):
    """
    Interrogate the metarelate triple store for all
    phenomenon translation mappings from the source
    scheme to the target scheme.

    Args:
    * fuseki:
        The :class:`metrelate.fuseki.FusekiServer` instance.
    * source:
        The source metarelate scheme for the mapping,
        i.e. 'um', 'grib' or 'cf'.
    * target:
        The target metarelate scheme for the mapping.

    Return:
        The sequence of :class:`metarelate.Mapping`
        instances.

    """
    msg = 'Retrieving {!r} to {!r} mappings ...'
    print msg.format(source.upper(), target.upper())
    return fuseki.retrieve_mappings(source, target)


def build_um_cf_map(fuseki, filename):
    """
    Encode the UM/CF phenomenon translation mappings
    within the specified file.

    Args:
    * fuseki:
        The :class:`metarelate.fuseki.FusekiServer` instance.
    * filename:
        The name of the file to contain the translations.

    """
    # Create the base directory.
    if not os.path.exists(os.path.dirname(filename)):
        os.makedirs(os.path.dirname(filename))

    # Create the file to contain UM/CF translations.
    with open(filename, 'w') as fh:
        fh.write(HEADER.format(year=YEAR, doc_string=DOC_STRING_UM))
        fh.write('\n')

        # Encode the relevant UM to CF translations.
        mappings = _retrieve_mappings(fuseki, 'um', 'cf')
        fh.writelines(FieldcodeCFMapping(mappings).lines())
        fh.writelines(StashCFMapping(mappings).lines())

        # Encode the relevant CF to UM translations.
        mappings = _retrieve_mappings(fuseki, 'cf', 'um')
        fh.writelines(CFFieldcodeMapping(mappings).lines())


def build_grib_cf_map(fuseki, filename):
    """
    Encode the GRIB/CF phenomenon translation mappings
    within the specified file.

    Args:
    * fuseki:
        The :class:`metarelate.fuseki.FusekiServer` instance.
    * filename:
        The name of the file to contain the translations.

    """
    # Create the base directory.
    if not os.path.exists(os.path.dirname(filename)):
        os.makedirs(os.path.dirname(filename))

    # Create the file to contain GRIB/CF translations.
    with open(FILE_GRIB_CF, 'w') as fh:
        fh.write(HEADER.format(year=YEAR, doc_string=DOC_STRING_GRIB))
        fh.write(HEADER_GRIB)
        fh.write('\n')

        # Encode the relevant GRIB to CF translations.
        mappings = _retrieve_mappings(fuseki, 'grib', 'cf')
        fh.writelines(GRIB1LocalParamCFConstrainedMapping(mappings).lines())
        fh.writelines(GRIB1LocalParamCFMapping(mappings).lines())
        fh.writelines(GRIB2ParamCFMapping(mappings).lines())

        # Encode the relevant CF to GRIB translations.
        mappings = _retrieve_mappings(fuseki, 'cf', 'grib')
        fh.writelines(CFConstrainedGRIB1LocalParamMapping(mappings).lines())
        fh.writelines(CFGRIB1LocalParamMapping(mappings).lines())
        fh.writelines(CFGRIB2ParamMapping(mappings).lines())


if __name__ == '__main__':
    with FusekiServer() as fuseki:
        build_um_cf_map(fuseki, FILE_UM_CF)
        build_grib_cf_map(fuseki, FILE_GRIB_CF)

########NEW FILE########
