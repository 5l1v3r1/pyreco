__FILENAME__ = build
import argparse

import build_owl
import build_supervisor
import build_tank
import build_utils

from minos_config import Log
from minos_config import TANK_DEFAULT_IP
from minos_config import TANK_DEFAULT_PORT

COMPONENT_BUILD_TOOL_MAP = {
  "tank": build_tank,
  "supervisor": build_supervisor,
  "owl": build_owl,
}

def add_component_arguments(parser):
  parser.add_argument("component",
    choices=COMPONENT_BUILD_TOOL_MAP.keys(),
    help="The component to built.")
  parser.add_argument("--tank_ip", type=str, nargs="?",
    default=TANK_DEFAULT_IP,
    help="The ip of localhost to use for tank server.")
  parser.add_argument("--tank_port", type=int, nargs="?",
    default=TANK_DEFAULT_PORT,
    help="The port to use for tank server.")
  parser.add_argument("--owl_ip", type=str, nargs="?",
    default='127.0.0.1',
    help="The localhost ip for owl configuration.")
  parser.add_argument("--owl_port", type=int, nargs="?",
    default=0,
    help="The port to use for owl monitor.")

def parse_command_line():
  parser = argparse.ArgumentParser(
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    description="Manage the Minos components.")

  subparsers = parser.add_subparsers(
    title="commands",
    help="Type '%(prog)s command -h' to get more help for individual command.")

  sub_parser = subparsers.add_parser(
    "start",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    help="Start the specified Minos component.")
  sub_parser.add_argument("--skip_setup_hbase", action="store_true", default=False,
    help="Whether skip setting up the default stand-alone hbase or not.")
  sub_parser.add_argument("--quota_updater", action="store_true", default=False,
    help="Whether starting quota updater or not.")
  add_component_arguments(sub_parser)
  sub_parser.set_defaults(handler=process_command_start)

  sub_parser = subparsers.add_parser(
    "stop",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    help="Stop the specified Minos component.")
  add_component_arguments(sub_parser)
  sub_parser.set_defaults(handler=process_command_stop)

  sub_parser = subparsers.add_parser(
    "build",
    formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    help="Build Minos client, Tank, Supervisor offline.")
  sub_parser.add_argument("--offline", action="store_true", default=False,
    help="Whether build offline or not for Client, Tank, Supervisor.")
  sub_parser.add_argument("--package_dir", type=str, nargs="?",
    default=None, help="The offline packages directory.")
  sub_parser.set_defaults(handler=process_command_build)

  args = parser.parse_args()
  return args

def process_command_start(args):
  build_tool = COMPONENT_BUILD_TOOL_MAP.get(args.component)
  if build_tool:
    return build_tool.start(args)
  Log.print_critical("Unknown component name: %s.", args.component)

def process_command_stop(args):
  build_tool = COMPONENT_BUILD_TOOL_MAP.get(args.component)
  if build_tool:
    return build_tool.stop(args)
  Log.print_critical("Unknown component name: %s.", args.component)

def process_command_build(args):
  if not args.offline or not args.package_dir:
    Log.print_critical("ERROR: Building Minos offline needs to specify " \
      "the arguments '--offline' and the offline packages directory " \
      "'--package_dir' explicitly")

  build_utils.pip_install_offline(args.package_dir)

def main():
  args = parse_command_line()
  return args.handler(args)

if __name__ == '__main__':
  main()


########NEW FILE########
__FILENAME__ = build_client
import os

import build_utils

from minos_config import CLIENT_PREREQUISITE_PYTHON_LIBS
from minos_config import Log

def build_client():
  # Check and install prerequisite python libraries
  Log.print_info("Check and install prerequisite python libraries")
  build_utils.check_and_install_modules(CLIENT_PREREQUISITE_PYTHON_LIBS)
  Log.print_success("Build Minos client success")

if __name__ == '__main__':
  build_client()

########NEW FILE########
__FILENAME__ = build_owl
import getpass
import os
import subprocess
import warnings

import build_utils
from build_utils import MINOS_ROOT

from minos_config import HBASE_CONFIG_FILE
from minos_config import HBASE_CONFIG_ROOT
from minos_config import HBASE_ROOT
from minos_config import HBASE_TARBALL
from minos_config import Log
from minos_config import OPENTSDB_COLLECTOR_CONFIG_FILE
from minos_config import OPENTSDB_CONFIG_ROOT
from minos_config import OPENTSDB_PORT
from minos_config import OPENTSDB_REPOSITORY
from minos_config import OWL_CONFIG_FILE
from minos_config import OWL_PREREQUISITE_PYTHON_LIBS

BUILD_BIN_ROOT = os.getenv("BUILD_BIN_ROOT")
BUILD_DOWNLOAD_ROOT = os.getenv("BUILD_DOWNLOAD_ROOT")
ENV_PYTHON = os.getenv("ENV_PYTHON")
HBASE_CONFIG_TEMPLATE = os.getenv("HBASE_CONFIG_TEMPLATE")
HBASE_PID_FILE = os.getenv("HBASE_PID_FILE")
OPENTSDB_BIN_PATH = os.getenv("OPENTSDB_BIN_PATH")
OPENTSDB_COLLECTOR_CONFIG_TEMPLATE = os.getenv("OPENTSDB_COLLECTOR_CONFIG_TEMPLATE")
OPENTSDB_COLLECTOR_PID_FILE = os.getenv("OPENTSDB_COLLECTOR_PID_FILE")
OPENTSDB_COLLECTOR_ROOT = os.getenv("OPENTSDB_COLLECTOR_ROOT")
OPENTSDB_PID_FILE = os.getenv("OPENTSDB_PID_FILE")
OPENTSDB_ROOT = os.getenv("OPENTSDB_ROOT")
OWL_COLLECTOR_PID_FILE = os.getenv("OWL_COLLECTOR_PID_FILE")
OWL_CONFIG_TEMPLATE = os.getenv("OWL_CONFIG_TEMPLATE")
OWL_MONITOR_PID_FILE = os.getenv("OWL_MONITOR_PID_FILE")
OWL_ROOT = os.getenv("OWL_ROOT")
OWL_SETTING_FILE = os.getenv("OWL_SETTING_FILE")
OWL_SETTING_TEMPLATE = os.getenv("OWL_SETTING_TEMPLATE")
QUOTA_UPDATER_PID_FILE = os.getenv("QUOTA_UPDATER_PID_FILE")
STOP_PROCESS_SCRIPT = os.getenv("STOP_PROCESS_SCRIPT")

# Check third-party tool exists
def check_third_party_tool_exists(tool_name):
  cmd = "which %s" % tool_name
  error_message = "Please install %s firstly" % tool_name
  build_utils.check_command_output(cmd, error_message=error_message)

def create_owl_database(args, database_name, host="", port=""):
  root_pwd = getpass.getpass("Please enter password of the Mysql root user: ")

  # Create owl
  import MySQLdb as db
  try:
    if host and port:
      conn = db.connect(host=host, user='root', passwd=root_pwd, port=int(port))
    else:
      conn = db.connect(user='root', passwd=root_pwd)
  except db.Error, e:
    Log.print_critical("ERROR: %s" % str(e))

  cursor = conn.cursor()
  warnings.filterwarnings('ignore', "Can't create .*")
  cursor.execute("create database if not exists %s;" % database_name)
  cursor.execute("use mysql;")
  cursor.execute("GRANT ALL ON %s.* TO 'owl'@'localhost' identified by 'owl';"
    % database_name)
  cursor.execute("GRANT ALL ON %s.* TO 'owl'@'%s' identified by 'owl';"
    % (database_name, args.owl_ip))
  cursor.execute("flush privileges;")

  cursor.close()
  conn.close()

def configure_mysql_for_owl(database_name, host='localhost', port='3306'):
  Log.print_info("Configuring mysql for owl in %s" % OWL_SETTING_FILE)
  owl_setting_dict = {
    'DATABASE': database_name,
    'HOST': host,
    'PORT': port,
  }
  build_utils.generate_config_file(OWL_SETTING_TEMPLATE,
    OWL_SETTING_FILE, owl_setting_dict)

def create_and_configure_mysql_for_owl(args):
  if build_utils.get_build_info_option('owl', 'mysql') == 'created':
    return
  # Support both local and remote database
  choice = raw_input("Please choose Mysql server you want to use " \
    "(1 for Local, 2 for Remote): ")
  owl_prefix = raw_input("Please enter the prefix of your owl database name " \
    "(default: %s): " % getpass.getuser())
  if not owl_prefix:
    owl_prefix = getpass.getuser()
  database_name = "%s_owl" % owl_prefix

  # Using local mysql
  if int(choice) == 1:
    # Check mysql server is running
    cmd = 'ps -ef | grep mysqld | grep -v grep'
    error_message = "Please start mysql server firstly"
    build_utils.check_command_output(cmd, error_message=error_message)
    # Create owl database
    create_owl_database(args, database_name)
    # Configure mysql for owl
    configure_mysql_for_owl(database_name)

  # Using remote mysql
  elif int(choice) == 2:
    remote_address = raw_input("Please input the remote mysql " \
      "server's address (ip:port): ")
    remote_host, remote_port = remote_address.split(":")
    # Create owl database
    create_owl_database(args, database_name, host=remote_host, port=remote_port)
    # Configure mysql for owl
    configure_mysql_for_owl(database_name, remote_host, remote_port)
  else:
    Log.print_critical("ERROR: invalid choice")

  # Mark mysql database created
  build_utils.output_build_info('owl', 'mysql', 'created')

def create_django_database():
  django_entry = os.path.join(OWL_ROOT, 'manage.py')
  cmd = [ENV_PYTHON, "%s" % django_entry, "syncdb"]
  build_utils.execute_command(cmd)

def deploy_opentsdb():
  if not os.path.exists(OPENTSDB_ROOT):
    log_message = "Checkout opentsdb in %s" % OPENTSDB_ROOT
    cmd = ["git", "clone", "%s" % OPENTSDB_REPOSITORY, "%s" % OPENTSDB_ROOT]
    build_utils.execute_command(cmd, log_message=log_message)
    # copy the startup script to the OPENTSDB_ROOT
    cmd = ["cp", "%s/start_opentsdb.sh" % BUILD_BIN_ROOT, OPENTSDB_ROOT]
    build_utils.execute_command(cmd)

  # Compile opentsdb
  os.chdir(OPENTSDB_ROOT)
  log_message = "Compiling opentsdb in %s" % OPENTSDB_ROOT
  cmd = ["./build.sh"]
  build_utils.execute_command(cmd, log_message=log_message)
  os.chdir(MINOS_ROOT)

def generate_hbase_configuration():
  Log.print_info("Modify hbase-site.xml in %s" % HBASE_CONFIG_ROOT)
  cmd = "hbase_rootdir=${TMPDIR-'/tmp'}/tsdhbase;" \
    "iface=lo`uname | sed -n s/Darwin/0/p`; echo $hbase_rootdir,$iface"
  hbase_rootdir, iface = build_utils.get_command_variable(cmd).split(',')

  configuration_dict = {
    'hbase_rootdir': hbase_rootdir,
    'iface': iface,
  }
  build_utils.generate_config_file(HBASE_CONFIG_TEMPLATE,
    HBASE_CONFIG_FILE, configuration_dict)

def build_hbase():
  if build_utils.get_build_info_option('owl', 'hbase') == 'built':
    return

  if not os.path.exists(BUILD_DOWNLOAD_ROOT):
    os.mkdir(BUILD_DOWNLOAD_ROOT)
  os.chdir(BUILD_DOWNLOAD_ROOT)

  log_message = "Setup hbase in %s" % BUILD_DOWNLOAD_ROOT
  if not os.path.exists(os.path.basename(HBASE_TARBALL)):
    cmd = ["wget", "%s" % HBASE_TARBALL]
    build_utils.execute_command(cmd, log_message=log_message)

  if not os.path.exists(HBASE_ROOT):
    cmd = ["tar", "xfz", "%s" % os.path.basename(HBASE_TARBALL)]
    build_utils.execute_command(cmd)

  generate_hbase_configuration()
  os.chdir(MINOS_ROOT)

  # Mark hbase built
  build_utils.output_build_info('owl', 'hbase', 'built')

def create_hbase_table():
  if build_utils.get_build_info_option('owl', 'hbase_table') == 'created':
    return
  os.chdir(OPENTSDB_ROOT)
  log_message = "Creating hbase table for opentsdb in %s" % OPENTSDB_ROOT
  cmd = ["env", "COMPRESSION=NONE", "HBASE_HOME=%s" % HBASE_ROOT, "./src/create_table.sh"]
  build_utils.execute_command(cmd, log_message=log_message)
  os.chdir(MINOS_ROOT)

  # Mark hbase table created
  build_utils.output_build_info('owl', 'hbase_table', 'created')

def configure_opentsdb_collector(owl_port):
  # Configure opentsdb collector config file
  Log.print_info("Configuring opentsdb collector in %s" %
    OPENTSDB_COLLECTOR_CONFIG_FILE)
  opentsdb_collector_dict = {
    'owl_monitor_http_port': owl_port,
    'tsdb': OPENTSDB_BIN_PATH,
  }
  build_utils.generate_config_file(OPENTSDB_COLLECTOR_CONFIG_TEMPLATE,
    OPENTSDB_COLLECTOR_CONFIG_FILE, opentsdb_collector_dict)

def configure_owl_config(args):
  Log.print_info("Configure owl config file: %s" % OWL_CONFIG_FILE)
  owl_config_dict = {
    'owl_ip': args.owl_ip,
    'opentsdb_port': OPENTSDB_PORT,
  }
  build_utils.generate_config_file(OWL_CONFIG_TEMPLATE,
    OWL_CONFIG_FILE, owl_config_dict)

def check_input(input, yes='y'):
  return input.strip().lower() == yes.lower()

def start_hbase():
  # Start the stand-alone hbase
  build_utils.start_daemon_process('Hbase', HBASE_PID_FILE, HBASE_ROOT,
    './bin/start-hbase.sh')

def start_opentsdb():
  # Create hbase table for opentsdb
  create_hbase_table()
  # Start a TSD
  build_utils.start_daemon_process('Opentsdb', OPENTSDB_PID_FILE,
    OPENTSDB_ROOT, './start_opentsdb.sh', OPENTSDB_PORT)

def start_opentsdb_collector():
  build_utils.start_daemon_process('Opentsdb collector', OPENTSDB_COLLECTOR_PID_FILE,
    OPENTSDB_COLLECTOR_ROOT, './start_opentsdb_collector.sh')

def start_owl_collector():
  build_utils.start_daemon_process('Owl collector', OWL_COLLECTOR_PID_FILE,
    OWL_ROOT, './start_owl_collector.sh')

def start_quota_updater():
  build_utils.start_daemon_process('Quota updater', QUOTA_UPDATER_PID_FILE,
    OWL_ROOT, './start_quota_updater.sh')

def start_owl_monitor():
  owl_monitor_http_port = build_utils.get_build_info_option('owl', 'owl_port')
  if not owl_monitor_http_port:
    Log.print_critical("Owl port is null")

  build_utils.start_daemon_process('Owl monitor', OWL_MONITOR_PID_FILE,
    OWL_ROOT, './start_owl_monitor.sh', owl_monitor_http_port)

def stop_opentsdb_collector():
  build_utils.stop_daemon_process('Opentsdb collector', OPENTSDB_COLLECTOR_PID_FILE,
    OPENTSDB_COLLECTOR_ROOT, STOP_PROCESS_SCRIPT)

def stop_owl_collector():
  build_utils.stop_daemon_process('Owl collector', OWL_COLLECTOR_PID_FILE,
    OWL_ROOT, STOP_PROCESS_SCRIPT)

def stop_quota_updater():
  build_utils.stop_daemon_process('Quota updater', QUOTA_UPDATER_PID_FILE,
    OWL_ROOT, STOP_PROCESS_SCRIPT)

def stop_owl_monitor():
  build_utils.stop_daemon_process('Owl monitor', OWL_MONITOR_PID_FILE,
    OWL_ROOT, STOP_PROCESS_SCRIPT)

def _build(args):
  if args.owl_ip == '127.0.0.1' or args.owl_port == 0:
    Log.print_critical("ERROR: Building owl needs to specify the localhost ip " \
      "with '--owl_ip' and the owl monitor http port with '--owl_port'")

  Log.print_info("Building owl")
  # Check and install prerequisite python libraries
  Log.print_info("Check and install prerequisite python libraries")
  build_utils.check_and_install_modules(OWL_PREREQUISITE_PYTHON_LIBS)

  check_third_party_tool_exists("gnuplot")
  check_third_party_tool_exists("mysql")
  create_and_configure_mysql_for_owl(args)
  create_django_database()

  # Deploy hbase
  if not args.skip_setup_hbase:
    build_hbase()
    start_hbase()

  # Deploy opentsdb
  deploy_opentsdb()
  if not args.skip_setup_hbase:
    start_opentsdb()

  # Configure opentsdb collector
  configure_opentsdb_collector(str(args.owl_port))
  # Configure owl config
  configure_owl_config(args)

  # Output build information
  build_utils.output_build_info(args.component, 'owl_port', args.owl_port)
  build_utils.output_build_info(args.component, 'build_status', 'success')
  Log.print_info("The component %s is built successfully" % args.component)

def _do_start(args):
  start_owl_collector()

  if not args.skip_setup_hbase:
    start_opentsdb_collector()
  if args.quota_updater:
    start_quota_updater()

  start_owl_monitor()

def _do_stop():
  stop_owl_collector()
  stop_opentsdb_collector()
  stop_quota_updater()
  stop_owl_monitor()

def start(args):
  if not build_utils.get_build_info_option('owl', 'build_status') == 'success':
    _build(args)
  _do_start(args)

def stop(args):
  input = raw_input("Do you really want to do this ? (y/n)")
  if check_input(input):
    _do_stop()
  else:
    Log.print_info("Skip stopping owl component")


########NEW FILE########
__FILENAME__ = build_supervisor
import getpass
import os
from string import Template

import build_utils
from build_utils import MINOS_ROOT

from minos_config import Log
from minos_config import SUPERVISOR_DEPLOYMENT_DIRS
from minos_config import SUPERVISOR_PREREQUISITE_PYTHON_LIBS
from minos_config import TANK_DEFAULT_IP

SUPERVISOR_CONFIG_FILE = os.getenv("SUPERVISOR_CONFIG_FILE")
SUPERVISOR_CONFIG_TEMPLATE = os.getenv("SUPERVISOR_CONFIG_TEMPLATE")
SUPERVISOR_PID_FILE = os.getenv("SUPERVISOR_PID_FILE")
SUPERVISOR_ROOT = os.getenv("SUPERVISOR_ROOT")

def _create_deployment_directory(deploy_path):
  # Create deployment directory for supervisor
  for dir in SUPERVISOR_DEPLOYMENT_DIRS:
    deploy_dir = os.path.join(deploy_path, dir)
    if not os.path.exists(deploy_dir):
      Log.print_info('Creating the %s root %s' % (dir, deploy_dir))
      os.makedirs(deploy_dir)

def _deploy_supervisor(args, deploy_path):
  Log.print_info("Deploying supervisor in %s" % SUPERVISOR_ROOT)

  # Generate supervisord.conf according to the deploying information
  deploy_info_dict = {
    'DEPLOY_PATH': deploy_path,
    'PACKAGE_SERVER': "%s:%d" % (args.tank_ip, args.tank_port),
  }
  build_utils.generate_config_file(SUPERVISOR_CONFIG_TEMPLATE,
    SUPERVISOR_CONFIG_FILE, deploy_info_dict)

def _build(args):
  if args.tank_ip == TANK_DEFAULT_IP:
    Log.print_critical("ERROR: Building supervisor needs to specify the package server " \
      "with '--tank_ip' and '--tank_port'")

  Log.print_info("Building supervisor")
  # Check and install prerequisite python libraries
  Log.print_info("Check and install prerequisite python libraries")
  build_utils.check_and_install_modules(SUPERVISOR_PREREQUISITE_PYTHON_LIBS)

  # Create deployment directory
  deploy_path = raw_input("Please input the root directory to deploy services " \
    "(default: /home/%s): " % getpass.getuser())

  if deploy_path:
    deploy_path = os.path.abspath(os.path.realpath(deploy_path))
  else:
    deploy_path = "/home/%s" % getpass.getuser()
  _create_deployment_directory(deploy_path)

  # Deploy supervisor
  _deploy_supervisor(args, deploy_path)

  # Output build information
  build_utils.output_build_info(args.component, 'build_status', 'success')
  Log.print_info("The component %s is built successfully" % args.component)

def _do_start():
  build_utils.start_daemon_process('Supervisor', SUPERVISOR_PID_FILE,
    SUPERVISOR_ROOT, './start_supervisor.sh')

def _do_stop():
  build_utils.stop_daemon_process('Supervisor', SUPERVISOR_PID_FILE,
    SUPERVISOR_ROOT, './stop_supervisor.sh')

def start(args):
  if not build_utils.get_build_info_option('supervisor', 'build_status') == 'success':
    _build(args)
  _do_start()

def stop(args):
  _do_stop()


########NEW FILE########
__FILENAME__ = build_tank
import os

import build_utils

from build_utils import MINOS_ROOT

from minos_config import Log
from minos_config import TANK_DEFAULT_IP
from minos_config import TANK_DEFAULT_PORT
from minos_config import TANK_PREREQUISITE_PYTHON_LIBS

STOP_PROCESS_SCRIPT = os.getenv("STOP_PROCESS_SCRIPT")
TANK_ROOT = os.getenv("TANK_ROOT")
TANK_PID_FILE = os.getenv("TANK_PID_FILE")

def _build(args):
  Log.print_info("Building tank server")

  # Check and install prerequisite python libraries
  Log.print_info("Check and install prerequisite python libraries")
  build_utils.check_and_install_modules(TANK_PREREQUISITE_PYTHON_LIBS)

  # Output build information
  if args.tank_ip != TANK_DEFAULT_IP or args.tank_port != TANK_DEFAULT_PORT:
    build_utils.output_build_info(args.component, 'tank_ip', args.tank_ip)
    build_utils.output_build_info(args.component, 'tank_port', args.tank_port)

  build_utils.output_build_info(args.component, 'build_status', 'success')
  Log.print_info("The component %s is built successfully" % args.component)

def _do_start(args):
  tank_ip = build_utils.get_build_info_option('tank', 'tank_ip')
  tank_port = build_utils.get_build_info_option('tank', 'tank_port')
  if tank_ip and tank_port:
    args.tank_ip = tank_ip
    args.tank_port = int(tank_port)

  build_utils.start_daemon_process('Tank server', TANK_PID_FILE,
    TANK_ROOT, './start_tank.sh', args.tank_ip, str(args.tank_port))

def _do_stop():
  build_utils.stop_daemon_process('Tank server', TANK_PID_FILE,
    TANK_ROOT, STOP_PROCESS_SCRIPT)

def start(args):
  if not build_utils.get_build_info_option('tank', 'build_status') == 'success':
    _build(args)
  _do_start(args)

def stop(args):
  _do_stop()

########NEW FILE########
__FILENAME__ = build_utils
import ConfigParser
import os
import subprocess
from string import Template

from minos_config import Log

MINOS_ROOT = os.getenv("MINOS_ROOT")
ENV_PIP = os.getenv("ENV_PIP")
BUILD_INFO_FILE = os.getenv("BUILD_INFO_FILE")
BUILD_OFFLINE_REQUIREMENTS_FILE = os.getenv("BUILD_OFFLINE_REQUIREMENTS_FILE")

def execute_command(cmd, log_message="", error_message=""):
  if log_message:
    Log.print_info(log_message)
  try:
    subprocess.check_call(cmd)
  except BaseException, e:
    Log.print_critical('ERROR: %s' % error_message if error_message else str(e))

def check_command_output(cmd, error_message="", skip_error=False):
  try:
    out = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT)
  except BaseException, e:
    if skip_error:
      return 0
    else:
      Log.print_critical('ERROR: %s' % error_message if error_message else str(e))
  return 1

def get_command_variable(cmd):
  child = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
  out = child.communicate()
  return out[0].strip()

def get_process_running_pid(pid_file):
  try:
    with open(pid_file) as fp:
      return int(fp.readline())
  except ValueError, e:
    Log.print_critical("Error: Pid file %s is null" % pid_file)

def check_process_is_running(pid_file):
  if not os.path.exists(pid_file):
    return False

  process_pid = get_process_running_pid(pid_file)
  try:
    os.kill(process_pid, 0)
    return True
  except OSError:
    return False

def exec_daemon_script(dest_path, script, *extra_para):
  os.chdir(dest_path)
  cmd = ["%s" % script]
  cmd.extend(list(extra_para))
  execute_command(cmd)
  os.chdir(MINOS_ROOT)

def start_daemon_process(process_name, pid_file, dest_path, script, *extra_para):
  if check_process_is_running(pid_file):
    Log.print_warning("%s is running, please check" % process_name)
    return

  Log.print_info("Starting %s" % process_name)
  exec_daemon_script(dest_path, script, *extra_para)
  Log.print_success("Start %s success" % process_name)

def stop_daemon_process(process_name, pid_file, dest_path, script):
  if not check_process_is_running(pid_file):
    Log.print_warning("%s is not running" % process_name)
    return

  Log.print_info("Stopping %s" % process_name)
  exec_daemon_script(dest_path, script, str(get_process_running_pid(pid_file)))
  Log.print_success("Stop %s success" % process_name)

def generate_config_file(template_file, dest_file, config_dict):
  config_template = Template(open(template_file).read())
  config_file = config_template.safe_substitute(config_dict)

  with open(dest_file, 'w') as output:
    output.write(config_file)

def output_build_info(component, info_key, info_val):
  build_info_parser = ConfigParser.SafeConfigParser()
  build_info_parser.read([BUILD_INFO_FILE])

  if not build_info_parser.has_section(component):
    build_info_parser.add_section(component)
  build_info_parser.set(component, info_key, str(info_val))

  with open(BUILD_INFO_FILE, 'wb') as build_info:
    build_info_parser.write(build_info)

def get_build_info_option(component, option):
  build_info_parser = ConfigParser.SafeConfigParser()
  build_info_parser.read([BUILD_INFO_FILE])
  if build_info_parser.has_option(component, option):
    return build_info_parser.get(component, option)
  return None

def check_module_installed(module):
  try:
    __import__(module)
  except ImportError:
    return 0
  return 1

def pip_install_offline(offline_package_dir):
  cmd = [ENV_PIP, "install", "--no-index", "--find-links",
    offline_package_dir, "-r", BUILD_OFFLINE_REQUIREMENTS_FILE]
  execute_command(cmd)

def pip_install(module, module_version):
  log_message = "Installing %s" % module
  cmd = [ENV_PIP, "install", "%s>=%s" % (module, module_version)]
  execute_command(cmd, log_message=log_message)

def check_and_install_modules(modules_list):
  for module_key, module_val, module_version in modules_list:
    if not check_module_installed(module_key):
      pip_install(module_val, module_version)


########NEW FILE########
__FILENAME__ = minos_config
import os
import sys

import deploy_config
from log import Log

CONFIG_DIR = deploy_config.get_deploy_config().get_config_dir()

# client
CLIENT_PREREQUISITE_PYTHON_LIBS = [
  ('configobj', 'configobj', '4.7.2'),
]

# tank
TANK_PREREQUISITE_PYTHON_LIBS = [
  ('django', 'django', '1.5.5'),
]

TANK_DEFAULT_PORT = 8000
TANK_DEFAULT_IP = '0.0.0.0'

# supervisor
SUPERVISOR_PREREQUISITE_PYTHON_LIBS = [
  ('setuptools', 'setuptools', '1.4.0'),
  ('meld3', 'meld3', '0.6.10'),
  ('elementtree', 'elementtree', '1.2.6-20050316'),
  ('pexpect', 'pexpect', '3.0'),
]

SUPERVISOR_DEPLOYMENT_DIRS = [
  'app',
  'data',
  'log',
  'packages',
]

# owl
OWL_PREREQUISITE_PYTHON_LIBS = [
  ('django', 'django', '1.5.5'),
  ('twisted', 'twisted', '13.2.0'),
  ('MySQLdb', 'Mysql-python', '1.2.5'),
  ('DBUtils', 'dbutils', '1.1'),
]
OWL_CONFIG_ROOT = os.path.join(CONFIG_DIR, 'owl')
OWL_CONFIG_FILE = os.path.join(OWL_CONFIG_ROOT, 'owl_config.py')
OWL_COLLECTOR_FILE = os.path.join(OWL_CONFIG_ROOT, 'collector.cfg')

OPENTSDB_CONFIG_ROOT = os.path.join(CONFIG_DIR, 'opentsdb')
OPENTSDB_COLLECTOR_CONFIG_FILE = os.path.join(
  OPENTSDB_CONFIG_ROOT, 'metrics_collector_config.py')
OPENTSDB_REPOSITORY = 'git://github.com/OpenTSDB/opentsdb.git'
OPENTSDB_PORT = '4242'

HBASE_VERSION = 'hbase-0.94.14'
HBASE_ROOT = os.path.join(os.getenv("BUILD_DOWNLOAD_ROOT"), HBASE_VERSION)
HBASE_CONFIG_ROOT = os.path.join(HBASE_ROOT, 'conf')
HBASE_CONFIG_FILE = os.path.join(HBASE_CONFIG_ROOT, 'hbase-site.xml')
HBASE_TARBALL = "http://www.apache.org/dist/hbase/%s/%s.tar.gz" % (
  HBASE_VERSION, HBASE_VERSION)

########NEW FILE########
__FILENAME__ = virtual_bootstrap
#!/usr/bin/env python
"""Create a "virtual" Python installation
"""

__version__ = "1.11.4"
virtualenv_version = __version__  # legacy

import base64
import sys
import os
import codecs
import optparse
import re
import shutil
import logging
import tempfile
import zlib
import errno
import glob
import distutils.sysconfig
from distutils.util import strtobool
import struct
import subprocess
import tarfile

if sys.version_info < (2, 6):
    print('ERROR: %s' % sys.exc_info()[1])
    print('ERROR: this script requires Python 2.6 or greater.')
    sys.exit(101)

try:
    set
except NameError:
    from sets import Set as set
try:
    basestring
except NameError:
    basestring = str

try:
    import ConfigParser
except ImportError:
    import configparser as ConfigParser

join = os.path.join
py_version = 'python%s.%s' % (sys.version_info[0], sys.version_info[1])

is_jython = sys.platform.startswith('java')
is_pypy = hasattr(sys, 'pypy_version_info')
is_win = (sys.platform == 'win32')
is_cygwin = (sys.platform == 'cygwin')
is_darwin = (sys.platform == 'darwin')
abiflags = getattr(sys, 'abiflags', '')

user_dir = os.path.expanduser('~')
if is_win:
    default_storage_dir = os.path.join(user_dir, 'virtualenv')
else:
    default_storage_dir = os.path.join(user_dir, '.virtualenv')
default_config_file = os.path.join(default_storage_dir, 'virtualenv.ini')

if is_pypy:
    expected_exe = 'pypy'
elif is_jython:
    expected_exe = 'jython'
else:
    expected_exe = 'python'

# Return a mapping of version -> Python executable
# Only provided for Windows, where the information in the registry is used
if not is_win:
    def get_installed_pythons():
        return {}
else:
    try:
        import winreg
    except ImportError:
        import _winreg as winreg

    def get_installed_pythons():
        python_core = winreg.CreateKey(winreg.HKEY_LOCAL_MACHINE,
                "Software\\Python\\PythonCore")
        i = 0
        versions = []
        while True:
            try:
                versions.append(winreg.EnumKey(python_core, i))
                i = i + 1
            except WindowsError:
                break
        exes = dict()
        for ver in versions:
            path = winreg.QueryValue(python_core, "%s\\InstallPath" % ver)
            exes[ver] = join(path, "python.exe")

        winreg.CloseKey(python_core)

        # Add the major versions
        # Sort the keys, then repeatedly update the major version entry
        # Last executable (i.e., highest version) wins with this approach
        for ver in sorted(exes):
            exes[ver[0]] = exes[ver]

        return exes

REQUIRED_MODULES = ['os', 'posix', 'posixpath', 'nt', 'ntpath', 'genericpath',
                    'fnmatch', 'locale', 'encodings', 'codecs',
                    'stat', 'UserDict', 'readline', 'copy_reg', 'types',
                    're', 'sre', 'sre_parse', 'sre_constants', 'sre_compile',
                    'zlib']

REQUIRED_FILES = ['lib-dynload', 'config']

majver, minver = sys.version_info[:2]
if majver == 2:
    if minver >= 6:
        REQUIRED_MODULES.extend(['warnings', 'linecache', '_abcoll', 'abc'])
    if minver >= 7:
        REQUIRED_MODULES.extend(['_weakrefset'])
    if minver <= 3:
        REQUIRED_MODULES.extend(['sets', '__future__'])
elif majver == 3:
    # Some extra modules are needed for Python 3, but different ones
    # for different versions.
    REQUIRED_MODULES.extend(['_abcoll', 'warnings', 'linecache', 'abc', 'io',
                             '_weakrefset', 'copyreg', 'tempfile', 'random',
                             '__future__', 'collections', 'keyword', 'tarfile',
                             'shutil', 'struct', 'copy', 'tokenize', 'token',
                             'functools', 'heapq', 'bisect', 'weakref',
                             'reprlib'])
    if minver >= 2:
        REQUIRED_FILES[-1] = 'config-%s' % majver
    if minver >= 3:
        import sysconfig
        platdir = sysconfig.get_config_var('PLATDIR')
        REQUIRED_FILES.append(platdir)
        # The whole list of 3.3 modules is reproduced below - the current
        # uncommented ones are required for 3.3 as of now, but more may be
        # added as 3.3 development continues.
        REQUIRED_MODULES.extend([
            #"aifc",
            #"antigravity",
            #"argparse",
            #"ast",
            #"asynchat",
            #"asyncore",
            "base64",
            #"bdb",
            #"binhex",
            #"bisect",
            #"calendar",
            #"cgi",
            #"cgitb",
            #"chunk",
            #"cmd",
            #"codeop",
            #"code",
            #"colorsys",
            #"_compat_pickle",
            #"compileall",
            #"concurrent",
            #"configparser",
            #"contextlib",
            #"cProfile",
            #"crypt",
            #"csv",
            #"ctypes",
            #"curses",
            #"datetime",
            #"dbm",
            #"decimal",
            #"difflib",
            #"dis",
            #"doctest",
            #"dummy_threading",
            "_dummy_thread",
            #"email",
            #"filecmp",
            #"fileinput",
            #"formatter",
            #"fractions",
            #"ftplib",
            #"functools",
            #"getopt",
            #"getpass",
            #"gettext",
            #"glob",
            #"gzip",
            "hashlib",
            #"heapq",
            "hmac",
            #"html",
            #"http",
            #"idlelib",
            #"imaplib",
            #"imghdr",
            "imp",
            "importlib",
            #"inspect",
            #"json",
            #"lib2to3",
            #"logging",
            #"macpath",
            #"macurl2path",
            #"mailbox",
            #"mailcap",
            #"_markupbase",
            #"mimetypes",
            #"modulefinder",
            #"multiprocessing",
            #"netrc",
            #"nntplib",
            #"nturl2path",
            #"numbers",
            #"opcode",
            #"optparse",
            #"os2emxpath",
            #"pdb",
            #"pickle",
            #"pickletools",
            #"pipes",
            #"pkgutil",
            #"platform",
            #"plat-linux2",
            #"plistlib",
            #"poplib",
            #"pprint",
            #"profile",
            #"pstats",
            #"pty",
            #"pyclbr",
            #"py_compile",
            #"pydoc_data",
            #"pydoc",
            #"_pyio",
            #"queue",
            #"quopri",
            #"reprlib",
            "rlcompleter",
            #"runpy",
            #"sched",
            #"shelve",
            #"shlex",
            #"smtpd",
            #"smtplib",
            #"sndhdr",
            #"socket",
            #"socketserver",
            #"sqlite3",
            #"ssl",
            #"stringprep",
            #"string",
            #"_strptime",
            #"subprocess",
            #"sunau",
            #"symbol",
            #"symtable",
            #"sysconfig",
            #"tabnanny",
            #"telnetlib",
            #"test",
            #"textwrap",
            #"this",
            #"_threading_local",
            #"threading",
            #"timeit",
            #"tkinter",
            #"tokenize",
            #"token",
            #"traceback",
            #"trace",
            #"tty",
            #"turtledemo",
            #"turtle",
            #"unittest",
            #"urllib",
            #"uuid",
            #"uu",
            #"wave",
            #"weakref",
            #"webbrowser",
            #"wsgiref",
            #"xdrlib",
            #"xml",
            #"xmlrpc",
            #"zipfile",
        ])
    if minver >= 4:
        REQUIRED_MODULES.extend([
            'operator',
            '_collections_abc',
            '_bootlocale',
        ])

if is_pypy:
    # these are needed to correctly display the exceptions that may happen
    # during the bootstrap
    REQUIRED_MODULES.extend(['traceback', 'linecache'])

class Logger(object):

    """
    Logging object for use in command-line script.  Allows ranges of
    levels, to avoid some redundancy of displayed information.
    """

    DEBUG = logging.DEBUG
    INFO = logging.INFO
    NOTIFY = (logging.INFO+logging.WARN)/2
    WARN = WARNING = logging.WARN
    ERROR = logging.ERROR
    FATAL = logging.FATAL

    LEVELS = [DEBUG, INFO, NOTIFY, WARN, ERROR, FATAL]

    def __init__(self, consumers):
        self.consumers = consumers
        self.indent = 0
        self.in_progress = None
        self.in_progress_hanging = False

    def debug(self, msg, *args, **kw):
        self.log(self.DEBUG, msg, *args, **kw)
    def info(self, msg, *args, **kw):
        self.log(self.INFO, msg, *args, **kw)
    def notify(self, msg, *args, **kw):
        self.log(self.NOTIFY, msg, *args, **kw)
    def warn(self, msg, *args, **kw):
        self.log(self.WARN, msg, *args, **kw)
    def error(self, msg, *args, **kw):
        self.log(self.ERROR, msg, *args, **kw)
    def fatal(self, msg, *args, **kw):
        self.log(self.FATAL, msg, *args, **kw)
    def log(self, level, msg, *args, **kw):
        if args:
            if kw:
                raise TypeError(
                    "You may give positional or keyword arguments, not both")
        args = args or kw
        rendered = None
        for consumer_level, consumer in self.consumers:
            if self.level_matches(level, consumer_level):
                if (self.in_progress_hanging
                    and consumer in (sys.stdout, sys.stderr)):
                    self.in_progress_hanging = False
                    sys.stdout.write('\n')
                    sys.stdout.flush()
                if rendered is None:
                    if args:
                        rendered = msg % args
                    else:
                        rendered = msg
                    rendered = ' '*self.indent + rendered
                if hasattr(consumer, 'write'):
                    consumer.write(rendered+'\n')
                else:
                    consumer(rendered)

    def start_progress(self, msg):
        assert not self.in_progress, (
            "Tried to start_progress(%r) while in_progress %r"
            % (msg, self.in_progress))
        if self.level_matches(self.NOTIFY, self._stdout_level()):
            sys.stdout.write(msg)
            sys.stdout.flush()
            self.in_progress_hanging = True
        else:
            self.in_progress_hanging = False
        self.in_progress = msg

    def end_progress(self, msg='done.'):
        assert self.in_progress, (
            "Tried to end_progress without start_progress")
        if self.stdout_level_matches(self.NOTIFY):
            if not self.in_progress_hanging:
                # Some message has been printed out since start_progress
                sys.stdout.write('...' + self.in_progress + msg + '\n')
                sys.stdout.flush()
            else:
                sys.stdout.write(msg + '\n')
                sys.stdout.flush()
        self.in_progress = None
        self.in_progress_hanging = False

    def show_progress(self):
        """If we are in a progress scope, and no log messages have been
        shown, write out another '.'"""
        if self.in_progress_hanging:
            sys.stdout.write('.')
            sys.stdout.flush()

    def stdout_level_matches(self, level):
        """Returns true if a message at this level will go to stdout"""
        return self.level_matches(level, self._stdout_level())

    def _stdout_level(self):
        """Returns the level that stdout runs at"""
        for level, consumer in self.consumers:
            if consumer is sys.stdout:
                return level
        return self.FATAL

    def level_matches(self, level, consumer_level):
        """
        >>> l = Logger([])
        >>> l.level_matches(3, 4)
        False
        >>> l.level_matches(3, 2)
        True
        >>> l.level_matches(slice(None, 3), 3)
        False
        >>> l.level_matches(slice(None, 3), 2)
        True
        >>> l.level_matches(slice(1, 3), 1)
        True
        >>> l.level_matches(slice(2, 3), 1)
        False
        """
        if isinstance(level, slice):
            start, stop = level.start, level.stop
            if start is not None and start > consumer_level:
                return False
            if stop is not None and stop <= consumer_level:
                return False
            return True
        else:
            return level >= consumer_level

    #@classmethod
    def level_for_integer(cls, level):
        levels = cls.LEVELS
        if level < 0:
            return levels[0]
        if level >= len(levels):
            return levels[-1]
        return levels[level]

    level_for_integer = classmethod(level_for_integer)

# create a silent logger just to prevent this from being undefined
# will be overridden with requested verbosity main() is called.
logger = Logger([(Logger.LEVELS[-1], sys.stdout)])

def mkdir(path):
    if not os.path.exists(path):
        logger.info('Creating %s', path)
        os.makedirs(path)
    else:
        logger.info('Directory %s already exists', path)

def copyfileordir(src, dest, symlink=True):
    if os.path.isdir(src):
        shutil.copytree(src, dest, symlink)
    else:
        shutil.copy2(src, dest)

def copyfile(src, dest, symlink=True):
    if not os.path.exists(src):
        # Some bad symlink in the src
        logger.warn('Cannot find file %s (bad symlink)', src)
        return
    if os.path.exists(dest):
        logger.debug('File %s already exists', dest)
        return
    if not os.path.exists(os.path.dirname(dest)):
        logger.info('Creating parent directories for %s', os.path.dirname(dest))
        os.makedirs(os.path.dirname(dest))
    if not os.path.islink(src):
        srcpath = os.path.abspath(src)
    else:
        srcpath = os.readlink(src)
    if symlink and hasattr(os, 'symlink') and not is_win:
        logger.info('Symlinking %s', dest)
        try:
            os.symlink(srcpath, dest)
        except (OSError, NotImplementedError):
            logger.info('Symlinking failed, copying to %s', dest)
            copyfileordir(src, dest, symlink)
    else:
        logger.info('Copying to %s', dest)
        copyfileordir(src, dest, symlink)

def writefile(dest, content, overwrite=True):
    if not os.path.exists(dest):
        logger.info('Writing %s', dest)
        f = open(dest, 'wb')
        f.write(content.encode('utf-8'))
        f.close()
        return
    else:
        f = open(dest, 'rb')
        c = f.read()
        f.close()
        if c != content.encode("utf-8"):
            if not overwrite:
                logger.notify('File %s exists with different content; not overwriting', dest)
                return
            logger.notify('Overwriting %s with new content', dest)
            f = open(dest, 'wb')
            f.write(content.encode('utf-8'))
            f.close()
        else:
            logger.info('Content %s already in place', dest)

def rmtree(dir):
    if os.path.exists(dir):
        logger.notify('Deleting tree %s', dir)
        shutil.rmtree(dir)
    else:
        logger.info('Do not need to delete %s; already gone', dir)

def make_exe(fn):
    if hasattr(os, 'chmod'):
        oldmode = os.stat(fn).st_mode & 0xFFF # 0o7777
        newmode = (oldmode | 0x16D) & 0xFFF # 0o555, 0o7777
        os.chmod(fn, newmode)
        logger.info('Changed mode of %s to %s', fn, oct(newmode))

def _find_file(filename, dirs):
    for dir in reversed(dirs):
        files = glob.glob(os.path.join(dir, filename))
        if files and os.path.isfile(files[0]):
            return True, files[0]
    return False, filename

def file_search_dirs():
    here = os.path.dirname(os.path.abspath(__file__))
    dirs = ['.', here,
            join(here, 'virtualenv_support')]
    if os.path.splitext(os.path.dirname(__file__))[0] != 'virtualenv':
        # Probably some boot script; just in case virtualenv is installed...
        try:
            import virtualenv
        except ImportError:
            pass
        else:
            dirs.append(os.path.join(os.path.dirname(virtualenv.__file__), 'virtualenv_support'))
    return [d for d in dirs if os.path.isdir(d)]


class UpdatingDefaultsHelpFormatter(optparse.IndentedHelpFormatter):
    """
    Custom help formatter for use in ConfigOptionParser that updates
    the defaults before expanding them, allowing them to show up correctly
    in the help listing
    """
    def expand_default(self, option):
        if self.parser is not None:
            self.parser.update_defaults(self.parser.defaults)
        return optparse.IndentedHelpFormatter.expand_default(self, option)


class ConfigOptionParser(optparse.OptionParser):
    """
    Custom option parser which updates its defaults by checking the
    configuration files and environmental variables
    """
    def __init__(self, *args, **kwargs):
        self.config = ConfigParser.RawConfigParser()
        self.files = self.get_config_files()
        self.config.read(self.files)
        optparse.OptionParser.__init__(self, *args, **kwargs)

    def get_config_files(self):
        config_file = os.environ.get('VIRTUALENV_CONFIG_FILE', False)
        if config_file and os.path.exists(config_file):
            return [config_file]
        return [default_config_file]

    def update_defaults(self, defaults):
        """
        Updates the given defaults with values from the config files and
        the environ. Does a little special handling for certain types of
        options (lists).
        """
        # Then go and look for the other sources of configuration:
        config = {}
        # 1. config files
        config.update(dict(self.get_config_section('virtualenv')))
        # 2. environmental variables
        config.update(dict(self.get_environ_vars()))
        # Then set the options with those values
        for key, val in config.items():
            key = key.replace('_', '-')
            if not key.startswith('--'):
                key = '--%s' % key  # only prefer long opts
            option = self.get_option(key)
            if option is not None:
                # ignore empty values
                if not val:
                    continue
                # handle multiline configs
                if option.action == 'append':
                    val = val.split()
                else:
                    option.nargs = 1
                if option.action == 'store_false':
                    val = not strtobool(val)
                elif option.action in ('store_true', 'count'):
                    val = strtobool(val)
                try:
                    val = option.convert_value(key, val)
                except optparse.OptionValueError:
                    e = sys.exc_info()[1]
                    print("An error occured during configuration: %s" % e)
                    sys.exit(3)
                defaults[option.dest] = val
        return defaults

    def get_config_section(self, name):
        """
        Get a section of a configuration
        """
        if self.config.has_section(name):
            return self.config.items(name)
        return []

    def get_environ_vars(self, prefix='VIRTUALENV_'):
        """
        Returns a generator with all environmental vars with prefix VIRTUALENV
        """
        for key, val in os.environ.items():
            if key.startswith(prefix):
                yield (key.replace(prefix, '').lower(), val)

    def get_default_values(self):
        """
        Overridding to make updating the defaults after instantiation of
        the option parser possible, update_defaults() does the dirty work.
        """
        if not self.process_default_values:
            # Old, pre-Optik 1.5 behaviour.
            return optparse.Values(self.defaults)

        defaults = self.update_defaults(self.defaults.copy())  # ours
        for option in self._get_all_options():
            default = defaults.get(option.dest)
            if isinstance(default, basestring):
                opt_str = option.get_opt_string()
                defaults[option.dest] = option.check_value(opt_str, default)
        return optparse.Values(defaults)


def main():
    parser = ConfigOptionParser(
        version=virtualenv_version,
        usage="%prog [OPTIONS] DEST_DIR",
        formatter=UpdatingDefaultsHelpFormatter())

    parser.add_option(
        '-v', '--verbose',
        action='count',
        dest='verbose',
        default=0,
        help="Increase verbosity.")

    parser.add_option(
        '-q', '--quiet',
        action='count',
        dest='quiet',
        default=0,
        help='Decrease verbosity.')

    parser.add_option(
        '-p', '--python',
        dest='python',
        metavar='PYTHON_EXE',
        help='The Python interpreter to use, e.g., --python=python2.5 will use the python2.5 '
        'interpreter to create the new environment.  The default is the interpreter that '
        'virtualenv was installed with (%s)' % sys.executable)

    parser.add_option(
        '--clear',
        dest='clear',
        action='store_true',
        help="Clear out the non-root install and start from scratch.")

    parser.set_defaults(system_site_packages=False)
    parser.add_option(
        '--no-site-packages',
        dest='system_site_packages',
        action='store_false',
        help="DEPRECATED. Retained only for backward compatibility. "
             "Not having access to global site-packages is now the default behavior.")

    parser.add_option(
        '--system-site-packages',
        dest='system_site_packages',
        action='store_true',
        help="Give the virtual environment access to the global site-packages.")

    parser.add_option(
        '--always-copy',
        dest='symlink',
        action='store_false',
        default=True,
        help="Always copy files rather than symlinking.")

    parser.add_option(
        '--unzip-setuptools',
        dest='unzip_setuptools',
        action='store_true',
        help="Unzip Setuptools when installing it.")

    parser.add_option(
        '--relocatable',
        dest='relocatable',
        action='store_true',
        help='Make an EXISTING virtualenv environment relocatable. '
             'This fixes up scripts and makes all .pth files relative.')

    parser.add_option(
        '--no-setuptools',
        dest='no_setuptools',
        action='store_true',
        help='Do not install setuptools (or pip) in the new virtualenv.')

    parser.add_option(
        '--no-pip',
        dest='no_pip',
        action='store_true',
        help='Do not install pip in the new virtualenv.')

    default_search_dirs = file_search_dirs()
    parser.add_option(
        '--extra-search-dir',
        dest="search_dirs",
        action="append",
        metavar='DIR',
        default=default_search_dirs,
        help="Directory to look for setuptools/pip distributions in. "
              "This option can be used multiple times.")

    parser.add_option(
        '--never-download',
        dest="never_download",
        action="store_true",
        default=True,
        help="DEPRECATED. Retained only for backward compatibility. This option has no effect. "
              "Virtualenv never downloads pip or setuptools.")

    parser.add_option(
        '--prompt',
        dest='prompt',
        help='Provides an alternative prompt prefix for this environment.')

    parser.add_option(
        '--setuptools',
        dest='setuptools',
        action='store_true',
        help="DEPRECATED. Retained only for backward compatibility. This option has no effect.")

    parser.add_option(
        '--distribute',
        dest='distribute',
        action='store_true',
        help="DEPRECATED. Retained only for backward compatibility. This option has no effect.")

    if 'extend_parser' in globals():
        extend_parser(parser)

    options, args = parser.parse_args()

    global logger

    if 'adjust_options' in globals():
        adjust_options(options, args)

    verbosity = options.verbose - options.quiet
    logger = Logger([(Logger.level_for_integer(2 - verbosity), sys.stdout)])

    if options.python and not os.environ.get('VIRTUALENV_INTERPRETER_RUNNING'):
        env = os.environ.copy()
        interpreter = resolve_interpreter(options.python)
        if interpreter == sys.executable:
            logger.warn('Already using interpreter %s' % interpreter)
        else:
            logger.notify('Running virtualenv with interpreter %s' % interpreter)
            env['VIRTUALENV_INTERPRETER_RUNNING'] = 'true'
            file = __file__
            if file.endswith('.pyc'):
                file = file[:-1]
            popen = subprocess.Popen([interpreter, file] + sys.argv[1:], env=env)
            raise SystemExit(popen.wait())

    if not args:
        print('You must provide a DEST_DIR')
        parser.print_help()
        sys.exit(2)
    if len(args) > 1:
        print('There must be only one argument: DEST_DIR (you gave %s)' % (
            ' '.join(args)))
        parser.print_help()
        sys.exit(2)

    home_dir = args[0]

    if os.environ.get('WORKING_ENV'):
        logger.fatal('ERROR: you cannot run virtualenv while in a workingenv')
        logger.fatal('Please deactivate your workingenv, then re-run this script')
        sys.exit(3)

    if 'PYTHONHOME' in os.environ:
        logger.warn('PYTHONHOME is set.  You *must* activate the virtualenv before using it')
        del os.environ['PYTHONHOME']

    if options.relocatable:
        make_environment_relocatable(home_dir)
        return

    if not options.never_download:
        logger.warn('The --never-download option is for backward compatibility only.')
        logger.warn('Setting it to false is no longer supported, and will be ignored.')

    create_environment(home_dir,
                       site_packages=options.system_site_packages,
                       clear=options.clear,
                       unzip_setuptools=options.unzip_setuptools,
                       prompt=options.prompt,
                       search_dirs=options.search_dirs,
                       never_download=True,
                       no_setuptools=options.no_setuptools,
                       no_pip=options.no_pip,
                       symlink=options.symlink)
    if 'after_install' in globals():
        after_install(options, home_dir)

def call_subprocess(cmd, show_stdout=True,
                    filter_stdout=None, cwd=None,
                    raise_on_returncode=True, extra_env=None,
                    remove_from_env=None):
    cmd_parts = []
    for part in cmd:
        if len(part) > 45:
            part = part[:20]+"..."+part[-20:]
        if ' ' in part or '\n' in part or '"' in part or "'" in part:
            part = '"%s"' % part.replace('"', '\\"')
        if hasattr(part, 'decode'):
            try:
                part = part.decode(sys.getdefaultencoding())
            except UnicodeDecodeError:
                part = part.decode(sys.getfilesystemencoding())
        cmd_parts.append(part)
    cmd_desc = ' '.join(cmd_parts)
    if show_stdout:
        stdout = None
    else:
        stdout = subprocess.PIPE
    logger.debug("Running command %s" % cmd_desc)
    if extra_env or remove_from_env:
        env = os.environ.copy()
        if extra_env:
            env.update(extra_env)
        if remove_from_env:
            for varname in remove_from_env:
                env.pop(varname, None)
    else:
        env = None
    try:
        proc = subprocess.Popen(
            cmd, stderr=subprocess.STDOUT, stdin=None, stdout=stdout,
            cwd=cwd, env=env)
    except Exception:
        e = sys.exc_info()[1]
        logger.fatal(
            "Error %s while executing command %s" % (e, cmd_desc))
        raise
    all_output = []
    if stdout is not None:
        stdout = proc.stdout
        encoding = sys.getdefaultencoding()
        fs_encoding = sys.getfilesystemencoding()
        while 1:
            line = stdout.readline()
            try:
                line = line.decode(encoding)
            except UnicodeDecodeError:
                line = line.decode(fs_encoding)
            if not line:
                break
            line = line.rstrip()
            all_output.append(line)
            if filter_stdout:
                level = filter_stdout(line)
                if isinstance(level, tuple):
                    level, line = level
                logger.log(level, line)
                if not logger.stdout_level_matches(level):
                    logger.show_progress()
            else:
                logger.info(line)
    else:
        proc.communicate()
    proc.wait()
    if proc.returncode:
        if raise_on_returncode:
            if all_output:
                logger.notify('Complete output from command %s:' % cmd_desc)
                logger.notify('\n'.join(all_output) + '\n----------------------------------------')
            raise OSError(
                "Command %s failed with error code %s"
                % (cmd_desc, proc.returncode))
        else:
            logger.warn(
                "Command %s had error code %s"
                % (cmd_desc, proc.returncode))

def filter_install_output(line):
    if line.strip().startswith('running'):
        return Logger.INFO
    return Logger.DEBUG

def find_wheels(projects, search_dirs):
    """Find wheels from which we can import PROJECTS.

    Scan through SEARCH_DIRS for a wheel for each PROJECT in turn. Return
    a list of the first wheel found for each PROJECT
    """

    wheels = []

    # Look through SEARCH_DIRS for the first suitable wheel. Don't bother
    # about version checking here, as this is simply to get something we can
    # then use to install the correct version.
    for project in projects:
        for dirname in search_dirs:
            # This relies on only having "universal" wheels available.
            # The pattern could be tightened to require -py2.py3-none-any.whl.
            files = glob.glob(os.path.join(dirname, project + '-*.whl'))
            if files:
                wheels.append(os.path.abspath(files[0]))
                break
        else:
            # We're out of luck, so quit with a suitable error
            logger.fatal('Cannot find a wheel for %s' % (project,))

    return wheels

def install_wheel(project_names, py_executable, search_dirs=None):
    if search_dirs is None:
        search_dirs = file_search_dirs()

    wheels = find_wheels(['setuptools', 'pip'], search_dirs)
    pythonpath = os.pathsep.join(wheels)
    findlinks = ' '.join(search_dirs)

    cmd = [
        py_executable, '-c',
        'import sys, pip; sys.exit(pip.main(["install", "--ignore-installed"] + sys.argv[1:]))',
    ] + project_names
    logger.start_progress('Installing %s...' % (', '.join(project_names)))
    logger.indent += 2
    try:
        call_subprocess(cmd, show_stdout=False,
            extra_env = {
                'PYTHONPATH': pythonpath,
                'PIP_FIND_LINKS': findlinks,
                'PIP_USE_WHEEL': '1',
                'PIP_PRE': '1',
                'PIP_NO_INDEX': '1'
            }
        )
    finally:
        logger.indent -= 2
        logger.end_progress()

def create_environment(home_dir, site_packages=False, clear=False,
                       unzip_setuptools=False,
                       prompt=None, search_dirs=None, never_download=False,
                       no_setuptools=False, no_pip=False, symlink=True):
    """
    Creates a new environment in ``home_dir``.

    If ``site_packages`` is true, then the global ``site-packages/``
    directory will be on the path.

    If ``clear`` is true (default False) then the environment will
    first be cleared.
    """
    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)

    py_executable = os.path.abspath(install_python(
        home_dir, lib_dir, inc_dir, bin_dir,
        site_packages=site_packages, clear=clear, symlink=symlink))

    install_distutils(home_dir)

    if not no_setuptools:
        to_install = ['setuptools']
        if not no_pip:
            to_install.append('pip')
        install_wheel(to_install, py_executable, search_dirs)

    install_activate(home_dir, bin_dir, prompt)

def is_executable_file(fpath):
    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

def path_locations(home_dir):
    """Return the path locations for the environment (where libraries are,
    where scripts go, etc)"""
    # XXX: We'd use distutils.sysconfig.get_python_inc/lib but its
    # prefix arg is broken: http://bugs.python.org/issue3386
    if is_win:
        # Windows has lots of problems with executables with spaces in
        # the name; this function will remove them (using the ~1
        # format):
        mkdir(home_dir)
        if ' ' in home_dir:
            import ctypes
            GetShortPathName = ctypes.windll.kernel32.GetShortPathNameW
            size = max(len(home_dir)+1, 256)
            buf = ctypes.create_unicode_buffer(size)
            try:
                u = unicode
            except NameError:
                u = str
            ret = GetShortPathName(u(home_dir), buf, size)
            if not ret:
                print('Error: the path "%s" has a space in it' % home_dir)
                print('We could not determine the short pathname for it.')
                print('Exiting.')
                sys.exit(3)
            home_dir = str(buf.value)
        lib_dir = join(home_dir, 'Lib')
        inc_dir = join(home_dir, 'Include')
        bin_dir = join(home_dir, 'Scripts')
    if is_jython:
        lib_dir = join(home_dir, 'Lib')
        inc_dir = join(home_dir, 'Include')
        bin_dir = join(home_dir, 'bin')
    elif is_pypy:
        lib_dir = home_dir
        inc_dir = join(home_dir, 'include')
        bin_dir = join(home_dir, 'bin')
    elif not is_win:
        lib_dir = join(home_dir, 'lib', py_version)
        multiarch_exec = '/usr/bin/multiarch-platform'
        if is_executable_file(multiarch_exec):
            # In Mageia (2) and Mandriva distros the include dir must be like:
            # virtualenv/include/multiarch-x86_64-linux/python2.7
            # instead of being virtualenv/include/python2.7
            p = subprocess.Popen(multiarch_exec, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdout, stderr = p.communicate()
            # stdout.strip is needed to remove newline character
            inc_dir = join(home_dir, 'include', stdout.strip(), py_version + abiflags)
        else:
            inc_dir = join(home_dir, 'include', py_version + abiflags)
        bin_dir = join(home_dir, 'bin')
    return home_dir, lib_dir, inc_dir, bin_dir


def change_prefix(filename, dst_prefix):
    prefixes = [sys.prefix]

    if is_darwin:
        prefixes.extend((
            os.path.join("/Library/Python", sys.version[:3], "site-packages"),
            os.path.join(sys.prefix, "Extras", "lib", "python"),
            os.path.join("~", "Library", "Python", sys.version[:3], "site-packages"),
            # Python 2.6 no-frameworks
            os.path.join("~", ".local", "lib","python", sys.version[:3], "site-packages"),
            # System Python 2.7 on OSX Mountain Lion
            os.path.join("~", "Library", "Python", sys.version[:3], "lib", "python", "site-packages")))

    if hasattr(sys, 'real_prefix'):
        prefixes.append(sys.real_prefix)
    if hasattr(sys, 'base_prefix'):
        prefixes.append(sys.base_prefix)
    prefixes = list(map(os.path.expanduser, prefixes))
    prefixes = list(map(os.path.abspath, prefixes))
    # Check longer prefixes first so we don't split in the middle of a filename
    prefixes = sorted(prefixes, key=len, reverse=True)
    filename = os.path.abspath(filename)
    for src_prefix in prefixes:
        if filename.startswith(src_prefix):
            _, relpath = filename.split(src_prefix, 1)
            if src_prefix != os.sep: # sys.prefix == "/"
                assert relpath[0] == os.sep
                relpath = relpath[1:]
            return join(dst_prefix, relpath)
    assert False, "Filename %s does not start with any of these prefixes: %s" % \
        (filename, prefixes)

def copy_required_modules(dst_prefix, symlink):
    import imp
    # If we are running under -p, we need to remove the current
    # directory from sys.path temporarily here, so that we
    # definitely get the modules from the site directory of
    # the interpreter we are running under, not the one
    # virtualenv.py is installed under (which might lead to py2/py3
    # incompatibility issues)
    _prev_sys_path = sys.path
    if os.environ.get('VIRTUALENV_INTERPRETER_RUNNING'):
        sys.path = sys.path[1:]
    try:
        for modname in REQUIRED_MODULES:
            if modname in sys.builtin_module_names:
                logger.info("Ignoring built-in bootstrap module: %s" % modname)
                continue
            try:
                f, filename, _ = imp.find_module(modname)
            except ImportError:
                logger.info("Cannot import bootstrap module: %s" % modname)
            else:
                if f is not None:
                    f.close()
                # special-case custom readline.so on OS X, but not for pypy:
                if modname == 'readline' and sys.platform == 'darwin' and not (
                        is_pypy or filename.endswith(join('lib-dynload', 'readline.so'))):
                    dst_filename = join(dst_prefix, 'lib', 'python%s' % sys.version[:3], 'readline.so')
                elif modname == 'readline' and sys.platform == 'win32':
                    # special-case for Windows, where readline is not a
                    # standard module, though it may have been installed in
                    # site-packages by a third-party package
                    pass
                else:
                    dst_filename = change_prefix(filename, dst_prefix)
                copyfile(filename, dst_filename, symlink)
                if filename.endswith('.pyc'):
                    pyfile = filename[:-1]
                    if os.path.exists(pyfile):
                        copyfile(pyfile, dst_filename[:-1], symlink)
    finally:
        sys.path = _prev_sys_path


def subst_path(prefix_path, prefix, home_dir):
    prefix_path = os.path.normpath(prefix_path)
    prefix = os.path.normpath(prefix)
    home_dir = os.path.normpath(home_dir)
    if not prefix_path.startswith(prefix):
        logger.warn('Path not in prefix %r %r', prefix_path, prefix)
        return
    return prefix_path.replace(prefix, home_dir, 1)


def install_python(home_dir, lib_dir, inc_dir, bin_dir, site_packages, clear, symlink=True):
    """Install just the base environment, no distutils patches etc"""
    if sys.executable.startswith(bin_dir):
        print('Please use the *system* python to run this script')
        return

    if clear:
        rmtree(lib_dir)
        ## FIXME: why not delete it?
        ## Maybe it should delete everything with #!/path/to/venv/python in it
        logger.notify('Not deleting %s', bin_dir)

    if hasattr(sys, 'real_prefix'):
        logger.notify('Using real prefix %r' % sys.real_prefix)
        prefix = sys.real_prefix
    elif hasattr(sys, 'base_prefix'):
        logger.notify('Using base prefix %r' % sys.base_prefix)
        prefix = sys.base_prefix
    else:
        prefix = sys.prefix
    mkdir(lib_dir)
    fix_lib64(lib_dir, symlink)
    stdlib_dirs = [os.path.dirname(os.__file__)]
    if is_win:
        stdlib_dirs.append(join(os.path.dirname(stdlib_dirs[0]), 'DLLs'))
    elif is_darwin:
        stdlib_dirs.append(join(stdlib_dirs[0], 'site-packages'))
    if hasattr(os, 'symlink'):
        logger.info('Symlinking Python bootstrap modules')
    else:
        logger.info('Copying Python bootstrap modules')
    logger.indent += 2
    try:
        # copy required files...
        for stdlib_dir in stdlib_dirs:
            if not os.path.isdir(stdlib_dir):
                continue
            for fn in os.listdir(stdlib_dir):
                bn = os.path.splitext(fn)[0]
                if fn != 'site-packages' and bn in REQUIRED_FILES:
                    copyfile(join(stdlib_dir, fn), join(lib_dir, fn), symlink)
        # ...and modules
        copy_required_modules(home_dir, symlink)
    finally:
        logger.indent -= 2
    mkdir(join(lib_dir, 'site-packages'))
    import site
    site_filename = site.__file__
    if site_filename.endswith('.pyc'):
        site_filename = site_filename[:-1]
    elif site_filename.endswith('$py.class'):
        site_filename = site_filename.replace('$py.class', '.py')
    site_filename_dst = change_prefix(site_filename, home_dir)
    site_dir = os.path.dirname(site_filename_dst)
    writefile(site_filename_dst, SITE_PY)
    writefile(join(site_dir, 'orig-prefix.txt'), prefix)
    site_packages_filename = join(site_dir, 'no-global-site-packages.txt')
    if not site_packages:
        writefile(site_packages_filename, '')

    if is_pypy or is_win:
        stdinc_dir = join(prefix, 'include')
    else:
        stdinc_dir = join(prefix, 'include', py_version + abiflags)
    if os.path.exists(stdinc_dir):
        copyfile(stdinc_dir, inc_dir, symlink)
    else:
        logger.debug('No include dir %s' % stdinc_dir)

    platinc_dir = distutils.sysconfig.get_python_inc(plat_specific=1)
    if platinc_dir != stdinc_dir:
        platinc_dest = distutils.sysconfig.get_python_inc(
            plat_specific=1, prefix=home_dir)
        if platinc_dir == platinc_dest:
            # Do platinc_dest manually due to a CPython bug;
            # not http://bugs.python.org/issue3386 but a close cousin
            platinc_dest = subst_path(platinc_dir, prefix, home_dir)
        if platinc_dest:
            # PyPy's stdinc_dir and prefix are relative to the original binary
            # (traversing virtualenvs), whereas the platinc_dir is relative to
            # the inner virtualenv and ignores the prefix argument.
            # This seems more evolved than designed.
            copyfile(platinc_dir, platinc_dest, symlink)

    # pypy never uses exec_prefix, just ignore it
    if sys.exec_prefix != prefix and not is_pypy:
        if is_win:
            exec_dir = join(sys.exec_prefix, 'lib')
        elif is_jython:
            exec_dir = join(sys.exec_prefix, 'Lib')
        else:
            exec_dir = join(sys.exec_prefix, 'lib', py_version)
        for fn in os.listdir(exec_dir):
            copyfile(join(exec_dir, fn), join(lib_dir, fn), symlink)

    if is_jython:
        # Jython has either jython-dev.jar and javalib/ dir, or just
        # jython.jar
        for name in 'jython-dev.jar', 'javalib', 'jython.jar':
            src = join(prefix, name)
            if os.path.exists(src):
                copyfile(src, join(home_dir, name), symlink)
        # XXX: registry should always exist after Jython 2.5rc1
        src = join(prefix, 'registry')
        if os.path.exists(src):
            copyfile(src, join(home_dir, 'registry'), symlink=False)
        copyfile(join(prefix, 'cachedir'), join(home_dir, 'cachedir'),
                 symlink=False)

    mkdir(bin_dir)
    py_executable = join(bin_dir, os.path.basename(sys.executable))
    if 'Python.framework' in prefix:
        # OS X framework builds cause validation to break
        # https://github.com/pypa/virtualenv/issues/322
        if os.environ.get('__PYVENV_LAUNCHER__'):
            del os.environ["__PYVENV_LAUNCHER__"]
        if re.search(r'/Python(?:-32|-64)*$', py_executable):
            # The name of the python executable is not quite what
            # we want, rename it.
            py_executable = os.path.join(
                    os.path.dirname(py_executable), 'python')

    logger.notify('New %s executable in %s', expected_exe, py_executable)
    pcbuild_dir = os.path.dirname(sys.executable)
    pyd_pth = os.path.join(lib_dir, 'site-packages', 'virtualenv_builddir_pyd.pth')
    if is_win and os.path.exists(os.path.join(pcbuild_dir, 'build.bat')):
        logger.notify('Detected python running from build directory %s', pcbuild_dir)
        logger.notify('Writing .pth file linking to build directory for *.pyd files')
        writefile(pyd_pth, pcbuild_dir)
    else:
        pcbuild_dir = None
        if os.path.exists(pyd_pth):
            logger.info('Deleting %s (not Windows env or not build directory python)' % pyd_pth)
            os.unlink(pyd_pth)

    if sys.executable != py_executable:
        ## FIXME: could I just hard link?
        executable = sys.executable
        shutil.copyfile(executable, py_executable)
        make_exe(py_executable)
        if is_win or is_cygwin:
            pythonw = os.path.join(os.path.dirname(sys.executable), 'pythonw.exe')
            if os.path.exists(pythonw):
                logger.info('Also created pythonw.exe')
                shutil.copyfile(pythonw, os.path.join(os.path.dirname(py_executable), 'pythonw.exe'))
            python_d = os.path.join(os.path.dirname(sys.executable), 'python_d.exe')
            python_d_dest = os.path.join(os.path.dirname(py_executable), 'python_d.exe')
            if os.path.exists(python_d):
                logger.info('Also created python_d.exe')
                shutil.copyfile(python_d, python_d_dest)
            elif os.path.exists(python_d_dest):
                logger.info('Removed python_d.exe as it is no longer at the source')
                os.unlink(python_d_dest)
            # we need to copy the DLL to enforce that windows will load the correct one.
            # may not exist if we are cygwin.
            py_executable_dll = 'python%s%s.dll' % (
                sys.version_info[0], sys.version_info[1])
            py_executable_dll_d = 'python%s%s_d.dll' % (
                sys.version_info[0], sys.version_info[1])
            pythondll = os.path.join(os.path.dirname(sys.executable), py_executable_dll)
            pythondll_d = os.path.join(os.path.dirname(sys.executable), py_executable_dll_d)
            pythondll_d_dest = os.path.join(os.path.dirname(py_executable), py_executable_dll_d)
            if os.path.exists(pythondll):
                logger.info('Also created %s' % py_executable_dll)
                shutil.copyfile(pythondll, os.path.join(os.path.dirname(py_executable), py_executable_dll))
            if os.path.exists(pythondll_d):
                logger.info('Also created %s' % py_executable_dll_d)
                shutil.copyfile(pythondll_d, pythondll_d_dest)
            elif os.path.exists(pythondll_d_dest):
                logger.info('Removed %s as the source does not exist' % pythondll_d_dest)
                os.unlink(pythondll_d_dest)
        if is_pypy:
            # make a symlink python --> pypy-c
            python_executable = os.path.join(os.path.dirname(py_executable), 'python')
            if sys.platform in ('win32', 'cygwin'):
                python_executable += '.exe'
            logger.info('Also created executable %s' % python_executable)
            copyfile(py_executable, python_executable, symlink)

            if is_win:
                for name in 'libexpat.dll', 'libpypy.dll', 'libpypy-c.dll', 'libeay32.dll', 'ssleay32.dll', 'sqlite.dll':
                    src = join(prefix, name)
                    if os.path.exists(src):
                        copyfile(src, join(bin_dir, name), symlink)

    if os.path.splitext(os.path.basename(py_executable))[0] != expected_exe:
        secondary_exe = os.path.join(os.path.dirname(py_executable),
                                     expected_exe)
        py_executable_ext = os.path.splitext(py_executable)[1]
        if py_executable_ext.lower() == '.exe':
            # python2.4 gives an extension of '.4' :P
            secondary_exe += py_executable_ext
        if os.path.exists(secondary_exe):
            logger.warn('Not overwriting existing %s script %s (you must use %s)'
                        % (expected_exe, secondary_exe, py_executable))
        else:
            logger.notify('Also creating executable in %s' % secondary_exe)
            shutil.copyfile(sys.executable, secondary_exe)
            make_exe(secondary_exe)

    if '.framework' in prefix:
        if 'Python.framework' in prefix:
            logger.debug('MacOSX Python framework detected')
            # Make sure we use the the embedded interpreter inside
            # the framework, even if sys.executable points to
            # the stub executable in ${sys.prefix}/bin
            # See http://groups.google.com/group/python-virtualenv/
            #                              browse_thread/thread/17cab2f85da75951
            original_python = os.path.join(
                prefix, 'Resources/Python.app/Contents/MacOS/Python')
        if 'EPD' in prefix:
            logger.debug('EPD framework detected')
            original_python = os.path.join(prefix, 'bin/python')
        shutil.copy(original_python, py_executable)

        # Copy the framework's dylib into the virtual
        # environment
        virtual_lib = os.path.join(home_dir, '.Python')

        if os.path.exists(virtual_lib):
            os.unlink(virtual_lib)
        copyfile(
            os.path.join(prefix, 'Python'),
            virtual_lib,
            symlink)

        # And then change the install_name of the copied python executable
        try:
            mach_o_change(py_executable,
                          os.path.join(prefix, 'Python'),
                          '@executable_path/../.Python')
        except:
            e = sys.exc_info()[1]
            logger.warn("Could not call mach_o_change: %s. "
                        "Trying to call install_name_tool instead." % e)
            try:
                call_subprocess(
                    ["install_name_tool", "-change",
                     os.path.join(prefix, 'Python'),
                     '@executable_path/../.Python',
                     py_executable])
            except:
                logger.fatal("Could not call install_name_tool -- you must "
                             "have Apple's development tools installed")
                raise

    if not is_win:
        # Ensure that 'python', 'pythonX' and 'pythonX.Y' all exist
        py_exe_version_major = 'python%s' % sys.version_info[0]
        py_exe_version_major_minor = 'python%s.%s' % (
            sys.version_info[0], sys.version_info[1])
        py_exe_no_version = 'python'
        required_symlinks = [ py_exe_no_version, py_exe_version_major,
                         py_exe_version_major_minor ]

        py_executable_base = os.path.basename(py_executable)

        if py_executable_base in required_symlinks:
            # Don't try to symlink to yourself.
            required_symlinks.remove(py_executable_base)

        for pth in required_symlinks:
            full_pth = join(bin_dir, pth)
            if os.path.exists(full_pth):
                os.unlink(full_pth)
            if symlink:
                os.symlink(py_executable_base, full_pth)
            else:
                copyfile(py_executable, full_pth, symlink)

    if is_win and ' ' in py_executable:
        # There's a bug with subprocess on Windows when using a first
        # argument that has a space in it.  Instead we have to quote
        # the value:
        py_executable = '"%s"' % py_executable
    # NOTE: keep this check as one line, cmd.exe doesn't cope with line breaks
    cmd = [py_executable, '-c', 'import sys;out=sys.stdout;'
        'getattr(out, "buffer", out).write(sys.prefix.encode("utf-8"))']
    logger.info('Testing executable with %s %s "%s"' % tuple(cmd))
    try:
        proc = subprocess.Popen(cmd,
                            stdout=subprocess.PIPE)
        proc_stdout, proc_stderr = proc.communicate()
    except OSError:
        e = sys.exc_info()[1]
        if e.errno == errno.EACCES:
            logger.fatal('ERROR: The executable %s could not be run: %s' % (py_executable, e))
            sys.exit(100)
        else:
            raise e

    proc_stdout = proc_stdout.strip().decode("utf-8")
    proc_stdout = os.path.normcase(os.path.abspath(proc_stdout))
    norm_home_dir = os.path.normcase(os.path.abspath(home_dir))
    if hasattr(norm_home_dir, 'decode'):
        norm_home_dir = norm_home_dir.decode(sys.getfilesystemencoding())
    if proc_stdout != norm_home_dir:
        logger.fatal(
            'ERROR: The executable %s is not functioning' % py_executable)
        logger.fatal(
            'ERROR: It thinks sys.prefix is %r (should be %r)'
            % (proc_stdout, norm_home_dir))
        logger.fatal(
            'ERROR: virtualenv is not compatible with this system or executable')
        if is_win:
            logger.fatal(
                'Note: some Windows users have reported this error when they '
                'installed Python for "Only this user" or have multiple '
                'versions of Python installed. Copying the appropriate '
                'PythonXX.dll to the virtualenv Scripts/ directory may fix '
                'this problem.')
        sys.exit(100)
    else:
        logger.info('Got sys.prefix result: %r' % proc_stdout)

    pydistutils = os.path.expanduser('~/.pydistutils.cfg')
    if os.path.exists(pydistutils):
        logger.notify('Please make sure you remove any previous custom paths from '
                      'your %s file.' % pydistutils)
    ## FIXME: really this should be calculated earlier

    fix_local_scheme(home_dir, symlink)

    if site_packages:
        if os.path.exists(site_packages_filename):
            logger.info('Deleting %s' % site_packages_filename)
            os.unlink(site_packages_filename)

    return py_executable


def install_activate(home_dir, bin_dir, prompt=None):
    home_dir = os.path.abspath(home_dir)
    if is_win or is_jython and os._name == 'nt':
        files = {
            'activate.bat': ACTIVATE_BAT,
            'deactivate.bat': DEACTIVATE_BAT,
            'activate.ps1': ACTIVATE_PS,
        }

        # MSYS needs paths of the form /c/path/to/file
        drive, tail = os.path.splitdrive(home_dir.replace(os.sep, '/'))
        home_dir_msys = (drive and "/%s%s" or "%s%s") % (drive[:1], tail)

        # Run-time conditional enables (basic) Cygwin compatibility
        home_dir_sh = ("""$(if [ "$OSTYPE" "==" "cygwin" ]; then cygpath -u '%s'; else echo '%s'; fi;)""" %
                       (home_dir, home_dir_msys))
        files['activate'] = ACTIVATE_SH.replace('__VIRTUAL_ENV__', home_dir_sh)

    else:
        files = {'activate': ACTIVATE_SH}

        # suppling activate.fish in addition to, not instead of, the
        # bash script support.
        files['activate.fish'] = ACTIVATE_FISH

        # same for csh/tcsh support...
        files['activate.csh'] = ACTIVATE_CSH

    files['activate_this.py'] = ACTIVATE_THIS
    if hasattr(home_dir, 'decode'):
        home_dir = home_dir.decode(sys.getfilesystemencoding())
    vname = os.path.basename(home_dir)
    for name, content in files.items():
        content = content.replace('__VIRTUAL_PROMPT__', prompt or '')
        content = content.replace('__VIRTUAL_WINPROMPT__', prompt or '(%s)' % vname)
        content = content.replace('__VIRTUAL_ENV__', home_dir)
        content = content.replace('__VIRTUAL_NAME__', vname)
        content = content.replace('__BIN_NAME__', os.path.basename(bin_dir))
        writefile(os.path.join(bin_dir, name), content)

def install_distutils(home_dir):
    distutils_path = change_prefix(distutils.__path__[0], home_dir)
    mkdir(distutils_path)
    ## FIXME: maybe this prefix setting should only be put in place if
    ## there's a local distutils.cfg with a prefix setting?
    home_dir = os.path.abspath(home_dir)
    ## FIXME: this is breaking things, removing for now:
    #distutils_cfg = DISTUTILS_CFG + "\n[install]\nprefix=%s\n" % home_dir
    writefile(os.path.join(distutils_path, '__init__.py'), DISTUTILS_INIT)
    writefile(os.path.join(distutils_path, 'distutils.cfg'), DISTUTILS_CFG, overwrite=False)

def fix_local_scheme(home_dir, symlink=True):
    """
    Platforms that use the "posix_local" install scheme (like Ubuntu with
    Python 2.7) need to be given an additional "local" location, sigh.
    """
    try:
        import sysconfig
    except ImportError:
        pass
    else:
        if sysconfig._get_default_scheme() == 'posix_local':
            local_path = os.path.join(home_dir, 'local')
            if not os.path.exists(local_path):
                os.mkdir(local_path)
                for subdir_name in os.listdir(home_dir):
                    if subdir_name == 'local':
                        continue
                    copyfile(os.path.abspath(os.path.join(home_dir, subdir_name)), \
                                                            os.path.join(local_path, subdir_name), symlink)

def fix_lib64(lib_dir, symlink=True):
    """
    Some platforms (particularly Gentoo on x64) put things in lib64/pythonX.Y
    instead of lib/pythonX.Y.  If this is such a platform we'll just create a
    symlink so lib64 points to lib
    """
    if [p for p in distutils.sysconfig.get_config_vars().values()
        if isinstance(p, basestring) and 'lib64' in p]:
        # PyPy's library path scheme is not affected by this.
        # Return early or we will die on the following assert.
        if is_pypy:
            logger.debug('PyPy detected, skipping lib64 symlinking')
            return

        logger.debug('This system uses lib64; symlinking lib64 to lib')

        assert os.path.basename(lib_dir) == 'python%s' % sys.version[:3], (
            "Unexpected python lib dir: %r" % lib_dir)
        lib_parent = os.path.dirname(lib_dir)
        top_level = os.path.dirname(lib_parent)
        lib_dir = os.path.join(top_level, 'lib')
        lib64_link = os.path.join(top_level, 'lib64')
        assert os.path.basename(lib_parent) == 'lib', (
            "Unexpected parent dir: %r" % lib_parent)
        if os.path.lexists(lib64_link):
            return
        cp_or_ln = (os.symlink if symlink else copyfile)
        cp_or_ln('lib', lib64_link)

def resolve_interpreter(exe):
    """
    If the executable given isn't an absolute path, search $PATH for the interpreter
    """
    # If the "executable" is a version number, get the installed executable for
    # that version
    python_versions = get_installed_pythons()
    if exe in python_versions:
        exe = python_versions[exe]

    if os.path.abspath(exe) != exe:
        paths = os.environ.get('PATH', '').split(os.pathsep)
        for path in paths:
            if os.path.exists(os.path.join(path, exe)):
                exe = os.path.join(path, exe)
                break
    if not os.path.exists(exe):
        logger.fatal('The executable %s (from --python=%s) does not exist' % (exe, exe))
        raise SystemExit(3)
    if not is_executable(exe):
        logger.fatal('The executable %s (from --python=%s) is not executable' % (exe, exe))
        raise SystemExit(3)
    return exe

def is_executable(exe):
    """Checks a file is executable"""
    return os.access(exe, os.X_OK)

############################################################
## Relocating the environment:

def make_environment_relocatable(home_dir):
    """
    Makes the already-existing environment use relative paths, and takes out
    the #!-based environment selection in scripts.
    """
    home_dir, lib_dir, inc_dir, bin_dir = path_locations(home_dir)
    activate_this = os.path.join(bin_dir, 'activate_this.py')
    if not os.path.exists(activate_this):
        logger.fatal(
            'The environment doesn\'t have a file %s -- please re-run virtualenv '
            'on this environment to update it' % activate_this)
    fixup_scripts(home_dir, bin_dir)
    fixup_pth_and_egg_link(home_dir)
    ## FIXME: need to fix up distutils.cfg

OK_ABS_SCRIPTS = ['python', 'python%s' % sys.version[:3],
                  'activate', 'activate.bat', 'activate_this.py',
                  'activate.fish', 'activate.csh']

def fixup_scripts(home_dir, bin_dir):
    if is_win:
        new_shebang_args = (
            '%s /c' % os.path.normcase(os.environ.get('COMSPEC', 'cmd.exe')),
            '', '.exe')
    else:
        new_shebang_args = ('/usr/bin/env', sys.version[:3], '')

    # This is what we expect at the top of scripts:
    shebang = '#!%s' % os.path.normcase(os.path.join(
        os.path.abspath(bin_dir), 'python%s' % new_shebang_args[2]))
    # This is what we'll put:
    new_shebang = '#!%s python%s%s' % new_shebang_args

    for filename in os.listdir(bin_dir):
        filename = os.path.join(bin_dir, filename)
        if not os.path.isfile(filename):
            # ignore subdirs, e.g. .svn ones.
            continue
        f = open(filename, 'rb')
        try:
            try:
                lines = f.read().decode('utf-8').splitlines()
            except UnicodeDecodeError:
                # This is probably a binary program instead
                # of a script, so just ignore it.
                continue
        finally:
            f.close()
        if not lines:
            logger.warn('Script %s is an empty file' % filename)
            continue

        old_shebang = lines[0].strip()
        old_shebang = old_shebang[0:2] + os.path.normcase(old_shebang[2:])

        if not old_shebang.startswith(shebang):
            if os.path.basename(filename) in OK_ABS_SCRIPTS:
                logger.debug('Cannot make script %s relative' % filename)
            elif lines[0].strip() == new_shebang:
                logger.info('Script %s has already been made relative' % filename)
            else:
                logger.warn('Script %s cannot be made relative (it\'s not a normal script that starts with %s)'
                            % (filename, shebang))
            continue
        logger.notify('Making script %s relative' % filename)
        script = relative_script([new_shebang] + lines[1:])
        f = open(filename, 'wb')
        f.write('\n'.join(script).encode('utf-8'))
        f.close()

def relative_script(lines):
    "Return a script that'll work in a relocatable environment."
    activate = "import os; activate_this=os.path.join(os.path.dirname(os.path.realpath(__file__)), 'activate_this.py'); exec(compile(open(activate_this).read(), activate_this, 'exec'), dict(__file__=activate_this)); del os, activate_this"
    # Find the last future statement in the script. If we insert the activation
    # line before a future statement, Python will raise a SyntaxError.
    activate_at = None
    for idx, line in reversed(list(enumerate(lines))):
        if line.split()[:3] == ['from', '__future__', 'import']:
            activate_at = idx + 1
            break
    if activate_at is None:
        # Activate after the shebang.
        activate_at = 1
    return lines[:activate_at] + ['', activate, ''] + lines[activate_at:]

def fixup_pth_and_egg_link(home_dir, sys_path=None):
    """Makes .pth and .egg-link files use relative paths"""
    home_dir = os.path.normcase(os.path.abspath(home_dir))
    if sys_path is None:
        sys_path = sys.path
    for path in sys_path:
        if not path:
            path = '.'
        if not os.path.isdir(path):
            continue
        path = os.path.normcase(os.path.abspath(path))
        if not path.startswith(home_dir):
            logger.debug('Skipping system (non-environment) directory %s' % path)
            continue
        for filename in os.listdir(path):
            filename = os.path.join(path, filename)
            if filename.endswith('.pth'):
                if not os.access(filename, os.W_OK):
                    logger.warn('Cannot write .pth file %s, skipping' % filename)
                else:
                    fixup_pth_file(filename)
            if filename.endswith('.egg-link'):
                if not os.access(filename, os.W_OK):
                    logger.warn('Cannot write .egg-link file %s, skipping' % filename)
                else:
                    fixup_egg_link(filename)

def fixup_pth_file(filename):
    lines = []
    prev_lines = []
    f = open(filename)
    prev_lines = f.readlines()
    f.close()
    for line in prev_lines:
        line = line.strip()
        if (not line or line.startswith('#') or line.startswith('import ')
            or os.path.abspath(line) != line):
            lines.append(line)
        else:
            new_value = make_relative_path(filename, line)
            if line != new_value:
                logger.debug('Rewriting path %s as %s (in %s)' % (line, new_value, filename))
            lines.append(new_value)
    if lines == prev_lines:
        logger.info('No changes to .pth file %s' % filename)
        return
    logger.notify('Making paths in .pth file %s relative' % filename)
    f = open(filename, 'w')
    f.write('\n'.join(lines) + '\n')
    f.close()

def fixup_egg_link(filename):
    f = open(filename)
    link = f.readline().strip()
    f.close()
    if os.path.abspath(link) != link:
        logger.debug('Link in %s already relative' % filename)
        return
    new_link = make_relative_path(filename, link)
    logger.notify('Rewriting link %s in %s as %s' % (link, filename, new_link))
    f = open(filename, 'w')
    f.write(new_link)
    f.close()

def make_relative_path(source, dest, dest_is_directory=True):
    """
    Make a filename relative, where the filename is dest, and it is
    being referred to from the filename source.

        >>> make_relative_path('/usr/share/something/a-file.pth',
        ...                    '/usr/share/another-place/src/Directory')
        '../another-place/src/Directory'
        >>> make_relative_path('/usr/share/something/a-file.pth',
        ...                    '/home/user/src/Directory')
        '../../../home/user/src/Directory'
        >>> make_relative_path('/usr/share/a-file.pth', '/usr/share/')
        './'
    """
    source = os.path.dirname(source)
    if not dest_is_directory:
        dest_filename = os.path.basename(dest)
        dest = os.path.dirname(dest)
    dest = os.path.normpath(os.path.abspath(dest))
    source = os.path.normpath(os.path.abspath(source))
    dest_parts = dest.strip(os.path.sep).split(os.path.sep)
    source_parts = source.strip(os.path.sep).split(os.path.sep)
    while dest_parts and source_parts and dest_parts[0] == source_parts[0]:
        dest_parts.pop(0)
        source_parts.pop(0)
    full_parts = ['..']*len(source_parts) + dest_parts
    if not dest_is_directory:
        full_parts.append(dest_filename)
    if not full_parts:
        # Special case for the current directory (otherwise it'd be '')
        return './'
    return os.path.sep.join(full_parts)



############################################################
## Bootstrap script creation:

def create_bootstrap_script(extra_text, python_version=''):
    """
    Creates a bootstrap script, which is like this script but with
    extend_parser, adjust_options, and after_install hooks.

    This returns a string that (written to disk of course) can be used
    as a bootstrap script with your own customizations.  The script
    will be the standard virtualenv.py script, with your extra text
    added (your extra text should be Python code).

    If you include these functions, they will be called:

    ``extend_parser(optparse_parser)``:
        You can add or remove options from the parser here.

    ``adjust_options(options, args)``:
        You can change options here, or change the args (if you accept
        different kinds of arguments, be sure you modify ``args`` so it is
        only ``[DEST_DIR]``).

    ``after_install(options, home_dir)``:

        After everything is installed, this function is called.  This
        is probably the function you are most likely to use.  An
        example would be::

            def after_install(options, home_dir):
                subprocess.call([join(home_dir, 'bin', 'easy_install'),
                                 'MyPackage'])
                subprocess.call([join(home_dir, 'bin', 'my-package-script'),
                                 'setup', home_dir])

        This example immediately installs a package, and runs a setup
        script from that package.

    If you provide something like ``python_version='2.5'`` then the
    script will start with ``#!/usr/bin/env python2.5`` instead of
    ``#!/usr/bin/env python``.  You can use this when the script must
    be run with a particular Python version.
    """
    filename = __file__
    if filename.endswith('.pyc'):
        filename = filename[:-1]
    f = codecs.open(filename, 'r', encoding='utf-8')
    content = f.read()
    f.close()
    py_exe = 'python%s' % python_version
    content = (('#!/usr/bin/env %s\n' % py_exe)
               + '## WARNING: This file is generated\n'
               + content)
    return content.replace('##EXT' 'END##', extra_text)

##EXTEND##

def convert(s):
    b = base64.b64decode(s.encode('ascii'))
    return zlib.decompress(b).decode('utf-8')

##file site.py
SITE_PY = convert("""
eJzFPf1z2zaWv/OvwMqToZTIdOJ0e3tOnRsncVrvuYm3SWdz63q0lARZrCmSJUjL2pu7v/3eBwAC
JCXbm+6cphNLJPDw8PC+8PAeOhgMTopCZnOxyud1KoWScTlbiiKulkos8lJUy6Sc7xdxWW3g6ewm
vpZKVLlQGxVhqygInn7lJ3gqPi8TZVCAb3Fd5au4SmZxmm5EsiryspJzMa/LJLsWSZZUSZwm/4AW
eRaJp1+PQXCWCZh5mshS3MpSAVwl8oW42FTLPBPDusA5v4j+GL8cjYWalUlRQYNS4wwUWcZVkEk5
BzShZa2AlEkl91UhZ8kimdmG67xO56JI45kUf/87T42ahmGg8pVcL2UpRQbIAEwJsArEA74mpZjl
cxkJ8UbOYhyAnzfEChjaGNdMIRmzXKR5dg1zyuRMKhWXGzGc1hUBIpTFPAecEsCgStI0WOfljRrB
ktJ6rOGRiJk9/Mkwe8A8cfwu5wCOH7Pg5yy5GzNs4B4EVy2ZbUq5SO5EjGDhp7yTs4l+NkwWYp4s
FkCDrBphk4ARUCJNpgcFLcd3eoVeHxBWlitjGEMiytyYX1KPKDirRJwqYNu6QBopwvydnCZxBtTI
bmE4gAgkDfrGmSeqsuPQ7EQOAEpcxwqkZKXEcBUnGTDrj/GM0P5rks3ztRoRBWC1lPi1VpU7/2EP
AaC1Q4BxgItlVrPO0uRGppsRIPAZsC+lqtMKBWKelHJW5WUiFQEA1DZC3gHSYxGXUpOQOdPI7Zjo
TzRJMlxYFDAUeHyJJFkk13VJEiYWCXAucMX7jz+Jd6dvzk4+aB4zwFhmr1eAM0ChhXZwggHEQa3K
gzQHgY6Cc/wj4vkchewaxwe8mgYH9650MIS5F1G7j7PgQHa9uHoYmGMFyoTGCqjff0OXsVoCff7n
nvUOgpNtVKGJ87f1MgeZzOKVFMuY+Qs5I/hOw3kdFdXyFXCDQjgVkErh4iCCCcIDkrg0G+aZFAWw
WJpkchQAhabU1l9FYIUPebZPa93iBIBQBhm8dJ6NaMRMwkS7sF6hvjCNNzQz3SSw67zKS1IcwP/Z
jHRRGmc3hKMihuJvU3mdZBkihLwQhHshDaxuEuDEeSTOqRXpBdNIhKy9uCWKRA28hEwHPCnv4lWR
yjGLL+rW3WqEBpOVMGudMsdBy4rUK61aM9Ve3juMvrS4jtCslqUE4PXUE7pFno/FFHQ2YVPEKxav
ap0T5wQ98kSdkCeoJfTF70DRE6XqlbQvkVdAsxBDBYs8TfM1kOwoCITYw0bGKPvMCW/hHfwLcPHf
VFazZRA4I1nAGhQivw0UAgGTIDPN1RoJj9s0K7eVTJKxpsjLuSxpqIcR+4ARf2BjnGvwIa+0UePp
4irnq6RClTTVJjNhi5eFFevHVzxvmAZYbkU0M00bOq1wemmxjKfSuCRTuUBJ0Iv0yi47jBn0jEm2
uBIrtjLwDsgiE7Yg/YoFlc6ikuQEAAwWvjhLijqlRgoZTMQw0Kog+KsYTXqunSVgbzbLASokNt8z
sD+A2z9AjNbLBOgzAwigYVBLwfJNk6pEB6HRR4Fv9E1/Hh849WyhbRMPuYiTVFv5OAvO6OFpWZL4
zmSBvcaaGApmmFXo2l1nQEcU88FgEATGHdoo8zVXQVVujoAVhBlnMpnWCRq+yQRNvf6hAh5FOAN7
3Ww7Cw80hOn0AajkdFmU+Qpf27l9AmUCY2GPYE9ckJaR7CB7nPgKyeeq9MI0RdvtsLNAPRRc/HT6
/uzL6SdxLC4blTZu67MrGPM0i4GtySIAU7WGbXQZtETFl6DuE+/BvBNTgD2j3iS+Mq5q4F1A/XNZ
02uYxsx7GZx+OHlzfjr5+dPpT5NPZ59PAUGwMzLYoymjeazBYVQRCAdw5VxF2r4GnR704M3JJ/sg
mCRq8u03wG7wZHgtK2DicggzHotwFd8pYNBwTE1HiGOnAVjwcDQSr8Xh06cvDwlasSk2AAzMrtMU
H060RZ8k2SIPR9T4V3bpj1lJaf/t8uibK3F8LMJf49s4DMCHapoyS/xI4vR5U0joWsGfYa5GQTCX
CxC9G4kCOnxKfvGIO8CSQMtc2+lf8yQz75kr3SFIfwypB+AwmczSWClsPJmEQATq0POBDhE71yh1
Q+hYbNyuI40KfkoJC5thlzH+04NiPKV+iAaj6HYxjUBcV7NYSW5F04d+kwnqrMlkqAcEYSaJAYeL
1VAoTBPUWWUCfi1xHuqwqcpT/InwUQuQAOLWCrUkLpLeOkW3cVpLNXQmBUQcDltkREWbKOJHcFGG
YImbpRuN2tQ0PAPNgHxpDlq0bFEOP3vg74C6Mps43Ojx3otphpj+mXcahAO4nCGqe6VaUFg7iovT
C/Hy+eE+ujOw55xb6njN0UInWS3twwWslpEHRph7GXlx6bJAPYtPj3bDXEV2ZbqssNBLXMpVfivn
gC0ysLPK4id6AztzmMcshlUEvU7+AKtQ4zfGuA/l2YO0oO8A1FsRFLP+Zun3OBggMwWKiDfWRGq9
62dTWJT5bYLOxnSjX4KtBGWJFtM4NoGzcB6ToUkEDQFecIaUWssQ1GFZs8NKeCNItBfzRrFGBO4c
NfUVfb3J8nU24Z3wMSrd4ciyLgqWZl5s0CzBnngPVgiQzGFj1xCNoYDLL1C29gF5mD5MFyhLewsA
BIZe0XbNgWW2ejRF3jXisAhj9EqQ8JYS/YVbMwRttQwxHEj0NrIPjJZASDA5q+CsatBMhrJmmsHA
Dkl8rjuPeAvqA2hRMQKzOdTQuJGh3+URKGdx7iolpx9a5C9fvjDbqCXFVxCxKU4aXYgFGcuo2IBh
TUAnGI+MozXEBmtwbgFMrTRriv1PIi/YG4P1vNCyDX4A7O6qqjg6OFiv15GOLuTl9YFaHPzxT99+
+6fnrBPnc+IfmI4jLTrUFh3QO/Roo++MBXptVq7Fj0nmcyPBGkryysgVRfy+r5N5Lo72R1Z/Ihc3
Zhr/Na4MKJCJGZSpDLQdNBg9UftPopdqIJ6QdbZthyP2S7RJtVbMt7rQo8rBEwC/ZZbXaKobTlDi
GVg32KHP5bS+Du3gno00P2CqKKdDywP7L64QA58zDF8ZUzxBLUFsgRbfIf1PzDYxeUdaQyB50UR1
ds+bfi1miDt/uLxbX9MRGjPDRCF3oET4TR4sgLZxV3Lwo11btHuOa2s+niEwlj4wzKsdyyEKDuGC
azF2pc7havR4QZrWrJpBwbiqERQ0OIlTprYGRzYyRJDo3ZjNPi+sbgF0akUOTXzArAK0cMfpWLs2
KzieEPLAsXhBTyS4yEedd895aes0pYBOi0c9qjBgb6HRTufAl0MDYCwG5c8Dbmm2KR9bi8Jr0AMs
5xgQMtiiw0z4xvUBB3uDHnbqWP1tvZnGfSBwkYYci3oQdEL5mEcoFUhTMfR7bmNxS9zuYDstDjGV
WSYSabVFuNrKo1eodhqmRZKh7nUWKZqlOXjFVisSIzXvfWeB9kH4uM+YaQnUZGjI4TQ6Jm/PE8BQ
t8Pw2XWNgQY3DoMYrRJF1g3JtIR/wK2g+AYFo4CWBM2CeaiU+RP7HWTOzld/2cIeltDIEG7TbW5I
x2JoOOb9nkAy6mgMSEEGJOwKI7mOrA5S4DBngTzhhtdyq3QTjEiBnDkWhNQM4E4vvQ0OPonwBIQk
FCHfVUoW4pkYwPK1RfVhuvt35VIThBg6DchV0NGLYzey4UQ1jltRDp+h/fgGnZUUOXDwFFweN9Dv
srlhWht0AWfdV9wWKdDIFIcZjFxUrwxh3GDyH46dFg2xzCCGobyBvCMdM9IosMutQcOCGzDemrfH
0o/diAX2HYa5OpSrO9j/hWWiZrkKKWbSjl24H80VXdpYbM+T6QD+eAswGF15kGSq4xcYZfknBgk9
6GEfdG+yGBaZx+U6yUJSYJp+x/7SdPCwpPSM3MEn2k4dwEQx4nnwvgQBoaPPAxAn1ASwK5eh0m5/
F+zOKQ4sXO4+8Nzmy6OXV13ijrdFeOynf6lO76oyVrhaKS8aCwWuVteAo9KFycXZRh9e6sNt3CaU
uYJdpPj46YtAQnBcdx1vHjf1huERm3vn5H0M6qDX7iVXa3bELoAIakVklIPw8Rz5cGQfO7kdE3sE
kEcxzI5FMZA0n/wzcHYtFIyxP99kGEdrqwz8wOtvv5n0REZdJL/9ZnDPKC1i9In9sOUJ2pE5qWDX
bEsZp+RqOH0oqJg1rGPbFCPW57T90zx21eNzarRs7Lu/BX4MFAypS/ARno8bsnWnih/fndoKT9up
HcA6u1Xz2aNFgL19Pv0VdshKB9Vu4ySlcwWY/P4+Klezued4Rb/28CDtVDAOCfr2X+ryOXBDyNGE
UXc62hk7MQHnnl2w+RSx6qKyp3MImiMwLy/APf7sQtUWzDDucz5eOOxRTd6M+5yJr1Gr+PldNJAF
5tFg0Ef2rez4/zHL5/+aST5wKubk+ne0ho8E9HvNhI0HQ9PGw4fVv+yu3TXAHmCetridO9zC7tB8
Vrkwzh2rJCWeou56KtaUrkCxVTwpAihz9vt64OAy6kPvt3VZ8tE1qcBClvt4HDsWmKllPL9eE7Mn
Dj7ICjGxzWYUq3byevI+NRLq6LOdSdjsG/rlbJmbmJXMbpMS+oLCHYY/fPzxNOw3IRjHhU4PtyIP
9xsQ7iOYNtTECR/Thyn0mC7/vFS1ty4+QU1GgIkIa7L12gc/EGziCP1rcE9EyDuw5WN23KHPlnJ2
M5GUOoBsil2doPhbfI2Y2IwCP/9LxQtKYoOZzNIaacWON2YfLupsRucjlQT/SqcKY+oQJQRw+G+R
xtdiSJ3nGHrS3EjRqdu41N5nUeaYnCrqZH5wncyF/K2OU9zWy8UCcMHDK/0q4uEpAiXecU4DJy0q
OavLpNoACWKV67M/Sn9wGk43PNGhhyQf8zABMSHiSHzCaeN7JtzckMsEB/wTD5wk7ruxg5OsENFz
eJ/lExx1Qjm+Y0aqey5Pj4P2CDkAGABQmP9gpCN3/htJr9wDRlpzl6ioJT1SupGGnJwxhDIcYaSD
f9NPnxFd3tqC5fV2LK93Y3ndxvK6F8trH8vr3Vi6IoELa4NWRhL6AlftY43efBs35sTDnMazJbfD
3E/M8QSIojAbbCNTnALtRbb4fI+AkNp2DpzpYZM/k3BSaZlzCFyDRO7HQyy9mTfJ605nysbRnXkq
xp3dlkPk9z2IIkoVm1J3lrd5XMWRJxfXaT4FsbXojhsAY9FOJ+JYaXY7mXJ0t2WpBhf/9fmHjx+w
OYIamPQG6oaLiIYFpzJ8GpfXqitNzeavAHakln4iDnXTAPceGFnjUfb4n3eU4YGMI9aUoZCLAjwA
yuqyzdzcpzBsPddJUvo5MzkfNh2LQVYNmkltIdLJxcW7k88nAwr5Df534AqMoa0vHS4+poVt0PXf
3OaW4tgHhFrHthrj587Jo3XDEffbWAO248O3Hhw+xGD3hgn8Wf5LKQVLAoSKdPD3MYR68B7oq7YJ
HfoYRuwk/7kna+ys2HeO7DkuiiP6fccO7QH8w07cY0yAANqFGpqdQbOZail9a153UNQB+kBf76u3
YO2tV3sn41PUTqLHAXQoa5ttd/+8cxo2ekpWb06/P/twfvbm4uTzD44LiK7cx08Hh+L0xy+C8kPQ
gLFPFGNqRIWZSGBY3EInMc/hvxojP/O64iAx9Hp3fq5PalZY6oK5z2hzInjOaUwWGgfNOAptH+r8
I8Qo1Rskp6aI0nWo5gj3SyuuZ1G5zo+mUqUpOqu13nrpWjFTU0bn2hFIHzR2ScEgOMUMXlEWe2V2
hSWfAOo6qx6ktI22iSEpBQU76QLO+Zc5XfECpdQZnjSdtaK/DF1cw6tIFWkCO7lXoZUl3Q3TYxrG
0Q/tATfj1acBne4wsm7Is96KBVqtVyHPTfcfNYz2Ww0YNgz2DuadSUoPoQxsTG4TITbik5xQ3sFX
u/R6DRQsGB70VbiIhukSmH0Mm2uxTGADATy5BOuL+wSA0FoJ/0DgyIkOyByzM8K3q/n+X0JNEL/1
L7/0NK/KdP9vooBdkOBUorCHmG7jd7DxiWQkTj++H4WMHKXmir/UWB4ADgkFQB1pp/wlPkGfDJVM
Fzq/xNcH+EL7CfS61b2URam797vGIUrAEzUkr+GJMvQLMd3Lwh7jVEYt0Fj5YDHDCkI3DcF89sSn
pUxTne9+9u78FHxHLMZACeJzt1MYjuMleISuk++4wrEFCg/Y4XWJbFyiC0tJFvPIa9YbtEaRo95e
XoZdJwoMd3t1osBlnCgX7SFOm2GZcoIIWRnWwiwrs3arDVLYbUMUR5lhlphclJTA6vME8DI9jXlL
BHslLPUwEXg+RU6yymQspskM9CioXFCoYxASJC7WMxLn5RnHwPNSmTIoeFhsyuR6WeHpBnSOqAQD
m/948uX87AOVJRy+bLzuHuYc005gzEkkx5giiNEO+OKm/SFXTSZ9PKtfIQzUPvCn/YqzU455gE4/
Dizin/YrrkM7dnaCPANQUHXRFg/cADjd+uSmkQXG1e6D8eOmADaY+WAoFollLzrRw51flxNty5Yp
obiPefmIA5xFYVPSdGc3Ja390XNcFHjONR/2N4K3fbJlPlPoetN5sy35zf10pBBLYgGjbmt/DJMd
1mmqp+Mw2zZuoW2ttrG/ZE6s1Gk3y1CUgYhDt/PIZbJ+JaybMwd6adQdYOI7ja6RxF5VPvglG2gP
w8PEEruzTzEdqYyFjABGMqSu/anBh0KLAAqEsn+HjuSOR08PvTk61uD+OWrdBbbxB1CEOheXajzy
EjgRvvzGjiO/IrRQjx6J0PFUMpnlNk8MP+slepUv/Dn2ygAFMVHsyji7lkOGNTYwn/nE3hKCJW3r
kfoyueozLOIMnNO7LRzelYv+gxODWosROu1u5KatjnzyYIPeUpCdBPPBl/EadH9RV0NeyS3n0L21
dNuh3g8Rsw+hqT59H4YYjvkt3LI+DeBeamhY6OH9tuUUltfGOLLWPraqmkL7QnuwsxK2ZpWiYxmn
ONH4otYLaAzucWPyB/apThSyv3vqxJyYkAXKg7sgvbkNdINWOGHA5UpcOZpQOnxTTaPfzeWtTMFo
gJEdYrXDr7baYRTZcEpvHthXY3exudj040ZvGsyOTDkGIkCFGL2Bnl0INTjgCv+idyJxdkPO8du/
no3F2w8/wb9v5EewoFjzOBZ/g9HF27yEbSUX7dJtCljAUfF+Ma8VFkYSNDqh4Isn0Fu78MiLpyG6
ssQvKbEKUmAybbni204ARZ4gFbI37oGpl4DfpqCr5YQaB7FvLQb6JdJge40L1oUc6JbRslqlaCac
4EiziJeD87O3px8+nUbVHTK2+Tlwgid+HhZORx8Nl3gMNhb2yazGJ1eOv/yDTIsed1nvNU29DO41
RQjbkcLuL/kmjdjuKeISAwai2MzzWYQtgdO5RK9ag/88craV99p3z7girOFIH541Tjw+BmqIX9r6
ZwANqY+eE/UkhOIp1orx42jQb4HHgiLa8OfpzXruBsR10Q9NsI1pM+uh392qwCXTWcOznER4Hdtl
MHWgaRKr1XTm1gd+zIS+CAWUGx1vyEVcp5WQGWylaG9PN1KAgndL+lhCmFXYilGdG0Vn0nW8UU7u
UazEAEcdUFE9nsNQoBC23j/GN2wGsNZQ1FwCDdAJUdo25U5XVc+WLMG8EyLq9eQbrJPspZvGoynM
g/LGeNb4rzBP9BYZo2tZ6fnzg+Ho8kWT4EDB6JlX0DsrwNi5bLIHGrN4+vTpQPzH/U4PoxKleX4D
3hjA7nVWzun1FoOtJ2dXq+vQmzcR8ONsKS/hwRUFze3zOqOI5I6utCDS/jUwQlyb0DKjad8yxxyr
K/l8mVvwOZU2GD9nCV13hBElicpW3xqF0SYjTcSSoBjCWM2SJOToBKzHJq+xFg+ji5pf5B1wfIJg
xvgWD8Z4h71Ex5LyZi33WHSOxYAADyiljEejYmaqRgM8JxcbjebkLEuqpozkuXtmqq8AqOwtRpqv
RLxGyTDzaBHDKev0WLVxrPOdLOptVPLZpRtnbM2SX9+HO7A2SFq+WBhM4aFZpFkuy5kxp7hiySyp
HDCmHcLhznR5E1mfKOhBaQDqnazC3Eq0ffsHuy4uph/p+HjfjKSzhip7IRbHhOKslVcYRc34FH2y
hLR8a76MYJQPFM3WnoA3lviDjqViDYF3b4dbzlhn+j4OTttoLukAOHQHlFWQlh09HeFcPGbhM9Nu
uUUDP7QzJ9xuk7Kq43Sir32YoJ82sefpGk9bBrezwNN6K+Db5+D47uuMfXAcTHIN0hMzbk1FxrFY
6MhE5FaW+UVYRY5e3iH7SuBTIGXmE1MPbWJHl5ZdbaGpTtV0VDyCemaKl7Y45KZqplNw4mI+pvQm
U+6wxXn2M0fp6grxWgxfjsVha+czKzZ4kxMg+2Qe+q4YdYOpOMEAM8f2vRji9bEYvhiLP+6AHm0Z
4OjQHaG9j21B2Ark5dWjyZgmUyJb2JfCfn9fncMImp5xHF21yd8l03dEpX9vUYkrBHWi8ot2onJr
7K371s7HRzJcgeJYJHK+/0QhCTXSjW7ezuCEHxbQ79kcLV073lTUUOHcFDYj60YPOhrRuM12EFOU
rtUX1++irmHDae8cMGkyrVRFe8scpjFq9FpEBQCTvqM0/IZ3u8B7TQrXP9t6xKqLACzYngiCrvTk
A7OmYSOo9zqCj9IA9zCKCPEwtVEUrmQ9QkRCugeHmOhZ6xDb4fjfnXm4xGDbUWgHy2+/2YWnK5i9
RR09C7q70sITWVte0Sy3+fQH5jxG6ev6VQLjQGlEB5xVc1UluZlHmL3Md9DkNot5hZdB0sk0msRU
um4Tb6X51i/0Yyh2QMlksBbgSdULPEi+pbstTxQlveEVNd8cvhibymAGpCfwMnr5TF8BSd3M5Qe+
jz3Wezd4qfsdRv/mAEsqv7d91dnN0LSOW3dB+YOFFD0bRRNLh8Yw3V8H0qxZLPDOxIaY7FvbC0De
g7czBT/HXH6ag8MGG9KoD11XYzTSu021bRHg+03GNsl5UNdGkSLSu4Rtm/LcpTgfLQq6V78FwRAC
cv4y5jfoCtbFkQ2xGZuCJ59DN5sTP9VNb90Z2xM0ttVNuGv63H/X3HWLwM7cJDN05u7Xl7o00H23
W9E+GnB4QxPiQSUSjcbvNyauHRjrHJr+CL3+IPndTjjTLWblPjAmYwfj/cSeGntj9lfxzP2OCWH7
fCGzW07c62y0pt2xGW2Of4inwMkv+NzeMEAZTXPNgbxfohv2JpwjO5HX12oS4+2OE9pkUz5XZ/dk
tm3v6XI+GauN2W3hpUUAwnCTzrx1k+uBMUBX8i3TnA7l3E4jaGhKGnaykFUyZ5Ogt3YALuKIKfU3
gXhOIx6kEgPdqi6LEnbDA30XMefp9KU2N0BNAG8VqxuDuukx1lfTkmKl5DBTgsxx2laSDxCBjXjH
NEwm9h3wyvPmmoVkbJlBZvVKlnHVXDHkZwQksOlqRqCic1xcJzzXSGWLS1zEEssbDlIYILPfn8HG
0ttU77hXYWS13cPZiXrokO9jrmxwjJHh4uTOXi/oXms1p6utXe/QNmu4zl6pBMtg7sojHaljZfxW
39/Fd8xyJB/9S4d/QN7dyks/C92qM/ZuLRrOM1chdC9swhsDyDj33cPY4YDujYutDbAd39cXllE6
HuaWxpaK2ifvVTjNaKMmgoQJo/dEkPyigEdGkDz4D4wg6VszwdBofLQe6C0TuCfUxOrBvYKyYQTo
MwEi4QF26wJDYyqHbtJ9kavkbmAvlGZd6VTyGfOAHNm9m4xA8FWTys1Q9q6C2xVB8qWLHn9//vHN
yTnRYnJx8vY/T76npCw8LmnZqgeH2LJ8n6m976V/u+E2nUjTN3iDbc8NsVzDpCF03ndyEHog9Ner
9S1oW5G5r7d16NT9dDsB4run3YK6TWX3Qu74ZbrGxE2faeVpB/opJ9WaX05mgnlkTupYHJqTOPO+
OTzRMtqJLW9bOCe9tatOtL+qbwHdEvce2SRrWgE8M0H+skcmpmLGBubZQWn/bz4oMxyrDc0NOiCF
M+nc5EiXODKoyv//iZSg7GLc27GjOLZ3c1M7Ph5S9tJ5PPudycgQxCv3G3Tn5wr7XKZbqBAErPD0
PYWMiNF/+kDVph88UeJynwqL91HZXNlfuGbauf1rgkkGlb3vS3GCEh+zQuNFnbqJA7ZPpwM5fXQa
lS+cShbQfAdA50Y8FbA3+kusEKcbEcLGUbtkmBxLdNSX9TnIo910sDe0ei72t5WdumWXQrzY3nDe
quzUPQ65h7qnh6pNcZ9jgTFLc1s9qXhNkPk4U9AFX57zgWfoetsPX28vXxzZwwXkd3ztKBLKJhs4
hv3Sycbceamk052YpRxTuh7u1ZyQsG5x5UBln2Db3qZTkrJl/2PyHBjSwHvfHzIzPbyr9wdtTC3r
HcGUxPCJGtG0nCIejbt9MupOt1FbXSBckPQAIB0VCLAQTEc3OgmiG87yHj7Xu8FpTdfxuidMoSMV
lCzmcwT3ML5fg1+7OxUSP6g7o2j6c4M2B+olB+Fm34FbjbxQyHaT0J56wwdbXACuye7v/+IB/btp
jLb74S6/2rZ62VsHyL4sZr5iZlCLROZxBEYG9OaQtDWWSxhBx2toGjq6DNXMDfkCHT/KpsXLtmmD
Qc7sRHsA1igE/wfVIOdx
""")

##file activate.sh
ACTIVATE_SH = convert("""
eJytVVFvokAQfudXTLEPtTlLeo9tvMSmJpq02hSvl7u2wRUG2QR2DSxSe7n/frOACEVNLlceRHa+
nfl25pvZDswCnoDPQ4QoTRQsENIEPci4CsBMZBq7CAsuLOYqvmYKTTj3YxnBgiXBudGBjUzBZUJI
BXEqgCvweIyuCjeG4eF2F5x14bcB9KQiQQWrjSddI1/oQIx6SYYeoFjzWIoIhYI1izlbhJjkKO7D
M/QEmKfO9O7WeRo/zr4P7pyHwWxkwitcgwpQ5Ej96OX+PmiFwLeVjFUOrNYKaq1Nud3nR2n8nI2m
k9H0friPTGVsUdptaxGrTEfpNVFEskxpXtUkkCkl1UNF9cgLBkx48J4EXyALuBtAwNYIjF5kcmUU
abMKmMq1ULoiRbgsDEkTSsKSGFCJ6Z8vY/2xYiSacmtyAfCDdCNTVZoVF8vSTQOoEwSnOrngBkws
MYGMBMg8/bMBLSYKS7pYEXP0PqT+ZmBT0Xuy+Pplj5yn4aM9nk72JD8/Wi+Gr98sD9eWSMOwkapD
BbUv91XSvmyVkICt2tmXR4tWmrcUCsjWOpw87YidEC8i0gdTSOFhouJUNxR+4NYBG0MftoCTD9F7
2rTtxG3oPwY1b2HncYwhrlmj6Wq924xtGDWqfdNxap+OYxplEurnMVo9RWks+rH8qKEtx7kZT5zJ
4H7oOFclrN6uFe+d+nW2aIUsSgs/42EIPuOhXq+jEo3S6tX6w2ilNkDnIpHCWdEQhFgwj9pkk7FN
l/y5eQvRSIQ5+TrL05lewxWpt/Lbhes5cJF3mLET1MGhcKCF+40tNWnUulxrpojwDo2sObdje3Bz
N3QeHqf3D7OjEXMVV8LN3ZlvuzoWHqiUcNKHtwNd0IbvPGKYYM31nPKCgkUILw3KL+Y8l7aO1ArS
Ad37nIU0fCj5NE5gQCuC5sOSu+UdI2NeXg/lFkQIlFpdWVaWZRfvqGiirC9o6liJ9FXGYrSY9mI1
D/Ncozgn13vJvsznr7DnkJWXsyMH7e42ljdJ+aqNDF1bFnKWFLdj31xtaJYK6EXFgqmV/ymD/ROG
+n8O9H8f5vsGOWXsL1+1k3g=
""")

##file activate.fish
ACTIVATE_FISH = convert("""
eJydVW2P2jgQ/s6vmAZQoVpA9/WkqqJaTou0u6x2uZVOVWWZZEKsS+yc7UDpr+84bziQbauLxEvs
eXnsZ56ZIWwTYSAWKUJWGAs7hMJgBEdhEwiMKnSIsBNywUMrDtziPBYmCeBDrFUG7v8HmCTW5n8u
Fu7NJJim81Bl08EQTqqAkEupLOhCgrAQCY2hTU+DQVxIiqgkRNiEBphFEKy+kd1BaFvwFOUBuIxA
oy20BKtAKp3xFMo0QNtCK5mhtMEA6BmSpUELKo38TThwLfguRVNaiRgs0llnEoIR29zfstf18/bv
5T17Wm7vAiiN3ONCzfbfwC3DtWXXDqHfAGX0q6z/bO82j3ebh1VwnbrduwTQbvwcRtesAfMGor/W
L3fs6Xnz8LRlm9fV8/P61sM0LDNwCZjl9gSpCokJRzpryGQ5t8kNGFUt51QjOZGu0Mj35FlYlXEr
yC09EVOp4lEXfF84Lz1qbhBsgl59vDedXI3rTV03xipduSgt9kLytI3XmBp3aV6MPoMQGNUU62T6
uQdeefTy1Hfj10zVHg2pq8fXDoHBiOv94csfXwN49xECqWREy7pwukKfvxdMY2j23vXDPuuxxeE+
JOdCOhxCE3N44B1ZeSLuZh8Mmkr2wEPAmPfKWHA2uxIRjEopdbQYjDz3BWOf14/scfmwoki1eQvX
ExBdF60Mqh+Y/QcX4uiH4Amwzx79KOVFtbL63sXJbtcvy8/3q5rupmO5CnE91wBviQAhjUUegYpL
vVEbpLt2/W+PklRgq5Ku6mp+rpMhhCo/lXthQTxJ2ysO4Ka0ad97S7VT/n6YXus6fzk3fLnBZW5C
KDC6gSO62QDqgFqLCCtPmjegjnLeAdArtSE8VYGbAJ/aLb+vnQutFhk768E9uRbSxhCMzdgEveYw
IZ5ZqFKl6+kz7UR4U+buqQZXu9SIujrAfD7f0FXpozB4Q0gwp31H9mVTZGGC4b871/wm7lvyDLu1
FUyvTj/yvD66k3UPTs08x1AQQaGziOl0S1qRkPG9COtBTSTWM9NzQ4R64B+Px/l3tDzCgxv5C6Ni
e+QaF9xFWrxx0V/G5uvYQOdiZzvYpQUVQSIsTr1TTghI33GnPbTA7/GCqcE3oE3GZurq4HeQXQD6
32XS1ITj/qLjN72ob0hc5C9bzw8MhfmL
""")

##file activate.csh
ACTIVATE_CSH = convert("""
eJx9VG1P2zAQ/u5fcYQKNgTNPtN1WxlIQ4KCUEGaxuQ6yYVYSuzKdhqVX7+zk3bpy5YPUXL3PPfc
ne98DLNCWshliVDV1kGCUFvMoJGugMjq2qQIiVSxSJ1cCofD1BYRnOVGV0CfZ0N2DD91DalQSjsw
tQLpIJMGU1euvPe7QeJlkKzgWixlhnAt4aoUVsLnLBiy5NtbJWQ5THX1ZciYKKWwkOFaE04dUm6D
r/zh7pq/3D7Nnid3/HEy+wFHY/gEJydg0aFaQrBFgz1c5DG1IhTs+UZgsBC2GMFBlaeH+8dZXwcW
VPvCjXdlAvCfQsE7al0+07XjZvrSCUevR5dnkVeKlFYZmUztG4BdzL2u9KyLVabTU0bdfg7a0hgs
cSmUg6UwUiQl2iHrcbcVGNvPCiLOe7+cRwG13z9qRGgx2z6DHjfm/Op2yqeT+xvOLzs0PTKHDz2V
tkckFHoQfQRXoGJAj9el0FyJCmEMhzgMS4sB7KPOE2ExoLcSieYwDvR+cP8cg11gKkVJc2wRcm1g
QhYFlXiTaTfO2ki0fQoiFM4tLuO4aZrhOzqR4dIPcWx17hphMBY+Srwh7RTyN83XOWkcSPh1Pg/k
TXX/jbJTbMtUmcxZ+/bbqOsy82suFQg/BhdSOTRhMNBHlUarCpU7JzBhmkKmRejKOQzayQe6MWoa
n1wqWmuh6LZAaHxcdeqIlVLhIBJdO9/kbl0It2oEXQj+eGjJOuvOIR/YGRqvFhttUB2XTvLXYN2H
37CBdbW2W7j2r2+VsCn0doVWcFG1/4y1VwBjfwAyoZhD
""")

##file activate.bat
ACTIVATE_BAT = convert("""
eJx9UdEKgjAUfW6wfxjiIH+hEDKUFHSKLCMI7kNOEkIf9P9pTJ3OLJ/03HPPPed4Es9XS9qqwqgT
PbGKKOdXL4aAFS7A4gvAwgijuiKlqOpGlATS2NeMLE+TjJM9RkQ+SmqAXLrBo1LLIeLdiWlD6jZt
r7VNubWkndkXaxg5GO3UaOOKS6drO3luDDiO5my3iA0YAKGzPRV1ack8cOdhysI0CYzIPzjSiH5X
0QcvC8Lfaj0emsVKYF2rhL5L3fCkVjV76kShi59NHwDniAHzkgDgqBcwOgTMx+gDQQqXCw==
""")

##file deactivate.bat
DEACTIVATE_BAT = convert("""
eJxzSE3OyFfIT0vj4ipOLVEI8wwKCXX0iXf1C7Pl4spMU0hJTcvMS01RiPf3cYmHyQYE+fsGhCho
cCkAAUibEkTEVhWLMlUlLk6QGixStlyaeCyJDPHw9/Pw93VFsQguim4ZXAJoIUw5DhX47XUM8UCx
EchHtwsohN1bILUgw61c/Vy4AJYPYm4=
""")

##file activate.ps1
ACTIVATE_PS = convert("""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""")

##file distutils-init.py
DISTUTILS_INIT = convert("""
eJytV1uL4zYUfvevOE0ottuMW9q3gVDa3aUMXXbLMlDKMBiNrSTqOJKRlMxkf33PkXyRbGe7Dw2E
UXTu37lpxLFV2oIyifAncxmOL0xLIfcG+gv80x9VW6maw7o/CANSWWBwFtqeWMPlGY6qPjV8A0bB
C4eKSTgZ5LRgFeyErMEeOBhbN+Ipgeizhjtnhkn7DdyjuNLPoCS0l/ayQTG0djwZC08cLXozeMss
aG5EzQ0IScpnWtHSTXuxByV/QCmxE7y+eS0uxWeoheaVVfqSJHiU7Mhhi6gULbOHorshkrEnKxpT
0n3A8Y8SMpuwZx6aoix3ouFlmW8gHRSkeSJ2g7hU+kiHLDaQw3bmRDaTGfTnty7gPm0FHbIBg9U9
oh1kZzAFLaue2R6htPCtAda2nGlDSUJ4PZBgCJBGVcwKTAMz/vJiLD+Oin5Z5QlvDPdulC6EsiyE
NFzb7McNTKJzbJqzphx92VKRFY1idenzmq3K0emRcbWBD0ryqc4NZGmKOOOX9Pz5x+/l27tP797c
f/z0d+4NruGNai8uAM0bfsYaw8itFk8ny41jsfpyO+BWlpqfhcG4yxLdi/0tQqoT4a8Vby382mt8
p7XSo7aWGdPBc+b6utaBmCQ7rQKQoWtAuthQCiold2KfJIPTT8xwg9blPumc+YDZC/wYGdAyHpJk
vUbHbHWAp5No6pK/WhhLEWrFjUwtPEv1Agf8YmnsuXUQYkeZoHm8ogP16gt2uHoxcEMdf2C6pmbw
hUMsWGhanboh4IzzmsIpWs134jVPqD/c74bZHdY69UKKSn/+KfVhxLgUlToemayLMYQOqfEC61bh
cbhwaqoGUzIyZRFHPmau5juaWqwRn3mpWmoEA5nhzS5gog/5jbcFQqOZvmBasZtwYlG93k5GEiyw
buHhMWLjDarEGpMGB2LFs5nIJkhp/nUmZneFaRth++lieJtHepIvKgx6PJqIlD9X2j6pG1i9x3pZ
5bHuCPFiirGHeO7McvoXkz786GaKVzC9DSpnOxJdc4xm6NSVq7lNEnKdVlnpu9BNYoKX2Iq3wvgh
gGEUM66kK6j4NiyoneuPLSwaCWDxczgaolEWpiMyDVDb7dNuLAbriL8ig8mmeju31oNvQdpnvEPC
1vAXbWacGRVrGt/uXN/gU0CDDwgooKRrHfTBb1/s9lYZ8ZqOBU0yLvpuP6+K9hLFsvIjeNhBi0KL
MlOuWRn3FRwx5oHXjl0YImUx0+gLzjGchrgzca026ETmYJzPD+IpuKzNi8AFn048Thd63OdD86M6
84zE8yQm0VqXdbbgvub2pKVnS76icBGdeTHHXTKspUmr4NYo/furFLKiMdQzFjHJNcdAnMhltBJK
0/IKX3DVFqvPJ2dLE7bDBkH0l/PJ29074+F0CsGYOxsb7U3myTUncYfXqnLLfa6sJybX4g+hmcjO
kMRBfA1JellfRRKJcyRpxdS4rIl6FdmQCWjo/o9Qz7yKffoP4JHjOvABcRn4CZIT2RH4jnxmfpVG
qgLaAvQBNfuO6X0/Ux02nb4FKx3vgP+XnkX0QW9pLy/NsXgdN24dD3LxO2Nwil7Zlc1dqtP3d7/h
kzp1/+7hGBuY4pk0XD/0Ao/oTe/XGrfyM773aB7iUhgkpy+dwAMalxMP0DrBcsVw/6p25+/hobP9
GBknrWExDhLJ1bwt1NcCNblaFbMKCyvmX0PeRaQ=
""")

##file distutils.cfg
DISTUTILS_CFG = convert("""
eJxNj00KwkAMhfc9xYNuxe4Ft57AjYiUtDO1wXSmNJnK3N5pdSEEAu8nH6lxHVlRhtDHMPATA4uH
xJ4EFmGbvfJiicSHFRzUSISMY6hq3GLCRLnIvSTnEefN0FIjw5tF0Hkk9Q5dRunBsVoyFi24aaLg
9FDOlL0FPGluf4QjcInLlxd6f6rqkgPu/5nHLg0cXCscXoozRrP51DRT3j9QNl99AP53T2Q=
""")

##file activate_this.py
ACTIVATE_THIS = convert("""
eJyNU01v2zAMvetXEB4K21jmDOstQA4dMGCHbeihlyEIDMWmG62yJEiKE//7kXKdpN2KzYBt8euR
fKSyLPs8wiEo8wh4wqZTGou4V6Hm0wJa1cSiTkJdr8+GsoTRHuCotBayiWqQEYGtMCgfD1KjGYBe
5a3p0cRKiAe2NtLADikftnDco0ko/SFEVgEZ8aRC5GLux7i3BpSJ6J1H+i7A2CjiHq9z7JRZuuQq
siwTIvpxJYCeuWaBpwZdhB+yxy/eWz+ZvVSU8C4E9FFZkyxFsvCT/ZzL8gcz9aXVE14Yyp2M+2W0
y7n5mp0qN+avKXvbsyyzUqjeWR8hjGE+2iCE1W1tQ82hsCZN9UzlJr+/e/iab8WfqsmPI6pWeUPd
FrMsd4H/55poeO9n54COhUs+sZNEzNtg/wanpjpuqHJaxs76HtZryI/K3H7KJ/KDIhqcbJ7kI4ar
XL+sMgXnX0D+Te2Iy5xdP8yueSlQB/x/ED2BTAtyE3K4SYUN6AMNfbO63f4lBW3bUJPbTL+mjSxS
PyRfJkZRgj+VbFv+EzHFi5pKwUEepa4JslMnwkowSRCXI+m5XvEOvtuBrxHdhLalG0JofYBok6qj
YdN2dEngUlbC4PG60M1WEN0piu7Nq7on0mgyyUw3iV1etLo6r/81biWdQ9MWHFaePWZYaq+nmp+t
s3az+sj7eA0jfgPfeoN1
""")

MH_MAGIC = 0xfeedface
MH_CIGAM = 0xcefaedfe
MH_MAGIC_64 = 0xfeedfacf
MH_CIGAM_64 = 0xcffaedfe
FAT_MAGIC = 0xcafebabe
BIG_ENDIAN = '>'
LITTLE_ENDIAN = '<'
LC_LOAD_DYLIB = 0xc
maxint = majver == 3 and getattr(sys, 'maxsize') or getattr(sys, 'maxint')


class fileview(object):
    """
    A proxy for file-like objects that exposes a given view of a file.
    Modified from macholib.
    """

    def __init__(self, fileobj, start=0, size=maxint):
        if isinstance(fileobj, fileview):
            self._fileobj = fileobj._fileobj
        else:
            self._fileobj = fileobj
        self._start = start
        self._end = start + size
        self._pos = 0

    def __repr__(self):
        return '<fileview [%d, %d] %r>' % (
            self._start, self._end, self._fileobj)

    def tell(self):
        return self._pos

    def _checkwindow(self, seekto, op):
        if not (self._start <= seekto <= self._end):
            raise IOError("%s to offset %d is outside window [%d, %d]" % (
                op, seekto, self._start, self._end))

    def seek(self, offset, whence=0):
        seekto = offset
        if whence == os.SEEK_SET:
            seekto += self._start
        elif whence == os.SEEK_CUR:
            seekto += self._start + self._pos
        elif whence == os.SEEK_END:
            seekto += self._end
        else:
            raise IOError("Invalid whence argument to seek: %r" % (whence,))
        self._checkwindow(seekto, 'seek')
        self._fileobj.seek(seekto)
        self._pos = seekto - self._start

    def write(self, bytes):
        here = self._start + self._pos
        self._checkwindow(here, 'write')
        self._checkwindow(here + len(bytes), 'write')
        self._fileobj.seek(here, os.SEEK_SET)
        self._fileobj.write(bytes)
        self._pos += len(bytes)

    def read(self, size=maxint):
        assert size >= 0
        here = self._start + self._pos
        self._checkwindow(here, 'read')
        size = min(size, self._end - here)
        self._fileobj.seek(here, os.SEEK_SET)
        bytes = self._fileobj.read(size)
        self._pos += len(bytes)
        return bytes


def read_data(file, endian, num=1):
    """
    Read a given number of 32-bits unsigned integers from the given file
    with the given endianness.
    """
    res = struct.unpack(endian + 'L' * num, file.read(num * 4))
    if len(res) == 1:
        return res[0]
    return res


def mach_o_change(path, what, value):
    """
    Replace a given name (what) in any LC_LOAD_DYLIB command found in
    the given binary with a new name (value), provided it's shorter.
    """

    def do_macho(file, bits, endian):
        # Read Mach-O header (the magic number is assumed read by the caller)
        cputype, cpusubtype, filetype, ncmds, sizeofcmds, flags = read_data(file, endian, 6)
        # 64-bits header has one more field.
        if bits == 64:
            read_data(file, endian)
        # The header is followed by ncmds commands
        for n in range(ncmds):
            where = file.tell()
            # Read command header
            cmd, cmdsize = read_data(file, endian, 2)
            if cmd == LC_LOAD_DYLIB:
                # The first data field in LC_LOAD_DYLIB commands is the
                # offset of the name, starting from the beginning of the
                # command.
                name_offset = read_data(file, endian)
                file.seek(where + name_offset, os.SEEK_SET)
                # Read the NUL terminated string
                load = file.read(cmdsize - name_offset).decode()
                load = load[:load.index('\0')]
                # If the string is what is being replaced, overwrite it.
                if load == what:
                    file.seek(where + name_offset, os.SEEK_SET)
                    file.write(value.encode() + '\0'.encode())
            # Seek to the next command
            file.seek(where + cmdsize, os.SEEK_SET)

    def do_file(file, offset=0, size=maxint):
        file = fileview(file, offset, size)
        # Read magic number
        magic = read_data(file, BIG_ENDIAN)
        if magic == FAT_MAGIC:
            # Fat binaries contain nfat_arch Mach-O binaries
            nfat_arch = read_data(file, BIG_ENDIAN)
            for n in range(nfat_arch):
                # Read arch header
                cputype, cpusubtype, offset, size, align = read_data(file, BIG_ENDIAN, 5)
                do_file(file, offset, size)
        elif magic == MH_MAGIC:
            do_macho(file, 32, BIG_ENDIAN)
        elif magic == MH_CIGAM:
            do_macho(file, 32, LITTLE_ENDIAN)
        elif magic == MH_MAGIC_64:
            do_macho(file, 64, BIG_ENDIAN)
        elif magic == MH_CIGAM_64:
            do_macho(file, 64, LITTLE_ENDIAN)

    assert(len(what) >= len(value))
    do_file(open(path, 'r+b'))


if __name__ == '__main__':
    main()

## TODO:
## Copy python.exe.manifest
## Monkeypatch distutils.sysconfig

########NEW FILE########
__FILENAME__ = deploy
import argparse
import os
import platform
import pwd
import sys

import deploy_hbase
import deploy_hdfs
import deploy_utils
import deploy_zookeeper
import deploy_yarn
import deploy_impala
import deploy_kafka
import deploy_storm
import deploy_fds
import deploy_chronos

from log import Log

SERVICE_DEPLOY_TOOL_MAP = {
  "hdfs": deploy_hdfs,
  "yarn": deploy_yarn,
  "hbase": deploy_hbase,
  "zookeeper": deploy_zookeeper,
  "impala": deploy_impala,
  "kafka": deploy_kafka,
  "storm": deploy_storm,
  "fds": deploy_fds,
  "chronos": deploy_chronos
}

LOG_LEVEL_RANGE_MAP = [
  "trace", "debug", "info", "warn", "error", "fatal"
]

def add_service_arguments(parser):
  # NOTE: add_service_arguments must be called lastly.
  parser.add_argument("service",
      choices=SERVICE_DEPLOY_TOOL_MAP.keys(),
      help="The service type to be deployed.")
  parser.add_argument("cluster",
      help="The cluster name where the service would be deployed.")
  parser.add_argument("--job", type=str, nargs="+",
      help="The list of jobs to be manipulated, separated by space. If empty, "
           "all jobs would be manipulated.")
  parser.add_argument("--log_level", type=str, default="",
      choices=LOG_LEVEL_RANGE_MAP,
      help="The global log level to be configured for the service.")
  task_group = parser.add_mutually_exclusive_group()
  task_group.add_argument("--task", type=str, nargs="+",
      help="The list of tasks to be manipulated, separated by space. If task "
           "and host are all empty, all tasks would be manipulated. "
           "Option --task is exclusive with --host.")
  task_group.add_argument("--host", type=str, nargs="+",
      help="The list of task hosts to be manipulated, separated by space. If "
           "task and host are all empty, all tasks would be manipulated. "
           "Option --task is exclusive with --host. "
           "--host option is only supported in hbase cluster now.")

def parse_command_line():
  parser = argparse.ArgumentParser(
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      description="Manage the hadoop cluster.")

  parser.add_argument("--version", action="version",
      version="%(prog)s 1.0.0-beta")
  parser.add_argument("-v", "--verbosity", default=0, type=int,
      help="The verbosity level of log, higher value, more details.")

  parser.add_argument("--remote_user", default="work",
      help="The user to login remote machines.")

  subparsers = parser.add_subparsers(
      title="commands",
      help="Type '%(prog)s command -h' to get more help for individual "
           "command.")

  sub_parser = subparsers.add_parser(
      "install",
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      help="Install binary packages to a cluster according to specified "
           "configuration. Only binary package (tarball) would be installed, "
           "config files and start/stop scripts would NOT be installed.")
  sub_parser.add_argument("--make_current", action="store_false",
      help="Make the installed pacakge as current version.")
  # NOTE: add_service_arguments must be called lastly.
  add_service_arguments(sub_parser)
  sub_parser.set_defaults(handler=process_command_install)

  sub_parser = subparsers.add_parser(
      "cleanup",
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      help="Cleanup all data files of a service. Used when you want to "
           "re-deploy a service and discard all old data.\n"
           "NOTE: before using it, make sure you know what's going to happen!")
  add_service_arguments(sub_parser)
  sub_parser.set_defaults(handler=process_command_cleanup)

  sub_parser = subparsers.add_parser(
      "bootstrap",
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      help="Bootstrap a new cluster for a service. "
           "It would fail if old data of this service exists.")
  add_specify_version_options(sub_parser)
  sub_parser.add_argument("--update_config", action="store_true",
      default=False, help="Update the config files")
  add_service_arguments(sub_parser)
  sub_parser.set_defaults(handler=process_command_bootstrap)

  sub_parser = subparsers.add_parser(
      "start",
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      help="Start whole service on the specified cluster. Config files and "
           "control scripts (start/stop/restart, etc) would be generated at "
           "this phase and copied to destination hosts.")
  add_specify_version_options(sub_parser)
  sub_parser.add_argument("--skip_confirm", action="store_true",
      default=False, help="Whether skip the confirmation or not")
  sub_parser.add_argument("--update_config", action="store_true",
      default=False, help="Update the config files")
  add_service_arguments(sub_parser)
  sub_parser.set_defaults(handler=process_command_start)

  sub_parser = subparsers.add_parser(
      "stop",
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      help="Stop whole service on the specified cluster.")
  add_service_arguments(sub_parser)
  sub_parser.add_argument("--skip_confirm", action="store_true",
      default=False, help="Whether skip the confirmation or not")
  sub_parser.set_defaults(handler=process_command_stop)

  sub_parser = subparsers.add_parser(
      "restart",
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      help="Restart whole service on the specified cluster.")
  add_specify_version_options(sub_parser)
  sub_parser.add_argument("--skip_confirm", action="store_true",
      default=False, help="Whether skip the confirmation or not")
  sub_parser.add_argument("--update_config", action="store_true",
      default=False, help="Update the config files")
  add_service_arguments(sub_parser)
  sub_parser.set_defaults(handler=process_command_restart)

  sub_parser = subparsers.add_parser(
      "show",
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      help="Show status of packages/services/jobs/tasks.")
  add_service_arguments(sub_parser)
  sub_parser.set_defaults(handler=process_command_show)

  sub_parser = subparsers.add_parser(
      "shell",
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      help="Run the shell of specified service %s" % SERVICE_DEPLOY_TOOL_MAP.keys(),
      )
  add_specify_version_options(sub_parser)
  add_service_arguments(sub_parser)
  sub_parser.add_argument("command", nargs=argparse.REMAINDER,
      help="The command to execute")
  sub_parser.set_defaults(handler=process_command_shell)

  sub_parser = subparsers.add_parser(
      "pack",
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      help="Pack client utilities of Hadoop/Hbase/Zookeeper for users")
  add_specify_version_options(sub_parser)
  sub_parser.add_argument("--package_root", default="./packages",
      help="The local root to store the packed pacakges")
  sub_parser.add_argument("--skip_tarball", action="store_true",
      help="Skip make the tarball of the packed package")
  add_service_arguments(sub_parser)
  sub_parser.set_defaults(handler=process_command_pack)

  sub_parser = subparsers.add_parser(
      "rolling_update",
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      help="Rolling update the specified job, users must specify "
           "the job through the --job option")
  sub_parser.add_argument("--skip_confirm", action="store_true",
      default=False, help="Whether skip the confirmation or not")
  sub_parser.add_argument("--vacate_rs", action="store_true",
      default=False, help="Whether to vacate region server before restart it")
  sub_parser.add_argument("--time_interval", default=120, type=int,
      help="The time interval between rolling update tasks")
  sub_parser.add_argument("--update_config", action="store_true",
      default=False, help="Update the config files")
  add_specify_version_options(sub_parser)
  add_service_arguments(sub_parser)
  sub_parser.set_defaults(handler=process_command_rolling_update)

  args = parser.parse_args()
  Log.verbosity = args.verbosity
  return args

def add_specify_version_options(sub_parser):
  sub_parser.add_argument("--package_name", default="",
      help="Specify a package to bootstrap")
  sub_parser.add_argument("--revision", default="",
      help="Specify a revision of a package to bootstrap, should be "
           "specified along with --package_name, otherwise, will be ignored")
  sub_parser.add_argument("--timestamp", default="",
      help="Specify a timestamp of a package to bootstrap, should be "
           "specified along with --package_name and --revision, otherwise "
           "will be ignored")
  sub_parser.add_argument("--update_package", action="store_true",
      help="Force the supervisor server to download the latest package from "
           "the package server, if the package_name, revsion and timestamp "
           "are specified, this option will be ignored")

def process_command_install(args):
  deploy_tool = SERVICE_DEPLOY_TOOL_MAP.get(args.service)
  if deploy_tool:
    return deploy_tool.install(args)
  Log.print_critical("Not implemented for service: %s", args.service)

def process_command_cleanup(args):
  deploy_utils.check_admin_priviledge(args)
  deploy_tool = SERVICE_DEPLOY_TOOL_MAP.get(args.service)
  if deploy_tool:
    return deploy_tool.cleanup(args)
  Log.print_critical("Not implemented for service: %s", args.service)

def process_command_bootstrap(args):
  deploy_utils.check_admin_priviledge(args)
  args.update_config = True
  deploy_tool = SERVICE_DEPLOY_TOOL_MAP.get(args.service)
  if deploy_tool:
    return deploy_tool.bootstrap(args)
  Log.print_critical("Not implemented for service: %s", args.service)

def process_command_start(args):
  deploy_utils.check_admin_priviledge(args)
  deploy_tool = SERVICE_DEPLOY_TOOL_MAP.get(args.service)
  if deploy_tool:
    return deploy_tool.start(args)
  Log.print_critical("Not implemented for service: %s", args.service)

def process_command_stop(args):
  deploy_utils.check_admin_priviledge(args)
  deploy_tool = SERVICE_DEPLOY_TOOL_MAP.get(args.service)
  if deploy_tool:
    return deploy_tool.stop(args)
  Log.print_critical("Not implemented for service: %s", args.service)

def process_command_restart(args):
  deploy_utils.check_admin_priviledge(args)
  deploy_tool = SERVICE_DEPLOY_TOOL_MAP.get(args.service)
  if deploy_tool:
    return deploy_tool.restart(args)
  Log.print_critical("Not implemented for service: %s", args.service)

def process_command_show(args):
  deploy_tool = SERVICE_DEPLOY_TOOL_MAP.get(args.service)
  if deploy_tool:
    return deploy_tool.show(args)
  Log.print_critical("Not implemented for service: %s", args.service)

def process_command_shell(args):
  deploy_utils.check_admin_priviledge(args)
  deploy_tool = SERVICE_DEPLOY_TOOL_MAP.get(args.service)
  if deploy_tool:
    return deploy_tool.run_shell(args)
  Log.print_critical("Not implemented for service: %s", args.service)

def process_command_pack(args):
  deploy_tool = SERVICE_DEPLOY_TOOL_MAP.get(args.service)
  if deploy_tool:
    return deploy_tool.pack(args)
  Log.print_critical("Not implemented for service: %s", args.service)

def process_command_rolling_update(args):
  deploy_utils.check_admin_priviledge(args)
  deploy_tool = SERVICE_DEPLOY_TOOL_MAP.get(args.service)
  if deploy_tool:
    return deploy_tool.rolling_update(args)
  Log.print_critical("Not implemented for service: %s", args.service)

def main():
  args = parse_command_line()
  return args.handler(args)

if __name__ == '__main__':
  main()

########NEW FILE########
__FILENAME__ = deploy_chronos
#!/usr/bin/env python

import argparse
import os
import service_config
import subprocess
import sys
import urlparse

import deploy_utils

from log import Log

ALL_JOBS = ["chronos"]

def _get_chronos_service_config(args):
  args.chronos_config = deploy_utils.get_service_config(args)

def generate_zk_jaas_config(args):
  if not deploy_utils.is_security_enabled(args):
    return ""

  config_dict = args.chronos_config.configuration.generated_files["jaas.conf"]

  for key, value in config_dict.items()[1:]:
    if value != "true" and value != "false" and value.find("\"") == -1:
      config_dict[key] = "\"" + value + "\""

  header_line = config_dict["headerLine"]
  return "Client {\n  %s\n%s;\n};" % (header_line,
    "\n".join(["  %s=%s" % (key, value)
      for (key, value) in config_dict.iteritems() if key != config_dict.keys()[0]]))

def generate_configs(args, job_name, host_id, instance_id):
  chronos_cfg_dict = args.chronos_config.configuration.generated_files["chronos.cfg"]
  hosts = args.chronos_config.jobs[job_name].hosts
  chronos_cfg = deploy_utils.generate_properties_file(args, chronos_cfg_dict)

  config_files = {
    "chronos.cfg": chronos_cfg,
    "jaas.conf" : generate_zk_jaas_config(args),
  }
  config_files.update(args.chronos_config.configuration.raw_files) # what's this?

  return config_files

def generate_run_scripts_params(args, host, job_name, host_id, instance_id):
  job = args.chronos_config.jobs[job_name]

  supervisor_client = deploy_utils.get_supervisor_client(host,
      "chronos", args.chronos_config.cluster.name, job_name, instance_id=instance_id)

  artifact_and_version = "chronos-" + args.chronos_config.cluster.version

  jar_dirs = "$package_dir/lib/*"
  log_level = deploy_utils.get_service_log_level(args, args.chronos_config)

  params = job.get_arguments(args, args.chronos_config.cluster, args.chronos_config.jobs,
    args.chronos_config.arguments_dict, job_name, host_id, instance_id)

  script_dict = {
      "artifact": artifact_and_version,
      "job_name": job_name,
      "jar_dirs": jar_dirs,
      "run_dir": supervisor_client.get_run_dir(),
      "params": params,
  }

  return script_dict

def generate_start_script(args, host, job_name, host_id, instance_id):
  script_params = generate_run_scripts_params(args, host, job_name, host_id, instance_id)
  return deploy_utils.create_run_script(
      "%s/start.sh.tmpl" % deploy_utils.get_template_dir(),
      script_params)

def install(args):
  _get_chronos_service_config(args)
  deploy_utils.install_service(args, "chronos", args.chronos_config, "chronos")

def cleanup(args):
  _get_chronos_service_config(args)

  cleanup_token = deploy_utils.confirm_cleanup(args,
      "chronos", args.chronos_config)
  for job_name in args.job or ALL_JOBS:
    hosts = args.chronos_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.cleanup_job("chronos", args.chronos_config,
          hosts[host_id].ip, job_name, instance_id, cleanup_token)

def bootstrap_job(args, host, job_name, host_id, instance_id, cleanup_token):
  # parse the service_config according to the instance_id
  args.chronos_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  deploy_utils.bootstrap_job(args, "chronos", "chronos",
      args.chronos_config, host, job_name, instance_id, cleanup_token, '0')
  start_job(args, host, job_name, host_id, instance_id)

def bootstrap(args):
  _get_chronos_service_config(args)
  cleanup_token = deploy_utils.confirm_bootstrap("chronos", args.chronos_config)

  for job_name in args.job or ALL_JOBS:
    hosts = args.chronos_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        bootstrap_job(args, hosts[host_id].ip, job_name, host_id, instance_id, cleanup_token)

def start_job(args, host, job_name, host_id, instance_id):
  # parse the service_config according to the instance_id
  args.chronos_config.parse_generated_config_files(args, job_name, host_id, instance_id)

  config_files = generate_configs(args, job_name, host_id, instance_id)
  start_script = generate_start_script(args, host, job_name, host_id, instance_id)
  http_url = deploy_utils.get_http_service_uri(host,
    args.chronos_config.jobs[job_name].base_port, instance_id)
  deploy_utils.start_job(args, "chronos", "chronos", args.chronos_config,
      host, job_name, instance_id, start_script, http_url, **config_files)

def start(args):
  if not args.skip_confirm:
    deploy_utils.confirm_start(args)
  _get_chronos_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.chronos_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def stop_job(args, host, job_name, instance_id):
  deploy_utils.stop_job("chronos", args.chronos_config, host, job_name, instance_id)

def stop(args):
  if not args.skip_confirm:
    deploy_utils.confirm_stop(args)
  _get_chronos_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.chronos_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

def restart(args):
  if not args.skip_confirm:
    deploy_utils.confirm_restart(args)
  _get_chronos_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.chronos_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

  for job_name in args.job or ALL_JOBS:
    hosts = args.chronos_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.wait_for_job_stopping("chronos",
          args.chronos_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def show(args):
  _get_chronos_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.chronos_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.show_job("chronos", args.chronos_config,
          hosts[host_id].ip, job_name, instance_id)

def run_shell(args):
  Log.print_critical("'shell' command is not supported!")

def pack(args):
  Log.print_critical("'pack' command is not supported!")

def rolling_update(args):
  if not args.job:
    Log.print_critical("You must specify the job name to do rolling update")

  _get_chronos_service_config(args)
  job_name = args.job[0]

  if not args.skip_confirm:
    deploy_utils.confirm_action(args, "rolling_update")

  Log.print_info("Rolling updating %s" % job_name)
  hosts = args.chronos_config.jobs[job_name].hosts
  wait_time = 0

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.iterkeys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.confirm_rolling_update(host_id, instance_id, wait_time)
      stop_job(args, hosts[host_id].ip, job_name, instance_id)
      deploy_utils.wait_for_job_stopping("chronos",
        args.chronos_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)
      deploy_utils.wait_for_job_starting("chronos",
        args.chronos_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      wait_time = args.time_interval
  Log.print_success("Rolling updating %s success" % job_name)

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = deploy_config
import ConfigParser
import os

from log import Log

DEPLOY_CONFIG = "../deploy.cfg"

class DeployConfig:
  '''
  The deploy config class.
  '''
  def __init__(self, file_name):
    self.config_file = os.path.abspath(file_name)
    self.config_parser = ConfigParser.SafeConfigParser()
    self.config_parser.optionxform = str
    self.config_parser.read([self.config_file])

  def get_supervisor_config(self):
    '''
    Get the supervisor config items from the deploy config file.
    '''
    config = {
      'server_port': self.config_parser.getint('supervisor', 'server_port'),
      'user': self.config_parser.get('supervisor', 'user'),
      'password': self.config_parser.get('supervisor', 'password'),
    }
    return config

  def get_tank_config(self):
    '''
    Get the tank config items from the deploy config file.
    '''
    config = {
      'server_host': self.config_parser.get('tank', 'server_host'),
      'server_port': self.config_parser.getint('tank', 'server_port'),
    }
    return config

  def get_config_dir(self):
    '''
    Get the service config file's root directory.
    '''
    return self._get_real_path(self.config_parser.get(
          'default', 'config_dir'))

  def get_zookeeper_root(self):
    '''
    Get the local zookeeper root directory.
    '''
    return self._get_real_path(self.config_parser.get(
          'default', 'zookeeper_root'))

  def get_zookeeper_package_dir(self):
    '''
    Get the local zookeeper tarball directory.
    '''
    return '%s/build' % self.get_zookeeper_root()

  def get_hadoop_root(self):
    '''
    Get the local hadoop root directory.
    '''
    return self._get_real_path(self.config_parser.get(
          'default', 'hadoop_root'))

  def get_hadoop_package_dir(self):
    '''
    Get the local hadoop tarball directory.
    '''
    return '%s/hadoop-dist/target' % self.get_hadoop_root()

  def get_hbase_root(self):
    '''
    Get the local hbase root directory.
    '''
    return self._get_real_path(self.config_parser.get(
          'default', 'hbase_root'))

  def get_hbase_package_dir(self):
    '''
    Get the local hbase tarball directory.
    '''
    return '%s/target' % self.get_hbase_root()

  def get_impala_root(self):
    '''
    Get the local impala root directory
    '''
    return self._get_real_path(self.config_parser.get(
          'default', 'impala_root'))

  def get_imapala_package_dir(self):
    '''
    Get the local impala tarball directory
    '''
    return '%s/release' % self.get_impala_root()

  def get_kafka_root(self):
    '''
    Get the local kafka root directory
    '''
    return self._get_real_path(self.config_parser.get(
          'default', 'kafka_root'))

  def get_kafka_package_dir(self):
    '''
    Get the local kafka tarball directory
    '''
    return '%s/release' % self.get_kafka_root()

  def get_storm_root(self):
    '''
    Get the local storm root directory
    '''
    return self._get_real_path(self.config_parser.get(
          'default', 'storm_root'))

  def get_storm_package_dir(self):
    '''
    Get the local storm tarball directory
    '''
    return '%s/storm-core/target' % self.get_storm_root()

  def get_galaxy_root(self):
    '''
    Get the local galaxy root directory
    '''
    return self._get_real_path(self.config_parser.get(
          'default', 'galaxy_root'))

  def get_galaxy_package_dir(self):
    '''
    Get the local galaxy dist tarball directory
    '''
    return '%s/galaxy-dist/target' % self.get_galaxy_root()

  def get_chronos_root(self):
    '''
    Get the local chronos root directory
    '''
    return self._get_real_path(self.config_parser.get(
          'default', 'chronos_root'))

  def get_chronos_package_dir(self):
    '''
    Get the local chronos tarball directory
    '''
    return '%s/target' % self.get_chronos_root()

  def get_package_download_root(self):
    '''
    Get the local packages download root directory
    '''
    return "%s/packages" % self._get_real_path(
      self.config_parser.get('default', 'minos_home'))

  def get_admin_list(self):
    '''
    Get the administrators list.
    '''
    return self.config_parser.get('default', 'admin_list').split(',')

  def _get_deploy_root(self):
    return os.path.dirname(self.config_file)

  def _get_real_path(self, path):
    if path.startswith('/'):
      return path
    elif path.startswith('~'):
      return os.path.expanduser(path)
    else:
      return os.path.abspath('%s/%s' % (
        self._get_deploy_root(), path))


def get_deploy_config():
  '''
  A factory method to construct the deploy config object.
  '''
  config_file = os.getenv('MINOS_CONFIG_FILE')
  if config_file:
    if not config_file.startswith('/'):
      config_file = '%s/%s' % (os.path.dirname(__file__), config_file)
  else:
    config_file = '%s/%s' % (os.path.dirname(__file__), DEPLOY_CONFIG)

  if os.path.exists(config_file):
    return DeployConfig(config_file)

  Log.print_critical('Cannot find the config file: deploy.cfg, you should'
      ' specify it by defining the environment variable MINOS_CONFIG_FILE'
      ', or just put the file under the directory: %s' % os.path.dirname(
        os.path.abspath('%s/%s' % (os.path.dirname(__file__), DEPLOY_CONFIG))))


########NEW FILE########
__FILENAME__ = deploy_fds
import deploy_utils

from log import Log

ALL_JOBS = ["restserver", "proxy"]

def _get_fds_service_config(args):
  args.fds_config = deploy_utils.get_service_config(args)

def install(args):
  _get_fds_service_config(args)
  deploy_utils.install_service(args, "fds", args.fds_config, "galaxy")

def bootstrap_job(args, host, job_name, host_id, instance_id, cleanup_token):
  # parse the service_config according to the instance_id
  args.fds_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  deploy_utils.bootstrap_job(args, "galaxy", "fds",
      args.fds_config, host, job_name, instance_id, cleanup_token, '0')
  start_job(args, host, job_name, host_id, instance_id)

def bootstrap(args):
  _get_fds_service_config(args)
  cleanup_token = deploy_utils.confirm_bootstrap("fds", args.fds_config)

  for job_name in args.job or ALL_JOBS:
    hosts = args.fds_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        bootstrap_job(args, hosts[host_id].ip, job_name, host_id, instance_id, cleanup_token)

def generate_run_scripts_params(args, host, job_name, host_id, instance_id):
  job = args.fds_config.jobs[job_name]

  supervisor_client = deploy_utils.get_supervisor_client(host,
      "fds", args.fds_config.cluster.name, job_name, instance_id=instance_id)

  artifact_and_version = "galaxy-fds-" + args.fds_config.cluster.version

  component_dir = "$package_dir"
  jar_dirs = "%s/lib/guava-11.0.2.jar:%s/:%s/lib/*" % (
    component_dir, component_dir, component_dir)
  log_level = deploy_utils.get_service_log_level(args, args.fds_config)

  params = job.get_arguments(args, args.fds_config.cluster, args.fds_config.jobs,
    args.fds_config.arguments_dict, job_name, host_id, instance_id)

  script_dict = {
      "artifact": artifact_and_version,
      "job_name": job_name,
      "jar_dirs": jar_dirs,
      "run_dir": supervisor_client.get_run_dir(),
      "params": params,
  }

  return script_dict

def generate_start_script(args, host, job_name, host_id, instance_id):
  script_params = generate_run_scripts_params(args, host, job_name, host_id, instance_id)
  return deploy_utils.create_run_script(
      "%s/start.sh.tmpl" % deploy_utils.get_template_dir(), script_params)

def generate_configs(args, host, job_name, instance_id):
  core_site_xml = deploy_utils.generate_site_xml(args,
    args.fds_config.configuration.generated_files["core-site.xml"])
  hdfs_site_xml = deploy_utils.generate_site_xml(args,
    args.fds_config.configuration.generated_files["hdfs-site.xml"])
  hbase_site_xml = deploy_utils.generate_site_xml(args,
    args.fds_config.configuration.generated_files["hbase-site.xml"])
  galaxy_site_xml = deploy_utils.generate_site_xml(args,
    args.fds_config.configuration.generated_files["galaxy-site.xml"])
  zookeeper_properties = deploy_utils.generate_properties_file(args,
      args.fds_config.configuration.generated_files["zookeeper.properties"])

  config_files = {
    "core-site.xml": core_site_xml,
    "hdfs-site.xml": hdfs_site_xml,
    "hbase-site.xml": hbase_site_xml,
    "galaxy-site.xml": galaxy_site_xml,
    "zookeeper.properties": zookeeper_properties,
  }
  config_files.update(args.fds_config.configuration.raw_files)

  return config_files

def start_job(args, host, job_name, host_id, instance_id):
  # parse the service_config according to the instance_id
  args.fds_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  config_files = generate_configs(args, host, job_name, instance_id)
  start_script = generate_start_script(args, host, job_name, host_id, instance_id)
  http_url = deploy_utils.get_http_service_uri(host,
    args.fds_config.jobs[job_name].base_port, instance_id)
  deploy_utils.start_job(args, "galaxy", "fds", args.fds_config,
      host, job_name, instance_id, start_script, http_url, **config_files)

def start(args):
  if not args.skip_confirm:
    deploy_utils.confirm_start(args)
  _get_fds_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.fds_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def stop_job(args, host, job_name, instance_id):
  deploy_utils.stop_job("fds", args.fds_config,
      host, job_name, instance_id)

def stop(args):
  if not args.skip_confirm:
    deploy_utils.confirm_stop(args)
  _get_fds_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.fds_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

def restart(args):
  if not args.skip_confirm:
    deploy_utils.confirm_restart(args)
  _get_fds_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.fds_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

  for job_name in args.job or ALL_JOBS:
    hosts = args.fds_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.wait_for_job_stopping("fds",
          args.fds_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def cleanup(args):
  _get_fds_service_config(args)

  cleanup_token = deploy_utils.confirm_cleanup(args,
      "fds", args.fds_config)
  for job_name in args.job or ALL_JOBS:
    hosts = args.fds_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.cleanup_job("fds", args.fds_config,
          hosts[host_id].ip, job_name, instance_id, cleanup_token)

def show(args):
  _get_fds_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.fds_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.show_job("fds", args.fds_config,
          hosts[host_id].ip, job_name, instance_id)

def rolling_update(args):
  if not args.job:
    Log.print_critical("You must specify the job name to do rolling update")

  _get_fds_service_config(args)
  job_name = args.job[0]

  if not args.skip_confirm:
    deploy_utils.confirm_action(args, "rolling_update")

  Log.print_info("Rolling updating %s" % job_name)
  hosts = args.fds_config.jobs[job_name].hosts
  wait_time = 0

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.iterkeys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.confirm_rolling_update(host_id, instance_id, wait_time)
      stop_job(args, hosts[host_id].ip, job_name, instance_id)
      deploy_utils.wait_for_job_stopping("fds",
        args.fds_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)
      deploy_utils.wait_for_job_starting("fds",
        args.fds_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      wait_time = args.time_interval
  Log.print_success("Rolling updating %s success" % job_name)

def run_shell(args):
  Log.print_critical("'shell' command is not supported!")

def pack(args):
  Log.print_critical("'pack' command is not supported!")

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = deploy_hbase
import argparse
import deploy_utils
import deploy_zookeeper
import os
import pwd
import socket
import subprocess
import sys
import tempfile
import urlparse

from log import Log

# regionserver must start before master
ALL_JOBS = ["regionserver", "master"]

SHELL_COMMAND_INFO = {
  "shell": ("org.jruby.Main", "run the HBase shell"),
  "ruby": ("org.jruby.Main", "run the ruby shell"),
  "hbck": ("org.apache.hadoop.hbase.util.HBaseFsck",
      "run the hbase 'fsck' tool"),
  "htck": ("com.xiaomi.infra.hbase.AvailabilityTool",
      "run the hbase table availability check tool"),
  "hlog": ("org.apache.hadoop.hbase.regionserver.wal.HLogPrettyPrinter",
      "write-ahead-log analyzer"),
  "hfile": ("org.apache.hadoop.hbase.io.hfile.HFile", "store file analyzer"),
  "version": ("org.apache.hadoop.hbase.util.VersionInfo", "print the version"),
}

def generate_metrics_config(args, host, job_name, instance_id=-1):
  job = args.hbase_config.jobs[job_name]

  supervisor_client = deploy_utils.get_supervisor_client(host,
      "hbase", args.hbase_config.cluster.name, job_name, instance_id=instance_id)

  ganglia_switch = "# "
  if args.hbase_config.cluster.ganglia_address:
    ganglia_switch = ""
  config_dict = {
      "job_name": job_name,
      "period": 10,
      "data_dir": supervisor_client.get_log_dir(),
      "ganglia_address": args.hbase_config.cluster.ganglia_address,
      "ganglia_switch": ganglia_switch,
  }

  local_path = "%s/hadoop-metrics.properties.tmpl" % deploy_utils.get_template_dir()
  template = deploy_utils.Template(open(local_path, "r").read())
  return template.substitute(config_dict)

def generate_zk_jaas_config(args):
  if not deploy_utils.is_security_enabled(args):
    return ""

  config_dict = args.hbase_config.configuration.generated_files["jaas.conf"]

  for key, value in config_dict.items()[1:]:
    if value != "true" and value != "false" and value.find("\"") == -1:
      config_dict[key] = "\"" + value + "\""

  header_line = config_dict["headerLine"]
  return "Client {\n  %s\n%s;\n};" % (header_line,
    "\n".join(["  %s=%s" % (key, value)
      for (key, value) in config_dict.iteritems() if key != config_dict.keys()[0]]))


def generate_configs(args, host, job_name, instance_id):
  core_site_xml = deploy_utils.generate_site_xml(args,
    args.hbase_config.configuration.generated_files["core-site.xml"])
  hdfs_site_xml = deploy_utils.generate_site_xml(args,
    args.hbase_config.configuration.generated_files["hdfs-site.xml"])
  hbase_site_xml = deploy_utils.generate_site_xml(args,
    args.hbase_config.configuration.generated_files["hbase-site.xml"])
  hadoop_metrics_properties = generate_metrics_config(args, host, job_name, instance_id)
  zk_jaas_conf = generate_zk_jaas_config(args)

  config_files = {
    "core-site.xml": core_site_xml,
    "hdfs-site.xml": hdfs_site_xml,
    "hbase-site.xml": hbase_site_xml,
    "hadoop-metrics.properties": hadoop_metrics_properties,
    "jaas.conf": zk_jaas_conf,
  }
  config_files.update(args.hbase_config.configuration.raw_files)

  return config_files

def generate_run_scripts_params(args, host, job_name, host_id, instance_id):
  job = args.hbase_config.jobs[job_name]

  supervisor_client = deploy_utils.get_supervisor_client(host,
      "hbase", args.hbase_config.cluster.name, job_name, instance_id=instance_id)

  artifact_and_version = "hbase-" + args.hbase_config.cluster.version

  component_dir = "$package_dir/"
  # must include both [dir]/ and [dir]/* as [dir]/* only import all jars under
  # this dir but we also need access the webapps under this dir.
  jar_dirs = "%s/:%s/lib/*:%s/*" % (component_dir, component_dir, component_dir)
  log_level = deploy_utils.get_service_log_level(args, args.hbase_config)

  params = job.get_arguments(args, args.hbase_config.cluster, args.hbase_config.jobs,
    args.hbase_config.arguments_dict, job_name, host_id, instance_id)

  script_dict = {
      "artifact": artifact_and_version,
      "job_name": job_name,
      "jar_dirs": jar_dirs,
      "run_dir": supervisor_client.get_run_dir(),
      "params": params,
  }

  return script_dict

def get_hbase_service_config(args):
  args.hbase_config = deploy_utils.get_service_config(args)
  if not args.hbase_config.cluster.zk_cluster:
    Log.print_critical(
        "hdfs cluster must depends on a zookeeper clusters: %s" %
        args.hbase_config.cluster.name)

def generate_start_script(args, host, job_name, host_id, instance_id):
  script_params = generate_run_scripts_params(args, host, job_name, host_id, instance_id)
  return deploy_utils.create_run_script(
      "%s/start.sh.tmpl" % deploy_utils.get_template_dir(),
      script_params)

def install(args):
  get_hbase_service_config(args)
  deploy_utils.install_service(args, "hbase", args.hbase_config, "hbase")

def cleanup(args):
  get_hbase_service_config(args)

  cleanup_token = deploy_utils.confirm_cleanup(args,
      "hbase", args.hbase_config)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hbase_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.cleanup_job("hbase", args.hbase_config,
          hosts[host_id].ip, job_name, instance_id, cleanup_token)

def bootstrap_job(args, host, job_name, host_id, instance_id, cleanup_token):
  # parse the service_config according to the instance_id
  args.hbase_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  deploy_utils.bootstrap_job(args, "hbase", "hbase",
      args.hbase_config, host, job_name, instance_id, cleanup_token, '0')
  start_job(args, host, job_name, host_id, instance_id)

def bootstrap(args):
  get_hbase_service_config(args)

  cleanup_token = deploy_utils.confirm_bootstrap("hbase", args.hbase_config)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hbase_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        bootstrap_job(args, hosts[host_id].ip, job_name, host_id, instance_id, cleanup_token)

def start_job(args, host, job_name, host_id, instance_id):
  # parse the service_config according to the instance_id
  args.hbase_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  config_files = generate_configs(args, host, job_name, instance_id)
  start_script = generate_start_script(args, host, job_name, host_id, instance_id)
  http_url = deploy_utils.get_http_service_uri(host,
    args.hbase_config.jobs[job_name].base_port, instance_id)
  deploy_utils.start_job(args, "hbase", "hbase", args.hbase_config,
      host, job_name, instance_id, start_script, http_url, **config_files)

def start(args):
  if not args.skip_confirm:
    deploy_utils.confirm_start(args)
  get_hbase_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hbase_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def stop_job(args, host, job_name, instance_id):
  deploy_utils.stop_job("hbase", args.hbase_config,
      host, job_name, instance_id)

def stop(args):
  if not args.skip_confirm:
    deploy_utils.confirm_stop(args)
  get_hbase_service_config(args)

  for job_name in args.job or reversed(ALL_JOBS):
    hosts = args.hbase_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

def restart(args):
  if not args.skip_confirm:
    deploy_utils.confirm_restart(args)
  get_hbase_service_config(args)

  for job_name in args.job or reversed(ALL_JOBS):
    hosts = args.hbase_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hbase_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.wait_for_job_stopping("hbase",
          args.hbase_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def show(args):
  get_hbase_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hbase_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.show_job("hbase", args.hbase_config,
          hosts[host_id].ip, job_name, instance_id)

def run_shell(args):
  get_hbase_service_config(args)

  main_class, options = deploy_utils.parse_shell_command(
      args, SHELL_COMMAND_INFO)
  if not main_class:
    return

  # parse the service_config, suppose the instance_id is -1
  args.hbase_config.parse_generated_config_files(args)
  core_site_dict = args.hbase_config.configuration.generated_files["core-site.xml"]
  hdfs_site_dict = args.hbase_config.configuration.generated_files["hdfs-site.xml"]
  hbase_site_dict = args.hbase_config.configuration.generated_files["hbase-site.xml"]

  hbase_opts = list()
  for key, value in core_site_dict.iteritems():
    hbase_opts.append("-D%s%s=%s" % (deploy_utils.HADOOP_PROPERTY_PREFIX,
          key, value))
  for key, value in hdfs_site_dict.iteritems():
    hbase_opts.append("-D%s%s=%s" % (deploy_utils.HADOOP_PROPERTY_PREFIX,
          key, value))
  for key, value in hbase_site_dict.iteritems():
    hbase_opts.append("-D%s%s=%s" % (deploy_utils.HADOOP_PROPERTY_PREFIX,
          key, value))

  if deploy_utils.is_security_enabled(args):
    hbase_opts.append("-Djava.security.krb5.conf=%s/krb5-hadoop.conf" %
        deploy_utils.get_config_dir())

    (jaas_fd, jaas_file) = tempfile.mkstemp()
    args.zookeeper_config.parse_generated_config_files(args)
    os.write(jaas_fd, deploy_zookeeper.generate_client_jaas_config(args))
    os.close(jaas_fd)
    hbase_opts.append("-Djava.security.auth.login.config=%s" % jaas_file)

  package_root = deploy_utils.get_artifact_package_root(args,
      args.hbase_config.cluster, "hbase")
  class_path = "%s/:%s/lib/*:%s/*" % (package_root, package_root, package_root)

  cmd = ["java", "-cp", class_path] + hbase_opts + [main_class]
  if args.command[0] == "shell":
    cmd += ["-X+O", "%s/bin/hirb.rb" % package_root]
  cmd += options
  p = subprocess.Popen(cmd, stdout=sys.stdout, stderr=sys.stderr)
  return p.wait()

def update_hbase_env_sh(args, artifact, version):
  current_path = os.path.abspath(os.path.dirname(
        os.path.realpath(args.package_root)))
  conf_path = "%s/%s/%s/%s-%s/conf" % (current_path, args.package_root,
    args.cluster, artifact, version)
  hbase_opts = "-Djava.security.auth.login.config=$HBASE_CONF_DIR/jaas.conf "
  hbase_opts += "-Djava.security.krb5.conf=$HBASE_CONF_DIR/krb5.conf"
  deploy_utils.append_to_file("%s/hbase-env.sh" % conf_path,
      'export HBASE_OPTS="$HBASE_OPTS %s"\n' % hbase_opts)

def generate_client_config(args, artifact, version):
  config_path = "%s/%s/%s-%s/conf" % (args.package_root,
      args.cluster, artifact, version)
  master_host = args.hbase_config.jobs["master"].hosts[0].ip
  config_path = "%s/%s/%s-%s/conf" % (args.package_root,
      args.cluster, artifact, version)
  deploy_utils.write_file("%s/hbase-site.xml" % config_path,
      deploy_utils.generate_site_xml(args,
        args.hbase_config.configuration.generated_files["hbase-site.xml"]))
  deploy_utils.write_file("%s/hadoop-metrics.properties" % config_path,
      generate_metrics_config(args, master_host, "master"))
  deploy_utils.write_file("%s/core-site.xml" % config_path,
      deploy_utils.generate_site_xml(args,
        args.hbase_config.configuration.generated_files["core-site.xml"]))
  deploy_utils.write_file("%s/hdfs-site.xml" % config_path,
      deploy_utils.generate_site_xml(args,
        args.hbase_config.configuration.generated_files["hdfs-site.xml"]))
  args.zookeeper_config.parse_generated_config_files(args)
  deploy_utils.write_file("%s/jaas.conf" % config_path,
      deploy_zookeeper.generate_client_jaas_config(args))
  deploy_utils.write_file("%s/krb5.conf" % config_path,
      args.hbase_config.configuration.raw_files["krb5.conf"])
  update_hbase_env_sh(args, artifact, version)

def pack(args):
  get_hbase_service_config(args)
  args.hbase_config.parse_generated_config_files(args)
  version = args.hbase_config.cluster.version
  deploy_utils.make_package_dir(args, "hbase", args.hbase_config.cluster)
  generate_client_config(args, "hbase", version)

  if not args.skip_tarball:
    deploy_utils.pack_package(args, "hbase", version)
  Log.print_success("Pack client utilities for hbase success!\n")

def vacate_region_server(args, ip, port):
  package_root = deploy_utils.get_artifact_package_root(args,
      args.hbase_config.cluster, "hbase")
  Log.print_info("Vacate region server: " + ip);
  try:
    host = socket.gethostbyaddr(ip)[0]
  except:
    host = ip
  args.command = ["ruby", "%s/bin/region_mover.rb" % package_root,
    "unload", "%s:%d" % (host, port)]
  if run_shell(args) != 0:
    Log.print_critical("Unload host %s failed." % host);

def recover_region_server(args, ip, port):
  package_root = deploy_utils.get_artifact_package_root(args,
      args.hbase_config.cluster, "hbase")
  Log.print_info("Recover region server: " + ip);
  try:
    host = socket.gethostbyaddr(ip)[0]
  except:
    host = ip
  args.command = ["ruby", "%s/bin/region_mover.rb" % package_root,
    "load", "%s:%d" % (host, port)]
  if run_shell(args) != 0:
    Log.print_critical("Load host %s failed." % host);

def balance_switch(args, flag):
  fd, filename = tempfile.mkstemp()
  f = os.fdopen(fd, 'w+')
  if flag:
    Log.print_info("balance_switch on for cluster: %s" % args.cluster)
    print >> f, 'balance_switch true'
  else:
    Log.print_info("balance_switch off for cluster: %s" % args.cluster)
    print >> f, 'balance_switch false'
  print >> f, 'exit'
  f.close()
  args.command = ["shell", filename]
  ret = run_shell(args)
  os.remove(filename)
  if ret != 0:
    Log.print_critical("balance_switch off for cluster: %s failed!" %
        args.cluster);

def rolling_update(args):
  if not args.job:
    Log.print_critical("You must specify the job name to do rolling update")

  if not args.skip_confirm:
    deploy_utils.confirm_action(args, "rolling_update")

  get_hbase_service_config(args)
  job_name = args.job[0]

  if job_name != 'regionserver':
    args.vacate_rs = False

  if args.vacate_rs:
    balance_switch(args, False)

  Log.print_info("Rolling updating %s" % job_name)
  hosts = args.hbase_config.jobs[job_name].hosts
  wait_time = 0

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.iterkeys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      if not args.skip_confirm:
        deploy_utils.confirm_rolling_update(host_id, instance_id, wait_time)

      port = deploy_utils.get_base_port(
        args.hbase_config.jobs[job_name].base_port, instance_id)
      if args.vacate_rs:
        vacate_region_server(args, hosts[host_id].ip, port)

      stop_job(args, hosts[host_id].ip, job_name, instance_id)
      deploy_utils.wait_for_job_stopping("hbase",
        args.hbase_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)
      deploy_utils.wait_for_job_starting("hbase",
        args.hbase_config.cluster.name, job_name, hosts[host_id].ip, instance_id)

      if args.vacate_rs:
        recover_region_server(args, hosts[host_id].ip, port)
      wait_time = args.time_interval

  if args.vacate_rs:
    balance_switch(args, True)
  Log.print_success("Rolling updating %s success" % job_name)

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = deploy_hdfs
import deploy_utils
import service_config
import subprocess
import sys
import time

from log import Log

ALL_JOBS = ["journalnode", "zkfc", "namenode", "datanode"]

SHELL_COMMAND_INFO = {
  "dfs": ("org.apache.hadoop.fs.FsShell",
      "run a filesystem command on the file systems supported in Hadoop"),
  "dfsadmin": ("org.apache.hadoop.hdfs.tools.DFSAdmin",
      "run a DFS admin client"),
  "haadmin": ("org.apache.hadoop.hdfs.tools.DFSHAAdmin",
      "run a DFS HA admin client"),
  "fsck": ("org.apache.hadoop.hdfs.tools.DFSck",
      "run a DFS filesystem checking utility"),
  "balancer": ("org.apache.hadoop.hdfs.server.balancer.Balancer",
      "run a cluster balancing utility"),
  "jmxget": ("org.apache.hadoop.hdfs.tools.JMXGet",
      "get JMX exported values from NameNode or DataNode"),
  "oiv": ("org.apache.hadoop.hdfs.tools.offlineImageViewer.OfflineImageViewer",
      "apply the offline fsimage viewer to an fsimage"),
  "oev": ("org.apache.hadoop.hdfs.tools.offlineEditsViewer.OfflineEditsViewer",
      "apply the offline edits viewer to an edits file"),
  "fetchdt": ("org.apache.hadoop.hdfs.tools.DelegationTokenFetcher",
      "fetch a delegation token from the NameNode"),
  "getconf": ("org.apache.hadoop.hdfs.tools.GetConf",
      "get config values from configuration"),
  "groups": ("org.apache.hadoop.hdfs.tools.GetGroups",
      "get the groups which users belong to"),
}

def generate_metrics_config(args, host, job_name, instance_id=-1):
  job = args.hdfs_config.jobs[job_name]

  supervisor_client = deploy_utils.get_supervisor_client(host,
      "hdfs", args.hdfs_config.cluster.name, job_name, instance_id=instance_id)

  ganglia_switch = "# "
  if args.hdfs_config.cluster.ganglia_address:
    ganglia_switch = ""
  config_dict = {
      "job_name": job_name,
      "period": 10,
      "data_dir": supervisor_client.get_log_dir(),
      "ganglia_address": args.hdfs_config.cluster.ganglia_address,
      "ganglia_switch": ganglia_switch,
  }

  local_path = "%s/hadoop-metrics2.properties.tmpl" % deploy_utils.get_template_dir()
  template = deploy_utils.Template(open(local_path, "r").read())
  return template.substitute(config_dict)

def generate_configs(args, host, job_name, instance_id):
  core_site_xml = deploy_utils.generate_site_xml(args,
    args.hdfs_config.configuration.generated_files["core-site.xml"])
  hdfs_site_xml = deploy_utils.generate_site_xml(args,
    args.hdfs_config.configuration.generated_files["hdfs-site.xml"])
  hadoop_metrics2_properties = generate_metrics_config(args, host, job_name, instance_id)

  config_files = {
    "core-site.xml": core_site_xml,
    "hdfs-site.xml": hdfs_site_xml,
    "hadoop-metrics2.properties": hadoop_metrics2_properties,
  }
  config_files.update(args.hdfs_config.configuration.raw_files)

  return config_files

def generate_run_scripts_params(args, host, job_name, host_id, instance_id):
  job = args.hdfs_config.jobs[job_name]

  supervisor_client = deploy_utils.get_supervisor_client(host,
      "hdfs", args.hdfs_config.cluster.name, job_name, instance_id=instance_id)

  artifact_and_version = "hadoop-" + args.hdfs_config.cluster.version

  jar_dirs = ""
  # must include both [dir]/ and [dir]/* as [dir]/* only import all jars under
  # this dir but we also need access the webapps under this dir.
  for component in ["common", "hdfs"]:
    if jar_dirs: jar_dirs += ":"
    component_dir = ("$package_dir/share/hadoop/%s" % component)
    jar_dirs += "%s/:%s/lib/*:%s/*" % (
        component_dir, component_dir, component_dir)
  log_level = deploy_utils.get_service_log_level(args, args.hdfs_config)

  params = job.get_arguments(args, args.hdfs_config.cluster, args.hdfs_config.jobs,
    args.hdfs_config.arguments_dict, job_name, host_id, instance_id)

  script_dict = {
      "artifact": artifact_and_version,
      "job_name": job_name,
      "jar_dirs": jar_dirs,
      "run_dir": supervisor_client.get_run_dir(),
      "params": params,
  }

  return script_dict

def get_hdfs_service_config(args):
  args.hdfs_config = deploy_utils.get_service_config(args)
  if not args.hdfs_config.cluster.zk_cluster:
    Log.print_critical(
        "hdfs cluster must depends on a zookeeper clusters: %s" %
        args.hdfs_config.cluster.name)

  namenode_hosts = args.hdfs_config.jobs["namenode"].hosts
  args.hdfs_config.jobs["zkfc"].hosts = namenode_hosts.copy()
  args.skip_gen_config_files = False

def generate_bootstrap_script(args, host, job_name, host_id, instance_id, active):
  option = str()
  script_params = generate_run_scripts_params(args, host, job_name, host_id, instance_id)
  script_params['ha_status'] = 'standby'
  if job_name == "zkfc":
    if active:
      option = "-formatZK"
      script_params['ha_status'] = 'active'
  elif job_name == "namenode":
    if active:
      option = "-format -nonInteractive"
    else:
      option = "-bootstrapStandby -skipSharedEditsCheck -nonInteractive"
  script_params['params'] += " %s" % option

  return deploy_utils.create_run_script(
      '%s/bootstrap_hdfs.sh.tmpl' % deploy_utils.get_template_dir(),
      script_params)

def generate_cleanup_script(args, host, job_name, host_id, instance_id, active):
  script_params = generate_run_scripts_params(args, host, job_name, host_id, instance_id)
  script_params['params'] += " -clearZK"
  if active:
    script_params['ha_status'] = 'active'
  else:
    script_params['ha_status'] = 'standby'
  return deploy_utils.create_run_script(
      '%s/cleanup_hdfs.sh.tmpl' % deploy_utils.get_template_dir(),
      script_params)

def generate_start_script(args, host, job_name, host_id, instance_id):
  script_params = generate_run_scripts_params(args, host, job_name, host_id, instance_id)
  return deploy_utils.create_run_script(
      '%s/start.sh.tmpl' % deploy_utils.get_template_dir(),
      script_params)

def check_journalnode_all_started(args):
  job = args.hdfs_config.jobs["journalnode"]
  hosts = job.hosts
  for host_id in hosts.iterkeys():
    for instance_id in range(hosts[host_id].instance_num):
      if not deploy_utils.check_service(hosts[host_id].ip,
        service_config.get_base_port(job.base_port, instance_id)):
        return False
  return True

def get_data_dir_indexes(args, job_name, host, instance_id):
  if job_name != "datanode":
    return "0"
  else:
    supervisor_client = deploy_utils.get_supervisor_client(host,
        "hdfs", args.hdfs_config.cluster.name, job_name, instance_id=instance_id)
    data_dirs = supervisor_client.get_available_data_dirs()
    return ",".join([str(i) for i in range(len(data_dirs))])

def install(args):
  get_hdfs_service_config(args)
  deploy_utils.install_service(args, "hdfs", args.hdfs_config, "hadoop")

def cleanup_job(args, host, job_name, host_id, instance_id, active, cleanup_token):
  cleanup_script = str()
  if job_name == "zkfc":
    cleanup_script = generate_cleanup_script(args, host, job_name, host_id, instance_id, active)
  deploy_utils.cleanup_job("hdfs", args.hdfs_config,
      host, job_name, instance_id, cleanup_token, cleanup_script)

def cleanup(args):
  get_hdfs_service_config(args)

  cleanup_token = deploy_utils.confirm_cleanup(args,
      "hdfs", args.hdfs_config)

  first = True
  for job_name in args.job or ALL_JOBS:
    hosts = args.hdfs_config.jobs[job_name].hosts

    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        cleanup_job(args, hosts[host_id].ip, job_name, host_id, instance_id, first, cleanup_token)
        if job_name == "zkfc":
          first = False

def bootstrap_job(args, host, job_name, host_id, instance_id, active, cleanup_token):
  # parse the service_config according to the instance_id
  args.hdfs_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  data_dir_indexes = get_data_dir_indexes(args, job_name, host, instance_id)
  config_files = generate_configs(args, host, job_name, instance_id)
  if job_name == "namenode" or job_name == "zkfc":
    bootstrap_script = generate_bootstrap_script(args, host, job_name, host_id, instance_id, active)
    deploy_utils.bootstrap_job(args, "hadoop", "hdfs", args.hdfs_config,
        host, job_name, instance_id, cleanup_token, data_dir_indexes, bootstrap_script,
        **config_files)
  else:
    deploy_utils.bootstrap_job(args, "hadoop", "hdfs", args.hdfs_config,
        host, job_name, instance_id, cleanup_token, data_dir_indexes, '', **config_files)
  # start job after bootstrapping
  args.skip_gen_config_files = True
  start_job(args, host, job_name, host_id, instance_id)

def bootstrap(args):
  get_hdfs_service_config(args)

  cleanup_token = deploy_utils.confirm_bootstrap("hdfs", args.hdfs_config)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hdfs_config.jobs[job_name].hosts
    first = True
    if job_name == "namenode":
      while not check_journalnode_all_started(args):
        Log.print_warning("Wait for journalnode starting")
        time.sleep(2)
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        if job_name == "namenode" and not first:
          while not deploy_utils.check_service(hosts[0].ip,
              args.hdfs_config.jobs["namenode"].base_port):
            Log.print_warning("Wait for active namenode starting")
            time.sleep(2)

        bootstrap_job(args, hosts[host_id].ip, job_name, host_id, instance_id, first, cleanup_token)
        first = False

def start_job(args, host, job_name, host_id, instance_id):
  # parse the service_config according to the instance_id
  args.hdfs_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  start_script = generate_start_script(args, host, job_name, host_id, instance_id)
  http_url = deploy_utils.get_http_service_uri(host,
    args.hdfs_config.jobs[job_name].base_port, instance_id)
  config_files = dict()
  if not args.skip_gen_config_files:
    config_files = generate_configs(args, host, job_name, instance_id)
  deploy_utils.start_job(args, "hadoop", "hdfs", args.hdfs_config,
      host, job_name, instance_id, start_script, http_url, **config_files)

def start(args):
  if not args.skip_confirm:
    deploy_utils.confirm_start(args)
  get_hdfs_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hdfs_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def stop_job(args, host, job_name, instance_id):
  deploy_utils.stop_job("hdfs", args.hdfs_config,
      host, job_name, instance_id)

def stop(args):
  if not args.skip_confirm:
    deploy_utils.confirm_stop(args)
  get_hdfs_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hdfs_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

def restart(args):
  if not args.skip_confirm:
    deploy_utils.confirm_restart(args)
  get_hdfs_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hdfs_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hdfs_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.wait_for_job_stopping("hdfs",
          args.hdfs_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def show(args):
  get_hdfs_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.hdfs_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.show_job("hdfs", args.hdfs_config, hosts[host_id].ip,
          job_name, instance_id)

def run_shell(args):
  get_hdfs_service_config(args)

  main_class, options = deploy_utils.parse_shell_command(
      args, SHELL_COMMAND_INFO)
  if not main_class:
    return
  # parse the service_config, suppose the instance_id is -1
  args.hdfs_config.parse_generated_config_files(args)
  core_site_dict = args.hdfs_config.configuration.generated_files["core-site.xml"]
  hdfs_site_dict = args.hdfs_config.configuration.generated_files["hdfs-site.xml"]

  hadoop_opts = list()
  for key, value in core_site_dict.iteritems():
    hadoop_opts.append("-D%s%s=%s" % (deploy_utils.HADOOP_PROPERTY_PREFIX,
          key, value))
  for key, value in hdfs_site_dict.iteritems():
    hadoop_opts.append("-D%s%s=%s" % (deploy_utils.HADOOP_PROPERTY_PREFIX,
          key, value))

  package_root = deploy_utils.get_artifact_package_root(args,
    args.hdfs_config.cluster, "hadoop")
  lib_root = "%s/share/hadoop" % package_root
  class_path = "%s/etc/hadoop" % package_root
  for component in ["common", "hdfs"]:
    component_dir = "%s/%s" % (lib_root, component)
    class_path += ":%s/:%s/*:%s/lib/*" % (component_dir,
        component_dir, component_dir)

  if deploy_utils.is_security_enabled(args):
    boot_class_path = "%s/common/lib/hadoop-security-%s.jar" % (lib_root,
        args.hdfs_config.cluster.version)
    hadoop_opts.append("-Xbootclasspath/p:%s" % boot_class_path)
    hadoop_opts.append("-Dkerberos.instance=hadoop")
    hadoop_opts.append(
        "-Djava.security.krb5.conf=%s/krb5-hadoop.conf" %
        deploy_utils.get_config_dir())

  cmd = (["java", "-cp", class_path] + hadoop_opts +
      [main_class] + options)
  p = subprocess.Popen(cmd, stdout=sys.stdout, stderr=sys.stderr)
  p.wait()

def generate_client_config(args, artifact, version):
  config_path = "%s/%s/%s-%s/etc/hadoop" % (args.package_root,
      args.cluster, artifact, version)
  deploy_utils.write_file("%s/core-site.xml" % config_path,
      deploy_utils.generate_site_xml(args,
        args.hdfs_config.configuration.generated_files["core-site.xml"]))
  deploy_utils.write_file("%s/hdfs-site.xml" % config_path,
      deploy_utils.generate_site_xml(args,
        args.hdfs_config.configuration.generated_files["hdfs-site.xml"]))
  deploy_utils.write_file("%s/hadoop-metrics2.properties" % config_path,
      generate_metrics_config(args, args.hdfs_config.jobs["namenode"].hosts[0].ip,
        "namenode"))
  deploy_utils.write_file("%s/krb5.conf" % config_path,
      args.hdfs_config.configuration.raw_files["krb5.conf"])
  update_hadoop_env_sh(args, artifact, version, "HADOOP_OPTS")

def update_hadoop_env_sh(args, artifact, version, opts_name):
  config_path = "%s/%s/%s-%s/etc/hadoop" % (args.package_root,
      args.cluster, artifact, version)
  hadoop_opts = "-Djava.security.krb5.conf=$HADOOP_CONF_DIR/krb5.conf"
  deploy_utils.append_to_file("%s/hadoop-env.sh" % config_path,
      'export %s="$%s %s"\n' % (opts_name, opts_name, hadoop_opts))

def pack(args):
  get_hdfs_service_config(args)
  args.hdfs_config.parse_generated_config_files(args)

  version = args.hdfs_config.cluster.version
  deploy_utils.make_package_dir(args, "hadoop", args.hdfs_config.cluster)
  generate_client_config(args, "hadoop", version)

  if not args.skip_tarball:
    deploy_utils.pack_package(args, "hadoop", args.hdfs_config.cluster.version)
  Log.print_success("Pack client utilities for hadoop success!\n")

def rolling_update(args):
  if not args.job:
    Log.print_critical("You must specify the job name to do rolling update")

  get_hdfs_service_config(args)
  job_name = args.job[0]

  if not args.skip_confirm:
    deploy_utils.confirm_action(args, "rolling_update")

  Log.print_info("Rolling updating %s" % job_name)
  hosts = args.hdfs_config.jobs[job_name].hosts
  wait_time = 0

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.iterkeys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.confirm_rolling_update(host_id, instance_id, wait_time)
      stop_job(args, hosts[host_id].ip, job_name, instance_id)
      deploy_utils.wait_for_job_stopping("hdfs",
        args.hdfs_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)
      deploy_utils.wait_for_job_starting("hdfs",
        args.hdfs_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      wait_time = args.time_interval
  Log.print_success("Rolling updating %s success" % job_name)

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = deploy_impala
import argparse
import os
import service_config
import subprocess
import sys
import urlparse

import deploy_utils

from log import Log

ALL_JOBS = ["statestored", "impalad"]

def get_impala_service_config(args):
  args.impala_config = deploy_utils.get_service_config(args)

def generate_configs(args):
  core_site_xml = deploy_utils.generate_site_xml(args,
    args.impala_config.configuration.generated_files["core-site.xml"])
  hdfs_site_xml = deploy_utils.generate_site_xml(args,
    args.impala_config.configuration.generated_files["hdfs-site.xml"])
  hive_site_xml = deploy_utils.generate_site_xml(args,
    args.impala_config.configuration.generated_files["hive-site.xml"])
  hbase_site_xml = deploy_utils.generate_site_xml(args,
    args.impala_config.configuration.generated_files["hbase-site.xml"])

  config_files = {
    "core-site.xml": core_site_xml,
    "hdfs-site.xml": hdfs_site_xml,
    "hive-site.xml": hive_site_xml,
    "hbase-site.xml": hbase_site_xml,
  }
  config_files.update(args.impala_config.configuration.raw_files)

  return config_files

def generate_run_scripts_params(args, host, job_name, host_id, instance_id):
  supervisor_client = deploy_utils.get_supervisor_client(host,
      "impala", args.impala_config.cluster.name, job_name, instance_id=instance_id)
  job = args.impala_config.jobs[job_name]

  artifact_and_version = "impala-" + args.impala_config.cluster.version
  log_level = deploy_utils.get_service_log_level(args, args.impala_config)

  params = job.get_arguments(args, args.impala_config.cluster, args.impala_config.jobs,
    args.impala_config.arguments_dict, job_name, host_id, instance_id)

  script_dict = {
    "artifact": artifact_and_version,
    "job_name": job_name,
    "run_dir": supervisor_client.get_run_dir(),
    "ticket_cache": "$run_dir/impala.tc",
    "log_level": log_level,
    "params": params,
  }

  return script_dict

def generate_start_script(args, host, job_name, host_id, instance_id):
  script_params = generate_run_scripts_params(args, host, job_name, host_id, instance_id)
  return deploy_utils.create_run_script(
      "%s/impala/start.sh.tmpl" % deploy_utils.get_template_dir(),
      script_params)

def install(args):
  get_impala_service_config(args)
  deploy_utils.install_service(args, "impala", args.impala_config, "impala")

def cleanup(args):
  get_impala_service_config(args)

  cleanup_token = deploy_utils.confirm_cleanup(args,
      "impala", args.impala_config)
  for job_name in args.job or ALL_JOBS:
    hosts = args.impala_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.cleanup_job("impala", args.impala_config,
          hosts[host_id].ip, job_name, instance_id, cleanup_token)

def bootstrap_job(args, host, job_name, host_id, instance_id, cleanup_token):
  # parse the service_config according to the instance_id
  args.impala_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  deploy_utils.bootstrap_job(args, "impala", "impala",
      args.impala_config, host, job_name, instance_id, cleanup_token, '0')
  start_job(args, host, job_name, host_id, instance_id)

def bootstrap(args):
  get_impala_service_config(args)
  cleanup_token = deploy_utils.confirm_bootstrap("impala", args.impala_config)

  for job_name in args.job or ALL_JOBS:
    hosts = args.impala_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        bootstrap_job(args, hosts[host_id].ip, job_name, host_id, instance_id, cleanup_token)

def start_job(args, host, job_name, host_id, instance_id):
  # parse the service_config according to the instance_id
  args.impala_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  config_files = generate_configs(args)
  start_script = generate_start_script(args, host, job_name, host_id, instance_id)
  http_url = deploy_utils.get_http_service_uri(host,
    args.impala_config.jobs[job_name].base_port, instance_id)
  deploy_utils.start_job(args, "impala", "impala", args.impala_config,
      host, job_name, instance_id, start_script, http_url, **config_files)

def start(args):
  if not args.skip_confirm:
    deploy_utils.confirm_start(args)
  get_impala_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.impala_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def stop_job(args, host, job_name, instance_id):
  deploy_utils.stop_job("impala", args.impala_config, host, job_name, instance_id)

def stop(args):
  if not args.skip_confirm:
    deploy_utils.confirm_stop(args)
  get_impala_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.impala_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

def restart(args):
  if not args.skip_confirm:
    deploy_utils.confirm_restart(args)
  get_impala_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.impala_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

  for job_name in args.job or ALL_JOBS:
    hosts = args.impala_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.wait_for_job_stopping("impala",
          args.impala_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def show(args):
  get_impala_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.impala_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.show_job("impala", args.impala_config,
          hosts[host_id].ip, job_name, instance_id)

def run_shell(args):
  get_impala_service_config(args)

  os.environ['IMPALA_HOME'] = deploy_utils.get_root_dir("impala")
  shell_script = "%s/bin/impala-shell.sh" % deploy_utils.get_root_dir("impala")

  if not args.command:
    args.command.append("-h")

  cmd = ["bash", shell_script] + args.command
  p = subprocess.Popen(cmd, stdout=sys.stdout, stderr=sys.stderr)
  p.wait()

def pack(args):
  get_impala_service_config(args)
  version = args.impala_config.cluster.version
  deploy_utils.make_package_dir(args, "impala-shell", args.impala_config.cluster)

  if not args.skip_tarball:
    deploy_utils.pack_package(args, "impala-shell",
        args.impala_config.cluster.version)
  Log.print_success("Pack client utilities for hadoop success!\n")

def rolling_update(args):
  if not args.job:
    Log.print_critical("You must specify the job name to do rolling update")

  get_impala_service_config(args)
  job_name = args.job[0]

  if not args.skip_confirm:
    deploy_utils.confirm_action(args, "rolling_update")

  Log.print_info("Rolling updating %s" % job_name)
  hosts = args.impala_config.jobs[job_name].hosts
  wait_time = 0

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.iterkeys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.confirm_rolling_update(host_id, instance_id, wait_time)
      stop_job(args, hosts[host_id].ip, job_name, instance_id)
      deploy_utils.wait_for_job_stopping("impala",
        args.impala_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)
      deploy_utils.wait_for_job_starting("impala",
        args.impala_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      wait_time = args.time_interval
  Log.print_success("Rolling updating %s success" % job_name)

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = deploy_kafka
#!/usr/bin/env python

import argparse
import os
import service_config
import subprocess
import sys
import urlparse

import deploy_utils

from log import Log

ALL_JOBS = ["kafka", "kafkascribe"]

def _get_kafka_service_config(args):
  args.kafka_config = deploy_utils.get_service_config(args)

def generate_configs(args, job_name, host_id, instance_id):
  kafka_cfg_dict = args.kafka_config.configuration.generated_files["kafka.cfg"]
  hosts = args.kafka_config.jobs[job_name].hosts
  kafka_cfg_dict["broker.id"] = deploy_utils.get_task_id(hosts, host_id, instance_id)
  kafka_cfg = deploy_utils.generate_properties_file(args, kafka_cfg_dict)

  kafka_scribe_cfg_dict = args.kafka_config.configuration.generated_files["kafka-scribe.cfg"]
  kafka_job = args.kafka_config.jobs["kafka"]
  kafka_scribe_cfg_dict["metadata.broker.list"] = ",".join(
      service_config.get_job_host_port_list(kafka_job))
  kafka_scribe_cfg = deploy_utils.generate_properties_file(args, kafka_scribe_cfg_dict)

  config_files = {
    "kafka.cfg": kafka_cfg,
    "kafka-scribe.cfg": kafka_scribe_cfg,
  }
  config_files.update(args.kafka_config.configuration.raw_files)

  return config_files

def generate_run_scripts_params(args, host, job_name, host_id, instance_id):
  job = args.kafka_config.jobs[job_name]

  supervisor_client = deploy_utils.get_supervisor_client(host,
      "kafka", args.kafka_config.cluster.name, job_name, instance_id=instance_id)

  artifact_and_version = "kafka-" + args.kafka_config.cluster.version

  jar_dirs = "$package_dir/*"
  log_level = deploy_utils.get_service_log_level(args, args.kafka_config)

  params = job.get_arguments(args, args.kafka_config.cluster, args.kafka_config.jobs,
    args.kafka_config.arguments_dict, job_name, host_id, instance_id)

  script_dict = {
      "artifact": artifact_and_version,
      "job_name": job_name,
      "jar_dirs": jar_dirs,
      "run_dir": supervisor_client.get_run_dir(),
      "params": params,
  }

  return script_dict

def generate_start_script(args, host, job_name, host_id, instance_id):
  script_params = generate_run_scripts_params(args, host, job_name, host_id, instance_id)
  return deploy_utils.create_run_script(
      "%s/start.sh.tmpl" % deploy_utils.get_template_dir(),
      script_params)

def install(args):
  _get_kafka_service_config(args)
  deploy_utils.install_service(args, "kafka", args.kafka_config, "kafka")

def cleanup(args):
  _get_kafka_service_config(args)

  cleanup_token = deploy_utils.confirm_cleanup(args,
      "kafka", args.kafka_config)
  for job_name in args.job or ALL_JOBS:
    hosts = args.kafka_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.cleanup_job("kafka", args.kafka_config,
          hosts[host_id].ip, job_name, instance_id, cleanup_token)

def bootstrap_job(args, host, job_name, host_id, instance_id, cleanup_token):
  # parse the service_config according to the instance_id
  args.kafka_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  deploy_utils.bootstrap_job(args, "kafka", "kafka",
      args.kafka_config, host, job_name, instance_id, cleanup_token, '0')
  start_job(args, host, job_name, host_id, instance_id)

def bootstrap(args):
  _get_kafka_service_config(args)
  cleanup_token = deploy_utils.confirm_bootstrap("kafka", args.kafka_config)

  for job_name in args.job or ALL_JOBS:
    hosts = args.kafka_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        bootstrap_job(args, hosts[host_id].ip, job_name, host_id, instance_id, cleanup_token)

def start_job(args, host, job_name, host_id, instance_id):
  # parse the service_config according to the instance_id
  args.kafka_config.parse_generated_config_files(args, job_name, host_id, instance_id)

  config_files = generate_configs(args, job_name, host_id, instance_id)
  start_script = generate_start_script(args, host, job_name, host_id, instance_id)
  http_url = deploy_utils.get_http_service_uri(host,
    args.kafka_config.jobs[job_name].base_port, instance_id)
  deploy_utils.start_job(args, "kafka", "kafka", args.kafka_config,
      host, job_name, instance_id, start_script, http_url, **config_files)

def start(args):
  if not args.skip_confirm:
    deploy_utils.confirm_start(args)
  _get_kafka_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.kafka_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def stop_job(args, host, job_name, instance_id):
  deploy_utils.stop_job("kafka", args.kafka_config, host, job_name, instance_id)

def stop(args):
  if not args.skip_confirm:
    deploy_utils.confirm_stop(args)
  _get_kafka_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.kafka_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

def restart(args):
  if not args.skip_confirm:
    deploy_utils.confirm_restart(args)
  _get_kafka_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.kafka_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

  for job_name in args.job or ALL_JOBS:
    hosts = args.kafka_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.wait_for_job_stopping("kafka",
          args.kafka_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def show(args):
  _get_kafka_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.kafka_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.show_job("kafka", args.kafka_config,
          hosts[host_id].ip, job_name, instance_id)

def run_shell(args):
  Log.print_critical("'shell' command is not supported!")

def pack(args):
  Log.print_critical("'pack' command is not supported!")

def rolling_update(args):
  if not args.job:
    Log.print_critical("You must specify the job name to do rolling update")

  _get_kafka_service_config(args)
  job_name = args.job[0]

  if not args.skip_confirm:
    deploy_utils.confirm_action(args, "rolling_update")

  Log.print_info("Rolling updating %s" % job_name)
  hosts = args.kafka_config.jobs[job_name].hosts
  wait_time = 0

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.iterkeys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.confirm_rolling_update(host_id, instance_id, wait_time)
      stop_job(args, hosts[host_id].ip, job_name, instance_id)
      deploy_utils.wait_for_job_stopping("kafka",
        args.kafka_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)
      deploy_utils.wait_for_job_starting("kafka",
        args.kafka_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      wait_time = args.time_interval
  Log.print_success("Rolling updating %s success" % job_name)

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = deploy_keytab
import argparse
import csv
import os
import pexpect
import sys

HADOOP_CONF_PATH = '/etc/hadoop/conf'

def parse_command_line():
  parser = argparse.ArgumentParser(
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      description='Keytab deploy tool')

  parser.add_argument('cluster_type', type=str,
      choices=['srv', 'prc', 'tst'], help='The cluster type')

  parser.add_argument('--host_file', type=str, default='hosts',
      help='The host file list in csv format')

  parser.add_argument('--keytab_dir', type=str, default='keytab',
      help='The keytab file directory')

  parser.add_argument('--prod_user', type=str, default='work',
      help='The production enviroment user')

  parser.add_argument('--root_password', type=str,
      help='The root password of the production enviroment')

  args = parser.parse_args()
  return args

def check_args(args):
  if not os.path.exists(args.host_file):
    print 'Invalid host_file: %s' % args.host_file
    sys.exit(-4)

  if not os.path.exists(args.keytab_dir):
    print 'Invalid keytab_dir: %s' % args.keytab_dir
    sys.exit(-5)

def parse_host_file(host_file):
  file = open(host_file, 'r')
  csv_reader = csv.reader(file, delimiter=' ', skipinitialspace=True)

  host_list = list()
  for line in csv_reader:
    if line[0].lstrip().startswith('#'):
      continue
    host_list.append(line)
  file.close()
  return host_list

def scp(host, user, passwd, local_file, remote_file):
  child = pexpect.spawn('scp %s %s@%s:%s' % (local_file,
        user, host, remote_file))
  print child.args

  ret = child.expect(['yes/no.*', 'password.*', pexpect.EOF])
  if ret == 0:
    child.sendline('yes')
    child.expect('password.*', timeout=10)
    child.sendline(passwd)
  elif ret == 1:
    child.sendline(passwd)
  else:
    print 'Error occured when execute expect()'
    sys.exit(-2)

  return child.expect([pexpect.EOF, pexpect.TIMEOUT])

def remote_exec(host, user, passwd, cmd):
  child = pexpect.spawn('ssh %s@%s "%s"' % (user, host, cmd))
  print child.args

  ret = child.expect(['yes/no.*', 'password.*', pexpect.EOF], timeout=30)
  if ret == 0:
    child.sendline('yes')
    child.expect('password.*', timeout=10)
    child.sendline(passwd)
  elif ret == 1:
    child.sendline(passwd)
  else:
    print 'Error occured when execute expect()'
    sys.exit(-3)

  return child.expect([pexpect.EOF, pexpect.TIMEOUT])

def deploy(args, host):
  # mkdir -p HADOOP_CONF_PATH
  remote_exec(host, 'root', args.root_password,
      'mkdir -p %s' % HADOOP_CONF_PATH)

  keytabs = [
    'hdfs_%s.keytab' % args.cluster_type,
    'hbase_%s.keytab' % args.cluster_type,
    'yarn_%s.keytab' % args.cluster_type,
    'zookeeper.keytab',
    'impala.keytab',
  ]

  for keytab in keytabs:
    # scp keytab to HADOOP_CONF_PATH
    scp(host, 'root', args.root_password,
        '%s/%s' % (args.keytab_dir, keytab), HADOOP_CONF_PATH)

    # chown of keytab to prod_user:prod_user
    remote_exec(host, 'root', args.root_password,
        '"chown %s:%s %s/%s"' % (args.prod_user, args.prod_user,
          HADOOP_CONF_PATH, keytab))

    # chmod of keytab to 400
    remote_exec(host, 'root', args.root_password,
        '"chmod 400 %s/%s"' % (HADOOP_CONF_PATH, keytab))

  print '\033[0;32mDeploy keytab on %s successfully\033[0m' % host

def main():
  args = parse_command_line()
  check_args(args)

  host_list = parse_host_file(args.host_file)
  for host_info in host_list:
    deploy(args, host_info[0])

if __name__ == '__main__':
  main()

########NEW FILE########
__FILENAME__ = deploy_storm
import deploy_utils

from log import Log

ALL_JOBS = ["nimbus", "supervisor", "ui", "logviewer"]

def generate_run_scripts_params(args, host, job_name, host_id, instance_id):
  job = args.storm_config.jobs[job_name]

  supervisor_client = deploy_utils.get_supervisor_client(host,
      "storm", args.storm_config.cluster.name, job_name, instance_id=instance_id)

  artifact_and_version = "storm-" + args.storm_config.cluster.version

  jar_dirs = "$package_dir/*"
  log_level = deploy_utils.get_service_log_level(args, args.storm_config)

  params = job.get_arguments(args, args.storm_config.cluster, args.storm_config.jobs,
    args.storm_config.arguments_dict, job_name, host_id, instance_id)

  service_env = "export SUPERVISOR_LOG_DIR=%s" % deploy_utils.get_supervisor_client(host,
    "storm", args.storm_config.cluster.name, 'supervisor', instance_id=instance_id).get_log_dir()

  script_dict = {
      "artifact": artifact_and_version,
      "job_name": job_name,
      "jar_dirs": jar_dirs,
      "run_dir": supervisor_client.get_run_dir(),
      "service_env": service_env,
      "params": params,
  }

  return script_dict

def generate_start_script(args, host, job_name, host_id, instance_id):
  script_params = generate_run_scripts_params(args, host, job_name, host_id, instance_id)
  return deploy_utils.create_run_script(
      "%s/start.sh.tmpl" % deploy_utils.get_template_dir(), script_params)

def generate_configs(args, host, job_name, instance_id):
  storm_yaml = deploy_utils.generate_yaml_file(
    args.storm_config.configuration.generated_files["storm.yaml"])
  config_files = {
    "storm.yaml": storm_yaml,
  }
  config_files.update(args.storm_config.configuration.raw_files)

  return config_files

def _get_storm_service_config(args):
  args.storm_config = deploy_utils.get_service_config(args)
  if not args.storm_config.cluster.zk_cluster:
    Log.print_critical(
        "storm cluster must depends on a zookeeper clusters: %s" %
        args.storm_config.cluster.name)

  nimbus_hosts = args.storm_config.jobs["nimbus"].hosts
  supervisor_hosts = args.storm_config.jobs["supervisor"].hosts
  args.storm_config.jobs["ui"].hosts = nimbus_hosts.copy()
  args.storm_config.jobs["logviewer"].hosts = supervisor_hosts.copy()

def install(args):
  _get_storm_service_config(args)
  deploy_utils.install_service(args, "storm", args.storm_config, "storm")

def bootstrap_job(args, host, job_name, host_id, instance_id, cleanup_token):
  # parse the service_config according to the instance_id
  args.storm_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  deploy_utils.bootstrap_job(args, "storm", "storm",
      args.storm_config, host, job_name, instance_id, cleanup_token, '0')
  start_job(args, host, job_name, host_id, instance_id)

def bootstrap(args):
  _get_storm_service_config(args)
  cleanup_token = deploy_utils.confirm_bootstrap("storm", args.storm_config)

  for job_name in args.job or ALL_JOBS:
    hosts = args.storm_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        bootstrap_job(args, hosts[host_id].ip, job_name, host_id, instance_id, cleanup_token)

def start_job(args, host, job_name, host_id, instance_id):
  # parse the service_config according to the instance_id
  args.storm_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  config_files = generate_configs(args, host, job_name, instance_id)
  start_script = generate_start_script(args, host, job_name, host_id, instance_id)
  http_url = deploy_utils.get_http_service_uri(host,
    args.storm_config.jobs[job_name].base_port, instance_id)
  deploy_utils.start_job(args, "storm", "storm", args.storm_config,
      host, job_name, instance_id, start_script, http_url, **config_files)

def start(args):
  if not args.skip_confirm:
    deploy_utils.confirm_start(args)
  _get_storm_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.storm_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def stop_job(args, host, job_name, instance_id):
  deploy_utils.stop_job("storm", args.storm_config,
      host, job_name, instance_id)

def stop(args):
  if not args.skip_confirm:
    deploy_utils.confirm_stop(args)
  _get_storm_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.storm_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

def restart(args):
  if not args.skip_confirm:
    deploy_utils.confirm_restart(args)
  _get_storm_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.storm_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

  for job_name in args.job or ALL_JOBS:
    hosts = args.storm_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.wait_for_job_stopping("storm",
          args.storm_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def cleanup(args):
  _get_storm_service_config(args)

  cleanup_token = deploy_utils.confirm_cleanup(args,
      "storm", args.storm_config)
  for job_name in args.job or ALL_JOBS:
    hosts = args.storm_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.cleanup_job("storm", args.storm_config,
          hosts[host_id].ip, job_name, instance_id, cleanup_token)

def show(args):
  _get_storm_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.storm_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.show_job("storm", args.storm_config,
          hosts[host_id].ip, job_name, instance_id)

def rolling_update(args):
  if not args.job:
    Log.print_critical("You must specify the job name to do rolling update")

  _get_storm_service_config(args)
  job_name = args.job[0]

  if not args.skip_confirm:
    deploy_utils.confirm_action(args, "rolling_update")

  Log.print_info("Rolling updating %s" % job_name)
  hosts = args.storm_config.jobs[job_name].hosts
  wait_time = 0

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.iterkeys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.confirm_rolling_update(host_id, instance_id, wait_time)
      stop_job(args, hosts[host_id].ip, job_name, instance_id)
      deploy_utils.wait_for_job_stopping("storm",
        args.storm_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)
      deploy_utils.wait_for_job_starting("storm",
        args.storm_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      wait_time = args.time_interval
  Log.print_success("Rolling updating %s success" % job_name)

def run_shell(args):
  Log.print_critical("'shell' command is not supported!")

def pack(args):
  Log.print_critical("'pack' command is not supported!")

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = deploy_utils
import argparse
import cStringIO
import deploy_config
import getpass
import hashlib
import os
import pprint
import re
import service_config
import socket
import string
import subprocess
import sys
import telnetlib
import time
import urllib2
import uuid

from log import Log
from service_config import ServiceConfig
from supervisor_client import SupervisorClient
from tank_client import TankClient

HOST_TASK_REGEX = re.compile('(?P<host>\d+)(\.(?P<task>\d+))?$')

SUPERVISOR_SUCCESS = "OK"

STOPPED_STATUS = ["STOPPED", "BACKOFF", "EXITED", "FATAL"]

HADOOP_PROPERTY_PREFIX = "hadoop.property."
HADOOP_CONF_PATH = "/etc/hadoop/conf"
LATEST_PACKAGE_INFO_URI = "get_latest_package_info"
DOWNLOAD_PACKAGE_URI = "packages"

FAKE_SVN_VERSION = "12345"

class Template(string.Template):
  # the orginal delimiter '$' is also commonly used by shell script, so
  # overwrite to '%' here.
  delimiter = '%'


def get_deploy_config():
  return deploy_config.get_deploy_config()

def get_real_instance_id(instance_id):
  return service_config.get_real_instance_id(instance_id)

def get_base_port(base_port, instance_id):
  return service_config.get_base_port(base_port, instance_id)

def get_http_service_uri(host, base_port, instance_id):
  return 'http://%s:%d' % (host,
    get_base_port(base_port, instance_id) + 1)

def get_host_id(hosts, host_ip):
  for id, host in hosts.iteritems():
    if host_ip == host.ip:
      return id
  Log.print_critical("Invalid host ip: %s, please check your config." % host_ip)

def get_task_id(hosts, host_id, instance_id):
  instance_id = 0 if (instance_id == -1) else instance_id
  task_id = 0
  for id, host in hosts.iteritems():
    if host_id == id:
      task_id += instance_id
      break
    else:
      task_id += host.instance_num
  return task_id

def get_service_log_level(args, service_config):
  if args.log_level:
    return args.log_level
  else:
    return service_config.cluster.log_level

def get_local_package_path_general(path, artifact, version):
  '''
  Get the local tarball path of the package of specified artifact and version

  @param  path      the base path of the tarball
  @param  artifact  the artifact of the package
  @param  version   the version of the package
  @return string    the full path of the tarball

  Note: This method is for internal use, users shouldn't call it directly.
    Users who want to obtain the local package path should call
    get_local_package_path().
  '''
  return ("%s/%s-%s.tar.gz" % (path, artifact, version))

def get_local_package_path(artifact, version):
  '''
  Get the local tarball path of the package of specified artifact and version

  @param  artifact  the artifact of the package
  @param  version   the version of the package
  @return string    the full path of the tarball
  '''
  if artifact == "zookeeper":
    package_path = get_local_package_path_general(
        get_deploy_config().get_zookeeper_package_dir(),
        artifact, version)
  elif artifact == "hadoop":
    package_path = get_local_package_path_general(
        get_deploy_config().get_hadoop_package_dir(),
        artifact, version)
  elif artifact == "hbase":
    package_path = get_local_package_path_general(
        get_deploy_config().get_hbase_package_dir(),
        artifact, version)
  elif artifact == "impala-shell" or artifact == "impala":
    package_path = get_local_package_path_general(
        get_deploy_config().get_imapala_package_dir(),
        artifact, version)
  elif artifact == "kafka":
    package_path = get_local_package_path_general(
        get_deploy_config().get_kafka_package_dir(),
        artifact, version)
  elif artifact == "storm":
    package_path = get_local_package_path_general(
        get_deploy_config().get_storm_package_dir(),
        artifact, version)
  elif artifact == "galaxy":
    package_path = get_local_package_path_general(
        get_deploy_config().get_galaxy_package_dir(),
        artifact, version)
  elif artifact == 'chronos':
    package_path = get_local_package_path_general(
        get_deploy_config().get_chronos_package_dir(),
        artifact, version)
  else:
    Log.print_critical("Unknow artifact: %s" % artifact)
  return package_path

def get_revision_number(cmd, output_prefix, work_space_dir):
  env = os.environ
  # Enforce English locale.
  env["LC_ALL"] = "C"
  current_work_dir = os.getcwd()
  os.chdir(work_space_dir)
  content = subprocess.check_output(cmd, stderr=subprocess.STDOUT, env=env)
  os.chdir(current_work_dir)
  for line in content.splitlines():
    if line.startswith(output_prefix):
      return line[len(output_prefix):]

def generate_package_revision(root):
  '''
  Get the revision of the package. Currently, svn revision and git commit are
  supported. If the package directory is neither a svn working directory nor
  a git working directory, a fake revision will be returned.

  @param  root   the local package root directory
  @return string the revision of the package
  '''
  if os.path.islink(root):
    real_path = os.readlink(root)
    if not real_path.startswith('/'):
      abs_path = "%s/%s" % (os.path.dirname(root), real_path)
    else:
      abs_path = real_path
  else:
    abs_path = root

  try:
    try:
      cmd = ["svn", "info"]
      revision_prefix = "Revision: "
      return "r%s" % get_revision_number(cmd, revision_prefix, abs_path)
    except:
      cmd = ["git", "show"]
      commit_prefix = "commit "
      return get_revision_number(cmd, commit_prefix, abs_path)
  except:
    # We cannot get the version No., just return a fake one
    return "r%s" % FAKE_SVN_VERSION

def generate_checksum(path):
  '''
  Generate the SHA-1 digest of specified file.

  @param  path   the path of the file
  @return string the SHA-1 digest
  '''
  fd = open(path, "r")
  sha1 = hashlib.sha1()
  while True:
    buffer = fd.read(4096)
    if not buffer: break
    sha1.update(buffer)
  fd.close()
  return sha1.hexdigest()

def upload_package(args, artifact, version):
  '''
  Upload the specified package to the package server(Tank). Note that
  if the file with the same checksum is already uploaded, this uploading
  will be skipped.

  @param  args     the command line arguments object parsed by artparse.py
  @param  artifact the artifact of the package
  @param  version  the version of the package
  @return dict     the package information return by the package server
  '''
  package_path = get_local_package_path(artifact, version)
  Log.print_info("Uploading pacakge: %s" % package_path)

  revision = generate_package_revision(get_root_dir(args.service))
  Log.print_success("Revision is: %s" % revision)

  Log.print_info("Generating checksum of package: %s" % package_path)
  checksum = generate_checksum(package_path)
  Log.print_success("Checksum is: %s" % checksum)

  tank_client = get_tank_client()
  package_info = tank_client.check_package(artifact, checksum)

  if not package_info:
    if 200 == tank_client.upload(package_path, artifact, revision):
      Log.print_success("Upload package %s success" % package_path)
      package_info = tank_client.check_package(artifact, checksum)
      return eval(package_info)
  else:
    Log.print_warning("Package %s has already uploaded, skip uploading" %
        package_path)
    return eval(package_info)
  return None

def generate_site_xml(args, template_dict):
  '''
  Generate the *-site.xml file according to the given properties dict.

  @param  args          the argument object parsed by argparse
  @param  template_dict the properties dict
  @return string        the generated file content
  '''
  template_path = "%s/site.xml.tmpl" % get_template_dir()

  template = Template(open(template_path).read())
  config_value = ""
  keys = template_dict.keys()
  keys.sort()
  for key in keys:
    config_value += """
  <property>
    <name>%s</name>
    <value>%s</value>
  </property>
""" % (key, template_dict[key])
  return template.substitute({"config_value": config_value})

def generate_properties_file(args, template_dict):
  '''
  Generate the *.properties file according to the given properties dict.

  @param  args          the argument object parsed by argparse
  @param  template_dict the properties dict
  @return string        the generated file content
  '''
  template_path = "%s/properties.tmpl" % get_template_dir()

  template = Template(open(template_path).read())
  return template.substitute(
      {"config_value":
          "\n".join(["%s=%s" % (k, v) for k, v in template_dict.iteritems()])})

def generate_yaml_file(yaml_dict):
  '''
  Generate the yaml format config file according to the given yaml dict.

  @param  yaml_dict     the yaml dict
  @return string        the generated file content
  '''
  yaml_format_string = ""
  for key, value in yaml_dict.iteritems():
    yaml_format_string += key
    if value.find(',') != -1:
      yaml_format_string += ":\n"
      for item in value.split(','):
        yaml_format_string += "  - %s\n" % item
    else:
      yaml_format_string += ": %s\n" % value

  return yaml_format_string

def create_run_script(template_path, template_dict):
  '''
  Generate the run script of given script template and variables dict.

  @param  template_path  the script template path
  @param  template_dict  the variables dict
  @return string         the generated file content
  '''
  template = Template(open(template_path).read())
  content = template.safe_substitute(template_dict)
  return content

def get_template_dir():
  '''
  Get the config templates directory.
  '''
  return '%s/template' % get_deploy_config().get_config_dir()

def get_config_dir():
  '''
  Get the service config directory.
  '''
  return get_deploy_config().get_config_dir()

def get_root_dir(service):
  '''
  Get the local root directory of specified service.

  @param  service  the service name
  @return string   the local root directory of the service
  '''
  if service == "hdfs" or service == "yarn":
    return get_deploy_config().get_hadoop_root()
  if service == "hbase":
    return get_deploy_config().get_hbase_root()
  if service == "zookeeper":
    return get_deploy_config().get_zookeeper_root()
  if service == "impala":
    return get_deploy_config().get_impala_root()
  if service == "kafka":
    return get_deploy_config().get_kafka_root()
  if service == "storm":
    return get_deploy_config().get_storm_root()
  if service == "fds":
    return get_deploy_config().get_galaxy_root()
  if service == "chronos":
    return get_deploy_config().get_chronos_root()
  Log.print_critical("Unknow service: %s" % service)

def get_supervisor_client(host, service, cluster, job, instance_id=-1):
  '''
  A factory method to construct a supervisor client object.

  @param  host        the remote server's host
  @param  service     the service name
  @param  cluster     the cluster name
  @param  job         the job name
  @param  instance_id the instance id
  @return object      the supervisor client object
  '''
  return service_config.get_supervisor_client(host, service, cluster, job, instance_id)

def get_tank_client():
  '''
  A factory method to construct a tank(package server) client object.
  '''
  deploy_config = get_deploy_config()
  tank_config = deploy_config.get_tank_config()

  return TankClient(tank_config.get('server_host'),
      tank_config.get('server_port'))

def get_service_config(args):
  '''
  Get service config, without any dependencies.

  @param  args       the command line arguments object parsed by argparse
  '''
  service_config.get_short_user_name(args)
  if not getattr(args, args.service + "_config", None):
    setattr(args, args.service+"_config", ServiceConfig(args))
  return getattr(args, args.service+"_config")

def generate_service_token(service, cluster):
  '''
  Generate a token used to bootstrap and cleanup.

  @param  service the service name
  @param  cluster the cluster name
  @return string  the generated token
  '''
  return str(uuid.uuid3(uuid.NAMESPACE_DNS,'%s-%s' % (
          service, cluster)))

def check_input(input, yes='y'):
  '''
  Check if the input string is yes or not.
  '''
  return input.strip().lower() == yes.lower()


def check_admin_priviledge(args):
  '''
  Check if the current user is in the administrators list or not. Note that
  this will be checked only when security is enabled.
  '''
  status, short_user_name = service_config.get_short_user_name_full()
  args.short_user_name = short_user_name

  if is_security_enabled(args):
    if status:
      admin_list = get_deploy_config().get_admin_list()
      if short_user_name not in admin_list:
        Log.print_critical("User %s is not an authorized administrator, "
          "this operation can't be processed" % short_user_name)
    else:
      Log.print_critical('You must kinit your kerberos principal first')

def is_security_enabled(args):
  '''
  Determine if security is enabled or not.
  '''
  get_service_config(args)

  if args.service == "zookeeper":
    return len(args.zookeeper_config.configuration.generated_files["jaas-server.conf"]) != 0
  elif args.service == "hdfs":
    core_site_dict = args.hdfs_config.configuration.generated_files["core-site.xml"]
    return (core_site_dict["hadoop.security.authentication"] == "kerberos") and (
             core_site_dict["hadoop.security.authorization"] == "true")
  elif args.service == "yarn":
    core_site_dict = args.yarn_config.configuration.generated_files["core-site.xml"]
    return (core_site_dict["hadoop.security.authentication"] == "kerberos") and (
             core_site_dict["hadoop.security.authorization"] == "true")
  elif args.service == "hbase":
    hbase_site_dict = args.hbase_config.configuration.generated_files["hbase-site.xml"]
    return (hbase_site_dict["hbase.security.authentication"] == "kerberos") and (
             hbase_site_dict["hbase.security.authorization"] == "true")
  elif args.service == "impala":
    core_site_dict = args.impala_config.configuration.generated_files["core-site.xml"]
    return (core_site_dict["hadoop.security.authentication"] == "kerberos") and (
             core_site_dict["hadoop.security.authorization"] == "true")
  elif args.service == "fds":
    core_site_dict = args.fds_config.configuration.generated_files["core-site.xml"]
    return (core_site_dict["hadoop.security.authentication"] == "kerberos") and (
             core_site_dict["hadoop.security.authorization"] == "true")
  elif args.service == "chronos":
    chronos_dict = args.chronos_config.configuration.generated_files["chronos.cfg"]
    return (chronos_dict["zkSecure"] == "true")
  else:
    return False

def confirm_bootstrap(service, service_config):
  '''
  Let the users confirm bootstrap interactively. Users will be asked to
  set a password, or a random password will be given. The password is
  the verification token when users want to do cleanup.
  '''
  Log.print_warning("You should set a bootstrap password, " \
      "it will be requried when you do cleanup")
  password = str()
  input = raw_input("Set a password manually? (y/n) ")
  if check_input(input):
    input = getpass.getpass("Please input your password: ")
    if len(input.strip()) >= 6:
      password = input.strip()
    else:
      Log.print_critical("The length of the password is at least 6")
  else:
    Log.print_info("A random password will be generated")
    password = generate_service_token(service, service_config.cluster.name)

  Log.print_warning("Your password is: %s, you should store this " \
      "in a safe place, because this is the verification code used " \
      "to do cleanup" % password)
  return password

def confirm_action(args, action):
  '''
  Let the users confirm the specify action interactively.
  '''
  Log.print_warning("You will %s the cluster \033[31;1m%s\033[0;33m, "
      "do you really want to do this?" % (action, args.cluster))
  token = generate_random_confirm_token()
  input = raw_input("Please input \033[31;1m%s\033[0m to confirm: " % token)
  if check_input(input, token):
    Log.print_info("Begin to %s the cluster" % action)
  else:
    Log.print_critical("%s canceled" % action.capitalize())

def confirm_cleanup(args, service, service_config):
  '''
  Let the user confirm cleanup interactively. Users will be asked to input
  the password set when the service is bootstrapped.
  '''
  confirm_action(args, 'cleanup')

  input = getpass.getpass("Please input your installation password: ")
  if len(input.strip()) >= 6:
    return input.strip()
  else:
    Log.print_critical("The length of the password is at least 6")

def confirm_stop(args):
  '''
  Let the user confirm the stop action interactively.
  '''
  confirm_action(args, 'stop')

def confirm_start(args):
  '''
  Let the user confirm the start action interactively.
  '''
  confirm_action(args, 'start')

def confirm_restart(args):
  '''
  Let the user confirm the restart action interactively.
  '''
  confirm_action(args, 'restart')

def install_service(args, service, service_config, artifact):
  '''
  Install the specified service. Here installation means uploading the
  service package to the package server(Tank).

  @param args           the command line arguments object
  @param service        the service name
  @param service_config the service config object
  @param artifact       the artifact name
  '''
  Log.print_info("Installing %s to package server" % artifact)
  package_info = upload_package(args, artifact, service_config.cluster.version)
  if package_info:
    Log.print_success("Install %s to package server success" % artifact)
    pprint.pprint(package_info)
  else:
    Log.print_critical("Install %s to package server fail" % artifact)

def cleanup_job(service, service_config, host, job_name,
    instance_id, cleanup_token, cleanup_script=""):
  '''
  Clean up a task of the specified service and job. Note that cleanup
  requires that the task must be stopped, so users should stop the task
  before cleanup.

  @param service         the service name
  @param service_config  the service config object
  @param host            the host of the task
  @param job_name        the job name
  @param instance_id     the instance id
  @param cleanup_token   the token used to verify cleanup
  @param cleanup_script  the user supplied cleanup script
  @param artifact        the artifact name
  '''
  real_instance_id = get_real_instance_id(instance_id)
  host_id = get_host_id(service_config.jobs[job_name].hosts, host)
  task_id = get_task_id(service_config.jobs[job_name].hosts, host_id, instance_id)
  Log.print_info("Cleaning up task %d of %s on %s(%d)" % (
    task_id, job_name, host, real_instance_id))
  supervisor_client = get_supervisor_client(host, service,
      service_config.cluster.name, job_name, instance_id)
  message = supervisor_client.cleanup(cleanup_token, cleanup_script)
  if SUPERVISOR_SUCCESS == message:
    Log.print_success("Cleanup task %d of %s on %s(%d) success" % (
      task_id, job_name, host, real_instance_id))
  else:
    Log.print_error("Cleanup task %d of %s on %s(%d) fail: %s" % (
      task_id, job_name, host, real_instance_id, message))

def bootstrap_job(args, artifact, service, service_config, host, job_name, instance_id,
    cleanup_token, data_dir_indexes='0', bootstrap_script='', **config_files):
  '''
  Bootstrap a task of the specified service and job. Note that before
  bootstrapping users should ensure that the data and log directories at
  the server side are empty.

  @param args             the command line arguments object
  @param artifact         the artifact name
  @param service          the service name
  @param service_config   the service config object
  @param host             the host of the task
  @param job_name         the job name
  @param instance_id      the instance id
  @param cleanup_token    the token used to verify cleanup
  @param data_dir_indexes the data directory indexes
  @param bootstrap_script the user supplied bootstrap script
  @param config_files     the config files dict
  '''
  real_instance_id = get_real_instance_id(instance_id)
  host_id = get_host_id(service_config.jobs[job_name].hosts, host)
  task_id = get_task_id(service_config.jobs[job_name].hosts, host_id, instance_id)
  Log.print_info("Bootstrapping task %d of %s on %s(%d)" % (
    task_id, job_name, host, real_instance_id))
  supervisor_client = get_supervisor_client(host, service,
      service_config.cluster.name, job_name, instance_id)

  try:
    if (service_config.cluster.package_name and service_config.cluster.revision
        and service_config.cluster.timestamp):
      message = supervisor_client.bootstrap(artifact,
          package_name=service_config.cluster.package_name,
          revision=service_config.cluster.revision,
          timestamp=service_config.cluster.timestamp,
          cleanup_token=cleanup_token,
          bootstrap_script=bootstrap_script,
          data_dir_indexes=data_dir_indexes,
          **config_files)
    elif args.update_package:
      message = supervisor_client.bootstrap(artifact, force_update=True,
          cleanup_token=cleanup_token, bootstrap_script=bootstrap_script,
          data_dir_indexes=data_dir_indexes, **config_files)
    else:
      message = supervisor_client.bootstrap(artifact,
          package_name=args.package_name, revision=args.revision,
          timestamp=args.timestamp, cleanup_token=cleanup_token,
          bootstrap_script=bootstrap_script, data_dir_indexes=data_dir_indexes,
          **config_files)
    if SUPERVISOR_SUCCESS == message:
      Log.print_success("Bootstrap task %d of %s on %s(%d) success" % (
        task_id, job_name, host, real_instance_id))
    else:
      Log.print_critical("Bootstrap task %d of %s on %s(%d) fail: %s" % (
        task_id, job_name, host, real_instance_id, message))

  except BaseException, e:
    message = str(e)
    Log.print_error("Bootstrap task %d of %s on %s(%d) fail: %s" % (
      task_id, job_name, host, real_instance_id, message))

def start_job(args, artifact, service, service_config, host, job_name,
    instance_id, start_script, http_url, **config_files):
  '''
  Start the task of specified service and job.

  @param args            the command line arguments object
  @param artifact        the artifact name
  @param service         the service name
  @param service_config  the service config object
  @param host            the host of the task
  @param job_name        the job name
  @param instance_id     the instance id
  @param start_script    the user supplied start script
  @param http_url        the task's http entry url
  @param config_files    the config files dict
  '''
  real_instance_id = get_real_instance_id(instance_id)
  host_id = get_host_id(service_config.jobs[job_name].hosts, host)
  task_id = get_task_id(service_config.jobs[job_name].hosts, host_id, instance_id)
  Log.print_info("Starting task %d of %s on %s(%d)" % (
    task_id, job_name, host, real_instance_id))
  supervisor_client = get_supervisor_client(host, service,
      service_config.cluster.name, job_name, instance_id)

  if not args.update_config:
    config_files = dict()
    start_script = ""

  if (service_config.cluster.package_name and service_config.cluster.revision
      and service_config.cluster.timestamp):
    message = supervisor_client.start(artifact,
        package_name=service_config.cluster.package_name,
        revision=service_config.cluster.revision,
        timestamp=service_config.cluster.timestamp,
        http_url=http_url, start_script=start_script,
        **config_files)
  elif args.update_package:
    message = supervisor_client.start(artifact, force_update=True,
        http_url=http_url, start_script=start_script, **config_files)
  else:
    message = supervisor_client.start(artifact, package_name=args.package_name,
        revision=args.revision, timestamp=args.timestamp, http_url=http_url,
        start_script=start_script, **config_files)
  if SUPERVISOR_SUCCESS == message:
    Log.print_success("Start task %d of %s on %s(%d) success" % (
      task_id, job_name, host, real_instance_id))
  else:
    Log.print_error("Start task %d of %s on %s(%d) fail: %s" % (
      task_id, job_name, host, real_instance_id, message))

def stop_job(service, service_config, host, job_name, instance_id):
  '''
  Stop the task of specified service and job.

  @param service         the service name
  @param service_config  the service config object
  @param host            the host of the task
  @param job_name        the job name
  @param instance_id     the instance id
  '''
  real_instance_id = get_real_instance_id(instance_id)
  host_id = get_host_id(service_config.jobs[job_name].hosts, host)
  task_id = get_task_id(service_config.jobs[job_name].hosts, host_id, instance_id)
  Log.print_info("Stopping task %d of %s on %s(%d)" % (
    task_id, job_name, host, real_instance_id))
  supervisor_client = get_supervisor_client(host, service,
      service_config.cluster.name, job_name, instance_id)
  message = supervisor_client.stop()
  if SUPERVISOR_SUCCESS == message:
    Log.print_success("Stop task %d of %s on %s(%d) success" % (
      task_id, job_name, host, real_instance_id))
  else:
    Log.print_error("Stop task %d of %s on %s(%d) fail: %s" % (
      task_id, job_name, host, real_instance_id, message))

def show_job(service, service_config, host, job_name, instance_id):
  '''
  Show the state the task of specified service and job.

  @param service         the service name
  @param service_config  the service config object
  @param host            the host of the task
  @param job_name        the job name
  @param instance_id     the instance id
  '''
  real_instance_id = get_real_instance_id(instance_id)
  host_id = get_host_id(service_config.jobs[job_name].hosts, host)
  task_id = get_task_id(service_config.jobs[job_name].hosts, host_id, instance_id)
  Log.print_info("Showing task %d of %s on %s(%d)" % (
    task_id, job_name, host, real_instance_id))
  supervisor_client = get_supervisor_client(host, service,
      service_config.cluster.name, job_name, instance_id)
  state = supervisor_client.show()
  if state == 'RUNNING':
    Log.print_success("Task %d of %s on %s(%d) is %s" % (
      task_id, job_name, host, real_instance_id, state))
  else:
    Log.print_error("Task %d of %s on %s(%d) is %s" % (
      task_id, job_name, host, real_instance_id, state))

def check_service(host, port):
  '''
  Check whether the given host:port is accessable or not.
  '''
  t = telnetlib.Telnet()
  try:
    t.open(host, port)
  except:
    return False
  t.close()
  return True

def check_job_stopped(service, cluster, job, host, instance_id):
  '''
  Check whether a specified task is already stopped or not.
  '''
  supervisor_client = get_supervisor_client(host,
      service, cluster, job, instance_id)
  status = supervisor_client.show()
  return status in STOPPED_STATUS

def wait_for_job_stopping(service, cluster, job, host, instance_id):
  '''
  Wait for a specified job to be stopped.
  '''
  while not check_job_stopped(service, cluster, job, host, instance_id):
    Log.print_warning("Wait for instance %d of %s on %s stopping" % (
      get_real_instance_id(instance_id), job, host))
    time.sleep(2)

def check_job_started(service, cluster, job, host, instance_id):
  '''
  Check whether a specified task is already started or not.
  '''
  supervisor_client = get_supervisor_client(host,
      service, cluster, job, instance_id)
  status = supervisor_client.show()
  return status == 'RUNNING'

def wait_for_job_starting(service, cluster, job, host, instance_id):
  '''
  Wait for a specified job to be started.
  '''
  # Wait 10 seconds to let supervisord start the task
  time.sleep(10)
  if not check_job_started(service, cluster, job, host, instance_id):
    Log.print_critical('Instance %d of %s on %s start failed' % (
      get_real_instance_id(instance_id), job, host))


def get_package_uri(artifact, package_name, revision, timestamp):
  tank_config = get_deploy_config().get_tank_config()

  return 'http://%s:%s/%s/%s/%s-%s/%s' % (tank_config['server_host'],
      tank_config['server_port'], DOWNLOAD_PACKAGE_URI, artifact,
      revision, timestamp, package_name)

def get_query_latest_package_info_uri(artifact, package_name):
  tank_config = get_deploy_config().get_tank_config()

  return 'http://%s:%s/%s/?artifact=%s&package_name=%s' % (
    tank_config['server_host'], tank_config['server_port'],
    LATEST_PACKAGE_INFO_URI, artifact, package_name)

def get_latest_package_info(artifact, package_name):
  uri = get_query_latest_package_info_uri(artifact, package_name)
  info_fp = urllib2.urlopen(uri, None, 30)
  info = info_fp.read()

  if info and info.startswith('{'):
    info_dict = eval(info)
    info_fp.close()
    return info_dict
  else:
    info_fp.close()
    return None

def check_cluster_version(cluster, specified_package_name):
  if specified_package_name.find(cluster.version) == -1:
    Log.print_critical("The version: %s is inconsistent with " \
      "the package_name: %s" % (cluster.version, specified_package_name))

def get_package_info(args, artifact, cluster):
  if (cluster.package_name and cluster.revision and cluster.timestamp):
    check_cluster_version(cluster, cluster.package_name)
    package_name = cluster.package_name
    revision = cluster.revision
    timestamp = cluster.timestamp
  elif (args.package_name and args.revision and args.timestamp):
    check_cluster_version(cluster, args.package_name)
    package_name = args.package_name
    revision = args.revision
    timestamp = args.timestamp
  else:
    package_info = get_latest_package_info(artifact,
      artifact + "-" + cluster.version + ".tar.gz")
    if package_info:
      package_name = package_info.get('package_name')
      revision = package_info.get('revision')
      timestamp = package_info.get('timestamp')
    else:
      Log.print_critical("No package found on package server of %s" %
        artifact + "-" + cluster.version + ".tar.gz")

  return {
    "package_name": package_name,
    "revision": revision,
    "timestamp": timestamp,
  }

def print_progress_bar(message):
  sys.stdout.write(message)
  sys.stdout.flush()

def download_package(download_uri, dest_file):
  try:
    data_file = urllib2.urlopen(download_uri, None, 30)
    data_size = int(dict(data_file.headers).get('content-length'))
  except urllib2.HTTPError, e:
    Log.print_critical("Not found package for uri: %s" % download_uri)

  if not os.path.exists(os.path.dirname(dest_file)):
    os.makedirs(os.path.dirname(dest_file))
  fp = open(dest_file, 'ab')

  read_unit_size = 1048576 # read at most 1M every time
  read_size = 0
  bar_length = 70  # print 70 '='
  speed_max_length = 11 # for example, 1023.99KB/s

  Log.print_info("Package downloading...\nLength: %s bytes\nSaving to %s" % (
    data_size, dest_file))
  start_time = time.time()
  while read_size < data_size:
    read_data = data_file.read(read_unit_size)
    fp.write(read_data)
    read_size += len(read_data)
    progress_bar = '=' * int(float(read_size) / data_size * bar_length)

    download_time = int(time.time() - start_time) + 1
    download_percent = int(float(read_size) / data_size * 100)
    blank_bar = " " * (bar_length - len(progress_bar))
    read_size_str = format(read_size, ',')

    download_speed = float(read_size)/download_time
    if download_speed >= 1024 * 1024:
      download_speed = format(download_speed / (1024 * 1024), '.2f') + 'M' # MB/s
    elif download_speed >= 1024:
      download_speed = format(download_speed / 1024, '.2f') + 'K'          # KB/s
    else:
      download_speed = format(download_speed, '.2f')                       # B/s

    speed_blanks = ' ' * (speed_max_length - len(download_speed) - len('B/s'))
    print_progress_bar(str(download_percent) + "% [" + progress_bar +
      ">" + blank_bar + "] " + read_size_str + "  " + speed_blanks +
      download_speed + "B/s\r")

  print_progress_bar("\n")
  Log.print_info("Download complete.")
  fp.close()
  data_file.close()

def make_package_download_dir(args, artifact, cluster):
  package_info = get_package_info(args, artifact, cluster)
  package_download_path = "%s/%s/%s-%s/%s" % (
    get_deploy_config().get_package_download_root(), artifact,
    package_info['revision'], package_info['timestamp'], package_info['package_name'])

  # check if the tarball is already downloaded, if not, download it
  if not os.path.exists(package_download_path):
    package_uri = get_package_uri(artifact, package_info['package_name'],
      package_info['revision'], package_info['timestamp'])
    download_package(package_uri, package_download_path)

  # unpack the tarball
  package_download_dir = package_download_path[
    0: len(package_download_path) - len('.tar.gz')]
  if not os.path.exists(package_download_dir):
    cmd = ['tar', '-zxf', package_download_path, '-C', os.path.dirname(package_download_dir)]
    subprocess.check_call(cmd)

  return package_download_dir

def get_artifact_package_root(args, cluster, artifact):
  '''
  Get the artifact package root directory
  '''
  if artifact == 'hbase':
    package_path = "hbase-%s/hbase-%s" % (cluster.version, cluster.version)
  else:
    package_path = "%s-%s" % (artifact, cluster.version)

  artifact_package_root = "%s/%s" % (
    eval("get_deploy_config().get_" + artifact + "_package_dir()"), package_path)

  if os.path.exists(artifact_package_root):
    return artifact_package_root
  else:
    return make_package_download_dir(args, artifact, cluster)

def parse_shell_command(args, command_dict):
  '''
  Parse the shell command and its options from the command line arguements.
  '''
  if len(args.command) == 0 or args.command[0] == 'help':
    print_shell_help_info(command_dict)
    return (None, None)

  command = args.command[0]
  command_info = command_dict.get(command)
  if not command_info:
    Log.print_warning(
        "Can't find main class of '%s', suppose it's a class name" % command)
    main_class = command
  else:
    main_class = command_info[0]
  return (main_class, args.command[1:])

def print_shell_help_info(command_dict):
  '''
  Print the help information for the specified shell commands.
  '''
  help_info="help      \tprint this help information"
  for key, value in command_dict.iteritems():
    help_info += "\n%-10s\t%s" % (key, value[1])
  print help_info

def write_file(file_name, content):
  '''
  Write the specified content to the specified file.
  '''
  file = open(file_name, "wb")
  file.write(content)
  file.close()

def make_package_dir(args, artifact, cluster):
  '''
  Make the local package directories.
  '''
  cmd = ["mkdir", "-p", "%s/%s/" % (args.package_root, args.cluster)]
  subprocess.check_call(cmd)

  package_path = get_local_package_path(artifact, cluster.version)
  if not os.path.exists(package_path):
    package_path = make_package_download_dir(args, artifact, cluster) + ".tar.gz"

  cmd = ["tar", "-zxf", package_path, "-C", "%s/%s/" % (
      args.package_root, args.cluster)]
  subprocess.check_call(cmd)

def pack_package(args, artifact, version):
  '''
  Pack the package with generated configuration files into a tarball.
  '''
  cmd = ["tar", "-C", "%s/%s" % (args.package_root, args.cluster),
    "-zchf", "%s/%s/%s-%s-%d.tar.gz" % (args.package_root,
        args.cluster, artifact, version, time.time()),
    "./%s-%s" % (artifact, version)]
  subprocess.check_call(cmd)

def append_to_file(file, content):
  '''
  Append specified content to the specified file.
  '''
  fp = open(file, "a")
  fp.write(content)
  fp.close()

def confirm_rolling_update(host_id, instance_id, wait_time):
  '''
  Let the user confirm the rolling update action interactively.
  '''
  while True:
    if wait_time > 0:
      Log.print_info("Waiting %d seconds before updating next task..."
          % wait_time)
      time.sleep(wait_time)

    while True:
      input = raw_input("Ready to update instance %d on host %d? (y/n) " % (
        get_real_instance_id(instance_id), host_id))
      if check_input(input):
        return True
  return False

def get_zk_address(cluster):
  '''
  Get the zookeeper name address according to the cluster name.
  '''
  return "bj%s-zk-%s.hadoop.srv" % (cluster[0:2], cluster[2:])

def generate_random_confirm_token():
  '''
  Generate a random 8 bytes token used to do confirm
  '''
  return str(uuid.uuid4())[0:8]

def add_task_to_map(task_map, host_id, instance_id):
  if host_id in task_map.keys():
    if instance_id not in task_map[host_id]:
      task_map[host_id].append(instance_id)
  else:
    task_map[host_id] = [instance_id]

def parse_task(args, hosts):
  task_map = {}
  for id in args.task:
    task_id = int(id)
    host_id, instance_id = service_config.parse_task_number(task_id, hosts)
    add_task_to_map(task_map, host_id, instance_id)
  return task_map

def get_task_by_hostname(hosts, hostnames):
  task_map = {}
  for hostname in hostnames:
    host_ip = socket.gethostbyname(hostname)
    found_task = False
    for host_id, host in hosts.iteritems():
      if host.ip == host_ip:
        for instance_id in range(host.instance_num):
          add_task_to_map(task_map, host_id, instance_id)
        found_task = True
        break
    # raise a ValueError if can't find valid task
    if found_task == False:
      raise ValueError(hostname + ' is not a valid host of cluster, please check your config')
  return task_map

def parse_args_host_and_task(args, hosts):
  # the format of task_map is:
  # { host_1 : [instance_1,instance_2...], host_2 : [instance_1,instance_2...] }
  task_map = {}
  if args.host is not None:
    task_map.update(get_task_by_hostname(hosts, args.host))
  elif args.task is not None:
    task_map.update(parse_task(args, hosts))
  return task_map

def is_multiple_instances(host_id, hosts):
  # return False if deploy only one instance on the host
  return hosts[host_id].instance_num > 1

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = deploy_yarn
import argparse
import deploy_hdfs
import deploy_utils
import subprocess
import sys
import urlparse

from log import Log

ALL_JOBS = ["resourcemanager", "nodemanager", "historyserver", "proxyserver"]

SHELL_COMMAND_INFO = {
  "rmadmin": ("org.apache.hadoop.yarn.server.resourcemanager.tools.RMAdmin",
      "admin tools"),
  "version": ("org.apache.hadoop.util.VersionInfo", "print the version"),
  "jar": ("org.apache.hadoop.util.RunJar", "run a jar file"),
  "logs": ("org.apache.hadoop.yarn.logaggregation.LogDumper",
      "dump container logs"),
  "daemonlog": ("org.apache.hadoop.log.LogLevel",
      "get/set the log level for each daemon"),
}

def get_yarn_service_config(args):
  args.yarn_config = deploy_utils.get_service_config(args)
  if not args.yarn_config.cluster.zk_cluster:
    Log.print_critical(
        "yarn cluster must depends on a zookeeper clusters: %s" %
        args.yarn_config.cluster.name)

def generate_metrics_config(args, host, job_name, instance_id=-1):
  job = args.yarn_config.jobs[job_name]
  supervisor_client = deploy_utils.get_supervisor_client(host, "yarn",
      args.yarn_config.cluster.name, job_name, instance_id=instance_id)

  ganglia_switch = "# "
  if args.yarn_config.cluster.ganglia_address:
    ganglia_switch = ""
  config_dict = {
      "job_name": job_name,
      "period": 10,
      "data_dir": supervisor_client.get_log_dir(),
      "ganglia_address": args.yarn_config.cluster.ganglia_address,
      "ganglia_switch": ganglia_switch,
  }

  local_path = "%s/hadoop-metrics2.properties.tmpl" % deploy_utils.get_template_dir()
  template = deploy_utils.Template(open(local_path, "r").read())
  return template.substitute(config_dict)

def generate_configs(args, host, job_name, instance_id):
  core_site_xml = deploy_utils.generate_site_xml(args,
    args.yarn_config.configuration.generated_files["core-site.xml"])
  hdfs_site_xml = deploy_utils.generate_site_xml(args,
    args.yarn_config.configuration.generated_files["hdfs-site.xml"])
  mapred_site_xml = deploy_utils.generate_site_xml(args,
    args.yarn_config.configuration.generated_files["mapred-site.xml"])
  yarn_site_xml = deploy_utils.generate_site_xml(args,
    args.yarn_config.configuration.generated_files["yarn-site.xml"])
  hadoop_metrics2_properties = generate_metrics_config(args, host, job_name, instance_id)

  config_files = {
    "core-site.xml": core_site_xml,
    "hdfs-site.xml": hdfs_site_xml,
    "mapred-site.xml": mapred_site_xml,
    "yarn-site.xml": yarn_site_xml,
    "hadoop-metrics2.properties": hadoop_metrics2_properties,
  }
  config_files.update(args.yarn_config.configuration.raw_files)

  return config_files

def generate_run_scripts_params(args, host, job_name, host_id, instance_id):
  job = args.yarn_config.jobs[job_name]

  supervisor_client = deploy_utils.get_supervisor_client(host,
      "yarn", args.yarn_config.cluster.name, job_name, instance_id=instance_id)

  artifact_and_version = "hadoop-" + args.yarn_config.cluster.version

  jar_dirs = ""
  for component in ["common", "mapreduce", "yarn", "hdfs"]:
    if jar_dirs: jar_dirs += ":"
    component_dir = ("$package_dir/share/hadoop/%s" % component)
    jar_dirs += "%s/:%s/lib/*:%s/*" % (
        component_dir, component_dir, component_dir)

  service_env = ""
  for component_path in ["HADOOP_COMMON_HOME", "HADOOP_HDFS_HOME", "YARN_HOME"]:
    service_env += "export %s=$package_dir\n" % (component_path)
  log_level = deploy_utils.get_service_log_level(args, args.yarn_config)

  params = job.get_arguments(args, args.yarn_config.cluster, args.yarn_config.jobs,
    args.yarn_config.arguments_dict, job_name, host_id, instance_id)

  script_dict = {
      "artifact": artifact_and_version,
      "job_name": job_name,
      "jar_dirs": jar_dirs,
      "run_dir": supervisor_client.get_run_dir(),
      "service_env": service_env,
      "params": params,
  }

  return script_dict

def generate_start_script(args, host, job_name, host_id, instance_id):
  script_params = generate_run_scripts_params(args, host, job_name, host_id, instance_id)
  return deploy_utils.create_run_script(
      "%s/start.sh.tmpl" % deploy_utils.get_template_dir(),
      script_params)

def install(args):
  get_yarn_service_config(args)
  deploy_utils.install_service(args, "yarn", args.yarn_config, "hadoop")

def cleanup(args):
  get_yarn_service_config(args)

  cleanup_token = deploy_utils.confirm_cleanup(args,
      "yarn", args.yarn_config)

  for job_name in args.job or ALL_JOBS:
    hosts = args.yarn_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.cleanup_job("yarn", args.yarn_config,
          hosts[host_id].ip, job_name, instance_id, cleanup_token)

def bootstrap_job(args, host, job_name, host_id, instance_id, cleanup_token):
  # parse the service_config according to the instance_id
  args.yarn_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  deploy_utils.bootstrap_job(args, "hadoop", "yarn",
      args.yarn_config, host, job_name, instance_id, cleanup_token, '0')
  start_job(args, host, job_name, host_id, instance_id)

def bootstrap(args):
  get_yarn_service_config(args)
  cleanup_token = deploy_utils.confirm_bootstrap("yarn", args.yarn_config)

  for job_name in args.job or ALL_JOBS:
    hosts = args.yarn_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        bootstrap_job(args, hosts[host_id].ip, job_name, host_id, instance_id, cleanup_token)

def start_job(args, host, job_name, host_id, instance_id):
  # parse the service_config according to the instance_id
  args.yarn_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  config_files = generate_configs(args, host, job_name, instance_id)
  start_script = generate_start_script(args, host, job_name, host_id, instance_id)
  http_url = deploy_utils.get_http_service_uri(host,
    args.yarn_config.jobs[job_name].base_port, instance_id)
  deploy_utils.start_job(args, "hadoop", "yarn", args.yarn_config,
      host, job_name, instance_id, start_script, http_url, **config_files)

def start(args):
  if not args.skip_confirm:
    deploy_utils.confirm_start(args)
  get_yarn_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.yarn_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def stop_job(args, host, job_name, instance_id):
  deploy_utils.stop_job("yarn", args.yarn_config,
      host, job_name, instance_id)

def stop(args):
  if not args.skip_confirm:
    deploy_utils.confirm_stop(args)
  get_yarn_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.yarn_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

def restart(args):
  if not args.skip_confirm:
    deploy_utils.confirm_restart(args)
  get_yarn_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.yarn_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        stop_job(args, hosts[host_id].ip, job_name, instance_id)

  for job_name in args.job or ALL_JOBS:
    hosts = args.yarn_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.wait_for_job_stopping("yarn",
          args.yarn_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
        start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)

def show(args):
  get_yarn_service_config(args)

  for job_name in args.job or ALL_JOBS:
    hosts = args.yarn_config.jobs[job_name].hosts
    args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
    for host_id in args.task_map.keys() or hosts.keys():
      for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
        instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
        deploy_utils.show_job("yarn", args.yarn_config,
          hosts[host_id].ip, job_name, instance_id)

def run_shell(args):
  get_yarn_service_config(args)

  main_class, options = deploy_utils.parse_shell_command(
      args, SHELL_COMMAND_INFO)
  if not main_class:
    return

  # parse the service_config, suppose the instance_id is -1
  args.yarn_config.parse_generated_config_files(args)
  core_site_dict = args.yarn_config.configuration.generated_files["core-site.xml"]
  hdfs_site_dict = args.yarn_config.configuration.generated_files["hdfs-site.xml"]
  mapred_site_dict = args.yarn_config.configuration.generated_files["mapred-site.xml"]
  yarn_site_dict = args.yarn_config.configuration.generated_files["yarn-site.xml"]

  hadoop_opts = list()
  for key, value in core_site_dict.iteritems():
    hadoop_opts.append("-D%s%s=%s" % (deploy_utils.HADOOP_PROPERTY_PREFIX,
          key, value))
  for key, value in hdfs_site_dict.iteritems():
    hadoop_opts.append("-D%s%s=%s" % (deploy_utils.HADOOP_PROPERTY_PREFIX,
          key, value))
  for key, value in mapred_site_dict.iteritems():
    hadoop_opts.append("-D%s%s=%s" % (deploy_utils.HADOOP_PROPERTY_PREFIX,
          key, value))
  for key, value in yarn_site_dict.iteritems():
    hadoop_opts.append("-D%s%s=%s" % (deploy_utils.HADOOP_PROPERTY_PREFIX,
          key, value))

  if deploy_utils.is_security_enabled(args):
    hadoop_opts.append(
        "-Djava.security.krb5.conf=%s/krb5-hadoop.conf" %
        deploy_utils.get_config_dir())

  package_root = deploy_utils.get_artifact_package_root(args,
      args.yarn_config.cluster, "hadoop")
  lib_root = "%s/share/hadoop" % package_root
  class_path = "%s/etc/hadoop" % package_root
  for component in ["common", "hdfs", "mapreduce", "yarn"]:
    component_dir = "%s/%s" % (lib_root, component)
    class_path += ":%s/:%s/*:%s/lib/*" % (component_dir,
        component_dir, component_dir)

  cmd = (["java", "-cp", class_path] + hadoop_opts +
      [main_class] + options)
  p = subprocess.Popen(cmd, stdout=sys.stdout, stderr=sys.stderr)
  p.wait()

def generate_client_config(args, artifact, version):
  config_path = "%s/%s/%s-%s/etc/hadoop" % (args.package_root,
      args.cluster, artifact, version)
  deploy_utils.write_file("%s/mapred-site.xml" % config_path,
      deploy_utils.generate_site_xml(args,
        args.yarn_config.configuration.generated_files["mapred-site.xml"]))
  deploy_utils.write_file("%s/yarn-site.xml" % config_path,
      deploy_utils.generate_site_xml(args,
        args.yarn_config.configuration.generated_files["yarn-site.xml"]))
  deploy_utils.write_file("%s/krb5.conf" % config_path,
      args.yarn_config.configuration.raw_files["krb5.conf"])
  deploy_hdfs.update_hadoop_env_sh(args, artifact, version, "YARN_OPTS")

def pack(args):
  get_yarn_service_config(args)
  args.yarn_config.parse_generated_config_files(args)
  version = args.yarn_config.cluster.version
  deploy_utils.make_package_dir(args, "hadoop", args.yarn_config.cluster)
  args.hdfs_config.parse_generated_config_files(args)
  deploy_hdfs.generate_client_config(args, "hadoop", version)
  generate_client_config(args, "hadoop", version)

  if not args.skip_tarball:
    deploy_utils.pack_package(args, "hadoop", args.yarn_config.cluster.version)
  Log.print_success("Pack client utilities for hadoop success!\n")

def rolling_update(args):
  if not args.job:
    Log.print_critical("You must specify the job name to do rolling update")

  get_yarn_service_config(args)
  job_name = args.job[0]

  if not args.skip_confirm:
    deploy_utils.confirm_action(args, "rolling_update")

  Log.print_info("Rolling updating %s" % job_name)
  hosts = args.yarn_config.jobs[job_name].hosts
  wait_time = 0

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.iterkeys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.confirm_rolling_update(host_id, instance_id, wait_time)
      stop_job(args, hosts[host_id].ip, job_name, instance_id)
      deploy_utils.wait_for_job_stopping("yarn",
        args.yarn_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)
      deploy_utils.wait_for_job_starting("yarn",
        args.yarn_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      wait_time = args.time_interval
  Log.print_success("Rolling updating %s success" % job_name)

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = deploy_zookeeper
import deploy_utils
import os
import pwd
import subprocess
import sys
import tempfile
import time

from log import Log

MYID_FILE = "myid"

SHELL_COMMAND_INFO = {
  "zkcli": ("org.apache.zookeeper.ZooKeeperMain",
      "run the zookeeper client shell"),
}

def generate_zookeeper_config(args):
  config_dict = args.zookeeper_config.configuration.generated_files["zookeeper.cfg"]
  local_path = "%s/zookeeper.cfg.tmpl" % deploy_utils.get_template_dir()
  template = deploy_utils.Template(open(local_path, "r").read())
  return template.substitute(config_dict)

def generate_jaas_config(args):
  if not deploy_utils.is_security_enabled(args):
    return ""

  config_dict = args.zookeeper_config.configuration.generated_files["jaas-server.conf"]

  for key, value in config_dict.items()[1:]:
    if value != "true" and value != "false" and value.find("\"") == -1:
      config_dict[key] = "\"" + value + "\""

  header_line = config_dict["headerLine"]
  return "Server {\n  %s\n%s;\n};" % (header_line,
      "\n".join(["  %s=%s" % (key, value)
        for (key, value) in config_dict.iteritems() if key != config_dict.keys()[0]]))

def generate_client_jaas_config(args):
  if not deploy_utils.is_security_enabled(args):
    return ""

  config_dict = args.zookeeper_config.configuration.generated_files["jaas-client.conf"]

  for key, value in config_dict.items()[1:]:
    if value != "true" and value != "false" and value.find("\"") == -1:
      config_dict[key] = "\"" + value + "\""

  header_line = config_dict["headerLine"]
  return "Client {\n  %s\n%s;\n};" % (header_line,
      "\n".join(["  %s=%s" % (key, value)
        for (key, value) in config_dict.iteritems() if key != config_dict.keys()[0]]))

def generate_run_scripts(args):
  config_files = dict()

  config_files.update({
      "zookeeper.cfg": generate_zookeeper_config(args),
      "jaas.conf": generate_jaas_config(args),
  })
  config_files.update(args.zookeeper_config.configuration.raw_files)

  return config_files

def generate_bootstrap_script(args, host, job_name, host_id, instance_id):
  supervisor_client = deploy_utils.get_supervisor_client(host,
    "zookeeper", args.zookeeper_config.cluster.name, job_name, instance_id=instance_id)
  data_dir = supervisor_client.get_available_data_dirs()[0]
  myid_file = "%s/%s" % (data_dir, MYID_FILE)

  hosts = args.zookeeper_config.jobs["zookeeper"].hosts
  task_id = deploy_utils.get_task_id(hosts, host_id, instance_id)

  script_dict = {
    'myid_file': myid_file,
    'host_id': task_id,
  }
  return deploy_utils.create_run_script(
      '%s/bootstrap_zk.sh.tmpl' % deploy_utils.get_template_dir(),
      script_dict)

def generate_start_script(args, host, job_name, host_id, instance_id):
  supervisor_client = deploy_utils.get_supervisor_client(host,
      "zookeeper", args.zookeeper_config.cluster.name, job_name, instance_id=instance_id)
  run_dir = supervisor_client.get_run_dir()

  artifact_and_version = "zookeeper-" + args.zookeeper_config.cluster.version
  component_dir = "$package_dir"
  # must include both [dir]/ and [dir]/* as [dir]/* only import all jars under
  # this dir but we also need access the webapps under this dir.
  jar_dirs = "%s/:%s/lib/*:%s/*" % (component_dir, component_dir, component_dir)
  job = args.zookeeper_config.jobs["zookeeper"]
  log_level = deploy_utils.get_service_log_level(args, args.zookeeper_config)

  params = job.get_arguments(args, args.zookeeper_config.cluster, args.zookeeper_config.jobs,
    args.zookeeper_config.arguments_dict, job_name, host_id, instance_id)

  script_dict = {
      "artifact": artifact_and_version,
      "job_name": job_name,
      "jar_dirs": jar_dirs,
      "run_dir": run_dir,
      "params": params,
  }

  return deploy_utils.create_run_script(
      '%s/start.sh.tmpl' % deploy_utils.get_template_dir(),
      script_dict)

def get_zk_service_config(args):
  args.zookeeper_config = deploy_utils.get_service_config(args)
  if args.zookeeper_config.cluster.zk_cluster:
    Log.print_critical(
        "zookeeper cluster can't depends on other clusters: %s" %
        args.zookeeper_config.cluster.name)

def install(args):
  get_zk_service_config(args)
  deploy_utils.install_service(args, "zookeeper", args.zookeeper_config, "zookeeper")

def cleanup(args):
  get_zk_service_config(args)

  cleanup_token = deploy_utils.confirm_cleanup(args,
      "zookeeper", args.zookeeper_config)

  hosts = args.zookeeper_config.jobs["zookeeper"].hosts
  for host_id in hosts.keys():
    for instance_id in range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.cleanup_job("zookeeper", args.zookeeper_config,
        hosts[host_id].ip, "zookeeper", instance_id, cleanup_token)

def bootstrap_job(args, host, job_name, host_id, instance_id, cleanup_token):
  # parse the service_config according to the instance_id
  args.zookeeper_config.parse_generated_config_files(args, job_name, host_id, instance_id)
  bootstrap_script = generate_bootstrap_script(args, host, job_name, host_id, instance_id)
  deploy_utils.bootstrap_job(args, "zookeeper", "zookeeper", args.zookeeper_config,
      host, job_name, instance_id, cleanup_token, '0', bootstrap_script)

  # start job after bootstrapping.
  start_job(args, host, job_name, host_id, instance_id)

def bootstrap(args):
  get_zk_service_config(args)

  cleanup_token = deploy_utils.confirm_bootstrap("zookeeper", args.zookeeper_config)
  hosts = args.zookeeper_config.jobs["zookeeper"].hosts

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.keys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      bootstrap_job(args, hosts[host_id].ip, "zookeeper", host_id, instance_id, cleanup_token)

def start_job(args, host, job_name, host_id, instance_id):
  # parse the service_config according to the instance_id
  args.zookeeper_config.parse_generated_config_files(args, job_name, host_id, instance_id)

  config_files = generate_run_scripts(args)
  start_script = generate_start_script(args, host, job_name, host_id, instance_id)
  http_url = ''
  deploy_utils.start_job(args, "zookeeper", "zookeeper", args.zookeeper_config,
      host, job_name, instance_id, start_script, http_url, **config_files)

def start(args):
  if not args.skip_confirm:
    deploy_utils.confirm_start(args)
  get_zk_service_config(args)
  hosts = args.zookeeper_config.jobs["zookeeper"].hosts

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.keys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      start_job(args, hosts[host_id].ip, "zookeeper", host_id, instance_id)

def stop_job(args, host, job_name, instance_id):
  deploy_utils.stop_job("zookeeper", args.zookeeper_config,
      host, job_name, instance_id)

def stop(args):
  if not args.skip_confirm:
    deploy_utils.confirm_stop(args)
  get_zk_service_config(args)
  hosts = args.zookeeper_config.jobs["zookeeper"].hosts

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.keys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      stop_job(args, hosts[host_id].ip, "zookeeper", instance_id)

def restart(args):
  if not args.skip_confirm:
    deploy_utils.confirm_restart(args)
  get_zk_service_config(args)
  hosts = args.zookeeper_config.jobs["zookeeper"].hosts

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.keys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      stop_job(args, hosts[host_id].ip, "zookeeper", instance_id)

  for host_id in args.task_map.keys() or hosts.keys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.wait_for_job_stopping("zookeeper",
        args.zookeeper_config.cluster.name, "zookeeper", hosts[host_id].ip, instance_id)
      start_job(args, hosts[host_id].ip, "zookeeper", host_id, instance_id)

def show(args):
  get_zk_service_config(args)
  hosts = args.zookeeper_config.jobs["zookeeper"].hosts

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.keys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.show_job("zookeeper", args.zookeeper_config,
        hosts[host_id].ip, "zookeeper", instance_id)

def run_shell(args):
  get_zk_service_config(args)

  main_class, options = deploy_utils.parse_shell_command(
      args, SHELL_COMMAND_INFO)
  if not main_class:
    return

  args.zookeeper_config.parse_generated_config_files(args)

  client_jaas = generate_client_jaas_config(args)
  jaas_fd, jaas_file = tempfile.mkstemp(suffix='zookeeper')
  os.write(jaas_fd, client_jaas)
  os.close(jaas_fd)
  zookeeper_opts = list()
  if deploy_utils.is_security_enabled(args):
    zookeeper_opts.append("-Djava.security.auth.login.config=%s" % jaas_file)
    zookeeper_opts.append(
      "-Djava.security.krb5.conf=%s/krb5-hadoop.conf" %
      deploy_utils.get_config_dir())

  package_root = deploy_utils.get_artifact_package_root(args,
      args.zookeeper_config.cluster, "zookeeper")
  class_path = "%s/:%s/lib/*:%s/*" % (package_root, package_root, package_root)

  zk_address = "%s:%d" % (
      deploy_utils.get_zk_address(args.zookeeper_config.cluster.name),
      args.zookeeper_config.jobs["zookeeper"].base_port)

  cmd = (["java", "-cp", class_path] + zookeeper_opts + [main_class,
      "-server", zk_address] + options)
  p = subprocess.Popen(cmd, stdout=sys.stdout, stderr=sys.stderr)
  p.wait()

def generate_client_config(args, artifact, version):
  config_path = "%s/%s/%s-%s/conf" % (args.package_root,
      args.cluster, artifact, version)
  deploy_utils.write_file("%s/zookeeper.cfg" % config_path,
      generate_zookeeper_config(args))
  deploy_utils.write_file("%s/jaas.conf" % config_path,
      generate_client_jaas_config(args))
  deploy_utils.write_file("%s/krb5.conf" % config_path,
      args.zookeeper_config.configuration.raw_files["krb5.conf"])
  update_zk_env_sh(args, artifact, version)

def update_zk_env_sh(args, artifact, version):
  current_path = os.path.abspath(os.path.dirname(
        os.path.realpath(args.package_root)))
  jvm_flags = '-Djava.security.auth.login.config=$ZOOCFGDIR/jaas.conf '
  jvm_flags += '-Djava.security.krb5.conf=$ZOOCFGDIR/krb5.conf '

  bin_path = "%s/%s/%s-%s/bin" % (args.package_root,
    args.cluster, artifact, version)
  deploy_utils.append_to_file("%s/zkEnv.sh" % bin_path,
      'export JVMFLAGS="%s"\n' % jvm_flags)

def pack(args):
  get_zk_service_config(args)
  args.zookeeper_config.parse_generated_config_files(args)

  version = args.zookeeper_config.cluster.version
  deploy_utils.make_package_dir(args, "zookeeper", args.zookeeper_config.cluster)
  generate_client_config(args, "zookeeper", version)

  if not args.skip_tarball:
    deploy_utils.pack_package(args, "zookeeper", version)
  Log.print_success("Pack client utilities for zookeeper success!")

def rolling_update(args):
  get_zk_service_config(args)
  job_name = "zookeeper"

  if not args.skip_confirm:
    deploy_utils.confirm_action(args, "rolling_update")

  Log.print_info("Rolling updating %s" % job_name)
  hosts = args.zookeeper_config.jobs[job_name].hosts
  wait_time = 0

  args.task_map = deploy_utils.parse_args_host_and_task(args, hosts)
  for host_id in args.task_map.keys() or hosts.iterkeys():
    for instance_id in args.task_map.get(host_id) or range(hosts[host_id].instance_num):
      instance_id = -1 if not deploy_utils.is_multiple_instances(host_id, hosts) else instance_id
      deploy_utils.confirm_rolling_update(host_id, instance_id, wait_time)
      stop_job(args, hosts[host_id].ip, job_name, instance_id)
      deploy_utils.wait_for_job_stopping("zookeeper",
        args.zookeeper_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      start_job(args, hosts[host_id].ip, job_name, host_id, instance_id)
      deploy_utils.wait_for_job_starting("zookeeper",
        args.zookeeper_config.cluster.name, job_name, hosts[host_id].ip, instance_id)
      wait_time = args.time_interval
  Log.print_success("Rolling updating %s success" % job_name)

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = log
import sys

from datetime import datetime

class Log:
  # We have such a agreement on verbosity level:
  # 0: equals to print_info
  # 1: summary of a host level operation (a batch of command)
  # 2: summary of a command
  # 3: details or content of a command
  verbosity = 0

  @staticmethod
  def _print(message):
    print "%s %s" % (datetime.now().strftime('%Y-%m-%d %H:%M:%S'), message)

  @staticmethod
  def error_exit(print_stack):
    if not print_stack:
      sys.exit(2)
    else:
      raise RuntimeError("fatal error")

  @staticmethod
  def print_verbose(message, verbosity):
    if verbosity <= Log.verbosity:
      Log.print_info(message)

  @staticmethod
  def print_info(message):
    Log._print(message)

  @staticmethod
  def print_success(message):
    Log._print("\033[0;32m%s\033[0m" % message)

  @staticmethod
  def print_warning(message):
    Log._print("\033[0;33m%s\033[0m" % message)

  @staticmethod
  def print_error(message):
    Log._print("\033[0;31m%s\033[0m" % message)

  @staticmethod
  def print_critical(message):
    Log.print_error(message)
    Log.error_exit(False)

########NEW FILE########
__FILENAME__ = service_config
import argparse
import copy
import deploy_config
import getpass
import os
import re
import socket
import subprocess

from configobj import ConfigObj
from log import Log
from supervisor_client import SupervisorClient

BASEPORT_INTERVAL = 10

def get_real_instance_id(instance_id):
  if instance_id == -1:
    return 0
  else:
    return instance_id

def get_base_port(base_port, instance_id):
  return base_port + BASEPORT_INTERVAL * get_real_instance_id(instance_id)

def parse_task_number(task_id, hosts):
  found_task = False
  instance_id = int(task_id)

  for host_id, host in hosts.iteritems():
    if instance_id + 1 > host.instance_num:
      instance_id -= host.instance_num
    else:
      found_task = True
      break
  if found_task == False:
    raise ValueError(str(task_id) + ' is not a valid task of cluster, please check your config')
  return host_id, instance_id

def get_port_addition_result(args, cluster, jobs, current_job, host_id, instance_id, val):
  reg_expr = JOB_PORT_EXPR_REGEX.match(val)
  job_name = reg_expr.group('job')
  add_num = int(reg_expr.group('num'))
  return get_base_port(jobs[job_name].base_port, instance_id) + add_num

def get_job_task_port_addition_result(args, cluster, jobs, current_job, host_id, instance_id, val):
  reg_expr = JOB_TASK_PORT_EXPR_REGEX.match(val)
  job_name = reg_expr.group('job')
  task_id = reg_expr.group('task')
  add_num = int(reg_expr.group('num'))
  host_id, instance_id = parse_task_number(task_id, jobs[job_name].hosts)
  return get_base_port(jobs[job_name].base_port, instance_id) + add_num

def get_service_job_task_port_addition_result(args, cluster, jobs, current_job, host_id, instance_id, val):
  reg_expr = SERVICE_JOB_TASK_PORT_EXPR_REGEX.match(val)
  service = reg_expr.group('service')
  job_name = reg_expr.group('job')
  task_id = reg_expr.group('task')
  add_num = int(reg_expr.group('num'))

  service_config = get_service_config(args, service, cluster)
  host_id, instance_id = parse_task_number(task_id, service_config.jobs[job_name].hosts)
  return get_base_port(service_config.jobs[job_name].base_port, instance_id)

def get_service_cluster_name(service, cluster):
  if service == "zookeeper":
    return cluster.zk_cluster
  elif service == "hdfs":
    hdfs_cluster = cluster.hdfs_cluster
    if hdfs_cluster != cluster.name:
      return hdfs_cluster
    else:
      return cluster.name
  elif service == "hbase":
    hbase_cluster = cluster.hbase_cluster
    if hbase_cluster != cluster.name:
      return hbase_cluster
    else:
      return cluster.name

def get_service_config(args, service, cluster):
  get_short_user_name(args)

  if not getattr(args, service + "_config", None):
    service_args = argparse.Namespace()
    service_args.service = service
    service_args.cluster = get_service_cluster_name(service, cluster)
    setattr(args, service + "_config", ServiceConfig(service_args))
  return getattr(args, service + "_config")

def get_zk_job(args, cluster):
  zk_config = get_service_config(args, "zookeeper", cluster)
  return zk_config.jobs["zookeeper"]

def get_zk_hosts(args, cluster, jobs, current_job, host_id):
  zk_job = get_zk_job(args, cluster)
  return ",".join(["%s" % (host.ip) for host in zk_job.hosts.itervalues()])

def get_job_host_port_list(job):
  host_port_list = []
  for host in job.hosts.itervalues():
    for instance_id in range(host.instance_num):
      host_port_list.append("%s:%d" % (
        host.ip, get_base_port(job.base_port, instance_id)))
  return host_port_list

def get_zk_hosts_with_port(args, cluster, jobs, current_job, host_id):
  zk_job = get_zk_job(args, cluster)
  host_port_list = get_job_host_port_list(zk_job)
  return ",".join(host_port_list)

def get_slots_ports_list(args, cluster, jobs, current_job, host_id):
  slot_port = jobs["supervisor"].base_port + 10
  slot_number = int(args.storm_config.configuration.generated_files['storm.yaml']['slot_number'])

  slots_ports_list = []
  for port_index in range(slot_number):
    slots_ports_list.append(str(slot_port + port_index))
  return ','.join(slots_ports_list)

def get_journalnode_hosts_with_port(args, cluster, jobs, current_job, host_id):
  hdfs_config = get_service_config(args, "hdfs", cluster)
  jour_job = hdfs_config.jobs["journalnode"]
  host_port_list = get_job_host_port_list(jour_job)
  return ";".join(host_port_list)

def get_zk_server_list(args, cluster, jobs, current_job, host_id):
  server_list = str()
  job = jobs[jobs.keys()[0]]
  hosts = job.hosts
  for host_id, host in hosts.iteritems():
    for instance_id in range(host.instance_num):
      server_list += ("server.%d=%s:%d:%d\n" %
        (host_id * host.instance_num + instance_id, host.ip,
          get_base_port(job.base_port, instance_id) + 2,
          get_base_port(job.base_port, instance_id) + 3))
  return server_list

def get_supervisor_client(host, service, cluster_name, job, instance_id):
  supervisor_config = deploy_config.get_deploy_config().get_supervisor_config()
  return SupervisorClient(host, supervisor_config.get('server_port'),
    supervisor_config.get('user'), supervisor_config.get('password'),
    service, cluster_name, job, instance_id)

def get_config_dir(args=None, cluster=None, jobs=None, current_job="", host_id=0):
  return deploy_config.get_deploy_config().get_config_dir()

def get_short_user_name(args, cluster=None, jobs=None, current_job="", host_id=0):
  if not getattr(args, "short_user_name", None):
    args.short_user_name = get_short_user_name_full()[1]
  return args.short_user_name

def get_remote_user(args, cluster, jobs, current_job, host_id):
  return args.remote_user

def get_current_host(args, cluster, jobs, current_job, host_id):
  return jobs[current_job].hosts[host_id].ip

def get_hadoop_conf_path(args, cluster, jobs, current_job, host_id):
  return "/etc/hadoop/conf"

def get_config_path(args):
  return "%s/conf/%s/%s-%s.cfg" % (get_config_dir(),
    args.service, args.service, args.cluster)

def get_short_user_name_full():
  try:
    cmd = ['klist']
    output = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT,)

    centos_line_prefix = 'Default principal:'
    macos_line_prefix = 'Principal:'
    for line in output.split('\n'):
      if (line.strip().startswith(centos_line_prefix) or
          line.strip().startswith(macos_line_prefix)):
        return True, line.split(':')[1].split('@')[0].strip()
  except:
    return False, getpass.getuser()

def get_specific_dir(host, service, cluster_name, job_name, instance_id, attribute):
  supervisor_client = get_supervisor_client(host, service, cluster_name, job_name, instance_id)

  if attribute == "data_dir":
    return supervisor_client.get_available_data_dirs()[0]
  elif attribute == "data_dirs":
    return ",".join(supervisor_client.get_available_data_dirs())
  elif attribute == "run_dir":
    return supervisor_client.get_run_dir()
  elif attribute == "log_dir":
    return supervisor_client.get_log_dir()
  elif attribute == "current_package_dir":
    return supervisor_client.get_current_package_dir()

def get_service_cluster_attribute(args, cluster, jobs, current_job, host_id, instance_id, val):
  reg_expr = SERVICE_CLUSTER_ATTRIBUTE_REGEX.match(val)
  service = reg_expr.group('service')
  attribute = reg_expr.group('attribute')
  service_config = get_service_config(args, service, cluster)
  return getattr(service_config.cluster, attribute)

def get_service_job_task_attribute(args, cluster, jobs, current_job, host_id, instance_id, val):
  reg_expr = SERVICE_JOB_TASK_ATTRIBUTE_REGEX.match(val)
  service = reg_expr.group('service')
  job_name = reg_expr.group('job')
  task_id = reg_expr.group('task')
  attribute = reg_expr.group('attribute')
  service_config = get_service_config(args, service, cluster)
  host_id, instance_id = parse_task_number(task_id, service_config.jobs[job_name].hosts)
  if attribute == 'host':
    return service_config.jobs[job_name].hosts[host_id].ip
  elif attribute == 'base_port':
    return get_base_port(service_config.jobs[job_name].base_port, instance_id)

def get_job_task_attribute(args, cluster, jobs, current_job, host_id, instance_id, val):
  reg_expr = JOB_TASK_ATTRIBUTE_REGEX.match(val)
  job_name = reg_expr.group('job')
  task_id = reg_expr.group('task')
  attribute = reg_expr.group('attribute')
  host_id, instance_id = parse_task_number(task_id, jobs[job_name].hosts)
  if attribute == 'host':
    return jobs[job_name].hosts[host_id].ip
  elif attribute == 'base_port':
    return get_base_port(jobs[job_name].base_port, instance_id)

def get_job_host_attribute(args, cluster, jobs, current_job, host_id, instance_id, val):
  reg_expr = JOB_HOST_ATTRIBUTE_REGEX.match(val)
  job_name = reg_expr.group('job')
  attribute = reg_expr.group('attribute')
  if not getattr(jobs[job_name].hosts[host_id], attribute, None):
    Log.print_critical("The attribute %s of %s--%s is not configured." \
      " Please check your configuration." % (
      attribute, job_name, jobs[job_name].hosts[host_id].ip))

  return getattr(jobs[job_name].hosts[host_id], attribute)

def get_section_attribute(args, cluster, jobs, current_job, host_id, instance_id, val):
  reg_expr = SECTION_ATTRIBUTE_REGEX.match(val)
  section = reg_expr.group('section')
  attribute = reg_expr.group('attribute')

  if section == "cluster":
    return getattr(cluster, attribute)
  else:
    section_instance_id = instance_id
    if attribute == "base_port":
      return get_base_port(jobs[section].base_port, section_instance_id)
    else:
      if current_job == section:
        host = jobs[section].hosts[host_id]
      else: # prevent index over boundary when host_id mapping another job
        host = jobs[section].hosts[0]
      # the parsing section may not be the job which is being started or bootstrapped,
      # so call get_specific_dir according to the section_instance_id.
      if host.instance_num == 1:
        section_instance_id = -1
      return get_specific_dir(host.ip, args.service, cluster.name, section, section_instance_id, attribute)


CLUSTER_NAME_REGEX = re.compile(r'((?P<zk>[a-z0-9]+)-)?([a-z0-9]+)')
HOST_RULE_REGEX = re.compile(r'host\.(?P<id>\d+)')
VARIABLE_REGEX = re.compile('%\{(.+?)\}')

SECTION_ATTRIBUTE_REGEX = re.compile('(?P<section>(?!zk\.)\w+)\.(?P<attribute>\w+)$')
JOB_PORT_EXPR_REGEX = re.compile('(?P<job>\w+)\.base_port[+-](?P<num>\d+)')
JOB_TASK_ATTRIBUTE_REGEX = re.compile('(?P<job>\w+)\.(?P<task>\d+)\.(?P<attribute>\w+)$')
JOB_TASK_PORT_EXPR_REGEX = re.compile('(?P<job>\w+)\.(?P<task>\d+)\.base_port[+-](?P<num>\d+)')
JOB_HOST_ATTRIBUTE_REGEX = re.compile('(?P<job>\w+)\.host\.(?P<attribute>\w+)$')
SERVICE_CLUSTER_ATTRIBUTE_REGEX = re.compile('(?P<service>\w+)\.cluster\.(?P<attribute>\w+)$')
SERVICE_JOB_TASK_ATTRIBUTE_REGEX = re.compile('(?P<service>\w+)\.(?P<job>\w+)\.(?P<task>\d+)\.(?P<attribute>\w+)$')
SERVICE_JOB_TASK_PORT_EXPR_REGEX = re.compile('(?P<service>\w+)\.(?P<job>\w+)\.(?P<task>\d+)\.base_port[+-](?P<num>\d+)')

SCHEMA_MAP = {
  JOB_PORT_EXPR_REGEX : get_port_addition_result,
  SECTION_ATTRIBUTE_REGEX : get_section_attribute,
  JOB_TASK_ATTRIBUTE_REGEX : get_job_task_attribute,
  JOB_TASK_PORT_EXPR_REGEX : get_job_task_port_addition_result,
  JOB_HOST_ATTRIBUTE_REGEX : get_job_host_attribute,
  SERVICE_CLUSTER_ATTRIBUTE_REGEX : get_service_cluster_attribute,
  SERVICE_JOB_TASK_ATTRIBUTE_REGEX : get_service_job_task_attribute,
  SERVICE_JOB_TASK_PORT_EXPR_REGEX : get_service_job_task_port_addition_result,
  "zk.hosts" : get_zk_hosts,
  "zk.hosts_with_port" : get_zk_hosts_with_port,
  "slots_ports_list" : get_slots_ports_list,
  "journalnode_task_list" : get_journalnode_hosts_with_port,
  "server_list" : get_zk_server_list,
  "config_dir" : get_config_dir,
  "short_user_name" : get_short_user_name,
  "remote_user" : get_remote_user,
  "current_host" : get_current_host,
  "hadoop_conf_path" : get_hadoop_conf_path,
  # "slaves" : "\n".join(jobs["datanode"].hosts.values()),
}

COMMON_JOB_SCHEMA = {
  # "param_name": (type, default_value)
  # type must be in {bool, int, float, str}
  # if default_value is None, it means it's NOT an optional parameter.
  "base_port": (int, None),
}

CLUSTER_SCHEMA = {
  "name": (str, None),
  "version": (str, None),
  "jobs": (str, None),
  "kerberos_realm": (str, "XIAOMI.NET"),
  "kerberos_username": (str, ""),
  "ganglia_address" : (str, ""),
  "package_name": (str, ""),
  "revision": (str, ""),
  "timestamp": (str, ""),
  "hdfs_cluster": (str, ""),
  "hbase_cluster": (str, ""),
  "log_level": (str, "info"),
}

MULTIPLE_INSTANCES_JOBS = ["datanode", "regionserver", "nodemanager", "historyserver", "impalad"]
ARGUMENTS_TYPE_LIST = ["jvm_args", "system_properties", "main_entry", "extra_args"]
HEAP_MEMORY_SETTING_LIST = ["-Xmx", "-Xms", "-Xmn", "-Xss"]

class ServiceConfig:
  '''
  The class represents the configuration of a service.
  '''
  def __init__(self, args):
    self.config_dict_full = self.get_config_dict_full(
      get_config_path(args))

    self.cluster_dict = self.config_dict_full["cluster"]
    self.configuration_dict = self.config_dict_full["configuration"]
    self.arguments_dict = self.config_dict_full["arguments"]

    self.cluster = ServiceConfig.Cluster(self.cluster_dict, args.cluster)
    self.jobs = {}
    for job_name in self.cluster.jobs:
      self.jobs[job_name] = ServiceConfig.Jobs(
        self.config_dict_full[job_name], job_name)
    self.configuration = ServiceConfig.Configuration(
      self.configuration_dict, args, self.cluster, self.jobs)

  class Cluster:
    '''
    The class represents a service cluster
    '''
    def __init__(self, cluster_dict, cluster_name):
      ServiceConfig.parse_params(self, "cluster", cluster_dict, CLUSTER_SCHEMA)

      self.jobs = self.jobs.split()
      if self.name != cluster_name:
        Log.print_critical(
          "Cluster name in config doesn't match the config file name: "
          "%s vs. %s" % (self.name, cluster_name))
      reg_expr = CLUSTER_NAME_REGEX.match(self.name)
      if not reg_expr:
        Log.print_critical("Illegal cluster name: %s" % self.name)
      self.zk_cluster = reg_expr.group("zk")

  class Jobs:
    '''
    The class represents all the jobs of a service
    '''
    def __init__(self, job_dict, job_name):
      self.name = job_name
      self.job_dict = job_dict
      ServiceConfig.parse_params(self, job_name, job_dict, COMMON_JOB_SCHEMA)
      if self.base_port % 100 != 0:
        Log.print_critical("base_port %d is NOT a multiple of 100!" %
                            self.base_port)

      self._parse_hosts_list(job_dict, job_name)

    def _parse_hosts_list(self, job_dict, job_name):
      '''
      Parse the hosts list for job
      '''
      self.hosts = {}
      self.hostnames = {}
      for name, value in job_dict.iteritems():
        reg_expr = HOST_RULE_REGEX.match(name)
        if not reg_expr:
          continue
        host_id = int(reg_expr.group("id"))
        self.hosts[host_id] = ServiceConfig.Jobs.Hosts(value)

        ip = self.hosts[host_id].ip
        try:
          self.hostnames[host_id] = socket.gethostbyaddr(ip)[0]
        except:
          self.hostnames[host_id] = ip

        instance_num = self.hosts[host_id].instance_num
        if instance_num > 1 and job_name not in MULTIPLE_INSTANCES_JOBS:
          Log.print_critical("The job %s doesn't support for multiple instances" \
            " on the same host. Please check your config." % job_name)

    def _generate_arguments_list(self, job_dict, job_name, arguments_dict):
      '''
      Generate the arguments lists as follows:
      job.jvm_args, job.system_properties, job.main_entry, job.extra_args.
      '''
      # prevent repeated generation for one job on different hosts/instances
      if any(getattr(self, args_type, None) != None for args_type in ARGUMENTS_TYPE_LIST):
        return

      if not job_dict.has_key("arguments"):
        Log.print_critical("The job %s must be configured with the `arguments` section." \
          " Please check your configuration file." % job_name)

      job_specific_arguments = job_dict["arguments"]
      job_common_arguments = arguments_dict[job_name]
      service_common_arguments = arguments_dict["service_common"]

      self._merge_arguments_dict(job_common_arguments, service_common_arguments)
      self._merge_arguments_dict(job_specific_arguments, job_common_arguments)

      # set job's attributes: job.jvm_args, job.system_properties, job.main_entry, job.extra_args
      for args_type in ARGUMENTS_TYPE_LIST:
        setattr(self, args_type, job_specific_arguments[args_type])

    def _get_argument_key(self, argument):
      # argument is a 'key=value' pair
      if argument.find('=') != -1:
        return argument.split('=')[0]
      else:
        # argument is a member of HEAP_MEMORY_SETTING_LIST
        for member in HEAP_MEMORY_SETTING_LIST:
          if argument.startswith(member):
            return member
        # argument is a normal string without '='
        return argument

    def _check_and_insert_argument(self, arguments_list, argument):
      '''
      Insert the argument into the arguments_list if
      the arguments_list doesn't contain the argument.
      '''
      argument_key = self._get_argument_key(argument)

      for item in arguments_list:
        item_key = self._get_argument_key(item)
        if item_key == argument_key:
          return
      arguments_list.append(argument)

    def _merge_arguments_dict(self, child_arguments_dict, base_arguments_dict):
      '''
      Merge the arguments from the base_arguments_dict to child_arguments_dict,
      for duplicate items, use the child item to override the base item.
      '''
      for args_type in ARGUMENTS_TYPE_LIST:
        base_arguments_list = base_arguments_dict[args_type]
        if type(base_arguments_list) == str:
          base_arguments_list = base_arguments_list.split()

        child_arguments_list = []
        if child_arguments_dict.has_key(args_type):
          child_arguments_list = child_arguments_dict[args_type].split()

        for argument in base_arguments_list:
          self._check_and_insert_argument(child_arguments_list, argument)

        child_arguments_dict[args_type] = child_arguments_list

    def _generate_string_format_arguments(self, args, cluster, jobs, current_job="", host_id=0, instance_id=-1):
      '''
      Parse the arguments list and generate/joint the string format arguments.
      All items in the arguments are connected with ' '.
      '''
      arguments_string = ""
      for type_id in range(len(ARGUMENTS_TYPE_LIST)):
        args_list = copy.deepcopy(getattr(self, ARGUMENTS_TYPE_LIST[type_id]))
        for argument_id in range(len(args_list)):
          if args_list[argument_id].find('%') != -1:
            args_list[argument_id] = ServiceConfig.parse_item(
              args, cluster, jobs, current_job, host_id, instance_id, args_list[argument_id])

        # joint the arguments string
        arguments_string += " ".join(args_list)
        if type_id < len(ARGUMENTS_TYPE_LIST) - 1:
          arguments_string += " "

      return arguments_string

    def get_arguments(self, args, cluster, jobs, arguments_dict, current_job="", host_id=0, instance_id=-1):
      self._generate_arguments_list(self.job_dict, self.name, arguments_dict)
      return self._generate_string_format_arguments(args, cluster, jobs, current_job, host_id, instance_id)

    class Hosts:
      '''
      The class represents all the hosts of a job
      '''
      def __init__(self, attribute_str):
        # parse the host attributes
        self._parse_host_attributes(attribute_str)

      def _parse_host_attributes(self, attribute_str):
        attribute_list = attribute_str.split('/')
        attribute_dict = {}

        # parse the attribute_str
        attribute_dict['ip'] = attribute_list[0]

        for attribute_item in attribute_list[1:]:
          if attribute_item.find('=') == -1:
            Log.print_critical("The host attributes definition are wrong." \
              " Please check your configuration file.")
          attribute_name, attribute_val = attribute_item.split('=')
          attribute_dict[attribute_name] = attribute_val

        # check the essential attributes 'instance'
        instance_num = int(attribute_dict.get('instance_num', 1))
        if instance_num < 1:
          Log.print_critical("The instance number must be greater than or equal to 1!")
        attribute_dict['instance_num'] = instance_num # store 'int' type

        # set the host attributes
        for attribute_name, attribute_val in attribute_dict.iteritems():
          setattr(self, attribute_name, attribute_val)

  class Configuration:
    '''
    The class represents all the config files to be generated of a service
    '''
    def __init__(self, configuration_dict, args, cluster, jobs):
      self.config_section_dict = configuration_dict
      self.raw_files, self.generated_files = ServiceConfig.parse_raw_files(
        self.config_section_dict, args, cluster, jobs)

  def get_config_dict_full(self, config_path):
    '''
    Get the whole configuration dict: reading the base common-config and
    using the child_config_dict to update the base_config_dict

    @param   config_path      The path for configuration file
    @return  dict             The whole configuration dict
    '''
    base_config_dict = {}
    child_config_dict = ConfigObj(config_path, file_error=True)
    arguments_config_dict = {}

    if child_config_dict['configuration'].has_key('base'):
      config_path = child_config_dict['configuration']['base']

      if config_path.find('%') != -1:
        config_path = self.parse_item(None, None, None, item=config_path)

      base_config_dict = self.get_config_dict_full(config_path)
      child_configuration_dict = child_config_dict['configuration']
      base_configuration_dict = base_config_dict['configuration']

      for file_name, file_dict in base_configuration_dict.iteritems():
        if file_name in child_configuration_dict:
          file_dict.update(child_configuration_dict[file_name])

      base_config_dict['configuration'].update(base_configuration_dict)
      child_config_dict.update(base_config_dict)
    return child_config_dict


  @staticmethod
  def parse_params(namespace, section_name, section_dict, schema):
    '''
    Parse the parameters specified by the schema dict from the specific section dict
    '''
    for param_name, param_def in schema.iteritems():
      if param_name in section_dict:
        if param_def[0] is bool:
          param_value = section_dict.as_bool(param_name)
        elif param_def[0] is int:
          param_value = section_dict.as_int(param_name)
        elif param_def[0] is float:
          param_value = section_dict.as_float(param_name)
        else:
          param_value = section_dict[param_name]
      else:
        # option not found, use the default value if there is.
        if param_def[1] is None:
          Log.print_critical("required option %s missed in section %s!" %
            (param_name, section_name))
        else:
          param_value = param_def[1]
      setattr(namespace, param_name, param_value)


  @staticmethod
  def parse_item(args, cluster, jobs, current_job="", host_id=0, instance_id=-1, item=None):
    '''
    Parse item which is enclosed by '%{}' in key/value
    '''
    reg_expr = VARIABLE_REGEX.findall(item)
    new_item = []
    for iter in range(len(reg_expr)):
      for key, callback in SCHEMA_MAP.iteritems():
        if reg_expr[iter] == key:
          new_item.append(callback(args, cluster, jobs, current_job, host_id))
          break
        elif type(key) == type(VARIABLE_REGEX) and key.match(reg_expr[iter]):
          new_item.append(callback(args, cluster, jobs, current_job, host_id, instance_id, reg_expr[iter]))
          break

    for iter in range(len(new_item)):
      item = item.replace("%{"+reg_expr[iter]+"}", str(new_item[iter]))
    return item

  @staticmethod
  def parse_raw_files(config_section_dict, args, cluster, jobs):
    '''
    Parse and calculate the dict value which contains '%{}',
    and read local configuration files as {file_name : file_content_str}.
    Generate configuration files dict as {file_name : file_dict}
    '''
    raw_files = {}
    generated_files = {}
    for file_name, file_dict in config_section_dict.iteritems():
      if type(file_dict) == str:
        file_dict = ServiceConfig.parse_item(args, cluster, jobs, item=file_dict)
        if os.path.exists(file_dict):
          raw_files[file_name] = open(file_dict).read()
        else:
          raw_files[file_name] = str()
      else:
        generated_files[file_name] = file_dict

    return raw_files, generated_files

  @staticmethod
  def parse_list_type_value(list_type_value, args, cluster, jobs, current_job, host_id, instance_id):
    for item_index in range(len(list_type_value)):
      if list_type_value[item_index].find('%') != -1:
        value_item = ServiceConfig.parse_item(args, cluster, jobs, current_job,
          host_id, instance_id, list_type_value[item_index])
        list_type_value[item_index] = value_item

    return list_type_value

  @staticmethod
  def parse_generated_files(config_section_dict, args, cluster, jobs, current_job, host_id, instance_id):
    '''
    Parse and calculate key/value which contains '%{}',
    update the generated files according to the instance_id
    '''
    generated_files = {}
    for file_name, file_dict in config_section_dict.iteritems():
      if isinstance(file_dict, dict):
        for key, value in file_dict.iteritems():
          if key.find('%') != -1:
            file_dict.pop(key)
            key = ServiceConfig.parse_item(args, cluster, jobs, current_job, host_id, instance_id, key)
            file_dict[key] = value
          if isinstance(value, list):
            file_dict[key] = ServiceConfig.parse_list_type_value(value, args, cluster, jobs,
              current_job, host_id, instance_id)
          elif value.find('%') != -1:
            file_dict[key] = ServiceConfig.parse_item(args, cluster, jobs, current_job, host_id, instance_id, value)
        generated_files[file_name] = file_dict

    return generated_files


  def parse_generated_config_files(self, args, current_job="", host_id=0, instance_id=-1):
    '''
    Parse the configuration section for the specified task.
    '''
    config_section_dict = copy.deepcopy(self.configuration_dict)
    self.configuration.generated_files.update(
      ServiceConfig.parse_generated_files(config_section_dict,
        args, self.cluster, self.jobs, current_job, host_id, instance_id))


########NEW FILE########
__FILENAME__ = supervisor_client
import socket
import xmlrpclib

class SupervisorClient:
  '''
  The supervisor client.
  '''
  def __init__(self, host, port, user, passwd, service, cluster, job, instance_id):
    self.proxy = xmlrpclib.ServerProxy('http://%s:%s@%s:%d' % (
      user, passwd, host, port))
    self.service = service
    self.cluster = cluster
    self.job = job
    self.instance_id = instance_id

  def get_available_data_dirs(self):
    '''
    Get the available data directories of the remote server.
    '''
    # The `if` statement block is intended to be compatible with the old supervisor.
    # After all supervisors upgrading to the latest version,
    # the `if` statement block will be deleted.
    # The following similar block will not say more.
    if self.instance_id == -1:
      return self.proxy.deployment.get_available_data_dirs(self.service,
        self.cluster, self.job)
    else:
      return self.proxy.deployment.get_available_data_dirs(self.service,
        self.cluster, self.job, self.instance_id)

  def get_data_dirs(self):
    '''
    Get the currently used data directories of this job.
    '''
    if self.instance_id == -1:
      return self.proxy.deployment.get_data_dirs(self.service,
        self.cluster, self.job)
    else:
      return self.proxy.deployment.get_data_dirs(self.service,
        self.cluster, self.job, self.instance_id)

  def get_log_dir(self):
    '''
    Get the log directory of this job.
    '''
    if self.instance_id == -1:
      return self.proxy.deployment.get_log_dir(self.service,
        self.cluster, self.job)
    else:
      return self.proxy.deployment.get_log_dir(self.service,
        self.cluster, self.job, self.instance_id)

  def get_cleanup_token(self):
    '''
    Get the cleanup token of this job.
    '''
    if self.instance_id == -1:
      return self.proxy.deployment.get_cleanup_token(self.service,
        self.cluster, self.job)
    else:
      return self.proxy.deployment.get_cleanup_token(self.service,
        self.cluster, self.job, self.instance_id)

  def get_run_dir(self):
    '''
    Get the running directory of this job.
    '''
    if self.instance_id == -1:
      return self.proxy.deployment.get_run_dir(self.service,
        self.cluster, self.job)
    else:
      return self.proxy.deployment.get_run_dir(self.service,
        self.cluster, self.job, self.instance_id)

  def get_package_dir(self):
    '''
    Get the package directory of this job.
    '''
    if self.instance_id == -1:
      return self.proxy.deployment.get_package_dir(self.service,
        self.cluster, self.job)
    else:
      return self.proxy.deployment.get_package_dir(self.service,
        self.cluster, self.job, self.instance_id)

  # The reture value of get_package_dir() is the symbol link path of
  # the package dir, the return value of get_real_package_dir() is
  # the result of os.readlink(get_package_dir())
  def get_real_package_dir(self):
    if self.instance_id == -1:
      return self.proxy.deployment.get_real_package_dir(
        self.service, self.cluster, self.job)
    else:
      return self.proxy.deployment.get_real_package_dir(
        self.service, self.cluster, self.job, self.instance_id)

  def get_current_package_dir(self):
    return self.proxy.deployment.get_current_package_dir(self.service, self.cluster)

  def bootstrap(self, artifact, force_update=False, package_name='',
      revision='', timestamp='', cleanup_token='', bootstrap_script='',
      data_dir_indexes='0', **config_files):
    '''
    Bootstrap the job.
    '''
    try:
      config_dict = {
        'artifact': artifact,
        'force_update': force_update,
        'package_name': package_name,
        'revision': revision,
        'timestamp': timestamp,
        'cleanup_token': cleanup_token,
        'bootstrap.sh': bootstrap_script,
        'data_dir_indexes': data_dir_indexes,
        'config_files': config_files,
      }
      if self.instance_id == -1:
        message = self.proxy.deployment.bootstrap(self.service, self.cluster,
          self.job, config_dict)
      else:
        message = self.proxy.deployment.bootstrap(self.service, self.cluster,
          self.job, config_dict, self.instance_id)
    except (xmlrpclib.Error, socket.error), e:
      raise e
    return message

  def start(self, artifact, force_update=False, package_name='', revision='',
      timestamp='', http_url='', start_script='', **config_files):
    '''
    Start the job.
    '''
    try:
      config_dict = {
        'start.sh': start_script,
        'artifact': artifact,
        'config_files': config_files,
        'http_url': http_url,
        'force_update': force_update,
        'package_name': package_name,
        'revision': revision,
        'timestamp': timestamp,
      }
      if self.instance_id == -1:
        message = self.proxy.deployment.start(self.service, self.cluster,
          self.job, config_dict)
      else:
        message = self.proxy.deployment.start(self.service, self.cluster,
          self.job, config_dict, self.instance_id)
    except (xmlrpclib.Error, socket.error), e:
      message = str(e)
    return message

  def stop(self):
    '''
    Stop the job.
    '''
    try:
      if self.instance_id == -1:
        message = self.proxy.deployment.stop(self.service, self.cluster,
          self.job, dict())
      else:
        message = self.proxy.deployment.stop(self.service, self.cluster,
          self.job, dict(), self.instance_id)
    except (xmlrpclib.Error, socket.error), e:
      message = str(e)
    return message

  def show(self):
    '''
    Show the running status the job.
    '''
    try:
      if self.instance_id == -1:
        message = self.proxy.deployment.show(self.service, self.cluster,
          self.job, dict())
      else:
        message = self.proxy.deployment.show(self.service, self.cluster,
          self.job, dict(), self.instance_id)
    except (xmlrpclib.Error, socket.error), e:
      message = str(e)
    return message

  def restart(self, start_script, **config_files):
    '''
    Restart the job.
    '''
    if self.stop() == 'OK':
      return self.start(start_script, **config_files)
    else:
      real_instance_id = self.instance_id
      real_instance_id = 0 if (real_instance_id == -1) else real_instance_id
      return 'Stop %s-%s-%s%s failed' % (self.service, self.cluster, self.job, real_instance_id)

  def cleanup(self, cleanup_token, cleanup_script):
    '''
    Cleanup the job's data and log directories.
    '''
    try:
      config_dict = {
        'cleanup_token': cleanup_token,
        'cleanup.sh': cleanup_script,
      }
      if self.instance_id == -1:
        message = self.proxy.deployment.cleanup(self.service, self.cluster,
          self.job, config_dict)
      else:
        message = self.proxy.deployment.cleanup(self.service, self.cluster,
          self.job, config_dict, self.instance_id)
    except (xmlrpclib.Error, socket.error), e:
      message = str(e)
    return message

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = tank_client
import httplib
import mimetypes
import os
import urllib

class TankClient:
  '''
  The package server client.
  '''
  def __init__(self, host, port=80, upload_uri='/upload_package/',
      check_uri='/check_package/'):
    self.conn = httplib.HTTPConnection(host, port)
    self.upload_uri = upload_uri
    self.check_uri = check_uri

  def check_package(self, artifact, checksum):
    '''
    Check whether a package of specified artifact and checksum already
    existed on the package server.

    @param  artifact the package artifact
    @param  checksum the package checksum
    @return string   the package infomation if the package already existed,
                     otherwise None
    '''
    data = urllib.urlencode({
        'artifact': artifact,
        'checksum': checksum,
    })

    self.conn.request('GET', '%s?%s' % (self.check_uri, data))
    response = self.conn.getresponse()

    if response.status == 200:
      body = response.read()
      if body.startswith('{'):
        return body
    return None

  def upload(self, package_path, artifact, revision):
    '''
    Upload the specified package to the package server.

    @param  package_path the package path
    @param  artifact     the package artifact
    @param  revision     the package revision
    @return integer      the http status code
    '''
    param = {
      'artifact': artifact,
      'revision': revision,
    }

    content_type, body = self._encode_multipart_formdata(param,
        [('file', package_path, open(package_path, 'rb').read())])

    headers = {
      'Content-Type': content_type,
      'Content-Length': str(len(body)),
    }

    self.conn.request('POST', self.upload_uri, body, headers)
    response = self.conn.getresponse()
    return response.status

  def _encode_multipart_formdata(self, fields, files):
    LIMIT = '----------lImIt_of_THE_fIle_eW_$'
    CRLF = '\r\n'
    L = []

    for (key, value) in fields.iteritems():
      L.append('--' + LIMIT)
      L.append('Content-Disposition: form-data; name="%s"' % key)
      L.append('')
      L.append(value)

    for (key, filename, value) in files:
      L.append('--' + LIMIT)
      L.append('Content-Disposition: form-data; name="%s"; filename="%s"' % (key, filename))
      L.append('Content-Type: %s' % self._get_content_type(filename))
      L.append('')
      L.append(value)
      L.append('--' + LIMIT + '--')
      L.append('')

    body = CRLF.join(L)
    content_type = 'multipart/form-data; boundary=%s' % LIMIT
    return content_type, body

  def _get_content_type(self, filename):
    return mimetypes.guess_type(filename)[0] or 'application/octet-stream'

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = metrics_collector_config
metrics_url = 'http://127.0.0.1:8000/monitor/metrics'
opentsdb_bin_path = 'tsdb'
opentsdb_extra_args = ''
collect_period = 10

########NEW FILE########
__FILENAME__ = business_view_config
BUSINESS_METRICS_VIEW_CONFIG = {
  'Write HBase' : [['write operation', 'Write/HBase'],
                   ['log length', 'Write/Log'],
                   ['memory/thread', 'Write/MemoryThread'],
  ],
  'Read HBase' : [['read operation', 'Read/HBase'],
                   ['result size', 'Read/ResultSize'],
                   ['memory/thread', 'Read/MemoryThread'],
  ],
}

ONLINE_METRICS_MENU_CONFIG = {
  'Online Write' : [['Qps', 'Write/Qps'],
                    ['HBase Latency', 'Write/HBase Latency'],
                    ['Total Latency', 'Write/Total Latency'],
                    ['WriteFail', 'Write/WriteFail'],
                    ['HTablePool', 'Write/HTablePool'],
                    ['Replication', 'Write/Replication'],
                    ['Exception', 'Write/Exception'],
  ],
  'Online Read' : [['Qps', 'Read/Qps'],
                   ['HBase Latency', 'Read/HBase Latency'],
                   ['Total Latency', 'Read/Total Latency'],
                   ['ReadFail', 'Read/ReadFail'],
                   ['HTablePool', 'Read/HTablePool'],
                   ['Exception', 'Read/Exception'],
  ],
}

ONLINE_METRICS_COUNTER_CONFIG = {
}

ONLINE_METRICS_TITLE_CONFIG = {
}

ONLINE_METRICS_ENDPOINT_CONFIG = {
}

########NEW FILE########
__FILENAME__ = owl_config
CHART_URL_PREFIX = 'charts'
TSDB_ADDR = 'tsdb_addr'
SUPERVISOR_PORT = '9001'

# cluster to generate quota report
QUOTA_REPORT_CLUSTER = ['dptst-example',]
# user that receive cluster quota report
QUOTA_REPORT_ADMINS = ''
# user that receive cluster quota alert
QUOTA_ALERT_ADMINS = ''
ALLERT_ADMIN_MAIL_ADDR = ''

KERBEROS_IDS_PATH = 'template/kerberos_ids.txt'

ALERT_FROM_EMAIL = 'admin'
ROBOT_EMAIL_PASSWORD = ''
SMTPHOST = 'localhost'
FAILOVER_TO_EMAIL = 'admin'

########NEW FILE########
__FILENAME__ = metrics_collector
import json
import logging
import logging.config
import os
import sys
import time
import tsdb_register
import urllib

from tsdb_register import collect_period
from tsdb_register import metrics_url
from tsdb_register import opentsdb_bin_path
from tsdb_register import opentsdb_extra_args
from tsdb_register import TsdbRegister

local_data_path = 'metrics_dump.data'

logging.config.fileConfig('metrics_logging.conf')
logger_metrics = logging.getLogger('metrics')

def verify_config():
  if not metrics_url:
    logger_metrics.warning("Please set metrics url")
    return False

  if not opentsdb_bin_path:
    logger_metrics.warning("Please set opentsdb_bin_path")
    return False

  if not collect_period:
    logger_metrics.warning("Please set collect_period")
    return False

  return True

class MetricsCollector():
  def __init__(self):
    self.tsdb_register = TsdbRegister()

  def run(self):
    while True:
      start = time.time()
      self.collect_metrics()
      self.tsdb_register.register_new_keys_to_tsdb()
      self.batch_output_to_tsdb()
      end = time.time()
      to_sleep_time = collect_period - (end - start)
      if to_sleep_time > 0:
        time.sleep(to_sleep_time)

  def collect_metrics(self):
    try:
      out_file = open(local_data_path, 'w')
      json_string = urllib.urlopen(metrics_url).read()
      metrics = json.loads(json_string)
      timestamp = metrics['timestamp']
      for endpoint, group_metrics in metrics['data'].iteritems():
        for group, key_metrics in group_metrics.iteritems():
          for key, metric in key_metrics.iteritems():
            value = metric['value']
            self.append_to_file(out_file, timestamp, key, value, endpoint, group)
            if key not in self.tsdb_register.register_keys:
              self.tsdb_register.new_keys.append(key)
              self.tsdb_register.register_keys.add(key)
      out_file.close()
    except Exception, e:
      logger_metrics.error("collect_metrics exception: %s", e)

  @staticmethod
  def append_to_file(out_file, timestamp, key, value, endpoint, group):
    # format example: metric_key 1288900000 42 host=127.0.0.1-10000 group=Master
    out_file.write("%s %s %s host=%s group=%s\n" % (key, timestamp, value, endpoint, group))


  def batch_output_to_tsdb(self):
    start_time = time.time()
    os.system('%s import %s %s' % (opentsdb_bin_path, opentsdb_extra_args, local_data_path))
    logger_metrics.info("Batch import metrics cost %f secs" % (time.time() - start_time))

if __name__ == '__main__':
  if not verify_config():
    sys.exit(-1)

  collector = MetricsCollector()
  collector.run()

########NEW FILE########
__FILENAME__ = tsdb_register
import logging
import os
import sys
import time

root_path = os.path.abspath(
  os.path.dirname(os.path.realpath(__file__))+ '/..')

client_path = os.path.join(root_path, 'client')
sys.path.append(client_path)

deploy_utils = __import__('deploy_utils')
conf_path = deploy_utils.get_config_dir()

opentsdb_config_path = os.path.join(conf_path, 'opentsdb')
sys.path.append(opentsdb_config_path)
metrics_collector_config = __import__('metrics_collector_config')

metrics_url = metrics_collector_config.metrics_url
opentsdb_bin_path = metrics_collector_config.opentsdb_bin_path
opentsdb_extra_args = metrics_collector_config.opentsdb_extra_args
collect_period = metrics_collector_config.collect_period

logger_metrics = logging.getLogger('metrics')
logger_quota = logging.getLogger('quota')


class TsdbRegister:
  def __init__(self):
    self.new_keys = []
    self.register_keys = set()

  def register_new_keys_to_tsdb(self):
    start_time = time.time()

    size = len(self.new_keys)
    offset = 0
    MAX_REGISTERED_KEYS = 1000;

    # register MAX_REGISTERED_KEYS one time
    while size - offset >= MAX_REGISTERED_KEYS:
      keys_to_add = self.new_keys[offset:offset+MAX_REGISTERED_KEYS]
      mkmetric_operation = '%s mkmetric %s %s' % (opentsdb_bin_path, opentsdb_extra_args, ' '.join(keys_to_add))
      logger_metrics.info(mkmetric_operation)
      logger_quota.info(mkmetric_operation)
      os.system(mkmetric_operation)
      offset += MAX_REGISTERED_KEYS

    # register remainings
    if offset < size:
      keys_to_add = self.new_keys[offset:]
      mkmetric_operation = '%s mkmetric %s %s' % (opentsdb_bin_path, opentsdb_extra_args, ' '.join(keys_to_add))
      logger_metrics.info(mkmetric_operation)
      logger_quota.info(mkmetric_operation)
      os.system(mkmetric_operation)

    self.new_keys = []
    registered_metrics_log = "Registered %d metrics cost %f secs" % (size, time.time() - start_time)
    logger_metrics.info(registered_metrics_log)
    logger_quota.info(registered_metrics_log)

########NEW FILE########
__FILENAME__ = alert
import ConfigParser
import argparse
import datetime
import json
import logging
import os
import smtplib
import sys
import time
import utils.mail

import deploy_utils

from optparse import make_option
from os import path

from django.conf import settings
from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone

from monitor.models import Status, Service, Cluster, Job, Task

BOOL_METRIC_MAP = {
    "tag.IsOutOfSync": "true",
    "tag.HAState": "active",
}

STATUS_FILE_PATH = 'cluster.status'
# alert when cluster is not OK for ERROR_TIMES_FOR_ALERT
ERROR_TIMES_FOR_ALERT = 3

logger = logging.getLogger(__name__)

class CollectorConfig:
  class Service:
    def __init__(self, options, config, name):
      # Parse service config.
      self.name = name
      self.jobs = config.get(name, "jobs").split()
      self.clusters = {}
      for cluster_name in config.get(name, "clusters").split():
        args = argparse.Namespace()
        args.service = self.name
        args.cluster = cluster_name
        # Parse cluster config.
        self.clusters[cluster_name] = deploy_utils.get_service_config(args)
      self.metric_url = config.get(name, "metric_url")

  def __init__(self, args, options):
    # Parse collector config.
    config_path = os.path.join(deploy_utils.get_config_dir(), 'owl/collector.cfg')
    self.args = args
    self.options = options
    self.config = self.parse_config_file(config_path)
    self.services = {}
    for service_name in self.config.get("collector", "services").split():
      self.services[service_name] = CollectorConfig.Service(
          options, self.config, service_name)
    self.period = self.config.getint("collector", "period")

  def parse_config_file(self, config_path):
    config_parser = ConfigParser.SafeConfigParser()
    config_parser.optionxform = str
    logger.info("Parsing config file: %s", config_path)
    if not config_parser.read(config_path):
      logger.critical("Can't parse config file: %s", config_path)
      sys.exit(1)
    logger.info("Successfully parsed config file")
    return config_parser

class StatusChecker:
  """Check status of all active clusters and jobs, which are inferred from
  tasks' status."""

  def __init__(self, collector_config, last_status, options, mailer):
    self.collector_config = collector_config
    self.last_status = last_status
    self.options = options
    self.alert_msg = ''
    self.mailer = mailer

  def get_latest_metric(self, task, group_name, metric_name):
    try:
      metric = json.loads(task.last_metrics)
      return metric[group_name][metric_name]
    except Exception as e:
      logger.warning("%r failed to get metric: %r", task, e)
      return 0

  def is_namenode_active(self, task):
    try:
      metric = self.get_latest_metric(
          task, "Hadoop:service=NameNode,name=FSNamesystem", "tag.HAState")
      return bool(metric)
    except Exception as e:
      logger.warning("%r failed to get metric: %r", task, e)
      return False

  def is_master_active(self, task):
    try:
      metric = self.get_latest_metric(
          task, "hadoop:service=Master,name=Master", "IsActiveMaster")
      return bool(metric)
    except Exception as e:
      logger.warning("%r failed to get metric: %r", task, e)
      return False

  def check_hdfs_cluster_status(self, cluster):
    job = cluster.jobs["journalnode"]
    if (job.running_tasks_count < 2 or
        job.running_tasks_count < (job.total_tasks_count / 2 + 1)):
      job.last_status = Status.ERROR
      job.last_message = "Too few running journalnodes!"

    job = cluster.jobs["namenode"]
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "No running namenodes!"
    else:
      active = 0
      for task in job.running_tasks.itervalues():
        if self.is_namenode_active(task):
          # update cluster entry
          cluster.entry = '%s:%d' % (task.host, task.port)
          cluster.version = self.get_latest_metric(task,
                                                   'Hadoop:service=NameNode,name=NameNodeInfo',
                                                   'Version')
          active += 1
      if active > 1:
        job.last_status = Status.ERROR
        job.last_message = "Too many active namenodes!"
      elif active < 1:
        job.last_status = Status.ERROR
        job.last_message = "No active namenodes!"
      elif job.running_tasks_count < 2:
        job.last_status = Status.WARN
        job.last_message = "Less than 2 running namenodes, no HA guarantee"

    job = cluster.jobs["datanode"]
    if job.running_tasks_count < 3:
      job.last_status = Status.ERROR
      job.last_message = "Too few running datanodes!"
    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])

  def check_hbase_cluster_status(self, cluster):
    job = cluster.jobs["master"]
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "No running masters!"
    else:
      active = 0
      for task in job.running_tasks.itervalues():
        if self.is_master_active(task):
          # update cluster entry
          cluster.entry = '%s:%d' % (task.host, task.port)
          version = self.get_latest_metric(task,
                                           'hadoop:service=HBase,name=Info',
                                           'version')
          revision = self.get_latest_metric(task,
                                           'hadoop:service=HBase,name=Info',
                                           'revision')
          cluster.version = '%s, r%s' % (version, revision)
          active += 1
      if active > 1:
        job.last_status = Status.ERROR
        job.last_message = "Too many active masters!"
      elif active < 1:
        job.last_status = Status.ERROR
        job.last_message = "No active masters!"
      elif job.running_tasks_count < 2:
        # TODO: Now it always reports warning as backup master doesn't run a http
        # server before it acquires zk lock. Comment this out and would change
        # master's startup workflow.
        #job.last_status = Status.WARN
        #job.last_message = "Less than 2 running masters, no HA guarantee"
        pass

    job = cluster.jobs["regionserver"]
    if job.running_tasks_count < 3:
      job.last_status = Status.ERROR
      job.last_message = "Too few running regionservers!"
    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])

  def check_yarn_cluster_status(self, cluster):
    job = cluster.jobs["resourcemanager"]
    for task in job.running_tasks.itervalues():
      # update cluster entry
      cluster.entry = '%s:%d' % (task.host, task.port)
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "No running resourcemanager!"

    job = cluster.jobs["proxyserver"]
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "No running proxyserver!"

    job = cluster.jobs["nodemanager"]
    if job.running_tasks_count < 3:
      job.last_status = Status.ERROR
      job.last_message = "Too few running nodemanager!"
    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])

    job = cluster.jobs["historyserver"]
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "Too few running historyserver!"
    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])

  def check_impala_cluster_status(self, cluster):
    job = cluster.jobs["statestored"]
    for task in job.running_tasks.itervalues():
      # update cluster entry
      cluster.entry = '%s:%d' % (task.host, task.port)
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "No running statestored!"

    job = cluster.jobs["impalad"]
    if job.running_tasks_count < 3:
      job.last_status = Status.ERROR
      job.last_message = "Too few running impalad!"
    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])

  def check_cluster_status(self, cluster):
    cluster.jobs = {}
    cluster.last_status = Status.OK
    cluster.last_message = ""

    for job in cluster.job_set.all():
      job.running_tasks = {}
      job.tasks = {}
      job.last_status = Status.OK
      job.last_message = ""
      job.running_tasks_count = 0
      job.total_tasks_count = 0
      for task in job.task_set.filter(active=True):
        if task.health:
          job.running_tasks[task.id] = task
          job.running_tasks_count += 1
        job.total_tasks_count += 1
      cluster.jobs[job.name] = job

    service_handler = {
        "hdfs": self.check_hdfs_cluster_status,
        "hbase": self.check_hbase_cluster_status,
        "yarn": self.check_yarn_cluster_status,
        "impala": self.check_impala_cluster_status,
    }
    service_handler[cluster.service.name](cluster)
    self.handle_status_result(cluster)

  def handle_status_result(self, cluster):
    # last_status store cluster_name->(status, status_times)
    (cluster_status, status_times) = self.last_status.setdefault(str(cluster), (Status.OK, 0))
    need_send_alert = False

    if cluster.last_status != cluster_status:
      self.last_status[str(cluster)] = (cluster.last_status, 1)
      if cluster.last_status == Status.OK and status_times >= ERROR_TIMES_FOR_ALERT:
        # send alert when cluster changed to from PROBLEM(alerted) to OK
        need_send_alert = True
    else:
      self.last_status[str(cluster)] = (cluster.last_status, status_times+1)
      # send alert when cluster in PROBLEM stutus reached ERROR_TIMES_FOR_ALERT times
      if cluster.last_status != Status.OK and status_times + 1 == ERROR_TIMES_FOR_ALERT:
        need_send_alert = True

    if need_send_alert:
      self.alert_msg += '[%s]Cluster[%s]\n' \
          % ('OK' if cluster.last_status == Status.OK else 'PROBLEM',
             cluster)
      for job in cluster.jobs.itervalues():
        if job.last_status != Status.OK:
          self.alert_msg += 'Job[%s] not healthy: %s\n' % (job.name, job.last_message)
      self.alert_msg += '******\n'


  def check_status(self):
    self.alert_msg = ''
    logger.info("checking clusters status")

    self.start_time = time.time()
    for cluster in Cluster.objects.filter(active=True).all():
      self.check_cluster_status(cluster)
    logger.info("spent %f seconds for updating clusters status",
        time.time() - self.start_time)
    if self.alert_msg:
      logger.warn('alert msg: %r' % self.alert_msg)
      self.mailer.send_email(subject = 'OWL cluster alert',
                             content = self.alert_msg,
                             to_email = self.options['to_email'])
    json.dump(self.last_status, open(STATUS_FILE_PATH, 'w'))

class Command(BaseCommand):
  args = ''
  help = "Run the background collector to fetch metrics from /jmx on each server."

  option_list = BaseCommand.option_list + (
      make_option(
        "--to_email",
        help="Email address to"),
      make_option(
        "--period",
        default=60,
        help="Check period"),
  )

  def handle(self, *args, **options):
    self.args = args
    self.options = options
    self.mailer = utils.mail.Mailer(options)

    self.stdout.write("args: %r\n" % (args, ))
    self.stdout.write("options: %r\n" % options)

    self.collector_config = CollectorConfig(self.args, self.options)

    self.last_status = {}
    try:
      self.last_status = json.load(open(STATUS_FILE_PATH, 'r'))
    except Exception as e:
      logger.warning('Failed to load status file: %r', e)

    status_checker = StatusChecker(self.collector_config,
                                   self.last_status,
                                   self.options,
                                   self.mailer)

    while True:
      try:
        status_checker.check_status()
      except Exception as e:
        logger.warning('OWL cluster checker error: %r', e)
        # send alert email when program got error
        admin_email = ''
        try:
          admin_email = settings.ADMINS[0][1]
        except:
          pass
        self.mailer.send_email(subject = 'OWL cluster check error',
                               content = repr(e),
                               to_email = admin_email,
                              )
      time.sleep(int(self.options['period']))

########NEW FILE########
__FILENAME__ = admin
from django.contrib import admin
from models import Business

admin.site.register(Business)

########NEW FILE########
__FILENAME__ = models
# -*- coding: utf-8 -*-

from django.db import models

class Business(models.Model):
  # The buisness name, sms/miliao/...
  business = models.CharField(max_length=32)
  # The hbase cluster name
  cluster = models.CharField(max_length=32)
  # The tables of business
  tables = models.CharField(max_length=32)
  # The server write or read data from hbase
  access_server = models.TextField()
  # The discription
  description = models.TextField()

  def getCounterGroup(self):
    return u"infra-hbase-business-%s" % (self.business)

  def __unicode__(self):
    return u"%s/%s" % (self.business, self.cluster)

########NEW FILE########
__FILENAME__ = urls
# -*- coding: utf-8 -*-

from django.conf.urls import patterns, url
import views

urlpatterns = patterns(
  '',
  url(r'^$', views.index),
  url(r'(?P<id>1)/(?P<access_type>[^/]+)/(?P<label>[^/]+)', views.show_online),
  url(r'(?P<id>2)/(?P<access_type>[^/]+)/(?P<label>[^/]+)', views.show_business),
)

########NEW FILE########
__FILENAME__ = views
# -*- coding: utf-8 -*-

from models import Business
from monitor.views import respond
from monitor.dbutil import get_counters_by_group_and_label
from business_view_config import BUSINESS_METRICS_VIEW_CONFIG
from business_view_config import ONLINE_METRICS_MENU_CONFIG
from business_view_config import ONLINE_METRICS_COUNTER_CONFIG
from business_view_config import ONLINE_METRICS_TITLE_CONFIG
from business_view_config import ONLINE_METRICS_ENDPOINT_CONFIG

#define access type
ACCESS_TYPE_WRITE = 'Write'
ACCESS_TYPE_READ = 'Read'
#define hbase operation : operation/qps/success_rate
HBASE_OPERATION_LABEL = 'HBase'
QPS_LABEL = 'Qps'
SUCCESS_RATE_LABEL = 'SuccessRate'

def index(request):
  # show all business
  businesses = Business.objects.all()
  params = {
    'businesses': businesses,
  }
  return respond(request, 'business/index.html', params)

def get_latency_counter_name(success_rate_counter_name):
  return success_rate_counter_name.replace('_Qps', '_Latency')

def get_success_rate_counter_name(success_rate_counter_name):
  return success_rate_counter_name.replace('_Qps', '_SuccessRate')

def get_counter_name(group, access_type, label):
  label = access_type + "_" + label
  counters = get_counters_by_group_and_label(group, label)
  names = []
  print label
  print counters
  for counter in counters:
    names.append(group + "-" + counter.name)
  return names

def get_counter_name_of_hbase_operation(group, access_type):
  label = access_type + "_" + QPS_LABEL
  qps_counters = get_counters_by_group_and_label(group, label)
  #order by qps desc
  qps_counters = sorted(qps_counters,cmp=lambda x,y:cmp(y.value,x.value))

  #return countrs as : latency, qps and success_rate order by success_rate desc
  counter_names = []
  for qps_counter in qps_counters:
    latency_counter_name = get_latency_counter_name(qps_counter.name)
    success_rate_counter_name = get_success_rate_counter_name(qps_counter.name)
    counter_names.append(group + '-' + qps_counter.name)
    counter_names.append(group + '-' + latency_counter_name)
    counter_names.append(group + '-' + success_rate_counter_name)
  return counter_names

def get_endpoint(group, access_type):
  endpoint = 'unknown'
  label = access_type + "_" + HBASE_OPERATION_LABEL
  counters = get_counters_by_group_and_label(group, label)
  for counter in counters:
    endpoint = counter.host
    endpoint = endpoint.replace(':', '-')
    break
  return endpoint

class Menu:
  def __init__(self, name, path):
    self.name = name
    self.path = path
  def __unicode__(self):
    return u"%s/%s" % (self.name, self.path)

#url: /business/$id/$access_type/$label
def show_business(request, id, access_type, label):
  business = Business.objects.get(id=id)
  group = business.getCounterGroup()
  endpoint = get_endpoint(group, access_type)
  metric_names = []
  if label == HBASE_OPERATION_LABEL:
    metric_names = get_counter_name_of_hbase_operation(group, access_type)
  else:
    metric_names = get_counter_name(group, access_type, label)

  params = {
    'business_id' : id,
    'endpoint': endpoint,
    'write_menus' : BUSINESS_METRICS_VIEW_CONFIG['Write HBase'],
    'read_menus' : BUSINESS_METRICS_VIEW_CONFIG['Read HBase'],
    'metric_names' : metric_names,
    'business': business,
  }
  return respond(request, 'business/business.html', params)

def get_online_counters(access_type, label):
  metric_names = ONLINE_METRICS_COUNTER_CONFIG['Online ' + access_type][label]
  titles = ONLINE_METRICS_TITLE_CONFIG['Online ' + access_type][label]
  endpoints = ONLINE_METRICS_ENDPOINT_CONFIG['Online ' + access_type][label]
  metrics = []
  index = 0
  for name in metric_names:
    metric = []
    metric.append(titles[index])
    metric.append(name)
    metric.append(endpoints[index])
    metrics.append(metric)
    index = index + 1
  return metrics

#url: /business/$id/$access_type/$label
def show_online(request, id, access_type, label):
  business = Business.objects.get(id=id)
  metrics = get_online_counters(access_type, label)

  params = {
    'business_id' : id,
    'write_menus' : ONLINE_METRICS_MENU_CONFIG['Online Write'],
    'read_menus' : ONLINE_METRICS_MENU_CONFIG['Online Read'],
    'metrics' : metrics,
    'business': business,
  }
  return respond(request, 'business/online.html', params)

########NEW FILE########
__FILENAME__ = collect
import argparse
import ConfigParser
import datetime
import json
import logging
import os
import random
import sys
import threading
import time
import urllib2

import deploy_utils

from optparse import make_option
from os import path

from django.db import transaction
from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone

from monitor import dbutil
from monitor import metric_helper
from monitor.models import Status
from monitor.models import Service, Cluster, Job, Task, RegionServer, Table, Region, HBaseCluster

from twisted.internet import reactor
from twisted.web import client

import socket

# now, we use multi-threads to read/write from/to db; if expect only one-thread, using the following config
#reactor.suggestThreadPoolSize(1)

# For debugging
import gc
import resource
import traceback

BOOL_METRIC_MAP = {
    "tag.IsOutOfSync": "true",
    "tag.HAState": "active",
}

REGION_SERVER_DYNAMIC_STATISTICS_BEAN_NAME = 'hadoop:service=RegionServer,name=RegionServerDynamicStatistics'
REGION_SERVER_BEAN_NAME = 'hadoop:service=RegionServer,name=RegionServer'
REGION_SERVER_REPLICATION_BEAN_NAME_PREFIX = 'hadoop:service=Replication,name=ReplicationSource for'

# TODO: move these suffix definition to monitor/metric_help.py
OPERATION_NUM_OPS = 'NumOps'
OPERATION_AVG_TIME = 'AvgTime'
OPERATION_MIN_TIME = 'MinTime'
OPERATION_MAX_TIME = 'MaxTime'
OPERATION_TOTAL_TIME = 'TotalTime'

logger = logging.getLogger(__name__)

class CollectorConfig:
  class Service:
    def __init__(self, options, config, name):
      # Parse service config.
      self.name = name
      self.jobs = config.get(name, "jobs").split()
      self.clusters = {}
      for cluster_name in config.get(name, "clusters").split():
        args = argparse.Namespace()
        args.service = self.name
        args.cluster = cluster_name
        # Parse cluster config.
        self.clusters[cluster_name] = deploy_utils.get_service_config(args)
      self.metric_url = config.get(name, "metric_url")
      self.need_analyze = True # analyze for default
      if config.has_option(name, "need_analyze"):
        self.need_analyze = config.getboolean(name, "need_analyze")

  def __init__(self, args, options):
    # Parse collector config.
    self.options = options
    config_path = os.path.join(deploy_utils.get_config_dir(), 'owl', self.options['collector_cfg'])
    self.args = args
    self.config = self.parse_config_file(config_path)
    self.services = {}
    for service_name in self.config.get("collector", "services").split():
      self.services[service_name] = CollectorConfig.Service(
          options, self.config, service_name)
    self.period = self.config.getint("collector", "period")

  def parse_config_file(self, config_path):
    config_parser = ConfigParser.SafeConfigParser()
    config_parser.optionxform = str
    logger.info("Parsing config file: %s", config_path)
    if not config_parser.read(config_path):
      logger.critical("Can't parse config file: %s", config_path)
      sys.exit(1)
    logger.info("Successfully parsed config file")
    return config_parser


class MetricObjectCache:
  cache = {}
  lock = threading.Lock()

  @classmethod
  def get(cls, group_name, metric_name):
    with cls.lock:
      # The group doesn't necessarily exist, set it to an empty dictionary if it
      # doesn't exist.
      group = cls.cache.setdefault(group_name, {})
      metric = group.get(metric_name, None)
      if not metric:
        logger.info(
            "cache missing for %s/%s, try to get or create from database",
            group_name, metric_name)
        # get_or_create is not thread safe. But in the Metric table, we set
        # group-metric to be unique. So only one save will be success, the
        # others will throw IntegrityError and get_or_create will catch it and
        # try to get again
        # NOTE: get_or_create seems to have problem with its internally using
        # of savepoint functions, now we call get_or_create after acquiring lock
        # and make sure it's executed sequentially.
        start_time = time.time()
        metric, created = Metric.objects.get_or_create(
            group=group_name, metric=metric_name)
        logger.info("%s/%s spent %f seconds, created: %r",
            group_name, metric_name, time.time() - start_time, created)
        group[metric_name] = metric
      return metric


class MetricSource:
  def __init__(self, collector_config, task):
    self.collector_config = collector_config
    self.task = task
    self.url = "http://%s:%d%s" % (
        task.host, task.port, task.job.cluster.service.metric_url)
    self.need_analyze = collector_config.services[task.job.cluster.service.name].need_analyze
    self.aggregated_metrics_key = ['memStoreSizeMB',
                                   'storefileSizeMB',
                                   'readRequestsCount',
                                   'writeRequestsCount',
                                   'readRequestsCountPerSec',
                                   'writeRequestsCountPerSec',
                                  ]

  def schedule_next_fetch(self):
    next_time = self.start_time + self.collector_config.period
    end_time = time.time()
    if end_time < next_time:
      wait_time = next_time - end_time
      logger.info(
          "%r waiting %f seconds for %s..." , self.task, wait_time, self.url)

      # reactor.callLater is NOT thread-safe but reactor.callFromThread is, so
      # we put the callLater to the main loop.
      reactor.callFromThread(reactor.callLater, wait_time, self.fetch_metrics)
    else:
      # We are behind the schedule, fetch the metrics right away.
      reactor.callFromThread(self.fetch_metrics)

  def fetch_metrics(self):
    logger.info("%r fetching %s...", self.task, self.url)
    self.start_time = time.time()
    # Always use utc time with timezone info, see:
    # https://docs.djangoproject.com/en/1.4/topics/i18n/timezones/#naive-and-aware-datetime-objects
    self.task.last_attempt_time = datetime.datetime.utcfromtimestamp(
        self.start_time).replace(tzinfo=timezone.utc)
    client.getPage(str(self.url),
        timeout=self.collector_config.period - 1,
        followRedirect=False).addCallbacks(
            callback=self.success_callback,
            errback=self.error_callback)

  def success_callback(self, data):
    logger.info("%r fetched %d bytes", self.task, len(data))
    try:
      # Save the raw data before passing it, in case the data is invalid and
      # throws an exception.
      self.task.last_metrics_raw = data
      self.task.last_status = Status.OK
      self.task.last_message = "Success"
      self.task.last_success_time = self.task.last_attempt_time
      self.update_metrics(data)

    except Exception as e:
      logger.warning("%r failed to process result: %r", self.task, e)
      self.schedule_next_fetch()

  def error_callback(self, error):
    logger.warning("%r failed to fetch: %r", self.task, error)
    try:
      self.task.last_status = Status.ERROR
      self.task.last_message = "Error: %r" % error
      self.update_metrics(None)
    except Exception as e:
      logger.warning("%r failed to process error: %r", self.task, e)
      self.schedule_next_fetch()

  def analyze_metrics(self, metrics):
    if 'beans' not in metrics:
      return
    # analyze hbase metric
    if self.task.job.cluster.service.name == 'hbase':
      start_time = time.time()
      if self.task.job.name == 'master':
        self.analyze_hbase_master_metrics(metrics)
      elif self.task.job.name == 'regionserver':
        self.analyze_hbase_region_server_metrics(metrics)

      logger.info("%r spent %f seconds for analyzing metrics for hbase",
                  self.task, time.time() - start_time)

  def analyze_hbase_region_server_metrics(self, metrics):
    region_server_name = None
    region_operation_metrics_dict = {}
    replication_metrics_dict = {}
    for bean in metrics['beans']:
      try:
        # because root and meta region have the names, we must use region server
        # name and region name to locate a region
        if bean['name'] == REGION_SERVER_BEAN_NAME:
          region_server_name = bean['ServerName']
        elif bean['name'] == REGION_SERVER_DYNAMIC_STATISTICS_BEAN_NAME:
          for metricName in bean.keys():
            if Region.is_region_operation_metric_name(metricName):
              encodeName = Region.get_encode_name_from_region_operation_metric_name(metricName)
              region_operation_metrics = region_operation_metrics_dict.setdefault(encodeName, {})
              region_operation_metrics[metricName] = bean[metricName]
        elif bean['name'].startswith(REGION_SERVER_REPLICATION_BEAN_NAME_PREFIX):
          peerId = metric_helper.parse_replication_source(bean['name'])
          replication_metrics = replication_metrics_dict.setdefault(peerId, {})
          for metricName in bean.keys():
            replication_metrics[metricName] = bean[metricName]
      except Exception as e:
        logger.warning("%r failed to analyze metrics: %r", self.task, e)
        continue

    region_server = None
    if region_server_name is None:
      return
    else:
      try:
        region_server = RegionServer.objects.get(name = region_server_name)
      except RegionServer.DoesNotExist:
        logger.warning("%r failed to find region_server with region_server_name=%s", self.task, region_server_name)
        return

    # save replication metrics for region server
    region_server.replication_last_attempt_time = self.task.last_attempt_time
    region_server.replicationMetrics = json.dumps(replication_metrics_dict)
    region_server.save()

    region_record_need_save = []
    for encodeName, operationMetrics in region_operation_metrics_dict.iteritems():
      region_record = dbutil.get_region_by_regionserver_and_encodename(region_server, encodeName)
      # we must wait region saved after analyzing master task
      if region_record is None:
        continue
      region_record.analyze_from_region_server_operation_metrics(operationMetrics,
                                                                 self.task.last_attempt_time)
      # we first buffer the regions needed to update, then do batch update
      region_record_need_save.append(region_record)

    # we do batch update
    begin = datetime.datetime.now()
    dbutil.update_regions_for_region_server_metrics(region_record_need_save)
    logger.info("%r batch save region record for region_server, saved regions=%d, consume=%s",
        self.task, len(region_record_need_save), str((datetime.datetime.now() - begin).total_seconds()))

  def get_host_and_port_from_region_server_name(self, rs_name):
    # rs name format is formatted as : host_name,port,start_code.
    # for some cluster, the format may be : host_ip,port,start_code.
    # we will try to convert host_ip to coprresonding host_name
    # because we always try to save host_name and port to identity a task
    # except that we can't get host_name from host_ip
    tokens = rs_name.split(',')
    host = tokens[0] # may be host_name or host_ip
    host_name = None
    try:
      host_name = socket.gethostbyaddr(host)[0]
    except:
      logger.warning("can't get host_name for host=%s", host)
      host_name = host
    # jmx port is rs_port + 1, host and jmx port will identify a task
    port = int(tokens[1]) + 1
    return [host_name, port]

  def analyze_hbase_master_metrics(self, metrics):
    cluster = self.task.job.cluster
    hbase_cluster_record, created = HBaseCluster.objects.get_or_create(cluster = cluster)
    self.reset_aggregated_metrics(hbase_cluster_record)
    tables = {}
    region_record_need_save = []
    for bean in metrics['beans']:
      try:
        if 'RegionServers' not in bean:
          continue
        for rs_metrics in bean['RegionServers']:
          rs_name = rs_metrics['key']
          [rs_hostname, rs_port] = self.get_host_and_port_from_region_server_name(rs_name)
          rs_task = dbutil.get_task_by_host_and_port(rs_hostname, rs_port)
          rs_record, created = RegionServer.objects.get_or_create(cluster = cluster,
                                                                  task = rs_task)
          # region server name includes startTime, which means the same region server
          # will lead different RegionServer records if the region server restarts.
          # Therefore, we won't create region server by its name.
          rs_record.name = rs_name

          rs_value = rs_metrics['value']
          rs_record.last_attempt_time = self.task.last_attempt_time
          rs_record.load = int(rs_value['load'])
          rs_record.numberOfRegions = int(rs_value['numberOfRegions'])
          self.reset_aggregated_metrics(rs_record)

          # we read out all regions belong to this region server and build a map
#          all_regions_in_rs = Region.objects.filter(region_server = rs_record)
          all_regions_in_rs = dbutil.get_alive_regions_by_rs(rs_record)
          all_regions_map = {}
          logger.info("%r Finish get region: %d", self.task, len(all_regions_in_rs))
          for region in all_regions_in_rs:
            all_regions_map[region.name] = region

          regionsLoad = rs_value['regionsLoad']
          for region_metrics in regionsLoad:
            region_value = region_metrics['value']
            region_name = region_value['nameAsString']
            try:
              table_name, startkey, region_id = region_name.split(',')
            except Exception as e:
              logger.warning("%r failed to get region name: %r, %s", self.task, e, region_name)
              continue

            region_metrics = {}

            if table_name not in tables:
              table_record, created = Table.objects.get_or_create(cluster = cluster,
                                                                  name = table_name)
              self.reset_aggregated_metrics(table_record)
              tables[table_name] = table_record

            table_record = tables[table_name]

            region_record = None
            if region_name in all_regions_map:
              region_record = all_regions_map[region_name]
            else:
              # if region record not in buffer, we get_or_create from db
              begin = datetime.datetime.now()
              region_record, created = Region.objects.get_or_create(table = table_record,
                                                                    name = region_name,
                                                                    encodeName = Region.get_encode_name(region_name),
                                                                    defaults={"region_server":rs_record})
              logger.info("%r get_or_create region in region_server from mysql, consume=%s, region_name=%s, buffered_rs=%s, get_rs=%s",
                self.task, str((datetime.datetime.now() - begin).total_seconds()), region_name, rs_record.name, region_record.region_server.name)


            logger.info("%r Finish analyze regionsLoad", self.task)

            region_record.region_server = rs_record
            region_record.analyze_region_record(region_value, self.task.last_attempt_time)
            # we buffer the regions needed update for batch update
            region_record_need_save.append(region_record)
            self.aggregate_metrics(region_record, rs_record)
            self.aggregate_metrics(region_record, table_record)
            self.aggregate_metrics(region_record, hbase_cluster_record)

          rs_record.save()

        for table_record in tables.itervalues():
          table_record.last_attempt_time = self.task.last_attempt_time
          table_record.availability = dbutil.getTableAvailability(table_record.cluster.name, table_record.name)
          table_record.save()

        hbase_cluster_record.save()

        # do batch update
        begin = datetime.datetime.now()
        dbutil.update_regions_for_master_metrics(region_record_need_save)
        logger.info("%r batch save region record for master, saved regions=%d, consume=%s", self.task,
            len(region_record_need_save), str((datetime.datetime.now() - begin).total_seconds()))
      except Exception as e:
        traceback.print_exc()
        logger.warning("%r failed to analyze metrics: %r", self.task, e)
        continue

  def reset_aggregated_metrics(self, record):
    for key in self.aggregated_metrics_key:
      setattr(record, key, 0)

  def aggregate_metrics(self, from_record, to_record):
    for key in self.aggregated_metrics_key:
      old_value = getattr(to_record, key)
      setattr(to_record, key, old_value + getattr(from_record, key))

  def update_metrics(self, metricsRawData):
    try:
      reactor.callInThread(self.update_metrics_in_thread, metricsRawData)
    except Eception, e:
      self.schedule_next_fetch()

  def update_metrics_in_thread(self, metricsRawData):
    try:
      logger.info("%r updating metrics, "
          "%d task in queue, %d workers, %d total threads",
          self.task,
          reactor.getThreadPool().q.qsize(),
          len(reactor.getThreadPool().working),
          len(reactor.getThreadPool().threads))

      start_time = time.time()
      # analyze the metric if needed
      if self.need_analyze:
        if metricsRawData:
          metrics = json.loads(metricsRawData)
          metrics_saved = {}
          for bean_output in metrics["beans"]:
            bean_name = bean_output["name"]
            for metric_name, metric_value in bean_output.iteritems():
              if metric_name in ["name", "modelerType"]: continue

              metric_type = type(metric_value)
              # Do some hadoop/hbase specific work :)
              if metric_name in BOOL_METRIC_MAP:
                metric_value = int(metric_value == BOOL_METRIC_MAP[metric_name])
              elif metric_type is list or metric_type is dict:
                # Just store the length.
                metric_value = len(metric_value)
              elif metric_type is bool:
                metric_value = int(metric_value)
              elif metric_value is None:
                metric_value = 0
              elif not (metric_type is int or metric_type is float
                        or metric_type is unicode or metric_type is str):
                logger.warning("Unexpected metric type %s/%s: %r/%r",
                    bean_name, metric_name, metric_type, metric_value)
                continue

              # TODO: comment this out temporarily, remove it forever if we don't
              # want to use it.
              #metric = MetricObjectCache.get(bean_name, metric_name)
              group = metrics_saved.setdefault(bean_name, {})
              group[metric_name] = metric_value
          self.task.last_metrics = json.dumps(metrics_saved)

          self.analyze_metrics(metrics)

      self.task.save()
      logger.info("%r spent %f seconds for saving task status",
                  self.task, time.time() - start_time)
    except Exception, e:
      logger.warning("%r failed to update metric: %r", self.task, e)
      traceback.print_exc()
    finally:
      self.schedule_next_fetch()

# Region operation include : get, multiput, multidelete, checkAndPut, BulkDelete etc.
# one region operation include operation_NumOps, operation_AvgTime, operation_MaxTime and
# operation.MinTime. We aggregate operation metrics of regions to compute operation metrics
# for table and cluster
class RegionOperationMetricAggregator:
  def __init__(self, collector_config):
    self.collector_config = collector_config

  def aggregate_region_operation_metric(self):
    reactor.callInThread(self.aggregate_region_operation_metric_in_thread)

  def aggregate_region_operation_metric_in_thread(self):
    try:
      self.last_aggregate_time = time.time()
      self.aggregate_region_operation_metric_for_table_and_cluster()
    except Exception as e:
      logger.warning("failed to aggregate region operation metric:%r", e)
    finally:
      self.schedule_next_aggregation()
    return

  def make_empty_operation_metric(self):
    operationMetric = {}
    operationMetric[OPERATION_NUM_OPS] = 0
    operationMetric[OPERATION_TOTAL_TIME] = 0
    operationMetric[OPERATION_MAX_TIME] = 0
    operationMetric[OPERATION_MIN_TIME] = sys.maxint
    return operationMetric

  def aggregate_one_region_operation_metric(self, aggregateMetric, deltaMetric):
    if OPERATION_NUM_OPS in deltaMetric:
      aggregateMetric[OPERATION_NUM_OPS] += deltaMetric[OPERATION_NUM_OPS]
      aggregateMetric[OPERATION_TOTAL_TIME] += deltaMetric[OPERATION_AVG_TIME] * deltaMetric[OPERATION_NUM_OPS]
      if aggregateMetric[OPERATION_MAX_TIME] < deltaMetric[OPERATION_MAX_TIME]:
        aggregateMetric[OPERATION_MAX_TIME] = deltaMetric[OPERATION_MAX_TIME]
      if aggregateMetric[OPERATION_MIN_TIME] > deltaMetric[OPERATION_MIN_TIME]:
        aggregateMetric[OPERATION_MIN_TIME] = deltaMetric[OPERATION_MIN_TIME]

  def compute_avg_time_and_num_ops_after_aggregation(self, operationMetrics):
    for operationName in operationMetrics.keys():
      if operationMetrics[operationName][OPERATION_NUM_OPS] > 0:
        # now, region operation metric will be collect every 10 seconds, the orignal ops is the sum of ops during 10 seconds
        operationMetrics[operationName][OPERATION_AVG_TIME] = \
          operationMetrics[operationName][OPERATION_TOTAL_TIME] / operationMetrics[operationName][OPERATION_NUM_OPS]
        operationMetrics[operationName][OPERATION_NUM_OPS] = operationMetrics[operationName][OPERATION_NUM_OPS] / 10
      else:
        operationMetrics[operationName][OPERATION_AVG_TIME] = 0

  def aggregate_region_operation_metric_for_table_and_cluster(self):
    allClusterOperationMetric = {}
    # because the number of regions could be huge. We read out region operation metrics
    # by table, then table operation metrics and cluster operation metrics could be aggregated
    tables = Table.objects.all()
    for table in tables:
      clusterName = table.cluster.name
      clusterOperationMetric = allClusterOperationMetric.setdefault(clusterName, {})
      tableOperationMetric = {}
      regions = dbutil.get_region_by_table(table)
      logger.info(
          "TableOperationMetricAggregation aggregate %d regions metric for table %s, cluster %s" ,
           len(regions), table.name, clusterName)

      for region in regions:
        if region.operationMetrics is None or region.operationMetrics == '':
          continue;
        regionOperationMetrics = json.loads(region.operationMetrics)
        for regionOperationName in regionOperationMetrics.keys():
          regionOperation = regionOperationMetrics[regionOperationName]
          self.aggregate_one_region_operation_metric(tableOperationMetric.setdefault(regionOperationName,
                                                    self.make_empty_operation_metric()), regionOperation)
          self.aggregate_one_region_operation_metric(clusterOperationMetric.setdefault(regionOperationName,
                                                     self.make_empty_operation_metric()), regionOperation)

      # compute avgTime for table operation metrics
      self.compute_avg_time_and_num_ops_after_aggregation(tableOperationMetric)
      table.operationMetrics = json.dumps(tableOperationMetric)
      table.save()

    # compute avgTime for clusetr operation metrics
    clusters = HBaseCluster.objects.all()
    for cluster in clusters:
      clusterName = cluster.cluster.name
      if clusterName in allClusterOperationMetric:
        clusterOperationMetric = allClusterOperationMetric[clusterName]
        self.compute_avg_time_and_num_ops_after_aggregation(clusterOperationMetric)
        cluster.operationMetrics = json.dumps(clusterOperationMetric)
        cluster.save()
    return

  def schedule_next_aggregation(self):
    next_time = self.last_aggregate_time + self.collector_config.period
    end_time = time.time()
    if end_time < next_time:
      wait_time = next_time - end_time
      logger.info(
          "RegionOperationMetricAggregator waiting %f seconds for aggregation" , wait_time)

      reactor.callFromThread(reactor.callLater, wait_time, self.aggregate_region_operation_metric)
    else:
      reactor.callFromThread(self.aggregate_region_operation_metric)
    return

class StatusUpdater:
  """Update status of all active clusters and jobs, which are inferred from
  tasks' status."""

  def __init__(self, collector_config):
    self.collector_config = collector_config

  def get_latest_metric(self, task, group_name, metric_name):
    try:
      metric = json.loads(task.last_metrics)
      return metric[group_name][metric_name]
    except Exception as e:
      logger.warning("%r failed to get metric: %r", task, e)
      return 0

  def is_namenode_active(self, task):
    try:
      metric = self.get_latest_metric(
          task, "Hadoop:service=NameNode,name=FSNamesystem", "tag.HAState")
      return bool(metric)
    except Exception as e:
      logger.warning("%r failed to get metric: %r", task, e)
      return False

  def is_master_active(self, task):
    try:
      metric = self.get_latest_metric(
          task, "hadoop:service=Master,name=Master", "IsActiveMaster")
      return bool(metric)
    except Exception as e:
      logger.warning("%r failed to get metric: %r", task, e)
      return False

  def update_hdfs_cluster_status(self, cluster):
    job = cluster.jobs["journalnode"]
    if (job.running_tasks_count < 2 or
        job.running_tasks_count < (job.total_tasks_count / 2 + 1)):
      job.last_status = Status.ERROR
      job.last_message = "Too few running journalnodes!"

    job = cluster.jobs["namenode"]
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "No running namenodes!"
    else:
      active = 0
      for task in job.running_tasks.itervalues():
        if self.is_namenode_active(task):
          # update cluster entry
          cluster.entry = '%s:%d' % (task.host, task.port)
          cluster.version = self.get_latest_metric(task,
                                                   'Hadoop:service=NameNode,name=NameNodeInfo',
                                                   'Version')
          active += 1
      if active > 1:
        job.last_status = Status.ERROR
        job.last_message = "Too many active namenodes!"
      elif active < 1:
        job.last_status = Status.ERROR
        job.last_message = "No active namenodes!"
      elif job.running_tasks_count < 2:
        job.last_status = Status.WARN
        job.last_message = "Less than 2 running namenodes, no HA guarantee"

    job = cluster.jobs["datanode"]
    if job.running_tasks_count < 3:
      job.last_status = Status.ERROR
      job.last_message = "Too few running datanodes!"
    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])

  def update_hbase_cluster_status(self, cluster):
    job = cluster.jobs["master"]
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "No running masters!"
    else:
      active = 0
      for task in job.running_tasks.itervalues():
        if self.is_master_active(task):
          # update cluster entry
          cluster.entry = '%s:%d' % (task.host, task.port)
          version = self.get_latest_metric(task,
                                           'hadoop:service=HBase,name=Info',
                                           'version')
          revision = self.get_latest_metric(task,
                                           'hadoop:service=HBase,name=Info',
                                           'revision')
          cluster.version = '%s, r%s' % (version, revision)
          active += 1
      if active > 1:
        job.last_status = Status.ERROR
        job.last_message = "Too many active masters!"
      elif active < 1:
        job.last_status = Status.ERROR
        job.last_message = "No active masters!"
      elif job.running_tasks_count < 2:
        # TODO: Now it always reports warning as backup master doesn't run a http
        # server before it acquires zk lock. Comment this out and would change
        # master's startup workflow.
        #job.last_status = Status.WARN
        #job.last_message = "Less than 2 running masters, no HA guarantee"
        pass

    job = cluster.jobs["regionserver"]
    if job.running_tasks_count < 3:
      job.last_status = Status.ERROR
      job.last_message = "Too few running regionservers!"
    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])

  def update_yarn_cluster_status(self, cluster):
    job = cluster.jobs["resourcemanager"]
    for task in job.running_tasks.itervalues():
      # update cluster entry
      cluster.entry = '%s:%d' % (task.host, task.port)
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "No running resourcemanager!"

    job = cluster.jobs["proxyserver"]
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "No running proxyserver!"

    job = cluster.jobs["nodemanager"]
    if job.running_tasks_count < 3:
      job.last_status = Status.ERROR
      job.last_message = "Too few running nodemanager!"
    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])

    job = cluster.jobs["historyserver"]
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "Too few running historyserver!"
    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])

  def update_impala_cluster_status(self, cluster):
    job = cluster.jobs["statestored"]
    for task in job.running_tasks.itervalues():
      # update cluster entry
      cluster.entry = '%s:%d' % (task.host, task.port)
    if job.running_tasks_count < 1:
      job.last_status = Status.ERROR
      job.last_message = "No running statestored!"

    job = cluster.jobs["impalad"]
    if job.running_tasks_count < 3:
      job.last_status = Status.ERROR
      job.last_message = "Too few running impalad!"
    cluster.last_status = max([job.last_status for job in cluster.jobs.itervalues()])

  def update_cluster_status(self, cluster):
    cluster.jobs = {}
    cluster.last_attempt_time = datetime.datetime.utcfromtimestamp(
        self.start_time).replace(tzinfo=timezone.utc)
    cluster.last_status = Status.OK
    cluster.last_message = ""

    for job in cluster.job_set.all():
      job.running_tasks = {}
      job.tasks = {}
      job.last_attempt_time = cluster.last_attempt_time
      job.last_status = Status.OK
      job.last_message = ""
      job.running_tasks_count = 0
      job.total_tasks_count = 0
      for task in job.task_set.filter(active=True):
        if task.health:
          job.running_tasks[task.id] = task
          job.running_tasks_count += 1
        job.total_tasks_count += 1
      cluster.jobs[job.name] = job

    service_handler = {
        "hdfs": self.update_hdfs_cluster_status,
        "hbase": self.update_hbase_cluster_status,
        "yarn": self.update_yarn_cluster_status,
        "impala": self.update_impala_cluster_status,
    }
    service_handler[cluster.service.name](cluster)

    for job in cluster.jobs.itervalues():
      if job.last_status < Status.ERROR:
        # OK or WARN
        job.last_success_time = job.last_attempt_time
      job.save()

    if cluster.last_status < Status.ERROR:
      # OK or WARN
      cluster.last_success_time = job.last_attempt_time
    cluster.save()

  def update_status(self):
    reactor.callInThread(self.update_status_in_thread)

  def update_status_in_thread(self):
    # TODO: make sure performance is not a problem as current approach queries
    # database many times.
    """
    tasks = get_tasks_by_service(service_id)
    clusters = []
    for task in tasks:
      if task.job.cluster not in clusters:
        clusters.append(task.job.cluster)
    """
    logger.info("updating clusters status, "
        "%d task in queue, %d workers, %d total threads",
        reactor.getThreadPool().q.qsize(),
        len(reactor.getThreadPool().working),
        len(reactor.getThreadPool().threads))

    try:
      self.start_time = time.time()
      for cluster in Cluster.objects.filter(active=True).all():
        self.update_cluster_status(cluster)
      logger.info("spent %f seconds for updating clusters status",
          time.time() - self.start_time)
      logger.info("gc: %r", gc.get_count())
      logger.info("usage: %r", resource.getrusage(resource.RUSAGE_SELF))
    except Exception as e:
      logger.warning("%Failed to update statu: %r", e)
    finally:
      # reactor.callLater is NOT thread-safe but reactor.callFromThread is, so we
      # put the callLater to the main loop.
      reactor.callFromThread(
        reactor.callLater, self.collector_config.period, self.update_status)


class Command(BaseCommand):
  args = ''
  help = "Run the background collector to fetch metrics from /jmx on each server."

  option_list = BaseCommand.option_list + (
      make_option(
        "--use_threadpool",
        action="store_true",
        default=False,
        help="Use thread pool to store metrics to database if the flag is on."),
      make_option(
        "--collector_cfg",
        default="collector.cfg",
        help="Specify collector configuration file"
      ),
      make_option(
        "--clear_old_tasks",
        action="store_true",
        default=False,
        help="Set true for clear old tasks"
      ),
  )

  def handle(self, *args, **options):
    gc.set_debug(gc.DEBUG_STATS)

    self.args = args
    self.options = options

    self.stdout.write("args: %r\n" % (args, ))
    self.stdout.write("options: %r\n" % options)

    self.collector_config = CollectorConfig(self.args, self.options)
    if self.options['clear_old_tasks']:
      self.clear_old_tasks()

    self.update_active_tasks()
    self.region_operation_aggregator = RegionOperationMetricAggregator(self.collector_config)
    # we start to aggregate region operation metric after one period
    reactor.callLater(self.collector_config.period + 1,
        self.region_operation_aggregator.aggregate_region_operation_metric)
    self.fetch_metrics()

  def clear_old_tasks():
    # Mark all current tasks as deactive.
    Service.objects.all().update(active=False)
    Cluster.objects.all().update(active=False)
    Job.objects.all().update(active=False)
    Task.objects.all().update(active=False)

  def update_active_tasks(self):
    # Add all active tasks
    self.metric_sources = []
    for service_name, service in self.collector_config.services.iteritems():
      # Save to database.
      # The active field has the default value True.
      service_record, created = Service.objects.get_or_create(
          name=service_name,
          defaults={"metric_url":service.metric_url})
      if not created:
        # Mark it as active if it exists.
        service_record.active = True
        service_record.save()

      for cluster_name, cluster in service.clusters.iteritems():
        cluster_record, created = Cluster.objects.get_or_create(
            service=service_record, name=cluster_name)
        if not created:
          cluster_record.active = True
          cluster_record.save()

        for job_name in service.jobs:
          job_record, created = Job.objects.get_or_create(
              cluster=cluster_record, name=job_name)
          if not created:
            job_record.active = True
            job_record.save()

          job = cluster.jobs[job_name]
          # We assume http port is always base_port + 1
          port = job.base_port + 1
          # support multiple instances
          hosts = job.hosts
          for host_id, host in hosts.iteritems():
            host_name = job.hostnames[host_id]
            for instance_id in range(host.instance_num):
              task_id = deploy_utils.get_task_id(hosts, host_id, instance_id)
              instance_port = deploy_utils.get_base_port(port,instance_id)
              task_record, created = Task.objects.get_or_create(
                job=job_record, task_id=task_id,
                defaults={"host":host_name, "port":instance_port})
              if not created or task_record.host != host_name or task_record.port != instance_port:
                task_record.active = True
                task_record.host = host_name
                task_record.port = instance_port
                task_record.save()
              self.metric_sources.append(
                MetricSource(self.collector_config, task_record))

  def fetch_metrics(self):
    for metric_source in self.metric_sources:
      # Randomize the start time of each metric source.
      # Because StatusUpdater will always update cluster status every 'self.collector_config.period',
      # here, we use 'self.collector_config.period - 2' to give each task at least 2 seconds to
      # download page and update its status into database before StatusUpdater starting to update cluster
      # status based on each task's status
      wait_time = random.uniform(0, self.collector_config.period - 2)
      logger.info(
          "%r waiting %f seconds for %s..." ,
          metric_source.task, wait_time, metric_source.url)
      reactor.callLater(wait_time, metric_source.fetch_metrics)

    status_updater = StatusUpdater(self.collector_config)
    reactor.callLater(self.collector_config.period + 1,
        status_updater.update_status)

    reactor.run()

########NEW FILE########
__FILENAME__ = models
from django.db import models

# Create your models here.

########NEW FILE########
__FILENAME__ = tests
"""
This file demonstrates writing tests using the unittest module. These will pass
when you run "manage.py test".

Replace this with more appropriate tests for your application.
"""

from django.test import TestCase


class SimpleTest(TestCase):
    def test_basic_addition(self):
        """
        Tests that 1 + 1 always equals 2.
        """
        self.assertEqual(1 + 1, 2)

########NEW FILE########
__FILENAME__ = views
# Create your views here.

########NEW FILE########
__FILENAME__ = admin
from django.contrib import admin

# Register your models here.

########NEW FILE########
__FILENAME__ = failover_framework_collect
# -*- coding: utf-8 -*-

import json
import logging
import os
import owl_config
import smtplib
import time
import urllib2
import utils.mail

from django.conf import settings
from django.core.management.base import BaseCommand

from failover_framework.models import Action
from failover_framework.models import Task

logger = logging.getLogger(__name__)

# Collect metrics from failover framework and insert into database periodically
class Command(BaseCommand):

  def handle(self, *args, **options):

    if len(args) >= 1:
      logger.warning("No need to give args for this script")

    mailer = utils.mail.Mailer(options)
    host = settings.FAILOVER_FRAMEWORK_HOST
    port = settings.FAILOVER_FRAMEWORK_PORT
    host_port = host + ":" + str(port)
    period = settings.FAILOVER_FRAMEWORK_PERIOD

    while True:
      start_time = time.time()
      self.collect_failover_framework_metrics(host_port, mailer)
      sleep_time = period - (time.time() - start_time)
      if sleep_time >= 0:
        logger.info("Sleep " + str(sleep_time) + " seconds for next time to collect metrics")
        time.sleep(sleep_time)
      else:
        logger.warning("Period is too short to collect metrics")

  def collect_failover_framework_metrics(self, host_port, mailer):
    try:
      # download json       
      metricsString = urllib2.urlopen("http://" + host_port + "/jmx")
      metrics = json.load(metricsString)

      # process json
      actions_info = []
      for metric in metrics["beans"]:
        # Task Metrics
        if "ActionsInfo" in metric: # the Task metric
          task_start_timestamp = metric["StartTimestamp"]
          task_start_time = metric["StartTime"]
          task_action_number = metric["ActionNumber"]
          actions_info = metric["ActionsInfo"]
          # Status Metrics
        elif "ClusterHealthy" in metric:
          task_cluster_healthy = True if metric["ClusterHealthy"] else False # int to boolean
          task_data_consistent = True if metric["DataConsistent"] else Fasle
          task_success = task_cluster_healthy and task_data_consistent

      # insert into database
      task = Task(start_timestamp=task_start_timestamp, start_time=task_start_time, action_number=task_action_number, cluster_healthy=task_cluster_healthy, data_consistent=task_data_consistent, success=task_success)
      task.save()
      logger.info("Insert Task into database which start at " + task_start_time)

      for action_info in actions_info:
        action = Action(task_id=task_start_timestamp, start_time=task_start_time, name=action_info["name"], success=action_info["success"], consume_time=action_info["consumeTime"])
        action.save()
        logger.info("Insert Action into database which is " + action.name)

      # send email
      if task_success == False:
        email_to = owl_config.FAILOVER_TO_EMAIL
        content = "Cluster healthy is " + str(task_cluster_healthy) + " and data consistent is " + str(task_data_consistent) + ".\nGo to owl for more details."
        logger.warning("Failover test fails, send email to " + email_to)
        mailer.send_email(content, "Failover Test Fails", to_mail)

    except:
      logger.warning("Can't get metrics from " + host_port + ", maybe failover framework is not running")

########NEW FILE########
__FILENAME__ = models
# -*- coding: utf-8 -*-

from django.db import models

class Task(models.Model):
  start_timestamp =models.BigIntegerField(primary_key=True)
  start_time = models.CharField(max_length=64)
  action_number = models.IntegerField()
  cluster_healthy = models.BooleanField()
  data_consistent = models.BooleanField()
  success = models.BooleanField()
  
class Action(models.Model):
  task = models.ForeignKey(Task)
  start_time = models.CharField(max_length=64)
  name = models.CharField(max_length=256)
  success = models.BooleanField()
  consume_time = models.IntegerField()
  

########NEW FILE########
__FILENAME__ = tests
from django.test import TestCase

# Create your tests here.

########NEW FILE########
__FILENAME__ = urls
# -*- coding: utf-8 -*-

from django.conf.urls import patterns, url
import views

urlpatterns = patterns(
  '',
  url(r'^$', views.index),
  url(r'^action/', views.show_actions),
  url(r'^task/', views.show_tasks),
)

########NEW FILE########
__FILENAME__ = views
# -*- coding: utf-8 -*-

import datetime
import httplib
import time

from django.conf import settings
from django.core.paginator import Paginator, EmptyPage, PageNotAnInteger
from django.core.paginator import Paginator, EmptyPage, PageNotAnInteger
from django.http import HttpResponse
from django.shortcuts import render_to_response
from django.template import RequestContext, Context, loader

from models import Action
from models import Task

# /failover/
def index(request):

  current_time = time.time() * 1000 # in ms
  now = datetime.datetime.now()
  today_first_timestamp = current_time - ((now.hour*60 + now.minute)*60 + now.second)*1000
  previous_hour = current_time - 3600000 # 60*60*1000
  previous_day = current_time - 86400000 # 24*60*60*1000
  previous_week = current_time - 604800000 # 7*24*60*60*1000
  previous_month = current_time - 2592000000 # 30*24*60*60*100
  previous_year = current_time - 31536000000 # 365*24*60*60*1000

  hour_task_number = Task.objects.filter(start_timestamp__gt=previous_hour).count()
  hour_fail_task_number = Task.objects.filter(start_timestamp__gt=previous_hour, success=False).count()
  hour_action_number = Action.objects.filter(task_id__gt=previous_hour).count()
  hour_fail_action_number = Action.objects.filter(task_id__gt=previous_hour, success=False).count()
  day_task_number = Task.objects.filter(start_timestamp__gt=previous_day).count()
  day_fail_task_number = Task.objects.filter(start_timestamp__gt=previous_day, success=False).count()
  day_action_number = Action.objects.filter(task_id__gt=previous_day).count()
  day_fail_action_number = Action.objects.filter(task_id__gt=previous_day, success=False).count()
  week_task_number = Task.objects.filter(start_timestamp__gt=previous_week).count()
  week_fail_task_number = Task.objects.filter(start_timestamp__gt=previous_week, success=False).count()
  week_action_number = Action.objects.filter(task_id__gt=previous_week).count()
  week_fail_action_number = Action.objects.filter(task_id__gt=previous_week, success=False).count()
  month_task_number = Task.objects.filter(start_timestamp__gt=previous_month).count()
  month_fail_task_number = Task.objects.filter(start_timestamp__gt=previous_month, success=False).count()
  month_action_number = Action.objects.filter(task_id__gt=previous_month).count()
  month_fail_action_number = Action.objects.filter(task_id__gt=previous_month, success=False).count()
  year_task_number = Task.objects.filter(start_timestamp__gt=previous_year).count()
  year_fail_task_number = Task.objects.filter(start_timestamp__gt=previous_year, success=False).count()
  year_action_number = Action.objects.filter(task_id__gt=previous_year).count()
  year_fail_action_number = Action.objects.filter(task_id__gt=previous_year, success=False).count()
  total_task_number = Task.objects.count()
  total_fail_task_number = Task.objects.filter(success=False).count()
  total_action_number = Action.objects.count()
  total_fail_action_number = Action.objects.filter(success=False).count()

  today_tasks = Task.objects.filter(start_timestamp__gt=today_first_timestamp)
  context = {
    "chart_id": "today_tasks",
    "chart_title": "Today Tasks",
    "tasks": today_tasks,
    }
  failover_task_chart = loader.get_template("failover_task_chart.tpl").render(Context(context))

  host = settings.FAILOVER_FRAMEWORK_HOST
  port = settings.FAILOVER_FRAMEWORK_PORT
  host_port = host + ":" + str(port)

  try:
    conn = httplib.HTTPConnection(host_port)
    conn.request('HEAD', "/")
    response = conn.getresponse()
    conn.close()
    is_running = response.status == 200
  except:
    is_running = False
  
  context = {
    "failover_task_chart": failover_task_chart,
    "is_running": is_running,
    "host_port": host_port,
    "hour_task_number": hour_task_number,
    "hour_fail_task_number": hour_fail_task_number,
    "hour_action_number": hour_action_number,
    "hour_fail_action_number": hour_fail_action_number,
    "day_task_number": day_task_number,
    "day_fail_task_number": day_fail_task_number,
    "day_action_number": day_action_number,
    "day_fail_action_number": day_fail_action_number,
    "week_task_number": week_task_number,
    "week_fail_task_number": week_fail_task_number,
    "week_action_number": week_action_number,
    "week_fail_action_number": week_fail_action_number,
    "month_task_number": month_task_number,
    "month_fail_task_number": month_fail_task_number,
    "month_action_number": month_action_number,
    "month_fail_action_number": month_fail_action_number,
    "year_task_number": year_task_number,
    "year_fail_task_number": year_fail_task_number,
    "year_action_number": year_action_number,
    "year_fail_action_number": year_fail_action_number,
    "total_task_number": total_task_number,
    "total_fail_task_number": total_fail_task_number,
    "total_action_number": total_action_number,
    "total_fail_action_number": total_fail_action_number,
    }
  return render_to_response("index.html", context, context_instance=RequestContext(request))

def paging_objects(request, objects, number):
  paginator = Paginator(objects, number)
  page = request.GET.get("page")
  try:
    objects_to_show = paginator.page(page)
  except PageNotAnInteger:
    objects_to_show = paginator.page(1)
  except EmptyPage:
    objects_to_show = paginator.page(page.num_pages)
  return objects_to_show

# /failover/task/
def show_tasks(request):
  
  # ?latest=10
  if request.GET.get("latest"):
    number = request.GET.get("latest")
    # ?latest=10&fail=Ture
    if request.GET.get("fail"):
      tasks = Task.objects.filter(success=False).order_by("start_timestamp").reverse()[:number]
    else:
      tasks = Task.objects.all().order_by("start_timestamp").reverse()[:number]
  # ?start_time=2013-09-11%2017:51:22
  elif request.GET.get("start_time"):
    tasks = Task.objects.filter(start_time=request.GET.get("start_time"))
  # no params
  else:
    tasks = Task.objects.all().order_by("start_timestamp").reverse()   

  tasks_to_show = paging_objects(request, tasks, 20)

  context = {
    "tasks": tasks_to_show,
    }
  return render_to_response("show_tasks.html", context, context_instance=RequestContext(request))


# /failover/action/
def show_actions(request):
  
  # ?latest=10
  if request.GET.get("latest"):
    number = request.GET.get("latest")
    # ?latest=10&fail=True
    if request.GET.get("fail"):
      actions = Action.objects.filter(success=False).order_by("task").reverse()[:number]
    else:
      actions = Action.objects.all().order_by("task").reverse()[:number]
  # ?start_time=2013-09-11_%2017:51:22
  elif request.GET.get("start_time"):
    actions = Action.objects.filter(start_time=request.GET.get("start_time"))
  else:
    actions = Action.objects.all().order_by("task").reverse()
        
  actions_to_show = paging_objects(request, actions, 20)

  context = {
    "actions": actions_to_show,
    }
  return render_to_response("show_actions.html", context, context_instance=RequestContext(request))


########NEW FILE########
__FILENAME__ = admin
from django.contrib import admin
from models import Longhaul

admin.site.register(Longhaul)

########NEW FILE########
__FILENAME__ = models
# -*- coding: utf-8 -*-

from django.db import models

class Longhaul(models.Model):
  # The cluster name
  cluster = models.CharField(max_length=32)
  # The table name
  table = models.CharField(max_length=32)
  # the column family of the long hual
  cf = models.CharField(max_length=32)
  # the load description of the longhaul test
  description = models.TextField()

  def getCounterGroup(self):
    return u"infra-hbase-longhaul-%s-%s-%s" % (self.cluster, self.table, self.cf)

  def __unicode__(self):
    return u"%s/%s" % (self.cluster, self.table)

########NEW FILE########
__FILENAME__ = urls
# -*- coding: utf-8 -*-

from django.conf.urls import patterns, url
import views

urlpatterns = patterns(
  '',
  url(r'^$', views.index),
  url(r'^longhaul/(?P<id>\d+)/$', views.show_longhaul),
)

########NEW FILE########
__FILENAME__ = views
# -*- coding: utf-8 -*-
from models import Longhaul
from monitor.views import respond
from monitor.dbutil import get_counters_by_group

def index(request):
  # show all cluster
  longhauls = Longhaul.objects.all()
  params = {
    'longhauls': longhauls,
  }
  return respond(request, 'hbase/index.html', params)

#url: /longhaul/$id/
def show_longhaul(request, id):
  longhaul = Longhaul.objects.get(id=id)
  group = longhaul.getCounterGroup()
  counters = get_counters_by_group(group)
  endpoint = 'unknown'
  counter_names = []
  for counter in counters:
    endpoint = counter.host
    counter_names.append(group + '-' + counter.name)
  
  params = {
    'endpoint': endpoint,
    'counter_names': counter_names,
    'longhaul': longhaul,
  }
  return respond(request, 'hbase/longhaul.html', params)

########NEW FILE########
__FILENAME__ = manage
import os
import sys
import ctypes

if __name__ == "__main__":
  os.environ.setdefault("DJANGO_SETTINGS_MODULE", "owl.settings")

  root_path = os.path.abspath(
      os.path.dirname(os.path.realpath(__file__))+ '/..')
  owl_path = os.path.join(root_path, 'owl')

  # add libs path for loading module zookeeper
  lib_path = os.path.join(owl_path, "libs")
  sys.path.append(lib_path)
  ctypes.cdll.LoadLibrary(os.path.join(lib_path, 'libzookeeper_mt.so.2'))

  client_path = os.path.join(root_path, 'client')
  sys.path.append(client_path)

  deploy_utils = __import__('deploy_utils')
  conf_path = deploy_utils.get_config_dir()

  owl_conf_path = os.path.join(conf_path, 'owl')
  sys.path.append(owl_conf_path)

  from django.core.management import execute_from_command_line

  execute_from_command_line(sys.argv)

########NEW FILE########
__FILENAME__ = admin
from django.contrib import admin
from models import Service, Cluster, Job, Task
from models import HBaseCluster, RegionServer, Table, Region
from models import Counter
from models import Quota

admin.site.register(Service)
admin.site.register(Cluster)
admin.site.register(Job)
admin.site.register(Task)

admin.site.register(HBaseCluster)
admin.site.register(RegionServer)
admin.site.register(Table)
admin.site.register(Region)

admin.site.register(Counter)
admin.site.register(Quota)

########NEW FILE########
__FILENAME__ = dbutil
# -*- coding: utf-8 -*-
import datetime
import hashlib
import json
import logging
import socket
import struct
import time

import MySQLdb
from DBUtils.PooledDB import PooledDB

from django.conf import settings
from django.utils import timezone

from models import Service, Cluster, Quota, Job, Task, Status
from models import Table, RegionServer, HBaseCluster, Region
from models import Counter
from django.db.models import Sum
import metric_helper

logger = logging.getLogger(__name__)

db_settings = settings.DATABASES['default']
# we use db connection pool to execute batch update
DBConnectionPool = PooledDB(MySQLdb, maxusage = 10, mincached = 5,
                            db = db_settings['NAME'],
                            host = db_settings['HOST'],
                            port = int(db_settings['PORT']),
                            user = db_settings['USER'],
                            passwd = db_settings['PASSWORD'],
                            charset = 'utf8')

def get_services():
  return Service.objects.filter(active=True).all()


def get_service(id):
  try:
    return Service.objects.get(id=id, active=True)
  except Service.DoesNotExist:
    return None


def get_clusters_by_service(service_id=None):
  filters = {"active": True}
  if service_id: filters["service"] = service_id
  return Cluster.objects.filter(**filters).all().order_by('service', 'name')


def get_cluster(id):
  try:
    return Cluster.objects.get(id=id, active=True)
  except Cluster.DoesNotExist:
    return None


def get_jobs_by_cluster(cluster_id):
  return Job.objects.filter(cluster=cluster_id, active=True).all()


def get_job(id):
  try:
    return Job.objects.get(id=id, active=True)
  except Job.DoesNotExist:
    return None


def get_tasks_by_job(job_id):
  return Task.objects.filter(job=job_id, active=True).all()

def get_healthy_tasks_by_job(job_id):
  return filter(lambda x: x.health, Task.objects.filter(job=job_id, active=True).all())

def get_tasks_by_cluster(cluster_id):
  return Task.objects.filter(job__cluster=cluster_id, active=True).order_by('job', 'id')


def get_tasks_by_service(service_id=None):
  filters = {"active": True}
  if service_id: filters["job__cluster__service"] = service_id
  return Task.objects.filter(**filters).all()

def get_task_by_host_and_port(host, port):
  try:
    return Task.objects.get(host = host, port = port)
  except:
    host = socket.gethostbyname(host)
    return Task.objects.get(host = host, port = port)

def get_task(id):
  try:
    return Task.objects.get(id=id, active=True)
  except Task.DoesNotExist:
    return None

def generate_perf_counter_for_task(result):
  tasks = get_alive_tasks()
  for task in tasks:
    if not task.health:
      continue
    result.update(generate_perf_counter(task))
  return result

def get_alive_tasks():
  return Task.objects.filter(active=True, last_status=Status.OK).all()

def get_alive_regions_by_rs(rs_record):
  return Region.objects.filter(region_server = rs_record,
                               last_attempt_time__gt=region_alive_threshold())

def region_alive_threshold():
  return datetime.datetime.utcfromtimestamp(time.time() - 60*24).replace(tzinfo=timezone.utc)

def getTableAvailability(cluster, table):
  group = 'infra-hbase-' + cluster
  name = table + '-Availability'
  try:
    return Counter.objects.get(group=group, name=name,
                               last_update_time__gt=counter_alive_threshold()).value
  except Counter.DoesNotExist:
    return -1.0

def generate_perf_counter(task):
  result = {}
  try:
    last_metrics = json.loads(task.last_metrics)
  except:
    print 'Failed to parse metrics of task:', task
    print task.last_metrics
    return result

  endpoint = result.setdefault(metric_helper.form_perf_counter_endpoint_name(task), {})
  for bean_name, bean_metrics in last_metrics.iteritems():
    group_name = metric_helper.form_perf_counter_group_name(task, bean_name)
    group = endpoint.setdefault(group_name, {})
    for metric_name, metric_value in bean_metrics.iteritems():
      metric_type = type(metric_value)
      if not metric_type is int and not metric_type is float:
        continue
      key_name = metric_helper.form_perf_counter_key_name(bean_name, metric_name)
      counter = group.setdefault(key_name, {})
      counter['type'] = 0
      counter['unit'] = ''
      counter['value'] = metric_value
  return result

# map cluster name to endpoint
def map_cluster_to_endpoint(cluster_name):
#  ip_int = int(hashlib.md5(cluster_name).hexdigest()[:8], 16)
#  return socket.inet_ntoa(struct.pack("!I", ip_int))
  # perf counter system support non-ip
  return cluster_name

# generate NumOps, AvgTime, MaxTime, MinTime counter for 'group'
def generate_perf_counter_of_operation_metrics(record, group):
    if record.operationMetrics is None or record.operationMetrics == '':
      return
    operationMetrics = json.loads(record.operationMetrics)
    for operationName in operationMetrics.keys():
      operation = operationMetrics[operationName]
      # report NumOps
      operationNumOpsName = operationName + '_NumOps'
      counter = group.setdefault(operationNumOpsName, {})
      counter['type'] = 0
      counter['unit'] = 'ops'
      counter['value'] = operation['NumOps']
      # report AvgTime
      operationAvgTimeName = operationName + '_AvgTime'
      counter = group.setdefault(operationAvgTimeName, {})
      counter['type'] = 0
      counter['unit'] = 'us'
      counter['value'] = operation['AvgTime']
      # report MinTime
      operationMinTimeName = operationName + '_MinTime'
      counter = group.setdefault(operationMinTimeName, {})
      counter['type'] = 0
      counter['unit'] = 'us'
      counter['value'] = operation['MinTime']
      # report MaxTime
      operationMaxTimeName = operationName + '_MaxTime'
      counter = group.setdefault(operationMaxTimeName, {})
      counter['type'] = 0
      counter['unit'] = 'us'
      counter['value'] = operation['MaxTime']

def generate_perf_counter_for_table(result):
  tables = Table.objects.filter(last_attempt_time__gte = alive_time_threshold())
  for table in tables:
    endpoint_name = map_cluster_to_endpoint(table.cluster.name)
    endpoint = result.setdefault(endpoint_name, {})
    group = endpoint.setdefault(str(table), {})
    counter = group.setdefault('readRequestsCountPerSec', {})
    counter['type'] = 0
    counter['unit'] = 'qps'
    counter['value'] = table.readRequestsCountPerSec
    counter = group.setdefault('writeRequestsCountPerSec', {})
    counter['type'] = 0
    counter['unit'] = 'qps'
    counter['value'] = table.writeRequestsCountPerSec
    # report operation perf counter for table
    generate_perf_counter_of_operation_metrics(table, group)

  return result

def generate_perf_counter_for_regionserver(result):
  regionservers = RegionServer.objects.filter(last_attempt_time__gte = alive_time_threshold())
  for regionserver in regionservers:
    endpoint_name = map_cluster_to_endpoint(regionserver.cluster.name)
    endpoint = result.setdefault(endpoint_name, {})
    group = endpoint.setdefault(str(regionserver), {})
    counter = group.setdefault('readRequestsCountPerSec', {})
    counter['type'] = 0
    counter['unit'] = 'qps'
    counter['value'] = regionserver.readRequestsCountPerSec
    counter = group.setdefault('writeRequestsCountPerSec', {})
    counter['type'] = 0
    counter['unit'] = 'qps'
    counter['value'] = regionserver.writeRequestsCountPerSec
  return result

def generate_perf_counter_for_cluster(result):
  hbase_clusters = HBaseCluster.objects.all()
  for hbase_cluster in hbase_clusters:
    last_update_time = hbase_cluster.cluster.last_attempt_time
    # filter not recently updated cluster
    if last_update_time < alive_time_threshold():
      continue
    endpoint_name = map_cluster_to_endpoint(hbase_cluster.cluster.name)
    endpoint = result.setdefault(endpoint_name, {})
    group = endpoint.setdefault('Cluster', {})
    counter = group.setdefault('readRequestsCountPerSec', {})
    counter['type'] = 0
    counter['unit'] = 'qps'
    counter['value'] = hbase_cluster.readRequestsCountPerSec
    counter = group.setdefault('writeRequestsCountPerSec', {})
    counter['type'] = 0
    counter['unit'] = 'qps'
    counter['value'] = hbase_cluster.writeRequestsCountPerSec
    # report operation perf counter for cluster
    generate_perf_counter_of_operation_metrics(hbase_cluster, group)
  return result

def get_all_metrics():
  result = {}
  generate_perf_counter_for_task(result)
  generate_perf_counter_for_table(result)
  generate_perf_counter_for_regionserver(result)
  generate_perf_counter_for_cluster(result)
  return result


def get_or_create_counter(group, name):
  return Counter.objects.get_or_create(group=group, name=name)

def get_counter(group, name):
  try:
    return Counter.objects.get(group=group, name=name)
  except Counter.DoesNotExist:
    return None

def counter_alive_threshold():
  return datetime.datetime.utcfromtimestamp(time.time() - 15).replace(tzinfo=timezone.utc)

def get_counters_by_group(group):
  return Counter.objects.filter(last_update_time__gt=counter_alive_threshold(), group=group).all()

def get_counters_by_group_and_label(group, label):
  return Counter.objects.filter(group=group, label=label).all()

def get_all_counters():
  result = {}
  counters = Counter.objects.filter(last_update_time__gt=counter_alive_threshold()).all()
  for counter in counters:
    endpoint = result.setdefault(counter.host, {})
    group = endpoint.setdefault(counter.group, {})
    key = group.setdefault(counter.name, {})
    key['type'] = 0
    key['unit'] = counter.unit
    key['value'] = counter.value
  return result


def get_metric(group, metric):
  try:
    return Metric.objects.get(group=group, metric=metric)
  except Metric.DoesNotExist:
    return None

def aggregate_metrics(records):
  agg_records = []
  first = None
  for record in records:
    if first is None:
      first = record
    else:
      if (record.time - first.time).seconds < 10:
        first.value += record.value
      else:
        agg_records.append(first)
        first = record
  print len(records), len(agg_records)
  return agg_records


def select_by_step(metrics, step):
  select_metrics = []
  for i in range(0, len(metrics), step):
    select_metrics.append(metrics[i])
  return select_metrics

def get_table(id):
  try:
    return Table.objects.get(id = id)
  except Table.DoesNotExist:
    return None

def get_all_tables():
  try:
    return Table.objects.all().filter(last_attempt_time__gte = alive_time_threshold(36000)).order_by('-storefileSizeMB')
  except Table.DoesNotExist:
    return None

def get_table_by_cluster(cluster):
  return Table.objects.filter(cluster = cluster)

# attr should be 'regionserver' or 'table'
def get_items_on_cluster(cluster, attr, order_by):
  # return alive items order by sum of read and write qps
  return getattr(cluster, attr+'_set').filter(last_attempt_time__gte = alive_time_threshold()).\
      extra(select = {'qps':'readRequestsCountPerSec + writeRequestsCountPerSec'},
            order_by = (order_by, ))

def get_regionserver(id):
  try:
    return RegionServer.objects.get(id = id)
  except RegionServer.DoesNotExist:
    return None

def get_quota(id):
  try:
    return Quota.objects.get(id = id)
  except Quota.DoesNotExist:
    return None

def get_regionservers_with_active_replication_metrics_by_cluster(cluster):
  return RegionServer.objects.filter(cluster = cluster,
                                     last_attempt_time__gte = alive_time_threshold(),
                                     replication_last_attempt_time__gte = alive_time_threshold())

def get_region_by_regionserver_and_encodename(region_server, encodeName):
  try:
    return Region.objects.get(region_server = region_server, encodeName = encodeName)
  except Region.DoesNotExist:
    return None

def get_region_by_table(tableObj):
  # must use last_attemp_time to filter deleted-regions
  return Region.objects.filter(table = tableObj).filter(last_attempt_time__gte = alive_time_threshold()).all()

# attr should be 'regionserver' or 'table'
def get_requests_distribution_groupby(cluster, attr):
  items = getattr(cluster, attr+'_set').filter(last_attempt_time__gte = alive_time_threshold()).all()
  read_requests_dist = {}
  write_requests_dist = {}
  for item in items:
    read_requests_dist[str(item)] = (item.id, item.readRequestsCountPerSec)
    write_requests_dist[str(item)] = (item.id, item.writeRequestsCountPerSec)

  return (read_requests_dist, write_requests_dist)

def get_requests_distribution(owner):
  read_requests_dist = []
  write_requests_dist = []
  for region in owner.region_set.filter(last_attempt_time__gte = alive_time_threshold()).order_by('name'):
    read_requests_dist.append((str(region), region.readRequestsCountPerSec))
    write_requests_dist.append((str(region), region.writeRequestsCountPerSec))

  return (read_requests_dist, write_requests_dist)


def alive_time_threshold(threshold_in_secs = 120):
  return datetime.datetime.utcfromtimestamp(time.time() - threshold_in_secs).replace(tzinfo=timezone.utc)

def get_hbase_basic_info(cluster):
  cluster_info = {}
  try:
    hbase_cluster_record = cluster.hbasecluster
    cluster_info['hbase_entry'] = cluster.entry

    cluster_info['hdfs_entry'] = get_hdfs_entry(cluster.name)
    cluster_info['zk_entry'] = get_zk_entry(cluster)

    cluster_info['read_qps'] = hbase_cluster_record.readRequestsCountPerSec
    cluster_info['write_qps'] = hbase_cluster_record.writeRequestsCountPerSec
  except Exception as e:
    logger.warning("Failed to get hbase cluster for cluster %r, %r", cluster, e)

  return cluster_info

def get_hdfs_entry(cluster_name):
  try:
    service_record = Service.objects.get(name='hdfs')
    hdfs_cluster_record = Cluster.objects.filter(service = service_record,
                                                 name = cluster_name)
    return hdfs_cluster_record[0].entry
  except Exception as e:
    logger.warning("Failed to get hdfs entry for cluster %r, %r", cluster_name, e)

  return ""

# parse zk address from hbase master's metrics
def get_zk_entry(cluster):
  try:
    master_task = cluster.job_set.filter(name='master')[0].task_set.all()
    for task in master_task:
      if not task.health:
        continue
      metric = json.loads(task.last_metrics)
      zk_metrics = metric['hadoop:service=Master,name=Master']['ZookeeperQuorum']
      return zk_metrics

  except Exception as e:
    logger.warning("Failed to get zk entry for cluster %r: %r", cluster.name, e)
  return ""

def quota_alive_threshold():
  threshold_in_secs = 60*60*24
  return datetime.datetime.utcfromtimestamp(time.time() - threshold_in_secs).replace(tzinfo=timezone.utc)

def get_quota_summary(cluster):
  try:
    return cluster.quota_set.filter(last_update_time__gte = quota_alive_threshold()).order_by('name')

  except Exception as e:
    logger.warning("Failed to get quota for cluster %r: %r", cluster.name, e)
    return []

def format_quota_data(quota_item):
  if not quota_item.isdigit():
    return 0
  else:
    return int(quota_item)

def get_quota_distribution(cluster):
  dirs = get_quota_summary(cluster)
  tsdb_quota_total = {}
  tsdb_space_quota_total = {}

  for dir in dirs:
    tsdb_quota_total[dir.name] = ((dir.id, format_quota_data(dir.used_quota)))
    tsdb_space_quota_total[dir.name] = ((dir.id, format_quota_data(dir.used_space_quota)))

  return (tsdb_quota_total, tsdb_space_quota_total)

def update_regions_for_region_server_metrics(regions):
  all_update_metrics = []
  for region in regions:
    update_metrics = []
    update_metrics.append(str(region.last_operation_attempt_time).split('.')[0])
    update_metrics.append(str(region.operationMetrics))
    update_metrics.append(str(region.id))
    all_update_metrics.append(update_metrics)

  conn = None
  try:
    conn=DBConnectionPool.connection()
    cur=conn.cursor()

    cur.executemany('update monitor_region set last_operation_attempt_time=%s, operationMetrics=%s where id=%s', all_update_metrics)
    conn.commit()
    cur.close()
  except MySQLdb.Error,e:
    print "Mysql Error %d: %s" % (e.args[0], e.args[1])
  finally:
    if conn is not None:
      conn.close()

def update_regions_for_master_metrics(regions):
  all_update_metrics = []
  for region in regions:
    update_metrics = []
    update_metrics.append(str(region.readRequestsCountPerSec))
    update_metrics.append(str(region.writeRequestsCountPerSec))
    update_metrics.append(str(region.last_attempt_time).split('.')[0])
    update_metrics.append(str(region.memStoreSizeMB))
    update_metrics.append(str(region.storefileSizeMB))
    update_metrics.append(str(region.readRequestsCount))
    update_metrics.append(str(region.writeRequestsCount))
    update_metrics.append(str(region.requestsCount))
    update_metrics.append(str(region.region_server.id))
    update_metrics.append(str(region.id))
    all_update_metrics.append(update_metrics)

  conn = None
  try:
    conn=DBConnectionPool.connection()
    cur=conn.cursor()

    cur.executemany('update monitor_region set readRequestsCountPerSec=%s, writeRequestsCountPerSec=%s, last_attempt_time=%s, memStoreSizeMB=%s, storefileSizeMB=%s, readRequestsCount=%s, writeRequestsCount=%s, requestsCount=%s, region_server_id=%s where id=%s', all_update_metrics)
    conn.commit()
    cur.close()
  except MySQLdb.Error,e:
    print "Mysql Error %d: %s" % (e.args[0], e.args[1])
  finally:
    if conn is not None:
      conn.close()



########NEW FILE########
__FILENAME__ = metric_helper
# -*- coding: utf-8 -*-

import dbutil
import json
import metric_view_config

# define operation metric suffix
OPERATION_HISTOGRAM_75th_TIME = 'histogram_75th_percentile'
OPERATION_HISTOGRAM_95th_TIME = 'histogram_95th_percentile'
OPERATION_HISTOGRAM_99th_TIME = 'histogram_99th_percentile'
OPERATION_HISTOGRAM_999th_TIME = 'histogram_999th_percentile'
OPERATION_HISTOGRAM_PERCENTILES = [OPERATION_HISTOGRAM_75th_TIME,
                                   OPERATION_HISTOGRAM_95th_TIME,
                                   OPERATION_HISTOGRAM_99th_TIME,
                                   OPERATION_HISTOGRAM_999th_TIME]

def form_perf_counter_endpoint_name(task):
  delimiter = '-'
  endpoint_name = delimiter.join((task.host, str(task.port)))
  return endpoint_name

def form_perf_counter_group_name(task, bean_name):
  return parse_bean_name(bean_name)[0]

def form_percentile_counter_name(endpoint, group, operationName):
  percentiles = []
  for suffix in OPERATION_HISTOGRAM_PERCENTILES:
    percentiles.append(make_latency_metric_query(endpoint, group, '%s_%s' % (operationName, suffix)))
  return ''.join(percentiles)

# parse bean name
# return (service, name)
# eg:
#   input 'hadoop:service=HBase,name=RPCStatistics-18600'
#   return ('HBase', 'RPCStatistics-18600')
def parse_bean_name(bean_name):
  items= bean_name.split(':')[1].split(',')[:2]
  return [item.split('=')[1] for item in items]

# input:
# 'ReplicationSource for 5-10.0.4.172%2C11600%2C1364508024855'
# return 5-bak
# input:
# 'ReplicationSource for 5'
# return 5
def parse_replication_source(name):
  fields = name.split('-')
  source_name = fields[0]
  try:
    source_num = source_name.split(' ')[2]
    if len(fields) > 1:
      return source_num + '-bak'
    else:
      return source_num
  except:
    return source_name

def form_perf_counter_key_name(bean_name, metric_name):
  # illegal perf counter char '~' exsit in hbase table metric.
  # replace it with '-'
  # eg:tbl.miliao_summary.cf.S~T.multiput_AvgTime
  #    to tbl.miliao_summary.cf.S-T.multiput_AvgTime
  service, name = parse_bean_name(bean_name)
  if service == 'Replication':
    replication_src = parse_replication_source(name)
    metric_name += '-' + replication_src
  return metric_name.replace('~', '-')

def task_metrics_view_config(task):
  result = {}
  service, cluster, job, task = str(task).split('/')
  return metric_view_config.TASK_METRICS_VIEW_CONFIG[service][job]

def job_metrics_view_config(job):
  result = {}
  service, cluster, job = str(job).split('/')
  return metric_view_config.JOB_METRICS_VIEW_CONFIG[service][job]

def get_all_metrics_config():
  inputs = (metric_view_config.JOB_METRICS_VIEW_CONFIG,
            metric_view_config.TASK_METRICS_VIEW_CONFIG,
           )

  metric_set = set()

  for input in inputs:
    for job_name, tasks in input.iteritems():
      for task, task_configs in tasks.iteritems():
        for view, view_config in  task_configs:
          for graph in view_config:
            for metric in graph:
              metric_set.add(metric)

  return list(metric_set)

def tsdb_task_metrics_view_config(task):
  result = {}
  service, cluster, job, task = str(task).split('/')
  return metric_view_config.TASK_METRICS_VIEW_CONFIG[service][job]

def tsdb_job_metrics_view_config(job):
  result = {}
  service, cluster, job = str(job).split('/')
  return metric_view_config.JOB_METRICS_VIEW_CONFIG[service][job]

def make_metric_query(endpoint, group, key, unit=""):
  if unit:
    return "&m=sum:%s{host=%s,group=%s}&o=&yformat=%%25.0s%%25c %s" % (key, endpoint, group, unit)
  else:
    return "&m=sum:%s{host=%s,group=%s}&o=" % (key, endpoint, group)

def make_quota_query(cluster_name, user_id, key):
  return "&m=sum:%s{cluster=%s,user_id=%s}&o=" % (key, cluster_name, user_id)

def make_metrics_query_for_task(endpoint, task):
  metrics = []
  task_view_config = task_metrics_view_config(task)
  for view_tag, view_config in task_view_config:
    metrics_view = []
    for graph_config in view_config:
      group, key, unit = graph_config[0]
      graph = {
        'title' : '%s:%s' % (group, key),
        'query' : make_metric_query(endpoint, group, key, unit),
      }
      metrics_view.append(graph)
    metrics.append((view_tag, metrics_view))
  return metrics

def make_metrics_query_for_job(endpoints, job, tasks):
  metrics = []
  task_view_config = job_metrics_view_config(job)
  for view_tag, view_config in task_view_config:
    metrics_view = []
    for graph_config in view_config:
      group, key, unit = graph_config[0]
      metrics_view.append(make_metric_query_graph_for_endpoints(endpoints, group, key, unit))
    metrics.append((view_tag, metrics_view))
  return metrics

def make_metric_query_graph_for_endpoints(endpoints, group, key, unit=""):
  graph = {
    'title' : '%s:%s' % (group, key),
    'query' : [],
  }
  for endpoint in endpoints:
    graph['query'].append(make_metric_query(endpoint, group, key, unit))
  return graph

def get_peer_id_endpoint_map_and_cluster(region_servers):
  peer_id_endpoint_map = {}
  peer_id_cluster = {}
  for region_server in region_servers:
    endpoint = form_perf_counter_endpoint_name(region_server.task)
    replicationMetrics = json.loads(region_server.replicationMetrics)
    for peer_id in replicationMetrics.keys():
      if "peerClusterName" in replicationMetrics[peer_id]:
        peer_id_cluster[peer_id] = replicationMetrics[peer_id]["peerClusterName"]
      peer_id_endpoints = peer_id_endpoint_map.setdefault(peer_id, []) 
      peer_id_endpoints.append(endpoint)
  return (peer_id_endpoint_map, peer_id_cluster)

def make_metrics_query_for_replication(peer_id_endpoint_map, peer_id_cluster_map):
  metrics = []
  for peer_id in peer_id_endpoint_map.keys():
    endpoints = peer_id_endpoint_map[peer_id]
    peer_graphs = []
    for key_and_unit in metric_view_config.REPLICATION_METRICS_VIEW_CONFIG:
      key = key_and_unit[0]
      unit = key_and_unit[1]
      replication_key = key + '-' + peer_id
      peer_graphs.append(make_metric_query_graph_for_endpoints(endpoints, "Replication", replication_key, unit))
    cluster_name = "unknown-cluster"
    if peer_id in peer_id_cluster_map.keys():
      cluster_name = peer_id_cluster_map[peer_id]
    metrics.append((peer_id, cluster_name, peer_graphs))
  return metrics

def make_ops_metric_query(endpoint, group, name):
  return make_metric_query(endpoint, group, name, metric_view_config.DEFAULT_OPS_UNIT)

def make_latency_metric_query(endpoint, group, name):
  return make_metric_query(endpoint, group, name, metric_view_config.DEFAULT_LATENCY_UNIT)

# metrics is an array of counters, where the counter is formatted as :
# [operationName, CounterOfNumOps, CounterOfAvgTime]
def make_operation_metrics(endpoint, record, group):
  metrics = []
  if record.operationMetrics is not None and record.operationMetrics != '':
    operationMetrics = json.loads(record.operationMetrics)
    for operationName in operationMetrics.keys():
      # remove common prefix for 'coprocessor-operation'
      tokens = operationName.split('-')
      operationShowName = tokens[len(tokens) - 1]
      operationCounter = []
      operationCounter.append(operationShowName)

      operationNumOpsName = operationName + '_NumOps'
      numOpsCounter = {}
      numOpsCounter['title'] = operationNumOpsName
      numOpsCounter['query'] = make_ops_metric_query(endpoint, group, operationNumOpsName)
      operationCounter.append(numOpsCounter)

      operationAvgTimeName = operationName + '_AvgTime'
      avgTimeCounter = {}
      avgTimeCounter['title'] = operationAvgTimeName
      avgTimeCounter['query'] = make_latency_metric_query(endpoint, group, operationAvgTimeName)
      operationCounter.append(avgTimeCounter)

      metrics.append(operationCounter)
  return metrics

# [op_name: [{op_num: [table1_op1_avg_query, table2_op1_avg_query]},
#            {op_avg: [table2 op1_ops, table2 op1_num]}],
# ]
def make_operation_metrics_for_tables_in_cluster(cluster):
  # we first read operation metrics for tables of the cluster
  tables = dbutil.get_table_by_cluster(cluster)
  clusterOperationMetrics = json.loads(cluster.hbasecluster.operationMetrics)
  operationCounterNameOfTables = {}
  metrics = {}
  for operationName in clusterOperationMetrics.keys():
    tokens = operationName.split('-')
    operationShowName = tokens[-1]
    numOpsCounterName = '%s_NumOps' % (operationShowName)
    avgTimeCounterName = '%s_AvgTime' % (operationShowName)
    metrics[operationShowName] = [{'title': numOpsCounterName, 'query': []},
                                  {'title': avgTimeCounterName, 'query': []}]  # reserved for num and avg graph

  for table in tables:
    if table.operationMetrics is not None and table.operationMetrics != '':
      tableOperationMetrics = json.loads(table.operationMetrics)
      endpoint = cluster.name
      group = table.name
      for operationName in tableOperationMetrics:
        if operationName not in metrics.keys():
          continue
        numOpsCounterName = '%s_NumOps' % (operationName)
        avgTimeCounterName = '%s_AvgTime' % (operationName)
        print type(endpoint)
        print type(group)
        print type(numOpsCounterName)
        metrics[operationName][0]['query'].append(make_ops_metric_query(endpoint, group, numOpsCounterName))
        metrics[operationName][1]['query'].append(make_latency_metric_query(endpoint, group, avgTimeCounterName))

  return metrics

# metric is an array of counters, where counter is formatted as:
# [operationName, CounterOfNumOps, CountersOfPercentile]
def generate_operation_metric_for_regionserver(regionserver):
  task = regionserver.task
  metric = []
  endpoint = form_perf_counter_endpoint_name(regionserver.task)
  group = 'HBase'
  for operationName in metric_view_config.REGION_SERVER_OPERATION_VIEW_CONFIG:
    counter = []
    # first append operationName
    counter.append(operationName)
    # then, append counter for NumOps
    num_ops_counter = {}
    num_ops_counter['title'] = operationName + '_histogram_num_ops'
    num_ops_counter['query'] = make_ops_metric_query(endpoint, group, num_ops_counter['title'])
    counter.append(num_ops_counter)

    # lastly, append counters for percentile
    percentile_counter = {}
    percentile_counter['title'] = 'Percentile-Comparision'
    percentile_counter['query'] = form_percentile_counter_name(endpoint, group, operationName)
    counter.append(percentile_counter)

    metric.append(counter)
  return metric

########NEW FILE########
__FILENAME__ = metric_view_config

TASK_METRICS_VIEW_CONFIG = {
  'hdfs': {
    'journalnode':[
      ('Rpc', [
        [('JournalNode', 'ReceivedBytes', 'byte(s)')],
        [('JournalNode', 'SentBytes', 'byte(s)')],
        [('JournalNode', 'RpcQueueTimeNumOps', 'op(s)')],
        [('JournalNode', 'RpcQueueTimeAvgTime', 'ms(s)')],
      ]),
    ],
    'namenode':[
      # view
      ('Overall', [
        # graph
        # TODO:support comparison multi-metric in one graph
        [('NameNode', 'BlockCapacity', 'block(s)')],
        [('NameNode', 'BlocksTotal', 'block(s)')],
        [('NameNode', 'CapacityRemainingGB', 'GB')],
        [('NameNode', 'CapacityTotalGB', 'GB')],
        [('NameNode', 'CapacityUsedGB', 'GB')],
        [('NameNode', 'CorruptBlocks', 'block(s)')],
        [('NameNode', 'ExcessBlocks', 'block(s)')],
        [('NameNode', 'FilesTotal', 'file(s)')],
      ]),
      ('Operation', [
        [('NameNode', 'AddBlockOps', 'op(s)')],
        [('NameNode', 'CreateFileOps', 'op(s)')],
        [('NameNode', 'DeleteFileOps', 'op(s)')],
        [('NameNode', 'FileInfoOps', 'op(s)')],
        [('NameNode', 'GetListingOps', 'op(s)')],
      ]),
      ('Rpc', [
        [('NameNode', 'ReceivedBytes', 'byte(s)')],
        [('NameNode', 'SentBytes', 'byte(s)')],
        [('NameNode', 'RpcQueueTimeNumOps', 'op(s)')],
        [('NameNode', 'RpcQueueTimeAvgTime', 'ms(s)')],
      ]
      )
    ],
    'datanode':[
      ('BlockOperation', [
        [('DataNode', 'BlockReportsAvgTime', 'ms(s)')],
        [('DataNode', 'BlockReportsNumOps', 'op(s)')],
        [('DataNode', 'BlocksGetLocalPathInfo', '')],
        [('DataNode', 'BlocksRead', 'block(s)')],
        [('DataNode', 'BlocksRemoved', 'block(s)')],
        [('DataNode', 'BlocksReplicated', 'block(s)')],
        [('DataNode', 'BlocksVerified', 'block(s)')],
        [('DataNode', 'BlocksWritten', 'block(s)')],
      ]),
      ('Activity', [
        [('DataNode', 'BytesWritten', '')],
        [('DataNode', 'BytesRead', '')],
        [('DataNode', 'BlocksWritten', '')],
        [('DataNode', 'BlocksRead', '')],
        [('DataNode', 'BlocksReplicated', '')],
        [('DataNode', 'BlocksRemoved', '')],
        [('DataNode', 'BlocksVerified', '')],
        [('DataNode', 'BlockVerificationFailures', '')],
        [('DataNode', 'ReadsFromLocalClient', '')],
        [('DataNode', 'ReadsFromRemoteClient', '')],
        [('DataNode', 'WritesFromLocalClient', '')],
        [('DataNode', 'WritesFromRemoteClient', '')],
        [('DataNode', 'BlocksGetLocalPathInfo', '')],
        [('DataNode', 'FsyncCount', '')],
        [('DataNode', 'VolumeFailures', '')],
        [('DataNode', 'ReadBlockOpNumOps', '')],
        [('DataNode', 'ReadBlockOpAvgTime', '')],
        [('DataNode', 'WriteBlockOpNumOps', '')],
        [('DataNode', 'WriteBlockOpAvgTime', '')],
        [('DataNode', 'BlockChecksumOpNumOps', '')],
        [('DataNode', 'BlockChecksumOpAvgTime', '')],
        [('DataNode', 'CopyBlockOpNumOps', '')],
        [('DataNode', 'CopyBlockOpAvgTime', '')],
        [('DataNode', 'ReplaceBlockOpNumOps', '')],
        [('DataNode', 'ReplaceBlockOpAvgTime', '')],
        [('DataNode', 'HeartbeatsNumOps', '')],
        [('DataNode', 'HeartbeatsAvgTime', '')],
        [('DataNode', 'BlockReportsNumOps', '')],
        [('DataNode', 'BlockReportsAvgTime', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanosNumOps', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanosAvgTime', '')],
        [('DataNode', 'FlushNanosNumOps', '')],
        [('DataNode', 'FlushNanosAvgTime', '')],
        [('DataNode', 'FsyncNanosNumOps', '')],
        [('DataNode', 'FsyncNanosAvgTime', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanosNumOps', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanosAvgTime', '')],
        [('DataNode', 'SendDataPacketTransferNanosNumOps', '')],
        [('DataNode', 'SendDataPacketTransferNanosAvgTime', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60sNumOps', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60s50thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60s75thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60s90thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60s95thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60s99thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300sNumOps', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300s50thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300s75thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300s90thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300s95thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300s99thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900sNumOps', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900s50thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900s75thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900s90thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900s95thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900s99thPercentileLatency', '')],
        [('DataNode', 'FlushNanos60sNumOps', '')],
        [('DataNode', 'FlushNanos60s50thPercentileLatency', '')],
        [('DataNode', 'FlushNanos60s75thPercentileLatency', '')],
        [('DataNode', 'FlushNanos60s90thPercentileLatency', '')],
        [('DataNode', 'FlushNanos60s95thPercentileLatency', '')],
        [('DataNode', 'FlushNanos60s99thPercentileLatency', '')],
        [('DataNode', 'FlushNanos300sNumOps', '')],
        [('DataNode', 'FlushNanos300s50thPercentileLatency', '')],
        [('DataNode', 'FlushNanos300s75thPercentileLatency', '')],
        [('DataNode', 'FlushNanos300s90thPercentileLatency', '')],
        [('DataNode', 'FlushNanos300s95thPercentileLatency', '')],
        [('DataNode', 'FlushNanos300s99thPercentileLatency', '')],
        [('DataNode', 'FlushNanos900sNumOps', '')],
        [('DataNode', 'FlushNanos900s50thPercentileLatency', '')],
        [('DataNode', 'FlushNanos900s75thPercentileLatency', '')],
        [('DataNode', 'FlushNanos900s90thPercentileLatency', '')],
        [('DataNode', 'FlushNanos900s95thPercentileLatency', '')],
        [('DataNode', 'FlushNanos900s99thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos60sNumOps', '')],
        [('DataNode', 'FsyncNanos60s50thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos60s75thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos60s90thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos60s95thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos60s99thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos300sNumOps', '')],
        [('DataNode', 'FsyncNanos300s50thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos300s75thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos300s90thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos300s95thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos300s99thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos900sNumOps', '')],
        [('DataNode', 'FsyncNanos900s50thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos900s75thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos900s90thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos900s95thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos900s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60sNumOps', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300sNumOps', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900sNumOps', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos60sNumOps', '')],
        [('DataNode', 'SendDataPacketTransferNanos60s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos60s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos60s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos60s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos60s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos300sNumOps', '')],
        [('DataNode', 'SendDataPacketTransferNanos300s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos300s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos300s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos300s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos300s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos900sNumOps', '')],
        [('DataNode', 'SendDataPacketTransferNanos900s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos900s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos900s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos900s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos900s99thPercentileLatency', '')],
      ]),
    ],
  },
  'hbase' : {
    'master':[
      ('Operation', [
        [('HBase', 'putNumOps', 'op(s)')],
        [('HBase', 'putAvgTime', 'us(s)')],
        [('HBase', 'checkAndPutNumOps', 'op(s)')],
        [('HBase', 'checkAndPutAvgTime', 'us(s)')],
        [('HBase', 'getNumOps', 'op(s)')],
        [('HBase', 'getAvgTime', 'us(s)')],
        [('HBase', 'deleteNumOps', 'op(s)')],
        [('HBase', 'deleteAvgTime', 'us(s)')],
      ]),
      ('RPC', [
        [('HBase', 'RpcQueueTimeNumOps', 'op(s)')],
        [('HBase', 'RpcQueueTimeAvgTime', 'ms(s)')],
        [('HBase', 'RpcProcessingTimeNumOps', 'op(s)')],
        [('HBase', 'RpcProcessingTimeAvgTime', 'us(s)')],
        [('HBase', 'RpcSlowResponseNumOps', 'op(s)')],
        [('HBase', 'RpcSlowResponseAvgTime', 'ms(s)')],
      ]),
      ('JvmStatistics', [
        [('Master', 'memHeapCommittedM', 'MB')],
        [('Master', 'fatalCount', 'count(s)')],
        [('Master', 'threadsWaiting', 'thread(s)')],
        [('Master', 'threadsBlocked', 'thread(s)')],
        [('Master', 'gcCount', 'count(s)')],
        [('Master', 'errorCount', 'count(s)')],
        [('Master', 'memNonHeapCommittedM', 'MB')],
        [('Master', 'warnCount', 'count(s)')],
        [('Master', 'gcTimeMillis', 'ms(s)')],
        [('Master', 'memNonHeapUsedM', 'MB')],
        [('Master', 'memHeapUsedM', 'MB')],
        [('Master', 'threadsNew', 'thread(s)')],
        [('Master', 'threadsTerminated', 'thread(s)')],
        [('Master', 'threadsTimedWaiting', 'thread(s)')],
        [('Master', 'maxMemoryM', 'MB')],
        [('Master', 'infoCount', 'count(s)')],
        [('Master', 'threadsRunnable', 'thread(s)')],
      ]),
    ],
    'regionserver': [
      ('Operation', [
        [('HBase', 'multiNumOps', 'op(s)')],
        [('HBase', 'multiAvgTime', 'us(s)')],
        [('HBase', 'checkAndPutNumOps', 'op(s)')],
        [('HBase', 'checkAndPutAvgTime', 'us(s)')],
        [('HBase', 'getNumOps', 'op(s)')],
        [('HBase', 'getAvgTime', 'us(s)')],
        [('HBase', 'openScannerNumOps', 'op(s)')],
        [('HBase', 'openScannerAvgTime', 'us(s)')],
        [('HBase', 'nextNumOps', 'op(s)')],
        [('HBase', 'nextAvgTime', 'us(s)')],
        [('HBase', 'deleteNumOps', 'op(s)')],
        [('HBase', 'deleteAvgTime', 'us(s)')],
      ]),
      ('RPC', [
        [('HBase', 'RpcQueueTimeNumOps', 'op(s)')],
        [('HBase', 'RpcQueueTimeAvgTime', 'ms(s)')],
        [('HBase', 'RpcProcessingTimeNumOps', 'op(s)')],
        [('HBase', 'RpcProcessingTimeAvgTime', 'us(s)')],
        [('HBase', 'RpcSlowResponseNumOps', 'op(s)')],
        [('HBase', 'RpcSlowResponseAvgTime', 'ms(s)')],
      ]),
      ('Store', [
        [('RegionServer', 'regions', 'region(s)')],
        [('RegionServer', 'memstoreSizeMB', 'MB')],
        [('RegionServer', 'storefileIndexSizeMB', 'MB')],
        [('RegionServer', 'storeFileSizeMB', 'MB')],
        [('RegionServer', 'storefiles', 'file(s)')],
        [('RegionServer', 'stores', 'store(s)')],
        [('RegionServer', 'largeCompactionQueueSize', 'count(s)')],
        [('RegionServer', 'smallCompactionQueueSize', 'count(s)')],
        [('RegionServer', 'compactionTime', 'ms(s)')],
        [('RegionServer', 'compactionSize', 'byte(s)')],
        [('RegionServer', 'flushQueueSize', 'count(s)')],
        [('RegionServer', 'flushTime', 'second(s)')],
        [('RegionServer', 'flushSize', 'byte(s)')],
        [('RegionServer', 'hlogRollCount', 'count(s)')],
      ]),
      ('BlockCache', [
        [('RegionServer', 'blockCacheCount', 'count(s)')],
        [('RegionServer', 'blockCacheFree', 'byte(s)')],
        [('RegionServer', 'blockCacheHitRatio', '%')],
        [('RegionServer', 'blockCacheHitCount', 'count(s)')],
        [('RegionServer', 'blockCacheMissCount', 'count(s)')],
        [('RegionServer', 'blockCacheSize', 'byte(s)' )],
      ]),
      ('FileSystem', [
        [('RegionServer', 'fsReadLatencyNumOps', 'op(s)')],
        [('RegionServer', 'fsReadLatencyAvgTime', 'ms(s)')],
        [('RegionServer', 'fsPreadLatencyNumOps', 'op(s)')],
        [('RegionServer', 'fsPreadLatencyAvgTime', 'ms(s)')],
        [('RegionServer', 'fsWriteLatencyNumOps', 'op(s)')],
        [('RegionServer', 'fsWriteLatencyAvgTime', 'ms(s)')],
        [('RegionServer', 'fsSyncLatencyNumOps', 'op(s)')],
        [('RegionServer', 'fsSyncLatencyAvgTime', 'ms(s)')],
      ]),
      ('JvmStatistics', [
        [('RegionServer', 'memHeapCommittedM', 'MB')],
        [('RegionServer', 'fatalCount', 'count(s)')],
        [('RegionServer', 'threadsWaiting', 'thread(s)')],
        [('RegionServer', 'threadsBlocked', 'thread(s)')],
        [('RegionServer', 'gcCount', 'count(s)')],
        [('RegionServer', 'errorCount', 'count(s)')],
        [('RegionServer', 'memNonHeapCommittedM', 'MB')],
        [('RegionServer', 'warnCount', 'count(s)')],
        [('RegionServer', 'gcTimeMillis', 'ms(s)')],
        [('RegionServer', 'memNonHeapUsedM', 'MB')],
        [('RegionServer', 'memHeapUsedM', 'MB')],
        [('RegionServer', 'threadsNew', 'thread(s)')],
        [('RegionServer', 'threadsTerminated', 'thread(s)')],
        [('RegionServer', 'threadsTimedWaiting', 'thread(s)')],
        [('RegionServer', 'maxMemoryM', 'MB')],
        [('RegionServer', 'infoCount', 'count(s)')],
        [('RegionServer', 'threadsRunnable', 'thread(s)')],
      ]),
    ],
  }
}

JOB_METRICS_VIEW_CONFIG = {
  'hdfs': {
    'journalnode':[
      ('Rpc', [
        [('JournalNode', 'ReceivedBytes', 'byte(s)')],
        [('JournalNode', 'SentBytes', 'byte(s)')],
        [('JournalNode', 'RpcQueueTimeNumOps', 'req(s)')],
        [('JournalNode', 'RpcQueueTimeAvgTime', 'ms(s)')],
      ]),
    ],
    'namenode':[
      # view
      ('Overall', [
        # graph
        [('NameNode', 'BlockCapacity', 'block(s)')],
        [('NameNode', 'BlocksTotal', 'block(s)')],
        [('NameNode', 'CapacityRemainingGB', 'GB')],
        [('NameNode', 'CapacityTotalGB', 'GB')],
        [('NameNode', 'CapacityUsedGB', 'GB')],
        [('NameNode', 'CorruptBlocks', 'block(s)')],
        [('NameNode', 'ExcessBlocks', 'block(s)')],
        [('NameNode', 'FilesTotal', 'file(s)')],
      ]),
      ('Operation', [
        [('NameNode', 'AddBlockOps', 'op(s)')],
        [('NameNode', 'CreateFileOps', 'op(s)')],
        [('NameNode', 'DeleteFileOps', 'op(s)')],
        [('NameNode', 'FileInfoOps', 'op(s)')],
        [('NameNode', 'GetListingOps', 'op(s)')],
      ]),
      ('Rpc', [
        [('NameNode', 'ReceivedBytes', 'byte(s)')],
        [('NameNode', 'SentBytes', 'byte(s)')],
        [('NameNode', 'RpcQueueTimeNumOps', 'op(s)')],
        [('NameNode', 'RpcQueueTimeAvgTime', 'ms(s)')],
      ]
      )
    ],
    'datanode':[
      ('BlockOperation', [
        [('DataNode', 'BlockReportsAvgTime', 'ms(s)')],
        [('DataNode', 'BlockReportsNumOps', 'op(s)')],
        [('DataNode', 'BlocksGetLocalPathInfo', '')],
        [('DataNode', 'BlocksRead', 'block(s)')],
        [('DataNode', 'BlocksRemoved', 'block(s)')],
        [('DataNode', 'BlocksReplicated', 'block(s)')],
        [('DataNode', 'BlocksVerified', 'block(s)')],
        [('DataNode', 'BlocksWritten', 'block(s)')],
      ]),
      ('Activity', [
        [('DataNode', 'BytesWritten', '')],
        [('DataNode', 'BytesRead', '')],
        [('DataNode', 'BlocksWritten', '')],
        [('DataNode', 'BlocksRead', '')],
        [('DataNode', 'BlocksReplicated', '')],
        [('DataNode', 'BlocksRemoved', '')],
        [('DataNode', 'BlocksVerified', '')],
        [('DataNode', 'BlockVerificationFailures', '')],
        [('DataNode', 'ReadsFromLocalClient', '')],
        [('DataNode', 'ReadsFromRemoteClient', '')],
        [('DataNode', 'WritesFromLocalClient', '')],
        [('DataNode', 'WritesFromRemoteClient', '')],
        [('DataNode', 'BlocksGetLocalPathInfo', '')],
        [('DataNode', 'FsyncCount', '')],
        [('DataNode', 'VolumeFailures', '')],
        [('DataNode', 'ReadBlockOpNumOps', '')],
        [('DataNode', 'ReadBlockOpAvgTime', '')],
        [('DataNode', 'WriteBlockOpNumOps', '')],
        [('DataNode', 'WriteBlockOpAvgTime', '')],
        [('DataNode', 'BlockChecksumOpNumOps', '')],
        [('DataNode', 'BlockChecksumOpAvgTime', '')],
        [('DataNode', 'CopyBlockOpNumOps', '')],
        [('DataNode', 'CopyBlockOpAvgTime', '')],
        [('DataNode', 'ReplaceBlockOpNumOps', '')],
        [('DataNode', 'ReplaceBlockOpAvgTime', '')],
        [('DataNode', 'HeartbeatsNumOps', '')],
        [('DataNode', 'HeartbeatsAvgTime', '')],
        [('DataNode', 'BlockReportsNumOps', '')],
        [('DataNode', 'BlockReportsAvgTime', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanosNumOps', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanosAvgTime', '')],
        [('DataNode', 'FlushNanosNumOps', '')],
        [('DataNode', 'FlushNanosAvgTime', '')],
        [('DataNode', 'FsyncNanosNumOps', '')],
        [('DataNode', 'FsyncNanosAvgTime', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanosNumOps', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanosAvgTime', '')],
        [('DataNode', 'SendDataPacketTransferNanosNumOps', '')],
        [('DataNode', 'SendDataPacketTransferNanosAvgTime', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60sNumOps', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60s50thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60s75thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60s90thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60s95thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos60s99thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300sNumOps', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300s50thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300s75thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300s90thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300s95thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos300s99thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900sNumOps', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900s50thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900s75thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900s90thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900s95thPercentileLatency', '')],
        [('DataNode', 'PacketAckRoundTripTimeNanos900s99thPercentileLatency', '')],
        [('DataNode', 'FlushNanos60sNumOps', '')],
        [('DataNode', 'FlushNanos60s50thPercentileLatency', '')],
        [('DataNode', 'FlushNanos60s75thPercentileLatency', '')],
        [('DataNode', 'FlushNanos60s90thPercentileLatency', '')],
        [('DataNode', 'FlushNanos60s95thPercentileLatency', '')],
        [('DataNode', 'FlushNanos60s99thPercentileLatency', '')],
        [('DataNode', 'FlushNanos300sNumOps', '')],
        [('DataNode', 'FlushNanos300s50thPercentileLatency', '')],
        [('DataNode', 'FlushNanos300s75thPercentileLatency', '')],
        [('DataNode', 'FlushNanos300s90thPercentileLatency', '')],
        [('DataNode', 'FlushNanos300s95thPercentileLatency', '')],
        [('DataNode', 'FlushNanos300s99thPercentileLatency', '')],
        [('DataNode', 'FlushNanos900sNumOps', '')],
        [('DataNode', 'FlushNanos900s50thPercentileLatency', '')],
        [('DataNode', 'FlushNanos900s75thPercentileLatency', '')],
        [('DataNode', 'FlushNanos900s90thPercentileLatency', '')],
        [('DataNode', 'FlushNanos900s95thPercentileLatency', '')],
        [('DataNode', 'FlushNanos900s99thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos60sNumOps', '')],
        [('DataNode', 'FsyncNanos60s50thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos60s75thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos60s90thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos60s95thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos60s99thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos300sNumOps', '')],
        [('DataNode', 'FsyncNanos300s50thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos300s75thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos300s90thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos300s95thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos300s99thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos900sNumOps', '')],
        [('DataNode', 'FsyncNanos900s50thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos900s75thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos900s90thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos900s95thPercentileLatency', '')],
        [('DataNode', 'FsyncNanos900s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60sNumOps', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos60s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300sNumOps', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos300s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900sNumOps', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketBlockedOnNetworkNanos900s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos60sNumOps', '')],
        [('DataNode', 'SendDataPacketTransferNanos60s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos60s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos60s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos60s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos60s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos300sNumOps', '')],
        [('DataNode', 'SendDataPacketTransferNanos300s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos300s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos300s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos300s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos300s99thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos900sNumOps', '')],
        [('DataNode', 'SendDataPacketTransferNanos900s50thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos900s75thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos900s90thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos900s95thPercentileLatency', '')],
        [('DataNode', 'SendDataPacketTransferNanos900s99thPercentileLatency', '')],
      ]),
    ],
  },
  'hbase' : {
    'master':[
      ('Operation', [
        [('HBase', 'putNumOps', 'op(s)')],
        [('HBase', 'putAvgTime', 'us(s)')],
        [('HBase', 'checkAndPutNumOps', 'op(s)')],
        [('HBase', 'checkAndPutAvgTime', 'us(s)')],
        [('HBase', 'getNumOps', 'op(s)')],
        [('HBase', 'getAvgTime', 'us(s)')],
        [('HBase', 'deleteNumOps', 'op(s)')],
        [('HBase', 'deleteAvgTime', 'us(s)')],
      ]),
      ('RPC', [
        [('HBase', 'RpcQueueTimeNumOps', 'op(s)')],
        [('HBase', 'RpcQueueTimeAvgTime', 'ms(s)')],
        [('HBase', 'RpcProcessingTimeNumOps', 'op(s)')],
        [('HBase', 'RpcProcessingTimeAvgTime', 'us(s)')],
        [('HBase', 'RpcSlowResponseNumOps', 'op(s)')],
        [('HBase', 'RpcSlowResponseAvgTime', 'ms(s)')],
      ]),
      ('JvmStatistics', [
        [('Master', 'memHeapCommittedM', 'MB')],
        [('Master', 'fatalCount', 'count(s)')],
        [('Master', 'threadsWaiting', 'thread(s)')],
        [('Master', 'threadsBlocked', 'thread(s)')],
        [('Master', 'gcCount', 'count(s)')],
        [('Master', 'errorCount', 'count(s)')],
        [('Master', 'memNonHeapCommittedM', 'MB')],
        [('Master', 'warnCount', 'count(s)')],
        [('Master', 'gcTimeMillis', 'ms(s)')],
        [('Master', 'memNonHeapUsedM', 'MB')],
        [('Master', 'memHeapUsedM', 'MB')],
        [('Master', 'threadsNew', 'thread(s)')],
        [('Master', 'threadsTerminated', 'thread(s)')],
        [('Master', 'threadsTimedWaiting', 'thread(s)')],
        [('Master', 'maxMemoryM', 'MB')],
        [('Master', 'infoCount', 'count(s)')],
        [('Master', 'threadsRunnable', 'thread(s)')],
      ]),
    ],
    'regionserver': [
      ('Operation', [
        [('HBase', 'multiNumOps', 'op(s)')],
        [('HBase', 'multiAvgTime', 'us(s)')],
        [('HBase', 'checkAndPutNumOps', 'op(s)')],
        [('HBase', 'checkAndPutAvgTime', 'us(s)')],
        [('HBase', 'getNumOps', 'op(s)')],
        [('HBase', 'getAvgTime', 'us(s)')],
        [('HBase', 'openScannerNumOps', 'op(s)')],
        [('HBase', 'openScannerAvgTime', 'us(s)')],
        [('HBase', 'nextNumOps', 'op(s)')],
        [('HBase', 'nextAvgTime', 'us(s)')],
        [('HBase', 'deleteNumOps', 'op(s)')],
        [('HBase', 'deleteAvgTime', 'us(s)')],
      ]),
      ('RPC', [
        [('HBase', 'RpcQueueTimeNumOps', 'op(s)')],
        [('HBase', 'RpcQueueTimeAvgTime', 'ms(s)')],
        [('HBase', 'RpcProcessingTimeNumOps', 'op(s)')],
        [('HBase', 'RpcProcessingTimeAvgTime', 'us(s)')],
        [('HBase', 'RpcSlowResponseNumOps', 'op(s)')],
        [('HBase', 'RpcSlowResponseAvgTime', 'ms(s)')],
      ]),
      ('Store', [
        [('RegionServer', 'regions', 'region(s)')],
        [('RegionServer', 'memstoreSizeMB', 'MB')],
        [('RegionServer', 'storefileIndexSizeMB', 'MB')],
        [('RegionServer', 'storeFileSizeMB', 'MB')],
        [('RegionServer', 'storefiles', 'file(s)')],
        [('RegionServer', 'stores', 'store(s)')],
        [('RegionServer', 'largeCompactionQueueSize', 'count(s)')],
        [('RegionServer', 'smallCompactionQueueSize', 'count(s)')],
        [('RegionServer', 'compactionTime', 'ms(s)')],
        [('RegionServer', 'compactionSize', 'byte(s)')],
        [('RegionServer', 'flushQueueSize', 'count(s)')],
        [('RegionServer', 'flushTime', 'second(s)')],
        [('RegionServer', 'flushSize', 'byte(s)')],
        [('RegionServer', 'hlogRollCount', 'count(s)')],
      ]),
      ('BlockCache', [
        [('RegionServer', 'blockCacheCount', 'count(s)')],
        [('RegionServer', 'blockCacheFree', 'byte(s)')],
        [('RegionServer', 'blockCacheHitRatio', '%')],
        [('RegionServer', 'blockCacheHitCount', 'count(s)')],
        [('RegionServer', 'blockCacheMissCount', 'count(s)')],
        [('RegionServer', 'blockCacheSize', 'byte(s)' )],
      ]),
      ('Replication', [
        [('Replication', 'sizeOfLogQueue-5', '')],
        [('Replication', 'ageOfLastShippedOp-5', '')],
        [('Replication', 'logEditsReadRate-5', '')],
        [('Replication', 'shippedOpsRate-5', '')],
        [('Replication', 'logEditsFilteredRate-5', '')],
        [('Replication', 'shippedBatchesRate-5', '')],
      ]),
      ('FileSystem', [
        [('RegionServer', 'fsReadLatencyNumOps', 'op(s)')],
        [('RegionServer', 'fsReadLatencyAvgTime', 'ms(s)')],
        [('RegionServer', 'fsPreadLatencyNumOps', 'op(s)')],
        [('RegionServer', 'fsPreadLatencyAvgTime', 'ms(s)')],
        [('RegionServer', 'fsWriteLatencyNumOps', 'op(s)')],
        [('RegionServer', 'fsWriteLatencyAvgTime', 'ms(s)')],
        [('RegionServer', 'fsSyncLatencyNumOps', 'op(s)')],
        [('RegionServer', 'fsSyncLatencyAvgTime', 'ms(s)')],
      ]),
      ('JvmStatistics', [
        [('RegionServer', 'memHeapCommittedM', 'MB')],
        [('RegionServer', 'fatalCount', 'count(s)')],
        [('RegionServer', 'threadsWaiting', 'thread(s)')],
        [('RegionServer', 'threadsBlocked', 'thread(s)')],
        [('RegionServer', 'gcCount', 'count(s)')],
        [('RegionServer', 'errorCount', 'count(s)')],
        [('RegionServer', 'memNonHeapCommittedM', 'MB')],
        [('RegionServer', 'warnCount', 'count(s)')],
        [('RegionServer', 'gcTimeMillis', 'ms(s)')],
        [('RegionServer', 'memNonHeapUsedM', 'MB')],
        [('RegionServer', 'memHeapUsedM', 'MB')],
        [('RegionServer', 'threadsNew', 'thread(s)')],
        [('RegionServer', 'threadsTerminated', 'thread(s)')],
        [('RegionServer', 'threadsTimedWaiting', 'thread(s)')],
        [('RegionServer', 'maxMemoryM', 'MB')],
        [('RegionServer', 'infoCount', 'count(s)')],
        [('RegionServer', 'threadsRunnable', 'thread(s)')],
      ]),
    ],
  }
}

DEFAULT_OPS_UNIT = "op(s)"
DEFAULT_LATENCY_UNIT = "us(s)"

REGION_SERVER_OPERATION_VIEW_CONFIG = ['multi', 'get', 'openScanner', 'next',
                                       'delete', 'checkAndPut', 'execCoprocessor']
REPLICATION_METRICS_VIEW_CONFIG = [('sizeOfLogQueue', 'count(s)'), ('ageOfLastShippedOp', 'ms(s)'),
				   ('logEditsReadRate', 'op(s)'), ('shippedOpsRate', 'op(s)',),
				   ('logEditsFilteredRate', 'op(s)'), ('shippedBatchesRate', 'op(s)'),
                                   ('logReadRateInByte', 'byte(s)')]

########NEW FILE########
__FILENAME__ = models
# -*- coding: utf-8 -*-
from django.db import models
from django.utils import timezone

import datetime
import json


DEFAULT_DATETIME = datetime.datetime(1970, 1, 1, tzinfo=timezone.utc)
# If a cluster/job/task's last success time has passed this many seconds, it's
# considered as fialed.
FAIL_TIME = 30

# The item could be cluster, job, or task.
def is_healthy(item, fail_time=FAIL_TIME):
  delta = datetime.timedelta(seconds=fail_time)
  if item.last_success_time + delta < datetime.datetime.now(tz=timezone.utc):
    return False
  return True


class Status:
  OK = 0
  WARN = 1
  ERROR = 2


class Service(models.Model):
  # Name of the service, like "hdfs", "hbase", etc.
  name = models.CharField(max_length=128)
  # Url to get metrics which is formatted in json.
  metric_url = models.CharField(max_length=128)
  # If the service is being actively monitored. We don't want to delete metrics
  # data once a service/cluster/job is deactive, so just use a boolean field to
  # indicate it.
  active = models.BooleanField(default=True)
  # A text description.
  description = models.CharField(max_length=1024)

  def __unicode__(self):
    return self.name


class Cluster(models.Model):
  # Each cluster must belong to one service.
  service = models.ForeignKey(Service, db_index=True)
  # The cluster name like "ggsrv-miliao", "sdtst-test", etc.
  name = models.CharField(max_length=128)
  # The same as service.
  active = models.BooleanField(default=True)
  # A text description.
  description = models.CharField(max_length=1024)
  # The last attempt time to fetch metics, whether successful or failed.
  # It's the time of client initiating the request, not the time of client
  # receiving the response.
  last_attempt_time = models.DateTimeField(default=DEFAULT_DATETIME)
  # If the last attempt is successful.
  last_status = models.IntegerField(default=Status.ERROR)
  # The status mesage of last attempt.
  last_message = models.CharField(max_length=128)
  # The last update time of this task's metics, must be successful.
  # The definition is the same as last_attempt.
  last_success_time = models.DateTimeField(default=DEFAULT_DATETIME)
  # cluster version in format: "version, revision"
  version = models.CharField(max_length=128)
  # Entry for service's native main page
  entry = models.CharField(max_length=128)

  @property
  def health(self):
    return is_healthy(self)

  def __unicode__(self):
    return u"%s/%s" % (unicode(self.service), self.name)


class Job(models.Model):
  # Each job must belong to one cluster.
  cluster = models.ForeignKey(Cluster, db_index=True)
  # The job name like "namenode", "regionserver", etc.
  name = models.CharField(max_length=128)
  # The same as service.
  active = models.BooleanField(default=True)
  # A text description.
  description = models.CharField(max_length=1024)
  # How many tasks are in running and healthy.
  running_tasks_count = models.IntegerField(default=0)
  # How many tasks in total.
  total_tasks_count = models.IntegerField(default=0)
  # The last attempt time to fetch metics, whether successful or failed.
  # It's the time of client initiating the request, not the time of client
  # receiving the response.
  last_attempt_time = models.DateTimeField(default=DEFAULT_DATETIME)
  # If the last attempt is successful.
  last_status = models.IntegerField(default=Status.ERROR)
  # The status mesage of last attempt.
  last_message = models.CharField(max_length=128)
  # The last update time of this task's metics, must be successful.
  # The definition is the same as last_attempt.
  last_success_time = models.DateTimeField(default=DEFAULT_DATETIME)

  @property
  def health(self):
    return is_healthy(self)

  def __unicode__(self):
    return u"%s/%s" % (unicode(self.cluster), self.name)


class Task(models.Model):
  job = models.ForeignKey(Job, db_index=True)
  # The task id.
  task_id = models.IntegerField()
  # The ip or hostname that the task is running on.
  host = models.CharField(max_length=128)
  # The port number where we could get metrics data from.
  port = models.IntegerField()
  # The same as service.
  active = models.BooleanField(default=True)
  # The last attempt time to fetch metics, whether successful or failed.
  # It's the time of client initiating the request, not the time of client
  # receiving the response.
  last_attempt_time = models.DateTimeField(default=DEFAULT_DATETIME)
  # If the last attempt is successful.
  last_status = models.IntegerField(default=Status.ERROR)
  # The status mesage of last attempt.
  last_message = models.CharField(max_length=128)
  # The last update time of this task's metics, must be successful.
  # The definition is the same as last_attempt.
  last_success_time = models.DateTimeField(default=DEFAULT_DATETIME)
  # The last metric values, encoded in json.
  last_metrics = models.TextField()
  # The last raw metric values fetched from http server, for debug purpose
  last_metrics_raw = models.TextField()

  class Meta:
    index_together = [["host", "port"],]

  @property
  def health(self):
    return is_healthy(self)

  def __unicode__(self):
    return u"%s/%d" % (unicode(self.job), self.task_id)

class HBaseCluster(models.Model):
  cluster = models.OneToOneField(Cluster, db_index=True)

  memStoreSizeMB = models.IntegerField(default = 0)
  storefileSizeMB = models.IntegerField(default = 0)
  # readRequestsCount and writeRequestsCount may exceed max integer
  readRequestsCount = models.FloatField(default = 0, max_length = 20)
  writeRequestsCount = models.FloatField(default = 0, max_length = 20)
  readRequestsCountPerSec = models.FloatField(default = 0, max_length = 20)
  writeRequestsCountPerSec = models.FloatField(default = 0, max_length = 20)
  operationMetrics = models.TextField() # save operation metrics as json format

class RegionServer(models.Model):
  cluster = models.ForeignKey(Cluster, db_index=True)
  task = models.OneToOneField(Task, db_index=True)
  name = models.CharField(max_length=128)
  last_attempt_time = models.DateTimeField(default=DEFAULT_DATETIME)
  load = models.IntegerField(default = 0)
  numberOfRegions = models.IntegerField(default = 0)
  numberOfRequests = models.IntegerField(default = 0)

  memStoreSizeMB = models.IntegerField(default = 0)
  storefileSizeMB = models.IntegerField(default = 0)
  readRequestsCount = models.IntegerField(default = 0)
  writeRequestsCount = models.IntegerField(default = 0)
  readRequestsCountPerSec = models.FloatField(default = 0, max_length = 20)
  writeRequestsCountPerSec = models.FloatField(default = 0, max_length = 20)
  replication_last_attempt_time = models.DateTimeField(default=DEFAULT_DATETIME)
  replicationMetrics = models.TextField() # save replication metrics as json format

  def __unicode__(self):
    return unicode(self.name.split(',')[0])

class Table(models.Model):
  cluster = models.ForeignKey(Cluster, db_index=True)
  name = models.CharField(max_length=128)
  last_attempt_time = models.DateTimeField(default=DEFAULT_DATETIME)

  memStoreSizeMB = models.IntegerField(default = 0)
  storefileSizeMB = models.IntegerField(default = 0)
  readRequestsCount = models.IntegerField(default = 0)
  writeRequestsCount = models.IntegerField(default = 0)
  readRequestsCountPerSec = models.FloatField(default = 0, max_length = 20)
  writeRequestsCountPerSec = models.FloatField(default = 0, max_length = 20)

  availability = models.FloatField(default=-1.0)
  operationMetrics = models.TextField() # save operation metrics as json format

  def __unicode__(self):
    return unicode(self.name)

ROOT_TABLE_NAME = '-ROOT-'
META_TABLE_NAME = '.META.'
ROOT_REGION_ENCODING_NAME = '70236052'
META_REGION_ENCODING_NAME = '1028785192'

class Region(models.Model):
  table = models.ForeignKey(Table, db_index=True)
  region_server = models.ForeignKey(RegionServer, db_index=True)
  last_attempt_time = models.DateTimeField(default=DEFAULT_DATETIME)
  name = models.CharField(max_length=256)
  encodeName = models.CharField(max_length = 128, db_index=True)

  memStoreSizeMB = models.IntegerField(default = 0)
  storefileSizeMB = models.IntegerField(default = 0)
  readRequestsCount = models.IntegerField(default = 0)
  writeRequestsCount = models.IntegerField(default = 0)
  readRequestsCountPerSec = models.FloatField(default = 0, max_length = 20)
  writeRequestsCountPerSec = models.FloatField(default = 0, max_length = 20)

  currentCompactedKVs = models.IntegerField(default = 0)
  requestsCount = models.IntegerField(default = 0)
  rootIndexSizeKB = models.IntegerField(default = 0)
  storefileIndexSizeMB = models.IntegerField(default = 0)
  storefiles = models.IntegerField(default = 0)
  stores = models.IntegerField(default = 0)
  totalCompactingKVs = models.IntegerField(default = 0)
  totalStaticBloomSizeKB = models.IntegerField(default = 0)
  totalStaticIndexSizeKB = models.IntegerField(default = 0)
  version = models.IntegerField(default = 0)
  last_operation_attempt_time = models.DateTimeField(default=DEFAULT_DATETIME)
  operationMetrics = models.TextField() # save operation metrics as json format

  # root and meta region use old region format, where root regon name is: '-ROOT-,,0.70236052'
  # and first meta region name is: .META.,,1.1028785192. 70236052 and 1028785192 will serve as
  # encode name for root region and meta region respectively. Other data region use new format,
  # such as hbase_client_test_table,01,1369368306964.7be6b8bda3e59d5e6d4556482fc84601. in which
  # 7be6b8bda3e59d5e6d4556482fc84601 will serve as encode name
  @staticmethod
  def get_encode_name(name):
    if name[0:6] == ROOT_TABLE_NAME:
      return ROOT_REGION_ENCODING_NAME
    if name[0:6] == META_TABLE_NAME:
      return META_REGION_ENCODING_NAME
    return name.split('.')[1]

  # the region operation metric name(AvgTime) for 'multiput' seems like:
  # tbl.hbase_client_test_table.region.9fdec6d4dbb175e2b098e16fc5987dcb.multiput_AvgTime where
  # 9fdec6d4dbb175e2b098e16fc5987dcb is the encode name
  @staticmethod
  def is_region_operation_metric_name(name):
    if name.find('tbl') >= 0 and name.find('region') >= 0:
      return True
    return False

  @staticmethod
  def get_encode_name_from_region_operation_metric_name(name):
    tokens = name.split('.')
    return tokens[len(tokens) - 2]

  def analyze_region_record(self, region_value, update_time):
    time_interval = (update_time - self.last_attempt_time).seconds
    self.readRequestsCountPerSec = \
        (float)(region_value['readRequestsCount'] - self.readRequestsCount)\
        / time_interval
    if self.readRequestsCountPerSec < 0:
      self.readRequestsCountPerSec = 0
    self.writeRequestsCountPerSec = \
        (float)(region_value['writeRequestsCount'] - self.writeRequestsCount)\
        / time_interval
    if self.writeRequestsCountPerSec < 0:
      self.writeRequestsCountPerSec = 0
    self.last_attempt_time = update_time
    self.memStoreSizeMB = region_value['memStoreSizeMB']
    self.storefileSizeMB = region_value['storefileSizeMB']
    self.readRequestsCount = region_value['readRequestsCount']
    self.writeRequestsCount = region_value['writeRequestsCount']
    self.requestsCount = region_value['requestsCount']

  # operation metric from jmx is formatted as: 'tbl.tableName.region.encodeName.operationName_Suffix : value'
  # where Suffix could be OpsNum, AvgTime, MaxTime, MinTime, histogram_75percentile, histogram_95percentile etc.
  # We save all operation metrics as the a map: {operationName : {{OpsNum : value}, {AvgTime, value}, ...}}.
  # Then, the map will be converted to a json format and into self.operationMetrics
  def analyze_from_region_server_operation_metrics(self, region_operation_metrics, update_time):
    self.last_operation_attempt_time = update_time
    metric_saved = {}
    for region_operation in region_operation_metrics.keys():
      tokens = region_operation.split('.')
      tokens = tokens[len(tokens) - 1].split('_')
      tokens_len = len(tokens)

      index = 0
      while index < tokens_len:
        if tokens[index] == 'histogram':
          break;
        index = index + 1

      operationName = ''
      suffix = ''
      if index < tokens_len:
        # for histogram metics
        operationName = '_'.join(tokens[0 : index])
        suffix = '_'.join(tokens[index : tokens_len])
      else:
        operationName = '_'.join(tokens[0 : tokens_len - 1])
        suffix = tokens[tokens_len - 1]

      operationMetric = metric_saved.setdefault(operationName, {})
      operationMetric[suffix] = region_operation_metrics[region_operation]
    self.operationMetrics = json.dumps(metric_saved)

  def __unicode__(self):
    return unicode(self.name)

  def __str__(self):
    return repr(','.join((self.name.split(',')[:2]))).replace("'", '')

class Counter(models.Model):
  # The from ip of the counter
  host = models.CharField(max_length=16)
  # The group name of the counter
  group = models.CharField(max_length=64)
  name = models.CharField(max_length=128)

  # The last update time of the counter
  last_update_time = models.DateTimeField(default=DEFAULT_DATETIME)

  value = models.FloatField(default=0)
  # The unit of the value, reqs/s, ms, ...
  unit = models.CharField(max_length=16)
  # The label of the counter, used to display in corresponding section of html page
  label = models.CharField(max_length=64)

  def identity(self):
    return u"%s-%s" % (self.group, self.name)

  class Meta:
    unique_together = ("group", "name")

  def __unicode__(self):
    return u"%s/%s/%s/%s" % (self.host, self.group, self.name, self.last_update_time)

class Quota(models.Model):
  cluster = models.ForeignKey(Cluster, db_index=True)
  name = models.CharField(max_length=256)
  quota = models.CharField(max_length=16)
  used_quota = models.CharField(max_length=16)
  remaining_quota = models.CharField(max_length=16)
  space_quota = models.CharField(max_length=16)
  used_space_quota = models.CharField(max_length=16)
  remaining_space_quota = models.CharField(max_length=16)

  last_update_time = models.DateTimeField(default=DEFAULT_DATETIME)

  class Meta:
    unique_together = ("cluster", "name")

  def __unicode__(self):
    return u"%s/%s:%s" % (unicode(self.cluster), self.name,self.last_update_time)

########NEW FILE########
__FILENAME__ = extended_filter
# extended filter for django template

from django import template
import utils.quota_util

register = template.Library()

# generate param for group
@register.filter(name='param_group')
def param_group(graph_config) :
  return '|'.join([group for group, key in graph_config])

# generate param for key in graph
@register.filter(name='param_key')
def param_key(graph_config):
  return '|'.join(['-'.join((group,key)) for group, key, unit in graph_config])

# generate param for multikey in view
@register.filter(name='param_multikey_for_view')
def param_multikey_for_view(view_config):
  return '|'.join([param_key(graph_config) for graph_config in view_config])

# generate param for multikey in view
@register.filter(name='param_height')
def param_height(view_config):
  graph_per_row = 3
  height_per_row = 295
  return (len(view_config) + (graph_per_row - 1)) / graph_per_row * height_per_row

# generate picture width
@register.filter(name='pic_width')
def pic_width(span):
  return span * 100

# generate picture height
@register.filter(name='pic_heigth')
def pic_heigth(metrics):
  return 350

# format big number
@register.filter(name='format_bigint')
def format_bigint(value):
  try:
    value = int(value)
  except (TypeError, ValueError):
    return value

  if value < 1024*1024:
    return value

  K = 1024
  formaters = (
    (2, '%.2fM'),
    (3, '%.2fG'),
    (4, '%.2fT'),
    (5, '%.2fP'),
  )

  for exponent, formater in formaters:
    larger_num = K ** exponent
    if value < larger_num * K:
      return formater % (value/float(larger_num))

# is space quota healthy
@register.filter(name='is_space_quota_healthy')
def is_space_quota_healthy(total, used):
  return utils.quota_util.is_space_quota_healthy(total, used)

# is name quota healthy
@register.filter(name='is_name_quota_healthy')
def is_name_quota_healthy(total, used):
  return utils.quota_util.is_name_quota_healthy(total, used)

########NEW FILE########
__FILENAME__ = urls
# -*- coding: utf-8 -*-
from django.conf.urls import patterns, url
import views

urlpatterns = patterns(
  '',
  url(r'^$', views.index),

  url(r'^metrics/', views.show_all_metrics),
  url(r'^metrics_config/', views.show_all_metrics_config),

  url(r'^counters/', views.show_all_counters),
  url(r'^addCounter/$', views.add_counter),


  url(r'^service/(?P<id>\d+)/$', views.show_service),
  url(r'^cluster/(?P<id>\d+)/$', views.show_cluster),
  url(r'^cluster/(?P<id>\d+)/task/$', views.show_cluster_task_board),
  url(r'^cluster/(?P<id>\d+)/user/$', views.show_cluster_user_board),
  url(r'^cluster/(?P<id>\d+)/total/$', views.show_quota_total_board),
  url(r'^cluster/(?P<id>\d+)/basic/$', views.show_cluster_basic_board),
  url(r'^cluster/(?P<id>\d+)/table/$', views.show_cluster_table_board),
  url(r'^cluster/(?P<id>\d+)/regionserver/$', views.show_cluster_regionserver_board),
  url(r'^cluster/(?P<id>\d+)/replication/$', views.show_cluster_replication),

  url(r'^job/(?P<id>[^/]+)/$', views.show_job),
  url(r'^task/(?P<id>[^/]+)/$', views.show_task),

  url(r'^table/$', views.show_all_tables),
  url(r'^table/(?P<id>\d+)/$', views.show_table),
  url(r'^table/operation/(?P<id>\d+)/$', views.show_table_operation),
  url(r'^cluster/operation/(?P<id>\d+)/$', views.show_cluster_operation),
  url(r'^cluster/operation/tablecomparsion/(?P<id>\d+)/$', views.show_cluster_operation_table_comparison),
  url(r'^regionserver/(?P<id>\d+)/$', views.show_regionserver),
  url(r'^user/(?P<id>\d+)/$', views.show_user_quota),
  url(r'^regionserver/operation/(?P<id>\d+)/$', views.show_regionserver_operation),
)

########NEW FILE########
__FILENAME__ = views
# -*- coding: utf-8 -*-
from django.views.decorators.http import require_http_methods
from django.views.decorators.csrf import csrf_exempt

from django.shortcuts import render_to_response, redirect
from django.template import RequestContext, Context, loader
from django.utils import timezone
from django.http import HttpResponse
from django.db import transaction
from utils.quota_util import QuotaUpdater

import datetime
import dbutil
import json
import metric_helper
import time
import owl_config

class Namespace:
  def __init__(self, **kwargs):
    for name, value in kwargs.iteritems():
      setattr(self, name, value)


def index(request):
  # show all cluster
  clusters = dbutil.get_clusters_by_service()
  service = Namespace(name="all services")
  params = {
    'service': service,
    'clusters': clusters,
  }
  return respond(request, 'monitor/service.html', params)


#url: /service/$id/
def show_service(request, id):
  service = dbutil.get_service(id)
  clusters = dbutil.get_clusters_by_service(id)
  params = {
    'service': service,
    'clusters': clusters,
  }
  if service.name == 'hbase':

    tsdb_read_query = []
    tsdb_write_query = []
    for cluster in clusters:
      tsdb_read_query.append(metric_helper.make_metric_query(cluster.name, 'Cluster', 'readRequestsCountPerSec'))
      tsdb_write_query.append(metric_helper.make_metric_query(cluster.name, 'Cluster', 'writeRequestsCountPerSec'))

    params.update({
      'tsdb_read_query': tsdb_read_query,
      'tsdb_write_query': tsdb_write_query,
    })

    return respond(request, 'monitor/hbase_service.html', params)
  else:
    return respond(request, 'monitor/service.html', params)

#url: /cluster/$id/
def show_cluster(request, id):
  # return task board by default
  return redirect('/monitor/cluster/%s/task/' % id)

#url: /cluster/$id/task/
def show_cluster_task_board(request, id):
  cluster = dbutil.get_cluster(id)
  tasks = dbutil.get_tasks_by_cluster(id)
  params = {'cluster': cluster,
            'tasks': tasks}
  if cluster.service.name == 'hdfs':
    return respond(request, 'monitor/hdfs_task_board.html', params)
  elif cluster.service.name == 'hbase':
    return respond(request, 'monitor/hbase_task_board.html', params)
  else:
    return respond(request, 'monitor/cluster.html', params)

#url: /cluster/$id/user/
def show_cluster_user_board(request, id):
  cluster = dbutil.get_cluster(id)
  if cluster.service.name == 'hdfs':
    return show_hdfs_user_board(request, cluster);
    # return empty paget for unsupported service
  return HttpResponse('')


def show_hdfs_user_board(request, cluster):
  if 'refresh' in request.GET:
    quota_updater = QuotaUpdater()
    quota_updater.update_cluster(cluster)
    return redirect('/monitor/cluster/%s/user/' % cluster.id)

  dirs = dbutil.get_quota_summary(cluster)
  params = {
    'cluster': cluster,
    'dirs': dirs,
  }
  return respond(request, 'monitor/hdfs_user_board.html', params)

#url: /cluster/$id/table/
def show_cluster_table_board(request, id):
  cluster = dbutil.get_cluster(id)
  if cluster.service.name != 'hbase':
    # return empty paget for unsupported service
    return HttpResponse('')
  read_requests_dist_by_table, write_requests_dist_by_table = dbutil.get_requests_distribution_groupby(cluster, 'table');
  params = {
    'chart_id': 'read_requests_on_table',
    'chart_title': 'read requests on table',
    'request_dist': read_requests_dist_by_table,
    'base_url': '/monitor/table/',
  }

  read_requests_dist_by_table_chart = loader.get_template('monitor/requests_dist_pie_chart.tpl').render(Context(params))

  params = {
    'chart_id': 'write_requests_on_table',
    'chart_title': 'write requests on table',
    'request_dist': write_requests_dist_by_table,
    'base_url': '/monitor/table/',
  }
  write_requests_dist_by_table_chart = loader.get_template('monitor/requests_dist_pie_chart.tpl').render(
    Context(params))

  tables = dbutil.get_items_on_cluster(cluster, 'table', order_by='-qps')
  system_tables = [table for table in tables if is_system_table(table)]
  user_tables = [table for table in tables if not is_system_table(table)]

  table_read_item_keys = '|'.join(['%s-readRequestsCountPerSec' % (table.name) for table in user_tables])
  table_write_item_keys ='|'.join(['%s-writeRequestsCountPerSec' % (table.name) for table in user_tables])

  tsdb_read_query = []
  tsdb_write_query = []
  for table in user_tables:
    tsdb_read_query.append(metric_helper.make_metric_query(cluster.name, table.name, 'readRequestsCountPerSec'))
    tsdb_write_query.append(metric_helper.make_metric_query(cluster.name, table.name, 'writeRequestsCountPerSec'))

  params = {
    'cluster': cluster,
    'read_requests_dist_by_table_chart': read_requests_dist_by_table_chart,
    'write_requests_dist_by_table_chart': write_requests_dist_by_table_chart,
    'system_tables': system_tables,
    'user_tables': user_tables,
    'table_read_item_keys': table_read_item_keys,
    'table_write_item_keys': table_write_item_keys,
    'tsdb_read_query': tsdb_read_query,
    'tsdb_write_query': tsdb_write_query,
  }
  return respond(request, 'monitor/hbase_table_board.html', params)

#url: /cluster/$id/total/
def show_quota_total_board(request, id):
  cluster = dbutil.get_cluster(id)
  if cluster.service.name != 'hdfs':
    return HttpResponse('')

  tsdb_quota_total, tsdb_space_quota_total = dbutil.get_quota_distribution(cluster)
  params = {
    'chart_id': 'used_quota_total',
    'chart_title': 'total name quota on users',
    'request_dist': tsdb_quota_total,
    'base_url': '/monitor/user/',
  }
  tsdb_quota_total_chart = loader.get_template('monitor/requests_dist_pie_chart.tpl').render(Context(params))

  params = {
    'chart_id': 'used_space_quota_total',
    'chart_title': 'total used space on users',
    'request_dist': tsdb_space_quota_total,
    'base_url': '/monitor/user/',
  }
  tsdb_space_quota_total_chart = loader.get_template('monitor/requests_dist_pie_chart.tpl').render(Context(params))

  tsdb_quota_total_query = [metric_helper.make_quota_query(cluster.name, 'used_quota_total', 'used_quota')]
  tsdb_space_quota_total_query = [metric_helper.make_quota_query(cluster.name,
    'used_space_quota_total', 'used_space_quota')]

  params = {
    'cluster': cluster,
    'tsdb_quota_total_chart': tsdb_quota_total_chart,
    'tsdb_space_quota_total_chart': tsdb_space_quota_total_chart,
    'tsdb_quota_total_query': tsdb_quota_total_query,
    'tsdb_space_quota_total_query': tsdb_space_quota_total_query,
  }
  return respond(request, 'monitor/quota_total_board.html', params)

def is_system_table(table):
  system_table_names = ('-ROOT-', '.META.', '_acl_')
  return table.name in system_table_names

#url: /cluster/$id/basic/
def show_cluster_basic_board(request, id):
  cluster = dbutil.get_cluster(id)
  if cluster.service.name != 'hbase':
    # return empty paget for unsupported service
    return HttpResponse('')

  basic_info = dbutil.get_hbase_basic_info(cluster)

  group = 'Cluster'
  tsdb_read_query = [metric_helper.make_metric_query(cluster.name, group, 'readRequestsCountPerSec')]
  tsdb_write_query = [metric_helper.make_metric_query(cluster.name, group, 'writeRequestsCountPerSec')]

  params = {
    'cluster': cluster,
    'basic_info': basic_info,
    'tsdb_read_query': tsdb_read_query,
    'tsdb_write_query': tsdb_write_query,
  }
  return respond(request, 'monitor/hbase_basic_board.html', params)

#url: /cluster/$id/regionserver/
def show_cluster_regionserver_board(request, id):
  cluster = dbutil.get_cluster(id)
  if cluster.service.name != 'hbase':
    # return empty paget for unsupported service
    return HttpResponse('')

  read_requests_dist_by_rs, write_requests_dist_by_rs = dbutil.get_requests_distribution_groupby(cluster, 'regionserver');
  params = {
    'chart_id': 'read_requests_on_rs',
    'chart_title': 'read requests on region server',
    'request_dist': read_requests_dist_by_rs,
    'base_url': '/monitor/regionserver/',
  }

  read_requests_dist_by_rs_chart = loader.get_template('monitor/requests_dist_pie_chart.tpl').render(Context(params))

  params = {
    'chart_id': 'write_requests_on_rs',
    'chart_title': 'write requests on region server',
    'request_dist': write_requests_dist_by_rs,
    'base_url': '/monitor/regionserver/',
  }
  write_requests_dist_by_rs_chart = loader.get_template('monitor/requests_dist_pie_chart.tpl').render(Context(params))

  regionservers = dbutil.get_items_on_cluster(cluster, 'regionserver', order_by='name')
  params = {
    'cluster': cluster,
    'read_requests_dist_by_rs_chart': read_requests_dist_by_rs_chart,
    'write_requests_dist_by_rs_chart': write_requests_dist_by_rs_chart,
    'regionservers': regionservers,
  }
  return respond(request, 'monitor/hbase_regionserver_board.html', params)

#url: /cluster/$id/replication/
def show_cluster_replication(request, id):
  cluster = dbutil.get_cluster(id)
  region_servers = dbutil.get_regionservers_with_active_replication_metrics_by_cluster(cluster) 
  (peer_id_endpoint_map, peer_id_cluster_map) = metric_helper.get_peer_id_endpoint_map_and_cluster(region_servers)
  params = {
    'cluster' : cluster,
    'replication_metrics' : metric_helper.make_metrics_query_for_replication(peer_id_endpoint_map, peer_id_cluster_map),
  }
  return respond(request, 'monitor/hbase_replication.html', params)

def is_test_table(table):
  if 'tst' in table.cluster.name:
    return True
  if 'test' in table.cluster.name:
      return True

  if 'longhaul' in table.name:
   return True
  if 'test' in table.name:
     return True

  return False

#url: /table
def show_all_tables(request):
  tables = dbutil.get_all_tables()
  tables = [table for table in tables if not is_system_table(table)]
  tables = [table for table in tables if not is_test_table(table)]
  params = {
    'tables': tables,
  }
  return respond(request, 'monitor/hbase_tables.html', params)

#url: /table/$table_id/
def show_table(request, id):
  table = dbutil.get_table(id)
  cluster = table.cluster

  read_requests_dist_by_rs, write_requests_dist_by_rs = dbutil.get_requests_distribution(table)
  params = {
    'chart_id': 'read_requests_on_rs',
    'chart_title': 'read requests on region',
    'request_dist': read_requests_dist_by_rs,
  }

  read_requests_dist_by_rs_chart = loader.get_template('monitor/requests_dist_column_chart.tpl').render(Context(params))

  params = {
    'chart_id': 'write_requests_on_rs',
    'chart_title': 'write requests on region',
    'request_dist': write_requests_dist_by_rs,
  }
  write_requests_dist_by_rs_chart = loader.get_template('monitor/requests_dist_column_chart.tpl').render(
    Context(params))

  group = str(table)
  tsdb_read_query = [metric_helper.make_metric_query(cluster.name, group, 'readRequestsCountPerSec')]
  tsdb_write_query = [metric_helper.make_metric_query(cluster.name, group, 'writeRequestsCountPerSec')]

  params = {
    'cluster': cluster,
    'table': table,
    'read_requests_dist_by_rs_chart': read_requests_dist_by_rs_chart,
    'write_requests_dist_by_rs_chart': write_requests_dist_by_rs_chart,
    'tsdb_read_query': tsdb_read_query,
    'tsdb_write_query': tsdb_write_query,
  }

  return respond(request, 'monitor/hbase_table.html', params)

#url: /table/operation/$table_id
def show_table_operation(request, id):
  table = dbutil.get_table(id)
  cluster = table.cluster
  endpoint = dbutil.map_cluster_to_endpoint(cluster.name)
  group = str(table)
  params = {
    'cluster' : cluster,
    'table' : table,
    'tsdb_metrics' : metric_helper.make_operation_metrics(endpoint, table, group),
    'endpoint' : endpoint
  }
  return respond(request, 'monitor/hbase_table_operation.html', params)

#url: /regionserver/operation/$rs_id
def show_regionserver_operation(request, id):
  regionserver = dbutil.get_regionserver(id)
  cluster = regionserver.cluster
  endpoint = dbutil.map_cluster_to_endpoint(cluster.name)
  params = {
    'cluster' : cluster,
    'regionserver' : regionserver,
    'tsdb_metrics' : metric_helper.generate_operation_metric_for_regionserver(regionserver),
    'endpoint' : endpoint
  }
  return respond(request, 'monitor/hbase_regionserver_operation.html', params)

#url: /cluster/operation/$cluster_id
def show_cluster_operation(request, id):
  cluster = dbutil.get_cluster(id)
  endpoint = dbutil.map_cluster_to_endpoint(cluster.name)
  group = 'Cluster'
  params = {
    'cluster' : cluster,
    'tsdb_metrics' : metric_helper.make_operation_metrics(endpoint, cluster.hbasecluster, group),
    'endpoint' : endpoint
  }

  return respond(request, 'monitor/hbase_cluster_operation.html', params)

#url: /cluster/operation/tablecomparsion
def show_cluster_operation_table_comparison(request, id):
  cluster = dbutil.get_cluster(id)
  endpoint = dbutil.map_cluster_to_endpoint(cluster.name)
  params = {
    'cluster' : cluster,
    'tsdb_metrics' : metric_helper.make_operation_metrics_for_tables_in_cluster(cluster),
    'endpoint' : endpoint
  }
  print params['tsdb_metrics']
  return respond(request, 'monitor/hbase_cluster_operation_table_comparsion.html', params)

#url: /regionserver/$rs_id/
def show_regionserver(request, id):
  rs = dbutil.get_regionserver(id)
  cluster = rs.cluster

  read_requests_dist_by_rs, write_requests_dist_by_rs = dbutil.get_requests_distribution(rs);
  params = {
    'chart_id': 'read_requests_on_rs',
    'chart_title': 'read requests on region',
    'request_dist': read_requests_dist_by_rs,
  }

  read_requests_dist_by_rs_chart = loader.get_template('monitor/requests_dist_column_chart.tpl').render(Context(params))

  params = {
    'chart_id': 'write_requests_on_rs',
    'chart_title': 'write requests on region',
    'request_dist': write_requests_dist_by_rs,
  }
  write_requests_dist_by_rs_chart = loader.get_template('monitor/requests_dist_column_chart.tpl').render(
    Context(params))

  group = str(rs)
  tsdb_read_query = [metric_helper.make_metric_query(cluster.name, group, 'readRequestsCountPerSec')]
  tsdb_write_query = [metric_helper.make_metric_query(cluster.name, group, 'writeRequestsCountPerSec')]

  params = {
    'cluster': cluster,
    'regionserver': rs,
    'read_requests_dist_by_rs_chart': read_requests_dist_by_rs_chart,
    'write_requests_dist_by_rs_chart': write_requests_dist_by_rs_chart,
    'tsdb_read_query': tsdb_read_query,
    'tsdb_write_query': tsdb_write_query,
  }
  return respond(request, 'monitor/hbase_regionserver.html', params)

#url: /user/$user_id
def show_user_quota(request, id):
  quota = dbutil.get_quota(id)
  cluster = quota.cluster

  used_quota_query = [metric_helper.make_quota_query(cluster.name, quota.name, 'used_quota')]
  used_space_quota_query = [metric_helper.make_quota_query(cluster.name, quota.name, 'used_space_quota')]

  params = {
    'cluster': cluster,
    'used_quota_query': used_quota_query,
    'used_space_quota_query': used_space_quota_query,
  }
  return respond(request, 'monitor/quota_user.html', params)

#url: /job/$id/
def show_job(request, id):
  tasks = dbutil.get_healthy_tasks_by_job(id)
  job = dbutil.get_job(id)

  endpoints = [metric_helper.form_perf_counter_endpoint_name(task) for task in tasks]
  tsdb_metrics = metric_helper.make_metrics_query_for_job(endpoints, job, tasks)
  print tsdb_metrics
  params = {
    'job': job,
    'tasks': tasks,
    'tsdb_metrics': tsdb_metrics,
  }

  return respond(request, 'monitor/job.html', params)

#url: /task/$id/
def show_task(request, id):
  task = dbutil.get_task(id)
  job = task.job
  tasks = dbutil.get_tasks_by_job(job)

  tsdb_metrics = metric_helper.make_metrics_query_for_task(
    metric_helper.form_perf_counter_endpoint_name(task),
    task)

  params = {
    'job': job,
    'task': task,
    'tasks': tasks,
    'tsdb_metrics': tsdb_metrics,
  }
  return respond(request, 'monitor/task.html', params)


def show_all_metrics(request):
  result = {}
  metrics = dbutil.get_all_metrics()
  if not metrics:
    return HttpResponse('', content_type='application/json; charset=utf8')

  result['timestamp'] = int(time.time())
  result['data'] = metrics
  # defaultly not format output
  indent = None
  if 'indent' in request.GET:
    # when indent is set, format json output with indent = 1
    indent = 1
  return HttpResponse(json.dumps(result, indent=indent),
                      content_type='application/json; charset=utf8')

def show_all_metrics_config(request):
  metrics_config = metric_helper.get_all_metrics_config()

  # defaultly not format output
  indent = None
  if 'indent' in request.GET:
    # when indent is set, format json output with indent = 1
    indent = 1
  return HttpResponse(json.dumps(metrics_config, indent=indent),
                      content_type='application/json; charset=utf8')

def get_time_range(request):
  start_time = datetime.datetime.today() + datetime.timedelta(hours=-1)
  end_time = datetime.datetime.today()
  if 'start_time' in request.COOKIES:
    start_time = datetime.datetime.strptime(request.COOKIES['start_time'], '%Y-%m-%d-%H-%M')

  if 'start_time' in request.GET:
    start_time = datetime.datetime.strptime(request.GET['start_time'], '%Y-%m-%d-%H-%M')

  if 'end_time' in request.COOKIES:
    end_time = datetime.datetime.strptime(request.COOKIES['end_time'], '%Y-%m-%d-%H-%M')

  if 'end_time' in request.GET:
    end_time = datetime.datetime.strptime(request.GET['end_time'], '%Y-%m-%d-%H-%M')
  return start_time, end_time


@transaction.commit_on_success
@csrf_exempt
@require_http_methods(["POST"])
def add_counter(request):
  counters = json.loads(request.body)
  remote_ip = request.META['REMOTE_ADDR']
  update_time = datetime.datetime.utcfromtimestamp(time.time()).replace(tzinfo=timezone.utc)
  for dict in counters:
    group = dict['group']
    endpoint = remote_ip
    if 'endpoint' in dict:
      endpoint = dict['endpoint']
    label = ''
    if 'label' in dict:
      label = dict['label']
    name = dict['name']
    counter, create = dbutil.get_or_create_counter(group, name)

    counter.host = endpoint
    counter.value = (float)(dict['value'])
    counter.unit = dict['unit']
    counter.last_update_time = update_time
    counter.label = label
    counter.save()
  return HttpResponse("ok")


def show_all_counters(request):
  result = {}
  metrics = dbutil.get_all_counters()
  if not metrics:
    return HttpResponse('', content_type='application/json; charset=utf8')

  result['timestamp'] = time.time()
  result['data'] = metrics
  # defaultly not format output
  indent = None
  if 'indent' in request.GET:
  # when indent is set, format json output with indent = 1
    indent = 1
  return HttpResponse(json.dumps(result, indent=indent),
                      content_type='application/json; charset=utf8')


def respond(request, template, params=None):
  """Helper to render a response, passing standard stuff to the response.
  Args:
  request: The request object.
  template: The template name; '.html' is appended automatically.
  params: A dict giving the template parameters; modified in-place.
  Returns:
  Whatever render_to_response(template, params) returns.
  Raises:
  Whatever render_to_response(template, params) raises.
  """
  params['request'] = request
  params['user'] = request.user
  params['chart_url_prefix'] = owl_config.CHART_URL_PREFIX
  params['tsdb_url_prefix'] = owl_config.TSDB_ADDR
  params['supervisor_port'] = owl_config.SUPERVISOR_PORT
  params['start_date'] = (datetime.datetime.now() - datetime.timedelta(minutes=15)).strftime('%Y/%m/%d-%H:%M:%S')
  params['quota_start_date'] = (datetime.datetime.now() - datetime.timedelta(hours=20)).strftime('%Y/%m/%d-%H:%M:%S')
  params.update(request.GET)
  response = render_to_response(template, params,
                                context_instance=RequestContext(request))
  return response

########NEW FILE########
__FILENAME__ = settings
# Django settings for owl project.

import os, django
# calculated paths for django and the site
# used as starting points for various other paths
DJANGO_ROOT = os.path.dirname(os.path.realpath(django.__file__))
SITE_ROOT = os.path.dirname(os.path.realpath(__file__))

DEBUG = False
TEMPLATE_DEBUG = DEBUG
ALLOWED_HOSTS = ['*']

ADMINS = (
  ('admin', ''),
)

MANAGERS = ADMINS

DATABASES = {
  'default': {
    'ENGINE': 'django.db.backends.mysql', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.
    'NAME': 'owl', # Or path to database file if using sqlite3.
    'USER': 'owl', # Not used with sqlite3.
    'PASSWORD': 'owl', # Not used with sqlite3.
    'HOST': 'localhost', # Set to empty string for localhost. Not used with sqlite3.
    'PORT': '3306', # Set to empty string for default. Not used with sqlite3.
  }
}

# Local time zone for this installation. Choices can be found here:
# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name
# although not all choices may be available on all operating systems.
# In a Windows environment this must be set to your system time zone.
TIME_ZONE = 'Asia/Shanghai'

# Language code for this installation. All choices can be found here:
# http://www.i18nguy.com/unicode/language-identifiers.html
LANGUAGE_CODE = 'en-us'

SITE_ID = 1

# If you set this to False, Django will make some optimizations so as not
# to load the internationalization machinery.
USE_I18N = True

# If you set this to False, Django will not format dates, numbers and
# calendars according to the current locale.
USE_L10N = True

# If you set this to False, Django will not use timezone-aware datetimes.
USE_TZ = True

# Absolute filesystem path to the directory that will hold user-uploaded files.
# Example: "/home/media/media.lawrence.com/media/"
MEDIA_ROOT = ''

# URL that handles the media served from MEDIA_ROOT. Make sure to use a
# trailing slash.
# Examples: "http://media.lawrence.com/media/", "http://example.com/media/"
MEDIA_URL = ''

# Absolute path to the directory static files should be collected to.
# Don't put anything in this directory yourself; store your static files
# in apps' "static/" subdirectories and in STATICFILES_DIRS.
# Example: "/home/media/media.lawrence.com/static/"
STATIC_ROOT = ''

# URL prefix for static files.
# Example: "http://media.lawrence.com/static/"
STATIC_URL = '/static/'

# Additional locations of static files
STATICFILES_DIRS = (
  os.path.join(SITE_ROOT, '../static'),
  # Put strings here, like "/home/html/static" or "C:/www/django/static".
  # Always use forward slashes, even on Windows.
  # Don't forget to use absolute paths, not relative paths.
  )

# List of finder classes that know how to find static files in
# various locations.
STATICFILES_FINDERS = (
  'django.contrib.staticfiles.finders.FileSystemFinder',
  'django.contrib.staticfiles.finders.AppDirectoriesFinder',
  #'django.contrib.staticfiles.finders.DefaultStorageFinder',
  )

# Make this unique, and don't share it with anybody.
SECRET_KEY = '#q8xc4y#r^u+7^lymd3^lg6%vips0hc-8&amp;#4^dncjr6p&amp;2rh@0'

# List of callables that know how to import templates from various sources.
TEMPLATE_LOADERS = (
  'django.template.loaders.filesystem.Loader',
  'django.template.loaders.app_directories.Loader',
  #     'django.template.loaders.eggs.Loader',
  )

MIDDLEWARE_CLASSES = (
  'django.middleware.common.CommonMiddleware',
  'django.contrib.sessions.middleware.SessionMiddleware',
  'django.middleware.csrf.CsrfViewMiddleware',
  'django.contrib.auth.middleware.AuthenticationMiddleware',
  'django.contrib.messages.middleware.MessageMiddleware',
  'django.middleware.transaction.TransactionMiddleware',
  # Uncomment the next line for simple clickjacking protection:
  # 'django.middleware.clickjacking.XFrameOptionsMiddleware',
  )

ROOT_URLCONF = 'owl.urls'

# Python dotted path to the WSGI application used by Django's runserver.
WSGI_APPLICATION = 'owl.wsgi.application'

TEMPLATE_DIRS = (os.path.join(SITE_ROOT, '../templates'))

INSTALLED_APPS = (
  'django.contrib.auth',
  'django.contrib.contenttypes',
  'django.contrib.sessions',
  'django.contrib.sites',
  'django.contrib.messages',
  'django.contrib.staticfiles',
  'django.contrib.humanize',
  # Uncomment the next line to enable the admin:
  'django.contrib.admin',
  # Uncomment the next line to enable admin documentation:
  # 'django.contrib.admindocs',
  'alert',
  'business',
  'collector',
  'hbase',
  'monitor',
  'quota',
  'zktree',
  'failover_framework',
  )

# A sample logging configuration. The only tangible logging
# performed by this configuration is to send an email to
# the site admins on every HTTP 500 error when DEBUG=False.
# See http://docs.djangoproject.com/en/dev/topics/logging for
# more details on how to customize your logging configuration.
LOGGING = {
  'version': 1,
  'disable_existing_loggers': False,
  'formatters': {
    'verbose': {
      'format': '%(levelname)s %(asctime)s %(module)s %(process)d %(thread)d %(message)s'
    },
    'simple': {
      'format': '%(levelname)s %(message)s'
    }
  },
  'filters': {
    'require_debug_false': {
      '()': 'django.utils.log.RequireDebugFalse'
    }
  },
  'handlers': {
    'null': {
      'level': 'DEBUG',
      'class': 'django.utils.log.NullHandler',
    },
    'console':{
      'level': 'DEBUG',
      'class': 'logging.StreamHandler',
      'formatter': 'verbose'
    },
    'mail_admins': {
      'level': 'ERROR',
      'filters': ['require_debug_false'],
      'class': 'django.utils.log.AdminEmailHandler'
    },
    'file':{
      'level': 'DEBUG',
      'class': 'logging.FileHandler',
      'filename': 'debug.log',
      'formatter': 'verbose'
    }
  },
  'loggers': {
    'django.request': {
      'handlers': ['mail_admins', 'file'],
      'level': 'ERROR',
      'propagate': True,
    },
    'alert': {
      'handlers': ['console'],
      'level': 'INFO',
      'propagate': True,
    },
    'collector': {
      'handlers': ['console'],
      'level': 'INFO',
      'propagate': True,
    },
    'quota': {
      'handlers': ['console'],
      'level': 'INFO',
      'propagate': True,
    },
    'monitor': {
      'handlers': ['console', 'file'],
      'level': 'INFO',
      'propagate': True,
    },
    'failover_framework': {
      'handlers': ['console'],
      'level': 'INFO',
      'propagate': True,
    }
  }
}

# site config
LOGIN_REDIRECT_URL = '/monitor/'

# for failover framework app
FAILOVER_FRAMEWORK_HOST = "127.0.0.1"
FAILOVER_FRAMEWORK_PORT = 9981
FAILOVER_FRAMEWORK_PERIOD = 600

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls import patterns, include, url
from django.views.generic import RedirectView

# Uncomment the next two lines to enable the admin:
from django.contrib import admin

admin.autodiscover()

urlpatterns = patterns(
  '',
  # Examples:
  url(r'^$', RedirectView.as_view(url='/monitor/')),
  # url(r'^owl/', include('owl.foo.urls')),

  # Uncomment the admin/doc line below to enable admin documentation:
  url(r'^admin/doc/', include('django.contrib.admindocs.urls')),

  # Uncomment the next line to enable the admin:
  url(r'^admin/', include(admin.site.urls)),

  url(r'^accounts/', include('django.contrib.auth.urls')),

  url(r'^monitor/', include('monitor.urls')),
  url(r'^hbase/', include('hbase.urls')),
  url(r'^business/', include('business.urls')),
  url(r'^zktree/', include('zktree.urls')),
  url(r'^failover/', include('failover_framework.urls')),
)

########NEW FILE########
__FILENAME__ = wsgi
"""
WSGI config for owl project.

This module contains the WSGI application used by Django's development server
and any production WSGI deployments. It should expose a module-level variable
named ``application``. Django's ``runserver`` and ``runfcgi`` commands discover
this application via the ``WSGI_APPLICATION`` setting.

Usually you will have the standard Django WSGI application here, but it also
might make sense to replace the whole Django WSGI application with a custom one
that later delegates to the Django one. For example, you could introduce WSGI
middleware here, or combine a Django application with an application of another
framework.

"""
import os

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "owl.settings")

# This application object is used by any WSGI server configured to use this
# file. This includes Django's development server, if the WSGI_APPLICATION
# setting points here.
from django.core.wsgi import get_wsgi_application

application = get_wsgi_application()

# Apply WSGI middleware here.
# from helloworld.wsgi import HelloWorldApplication
# application = HelloWorldApplication(application)

########NEW FILE########
__FILENAME__ = quota_reportor
import collections
import datetime
import logging
import smtplib
import sys
import time

from optparse import make_option
from os import path

from django.conf import settings
from django.core.management.base import BaseCommand, CommandError
from django.utils import timezone

import owl_config
import monitor.dbutil
import utils.mail
import utils.quota_util
import deploy_utils
from monitor.models import Cluster, Quota, Service

logger = logging.getLogger('quota')

# cluster to generate report
QUOTA_REPORT_CLUSTER = owl_config.QUOTA_REPORT_CLUSTER
# user that receive cluster report
QUOTA_REPORT_ADMINS = owl_config.QUOTA_REPORT_ADMINS
# user that receive cluster quota alert
QUOTA_ALERT_ADMINS = owl_config.QUOTA_ALERT_ADMINS

KERBEROS_IDS_PATH = owl_config.KERBEROS_IDS_PATH

admin_email = ''
try:
  admin_email = settings.ADMINS[0][1]
except:
  pass

class QuotaReportor:
  """Update path quota in hdfs"""
  def __init__(self, options):
    self.options = options
    self.mailer = utils.mail.Mailer(options)
    self.user_report = {} # report group by user
    self.cluster_report = {} # report group by cluster
    self.today = datetime.date.today()
    self.kerb_user_map = self.init_kerb_user_map()

  def report(self):
    logger.info('start make quota report')
    self.start_time = time.time()
    try:
      for cluster_name in QUOTA_REPORT_CLUSTER:
        self.update_cluster(cluster_name)
    except Exception as e:
      logger.info('gather quota info failed: %r', e)
      self.mailer.send_email(subject = 'Make quota report failed',
                             content = repr(e),
                             to_email = admin_email,
                            )
    else:
      self.send_report_mail()

    logger.info('spent %f seconds for make quota report',
        time.time() - self.start_time)

  def update_cluster(self, cluster_name):
    hdfs_service = Service.objects.get(name='hdfs')
    cluster = Cluster.objects.get(service=hdfs_service, name = cluster_name)
    quota_list = monitor.dbutil.get_quota_summary(cluster)
    for quota_record in quota_list:
      user_report = self.user_report.setdefault(quota_record.name, {})
      user_report[cluster_name] = quota_record
      cluster_report = self.cluster_report.setdefault(cluster_name, {})
      cluster_report[quota_record.name] = quota_record

  def send_report_mail(self):
    self.send_user_report_mail()
    self.send_cluster_report_mail()
    self.alert_to_not_healthy_users()

  def send_user_report_mail(self):
    for user, cluster_quota in self.user_report.iteritems():
      subject = 'Hadoop hdfs quota report for user %s' % user
      content = 'Report date: %s<br>' % self.today
      content += self.format_quota_report_content('cluster', cluster_quota)
      email_user = self.map_kerb_user_to_email_user(user)
      if email_user:
        email_addr = ','.join([addr for addr in email_user.split()])

        self.mailer.send_email(to_email = email_addr,
                               subject = subject,
                               content = content,
                               type = 'html')
      else:
        logger.error('User %s has no email user' % user)

  def send_cluster_report_mail(self):
    subject = 'Hadoop hdfs quota report for admin'
    content = 'Report date: %s<br>' % self.today
    for cluster, user_quota in self.cluster_report.iteritems():
      content += 'Quota summary on cluster[%s]<br>' % cluster
      content += self.format_quota_report_content('user', user_quota)
      content += '********<br>'
    self.mailer.send_email(to_email = QUOTA_REPORT_ADMINS,
                           subject = subject,
                           content = content,
                           type = 'html')

  def alert_to_not_healthy_users(self):
    subject = 'Hadoop hdfs quota alert'
    for user, cluster_quota in self.user_report.iteritems():
      for cluster, quota in cluster_quota.iteritems():
        need_alert = False
        content = 'Cluster: %s\n' % cluster
        content += 'User: %s\n' % user

        if not utils.quota_util.is_space_quota_healthy(
          quota.space_quota, quota.used_space_quota):
          content += 'Alert: space quota exceeded the threshold. \
              Please cleanup trash or apply for more space quota.\n'
          need_alert = True

        if not utils.quota_util.is_name_quota_healthy(
          quota.quota, quota.used_quota):
          content += 'Alert: name quota exceeded the threshold. \
              Please cleanup trash or apply for more name quota.\n'
          need_alert = True

        if need_alert:
          email_addrs = QUOTA_ALERT_ADMINS
          email_user = self.map_kerb_user_to_email_user(user)
          if email_user:
            email_addrs += ','.join([addr for addr in email_user.split()])
          self.mailer.send_email(to_email = email_addrs,
                                 subject = subject,
                                 content = content)

  @staticmethod
  def format_quota_report_content(key_name, quota_map):
    content = '<table>'
    HEADER_FORMAT_STR = '<tr><th>{}</th><th>{}</th><th>{}</th><th>{}</th><th>{}</th><th>{}</th><th>{}</th></tr>'
    content += HEADER_FORMAT_STR.format(key_name, 'SpaceQuota', 'UsedSpace', 'RemainingSpace', 'NameQuota', 'UsedName', 'RemainingName')

    ROW_FORMAT_STR = '<tr><td>{}</td><td>{}</td><td>{}</td><td %s>{}</td><td>{}</td><td>{}</td><td %s>{}</td></tr>'

    ordered_dict = collections.OrderedDict(sorted(quota_map.items()))
    for key, quota in ordered_dict.iteritems():
      space_quota_color = '' if utils.quota_util.is_space_quota_healthy(
        quota.space_quota, quota.used_space_quota) \
        else 'style="color:rgb(255,0,0)"'
      name_quota_color = '' if utils.quota_util.is_name_quota_healthy(
        quota.quota, quota.used_quota) \
        else 'style="color:rgb(255,0,0)"'
      format_str = ROW_FORMAT_STR % (space_quota_color, name_quota_color)
      content += format_str.format(key,
                                   format_bigint(quota.space_quota),
                                   format_bigint(quota.used_space_quota),
                                   format_bigint(quota.remaining_space_quota),
                                   quota.quota, quota.used_quota, quota.remaining_quota)
    content += '</table>'
    return content



  def init_kerb_user_map(self):
    res = {}
    config_path = deploy_utils.get_config_dir()
    with open(path.join(config_path, KERBEROS_IDS_PATH)) as f:
      for line in f:
        if line.startswith('#'):
          continue
        try:
          # file format: kerb_user user1[ user2 user3]
          kerb_user, email_users = line.strip().split(' ', 1)
          if kerb_user in res:
            logger.warn('Duplicated kerb user config for user: %s' % kerb_user)
          res[kerb_user] = email_users
        except Exception as e:
          logger.warn('Failed to parse user config [%r]: %s' % (e, line))
    return res


  def map_kerb_user_to_email_user(self, kerb_user):
    if kerb_user in self.kerb_user_map:
      return self.kerb_user_map[kerb_user]
    else:
      return None

class Command(BaseCommand):
  args = ''
  help = "Run the background updater to collector quota on hdfs clusters."

  def handle(self, *args, **options):
    self.args = args
    self.options = options
    self.mailer = utils.mail.Mailer(options)

    self.stdout.write("args: %r\n" % (args, ))
    self.stdout.write("options: %r\n" % options)

    quota_reportor = QuotaReportor(options)

    try:
      quota_reportor.report()
    except Exception as e:
      logger.warning('Quota repotor aborted: %r', e)
      self.mailer.send_email(subject = 'Make quota report failed',
                             content = repr(e),
                             to_email = admin_email,
                            )

def format_bigint(value):
  try:
    value = int(value)
  except (TypeError, ValueError):
    return value

  if value < 1024*1024:
    return value

  K = 1024
  formaters = (
    (2, '%.2fM'),
    (3, '%.2fG'),
    (4, '%.2fT'),
    (5, '%.2fP'),
  )

  for exponent, formater in formaters:
    larger_num = K ** exponent
    if value < larger_num * K:
      return formater % (value/float(larger_num))

########NEW FILE########
__FILENAME__ = quota_updater
import logging
import time
import utils.mail

from django.conf import settings
from django.core.management.base import BaseCommand, CommandError
from optparse import make_option
from os import path
from utils.quota_util import QuotaUpdater

from quota_reportor import QUOTA_REPORT_ADMINS

logger = logging.getLogger('quota')

class Command(BaseCommand):
  args = ''
  help = "Run the background updater to collector quota on hdfs clusters."

  option_list = BaseCommand.option_list + (
      make_option(
        "--period",
        default=3600, # check per hour
        help="Check period"),
  )

  def handle(self, *args, **options):
    self.args = args
    self.options = options
    self.mailer = utils.mail.Mailer(options)

    self.stdout.write("args: %r\n" % (args, ))
    self.stdout.write("options: %r\n" % options)

    quota_updater = QuotaUpdater()

    while True:
      try:
        quota_updater.update_all_cluster()
      except Exception as e:
        # send alert email when program error
        logger.warning('Quota updater error: %r', e)
        admin_email = ''
        try:
          admin_email = QUOTA_REPORT_ADMINS
        except:
          pass
        self.mailer.send_email(subject = 'Quota updater error',
                               content = repr(e),
                               to_email = admin_email,
                              )
      time.sleep(int(self.options['period']))

########NEW FILE########
__FILENAME__ = hadoop_util
# -*- coding: utf-8 -*-
import logging
import subprocess
import os

CLIENT_DEPLOY_ENTRY = os.getenv("CLIENT_DEPLOY_ENTRY")
ENV_PYTHON = os.getenv("ENV_PYTHON")

logger = logging.getLogger('quota')
def get_quota_summary(cluster_name):
  res = []
  try:
    cmd_user = [ENV_PYTHON, CLIENT_DEPLOY_ENTRY, 'shell', 'hdfs', cluster_name, 'dfs', '-quota', '/user/*']
    cmd_hbase = [ENV_PYTHON, CLIENT_DEPLOY_ENTRY, 'shell', 'hdfs', cluster_name, 'dfs', '-quota', '/hbase']
    #cmd_user = ["python2.7", "%s/deploy.py" % client_root, 'shell', 'hdfs', cluster_name, 'dfs', '-quota', '/user/*']
    #cmd_hbase = ["python2.7", "%s/deploy.py" % client_root, 'shell', 'hdfs', cluster_name, 'dfs', '-quota', '/hbase']
    for cmd in [cmd_user, cmd_hbase]:
      content = subprocess.check_output(cmd)

      for line in content.strip().split('\n'):
        dir_info = {}
        (dir_info['quota'], dir_info['used_quota'],
         dir_info['remaining_quota'], dir_info['space_quota'],
         dir_info['used_space_quota'], dir_info['remaining_space_quota'],
         dir_info['name']) = line.split()
        # discard prefix '/user/', only keep with user name
        if len(dir_info['name']) > 7:
          dir_info['name'] = dir_info['name'][6:]
        else:
          dir_info['name'] = 'hbase'
        res.append(dir_info)
  except Exception, e:
    if repr(e).find("No such file") == -1:
      return ""
    raise e
  return res


########NEW FILE########
__FILENAME__ = mail
import owl_config
import smtplib
from email.mime.text import MIMEText

class Mailer:
  def __init__(self, options):
    self.options = options
    self.from_email = owl_config.ALERT_FROM_EMAIL
    self.smtp_host = owl_config.SMTPHOST
    self.password = owl_config.ROBOT_EMAIL_PASSWORD

  def send_email(self, content, subject, to_email, type='plain'):
    send_email(content = content,
               subject = subject,
               from_email = self.from_email,
               to_email = to_email,
               smtp_host = self.smtp_host,
               password = self.password,
               type = type,
              )

def send_email(subject, content, from_email, to_email, smtp_host, password, type):
  if not to_email:
    return

  msg = MIMEText(content, type)
  msg['Subject'] = subject
  msg['From'] = from_email
  to_emails = [addr.strip() for addr in to_email.split(',')]
  msg['To'] = ','.join(to_emails)

  connected = False
  try:
    smtp = smtplib.SMTP(smtp_host)
    if password:
      smtp.login(from_email.split('@')[0], password)
      connected = True

    smtp.sendmail(msg['From'], to_emails, msg.as_string())
  except Exception as e:
    print 'Send email failed: %r' % e
    if connected:
      smtp.quit()

########NEW FILE########
__FILENAME__ = quota_injector
import logging
import os
import sys
import time

from urlparse import urlparse

root_path = os.path.abspath(
  os.path.dirname(os.path.realpath(__file__))+ '/../..')
opentsdb_path = os.path.join(root_path, 'opentsdb')
sys.path.append(opentsdb_path)
tsdb_register = __import__('tsdb_register')
from tsdb_register import conf_path
from tsdb_register import TsdbRegister

owl_config_path = os.path.join(conf_path, 'owl')
sys.path.append(owl_config_path)
owl_config = __import__('owl_config')
tsdb_host, tsdb_port = urlparse(owl_config.TSDB_ADDR).netloc.split(':')

logger_quota = logging.getLogger('quota')

# the quota items need to calculate the total value
QUOTA_TOTAL_DICT = {
  'used_quota': (int, 0),
  'used_space_quota': (int, 0),
}

class QuotaInjector():
  '''
  Push quota information into opentsdb
  '''
  def __init__(self):
    self.tsdb_register = TsdbRegister()

  def check_quota_new_keys(self, quota_list):
    if len(quota_list) > 0:
      for quota_key in quota_list[0].keys():
        if quota_key not in self.tsdb_register.register_keys and quota_key != 'name':
          self.tsdb_register.new_keys.append(quota_key)
          self.tsdb_register.register_keys.add(quota_key)
      self.tsdb_register.register_new_keys_to_tsdb()

  def push_quota_to_tsdb(self, quota_list, cluster_name):
    self.check_quota_new_keys(quota_list)
    timestamp = int(time.time())

    # reset the quota_total_dict
    quota_total_dict = dict.fromkeys(QUOTA_TOTAL_DICT, 0)

    # push every user's quota to tsdb for cluster_name
    for quota_dict in quota_list:
      for quota_key, quota_value in quota_dict.iteritems():
        if quota_key != 'name':
          if not quota_value.isdigit():
            quota_value = '0'
          quota_record = "%s %d %d user_id=%s cluster=%s" % (
            quota_key, timestamp, int(quota_value), quota_dict['name'], cluster_name)
          put_operation = 'echo put %s | nc -w 10 %s %s' % (quota_record, tsdb_host, tsdb_port)
          logger_quota.info(put_operation)
          os.system(put_operation)
        if quota_key in quota_total_dict.keys():
          quota_total_dict[quota_key] += int(quota_value)

    # push the total values to tsdb
    for quota_key, quota_value in quota_total_dict.iteritems():
      quota_record = "%s %d %d user_id=%s cluster=%s" % (
        quota_key, timestamp, quota_value, quota_key+'_total', cluster_name)
      put_operation = 'echo put %s | nc -w 10 %s %s' % (quota_record, tsdb_host, tsdb_port)
      logger_quota.info(put_operation)
      os.system(put_operation)

########NEW FILE########
__FILENAME__ = quota_util
import datetime
import logging
import quota_injector
import time
import utils.hadoop_util

from django.db import transaction
from django.utils import timezone
from monitor.models import Cluster, Quota, Service

logger = logging.getLogger('quota')
quota_injector = quota_injector.QuotaInjector()

class QuotaUpdater:
  """Update path quota in hdfs"""

  def update_all_cluster(self):
    logger.info("start updating clusters quota")
    self.start_time = time.time()
    hdfs_service = Service.objects.get(name='hdfs')
    for cluster in Cluster.objects.filter(active=True, service=hdfs_service).all():
      self.update_cluster(cluster)
    logger.info("spent %f seconds for updating clusters quota",
        time.time() - self.start_time)

  @transaction.commit_on_success
  def update_cluster(self, cluster):
    logger.info("start update cluster %s" % cluster.name),
    cluster_name = cluster.name
    now = time.time()
    quota_list = utils.hadoop_util.get_quota_summary(cluster_name)
    quota_injector.push_quota_to_tsdb(quota_list, cluster_name)
    for quota in quota_list:
      quota_record, ok = Quota.objects.get_or_create(cluster=cluster, name=quota['name'])
      quota_record.quota = quota['quota']
      quota_record.used_quota = quota['used_quota']
      quota_record.remaining_quota = quota['remaining_quota']
      quota_record.space_quota = quota['space_quota']
      quota_record.used_space_quota = quota['used_space_quota']
      quota_record.remaining_space_quota = quota['remaining_space_quota']
      quota_record.last_update_time = datetime.datetime.utcfromtimestamp(
          now).replace(tzinfo=timezone.utc)
      quota_record.save()
    logger.info("end update cluster %s" % cluster.name),

def is_space_quota_healthy(total, used):
  try:
    # remaining < 1G or used ratio > 80% means not healthy
    if (int(total) - int(used)) < 1024*1024*1024 \
      or float(used) / float(total) > 0.8:
      return False
  except Exception, e:
    pass
  return True

def is_name_quota_healthy(total, used):
  try:
    # remaining < 500 or used ratio > 80% means not healthy
    if (int(total) - int(used)) < 500\
      or float(used) / float(total) > 0.8:
      return False
  except Exception, e:
    pass
  return True

########NEW FILE########
__FILENAME__ = models
from datetime import datetime
import threading
import zookeeper

PERM_READ = 1
PERM_WRITE = 2
PERM_CREATE = 4
PERM_DELETE = 8
PERM_ADMIN = 16
PERM_ALL = PERM_READ | PERM_WRITE | PERM_CREATE | PERM_DELETE | PERM_ADMIN

zookeeper.set_log_stream(open("cli_log.txt","w"))

TIMEOUT = 10.0

class ZKClient(object):
    def __init__(self, servers, timeout):
        self.connected = False
        self.conn_cv = threading.Condition( )
        self.handle = -1

        self.conn_cv.acquire()
        self.handle = zookeeper.init(servers, self.connection_watcher, 30000)
        self.conn_cv.wait(timeout)
        self.conn_cv.release()

        if not self.connected:
            raise Exception("Unable to connect to %s" % (servers))

    def connection_watcher(self, h, type, state, path):
        self.handle = h
        self.conn_cv.acquire()
        self.connected = True
        self.conn_cv.notifyAll()
        self.conn_cv.release()

    def close(self):
        zookeeper.close(self.handle)

    def get(self, path, watcher=None):
        return zookeeper.get(self.handle, path, watcher)

    def get_children(self, path, watcher=None):
        return zookeeper.get_children(self.handle, path, watcher)

    def get_acls(self, path):
        return zookeeper.get_acl(self.handle, path)

class ZNode(object):
    def __init__(self, addrs, path="/"):
        self.path = path
        zk = ZKClient(addrs, TIMEOUT)
        try:
            self.data, self.stat = zk.get(path)
            self.stat['ctime'] = datetime.fromtimestamp(self.stat['ctime']/1000)
            self.stat['mtime'] = datetime.fromtimestamp(self.stat['mtime']/1000)
            self.children = zk.get_children(path) or []
            self.acls = zk.get_acls(path)[1] or []
            for acl in self.acls:
                perms = acl['perms']
                perms_list = []
                if perms & PERM_READ:
                    perms_list.append("PERM_READ")
                if perms & PERM_WRITE:
                    perms_list.append("PERM_WRITE")
                if perms & PERM_CREATE:
                    perms_list.append("PERM_CREATE")
                if perms & PERM_DELETE:
                    perms_list.append("PERM_DELETE")
                if perms & PERM_ADMIN:
                    perms_list.append("PERM_ADMIN")
                if perms & PERM_ALL == PERM_ALL:
                    perms_list = ["PERM_ALL"]
                acl['perm_list'] = perms_list
        finally:
            zk.close()

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls import patterns, url
import views

urlpatterns = patterns(
  '',
  url(r'^(?P<addrs>[^/]+)/(?P<path>.*)/$',views.index),
  url(r'^(?P<addrs>.+)/$',views.index),
)

########NEW FILE########
__FILENAME__ = views
from django.shortcuts import render_to_response
from monitor.views import respond
import os
import string

from models import ZNode

def istext(s, text_chars="".join(map(chr, range(32, 127))) + "\n\r\t\b"):
    if "\0" in s: return False
    if not s: return True
    t = s.translate(string.maketrans("", ""), text_chars)
    return len(t) == 0

def index(request, addrs, path=""):
    path = "/" + path
    try:
        parent_path = os.path.dirname(path)
        znode = ZNode(addrs, path)
        znode.children.sort()
        if not istext(znode.data):
            znode.data = "0x" + "".join(["%d" % (ord(d)) for d in znode.data])
            znode.datatype = "bin"
        else:
            znode.datatype = "str"

        params = {'znode':znode,
                  'addrs':addrs,
                  'parent_path':parent_path}
        return respond(request, 'zktree/index.html', params)
    except Exception as err:
        return respond(request, 'zktree/error.html',
                       {'error':str(err)})

########NEW FILE########
__FILENAME__ = rpcinterface
#!/usr/bin/env python
#
# Copyright (c) 2012, Xiaomi.com.
# Author:  Wu Zesheng <wuzesheng@xiaomi.com>

import ConfigParser
import cStringIO
import subprocess
import os
import time
import urllib2

from exceptions import RuntimeError
from supervisor.datatypes import DEFAULT_EXPECTED_EXIT_CODE
from supervisor.http import NOT_DONE_YET
from supervisor.options import ClientOptions
from supervisor.rpcinterface import SupervisorNamespaceRPCInterface
from supervisor.states import STOPPED_STATES
from supervisor.supervisorctl import Controller
from supervisor.xmlrpc import Faults
from supervisor.xmlrpc import RPCError

DEFAULT_PACKAGE_ROOT = '/home/work/packages'
DEFAULT_APP_ROOT     = '/home/work/app'
DEFAULT_LOG_ROOT     = '/home/work/log'
DEFAULT_DATA_DIRS    = '/home/work/data'

CONFIG_PATH = 'conf'
JOB_RUN_CONFIG = 'run.cfg'

SUCCESS_STATUS = 'OK'

class DeploymentRPCInterface:
  def __init__(self, supervisord, **config):
    self.supervisord = supervisord
    self.global_config = config
    self.supervisor_rpcinterface = SupervisorNamespaceRPCInterface(supervisord)
    self.package_server = config.get('package_server')
    self.download_package_uri = config.get('download_package_uri')
    self.get_latest_package_info_uri = config.get('get_latest_package_info_uri')

  def get_run_dir(self, service, cluster, job, instance_id=-1):
    '''
    Get the run directory of the specified job

    @param service     the server name
    @param cluster     the cluster name
    @param job         the job name
    @param instance_id the instance id
    @return string     the job's run root directory
    '''
    app_root = self.global_config.get('app_root', DEFAULT_APP_ROOT)
    if instance_id == -1:
      return '%s/%s/%s/%s' % (app_root, service, cluster, job)
    else:
      return '%s/%s/%s/%s/%s' % (app_root, service, cluster, job, instance_id)

  def get_log_dir(self, service, cluster, job, instance_id=-1):
    '''
    Get the log directory of the specified job

    @param service     the server name
    @param cluster     the cluster name
    @param job         the job name
    @param instance_id the instance id
    @return string     the job's log root directory
    '''
    log_root = self.global_config.get('log_root', DEFAULT_LOG_ROOT)
    if instance_id == -1:
      return '%s/%s/%s/%s' % (log_root, service, cluster, job)
    else:
      return '%s/%s/%s/%s/%s' % (log_root, service, cluster, job, instance_id)

  def get_stdout_dir(self, service, cluster, job, instance_id=-1):
    '''
    Get the stdout directory of the specified job

    @param service     the server name
    @param cluster     the cluster name
    @param job         the job name
    @param instance_id the instance id
    @return string     the job's log root directory
    '''
    run_dir = self.get_run_dir(service, cluster, job, instance_id)
    return '%s/stdout' % run_dir

  def get_available_data_dirs(self, service, cluster, job, instance_id=-1):
    '''
    Get all the available data directories that the specified job may use

    @param service     the server name
    @param cluster     the cluster name
    @param job         the job name
    @param instance_id the instance id
    @return list       all the available data root directories
    '''
    data_dirs = self.global_config.get('data_dirs', DEFAULT_DATA_DIRS)
    if instance_id == -1:
      return ['%s/%s/%s/%s' % (data_dir, service, cluster, job)
        for data_dir in data_dirs.split(',')
      ]
    else:
      return ['%s/%s/%s/%s/%s' % (data_dir, service, cluster, job, instance_id)
        for data_dir in data_dirs.split(',')
      ]

  def get_data_dirs(self, service, cluster, job, instance_id=-1):
    '''
    Get all the data directories of the specified job

    @param service     the server name
    @param cluster     the cluster name
    @param job         the job name
    @param instance_id the instance id
    @return list       the job's data root directories
    '''
    file_name = '%s/%s' % (self.get_run_dir(service, cluster, job, instance_id),
        JOB_RUN_CONFIG)
    if not os.path.exists(file_name):
      return 'You should bootstrapped the job first'

    data_dirs = self.get_available_data_dirs(service, cluster, job, instance_id)
    run_config = ConfigParser.SafeConfigParser()
    run_config.read([file_name])
    data_dir_indexes = run_config.get('run_info', 'data_dir_indexes')
    job_data_dirs = []
    for i in data_dir_indexes.split(','):
      job_data_dirs.append(data_dirs[int(i)])
    return job_data_dirs

  def get_package_dir(self, service, cluster, job, instance_id=-1):
    '''
    Get the current package directory of the specified job

    @param service     the server name
    @param cluster     the cluster name
    @param job         the job name
    @param instance_id the instance id
    @return string     the job's package root directory(symbol link)
    '''
    return '%s/package' % self.get_run_dir(service, cluster, job, instance_id)

  def get_real_package_dir(self, service, cluster, job, instance_id=-1):
    '''
    Get the current package directory real path of the specified job

    @param service     the server name
    @param cluster     the cluster name
    @param job         the job name
    @param instance_id the instance id
    @return string     the job's package root directory(real path)
    '''
    return os.readlink(self.get_package_dir(service, cluster, job, instance_id))

  def get_current_package_dir(self, service, cluster):
    '''
    Get the currently used package directory of the specified service

    @param service   the service name
    @param cluster   the cluster name
    @return string   the currently used package directory
    '''
    package_root = self.global_config.get('package_root')
    return '%s/%s/%s/current' % (package_root, service, cluster)

  def get_cleanup_token(self, service, cluster, job, instance_id=-1):
    '''
    Get the token used to do cleanuping

    @param service     the server name
    @param cluster     the cluster name
    @param job         the job name
    @param instance_id the instance id
    @return string     the job's cleanup token
    '''
    file_name = '%s/%s' % (self.get_run_dir(service, cluster, job, instance_id),
        JOB_RUN_CONFIG)
    if not os.path.exists(file_name):
      return 'You should bootstrap the job first'

    run_config = ConfigParser.SafeConfigParser()
    run_config.read([file_name])
    return run_config.get('run_info', 'cleanup_token')

  def bootstrap(self, service, cluster, job, config_dict, instance_id=-1):
    '''
    Bootstrap the specified job

    @param service      the server name
    @param cluster      the cluster name
    @param job          the job name
    @param instance_id  the instance id
    @param config_dict  the config information dictionary
    @return string      'OK' on success, otherwise, the error message

    Note: config_dict must contain the following item:
      1. artifact
      2. bootstrap.sh
      3. if any config files are needed, just put it in 'config_files' item

    config_dict can also contain the following optional items:
      1. cleanup_token: if this token is specified, user should supply
         the token to do cleanup
      2. package_name: package_name, revision, timestamp should be specified
         simultaneously, otherwise will be ignored
      3. revision
      4. timestamp
      5. data_dir_indexes: if this is not specified, the first data_dir is
         used by default
      6. force_update
    This is an example:
      config_dict = {
        'artifact': 'hadoop',
        'bootstrap.sh': $bootstrap_file_content,
        'config_files': {
          'core-site.xml': $core_site_xml_content,
          ...
        },
      }
    '''
    return self._do_bootstrap(service, cluster, job, instance_id, **config_dict)

  def start(self, service, cluster, job, config_dict, instance_id=-1):
    '''
    Start the specified job

    @param service      the server name
    @param cluster      the cluster name
    @param job          the job name
    @param instance_id  the instance id
    @param config_dict  the config information dictionary
    @return string      'OK' on success, otherwise, the error message

    Note: config_dict must contain the following item:
      1. start.sh
      2. artifact
      3. if any config files are needed, just put it in 'config_files' item

    config_dict can also contain the following optional items:
      1. http_url: the server's http service url
      2. package_name: package_name, revision, timestamp should be specified
         simultaneously, otherwise will be ignored
      3. revision
      4. timestamp
      5. force_update
    This is an example:
      config_dict = {
        'start.sh': $start_file_content,
        'artifact': hadoop,
        'config_files': {
          'core-site.xml': $core_site_xml_content,
          ...
        },
        'http_url': 'http://10.235.3.67:11201',
      }
    '''
    return self._do_start(service, cluster, job, instance_id, **config_dict)

  def stop(self, service, cluster, job, config_dict, instance_id=-1):
    '''
    Stop the specified job

    @param service      the server name
    @param cluster      the cluster name
    @param job          the job name
    @param instance_id  the instance id
    @param config_dict  the config information dictionary
    @return string      'OK' on success, otherwise, the error message

    Note: config_dict is not used currently, reserved for extendibility
    '''
    return self._do_stop(service, cluster, job, instance_id, **config_dict)

  def cleanup(self, service, cluster, job, config_dict, instance_id=-1):
    '''
    Cleanup the specified job's data/log directories

    @param service      the server name
    @param cluster      the cluster name
    @param job          the job name
    @param instance_id  the instance id
    @param config_dict  the config information dictionary
    @return string      'OK' on success, otherwise, the error message

    Note: config_dict may contain the following item:
      1. cleanup_token: [optional] token used to do verification
      2. cleanup.sh: [optional] script used to do cleanuping
    This is an example:
      config_dict = {
        'cleanup_token': '550e8400-e29b-41d4-a716-446655440000',
        'cleanup.sh': $cleanup_script,
      }
    '''
    return self._do_cleanup(service, cluster, job, instance_id, **config_dict)

  def show(self, service, cluster, job, config_dict, instance_id=-1):
    '''
    Get the specified job's current status
    @param service      the server name
    @param cluster      the cluster name
    @param job          the job name
    @param instance_id  the instance id
    @param config_dict  the config information dictionary
    @return string      the process status
    Possible values of  process status:
      RUNNING STARTING  BACKOFF STOPPING EXITED FATAL UNKNOWN

    Note: config_dict is not used currently, reserved for extendibility
    '''
    return self._do_show(service, cluster, job, instance_id, **config_dict)

  def read_file(self, file_path):
    '''
    Read the file with the given file path on server
    @param file_path      the name of file to read
    '''
    with open(file_path) as fi:
      return fi.read()

  def write_text_to_file(self, file_path, content):
    '''
    Write content to the file with the given file path on server
    @param file_path      the name of file to write
    @param content        the content to write
    '''
    with open(file_path, 'w') as fi:
      fi.write(content)
    return 'OK'

  def append_text_to_file(self, file_path, content):
    '''
    Append content to the file with the given file path on server
    @param file_path      the name of file to append
    @param content        the content to append
    '''
    with open(file_path, 'a') as fi:
      fi.write(content)
    return 'OK'

  def _get_package_uri(self, artifact, revision, timestamp, package_name):
    return '%s/%s/%s/%s-%s/%s' % (self.package_server,
        self.download_package_uri, artifact,
        revision, timestamp, package_name)

  def _get_query_latest_package_info_uri(self, artifact):
    return '%s/%s/?artifact=%s' % (self.package_server,
        self.get_latest_package_info_uri, artifact)

  def _downlowd_package(self, uri, dest_file):
    data_file = urllib2.urlopen(uri, None, 30)
    if not os.path.exists(os.path.dirname(dest_file)):
      os.makedirs(os.path.dirname(dest_file))
    fp = open(dest_file, 'wb')
    fp.write(data_file.read())
    fp.close()
    data_file.close()

  def _write_file(self, file_path, file_content):
    fp = open(file_path, 'wb')
    fp.write(file_content)
    fp.close()

  def _write_config_files(self, run_dir, **config_dict):
    for file_name, content in config_dict.iteritems():
      file_path = '%s/%s' % (run_dir, file_name)
      if os.path.exists(file_path):
        os.remove(file_path)
      self._write_file(file_path, content)

  def _get_process_name(self, service, cluster, job, instance_id):
    if instance_id == -1:
      return '%s--%s--%s' % (service, cluster, job)
    else:
      return '%s--%s--%s%d' % (service, cluster, job, instance_id)

  def _cleanup_dir(self, path):
    # Remove the whole directory in case there are some hidden files.
    cmd = 'rm -rf %s/' % path
    subprocess.check_call(cmd, shell=True)

  def _check_dir_empty(self, path):
    if not os.path.exists(path):
      return True

    lists = os.listdir(path)
    return len(lists) == 0

  def _check_bootstrapped(self, service, cluster, job, instance_id):
    run_dir = self.get_run_dir(service, cluster, job, instance_id)
    return os.path.exists('%s/%s' % (run_dir, JOB_RUN_CONFIG))

  def _get_latest_package_info(self, artifact):
    uri = self._get_query_latest_package_info_uri(artifact)
    info_fp = urllib2.urlopen(uri, None, 30)
    info = info_fp.read()

    if info and info.startswith('{'):
      info_dict = eval(info)
      info_fp.close()
      return info_dict
    else:
      info_fp.close()
      return None

  def _make_package_dir(self, artifact, service, cluster, job, instance_id,
    revision, timestamp, package_name):
    # Check if the tarball is already downloaded, if not, download it
    package_path = '%s/%s/%s/%s-%s/%s' % (self.global_config.get('package_root'),
        service, cluster, revision, timestamp, package_name)
    if not os.path.exists(package_path):
      self._downlowd_package(
          self._get_package_uri(artifact, revision, timestamp, package_name),
          package_path)

    # Unpack the tarball
    package_dir = package_path[0: len(package_path) - len('.tar.gz')]
    if os.path.exists(package_dir):
      cmd = ['rm', '-rf', package_dir]
      subprocess.check_call(cmd)
    cmd = ['tar', '-zxf', package_path, '-C', os.path.dirname(package_dir)]
    subprocess.check_call(cmd)

    # Link the package dir to the 'current'
    current_dir = self.get_current_package_dir(service, cluster)
    if os.path.lexists(current_dir):
      os.unlink(current_dir)
    os.symlink(package_dir, current_dir)

    # Link the package dir to the run dir
    symbol_package_dir = self.get_package_dir(service, cluster, job, instance_id)
    if os.path.lexists(symbol_package_dir):
      os.unlink(symbol_package_dir)
    os.symlink(package_dir, symbol_package_dir)
    return package_dir

  def _update_run_cfg(self, file_path, section, key, value):
    run_config = ConfigParser.SafeConfigParser()
    run_config.read([file_path])
    run_config.set(section, key, value)
    fp = open(file_path, 'w')
    run_config.write(fp)
    fp.close()

  def _prepare_run_env(self, service, cluster, job, instance_id, **config_dict):
    artifact = config_dict.get('artifact')
    if not artifact:
      return 'Invalid config_dict: can\'t find artifact'

    # Create run dirs
    run_dir = self.get_run_dir(service, cluster, job, instance_id)
    if not os.path.exists(run_dir):
      os.makedirs(run_dir)

    # Create stdout dir
    stdout_dir = self.get_stdout_dir(service, cluster, job, instance_id)
    if not os.path.exists(stdout_dir):
      os.makedirs(stdout_dir)

    # Create and link log dir to the run dir
    log_dir = self.get_log_dir(service, cluster, job, instance_id)
    if os.path.exists(log_dir):
      if not self._check_dir_empty(log_dir):
        return 'The log dir %s is not empty, please do cleanup first' % log_dir
    else:
      os.makedirs(log_dir)
    symbol_log_dir = '%s/log' % run_dir
    if not os.path.exists(symbol_log_dir):
      os.symlink(log_dir, symbol_log_dir)

    # Create and link data dirs to the run dir
    data_dirs = self.global_config.get('data_dirs', DEFAULT_DATA_DIRS).split(',')
    data_dir_indexes  = (config_dict.get('data_dir_indexes') or '0')
    for i in data_dir_indexes.split(','):
      if instance_id == -1:
        data_dir = '%s/%s/%s/%s' % (data_dirs[int(i)], service, cluster, job)
      else:
        data_dir = '%s/%s/%s/%s/%s' % (data_dirs[int(i)], service, cluster, job, instance_id)
      if os.path.exists(data_dir):
        if not self._check_dir_empty(data_dir):
          return 'The data dir %s is not empty, please do cleanup first' % data_dir
      else:
        try:
          os.makedirs(data_dir)
        except OSError, e:
          return "Error: %s" % str(e)
      symbol_data_dir = '%s/%s' % (run_dir, os.path.basename(data_dirs[int(i)]))
      if not os.path.exists(symbol_data_dir):
        os.symlink(data_dir, symbol_data_dir)

    # Check the package information
    force_update = config_dict.get('force_update', False)
    if force_update:
      package_info = self._get_latest_package_info(artifact)
      if package_info:
        package_name = package_info.get('package_name')
        revision = package_info.get('revision')
        timestamp = package_info.get('timestamp')
    else:
      package_name = config_dict.get('package_name')
      revision = config_dict.get('revision')
      timestamp = config_dict.get('timestamp')
      if not (package_name and revision and timestamp):
        package_info = self._get_latest_package_info(artifact)
        if package_info:
          package_name = package_info.get('package_name')
          revision = package_info.get('revision')
          timestamp = package_info.get('timestamp')
    if not (package_name and revision and timestamp):
      return 'No package found on package server of %s' % artifact

    # Write the job's run.cfg
    try:
      package_dir = self._make_package_dir(artifact, service, cluster, job,
          instance_id, revision, timestamp, package_name)
    except urllib2.URLError, e:
      return "%s. There may be an error about your package information." % str(e)
    except subprocess.CalledProcessError, e:
      return "Error: %s" % str(e)
    cleanup_token = config_dict.get('cleanup_token', str())
    run_config = ConfigParser.SafeConfigParser()
    run_config.add_section('run_info')
    run_config.set('run_info', 'cleanup_token', cleanup_token)
    run_config.set('run_info', 'data_dir_indexes', data_dir_indexes)
    run_config.set('run_info', 'run_dir', run_dir)
    run_config.set('run_info', 'log_dir', log_dir)
    run_config.set('run_info', 'package_dir', package_dir)
    fp = open('%s/%s' % (run_dir, JOB_RUN_CONFIG), 'w')
    run_config.write(fp)
    fp.close()
    return SUCCESS_STATUS

  def _do_bootstrap(self, service, cluster, job, instance_id, **config_dict):
    # prepare run dir
    message = self._prepare_run_env(service, cluster, job, instance_id, **config_dict)
    if message != SUCCESS_STATUS:
      return message

    # Write other config files to local disk
    config_files = config_dict.get('config_files')
    service_root = self.get_run_dir(service, cluster, job, instance_id)
    if config_files:
      self._write_config_files(service_root, **config_files)

    # Do bootstraping
    bootstrap_sh = config_dict.get('bootstrap.sh')
    if bootstrap_sh:
      self._write_file('%s/bootstrap.sh' % service_root, bootstrap_sh)
      cmd = ['/bin/bash', '%s/bootstrap.sh' % service_root]
      subprocess.call(cmd)
    return SUCCESS_STATUS

  def _do_start(self, service, cluster, job, instance_id, **config_dict):
    artifact = config_dict.get('artifact')
    if not artifact:
      return 'Inval config_dict: can\'t find artifact'

    if not self._check_bootstrapped(service, cluster, job, instance_id):
      return "You should bootstrap the job first"

    # Check if need update the package
    force_update = config_dict.get('force_update', False)
    if force_update:
      package_info = self._get_latest_package_info(artifact)
      if package_info:
        package_name = package_info.get('package_name')
        revision = package_info.get('revision')
        timestamp = package_info.get('timestamp')
    else:
      package_name = config_dict.get('package_name')
      revision = config_dict.get('revision')
      timestamp = config_dict.get('timestamp')

    if (package_name and revision and timestamp):
      package_path = '%s/%s/%s-%s/%s' % (
          self.global_config.get('package_root'),
          artifact, revision, timestamp, package_name)
      try:
        if not os.path.exists(package_path):
          self._downlowd_package(
              self._get_package_uri(artifact, revision, timestamp, package_name),
              package_path)
        package_dir = self._make_package_dir(artifact, service, cluster, job,
            instance_id, revision, timestamp, package_name)
      except urllib2.URLError, e:
        return "%s. There may be an error about your package information." % str(e)
      except subprocess.CalledProcessError, e:
        return "Error: %s" % str(e)
      run_cfg = '%s/%s' % (self.get_run_dir(service, cluster, job, instance_id),
          JOB_RUN_CONFIG)
      self._update_run_cfg(run_cfg, 'run_info', 'package_dir', package_dir)

    # Write the start script to local disk
    start_sh = config_dict.get('start.sh')
    service_root = self.get_run_dir(service, cluster, job, instance_id)
    if not start_sh and not os.path.exists('%s/start.sh' % service_root):
      return 'No start script found'
    elif start_sh:
      self._write_file('%s/start.sh' % service_root, start_sh)

    # Write other config files to local disk
    config_files = config_dict.get('config_files')
    if config_files:
      self._write_config_files(service_root, **config_files)

    # Write supervisor config
    http_url = config_dict.get('http_url', '')
    process_name = self._get_process_name(service, cluster, job, instance_id)
    job_config = ConfigParser.SafeConfigParser()
    section = 'program:%s' % process_name
    job_config.add_section(section)
    job_config.set(section, 'command', '/bin/bash %s/start.sh' % service_root)
    job_config.set(section, 'process_name', process_name)
    job_config.set(section, 'directory', service_root)
    job_config.set(section, 'http_url', http_url)
    # Process will be unconditionally restarted when it exits, without regard
    # to its exit code
    job_config.set(section, 'autorestart', 'true')
    job_config.set(section, 'exitcodes', str(DEFAULT_EXPECTED_EXIT_CODE))
    # Process will NOT be automatically started when supervisor restart.
    job_config.set(section, 'autostart', 'false')
    fp = open('%s/%s/%s.cfg' % (os.getcwd(), CONFIG_PATH, process_name), 'wb')
    job_config.write(fp)
    fp.close()

    # Start the job
    self.supervisor_rpcinterface.reloadConfig()
    try:
      self.supervisor_rpcinterface.addProcessGroup(process_name)
    except RPCError, e:
      if e.code != Faults.ALREADY_ADDED:
        raise e
    self.supervisor_rpcinterface.startProcess(process_name)()
    return SUCCESS_STATUS

  def _do_stop(self, service, cluster, job, instance_id, **config_dict):
    process_name = self._get_process_name(service, cluster, job, instance_id)
    self.supervisor_rpcinterface.stopProcess(process_name)()
    return SUCCESS_STATUS

  def _do_cleanup(self, service, cluster, job, instance_id, **config_dict):
    # check cleanup token
    cleanup_token = config_dict.get('cleanup_token')
    if cleanup_token:
      local_token = self.get_cleanup_token(service, cluster, job, instance_id)
      if local_token != cleanup_token:
        return 'Cleanup token is invalid'

    try:
      state = self._do_show(service, cluster, job, instance_id, **config_dict)
      if state == 'RUNNING':
        return 'You should stop the job first'
    except RPCError, e:
      pass

    log_dir = self.get_log_dir(service, cluster, job, instance_id)
    cleanup_script = config_dict.get('cleanup.sh', str())
    if cleanup_script:
      service_root = self.get_run_dir(service, cluster, job, instance_id)
      self._write_file('%s/cleanup.sh' % service_root, cleanup_script)
      cmd = ['/bin/bash', '%s/cleanup.sh' % service_root]
      if subprocess.call(cmd) != 0:
        self._cleanup_dir(log_dir)
        return 'Execute cleanup.sh failed'

    self._cleanup_dir(log_dir)
    data_dirs = self.get_data_dirs(service, cluster, job, instance_id)
    for data_dir in data_dirs:
      self._cleanup_dir(data_dir)

    process_name = self._get_process_name(service, cluster, job, instance_id)
    job_config = '%s/%s/%s.cfg' % (os.getcwd(), CONFIG_PATH, process_name)
    if os.path.exists(job_config):
      os.remove(job_config)
      try:
        self.supervisor_rpcinterface.removeProcessGroup(process_name)
        self.supervisor_rpcinterface.reloadConfig()
      except RPCError, e:
        pass
    return SUCCESS_STATUS

  def _do_show(self, service, cluster, job, instance_id, **config_dict):
    info = self.supervisor_rpcinterface.getProcessInfo(
        self._get_process_name(service, cluster, job, instance_id))
    return info.get('statename')

def check_and_create(path):
  if not os.path.exists(path):
    os.makedirs(path)

def initialize_deployment_env(**config):
  app_root = config.get('app_root', DEFAULT_APP_ROOT)
  check_and_create(app_root)

  log_root = config.get('log_root', DEFAULT_LOG_ROOT)
  check_and_create(app_root)

  package_root = config.get('package_root', DEFAULT_PACKAGE_ROOT)
  check_and_create(package_root)

  data_dirs = config.get('data_dirs', DEFAULT_DATA_DIRS).split(',')
  for data_dir in data_dirs:
    if not os.path.exists(data_dir):
      raise RuntimeError(
          'Data dir %s must created before starting supervisord'
          % data_dir)

def deployment_rpcinterface(supervisord, **config):
  initialize_deployment_env(**config)
  return DeploymentRPCInterface(supervisord, **config)

def test():
  pass

if __name__ == '__main__':
  test()

########NEW FILE########
__FILENAME__ = deploy_supervisor
#!/usr/bin/env python
#

import ConfigParser
import os
import pexpect
import sys

def scp(host, user, passwd, local_file, remote_file):
  child = pexpect.spawn('scp -r %s %s@%s:%s' % (local_file,
        user, host, remote_file))
  print child.args

  ret = child.expect(['yes/no.*', 'password.*', pexpect.EOF,
      pexpect.TIMEOUT], timeout=30)
  if ret == 0:
    child.sendline('yes')
    child.expect('password.*', timeout=30)
    child.sendline(passwd)
    child.expect(pexpect.EOF)
  elif ret == 1:
    child.sendline(passwd)
    child.expect(pexpect.EOF)

def remote_exec(host, user, passwd, cmd):
  child = pexpect.spawn('ssh %s@%s "%s"' % (user, host, cmd))
  print child.args

  ret = child.expect(['yes/no.*', 'password.*', pexpect.EOF,
      pexpect.TIMEOUT], timeout=30)
  if ret == 0:
    child.sendline('yes')
    child.expect('password.*', timeout=30)
    child.sendline(passwd)
    child.expect(pexpect.EOF)
  elif ret == 1:
    child.sendline(passwd)
    child.expect(pexpect.EOF)

class Config:
  class NodeConfig:
    def __init__(self, config_dict):
      self.password = str()
      self.hosts = dict()
      for key, value in config_dict.iteritems():
        if key.startswith('host.'):
          self.hosts.update({key.split('.')[1]: value})
        else:
          setattr(self, key, value)

  def __init__(self, config_file):
    self.config = ConfigParser.SafeConfigParser()
    self.config.read([config_file])
    self.groups = set()

  def parse(self):
    for section in self.config.sections():
      config_dict = dict()
      for option in self.config.options(section):
        value = self.config.get(section, option)
        config_dict.update({option: value})
      node_config = Config.NodeConfig(config_dict)
      self.groups.add(section)
      setattr(self, section, node_config)

def generate_supervisor_config(run_dir, config, file):
  parser = ConfigParser.SafeConfigParser()
  parser.read([file])
  parser.set('rpcinterface:deployment', 'data_dirs', config.data_dirs)
  parser.write(open('%s/%s.tmp' % (run_dir, os.path.basename(file)), 'w'))

def deploy(supervisor_config, config):
  run_dir = os.path.dirname(sys.argv[0])
  generate_supervisor_config(run_dir, config, supervisor_config)

  for host in config.hosts.itervalues():
    user = config.user
    password = config.password
    dest_path = '%s/supervisor/' % config.root_dir
    remote_exec(host, user, password,
        'cd %s; mkdir -p supervisor' % config.root_dir)
    scp(host, user, password, '%s/conf' % run_dir, dest_path)
    scp(host, user, password, '%s/deployment' % run_dir, dest_path)
    scp(host, user, password, '%s/metrics' % run_dir, dest_path)
    scp(host, user, password, '%s/superlance' % run_dir, dest_path)
    scp(host, user, password, '%s/supervisor' % run_dir, dest_path)
    scp(host, user, password, '%s/start_supervisor.sh' % run_dir, dest_path)
    scp(host, user, password, '%s/stop_supervisor.sh' % run_dir, dest_path)
    scp(host, user, password, '%s/supervisorctl.py' % run_dir, dest_path)
    scp(host, user, password, '%s/supervisord.py' % run_dir, dest_path)
    scp(host, user, password, '%s/%s.tmp' % (run_dir,
          os.path.basename(supervisor_config)),
        '%s/supervisord.conf' % dest_path)
    remote_exec(host, user, password,
        'cd %s/supervisor; ./start_supervisor.sh' % config.root_dir)

def main(supervisor_config, deploy_config):
  config = Config(deploy_config)
  config.parse()
  for group in config.groups:
    deploy(supervisor_config, getattr(config, group))

if __name__ == '__main__':
  sys.path.append('%s/../client' % os.path.dirname(__file__))
  from deploy import deploy_utils
  supervisor_config = '%s/supervisord.conf' % deploy_utils.get_config_dir()
  deploy_config = '%s/deploy_supervisor.cfg' % deploy_utils.get_config_dir()
  main(supervisor_config, deploy_config)

########NEW FILE########
__FILENAME__ = crashmail
#!/usr/bin/env python -u
##############################################################################
#
# Copyright (c) 2007 Agendaless Consulting and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the BSD-like license at
# http://www.repoze.org/LICENSE.txt.  A copy of the license should accompany
# this distribution.  THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL
# EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND
# FITNESS FOR A PARTICULAR PURPOSE
#
##############################################################################

# A event listener meant to be subscribed to PROCESS_STATE_CHANGE
# events.  It will send mail when processes that are children of
# supervisord transition unexpectedly to the EXITED state.

# A supervisor config snippet that tells supervisor to use this script
# as a listener is below.
#
# [eventlistener:crashmail]
# command=/usr/bin/crashmail -o hostname -a -m notify-on-crash@domain.com -s '/usr/sbin/sendmail -t -i -f crash-notifier@domain.com'
# events=PROCESS_STATE
#
# Sendmail is used explicitly here so that we can specify the 'from' address.

doc = """\
crashmail.py [-p processname] [-a] [-o string] [-m mail_address]
             [-s sendmail] URL

Options:

-p -- specify a supervisor process_name.  Send mail when this process
      transitions to the EXITED state unexpectedly. If this process is
      part of a group, it can be specified using the
      'process_name:group_name' syntax.

-a -- Send mail when any child of the supervisord transitions
      unexpectedly to the EXITED state unexpectedly.  Overrides any -p
      parameters passed in the same crashmail process invocation.

-o -- Specify a parameter used as a prefix in the mail subject header.

-s -- the sendmail command to use to send email
      (e.g. "/usr/sbin/sendmail -t -i").  Must be a command which accepts
      header and message data on stdin and sends mail.  Default is
      "/usr/sbin/sendmail -t -i".

-m -- specify an email address.  The script will send mail to this
      address when crashmail detects a process crash.  If no email
      address is specified, email will not be sent.

The -p option may be specified more than once, allowing for
specification of multiple processes.  Specifying -a overrides any
selection of -p.

A sample invocation:

crashmail.py -p program1 -p group1:program2 -m dev@example.com

"""

import os
import sys

from supervisor import childutils

def usage():
    print doc
    sys.exit(255)

class CrashMail:

    def __init__(self, programs, any, email, sendmail, optionalheader):

        self.programs = programs
        self.any = any
        self.email = email
        self.sendmail = sendmail
        self.optionalheader = optionalheader
        self.stdin = sys.stdin
        self.stdout = sys.stdout
        self.stderr = sys.stderr

    def runforever(self, test=False):
        while 1:
            # we explicitly use self.stdin, self.stdout, and self.stderr
            # instead of sys.* so we can unit test this code
            headers, payload = childutils.listener.wait(self.stdin, self.stdout)

            if not headers['eventname'] == 'PROCESS_STATE_EXITED':
                # do nothing with non-TICK events
                childutils.listener.ok(self.stdout)
                if test:
                    self.stderr.write('non-exited event\n')
                    self.stderr.flush()
                    break
                continue

            pheaders, pdata = childutils.eventdata(payload+'\n')

            if int(pheaders['expected']):
                childutils.listener.ok(self.stdout)
                if test:
                    self.stderr.write('expected exit\n')
                    self.stderr.flush()
                    break
                continue

            msg = ('Process %(processname)s in group %(groupname)s exited '
                   'unexpectedly (pid %(pid)s) from state %(from_state)s' %
                   pheaders)

            subject = ' %s crashed at %s' % (pheaders['processname'],
                                             childutils.get_asctime())
            if self.optionalheader:
                subject = self.optionalheader + ':' + subject

            self.stderr.write('unexpected exit, mailing\n')
            self.stderr.flush()

            self.mail(self.email, subject, msg)

            childutils.listener.ok(self.stdout)
            if test:
                break

    def mail(self, email, subject, msg):
        body =  'To: %s\n' % self.email
        body += 'Subject: %s\n' % subject
        body += '\n'
        body += msg
        m = os.popen(self.sendmail, 'w')
        m.write(body)
        m.close()
        self.stderr.write('Mailed:\n\n%s' % body)
        self.mailed = body

def main(argv=sys.argv):
    import getopt
    short_args="hp:ao:s:m:"
    long_args=[
        "help",
        "program=",
        "any",
        "optionalheader="
        "sendmail_program=",
        "email=",
        ]
    arguments = argv[1:]
    try:
        opts, args = getopt.getopt(arguments, short_args, long_args)
    except:
        usage()

    programs = []
    any = False
    sendmail = '/usr/sbin/sendmail -t -i'
    email = None
    timeout = 10
    status = '200'
    inbody = None
    optionalheader = None

    for option, value in opts:

        if option in ('-h', '--help'):
            usage()

        if option in ('-p', '--program'):
            programs.append(value)

        if option in ('-a', '--any'):
            any = True

        if option in ('-s', '--sendmail_program'):
            sendmail = value

        if option in ('-m', '--email'):
            email = value

        if option in ('-o', '--optionalheader'):
            optionalheader = value

    if not 'SUPERVISOR_SERVER_URL' in os.environ:
        sys.stderr.write('crashmail must be run as a supervisor event '
                         'listener\n')
        sys.stderr.flush()
        return

    prog = CrashMail(programs, any, email, sendmail, optionalheader)
    prog.runforever()

if __name__ == '__main__':
    main()


########NEW FILE########
__FILENAME__ = crashmailbatch
#!/usr/bin/env python -u
##############################################################################
#
# Copyright (c) 2007 Agendaless Consulting and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the BSD-like license at
# http://www.repoze.org/LICENSE.txt.  A copy of the license should accompany
# this distribution.  THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL
# EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND
# FITNESS FOR A PARTICULAR PURPOSE
#
##############################################################################

# A event listener meant to be subscribed to PROCESS_STATE_CHANGE
# events.  It will send mail when processes that are children of
# supervisord transition unexpectedly to the EXITED state.

# A supervisor config snippet that tells supervisor to use this script
# as a listener is below.
#
# [eventlistener:crashmailbatch]
# command=python crashmailbatch --toEmail=you@bar.com --fromEmail=me@bar.com
# events=PROCESS_STATE,TICK_60

doc = """\
crashmailbatch.py [--interval=<batch interval in minutes>]
        [--toEmail=<email address>]
        [--fromEmail=<email address>]
        [--subject=<email subject>]
        [--smtpHost=<hostname or address>]

Options:

--interval  - batch cycle length (in minutes).  The default is 1.0 minute.
                  This means that all events in each cycle are batched together
                  and sent as a single email

--toEmail   - the email address to send alerts to

--fromEmail - the email address to send alerts from

--password  - the password of the from mail user

--subject   - the email subject line

--smtpHost  - the SMTP server's hostname or address (defaults to 'localhost')

--supervisordPort - the supervisord server's listening port

A sample invocation:

crashmailbatch.py --toEmail="you@bar.com" --fromEmail="me@bar.com"

"""

import ConfigParser
import os
import socket

from supervisor import childutils
from superlance.process_state_email_monitor import ProcessStateEmailMonitor

class CrashMailBatch(ProcessStateEmailMonitor):

    process_state_events = ['PROCESS_STATE_EXITED']

    def load_alert_config_file(self):
      alert_config_path = '%s/../alert.cfg' % os.path.dirname(__file__)
      parser = ConfigParser.SafeConfigParser()

      if os.path.exists(alert_config_path):
        parser.read([alert_config_path])
      return parser

    def add_customized_mail_list(self, pheaders):
      self.customized_mail_list = []
      name_list = pheaders['groupname'].split('--')
      alert_section = str()
      if len(name_list) == 3:
        service, cluster, job = name_list
        alert_section = service + "--" + cluster
      else:
        raise ValueError("Invalid cluster name: %s" % pheaders['groupname'])

      if self.alert_config_parser.has_option(alert_section, 'to_emails'):
        mail_list = [mail.strip()
          for mail in self.alert_config_parser.get(alert_section, 'to_emails').split(",")]
        for mail in mail_list:
          if mail not in self.to_emails:
            self.customized_mail_list.append(mail)

    def __init__(self, **kwargs):
        ProcessStateEmailMonitor.__init__(self, **kwargs)
        self.hostname = socket.gethostname()
        self.local_ip = socket.gethostbyname(self.hostname)
        self.subject = 'Crash alert from supervisord on %s' % self.hostname
        self.now = kwargs.get('now', None)
        self.alert_config_parser = self.load_alert_config_file()

    def get_process_state_change_msg(self, headers, payload):
        pheaders, pdata = childutils.eventdata(payload+'\n')

        if int(pheaders['expected']):
            return None

        self.add_customized_mail_list(pheaders)
        txt = 'Process %(groupname)s:%(processname)s (pid %(pid)s) died \
unexpectedly' % pheaders
        return '%s -- http://%s:%d -- %s' % (childutils.get_asctime(self.now),
            self.local_ip, self.supervisord_port, txt)

def main():
    crash = CrashMailBatch.create_from_cmd_line()
    crash.run()

if __name__ == '__main__':
    main()


########NEW FILE########
__FILENAME__ = crashsms
#!/usr/bin/env python -u
##############################################################################
#
# Copyright (c) 2007 Agendaless Consulting and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the BSD-like license at
# http://www.repoze.org/LICENSE.txt.  A copy of the license should accompany
# this distribution.  THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL
# EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND
# FITNESS FOR A PARTICULAR PURPOSE
#
##############################################################################

##############################################################################
# crashsms
# author: Juan Batiz-Benet (http://github.com/jbenet)
# based on crashmailbatch.py
##############################################################################


# A event listener meant to be subscribed to PROCESS_STATE_CHANGE
# events.  It will send mail when processes that are children of
# supervisord transition unexpectedly to the EXITED state.

# A supervisor config snippet that tells supervisor to use this script
# as a listener is below.
#
# [eventlistener:crashsms]
# command=python crashsms -t <mobile phone>@<mobile provider> -f me@bar.com -e TICK_5
# events=PROCESS_STATE,TICK_5

doc = """\
crashsms.py [--interval=<batch interval in minutes>]
        [--toEmail=<email address>]
        [--fromEmail=<email address>]
        [--subject=<email subject>]

Options:

-i,--interval  - batch cycle length (in minutes).  The default is 1 minute.
                 This means that all events in each cycle are batched together
                 and sent as a single email

-t,--toEmail   - the email address to send alerts to. Mobile providers
                 tend to allow sms messages to be sent to their phone numbers
                 via an email address (e.g.: 1234567890@txt.att.net)

-f,--fromEmail - the email address to send alerts from

-s,--subject   - the email subject line

-e, --tickEvent - specify which TICK event to use (e.g. TICK_5, TICK_60, TICK_3600)

A sample invocation:

crashsms.py -t <mobile phone>@<mobile provider> -f me@bar.com -e TICK_5

"""

from supervisor import childutils
from superlance.process_state_email_monitor import ProcessStateEmailMonitor

class CrashSMS(ProcessStateEmailMonitor):
  process_state_events = ['PROCESS_STATE_EXITED']

  def __init__(self, **kwargs):
    ProcessStateEmailMonitor.__init__(self, **kwargs)
    self.now = kwargs.get('now', None)

  def get_process_state_change_msg(self, headers, payload):
    pheaders, pdata = childutils.eventdata(payload+'\n')

    if int(pheaders['expected']):
        return None

    txt = '[%(groupname)s:%(processname)s](%(pid)s) exited unexpectedly' \
      % pheaders
    return '%s %s' % (txt, childutils.get_asctime(self.now))

def main():
  crash = CrashSMS.create_from_cmd_line()
  crash.run()

if __name__ == '__main__':
  main()
########NEW FILE########
__FILENAME__ = fatalmailbatch
#!/usr/bin/env python -u
##############################################################################
#
# Copyright (c) 2007 Agendaless Consulting and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the BSD-like license at
# http://www.repoze.org/LICENSE.txt.  A copy of the license should accompany
# this distribution.  THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL
# EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND
# FITNESS FOR A PARTICULAR PURPOSE
#
##############################################################################

# A event listener meant to be subscribed to PROCESS_STATE_CHANGE
# events.  It will send mail when processes that are children of
# supervisord transition unexpectedly to the EXITED state.

# A supervisor config snippet that tells supervisor to use this script
# as a listener is below.
#
# [eventlistener:fatalmailbatch]
# command=python fatalmailbatch
# events=PROCESS_STATE,TICK_60

doc = """\
fatalmailbatch.py [--interval=<batch interval in minutes>]
        [--toEmail=<email address>]
        [--fromEmail=<email address>]
        [--subject=<email subject>]

Options:

--interval  - batch cycle length (in minutes).  The default is 1 minute.
                  This means that all events in each cycle are batched together
                  and sent as a single email
                  
--toEmail   - the email address to send alerts to

--fromEmail - the email address to send alerts from

--subject - the email subject line

A sample invocation:

fatalmailbatch.py --toEmail="you@bar.com" --fromEmail="me@bar.com"

"""

from supervisor import childutils
from superlance.process_state_email_monitor import ProcessStateEmailMonitor

class FatalMailBatch(ProcessStateEmailMonitor):
    
    process_state_events = ['PROCESS_STATE_FATAL']

    def __init__(self, **kwargs):
        kwargs['subject'] = kwargs.get('subject', 'Fatal start alert from supervisord')
        ProcessStateEmailMonitor.__init__(self, **kwargs)
        self.now = kwargs.get('now', None)
 
    def get_process_state_change_msg(self, headers, payload):
        pheaders, pdata = childutils.eventdata(payload+'\n')

        txt = 'Process %(groupname)s:%(processname)s failed to start too many \
times' % pheaders
        return '%s -- %s' % (childutils.get_asctime(self.now), txt)

def main():
    fatal = FatalMailBatch.create_from_cmd_line()
    fatal.run()

if __name__ == '__main__':
    main()
########NEW FILE########
__FILENAME__ = grower
#!/usr/bin/env python
##############################################################################
#
# Copyright (c) 2007 Agendaless Consulting and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the BSD-like license at
# http://www.repoze.org/LICENSE.txt.  A copy of the license should accompany
# this distribution.  THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL
# EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND
# FITNESS FOR A PARTICULAR PURPOSE
#
##############################################################################

# A process which leaks 1MB per second on purpose so we can test the
# memmon killer

import time
L = []
M = pow(2, 20)

while 1:
    L.append('x'*M)
    time.sleep(1)
    

########NEW FILE########
__FILENAME__ = httpok
#!/usr/bin/env python -u
##############################################################################
#
# Copyright (c) 2007 Agendaless Consulting and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the BSD-like license at
# http://www.repoze.org/LICENSE.txt.  A copy of the license should accompany
# this distribution.  THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL
# EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND
# FITNESS FOR A PARTICULAR PURPOSE
#
##############################################################################

# A event listener meant to be subscribed to TICK_60 (or TICK_5)
# events, which restarts processes that are children of
# supervisord based on the response from an HTTP port.

# A supervisor config snippet that tells supervisor to use this script
# as a listener is below.
#
# [eventlistener:httpok]
# command=python -u /bin/httpok http://localhost:8080/tasty/service
# events=TICK_60

doc = """\
httpok.py [-p processname] [-a] [-g] [-t timeout] [-c status_code] [-b inbody]
          [-m mail_address] [-s sendmail] URL

Options:

-p -- specify a supervisor process_name.  Restart the supervisor
      process named 'process_name' if it's in the RUNNING state when
      the URL returns an unexpected result or times out.  If this
      process is part of a group, it can be specified using the
      'group_name:process_name' syntax.
      
-a -- Restart any child of the supervisord under in the RUNNING state
      if the URL returns an unexpected result or times out.  Overrides
      any -p parameters passed in the same httpok process
      invocation.

-g -- The ``gcore`` program.  By default, this is ``/usr/bin/gcore
      -o``.  The program should accept two arguments on the command
      line: a filename and a pid.

-d -- Core directory.  If a core directory is specified, httpok will
      try to use the ``gcore`` program (see ``-g``) to write a core
      file into this directory against each hung process before we
      restart it.  Append gcore stdout output to email.

-t -- The number of seconds that httpok should wait for a response
      before timing out.  If this timeout is exceeded, httpok will
      attempt to restart processes in the RUNNING state specified by
      -p or -a.  This defaults to 10 seconds.

-c -- specify an expected HTTP status code from a GET request to the
      URL.  If this status code is not the status code provided by the
      response, httpok will attempt to restart processes in the
      RUNNING state specified by -p or -a.  This defaults to the
      string, "200".

-b -- specify a string which should be present in the body resulting
      from the GET request.  If this string is not present in the
      response, the processes in the RUNNING state specified by -p
      or -a will be restarted.  The default is to ignore the
      body.

-s -- the sendmail command to use to send email
      (e.g. "/usr/sbin/sendmail -t -i").  Must be a command which accepts
      header and message data on stdin and sends mail.
      Default is "/usr/sbin/sendmail -t -i".

-m -- specify an email address.  The script will send mail to this
      address when httpok attempts to restart processes.  If no email
      address is specified, email will not be sent.

-e -- "eager":  check URL / emit mail even if no process we are monitoring
      is in the RUNNING state.  Enabled by default.

-E -- not "eager":  do not check URL / emit mail if no process we are
      monitoring is in the RUNNING state.

URL -- The URL to which to issue a GET request.

The -p option may be specified more than once, allowing for
specification of multiple processes.  Specifying -a overrides any
selection of -p.

A sample invocation:

httpok.py -p program1 -p group1:program2 http://localhost:8080/tasty

"""

import os
import sys
import time
import urlparse
import xmlrpclib

from supervisor import childutils
from supervisor.states import ProcessStates
from supervisor.options import make_namespec

import timeoutconn

def usage():
    print doc
    sys.exit(255)

class HTTPOk:
    connclass = None
    def __init__(self, rpc, programs, any, url, timeout, status, inbody,
                 email, sendmail, coredir, gcore, eager):
        self.rpc = rpc
        self.programs = programs
        self.any = any
        self.url = url
        self.timeout = timeout
        self.status = status
        self.inbody = inbody
        self.email = email
        self.sendmail = sendmail
        self.coredir = coredir
        self.gcore = gcore
        self.eager = eager
        self.stdin = sys.stdin
        self.stdout = sys.stdout
        self.stderr = sys.stderr

    def listProcesses(self, state=None):
        return [x for x in self.rpc.supervisor.getAllProcessInfo()
                   if x['name'] in self.programs and
                      (state is None or x['state'] == state)]

    def runforever(self, test=False):
        parsed = urlparse.urlsplit(self.url)
        scheme = parsed[0].lower()
        hostport = parsed[1]
        path = parsed[2]
        query = parsed[3]

        if query:
            path += '?' + query

        if self.connclass:
            ConnClass = self.connclass
        elif scheme == 'http':
            ConnClass = timeoutconn.TimeoutHTTPConnection
        elif scheme == 'https':
            ConnClass = timeoutconn.TimeoutHTTPSConnection
        else:
            raise ValueError('Bad scheme %s' % scheme)

        while 1:
            # we explicitly use self.stdin, self.stdout, and self.stderr
            # instead of sys.* so we can unit test this code
            headers, payload = childutils.listener.wait(self.stdin, self.stdout)

            if not headers['eventname'].startswith('TICK'):
                # do nothing with non-TICK events
                childutils.listener.ok(self.stdout)
                if test:
                    break
                continue

            conn = ConnClass(hostport)
            conn.timeout = self.timeout

            act = False

            specs = self.listProcesses(ProcessStates.RUNNING)
            if self.eager or len(specs) > 0:

                try:
                    conn.request('GET', path)
                    res = conn.getresponse()
                    body = res.read()
                    status = res.status
                    msg = 'status contacting %s: %s %s' % (self.url,
                                                           res.status,
                                                           res.reason)
                except Exception, why:
                    body = ''
                    status = None
                    msg = 'error contacting %s:\n\n %s' % (self.url, why)

                if str(status) != str(self.status):
                    subject = 'httpok for %s: bad status returned' % self.url
                    self.act(subject, msg)
                elif self.inbody and self.inbody not in body:
                    act = True
                    subject = 'httpok for %s: bad body returned' % self.url
                    self.act(subject, msg)

            childutils.listener.ok(self.stdout)
            if test:
                break

    def act(self, subject, msg):
        messages = [msg]

        def write(msg):
            self.stderr.write('%s\n' % msg)
            self.stderr.flush()
            messages.append(msg)

        try:
            specs = self.rpc.supervisor.getAllProcessInfo()
        except Exception, why:
            write('Exception retrieving process info %s, not acting' % why)
            return
            
        waiting = list(self.programs)
            
        if self.any:
            write('Restarting all running processes')
            for spec in specs:
                name = spec['name']
                group = spec['group']
                self.restart(spec, write)
                namespec = make_namespec(group, name)
                if name in waiting:
                    waiting.remove(name)
                if namespec in waiting:
                    waiting.remove(namespec)
        else:
            write('Restarting selected processes %s' % self.programs)
            for spec in specs:
                name = spec['name']
                group = spec['group']
                namespec = make_namespec(group, name)
                if (name in self.programs) or (namespec in self.programs):
                    self.restart(spec, write)
                    if name in waiting:
                        waiting.remove(name)
                    if namespec in waiting:
                        waiting.remove(namespec)

        if waiting:
            write(
                'Programs not restarted because they did not exist: %s' %
                waiting)

        if self.email:
            now = time.asctime()
            message = '\n'.join(messages)
            self.mail(self.email, subject, message)

    def mail(self, email, subject, msg):
        body =  'To: %s\n' % self.email
        body += 'Subject: %s\n' % subject
        body += '\n'
        body += msg
        m = os.popen(self.sendmail, 'w')
        m.write(body)
        m.close()
        self.stderr.write('Mailed:\n\n%s' % body)
        self.mailed = body

    def restart(self, spec, write):
        namespec = make_namespec(spec['group'], spec['name'])
        if spec['state'] is ProcessStates.RUNNING:
            if self.coredir and self.gcore:
                corename = os.path.join(self.coredir, namespec)
                m = os.popen(self.gcore + ' "%s" %s' % (corename, spec['pid']))
                write('gcore output for %s:\n\n %s' % (namespec, m.read()))
                m.close()
            write('%s is in RUNNING state, restarting' % namespec)
            try:
                self.rpc.supervisor.stopProcess(namespec)
            except xmlrpclib.Fault, what:
                write('Failed to stop process %s: %s' % (
                    namespec, what))

            try:
                self.rpc.supervisor.startProcess(namespec)
            except xmlrpclib.Fault, what:
                write('Failed to start process %s: %s' % (
                    namespec, what))
            else:
                write('%s restarted' % namespec)

        else:
            write('%s not in RUNNING state, NOT restarting' % namespec)
            

def main(argv=sys.argv):
    import getopt
    short_args="hp:at:c:b:s:m:g:d:eE"
    long_args=[
        "help",
        "program=",
        "any",
        "timeout=",
        "code=",
        "body=",
        "sendmail_program=",
        "email=",
        "gcore=",
        "coredir=",
        "eager",
        "not-eager",
        ]
    arguments = argv[1:]
    try:
        opts, args = getopt.getopt(arguments, short_args, long_args)
    except:
        usage()

    if not args:
        usage()
    if len(args) > 1:
        usage()

    programs = []
    any = False
    sendmail = '/usr/sbin/sendmail -t -i'
    gcore = '/usr/bin/gcore -o'
    coredir = None
    eager = True
    email = None
    timeout = 10
    status = '200'
    inbody = None

    for option, value in opts:

        if option in ('-h', '--help'):
            usage()

        if option in ('-p', '--program'):
            programs.append(value)

        if option in ('-a', '--any'):
            any = True

        if option in ('-s', '--sendmail_program'):
            sendmail = value

        if option in ('-m', '--email'):
            email = value

        if option in ('-t', '--timeout'):
            timeout = int(value)

        if option in ('-c', '--code'):
            status = value

        if option in ('-b', '--body'):
            inbody = value

        if option in ('-g', '--gcore'):
            gcore = value

        if option in ('-d', '--coredir'):
            coredir = value

        if option in ('-e', '--eager'):
            eager = True

        if option in ('-E', '--not-eager'):
            eager = False

    url = arguments[-1]

    try:
        rpc = childutils.getRPCInterface(os.environ)
    except KeyError, why:
        if why[0] != 'SUPERVISOR_SERVER_URL':
            raise
        sys.stderr.write('httpok must be run as a supervisor event '
                         'listener\n')
        sys.stderr.flush()
        return

    prog = HTTPOk(rpc, programs, any, url, timeout, status, inbody, email,
                  sendmail, coredir, gcore, eager)
    prog.runforever()

if __name__ == '__main__':
    main()
    
    
    

########NEW FILE########
__FILENAME__ = memmon
#!/usr/bin/env python -u
##############################################################################
#
# Copyright (c) 2007 Agendaless Consulting and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the BSD-like license at
# http://www.repoze.org/LICENSE.txt.  A copy of the license should accompany
# this distribution.  THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL
# EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND
# FITNESS FOR A PARTICULAR PURPOSE
#
##############################################################################

# A event listener meant to be subscribed to TICK_60 (or TICK_5)
# events, which restarts any processes that are children of
# supervisord that consume "too much" memory.  Performs horrendous
# screenscrapes of ps output.  Works on Linux and OS X (Tiger/Leopard)
# as far as I know.

# A supervisor config snippet that tells supervisor to use this script
# as a listener is below.
#
# [eventlistener:memmon]
# command=python memmon.py [options]
# events=TICK_60

doc = """\
memmon.py [-p processname=byte_size]  [-g groupname=byte_size] 
          [-a byte_size] [-s sendmail] [-m email_address]

Options:

-p -- specify a process_name=byte_size pair.  Restart the supervisor
      process named 'process_name' when it uses more than byte_size
      RSS.  If this process is in a group, it can be specified using
      the 'process_name:group_name' syntax.
      
-g -- specify a group_name=byte_size pair.  Restart any process in this group
      when it uses more than byte_size RSS.
      
-a -- specify a global byte_size.  Restart any child of the supervisord
      under which this runs if it uses more than byte_size RSS.

-s -- the sendmail command to use to send email
      (e.g. "/usr/sbin/sendmail -t -i").  Must be a command which accepts
      header and message data on stdin and sends mail.
      Default is "/usr/sbin/sendmail -t -i".

-m -- specify an email address.  The script will send mail to this
      address when any process is restarted.  If no email address is
      specified, email will not be sent.

The -p and -g options may be specified more than once, allowing for
specification of multiple groups and processes.

Any byte_size can be specified as a plain integer (10000) or a
suffix-multiplied integer (e.g. 1GB).  Valid suffixes are 'KB', 'MB'
and 'GB'.

A sample invocation:

memmon.py -p program1=200MB -p theprog:thegroup=100MB -g thegroup=100MB -a 1GB -s "/usr/sbin/sendmail -t -i" -m chrism@plope.com
"""

import os
import sys
import time
import xmlrpclib

from supervisor import childutils
from supervisor.datatypes import byte_size

def usage():
    print doc
    sys.exit(255)

def shell(cmd):
    return os.popen(cmd).read()

class Memmon:
    def __init__(self, programs, groups, any, sendmail, email, rpc):
        self.programs = programs
        self.groups = groups
        self.any = any
        self.sendmail = sendmail
        self.email = email
        self.rpc = rpc
        self.stdin = sys.stdin
        self.stdout = sys.stdout
        self.stderr = sys.stderr
        self.pscommand = 'ps -orss= -p %s'
        self.mailed = False # for unit tests

    def runforever(self, test=False):
        while 1:
            # we explicitly use self.stdin, self.stdout, and self.stderr
            # instead of sys.* so we can unit test this code
            headers, payload = childutils.listener.wait(self.stdin, self.stdout)

            if not headers['eventname'].startswith('TICK'):
                # do nothing with non-TICK events
                childutils.listener.ok(self.stdout)
                if test:
                    break
                continue

            status = []
            if self.programs:
                status.append(
                    'Checking programs %s' % ', '.join(
                    [ '%s=%s' % x for x in self.programs.items() ] )
                    )

            if self.groups:
                status.append(
                    'Checking groups %s' % ', '.join(
                    [ '%s=%s' % x for x in self.groups.items() ] )
                    )
            if self.any is not None:
                status.append('Checking any=%s' % self.any)

            self.stderr.write('\n'.join(status) + '\n')

            infos = self.rpc.supervisor.getAllProcessInfo()

            for info in infos:
                pid = info['pid']
                name = info['name']
                group = info['group']
                pname = '%s:%s' % (group, name)

                if not pid:
                    # ps throws an error in this case (for processes
                    # in standby mode, non-auto-started).
                    continue

                data = shell(self.pscommand % pid)
                if not data:
                    # no such pid (deal with race conditions)
                    continue

                try:
                    rss = data.lstrip().rstrip()
                    rss = int(rss) * 1024 # rss is in KB
                except ValueError:
                    # line doesn't contain any data, or rss cant be intified
                    continue

                for n in name, pname:
                    if n in self.programs:
                        self.stderr.write('RSS of %s is %s\n' % (pname, rss))
                        if  rss > self.programs[name]:
                            self.restart(pname, rss)
                            continue

                if group in self.groups:
                    self.stderr.write('RSS of %s is %s\n' % (pname, rss))
                    if rss > self.groups[group]:
                        self.restart(pname, rss)
                        continue

                if self.any is not None:
                    self.stderr.write('RSS of %s is %s\n' % (pname, rss))
                    if rss > self.any:
                        self.restart(pname, rss)
                        continue

            self.stderr.flush()
            childutils.listener.ok(self.stdout)
            if test:
                break

    def restart(self, name, rss):
        self.stderr.write('Restarting %s\n' % name)

        try:
            self.rpc.supervisor.stopProcess(name)
        except xmlrpclib.Fault, what:
            msg = ('Failed to stop process %s (RSS %s), exiting: %s' %
                   (name, rss, what))
            self.stderr.write(str(msg))
            if self.email:
                subject = 'memmon: failed to stop process %s, exiting' % name
                self.mail(self.email, subject, msg)
            raise

        try:
            self.rpc.supervisor.startProcess(name)
        except xmlrpclib.Fault, what:
            msg = ('Failed to start process %s after stopping it, '
                   'exiting: %s' % (name, what))
            self.stderr.write(str(msg))
            if self.email:
                subject = 'memmon: failed to start process %s, exiting' % name
                self.mail(self.email, subject, msg)
            raise

        if self.email:
            now = time.asctime()
            msg = (
                'memmon.py restarted the process named %s at %s because '
                'it was consuming too much memory (%s bytes RSS)' % (
                name, now, rss)
                )
            subject = 'memmon: process %s restarted' % name
            self.mail(self.email, subject, msg)

    def mail(self, email, subject, msg):
        body =  'To: %s\n' % self.email
        body += 'Subject: %s\n' % subject
        body += '\n'
        body += msg
        m = os.popen(self.sendmail, 'w')
        m.write(body)
        m.close()
        self.mailed = body
        
def parse_namesize(option, value):
    try:
        name, size = value.split('=')
    except ValueError:
        print 'Unparseable value %r for %r' % (value, option)
        usage()
    size = parse_size(option, size)
    return name, size

def parse_size(option, value):
    try:
        size = byte_size(value)
    except:
        print 'Unparseable byte_size in %r for %r' % (value, option)
        usage()
        
    return size

def main():
    import getopt
    short_args="hp:g:a:s:m:"
    long_args=[
        "help",
        "program=",
        "group=",
        "any=",
        "sendmail_program=",
        "email=",
        ]
    arguments = sys.argv[1:]
    if not arguments:
        usage()
    try:
        opts, args=getopt.getopt(arguments, short_args, long_args)
    except:
        print __doc__
        sys.exit(2)

    programs = {}
    groups = {}
    any = None
    sendmail = '/usr/sbin/sendmail -t -i'
    email = None

    for option, value in opts:

        if option in ('-h', '--help'):
            usage()

        if option in ('-p', '--program'):
            name, size = parse_namesize(option, value)
            programs[name] = size

        if option in ('-g', '--group'):
            name, size = parse_namesize(option, value)
            groups[name] = size

        if option in ('-a', '--any'):
            size = parse_size(option, value)
            any = size

        if option in ('-s', '--sendmail_program'):
            sendmail = value

        if option in ('-m', '--email'):
            email = value

    rpc = childutils.getRPCInterface(os.environ)
    memmon = Memmon(programs, groups, any, sendmail, email, rpc)
    memmon.runforever()

if __name__ == '__main__':
    main()
    
    
    

########NEW FILE########
__FILENAME__ = process_exit_monitor
#!/usr/bin/env python -u

##############################################################################
#
# This script is subject to the execution of the post script when some process
# has entered the stopped or exited state.
#
##############################################################################

import ConfigParser
import os
import re
import subprocess
import sys

from supervisor import childutils

JOB_INSTANCES_REGEX = re.compile('(?P<job>[A-Za-z_]+)(?P<instance_id>\d+)?$')

def handle_event(payload):
  '''
  Execute the post script when the monitored events happen
  '''
  pheaders, pdata = childutils.eventdata(payload+'\n')
  name_list = pheaders['groupname'].split('--')
  if len(name_list) == 3:
    service, cluster, job = name_list
  else:
    return None

  childutils.pcomm.stderr(childutils.get_asctime()+' Process %(processname)s '
    'in group %(groupname)s exited from state %(from_state)s. '
    'Now execute the post script.\n' % pheaders)

  supervisor_config_path = '%s/../supervisord.conf' % os.path.dirname(__file__)
  if not os.path.exists(supervisor_config_path):
    childutils.pcomm.stderr('Cannot find the config file: supervisord.conf.\n')

  parser = ConfigParser.SafeConfigParser()
  parser.read([supervisor_config_path])

  sys.path.append('%s/../deployment' % os.path.dirname(__file__))
  from rpcinterface import DEFAULT_APP_ROOT
  app_root = parser.get('rpcinterface:deployment', 'app_root', DEFAULT_APP_ROOT)
  reg_expr = JOB_INSTANCES_REGEX.match(job)
  job = reg_expr.group('job')

  if reg_expr.group('instance_id'):
    instance_id = reg_expr.group('instance_id')
    service_root = '%s/%s/%s/%s/%s' % (app_root, service, cluster, job, instance_id)
  else:
    service_root = '%s/%s/%s/%s' % (app_root, service, cluster, job)

  if not os.path.exists('%s/post.sh' % service_root):
    childutils.pcomm.stderr('No post.sh for %s found.\n' % service)
    return None

  cmd = ['/bin/bash', '%s/post.sh' % service_root]
  subprocess.call(cmd)


def main():
  process_state_events = ['PROCESS_STATE_STOPPED', 'PROCESS_STATE_BACKOFF',
    'PROCESS_STATE_EXITED', 'PROCESS_STATE_FATAL']
  while True:
    headers, payload = childutils.listener.wait()

    if headers['eventname'] in process_state_events:
      handle_event(payload)

    childutils.listener.ok()

if __name__ == '__main__':
  main()


########NEW FILE########
__FILENAME__ = process_state_email_monitor
#!/usr/bin/env python -u
##############################################################################
#
# Copyright (c) 2007 Agendaless Consulting and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the BSD-like license at
# http://www.repoze.org/LICENSE.txt.  A copy of the license should accompany
# this distribution.  THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL
# EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND
# FITNESS FOR A PARTICULAR PURPOSE
#
##############################################################################
import os
import sys
import smtplib
import copy
# Using old reference for Python 2.4
from email.MIMEText import MIMEText
# from email.mime.text import MIMEText
from superlance.process_state_monitor import ProcessStateMonitor

doc = """\
Base class for common functionality when monitoring process state changes
and sending email notification
"""

class ProcessStateEmailMonitor(ProcessStateMonitor):
    COMMASPACE = ', '

    @classmethod
    def parse_cmd_line_options(cls):
        from optparse import OptionParser

        parser = OptionParser()
        parser.add_option("-i", "--interval", dest="interval", type="float", default=1.0,
                          help="batch interval in minutes (defaults to 1 minute)")
        parser.add_option("-t", "--toEmail", dest="to_emails",
                          help="destination email address(es) - comma separated")
        parser.add_option("-f", "--fromEmail", dest="from_email",
                          help="source email address")
        parser.add_option("-p", "--password", dest="password",
                          help="source email password")
        parser.add_option("-s", "--subject", dest="subject",
                          help="email subject")
        parser.add_option("-H", "--smtpHost", dest="smtp_host", default="localhost",
                          help="SMTP server hostname or address")
        parser.add_option("-e", "--tickEvent", dest="eventname", default="TICK_60",
                          help="TICK event name (defaults to TICK_60)")
        parser.add_option("-l", "--supervisordPort", dest="supervisord_port",
                          default=9001, help="the supervisord server port")
        
        (options, args) = parser.parse_args()
        return options
        
    @classmethod
    def validate_cmd_line_options(cls, options):
        if not options.to_emails:
            parser.print_help()
            sys.exit(1)
        if not options.from_email:
            parser.print_help()
            sys.exit(1)
        
        validated = copy.copy(options)
        validated.to_emails = [x.strip() for x in options.to_emails.split(",")]
        return validated
        
    @classmethod
    def get_cmd_line_options(cls):
        return cls.validate_cmd_line_options(cls.parse_cmd_line_options())

    @classmethod
    def create_from_cmd_line(cls):
        options = cls.get_cmd_line_options()

        if not 'SUPERVISOR_SERVER_URL' in os.environ:
            sys.stderr.write('Must run as a supervisor event listener\n')
            sys.exit(1)

        return cls(**options.__dict__)

    def __init__(self, **kwargs):
        ProcessStateMonitor.__init__(self, **kwargs)

        self.from_email = kwargs['from_email']
        self.to_emails = kwargs['to_emails']
        self.subject = kwargs.get('subject')
        self.smtp_host = kwargs.get('smtp_host', 'localhost')
        self.digest_len = 76
        self.password = kwargs.get('password')
        self.supervisord_port = kwargs.get('supervisord_port')
        self.customized_mail_list = []

    def send_batch_notification(self):
        email = self.get_batch_email()
        if email:
            self.send_email(email)
            self.log_email(email)

    def log_email(self, email):
        email_for_log = copy.copy(email)
        email_for_log['to'] = self.COMMASPACE.join(email['to'])
        if len(email_for_log['body']) > self.digest_len:
            email_for_log['body'] = '%s...' % email_for_log['body'][:self.digest_len]
        self.write_stderr("Sending notification email:\nTo: %(to)s\n\
From: %(from)s\nSubject: %(subject)s\nBody:\n%(body)s\n" % email_for_log)

    def get_batch_email(self):
        if len(self.batchmsgs):
            self.customized_mail_list.extend(self.to_emails)
            return {
                'to': self.customized_mail_list,
                'from': self.from_email,
                'subject': self.subject,
                'body': '\n'.join(self.get_batch_msgs()),
            }
        return None

    def send_email(self, email):
        msg = MIMEText(email['body'])
        if self.subject:
          msg['Subject'] = email['subject']
        msg['From'] = email['from']
        msg['To'] = self.COMMASPACE.join(email['to'])

        try:
            self.send_smtp(msg, email['to'])
        except Exception, e:
            self.write_stderr("Error sending email: %s\n" % e)

    def send_smtp(self, mime_msg, to_emails):
        s = smtplib.SMTP(self.smtp_host)
        if self.password:
            try:
                s.login(self.from_email.split('@')[0], self.password)
            except:
                s.quit()
                raise
        try:
            s.sendmail(mime_msg['From'], to_emails, mime_msg.as_string())
        except:
            s.quit()
            raise
        s.quit()


########NEW FILE########
__FILENAME__ = process_state_monitor
#!/usr/bin/env python -u
##############################################################################
#
# Copyright (c) 2007 Agendaless Consulting and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the BSD-like license at
# http://www.repoze.org/LICENSE.txt.  A copy of the license should accompany
# this distribution.  THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL
# EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO,
# THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND
# FITNESS FOR A PARTICULAR PURPOSE
#
##############################################################################
doc = """\
Base class for common functionality when monitoring process state changes
"""

import os
import sys

from supervisor import childutils

class ProcessStateMonitor:

    # In child class, define a list of events to monitor
    process_state_events = []

    def __init__(self, **kwargs):
        self.interval = kwargs.get('interval', 1.0)
        
        self.debug = kwargs.get('debug', False)
        self.stdin = kwargs.get('stdin', sys.stdin)
        self.stdout = kwargs.get('stdout', sys.stdout)
        self.stderr = kwargs.get('stderr', sys.stderr)
        self.eventname = kwargs.get('eventname', 'TICK_60')
        self.tickmins = self._get_tick_mins(self.eventname)
        
        self.batchmsgs = []
        self.batchmins = 0.0

    def _get_tick_mins(self, eventname):
        return float(self._get_tick_secs(eventname))/60.0

    def _get_tick_secs(self, eventname):
        self._validate_tick_name(eventname)
        return int(eventname.split('_')[1])
        
    def _validate_tick_name(self, eventname):
        if not eventname.startswith('TICK_'):
            raise ValueError("Invalid TICK event name: %s" % eventname)
 
    def run(self):
        while 1:
            hdrs, payload = childutils.listener.wait(self.stdin, self.stdout)
            self.handle_event(hdrs, payload)
            childutils.listener.ok(self.stdout)
    
    def handle_event(self, headers, payload):
        if headers['eventname'] in self.process_state_events:
            self.handle_process_state_change_event(headers, payload)
        elif headers['eventname'] == self.eventname:
            self.handle_tick_event(headers, payload)
    
    def handle_process_state_change_event(self, headers, payload):
        msg = self.get_process_state_change_msg(headers, payload)
        if msg:
            self.write_stderr('%s\n' % msg)
            self.batchmsgs.append(msg)

    """
    Override this method in child classes to customize messaging
    """
    def get_process_state_change_msg(self, headers, payload):
        return None

    def handle_tick_event(self, headers, payload):
        self.batchmins += self.tickmins
        if self.batchmins >= self.interval:
            self.send_batch_notification()
            self.clear_batch()
            
    """
    Override this method in child classes to send notification
    """
    def send_batch_notification(self):
        pass
    
    def get_batch_minutes(self):
        return self.batchmins
    
    def get_batch_msgs(self):
        return self.batchmsgs
        
    def clear_batch(self):
        self.batchmins = 0.0;
        self.batchmsgs = [];

    def write_stderr(self, msg):
        self.stderr.write(msg)
        self.stderr.flush()
########NEW FILE########
__FILENAME__ = crashmailbatch_test
import unittest
import mock
from StringIO import StringIO

class CrashMailBatchTests(unittest.TestCase):
    from_email = 'testFrom@blah.com'
    to_emails = ('testTo@blah.com')
    subject = 'Test Alert'
    unexpected_err_msg = 'Process bar:foo (pid 58597) died unexpectedly'

    def _get_target_class(self):
        from superlance.crashmailbatch import CrashMailBatch
        return CrashMailBatch
        
    def _make_one_mocked(self, **kwargs):
        kwargs['stdin'] = StringIO()
        kwargs['stdout'] = StringIO()
        kwargs['stderr'] = StringIO()
        kwargs['from_email'] = kwargs.get('from_email', self.from_email)
        kwargs['to_emails'] = kwargs.get('to_emails', self.to_emails)
        kwargs['subject'] = kwargs.get('subject', self.subject)
        
        obj = self._get_target_class()(**kwargs)
        obj.send_email = mock.Mock()
        return obj

    def get_process_exited_event(self, pname, gname, expected):
        headers = {
            'ver': '3.0', 'poolserial': '7', 'len': '71',
            'server': 'supervisor', 'eventname': 'PROCESS_STATE_EXITED',
            'serial': '7', 'pool': 'checkmailbatch',
        }
        payload = 'processname:%s groupname:%s from_state:RUNNING expected:%d \
pid:58597' % (pname, gname, expected)
        return (headers, payload)
        
    def test_get_process_state_change_msg_expected(self):
        crash = self._make_one_mocked()
        hdrs, payload = self.get_process_exited_event('foo', 'bar', 1)
        self.assertEquals(None, crash.get_process_state_change_msg(hdrs, payload))

    def test_get_process_state_change_msg_unexpected(self):
        crash = self._make_one_mocked()
        hdrs, payload = self.get_process_exited_event('foo', 'bar', 0)
        msg = crash.get_process_state_change_msg(hdrs, payload)
        self.failUnless(self.unexpected_err_msg in msg)
        
    def test_handle_event_exit_expected(self):
        crash = self._make_one_mocked()
        hdrs, payload = self.get_process_exited_event('foo', 'bar', 1)
        crash.handle_event(hdrs, payload)
        self.assertEquals([], crash.get_batch_msgs())
        self.assertEquals('', crash.stderr.getvalue())

    def test_handle_event_exit_unexpected(self):
        crash = self._make_one_mocked()
        hdrs, payload = self.get_process_exited_event('foo', 'bar', 0)
        crash.handle_event(hdrs, payload)
        msgs = crash.get_batch_msgs()
        self.assertEquals(1, len(msgs))
        self.failUnless(self.unexpected_err_msg in msgs[0])
        self.failUnless(self.unexpected_err_msg in crash.stderr.getvalue())

if __name__ == '__main__':
    unittest.main()         
########NEW FILE########
__FILENAME__ = crashmail_test
import sys
import unittest
from StringIO import StringIO

class CrashMailTests(unittest.TestCase):
    def _getTargetClass(self):
        from superlance.crashmail import CrashMail
        return CrashMail
    
    def _makeOne(self, *opts):
        return self._getTargetClass()(*opts)

    def setUp(self):
        import tempfile
        self.tempdir = tempfile.mkdtemp()

    def tearDown(self):
        import shutil
        shutil.rmtree(self.tempdir)

    def _makeOnePopulated(self, programs, any, response=None):

        import os
        sendmail = 'cat - > %s' % os.path.join(self.tempdir, 'email.log')
        email = 'chrism@plope.com'
        header = '[foo]'
        prog = self._makeOne(programs, any, email, sendmail, header)
        prog.stdin = StringIO()
        prog.stdout = StringIO()
        prog.stderr = StringIO()
        return prog

    def test_runforever_not_process_state_exited(self):
        programs = {'foo':0, 'bar':0, 'baz_01':0 }
        groups = {}
        any = None
        prog = self._makeOnePopulated(programs, any)
        prog.stdin.write('eventname:PROCESS_STATE len:0\n')
        prog.stdin.seek(0)
        prog.runforever(test=True)
        self.assertEqual(prog.stderr.getvalue(), 'non-exited event\n')

    def test_runforever_expected_exit(self):
        programs = ['foo']
        any = None
        prog = self._makeOnePopulated(programs, any)
        payload=('expected:1 processname:foo groupname:bar '
                 'from_state:RUNNING pid:1')
        prog.stdin.write(
            'eventname:PROCESS_STATE_EXITED len:%s\n' % len(payload))
        prog.stdin.write(payload)
        prog.stdin.seek(0)
        prog.runforever(test=True)
        self.assertEqual(prog.stderr.getvalue(), 'expected exit\n')

    def test_runforever_unexpected_exit(self):
        programs = ['foo']
        any = None
        prog = self._makeOnePopulated(programs, any)
        payload=('expected:0 processname:foo groupname:bar '
                 'from_state:RUNNING pid:1')
        prog.stdin.write(
            'eventname:PROCESS_STATE_EXITED len:%s\n' % len(payload))
        prog.stdin.write(payload)
        prog.stdin.seek(0)
        prog.runforever(test=True)
        output = prog.stderr.getvalue()
        lines = output.split('\n')
        self.assertEqual(lines[0], 'unexpected exit, mailing')
        self.assertEqual(lines[1], 'Mailed:')
        self.assertEqual(lines[2], '')
        self.assertEqual(lines[3], 'To: chrism@plope.com')
        self.failUnless('Subject: [foo]: foo crashed at' in lines[4])
        self.assertEqual(lines[5], '')
        self.failUnless(
            'Process foo in group bar exited unexpectedly' in lines[6])
        import os
        mail = open(os.path.join(self.tempdir, 'email.log'), 'r').read()
        self.failUnless(
            'Process foo in group bar exited unexpectedly' in mail)

if __name__ == '__main__':
    unittest.main()
########NEW FILE########
__FILENAME__ = crashsms_test
import unittest
import mock
import time
from StringIO import StringIO

from crashmailbatch_test import CrashMailBatchTests

class CrashSMSTests(CrashMailBatchTests):
    subject = 'None'
    unexpected_err_msg = '[bar:foo](58597) exited unexpectedly'

    def _get_target_class(self):
        from superlance.crashsms import CrashSMS
        return CrashSMS

if __name__ == '__main__':
    unittest.main()
########NEW FILE########
__FILENAME__ = dummy
class DummyRPCServer:
    def __init__(self):
        self.supervisor = DummySupervisorRPCNamespace()
        self.system = DummySystemRPCNamespace()

class DummyResponse:
    status = 200
    reason = 'OK'
    body = 'OK'
    def read(self):
        return self.body 
        
class DummySystemRPCNamespace:
    pass


import time
from supervisor.process import ProcessStates

_NOW = time.time()
    
class DummySupervisorRPCNamespace:
    _restartable = True
    _restarted = False
    _shutdown = False
    _readlog_error = False


    all_process_info = [
        {
        'name':'foo',
        'group':'foo',
        'pid':11,
        'state':ProcessStates.RUNNING,
        'statename':'RUNNING',
        'start':_NOW - 100,
        'stop':0,
        'spawnerr':'',
        'now':_NOW,
        'description':'foo description',
        },
        {
        'name':'bar',
        'group':'bar',
        'pid':12,
        'state':ProcessStates.FATAL,
        'statename':'FATAL',
        'start':_NOW - 100,
        'stop':_NOW - 50,
        'spawnerr':'screwed',
        'now':_NOW,
        'description':'bar description',
        },
        {
        'name':'baz_01',
        'group':'baz',
        'pid':12,
        'state':ProcessStates.STOPPED,
        'statename':'STOPPED',
        'start':_NOW - 100,
        'stop':_NOW - 25,
        'spawnerr':'',
        'now':_NOW,
        'description':'baz description',
        },
        ]

    def getAllProcessInfo(self):
        return self.all_process_info

    def startProcess(self, name):
        from supervisor import xmlrpc
        from xmlrpclib import Fault
        if name.endswith('SPAWN_ERROR'):
            raise Fault(xmlrpc.Faults.SPAWN_ERROR, 'SPAWN_ERROR')
        return True

    def stopProcess(self, name):
        from supervisor import xmlrpc
        from xmlrpclib import Fault
        if name == 'BAD_NAME:BAD_NAME':
            raise Fault(xmlrpc.Faults.BAD_NAME, 'BAD_NAME:BAD_NAME') 
        if name.endswith('FAILED'):
            raise Fault(xmlrpc.Faults.FAILED, 'FAILED')
        return True


########NEW FILE########
__FILENAME__ = fatalmailbatch_test
import unittest
import mock
from StringIO import StringIO

class FatalMailBatchTests(unittest.TestCase):
    from_email = 'testFrom@blah.com'
    to_emails = ('testTo@blah.com')
    subject = 'Test Alert'
    unexpected_err_msg = 'Process bar:foo failed to start too many times'
    
    def _get_target_class(self):
        from superlance.fatalmailbatch import FatalMailBatch
        return FatalMailBatch
        
    def _make_one_mocked(self, **kwargs):
        kwargs['stdin'] = StringIO()
        kwargs['stdout'] = StringIO()
        kwargs['stderr'] = StringIO()
        kwargs['from_email'] = kwargs.get('from_email', self.from_email)
        kwargs['to_emails'] = kwargs.get('to_emails', self.to_emails)
        kwargs['subject'] = kwargs.get('subject', self.subject)
        
        obj = self._get_target_class()(**kwargs)
        obj.send_email = mock.Mock()
        return obj

    def get_process_fatal_event(self, pname, gname):
        headers = {
            'ver': '3.0', 'poolserial': '7', 'len': '71',
            'server': 'supervisor', 'eventname': 'PROCESS_STATE_FATAL',
            'serial': '7', 'pool': 'checkmailbatch',
        }
        payload = 'processname:%s groupname:%s from_state:BACKOFF' \
                % (pname, gname)
        return (headers, payload)
        
    def test_get_process_state_change_msg(self):
        crash = self._make_one_mocked()
        hdrs, payload = self.get_process_fatal_event('foo', 'bar')
        msg = crash.get_process_state_change_msg(hdrs, payload)
        self.failUnless(self.unexpected_err_msg in msg)
        
if __name__ == '__main__':
    unittest.main()
########NEW FILE########
__FILENAME__ = httpok_test
import sys
import time
import unittest
from StringIO import StringIO
from supervisor.process import ProcessStates
from superlance.tests.dummy import *

_NOW = time.time()

_FAIL = [ {
        'name':'FAILED',
        'group':'foo',
        'pid':11,
        'state':ProcessStates.RUNNING,
        'statename':'RUNNING',
        'start':_NOW - 100,
        'stop':0,
        'spawnerr':'',
        'now':_NOW,
        'description':'foo description',
        },
{
        'name':'SPAWN_ERROR',
        'group':'foo',
        'pid':11,
        'state':ProcessStates.RUNNING,
        'statename':'RUNNING',
        'start':_NOW - 100,
        'stop':0,
        'spawnerr':'',
        'now':_NOW,
        'description':'foo description',
        },]

def make_connection(response, exc=None):
    class TestConnection:
        def __init__(self, hostport):
            self.hostport = hostport

        def request(self, method, path):
            if exc:
                raise ValueError('foo')
            self.method = method
            self.path = path

        def getresponse(self):
            return response

    return TestConnection

class HTTPOkTests(unittest.TestCase):
    def _getTargetClass(self):
        from superlance.httpok import HTTPOk
        return HTTPOk
    
    def _makeOne(self, *opts):
        return self._getTargetClass()(*opts)

    def _makeOnePopulated(self, programs, any, response=None, exc=None,
                          gcore=None, coredir=None, eager=True):
        if response is None:
            response = DummyResponse()
        rpc = DummyRPCServer()
        sendmail = 'cat - > /dev/null'
        email = 'chrism@plope.com'
        url = 'http://foo/bar'
        timeout = 10
        status = '200'
        inbody = None
        gcore = gcore
        coredir = coredir
        prog = self._makeOne(rpc, programs, any, url, timeout, status,
                             inbody, email, sendmail, coredir, gcore, eager)
        prog.stdin = StringIO()
        prog.stdout = StringIO()
        prog.stderr = StringIO()
        prog.connclass = make_connection(response, exc=exc)
        return prog

    def test_listProcesses_no_programs(self):
        programs = []
        any = None
        prog = self._makeOnePopulated(programs, any)
        specs = list(prog.listProcesses())
        self.assertEqual(len(specs), 0)

    def test_listProcesses_w_RUNNING_programs_default_state(self):
        programs = ['foo']
        any = None
        prog = self._makeOnePopulated(programs, any)
        specs = list(prog.listProcesses())
        self.assertEqual(len(specs), 1)
        self.assertEqual(specs[0],
                         DummySupervisorRPCNamespace.all_process_info[0])

    def test_listProcesses_w_nonRUNNING_programs_default_state(self):
        programs = ['bar']
        any = None
        prog = self._makeOnePopulated(programs, any)
        specs = list(prog.listProcesses())
        self.assertEqual(len(specs), 1)
        self.assertEqual(specs[0],
                         DummySupervisorRPCNamespace.all_process_info[1])

    def test_listProcesses_w_nonRUNNING_programs_RUNNING_state(self):
        programs = ['bar']
        any = None
        prog = self._makeOnePopulated(programs, any)
        specs = list(prog.listProcesses(ProcessStates.RUNNING))
        self.assertEqual(len(specs), 0, (prog.programs, specs))

    def test_runforever_eager_notatick(self):
        programs = {'foo':0, 'bar':0, 'baz_01':0 }
        groups = {}
        any = None
        prog = self._makeOnePopulated(programs, any)
        prog.stdin.write('eventname:NOTATICK len:0\n')
        prog.stdin.seek(0)
        prog.runforever(test=True)
        self.assertEqual(prog.stderr.getvalue(), '')

    def test_runforever_eager_error_on_request_some(self):
        programs = ['foo', 'bar', 'baz_01', 'notexisting']
        any = None
        prog = self._makeOnePopulated(programs, any, exc=True)
        prog.stdin.write('eventname:TICK len:0\n')
        prog.stdin.seek(0)
        prog.runforever(test=True)
        lines = prog.stderr.getvalue().split('\n')
        #self.assertEqual(len(lines), 7)
        self.assertEqual(lines[0],
                         ("Restarting selected processes ['foo', 'bar', "
                          "'baz_01', 'notexisting']")
                         )
        self.assertEqual(lines[1], 'foo is in RUNNING state, restarting')
        self.assertEqual(lines[2], 'foo restarted')
        self.assertEqual(lines[3], 'bar not in RUNNING state, NOT restarting')
        self.assertEqual(lines[4],
                         'baz:baz_01 not in RUNNING state, NOT restarting')
        self.assertEqual(lines[5],
          "Programs not restarted because they did not exist: ['notexisting']")
        mailed = prog.mailed.split('\n')
        self.assertEqual(len(mailed), 12)
        self.assertEqual(mailed[0], 'To: chrism@plope.com')
        self.assertEqual(mailed[1],
                    'Subject: httpok for http://foo/bar: bad status returned')

    def test_runforever_eager_error_on_request_any(self):
        programs = []
        any = True
        prog = self._makeOnePopulated(programs, any, exc=True)
        prog.stdin.write('eventname:TICK len:0\n')
        prog.stdin.seek(0)
        prog.runforever(test=True)
        lines = prog.stderr.getvalue().split('\n')
        #self.assertEqual(len(lines), 6)
        self.assertEqual(lines[0], 'Restarting all running processes')
        self.assertEqual(lines[1], 'foo is in RUNNING state, restarting')
        self.assertEqual(lines[2], 'foo restarted')
        self.assertEqual(lines[3], 'bar not in RUNNING state, NOT restarting')
        self.assertEqual(lines[4],
                         'baz:baz_01 not in RUNNING state, NOT restarting')
        mailed = prog.mailed.split('\n')
        self.assertEqual(len(mailed), 11)
        self.assertEqual(mailed[0], 'To: chrism@plope.com')
        self.assertEqual(mailed[1],
                    'Subject: httpok for http://foo/bar: bad status returned')

    def test_runforever_eager_error_on_process_stop(self):
        programs = ['FAILED']
        any = False
        prog = self._makeOnePopulated(programs, any, exc=True)
        prog.rpc.supervisor.all_process_info = _FAIL
        prog.stdin.write('eventname:TICK len:0\n')
        prog.stdin.seek(0)
        prog.runforever(test=True)
        lines = prog.stderr.getvalue().split('\n')
        #self.assertEqual(len(lines), 5)
        self.assertEqual(lines[0], "Restarting selected processes ['FAILED']")
        self.assertEqual(lines[1], 'foo:FAILED is in RUNNING state, restarting')
        self.assertEqual(lines[2],
                    "Failed to stop process foo:FAILED: <Fault 30: 'FAILED'>")
        self.assertEqual(lines[3], 'foo:FAILED restarted')
        mailed = prog.mailed.split('\n')
        self.assertEqual(len(mailed), 10)
        self.assertEqual(mailed[0], 'To: chrism@plope.com')
        self.assertEqual(mailed[1],
                    'Subject: httpok for http://foo/bar: bad status returned')

    def test_runforever_eager_error_on_process_start(self):
        programs = ['SPAWN_ERROR']
        any = False
        prog = self._makeOnePopulated(programs, any, exc=True)
        prog.rpc.supervisor.all_process_info = _FAIL
        prog.stdin.write('eventname:TICK len:0\n')
        prog.stdin.seek(0)
        prog.runforever(test=True)
        lines = prog.stderr.getvalue().split('\n')
        #self.assertEqual(len(lines), 4)
        self.assertEqual(lines[0],
                         "Restarting selected processes ['SPAWN_ERROR']")
        self.assertEqual(lines[1],
                         'foo:SPAWN_ERROR is in RUNNING state, restarting')
        self.assertEqual(lines[2],
           "Failed to start process foo:SPAWN_ERROR: <Fault 50: 'SPAWN_ERROR'>")
        mailed = prog.mailed.split('\n')
        self.assertEqual(len(mailed), 9)
        self.assertEqual(mailed[0], 'To: chrism@plope.com')
        self.assertEqual(mailed[1],
                    'Subject: httpok for http://foo/bar: bad status returned')

    def test_runforever_eager_gcore(self):
        programs = ['foo', 'bar', 'baz_01', 'notexisting']
        any = None
        prog = self._makeOnePopulated(programs, any, exc=True, gcore="true",
                                      coredir="/tmp")
        prog.stdin.write('eventname:TICK len:0\n')
        prog.stdin.seek(0)
        prog.runforever(test=True)
        lines = prog.stderr.getvalue().split('\n')
        self.assertEqual(lines[0],
                         ("Restarting selected processes ['foo', 'bar', "
                          "'baz_01', 'notexisting']")
                         )
        self.assertEqual(lines[1], 'gcore output for foo:')
        self.assertEqual(lines[2], '')
        self.assertEqual(lines[3], ' ')
        self.assertEqual(lines[4], 'foo is in RUNNING state, restarting')
        self.assertEqual(lines[5], 'foo restarted')
        self.assertEqual(lines[6], 'bar not in RUNNING state, NOT restarting')
        self.assertEqual(lines[7],
                         'baz:baz_01 not in RUNNING state, NOT restarting')
        self.assertEqual(lines[8],
          "Programs not restarted because they did not exist: ['notexisting']")
        mailed = prog.mailed.split('\n')
        self.assertEqual(len(mailed), 15)
        self.assertEqual(mailed[0], 'To: chrism@plope.com')
        self.assertEqual(mailed[1],
                    'Subject: httpok for http://foo/bar: bad status returned')

    def test_runforever_not_eager_none_running(self):
        programs = ['bar', 'baz_01']
        any = None
        prog = self._makeOnePopulated(programs, any, exc=True, gcore="true",
                                      coredir="/tmp", eager=False)
        prog.stdin.write('eventname:TICK len:0\n')
        prog.stdin.seek(0)
        prog.runforever(test=True)
        lines = filter(None, prog.stderr.getvalue().split('\n'))
        self.assertEqual(len(lines), 0, lines)
        self.failIf('mailed' in prog.__dict__)

    def test_runforever_not_eager_running(self):
        programs = ['foo', 'bar']
        any = None
        prog = self._makeOnePopulated(programs, any, exc=True, eager=False)
        prog.stdin.write('eventname:TICK len:0\n')
        prog.stdin.seek(0)
        prog.runforever(test=True)
        lines = filter(None, prog.stderr.getvalue().split('\n'))
        self.assertEqual(lines[0],
                         ("Restarting selected processes ['foo', 'bar']")
                         )
        self.assertEqual(lines[1], 'foo is in RUNNING state, restarting')
        self.assertEqual(lines[2], 'foo restarted')
        self.assertEqual(lines[3], 'bar not in RUNNING state, NOT restarting')
        mailed = prog.mailed.split('\n')
        self.assertEqual(len(mailed), 10)
        self.assertEqual(mailed[0], 'To: chrism@plope.com')
        self.assertEqual(mailed[1],
                    'Subject: httpok for http://foo/bar: bad status returned')

if __name__ == '__main__':
    unittest.main()
########NEW FILE########
__FILENAME__ = memmon_test
import sys
import unittest
from StringIO import StringIO
from superlance.tests.dummy import *

class MemmonTests(unittest.TestCase):
    def _getTargetClass(self):
        from superlance.memmon import Memmon
        return Memmon
    
    def _makeOne(self, *opts):
        return self._getTargetClass()(*opts)

    def _makeOnePopulated(self, programs, groups, any):
        rpc = DummyRPCServer()
        sendmail = 'cat - > /dev/null'
        email = 'chrism@plope.com'
        memmon = self._makeOne(programs, groups, any, sendmail, email, rpc)
        memmon.stdin = StringIO()
        memmon.stdout = StringIO()
        memmon.stderr = StringIO()
        memmon.pscommand = 'echo 22%s'
        return memmon
        
    def test_runforever_notatick(self):
        programs = {'foo':0, 'bar':0, 'baz_01':0 }
        groups = {}
        any = None
        memmon = self._makeOnePopulated(programs, groups, any)
        memmon.stdin.write('eventname:NOTATICK len:0\n')
        memmon.stdin.seek(0)
        memmon.runforever(test=True)
        self.assertEqual(memmon.stderr.getvalue(), '')

    def test_runforever_tick_programs(self):
        programs = {'foo':0, 'bar':0, 'baz_01':0 }
        groups = {}
        any = None
        memmon = self._makeOnePopulated(programs, groups, any)
        memmon.stdin.write('eventname:TICK len:0\n')
        memmon.stdin.seek(0)
        memmon.runforever(test=True)
        lines = memmon.stderr.getvalue().split('\n')
        self.assertEqual(len(lines), 8)
        self.assertEqual(lines[0], 'Checking programs foo=0, bar=0, baz_01=0')
        self.assertEqual(lines[1], 'RSS of foo:foo is 2264064')
        self.assertEqual(lines[2], 'Restarting foo:foo')
        self.assertEqual(lines[3], 'RSS of bar:bar is 2265088')
        self.assertEqual(lines[4], 'Restarting bar:bar')
        self.assertEqual(lines[5], 'RSS of baz:baz_01 is 2265088')
        self.assertEqual(lines[6], 'Restarting baz:baz_01')
        self.assertEqual(lines[7], '')
        mailed = memmon.mailed.split('\n')
        self.assertEqual(len(mailed), 4)
        self.assertEqual(mailed[0], 'To: chrism@plope.com')
        self.assertEqual(mailed[1],
                         'Subject: memmon: process baz:baz_01 restarted')
        self.assertEqual(mailed[2], '')
        self.failUnless(mailed[3].startswith('memmon.py restarted'))

    def test_runforever_tick_groups(self):
        programs = {}
        groups = {'foo':0}
        any = None
        memmon = self._makeOnePopulated(programs, groups, any)
        memmon.stdin.write('eventname:TICK len:0\n')
        memmon.stdin.seek(0)
        memmon.runforever(test=True)
        lines = memmon.stderr.getvalue().split('\n')
        self.assertEqual(len(lines), 4)
        self.assertEqual(lines[0], 'Checking groups foo=0')
        self.assertEqual(lines[1], 'RSS of foo:foo is 2264064')
        self.assertEqual(lines[2], 'Restarting foo:foo')
        self.assertEqual(lines[3], '')
        mailed = memmon.mailed.split('\n')
        self.assertEqual(len(mailed), 4)
        self.assertEqual(mailed[0], 'To: chrism@plope.com')
        self.assertEqual(mailed[1],
          'Subject: memmon: process foo:foo restarted')
        self.assertEqual(mailed[2], '')
        self.failUnless(mailed[3].startswith('memmon.py restarted'))

    def test_runforever_tick_any(self):
        programs = {}
        groups = {}
        any = 0
        memmon = self._makeOnePopulated(programs, groups, any)
        memmon.stdin.write('eventname:TICK len:0\n')
        memmon.stdin.seek(0)
        memmon.runforever(test=True)
        lines = memmon.stderr.getvalue().split('\n')
        self.assertEqual(len(lines), 8)
        self.assertEqual(lines[0], 'Checking any=0')
        self.assertEqual(lines[1], 'RSS of foo:foo is 2264064')
        self.assertEqual(lines[2], 'Restarting foo:foo')
        self.assertEqual(lines[3], 'RSS of bar:bar is 2265088')
        self.assertEqual(lines[4], 'Restarting bar:bar')
        self.assertEqual(lines[5], 'RSS of baz:baz_01 is 2265088')
        self.assertEqual(lines[6], 'Restarting baz:baz_01')
        self.assertEqual(lines[7], '')
        mailed = memmon.mailed.split('\n')
        self.assertEqual(len(mailed), 4)

    def test_runforever_tick_programs_and_groups(self):
        programs = {'baz_01':0}
        groups = {'foo':0}
        any = None
        memmon = self._makeOnePopulated(programs, groups, any)
        memmon.stdin.write('eventname:TICK len:0\n')
        memmon.stdin.seek(0)
        memmon.runforever(test=True)
        lines = memmon.stderr.getvalue().split('\n')
        self.assertEqual(len(lines), 7)
        self.assertEqual(lines[0], 'Checking programs baz_01=0')
        self.assertEqual(lines[1], 'Checking groups foo=0')
        self.assertEqual(lines[2], 'RSS of foo:foo is 2264064')
        self.assertEqual(lines[3], 'Restarting foo:foo')
        self.assertEqual(lines[4], 'RSS of baz:baz_01 is 2265088')
        self.assertEqual(lines[5], 'Restarting baz:baz_01')
        self.assertEqual(lines[6], '')
        mailed = memmon.mailed.split('\n')
        self.assertEqual(len(mailed), 4)
        self.assertEqual(mailed[0], 'To: chrism@plope.com')
        self.assertEqual(mailed[1],
                         'Subject: memmon: process baz:baz_01 restarted')
        self.assertEqual(mailed[2], '')
        self.failUnless(mailed[3].startswith('memmon.py restarted'))

    def test_runforever_tick_programs_norestart(self):
        programs = {'foo': sys.maxint}
        groups = {}
        any = None
        memmon = self._makeOnePopulated(programs, groups, any)
        memmon.stdin.write('eventname:TICK len:0\n')
        memmon.stdin.seek(0)
        memmon.runforever(test=True)
        lines = memmon.stderr.getvalue().split('\n')
        self.assertEqual(len(lines), 3)
        self.assertEqual(lines[0], 'Checking programs foo=%s' % sys.maxint)
        self.assertEqual(lines[1], 'RSS of foo:foo is 2264064')
        self.assertEqual(lines[2], '')
        self.assertEqual(memmon.mailed, False)

    def test_stopprocess_fault_tick_programs_norestart(self):
        programs = {'foo': sys.maxint}
        groups = {}
        any = None
        memmon = self._makeOnePopulated(programs, groups, any)
        memmon.stdin.write('eventname:TICK len:0\n')
        memmon.stdin.seek(0)
        memmon.runforever(test=True)
        lines = memmon.stderr.getvalue().split('\n')
        self.assertEqual(len(lines), 3)
        self.assertEqual(lines[0], 'Checking programs foo=%s' % sys.maxint)
        self.assertEqual(lines[1], 'RSS of foo:foo is 2264064')
        self.assertEqual(lines[2], '')
        self.assertEqual(memmon.mailed, False)

    def test_stopprocess_fails_to_stop(self):
        programs = {'BAD_NAME': 0}
        groups = {}
        any = None
        memmon = self._makeOnePopulated(programs, groups, any)
        memmon.stdin.write('eventname:TICK len:0\n')
        memmon.stdin.seek(0)
        from supervisor.process import ProcessStates
        memmon.rpc.supervisor.all_process_info =  [ {
            'name':'BAD_NAME',
            'group':'BAD_NAME',
            'pid':11,
            'state':ProcessStates.RUNNING,
            'statename':'RUNNING',
            'start':0,
            'stop':0,
            'spawnerr':'',
            'now':0,
            'description':'BAD_NAME description',
             } ]
        import xmlrpclib
        self.assertRaises(xmlrpclib.Fault, memmon.runforever, True)
        lines = memmon.stderr.getvalue().split('\n')
        self.assertEqual(len(lines), 4)
        self.assertEqual(lines[0], 'Checking programs BAD_NAME=%s' % 0)
        self.assertEqual(lines[1], 'RSS of BAD_NAME:BAD_NAME is 2264064')
        self.assertEqual(lines[2], 'Restarting BAD_NAME:BAD_NAME')
        self.failUnless(lines[3].startswith('Failed'))
        mailed = memmon.mailed.split('\n')
        self.assertEqual(len(mailed), 4)
        self.assertEqual(mailed[0], 'To: chrism@plope.com')
        self.assertEqual(mailed[1],
          'Subject: memmon: failed to stop process BAD_NAME:BAD_NAME, exiting')
        self.assertEqual(mailed[2], '')
        self.failUnless(mailed[3].startswith('Failed'))
        
if __name__ == '__main__':
    unittest.main()  
########NEW FILE########
__FILENAME__ = process_state_email_monitor_test
import unittest
import mock
import time
from StringIO import StringIO

class ProcessStateEmailMonitorTestException(Exception):
    pass

class ProcessStateEmailMonitorTests(unittest.TestCase):
    from_email = 'testFrom@blah.com'
    to_emails = ('testTo@blah.com', 'testTo2@blah.com')
    to_str = 'testTo@blah.com, testTo2@blah.com'
    subject = 'Test Alert'
    
    def _get_target_class(self):
        from superlance.process_state_email_monitor \
        import ProcessStateEmailMonitor
        return ProcessStateEmailMonitor
    
    def _make_one(self, **kwargs):
        kwargs['stdin'] = StringIO()
        kwargs['stdout'] = StringIO()
        kwargs['stderr'] = StringIO()
        kwargs['from_email'] = kwargs.get('from_email', self.from_email)
        kwargs['to_emails'] = kwargs.get('to_emails', self.to_emails)
        kwargs['subject'] = kwargs.get('subject', self.subject)
        
        obj = self._get_target_class()(**kwargs)
        return obj
            
    def _make_one_mock_send_email(self, **kwargs):
        obj = self._make_one(**kwargs)
        obj.send_email = mock.Mock()
        return obj

    def _make_one_mock_send_smtp(self, **kwargs):
        obj = self._make_one(**kwargs)
        obj.send_smtp = mock.Mock()
        return obj
        
    def test_validate_cmd_line_options_single_to_email_ok(self):
        klass = self._get_target_class()
        
        options = mock.Mock()
        options.from_email = 'blah'
        options.to_emails = 'frog'
        
        validated = klass.validate_cmd_line_options(options)
        self.assertEquals(['frog'], validated.to_emails)

    def test_validate_cmd_line_options_multi_to_emails_ok(self):
        klass = self._get_target_class()
        
        options = mock.Mock()
        options.from_email = 'blah'
        options.to_emails = 'frog, log,dog'
        
        validated = klass.validate_cmd_line_options(options)
        self.assertEquals(['frog', 'log', 'dog'], validated.to_emails)
    
    def test_send_email_ok(self):
        email = {
            'body': 'msg1\nmsg2',
            'to': self.to_emails,
            'from': 'testFrom@blah.com',
            'subject': 'Test Alert',
        }
        monitor = self._make_one_mock_send_smtp()
        monitor.send_email(email)
        
        #Test that email was sent
        self.assertEquals(1, monitor.send_smtp.call_count)
        smtpCallArgs = monitor.send_smtp.call_args[0]
        mimeMsg = smtpCallArgs[0]
        self.assertEquals(self.to_str, mimeMsg['To'])
        self.assertEquals(email['from'], mimeMsg['From'])
        self.assertEquals(email['subject'], mimeMsg['Subject'])
        self.assertEquals(email['body'], mimeMsg.get_payload())

    def _raiseSTMPException(self, mime, to_emails):
        raise ProcessStateEmailMonitorTestException('test')
        
    def test_send_email_exception(self):
        email = {
            'body': 'msg1\nmsg2',
            'to': self.to_emails,
            'from': 'testFrom@blah.com',
            'subject': 'Test Alert',
        }
        monitor = self._make_one_mock_send_smtp()
        monitor.send_smtp.side_effect = self._raiseSTMPException
        monitor.send_email(email)

        #Test that error was logged to stderr
        self.assertEquals("Error sending email: test\n", monitor.stderr.getvalue())
    
    def test_send_batch_notification(self):
        test_msgs = ['msg1', 'msg2']
        monitor = self._make_one_mock_send_email()
        monitor.batchmsgs = test_msgs
        monitor.send_batch_notification()
        
        #Test that email was sent
        expected = {
            'body': 'msg1\nmsg2',
            'to': self.to_emails,
            'from': 'testFrom@blah.com',
            'subject': 'Test Alert',
        }
        self.assertEquals(1, monitor.send_email.call_count)
        monitor.send_email.assert_called_with(expected)
        
        #Test that email was logged
        self.assertEquals("""Sending notification email:
To: %s
From: testFrom@blah.com
Subject: Test Alert
Body:
msg1
msg2
""" % (self.to_str), monitor.stderr.getvalue())
        
    def test_log_email_with_body_digest(self):
        bodyLen = 80
        monitor = self._make_one_mock_send_email()
        email = {
            'to': ['you@fubar.com'],
            'from': 'me@fubar.com',
            'subject': 'yo yo',
            'body': 'a' * bodyLen,
        }
        monitor.log_email(email)
        self.assertEquals("""Sending notification email:
To: you@fubar.com
From: me@fubar.com
Subject: yo yo
Body:
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa...
""", monitor.stderr.getvalue())
        self.assertEquals('a' * bodyLen, email['body'])

    def test_log_email_without_body_digest(self):
        monitor = self._make_one_mock_send_email()
        email = {
            'to': ['you@fubar.com'],
            'from': 'me@fubar.com',
            'subject': 'yo yo',
            'body': 'a' * 20,
        }
        monitor.log_email(email)
        self.assertEquals("""Sending notification email:
To: you@fubar.com
From: me@fubar.com
Subject: yo yo
Body:
aaaaaaaaaaaaaaaaaaaa
""", monitor.stderr.getvalue())

if __name__ == '__main__':
    unittest.main()
########NEW FILE########
__FILENAME__ = process_state_monitor_test
import unittest
import mock
import time
from StringIO import StringIO
from superlance.process_state_monitor import ProcessStateMonitor

class TestProcessStateMonitor(ProcessStateMonitor):
    
    process_state_events = ['PROCESS_STATE_EXITED']
            
    def get_process_state_change_msg(self, headers, payload):
        return repr(payload)

class ProcessStateMonitorTests(unittest.TestCase):
    
    def _get_target_class(self):
        return TestProcessStateMonitor
        
    def _make_one_mocked(self, **kwargs):
        kwargs['stdin'] = StringIO()
        kwargs['stdout'] = StringIO()
        kwargs['stderr'] = StringIO()
        
        obj = self._get_target_class()(**kwargs)
        obj.send_batch_notification = mock.Mock()
        return obj

    def get_process_exited_event(self, pname, gname, expected,
                                eventname='PROCESS_STATE_EXITED'):
        headers = {
            'ver': '3.0', 'poolserial': '7', 'len': '71',
            'server': 'supervisor', 'eventname': eventname,
            'serial': '7', 'pool': 'checkmailbatch',
        }
        payload = 'processname:%s groupname:%s from_state:RUNNING expected:%d \
pid:58597' % (pname, gname, expected)
        return (headers, payload)
        
    def get_tick60_event(self):
        headers = {
            'ver': '3.0', 'poolserial': '5', 'len': '15',
            'server': 'supervisor', 'eventname': 'TICK_60',
            'serial': '5', 'pool': 'checkmailbatch',
        }
        payload = 'when:1279665240'
        return (headers, payload)

    def test__get_tick_secs(self):
        monitor = self._make_one_mocked()
        self.assertEquals(5, monitor._get_tick_secs('TICK_5'))
        self.assertEquals(60, monitor._get_tick_secs('TICK_60'))
        self.assertEquals(3600, monitor._get_tick_secs('TICK_3600'))
        self.assertRaises(ValueError, monitor._get_tick_secs, 'JUNK_60')

    def test__get_tick_mins(self):
        monitor = self._make_one_mocked()
        self.assertEquals(5.0/60.0, monitor._get_tick_mins('TICK_5'))
        
    def test_handle_event_exit(self):
        monitor = self._make_one_mocked()
        hdrs, payload = self.get_process_exited_event('foo', 'bar', 0)
        monitor.handle_event(hdrs, payload)
        unexpected_err_msg = repr(payload)
        self.assertEquals([unexpected_err_msg], monitor.get_batch_msgs())
        self.assertEquals('%s\n' % unexpected_err_msg, monitor.stderr.getvalue())

    def test_handle_event_non_exit(self):
        monitor = self._make_one_mocked()
        hdrs, payload = self.get_process_exited_event('foo', 'bar', 0,
                                            eventname='PROCESS_STATE_FATAL')
        monitor.handle_event(hdrs, payload)
        self.assertEquals([], monitor.get_batch_msgs())
        self.assertEquals('', monitor.stderr.getvalue())

    def test_handle_event_tick_interval_expired(self):
        monitor = self._make_one_mocked()
        #Put msgs in batch
        hdrs, payload = self.get_process_exited_event('foo', 'bar', 0)
        monitor.handle_event(hdrs, payload)
        hdrs, payload = self.get_process_exited_event('bark', 'dog', 0)
        monitor.handle_event(hdrs, payload)
        self.assertEquals(2, len(monitor.get_batch_msgs()))
        #Time expired
        hdrs, payload = self.get_tick60_event()
        monitor.handle_event(hdrs, payload)
        
        #Test that batch messages are now gone
        self.assertEquals([], monitor.get_batch_msgs())
        #Test that email was sent
        self.assertEquals(1, monitor.send_batch_notification.call_count)

    def test_handle_event_tick_interval_not_expired(self):
        monitor = self._make_one_mocked(interval=3)
        hdrs, payload = self.get_tick60_event()
        monitor.handle_event(hdrs, payload)
        self.assertEquals(1.0, monitor.get_batch_minutes())
        monitor.handle_event(hdrs, payload)
        self.assertEquals(2.0, monitor.get_batch_minutes())

if __name__ == '__main__':
    unittest.main()
########NEW FILE########
__FILENAME__ = timeoutconn
import httplib
import socket

class TimeoutHTTPConnection(httplib.HTTPConnection):
    """A customised HTTPConnection allowing a per-connection
    timeout, specified at construction."""
    timeout = None

    def connect(self):
        """Override HTTPConnection.connect to connect to
        host/port specified in __init__."""

        msg = "getaddrinfo returns an empty list"
        for res in socket.getaddrinfo(self.host, self.port,
                0, socket.SOCK_STREAM):
            af, socktype, proto, canonname, sa = res
            try:
                self.sock = socket.socket(af, socktype, proto)
                if self.timeout:   # this is the new bit
                    self.sock.settimeout(self.timeout)
                self.sock.connect(sa)
            except socket.error, msg:
                if self.sock:
                    self.sock.close()
                self.sock = None
                continue
            break
        if not self.sock:
            raise socket.error, msg

class TimeoutHTTPSConnection(httplib.HTTPSConnection):
    timeout = None
    
    def connect(self):
        "Connect to a host on a given (SSL) port."

        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        if self.timeout:
            self.sock.settimeout(self.timeout)
        sock.connect((self.host, self.port))
        ssl = socket.ssl(sock, self.key_file, self.cert_file)
        self.sock = httplib.FakeSocket(sock, ssl)

########NEW FILE########
__FILENAME__ = childutils
import sys
import time
import xmlrpclib
from supervisor.xmlrpc import SupervisorTransport
from supervisor.events import ProcessCommunicationEvent
from supervisor.dispatchers import PEventListenerDispatcher

def getRPCTransport(env):
    u = env.get('SUPERVISOR_USERNAME', '')
    p = env.get('SUPERVISOR_PASSWORD', '')
    return SupervisorTransport(u, p, env['SUPERVISOR_SERVER_URL'])

def getRPCInterface(env):
    # dumbass ServerProxy won't allow us to pass in a non-HTTP url,
    # so we fake the url we pass into it and always use the transport's
    # 'serverurl' to figure out what to attach to
    return xmlrpclib.ServerProxy('http://127.0.0.1', getRPCTransport(env))

def get_headers(line):
    return dict([ x.split(':') for x in line.split() ])

def eventdata(payload):
    headerinfo, data = payload.split('\n', 1)
    headers = get_headers(headerinfo)
    return headers, data

def get_asctime(now=None):
    if now is None: # for testing
        now = time.time() # pragma: no cover
    msecs = (now - long(now)) * 1000
    part1 = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(now))
    asctime = '%s,%03d' % (part1, msecs)
    return asctime

class ProcessCommunicationsProtocol:
    def send(self, msg, fp=sys.stdout):
        fp.write(ProcessCommunicationEvent.BEGIN_TOKEN)
        fp.write(msg)
        fp.write(ProcessCommunicationEvent.END_TOKEN)

    def stdout(self, msg):
        return self.send(msg, sys.stdout)

    def stderr(self, msg):
        return self.send(msg, sys.stderr)

pcomm = ProcessCommunicationsProtocol()

class EventListenerProtocol:
    def wait(self, stdin=sys.stdin, stdout=sys.stdout):
        self.ready(stdout)
        line = stdin.readline()
        headers = get_headers(line)
        payload = stdin.read(int(headers['len']))
        return headers, payload

    def ready(self, stdout=sys.stdout):
        stdout.write(PEventListenerDispatcher.READY_FOR_EVENTS_TOKEN)
        stdout.flush()

    def ok(self, stdout=sys.stdout):
        self.send('OK', stdout)

    def fail(self, stdout=sys.stdout):
        self.send('FAIL', stdout)

    def send(self, data, stdout=sys.stdout):
        resultlen = len(data)
        result = '%s%s\n%s' % (PEventListenerDispatcher.RESULT_TOKEN_START,
                               str(resultlen),
                               data)
        stdout.write(result)
        stdout.flush()

listener = EventListenerProtocol()

########NEW FILE########
__FILENAME__ = confecho
import pkg_resources
import sys

def main(out=sys.stdout):
    config = pkg_resources.resource_string(__name__, 'skel/sample.conf')
    out.write(config)

########NEW FILE########
__FILENAME__ = datatypes
# This file was modified by Xiaomi.com on 2013-6-27.

import grp
import os
import pwd
import re
import signal
import sys
import socket
import shlex
import urlparse
from supervisor.loggers import getLevelNumByDescription

# I dont know why we bother, this doesn't run on Windows, but just
# in case it ever does, avoid this bug magnet by leaving it.
if sys.platform[:3] == "win": # pragma: no cover
    DEFAULT_HOST = "localhost"
else:
    DEFAULT_HOST = ""

DEFAULT_EXPECTED_EXIT_CODE = 999

here = None

def set_here(v):
    global here
    here = v

def integer(value):
    try:
        return int(value)
    except ValueError:
        return long(value) # why does this help? (CM)
    except OverflowError:
        return long(value)

TRUTHY_STRINGS = ('yes', 'true', 'on', '1')
FALSY_STRINGS  = ('no', 'false', 'off', '0')

def boolean(s):
    """Convert a string value to a boolean value."""
    ss = str(s).lower()
    if ss in TRUTHY_STRINGS:
        return True
    elif ss in FALSY_STRINGS:
        return False
    else:
        raise ValueError("not a valid boolean value: " + repr(s))

def list_of_strings(arg):
    if not arg:
        return []
    try:
        return [x.strip() for x in arg.split(',')]
    except:
        raise ValueError("not a valid list of strings: " + repr(arg))

def list_of_ints(arg):
    if not arg:
        return []
    else:
        try:
            return map(int, arg.split(","))
        except:
            raise ValueError("not a valid list of ints: " + repr(arg))

def list_of_exitcodes(arg):
    try:
        vals = list_of_ints(arg)
        for val in vals:
            if val != DEFAULT_EXPECTED_EXIT_CODE and ((val > 255) or (val < 0)):
                raise ValueError('Invalid exit code "%s"' % val)
        return vals
    except:
        raise ValueError("not a valid list of exit codes: " + repr(arg))

def dict_of_key_value_pairs(arg):
    """ parse KEY=val,KEY2=val2 into {'KEY':'val', 'KEY2':'val2'}
        Quotes can be used to allow commas in the value
    """
    lexer = shlex.shlex(arg)
    lexer.wordchars += '/.+-():'

    tokens = list(lexer)
    tokens_len = len(tokens)

    D = {}
    i = 0
    while i < tokens_len:
        k_eq_v = tokens[i:i+3]
        if len(k_eq_v) != 3 or k_eq_v[1] != '=':
            raise ValueError, "Unexpected end of key/value pairs"
        D[k_eq_v[0]] = k_eq_v[2].strip('\'"')
        i += 4
    return D

class Automatic:
    pass

LOGFILE_NONES = ('none', 'off', None)
LOGFILE_AUTOS = (Automatic, 'auto')

def logfile_name(val):
    if hasattr(val, 'lower'):
        coerced = val.lower()
    else:
        coerced = val

    if coerced in LOGFILE_NONES:
        return None
    elif coerced in LOGFILE_AUTOS:
        return Automatic
    else:
        return existing_dirpath(val)

class RangeCheckedConversion:
    """Conversion helper that range checks another conversion."""

    def __init__(self, conversion, min=None, max=None):
        self._min = min
        self._max = max
        self._conversion = conversion

    def __call__(self, value):
        v = self._conversion(value)
        if self._min is not None and v < self._min:
            raise ValueError("%s is below lower bound (%s)"
                             % (`v`, `self._min`))
        if self._max is not None and v > self._max:
            raise ValueError("%s is above upper bound (%s)"
                             % (`v`, `self._max`))
        return v

port_number = RangeCheckedConversion(integer, min=1, max=0xffff).__call__

def inet_address(s):
    # returns (host, port) tuple
    host = ''
    port = None
    if ":" in s:
        host, s = s.split(":", 1)
        if not s:
            raise ValueError("no port number specified in %r" % s)
        port = port_number(s)
        host = host.lower()
    else:
        try:
            port = port_number(s)
        except ValueError:
            raise ValueError("not a valid port number: %r " %s)
    if not host or host == '*':
        host = DEFAULT_HOST
    return host, port

class SocketAddress:
    def __init__(self, s):
        # returns (family, address) tuple
        if "/" in s or s.find(os.sep) >= 0 or ":" not in s:
            self.family = getattr(socket, "AF_UNIX", None)
            self.address = s
        else:
            self.family = socket.AF_INET
            self.address = inet_address(s)

class SocketConfig:
    """ Abstract base class which provides a uniform abstraction
    for TCP vs Unix sockets """
    url = '' # socket url
    addr = None #socket addr

    def __repr__(self):
        return '<%s at %s for %s>' % (self.__class__,
                                      id(self),
                                      self.url)

    def __str__(self):
        return str(self.url)

    def __eq__(self, other):
        if not isinstance(other, SocketConfig):
            return False

        if self.url != other.url:
            return False

        return True

    def __ne__(self, other):
        return not self.__eq__(other)

    def addr(self): # pragma: no cover
        raise NotImplementedError

    def create_and_bind(self): # pragma: no cover
        raise NotImplementedError

class InetStreamSocketConfig(SocketConfig):
    """ TCP socket config helper """

    host = None # host name or ip to bind to
    port = None # integer port to bind to

    def __init__(self, host, port):
        self.host = host.lower()
        self.port = port_number(port)
        self.url = 'tcp://%s:%d' % (self.host, self.port)

    def addr(self):
        return (self.host, self.port)

    def create_and_bind(self):
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        sock.bind(self.addr())
        return sock

class UnixStreamSocketConfig(SocketConfig):
    """ Unix domain socket config helper """

    path = None # Unix domain socket path
    mode = None # Unix permission mode bits for socket
    owner = None # Tuple (uid, gid) for Unix ownership of socket
    sock = None # socket object

    def __init__(self, path, **kwargs):
        self.path = path
        self.url = 'unix://%s' % (path)
        self.mode = kwargs.get('mode', None)
        self.owner = kwargs.get('owner', None)

    def addr(self):
        return self.path

    def create_and_bind(self):
        if os.path.exists(self.path):
            os.unlink(self.path)
        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        sock.bind(self.addr())
        try:
            self._chown()
            self._chmod()
        except:
            sock.close()
            os.unlink(self.path)
            raise
        return sock

    def get_mode(self):
        return self.mode

    def get_owner(self):
        return self.owner

    def _chmod(self):
        if self.mode is not None:
            try:
                os.chmod(self.path, self.mode)
            except Exception, e:
                raise ValueError("Could not change permissions of socket "
                                    + "file: %s" % (e))

    def _chown(self):
        if self.owner is not None:
            try:
                os.chown(self.path, self.owner[0], self.owner[1])
            except Exception, e:
                raise ValueError("Could not change ownership of socket file: "
                                    + "%s" % (e))


def colon_separated_user_group(arg):
    try:
        result = arg.split(':', 1)
        if len(result) == 1:
            username = result[0]
            uid = name_to_uid(username)
            if uid is None:
                raise ValueError('Invalid user name %s' % username)
            return (uid, -1)
        else:
            username = result[0]
            groupname = result[1]
            uid = name_to_uid(username)
            gid = name_to_gid(groupname)
            if uid is None:
                raise ValueError('Invalid user name %s' % username)
            if gid is None:
                raise ValueError('Invalid group name %s' % groupname)
            return (uid, gid)
        return result
    except:
        raise ValueError, 'Invalid user.group definition %s' % arg

def octal_type(arg):
    try:
        return int(arg, 8)
    except TypeError:
        raise ValueError('%s is not convertable to an octal type' % arg)

def name_to_uid(name):
    if name is None:
        return None

    try:
        uid = int(name)
    except ValueError:
        try:
            pwrec = pwd.getpwnam(name)
        except KeyError:
            return None
        uid = pwrec[2]
    else:
        try:
            pwrec = pwd.getpwuid(uid)
        except KeyError:
            return None
    return uid

def name_to_gid(name):
    try:
        gid = int(name)
    except ValueError:
        try:
            pwrec = grp.getgrnam(name)
        except KeyError:
            return None
        gid = pwrec[2]
    else:
        try:
            pwrec = grp.getgrgid(gid)
        except KeyError:
            return None
    return gid

def gid_for_uid(uid):
    pwrec = pwd.getpwuid(uid)
    return pwrec[3]

def existing_directory(v):
    nv = v % {'here':here}
    nv = os.path.expanduser(nv)
    if os.path.isdir(nv):
        return nv
    raise ValueError('%s is not an existing directory' % v)

def existing_dirpath(v):
    nv = v % {'here':here}
    nv = os.path.expanduser(nv)
    dir = os.path.dirname(nv)
    if not dir:
        # relative pathname with no directory component
        return nv
    if os.path.isdir(dir):
        return nv
    raise ValueError, ('The directory named as part of the path %s '
                       'does not exist.' % v)

def logging_level(value):
    s = str(value).lower()
    level = getLevelNumByDescription(value)
    if level is None:
        raise ValueError('bad logging level name %r' % value)
    return level

class SuffixMultiplier:
    # d is a dictionary of suffixes to integer multipliers.  If no suffixes
    # match, default is the multiplier.  Matches are case insensitive.  Return
    # values are in the fundamental unit.
    def __init__(self, d, default=1):
        self._d = d
        self._default = default
        # all keys must be the same size
        self._keysz = None
        for k in d.keys():
            if self._keysz is None:
                self._keysz = len(k)
            else:
                assert self._keysz == len(k)

    def __call__(self, v):
        v = v.lower()
        for s, m in self._d.items():
            if v[-self._keysz:] == s:
                return int(v[:-self._keysz]) * m
        return int(v) * self._default

byte_size = SuffixMultiplier({'kb': 1024,
                              'mb': 1024*1024,
                              'gb': 1024*1024*1024L,})

def url(value):
    # earlier Python 2.6 urlparse (2.6.4 and under) can't parse unix:// URLs,
    # later ones can but we need to straddle
    uri = value.replace('unix://', 'http://', 1).strip()
    scheme, netloc, path, params, query, fragment = urlparse.urlparse(uri)
    if scheme and (netloc or path):
        return value
    raise ValueError("value %s is not a URL" % value)

def signal_number(value):
    result = None
    try:
        result = int(value)
    except (ValueError, TypeError):
        result = getattr(signal, 'SIG'+value, None)
    try:
        result = int(result)
        return result
    except (ValueError, TypeError):
        raise ValueError('value %s is not a signal name/number' % value)

class RestartWhenExitUnexpected:
    pass

class RestartUnconditionally:
    pass

def auto_restart(value):
    value = str(value.lower())
    computed_value  = value
    if value in TRUTHY_STRINGS:
        computed_value = RestartUnconditionally
    elif value in FALSY_STRINGS:
        computed_value = False
    elif value == 'unexpected':
        computed_value = RestartWhenExitUnexpected
    if computed_value not in (RestartWhenExitUnexpected,
                              RestartUnconditionally, False):
        raise ValueError("invalid 'autorestart' value %r" % value)
    return computed_value

def profile_options(value):
    options = [x.lower() for x in list_of_strings(value) ]
    sort_options = []
    callers = False
    for thing in options:
        if thing != 'callers':
            sort_options.append(thing)
        else:
            callers = True
    return sort_options, callers

########NEW FILE########
__FILENAME__ = dispatchers
import errno
from supervisor.medusa.asyncore_25 import compact_traceback

from supervisor.events import notify
from supervisor.events import EventRejectedEvent
from supervisor.events import ProcessLogStderrEvent
from supervisor.events import ProcessLogStdoutEvent
from supervisor.states import EventListenerStates
from supervisor import loggers

def find_prefix_at_end(haystack, needle):
    l = len(needle) - 1
    while l and not haystack.endswith(needle[:l]):
        l -= 1
    return l

class PDispatcher:
    """ Asyncore dispatcher for mainloop, representing a process channel
    (stdin, stdout, or stderr).  This class is abstract. """

    closed = False # True if close() has been called

    def __repr__(self):
        return '<%s at %s for %s (%s)>' % (self.__class__.__name__,
                                           id(self),
                                           self.process,
                                           self.channel)

    def readable(self):
        raise NotImplementedError

    def writable(self):
        raise NotImplementedError

    def handle_read_event(self):
        raise NotImplementedError

    def handle_write_event(self):
        raise NotImplementedError

    def handle_error(self):
        nil, t, v, tbinfo = compact_traceback()

        self.process.config.options.logger.critical(
            'uncaptured python exception, closing channel %s (%s:%s %s)' % (
                repr(self),
                t,
                v,
                tbinfo
                )
            )
        self.close()

    def close(self):
        if not self.closed:
            self.process.config.options.logger.debug(
                'fd %s closed, stopped monitoring %s' % (self.fd, self))
            self.closed = True

    def flush(self):
        pass

class POutputDispatcher(PDispatcher):
    """ Output (stdout/stderr) dispatcher, capture output sent within
    <!--XSUPERVISOR:BEGIN--><!--XSUPERVISOR:END--> tags and notify
    with a ProcessCommunicationEvent """

    process = None # process which "owns" this dispatcher
    channel = None # 'stderr' or 'stdout'
    capturemode = False # are we capturing process event data
    mainlog = None #  the process' "normal" logger
    capturelog = None # the logger while we're in capturemode
    childlog = None # the current logger (event or main)
    output_buffer = '' # data waiting to be logged

    def __init__(self, process, event_type, fd):
        self.process = process
        self.event_type = event_type
        self.fd = fd
        self.channel = channel = self.event_type.channel

        logfile = getattr(process.config, '%s_logfile' % channel)
        capture_maxbytes = getattr(process.config,
                                   '%s_capture_maxbytes' % channel)

        if logfile:
            maxbytes = getattr(process.config, '%s_logfile_maxbytes' % channel)
            backups = getattr(process.config, '%s_logfile_backups' % channel)
            fmt = '%(message)s'
            if logfile == 'syslog':
                fmt = ' '.join((process.config.name, fmt))
            self.mainlog = process.config.options.getLogger(
                logfile,
                loggers.LevelsByName.INFO,
                fmt=fmt,
                rotating=not not maxbytes, # optimization
                maxbytes=maxbytes,
                backups=backups)

        if capture_maxbytes:
            self.capturelog = self.process.config.options.getLogger(
                None, # BoundIO
                loggers.LevelsByName.INFO,
                '%(message)s',
                rotating=False,
                maxbytes=capture_maxbytes,
                )

        self.childlog = self.mainlog

        # all code below is purely for minor speedups
        begintoken = self.event_type.BEGIN_TOKEN
        endtoken = self.event_type.END_TOKEN
        self.begintoken_data = (begintoken, len(begintoken))
        self.endtoken_data = (endtoken, len(endtoken))
        self.mainlog_level = loggers.LevelsByName.DEBG
        config = self.process.config
        self.log_to_mainlog = config.options.loglevel <= self.mainlog_level
        self.stdout_events_enabled = config.stdout_events_enabled
        self.stderr_events_enabled = config.stderr_events_enabled

    def removelogs(self):
        for log in (self.mainlog, self.capturelog):
            if log is not None:
                for handler in log.handlers:
                    handler.remove()
                    handler.reopen()

    def reopenlogs(self):
        for log in (self.mainlog, self.capturelog):
            if log is not None:
                for handler in log.handlers:
                    handler.reopen()

    def _log(self, data):
        if data:
            config = self.process.config
            if config.options.strip_ansi:
                data = stripEscapes(data)
            if self.childlog:
                self.childlog.info(data)
            if self.log_to_mainlog:
                msg = '%(name)r %(channel)s output:\n%(data)s'
                config.options.logger.log(
                    self.mainlog_level, msg, name=config.name,
                    channel=self.channel, data=data)
            if self.channel == 'stdout':
                if self.stdout_events_enabled:
                    notify(
                        ProcessLogStdoutEvent(self.process,
                            self.process.pid, data)
                    )
            else: # channel == stderr
                if self.stderr_events_enabled:
                    notify(
                        ProcessLogStderrEvent(self.process,
                            self.process.pid, data)
                    )

    def record_output(self):
        if self.capturelog is None:
            # shortcut trying to find capture data
            data = self.output_buffer
            self.output_buffer = ''
            self._log(data)
            return

        if self.capturemode:
            token, tokenlen = self.endtoken_data
        else:
            token, tokenlen = self.begintoken_data

        if len(self.output_buffer) <= tokenlen:
            return # not enough data

        data = self.output_buffer
        self.output_buffer = ''

        try:
            before, after = data.split(token, 1)
        except ValueError:
            after = None
            index = find_prefix_at_end(data, token)
            if index:
                self.output_buffer = self.output_buffer + data[-index:]
                data = data[:-index]
            self._log(data)
        else:
            self._log(before)
            self.toggle_capturemode()
            self.output_buffer = after

        if after:
            self.record_output()

    def toggle_capturemode(self):
        self.capturemode = not self.capturemode

        if self.capturelog is not None:
            if self.capturemode:
                self.childlog = self.capturelog
            else:
                for handler in self.capturelog.handlers:
                    handler.flush()
                data = self.capturelog.getvalue()
                channel = self.channel
                procname = self.process.config.name
                event = self.event_type(self.process, self.process.pid, data)
                notify(event)

                msg = "%(procname)r %(channel)s emitted a comm event"
                self.process.config.options.logger.debug(msg,
                                                         procname=procname,
                                                         channel=channel)
                for handler in self.capturelog.handlers:
                    handler.remove()
                    handler.reopen()
                self.childlog = self.mainlog

    def writable(self):
        return False

    def readable(self):
        if self.closed:
            return False
        return True

    def handle_read_event(self):
        data = self.process.config.options.readfd(self.fd)
        self.output_buffer += data
        self.record_output()
        if not data:
            # if we get no data back from the pipe, it means that the
            # child process has ended.  See
            # mail.python.org/pipermail/python-dev/2004-August/046850.html
            self.close()

class PEventListenerDispatcher(PDispatcher):
    """ An output dispatcher that monitors and changes a process'
    listener_state """
    process = None # process which "owns" this dispatcher
    channel = None # 'stderr' or 'stdout'
    childlog = None # the logger
    state_buffer = ''  # data waiting to be reviewed for state changes

    READY_FOR_EVENTS_TOKEN = 'READY\n'
    RESULT_TOKEN_START = 'RESULT '
    READY_FOR_EVENTS_LEN = len(READY_FOR_EVENTS_TOKEN)
    RESULT_TOKEN_START_LEN = len(RESULT_TOKEN_START)

    def __init__(self, process, channel, fd):
        self.process = process
        # the initial state of our listener is ACKNOWLEDGED; this is a
        # "busy" state that implies we're awaiting a READY_FOR_EVENTS_TOKEN
        self.process.listener_state = EventListenerStates.ACKNOWLEDGED
        self.process.event = None
        self.result = ''
        self.resultlen = None
        self.channel = channel
        self.fd = fd

        logfile = getattr(process.config, '%s_logfile' % channel)

        if logfile:
            maxbytes = getattr(process.config, '%s_logfile_maxbytes' % channel)
            backups = getattr(process.config, '%s_logfile_backups' % channel)
            self.childlog = process.config.options.getLogger(
                logfile,
                loggers.LevelsByName.INFO,
                '%(message)s',
                rotating=not not maxbytes, # optimization
                maxbytes=maxbytes,
                backups=backups)

    def removelogs(self):
        if self.childlog is not None:
            for handler in self.childlog.handlers:
                handler.remove()
                handler.reopen()

    def reopenlogs(self):
        if self.childlog is not None:
            for handler in self.childlog.handlers:
                handler.reopen()


    def writable(self):
        return False

    def readable(self):
        if self.closed:
            return False
        return True

    def handle_read_event(self):
        data = self.process.config.options.readfd(self.fd)
        if data:
            self.state_buffer += data
            procname = self.process.config.name
            msg = '%r %s output:\n%s' % (procname, self.channel, data)
            self.process.config.options.logger.debug(msg)

            if self.childlog:
                if self.process.config.options.strip_ansi:
                    data = stripEscapes(data)
                self.childlog.info(data)
        else:
            # if we get no data back from the pipe, it means that the
            # child process has ended.  See
            # mail.python.org/pipermail/python-dev/2004-August/046850.html
            self.close()

        self.handle_listener_state_change()

    def handle_listener_state_change(self):
        data = self.state_buffer

        if not data:
            return

        process = self.process
        procname = process.config.name
        state = process.listener_state

        if state == EventListenerStates.UNKNOWN:
            # this is a fatal state
            self.state_buffer = ''
            return

        if state == EventListenerStates.ACKNOWLEDGED:
            if len(data) < self.READY_FOR_EVENTS_LEN:
                # not enough info to make a decision
                return
            elif data.startswith(self.READY_FOR_EVENTS_TOKEN):
                msg = '%s: ACKNOWLEDGED -> READY' % procname
                process.config.options.logger.debug(msg)
                process.listener_state = EventListenerStates.READY
                tokenlen = self.READY_FOR_EVENTS_LEN
                self.state_buffer = self.state_buffer[tokenlen:]
                process.event = None
            else:
                msg = '%s: ACKNOWLEDGED -> UNKNOWN' % procname
                process.config.options.logger.debug(msg)
                process.listener_state = EventListenerStates.UNKNOWN
                self.state_buffer = ''
                process.event = None
            if self.state_buffer:
                # keep going til its too short
                self.handle_listener_state_change()
            else:
                return

        elif state == EventListenerStates.READY:
            # the process sent some spurious data, be a hardass about it
            msg = '%s: READY -> UNKNOWN' % procname
            process.config.options.logger.debug(msg)
            process.listener_state = EventListenerStates.UNKNOWN
            self.state_buffer = ''
            process.event = None
            return

        elif state == EventListenerStates.BUSY:
            if self.resultlen is None:
                # we haven't begun gathering result data yet
                pos = data.find('\n')
                if pos == -1:
                    # we can't make a determination yet, we dont have a full
                    # results line
                    return

                result_line = self.state_buffer[:pos]
                self.state_buffer = self.state_buffer[pos+1:] # rid LF
                resultlen = result_line[self.RESULT_TOKEN_START_LEN:]
                try:
                    self.resultlen = int(resultlen)
                except ValueError:
                    msg = ('%s: BUSY -> UNKNOWN (bad result line %r)'
                           % (procname, result_line))
                    process.config.options.logger.debug(msg)
                    process.listener_state = EventListenerStates.UNKNOWN
                    self.state_buffer = ''
                    notify(EventRejectedEvent(process, process.event))
                    process.event = None
                    return

            else:
                needed = self.resultlen - len(self.result)

                if needed:
                    self.result += self.state_buffer[:needed]
                    self.state_buffer = self.state_buffer[needed:]
                    needed = self.resultlen - len(self.result)

                if not needed:
                    self.handle_result(self.result)
                    self.process.event = None
                    self.result = ''
                    self.resultlen = None

            if self.state_buffer:
                # keep going til its too short
                self.handle_listener_state_change()
            else:
                return

    def handle_result(self, result):
        process = self.process
        procname = process.config.name

        try:
            self.process.group.config.result_handler(process.event, result)
            msg = '%s: BUSY -> ACKNOWLEDGED (processed)' % procname
            process.listener_state = EventListenerStates.ACKNOWLEDGED
        except RejectEvent:
            msg = '%s: BUSY -> ACKNOWLEDGED (rejected)' % procname
            process.listener_state = EventListenerStates.ACKNOWLEDGED
            notify(EventRejectedEvent(process, process.event))
        except:
            msg = '%s: BUSY -> UNKNOWN' % procname
            process.listener_state = EventListenerStates.UNKNOWN
            notify(EventRejectedEvent(process, process.event))

        process.config.options.logger.debug(msg)

class PInputDispatcher(PDispatcher):
    """ Input (stdin) dispatcher """
    process = None # process which "owns" this dispatcher
    channel = None # 'stdin'
    input_buffer = '' # data waiting to be sent to the child process

    def __init__(self, process, channel, fd):
        self.process = process
        self.channel = channel
        self.fd = fd
        self.input_buffer = ''

    def writable(self):
        if self.input_buffer and not self.closed:
            return True
        return False

    def readable(self):
        return False

    def flush(self):
        # other code depends on this raising EPIPE if the pipe is closed
        sent = self.process.config.options.write(self.fd,
                                                 self.input_buffer)
        self.input_buffer = self.input_buffer[sent:]

    def handle_write_event(self):
        if self.input_buffer:
            try:
                self.flush()
            except OSError, why:
                if why[0] == errno.EPIPE:
                    self.input_buffer = ''
                    self.close()
                else:
                    raise

ANSI_ESCAPE_BEGIN = '\x1b['
ANSI_TERMINATORS = ('H', 'f', 'A', 'B', 'C', 'D', 'R', 's', 'u', 'J',
                    'K', 'h', 'l', 'p', 'm')

def stripEscapes(string):
    """
    Remove all ANSI color escapes from the given string.
    """
    result = ''
    show = 1
    i = 0
    L = len(string)
    while i < L:
        if show == 0 and string[i] in ANSI_TERMINATORS:
            show = 1
        elif show:
            n = string.find(ANSI_ESCAPE_BEGIN, i)
            if n == -1:
                return result + string[i:]
            else:
                result = result + string[i:n]
                i = n
                show = 0
        i = i + 1
    return result

class RejectEvent(Exception):
    """ The exception type expected by a dispatcher when a handler wants
    to reject an event """

def default_handler(event, response):
    if response != 'OK':
        raise RejectEvent(response)

########NEW FILE########
__FILENAME__ = events
from supervisor.states import getProcessStateDescription

callbacks = []

def subscribe(type, callback):
    callbacks.append((type, callback))
    
def notify(event):
    for type, callback in callbacks:
        if isinstance(event, type):
            callback(event)

def clear():
    callbacks[:] = []

class Event:
    """ Abstract event type """
    pass

class ProcessLogEvent(Event):
    """ Abstract """
    def __init__(self, process, pid, data):
        self.process = process
        self.pid = pid
        self.data = data

    def __str__(self):
        groupname = ''
        if self.process.group is not None:
            groupname = self.process.group.config.name
        return 'processname:%s groupname:%s pid:%s channel:%s\n%s' % (
            self.process.config.name,
            groupname,
            self.pid,
            self.channel,
            self.data)

class ProcessLogStdoutEvent(ProcessLogEvent):
    channel = 'stdout'

class ProcessLogStderrEvent(ProcessLogEvent):
    channel = 'stderr'

class ProcessCommunicationEvent(Event):
    """ Abstract """
    # event mode tokens
    BEGIN_TOKEN = '<!--XSUPERVISOR:BEGIN-->'
    END_TOKEN   = '<!--XSUPERVISOR:END-->'

    def __init__(self, process, pid, data):
        self.process = process
        self.pid = pid
        self.data = data

    def __str__(self):
        groupname = ''
        if self.process.group is not None:
            groupname = self.process.group.config.name
        return 'processname:%s groupname:%s pid:%s\n%s' % (
            self.process.config.name,
            groupname,
            self.pid,
            self.data)

class ProcessCommunicationStdoutEvent(ProcessCommunicationEvent):
    channel = 'stdout'

class ProcessCommunicationStderrEvent(ProcessCommunicationEvent):
    channel = 'stderr'

class RemoteCommunicationEvent(Event):
    def __init__(self, type, data):
        self.type = type
        self.data = data

    def __str__(self):
        return 'type:%s\n%s' % (self.type, self.data)

class SupervisorStateChangeEvent(Event):
    """ Abstract class """
    def __str__(self):
        return ''

class SupervisorRunningEvent(SupervisorStateChangeEvent):
    pass

class SupervisorStoppingEvent(SupervisorStateChangeEvent):
    pass

class EventRejectedEvent: # purposely does not subclass Event 
    def __init__(self, process, event):
        self.process = process
        self.event = event

class ProcessStateEvent(Event):
    """ Abstract class, never raised directly """
    frm = None
    to = None
    def __init__(self, process, from_state, expected=True):
        self.process = process
        self.from_state = from_state
        self.expected = expected
        # we eagerly render these so if the process pid, etc changes beneath
        # us, we stash the values at the time the event was sent
        self.extra_values = self.get_extra_values()

    def __str__(self):
        groupname = ''
        if self.process.group is not None:
            groupname = self.process.group.config.name
        L = []
        L.append(('processname',  self.process.config.name))
        L.append(('groupname', groupname))
        L.append(('from_state', getProcessStateDescription(self.from_state)))
        L.extend(self.extra_values)
        s = ' '.join( [ '%s:%s' % (name, val) for (name, val) in L ] )
        return s

    def get_extra_values(self):
        return []

class ProcessStateFatalEvent(ProcessStateEvent):
    pass

class ProcessStateUnknownEvent(ProcessStateEvent):
    pass

class ProcessStateStartingOrBackoffEvent(ProcessStateEvent):
    def get_extra_values(self):
        return [('tries', int(self.process.backoff))]

class ProcessStateBackoffEvent(ProcessStateStartingOrBackoffEvent):
    pass

class ProcessStateStartingEvent(ProcessStateStartingOrBackoffEvent):
    pass

class ProcessStateExitedEvent(ProcessStateEvent):
    def get_extra_values(self):
        return [('expected', int(self.expected)), ('pid', self.process.pid)]

class ProcessStateRunningEvent(ProcessStateEvent):
    def get_extra_values(self):
        return [('pid', self.process.pid)]

class ProcessStateStoppingEvent(ProcessStateEvent):
    def get_extra_values(self):
        return [('pid', self.process.pid)]

class ProcessStateStoppedEvent(ProcessStateEvent):
    def get_extra_values(self):
        return [('pid', self.process.pid)]

class TickEvent(Event):
    """ Abstract """
    def __init__(self, when, supervisord):
        self.when = when
        self.supervisord = supervisord

    def __str__(self):
        return 'when:%s' % self.when

class Tick5Event(TickEvent):
    period = 5

class Tick60Event(TickEvent):
    period = 60

class Tick3600Event(TickEvent):
    period = 3600

TICK_EVENTS = [ Tick5Event, Tick60Event, Tick3600Event ] # imported elsewhere

class EventTypes:
    EVENT = Event # abstract
    PROCESS_STATE = ProcessStateEvent # abstract
    PROCESS_STATE_STOPPED = ProcessStateStoppedEvent
    PROCESS_STATE_EXITED = ProcessStateExitedEvent
    PROCESS_STATE_STARTING = ProcessStateStartingEvent
    PROCESS_STATE_STOPPING = ProcessStateStoppingEvent
    PROCESS_STATE_BACKOFF = ProcessStateBackoffEvent
    PROCESS_STATE_FATAL = ProcessStateFatalEvent
    PROCESS_STATE_RUNNING = ProcessStateRunningEvent
    PROCESS_STATE_UNKNOWN = ProcessStateUnknownEvent
    PROCESS_COMMUNICATION = ProcessCommunicationEvent # abstract
    PROCESS_COMMUNICATION_STDOUT = ProcessCommunicationStdoutEvent
    PROCESS_COMMUNICATION_STDERR = ProcessCommunicationStderrEvent
    PROCESS_LOG = ProcessLogEvent
    PROCESS_LOG_STDOUT = ProcessLogStdoutEvent
    PROCESS_LOG_STDERR = ProcessLogStderrEvent     
    REMOTE_COMMUNICATION = RemoteCommunicationEvent
    SUPERVISOR_STATE_CHANGE = SupervisorStateChangeEvent # abstract
    SUPERVISOR_STATE_CHANGE_RUNNING = SupervisorRunningEvent
    SUPERVISOR_STATE_CHANGE_STOPPING = SupervisorStoppingEvent
    TICK = TickEvent # abstract
    TICK_5 = Tick5Event
    TICK_60 = Tick60Event
    TICK_3600 = Tick3600Event

def getEventNameByType(requested):
    for name, typ in EventTypes.__dict__.items():
        if typ is requested:
            return name

def register(name, event):
    EventTypes.__dict__[name] = event

########NEW FILE########
__FILENAME__ = http
import os
import stat
import time
import sys
import socket
import errno
import pwd
import urllib

try:
    from hashlib import sha1
except ImportError:
    from sha import new as sha1

from supervisor.medusa import asyncore_25 as asyncore
from supervisor.medusa import http_date
from supervisor.medusa import http_server
from supervisor.medusa import producers
from supervisor.medusa import filesys
from supervisor.medusa import default_handler

from supervisor.medusa.auth_handler import auth_handler

class NOT_DONE_YET:
    pass

class deferring_chunked_producer:
    """A producer that implements the 'chunked' transfer coding for HTTP/1.1.
    Here is a sample usage:
            request['Transfer-Encoding'] = 'chunked'
            request.push (
                    producers.chunked_producer (your_producer)
                    )
            request.done()
    """

    def __init__ (self, producer, footers=None):
        self.producer = producer
        self.footers = footers
        self.delay = 0.1

    def more (self):
        if self.producer:
            data = self.producer.more()
            if data is NOT_DONE_YET:
                return NOT_DONE_YET
            elif data:
                return '%x\r\n%s\r\n' % (len(data), data)
            else:
                self.producer = None
                if self.footers:
                    return '\r\n'.join(['0'] + self.footers) + '\r\n\r\n'
                else:
                    return '0\r\n\r\n'
        else:
            return ''

class deferring_composite_producer:
    "combine a fifo of producers into one"
    def __init__ (self, producers):
        self.producers = producers
        self.delay = 0.1

    def more (self):
        while len(self.producers):
            p = self.producers[0]
            d = p.more()
            if d is NOT_DONE_YET:
                return NOT_DONE_YET
            if d:
                return d
            else:
                self.producers.pop(0)
        else:
            return ''


class deferring_globbing_producer:
    """
    'glob' the output from a producer into a particular buffer size.
    helps reduce the number of calls to send().  [this appears to
    gain about 30% performance on requests to a single channel]
    """

    def __init__ (self, producer, buffer_size=1<<16):
        self.producer = producer
        self.buffer = ''
        self.buffer_size = buffer_size
        self.delay = 0.1

    def more (self):
        while len(self.buffer) < self.buffer_size:
            data = self.producer.more()
            if data is NOT_DONE_YET:
                return NOT_DONE_YET
            if data:
                self.buffer = self.buffer + data
            else:
                break
        r = self.buffer
        self.buffer = ''
        return r


class deferring_hooked_producer:
    """
    A producer that will call <function> when it empties,.
    with an argument of the number of bytes produced.  Useful
    for logging/instrumentation purposes.
    """

    def __init__ (self, producer, function):
        self.producer = producer
        self.function = function
        self.bytes = 0
        self.delay = 0.1

    def more (self):
        if self.producer:
            result = self.producer.more()
            if result is NOT_DONE_YET:
                return NOT_DONE_YET
            if not result:
                self.producer = None
                self.function (self.bytes)
            else:
                self.bytes = self.bytes + len(result)
            return result
        else:
            return ''


class deferring_http_request(http_server.http_request):
    """ The medusa http_request class uses the default set of producers in
    medusa.prodcers.  We can't use these because they don't know anything about
    deferred responses, so we override various methods here.  This was added
    to support tail -f like behavior on the logtail handler """

    def get_header(self, header):
        # this is overridden purely for speed (the base class doesn't
        # use string methods
        header = header.lower()
        hc = self._header_cache
        if not hc.has_key(header):
            h = header + ': '
            for line in self.header:
                if line.lower().startswith(h):
                    hl = len(h)
                    r = line[hl:]
                    hc[header] = r
                    return r
            hc[header] = None
            return None
        else:
            return hc[header]

    def done(self, *arg, **kw):

        """ I didn't want to override this, but there's no way around
        it in order to support deferreds - CM

        finalize this transaction - send output to the http channel"""

        # ----------------------------------------
        # persistent connection management
        # ----------------------------------------

        #  --- BUCKLE UP! ----

        connection = http_server.get_header(http_server.CONNECTION,self.header)
        connection = connection.lower()

        close_it = 0
        wrap_in_chunking = 0
        globbing = 1

        if self.version == '1.0':
            if connection == 'keep-alive':
                if not self.has_key ('Content-Length'):
                    close_it = 1
                else:
                    self['Connection'] = 'Keep-Alive'
            else:
                close_it = 1
        elif self.version == '1.1':
            if connection == 'close':
                close_it = 1
            elif not self.has_key('Content-Length'):
                if self.has_key('Transfer-Encoding'):
                    if not self['Transfer-Encoding'] == 'chunked':
                        close_it = 1
                elif self.use_chunked:
                    self['Transfer-Encoding'] = 'chunked'
                    wrap_in_chunking = 1
                    # globbing slows down tail -f output, so only use it if
                    # we're not in chunked mode
                    globbing = 0
                else:
                    close_it = 1
        elif self.version is None:
            # Although we don't *really* support http/0.9 (because
            # we'd have to use \r\n as a terminator, and it would just
            # yuck up a lot of stuff) it's very common for developers
            # to not want to type a version number when using telnet
            # to debug a server.
            close_it = 1

        outgoing_header = producers.simple_producer(self.build_reply_header())

        if close_it:
            self['Connection'] = 'close'

        if wrap_in_chunking:
            outgoing_producer = deferring_chunked_producer(
                    deferring_composite_producer(self.outgoing)
                    )
            # prepend the header
            outgoing_producer = deferring_composite_producer(
                [outgoing_header, outgoing_producer]
                )
        else:
            # prepend the header
            self.outgoing.insert(0, outgoing_header)
            outgoing_producer = deferring_composite_producer(self.outgoing)

        # hook logging into the output
        outgoing_producer = deferring_hooked_producer(outgoing_producer,
                                                      self.log)

        if globbing:
            outgoing_producer = deferring_globbing_producer(outgoing_producer)

        self.channel.push_with_producer(outgoing_producer)

        self.channel.current_request = None

        if close_it:
            self.channel.close_when_done()

    def log (self, bytes):
        """ We need to override this because UNIX domain sockets return
        an empty string for the addr rather than a (host, port) combination """
        if self.channel.addr:
            host = self.channel.addr[0]
            port = self.channel.addr[1]
        else:
            host = 'localhost'
            port = 0
        self.channel.server.logger.log (
                host,
                '%d - - [%s] "%s" %d %d\n' % (
                        port,
                        self.log_date_string (time.time()),
                        self.request,
                        self.reply_code,
                        bytes
                        )
                )

    def cgi_environment(self):
        env = {}

        # maps request some headers to environment variables.
        # (those that don't start with 'HTTP_')
        header2env= {'content-length'    : 'CONTENT_LENGTH',
                     'content-type'      : 'CONTENT_TYPE',
                     'connection'        : 'CONNECTION_TYPE'}

        workdir = os.getcwd()
        (path, params, query, fragment) = self.split_uri()

        if params:
            path = path + params # undo medusa bug!

        while path and path[0] == '/':
            path = path[1:]
        if '%' in path:
            path = http_server.unquote(path)
        if query:
            query = query[1:]

        server = self.channel.server
        env['REQUEST_METHOD'] = self.command.upper()
        env['SERVER_PORT'] = str(server.port)
        env['SERVER_NAME'] = server.server_name
        env['SERVER_SOFTWARE'] = server.SERVER_IDENT
        env['SERVER_PROTOCOL'] = "HTTP/" + self.version
        env['channel.creation_time'] = self.channel.creation_time
        env['SCRIPT_NAME'] = ''
        env['PATH_INFO'] = '/' + path
        env['PATH_TRANSLATED'] = os.path.normpath(os.path.join(
                workdir, env['PATH_INFO']))
        if query:
            env['QUERY_STRING'] = query
        env['GATEWAY_INTERFACE'] = 'CGI/1.1'
        if self.channel.addr:
            env['REMOTE_ADDR'] = self.channel.addr[0]
        else:
            env['REMOTE_ADDR'] = '127.0.0.1'

        for header in self.header:
            key,value=header.split(":",1)
            key=key.lower()
            value=value.strip()
            if header2env.has_key(key) and value:
                env[header2env.get(key)]=value
            else:
                key='HTTP_%s' % ("_".join(key.split( "-"))).upper()
                if value and not env.has_key(key):
                    env[key]=value
        return env

    def get_server_url(self):
        """ Functionality that medusa's http request doesn't have; set an
        attribute named 'server_url' on the request based on the Host: header
        """
        default_port={'http': '80', 'https': '443'}
        environ = self.cgi_environment()
        if (environ.get('HTTPS') in ('on', 'ON') or
            environ.get('SERVER_PORT_SECURE') == "1"):
            # XXX this will currently never be true
            protocol = 'https'
        else:
            protocol = 'http'

        if environ.has_key('HTTP_HOST'):
            host = environ['HTTP_HOST'].strip()
            hostname, port = urllib.splitport(host)
        else:
            hostname = environ['SERVER_NAME'].strip()
            port = environ['SERVER_PORT']

        if (port is None or default_port[protocol] == port):
            host = hostname
        else:
            host = hostname + ':' + port
        server_url = '%s://%s' % (protocol, host)
        if server_url[-1:]=='/':
            server_url=server_url[:-1]
        return server_url

class deferring_http_channel(http_server.http_channel):

    # use a 4096-byte buffer size instead of the default 65536-byte buffer in
    # order to spew tail -f output faster (speculative)
    ac_out_buffer_size = 4096

    delay = False
    writable_check = time.time()

    def writable(self, t=time.time):
        now = t()
        if self.delay:
            # we called a deferred producer via this channel (see refill_buffer)
            last_writable_check = self.writable_check
            self.writable_check = now
            elapsed = now - last_writable_check
            if elapsed > self.delay:
                return True
            else:
                return False

        return http_server.http_channel.writable(self)

    def refill_buffer (self):
        """ Implement deferreds """
        while 1:
            if len(self.producer_fifo):
                p = self.producer_fifo.first()
                # a 'None' in the producer fifo is a sentinel,
                # telling us to close the channel.
                if p is None:
                    if not self.ac_out_buffer:
                        self.producer_fifo.pop()
                        self.close()
                    return
                elif isinstance(p, str):
                    self.producer_fifo.pop()
                    self.ac_out_buffer = self.ac_out_buffer + p
                    return

                data = p.more()

                if data is NOT_DONE_YET:
                    self.delay = p.delay
                    return

                elif data:
                    self.ac_out_buffer = self.ac_out_buffer + data
                    self.delay = False
                    return
                else:
                    self.producer_fifo.pop()
            else:
                return

    def found_terminator (self):
        """ We only override this to use 'deferring_http_request' class
        instead of the normal http_request class; it sucks to need to override
        this """
        if self.current_request:
            self.current_request.found_terminator()
        else:
            header = self.in_buffer
            self.in_buffer = ''
            lines = header.split('\r\n')

            # --------------------------------------------------
            # crack the request header
            # --------------------------------------------------

            while lines and not lines[0]:
                # as per the suggestion of http-1.1 section 4.1, (and
                # Eric Parker <eparker@zyvex.com>), ignore a leading
                # blank lines (buggy browsers tack it onto the end of
                # POST requests)
                lines = lines[1:]

            if not lines:
                self.close_when_done()
                return

            request = lines[0]

            command, uri, version = http_server.crack_request (request)
            header = http_server.join_headers (lines[1:])

            # unquote path if necessary (thanks to Skip Montanaro for pointing
            # out that we must unquote in piecemeal fashion).
            rpath, rquery = http_server.splitquery(uri)
            if '%' in rpath:
                if rquery:
                    uri = http_server.unquote (rpath) + '?' + rquery
                else:
                    uri = http_server.unquote (rpath)

            r = deferring_http_request (self, request, command, uri, version,
                                         header)
            self.request_counter.increment()
            self.server.total_requests.increment()

            if command is None:
                self.log_info ('Bad HTTP request: %s' % repr(request), 'error')
                r.error (400)
                return

            # --------------------------------------------------
            # handler selection and dispatch
            # --------------------------------------------------
            for h in self.server.handlers:
                if h.match (r):
                    try:
                        self.current_request = r
                        # This isn't used anywhere.
                        # r.handler = h # CYCLE
                        h.handle_request (r)
                    except:
                        self.server.exceptions.increment()
                        (file, fun, line), t, v, tbinfo = \
                               asyncore.compact_traceback()
                        self.server.log_info(
                            'Server Error: %s, %s: file: %s line: %s' %
                            (t,v,file,line),
                            'error')
                        try:
                            r.error (500)
                        except:
                            pass
                    return

            # no handlers, so complain
            r.error (404)

class supervisor_http_server(http_server.http_server):
    channel_class = deferring_http_channel
    ip = None

    def prebind(self, sock, logger_object):
        """ Override __init__ to do logger setup earlier so it can
        go to our logger object instead of stdout """
        from supervisor.medusa import logger

        if not logger_object:
            logger_object = logger.file_logger(sys.stdout)

        logger_object = logger.unresolving_logger(logger_object)
        self.logger = logger_object

        asyncore.dispatcher.__init__ (self)
        self.set_socket(sock)

        self.handlers = []

        sock.setblocking(0)
        self.set_reuse_addr()

    def postbind(self):
        from supervisor.medusa.counter import counter
        from supervisor.medusa.http_server import VERSION_STRING

        self.listen(1024)

        self.total_clients = counter()
        self.total_requests = counter()
        self.exceptions = counter()
        self.bytes_out = counter()
        self.bytes_in  = counter()

        self.log_info (
                'Medusa (V%s) started at %s'
                '\n\tHostname: %s'
                '\n\tPort:%s'
                '\n' % (
                        VERSION_STRING,
                        time.ctime(time.time()),
                        self.server_name,
                        self.port,
                        )
                )

    def log_info(self, message, type='info'):
        ip = ''
        if getattr(self, 'ip', None) is not None:
            ip = self.ip
        self.logger.log(ip, message)

class supervisor_af_inet_http_server(supervisor_http_server):
    """ AF_INET version of supervisor HTTP server """

    def __init__(self, ip, port, logger_object):
        self.ip = ip
        self.port = port
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.prebind(sock, logger_object)
        self.bind((ip, port))

        host, port = self.socket.getsockname()
        if not ip:
            self.log_info('Computing default hostname', 'warning')
            hostname = socket.gethostname()
            try:
                ip = socket.gethostbyname(hostname)
            except socket.error:
                raise ValueError(
                    'Could not determine IP address for hostname %s, '
                    'please try setting an explicit IP address in the "port" '
                    'setting of your [inet_http_server] section.  For example, '
                    'instead of "port = 9001", try "port = 127.0.0.1:9001."'
                    % hostname)
        try:
            self.server_name = socket.gethostbyaddr (ip)[0]
        except socket.error:
            self.log_info('Cannot do reverse lookup', 'warning')
            self.server_name = ip       # use the IP address as the "hostname"

        self.postbind()

class supervisor_af_unix_http_server(supervisor_http_server):
    """ AF_UNIX version of supervisor HTTP server """

    def __init__(self, socketname, sockchmod, sockchown, logger_object):
        self.ip = socketname
        self.port = socketname

        # XXX this is insecure.  We really should do something like
        # http://developer.apple.com/samplecode/CFLocalServer/listing6.html
        # (see also http://developer.apple.com/technotes/tn2005/tn2083.html#SECUNIXDOMAINSOCKETS)
        # but it would be very inconvenient for the user to need to get all
        # the directory setup right.

        tempname = "%s.%d" % (socketname, os.getpid())

        try:
            os.unlink(tempname)
        except OSError:
            pass

        while 1:
            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
            try:
                sock.bind(tempname)
                os.chmod(tempname, sockchmod)
                try:
                    # hard link
                    os.link(tempname, socketname)
                except OSError:
                    # Lock contention, or stale socket.
                    used = self.checkused(socketname)
                    if used:
                        # cooperate with 'openhttpserver' in supervisord
                        raise socket.error(errno.EADDRINUSE)

                    # Stale socket -- delete, sleep, and try again.
                    msg = "Unlinking stale socket %s\n" % socketname
                    sys.stderr.write(msg)
                    try:
                        os.unlink(socketname)
                    except:
                        pass
                    sock.close()
                    time.sleep(.3)
                    continue
                else:
                    try:
                        os.chown(socketname, sockchown[0], sockchown[1])
                    except OSError, why:
                        if why[0] == errno.EPERM:
                            msg = ('Not permitted to chown %s to uid/gid %s; '
                                   'adjust "sockchown" value in config file or '
                                   'on command line to values that the '
                                   'current user (%s) can successfully chown')
                            raise ValueError(msg % (socketname,
                                                    repr(sockchown),
                                                    pwd.getpwuid(
                                                        os.geteuid())[0],
                                                    ),
                                             )
                        else:
                            raise
                    self.prebind(sock, logger_object)
                    break

            finally:
                try:
                    os.unlink(tempname)
                except OSError:
                    pass

        self.server_name = '<unix domain socket>'
        self.postbind()

    def checkused(self, socketname):
        s = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        try:
            s.connect(socketname)
            s.send("GET / HTTP/1.0\r\n\r\n")
            data = s.recv(1)
            s.close()
        except socket.error:
            return False
        else:
            return True

class tail_f_producer:
    def __init__(self, request, filename, head):
        self.file = open(filename, 'rb')
        self.request = request
        self.delay = 0.1
        sz = self.fsize()
        if sz >= head:
            self.sz = sz - head
        else:
            self.sz = 0

    def more(self):
        try:
            newsz = self.fsize()
        except OSError:
            # file descriptor was closed
            return ''
        bytes_added = newsz - self.sz
        if bytes_added < 0:
            self.sz = 0
            return "==> File truncated <==\n"
        if bytes_added > 0:
            self.file.seek(-bytes_added, 2)
            bytes = self.file.read(bytes_added)
            self.sz = newsz
            return bytes
        return NOT_DONE_YET

    def fsize(self):
        return os.fstat(self.file.fileno())[stat.ST_SIZE]

class logtail_handler:
    IDENT = 'Logtail HTTP Request Handler'
    path = '/logtail'

    def __init__(self, supervisord):
        self.supervisord = supervisord

    def match(self, request):
        return request.uri.startswith(self.path)

    def handle_request(self, request):
        if request.command != 'GET':
            request.error (400) # bad request
            return

        path, params, query, fragment = request.split_uri()

        if '%' in path:
            path = http_server.unquote(path)

        # strip off all leading slashes
        while path and path[0] == '/':
            path = path[1:]

        path, process_name_and_channel = path.split('/', 1)

        try:
            process_name, channel = process_name_and_channel.split('/', 1)
        except ValueError:
            # no channel specified, default channel to stdout
            process_name = process_name_and_channel
            channel = 'stdout'

        from supervisor.options import split_namespec
        group_name, process_name = split_namespec(process_name)

        group = self.supervisord.process_groups.get(group_name)
        if group is None:
            request.error(404) # not found
            return

        process = group.processes.get(process_name)
        if process is None:
            request.error(404) # not found
            return

        logfile = getattr(process.config, '%s_logfile' % channel, None)

        if logfile is None or not os.path.exists(logfile):
            # XXX problematic: processes that don't start won't have a log
            # file and we probably don't want to go into fatal state if we try
            # to read the log of a process that did not start.
            request.error(410) # gone
            return

        mtime = os.stat(logfile)[stat.ST_MTIME]
        request['Last-Modified'] = http_date.build_http_date(mtime)
        request['Content-Type'] = 'text/plain'
        # the lack of a Content-Length header makes the outputter
        # send a 'Transfer-Encoding: chunked' response

        request.push(tail_f_producer(request, logfile, 1024))

        request.done()

class mainlogtail_handler:
    IDENT = 'Main Logtail HTTP Request Handler'
    path = '/mainlogtail'

    def __init__(self, supervisord):
        self.supervisord = supervisord

    def match(self, request):
        return request.uri.startswith(self.path)

    def handle_request(self, request):
        if request.command != 'GET':
            request.error (400) # bad request
            return

        logfile = self.supervisord.options.logfile

        if logfile is None or not os.path.exists(logfile):
            request.error(410) # gone
            return

        mtime = os.stat(logfile)[stat.ST_MTIME]
        request['Last-Modified'] = http_date.build_http_date(mtime)
        request['Content-Type'] = 'text/plain'
        # the lack of a Content-Length header makes the outputter
        # send a 'Transfer-Encoding: chunked' response

        request.push(tail_f_producer(request, logfile, 1024))

        request.done()

def make_http_servers(options, supervisord):
    servers = []
    class LogWrapper:
        def log(self, msg):
            if msg.endswith('\n'):
                msg = msg[:-1]
            options.logger.trace(msg)
    wrapper = LogWrapper()

    for config in options.server_configs:
        family = config['family']

        if family == socket.AF_INET:
            host, port = config['host'], config['port']
            hs = supervisor_af_inet_http_server(host, port,
                                                logger_object=wrapper)
        elif family == socket.AF_UNIX:
            socketname = config['file']
            sockchmod = config['chmod']
            sockchown = config['chown']
            hs = supervisor_af_unix_http_server(socketname,sockchmod, sockchown,
                                                logger_object=wrapper)
        else:
            raise ValueError('Cannot determine socket type %r' % family)

        from xmlrpc import supervisor_xmlrpc_handler
        from xmlrpc import SystemNamespaceRPCInterface
        from web import supervisor_ui_handler

        subinterfaces = []
        for name, factory, d in options.rpcinterface_factories:
            try:
                inst = factory(supervisord, **d)
            except:
                import traceback; traceback.print_exc()
                raise ValueError('Could not make %s rpc interface' % name)
            subinterfaces.append((name, inst))
            options.logger.info('RPC interface %r initialized' % name)

        subinterfaces.append(('system',
                              SystemNamespaceRPCInterface(subinterfaces)))
        xmlrpchandler = supervisor_xmlrpc_handler(supervisord, subinterfaces)
        tailhandler = logtail_handler(supervisord)
        maintailhandler = mainlogtail_handler(supervisord)
        uihandler = supervisor_ui_handler(supervisord)
        here = os.path.abspath(os.path.dirname(__file__))
        templatedir = os.path.join(here, 'ui')
        filesystem = filesys.os_filesystem(templatedir)
        defaulthandler = default_handler.default_handler(filesystem)

        username = config['username']
        password = config['password']

        if username:
            # wrap the xmlrpc handler and tailhandler in an authentication
            # handler
            users = {username:password}
            xmlrpchandler = supervisor_auth_handler(users, xmlrpchandler)
            tailhandler = supervisor_auth_handler(users, tailhandler)
            maintailhandler = supervisor_auth_handler(users, maintailhandler)
            uihandler = supervisor_auth_handler(users, uihandler)
            defaulthandler = supervisor_auth_handler(users, defaulthandler)
        else:
            options.logger.critical(
                'Server %r running without any HTTP '
                'authentication checking' % config['section'])
        # defaulthandler must be consulted last as its match method matches
        # everything, so it's first here (indicating last checked)
        hs.install_handler(defaulthandler)
        hs.install_handler(uihandler)
        hs.install_handler(maintailhandler)
        hs.install_handler(tailhandler)
        hs.install_handler(xmlrpchandler) # last for speed (first checked)
        servers.append((config, hs))

    return servers

class encrypted_dictionary_authorizer:
    def __init__ (self, dict):
        self.dict = dict

    def authorize(self, auth_info):
        username, password = auth_info
        if self.dict.has_key(username):
            stored_password = self.dict[username]
            if stored_password.startswith('{SHA}'):
                password_hash = sha1(password).hexdigest()
                return stored_password[5:] == password_hash
            else:
                return stored_password == password
        else:
            return False

class supervisor_auth_handler(auth_handler):
    def __init__(self, dict, handler, realm='default'):
        auth_handler.__init__(self, dict, handler, realm)
        # override the authorizer with one that knows about SHA hashes too
        self.authorizer = encrypted_dictionary_authorizer(dict)


########NEW FILE########
__FILENAME__ = http_client
# this code based on Daniel Krech's RDFLib HTTP client code (see rdflib.net)

import sys
import socket
import base64
from urlparse import urlparse

from supervisor.medusa import asyncore_25 as aysncore
from supervisor.medusa import asynchat_25 as asynchat

CR="\x0d"
LF="\x0a"
CRLF=CR+LF

class Listener(object):

    def status(self, url, status):
        pass

    def error(self, url, error):
        print url, error
    
    def response_header(self, url, name, value):
        pass
    
    def done(self, url):
        pass

    def feed(self, url, data):
        sys.stdout.write(data)
        sys.stdout.flush()

    def close(self, url):
        pass

class HTTPHandler(object, asynchat.async_chat):
    def __init__(self, listener, username='', password=None):
        super(HTTPHandler, self).__init__()
        asynchat.async_chat.__init__(self)
        self.listener = listener
        self.user_agent = 'Supervisor HTTP Client'
        self.buffer = ''
        self.set_terminator(CRLF)
        self.connected = 0
        self.part = self.status_line
        self.chunk_size = 0
        self.chunk_read = 0
        self.length_read = 0        
        self.length = 0
        self.encoding = None
        self.username = username
        self.password = password
        self.url = None
        self.error_handled = False

    def get(self, serverurl, path):
        if self.url != None:
            raise AssertionError('Already doing a get')
        self.url = serverurl + path
        scheme, host, path_ignored, params, query, fragment = urlparse(self.url)
        if not scheme in ("http", "unix"):
            raise NotImplementedError
        self.host = host
        if ":" in host:
            hostname, port = host.split(":", 1)
            port = int(port)
        else:
            hostname = host
            port = 80

        self.path = path
        self.port = port

        if scheme == "http":
            ip = hostname
            self.create_socket(socket.AF_INET, socket.SOCK_STREAM)
            self.connect((ip, self.port))
        elif scheme == "unix":
            socketname = serverurl[7:]
            self.create_socket(socket.AF_UNIX, socket.SOCK_STREAM)
            self.connect(socketname)
    
    def close (self):
        self.listener.close(self.url)
        self.connected = 0
        self.del_channel()
        self.socket.close()
        self.url = "CLOSED"

    def header(self, name, value):
        self.push('%s: %s' % (name, value))
        self.push(CRLF)
        
    def handle_error (self):
        if self.error_handled == True:
            return
        if 1 or self.connected:
            t,v,tb = sys.exc_info()
            msg = 'Cannot connect, error: %s (%s)' % (t, v)
            self.listener.error(self.url, msg)
            self.part = self.ignore                
            self.close()
            self.error_handled = True
            del t
            del v
            del tb
        
    def handle_connect(self):
        self.connected = 1        
        method = "GET"
        version = "HTTP/1.1"
        self.push("%s %s %s" % (method, self.path, version))
        self.push(CRLF)
        self.header("Host", self.host)

        self.header('Accept-Encoding', 'chunked')
        self.header('Accept', '*/*')
        self.header('User-agent', self.user_agent)
        if self.password:
            auth = '%s:%s' % (self.username, self.password)
            auth = base64.encodestring(auth).strip()
            self.header('Authorization', 'Basic %s' % auth)
        self.push(CRLF)
        self.push(CRLF)


    def feed(self, data):
        self.listener.feed(self.url, data)
        
    def collect_incoming_data(self, bytes):
        self.buffer = self.buffer + bytes
        if self.part==self.body:
            self.feed(self.buffer)
            self.buffer = ''

    def found_terminator(self):
        self.part()
        self.buffer = ''        

    def ignore(self):
        self.buffer = ''
    
    def status_line(self):
        line = self.buffer

        version, status, reason = line.split(None, 2)
        status = int(status)
        if not version.startswith('HTTP/'):
            raise ValueError(line)
            
        self.listener.status(self.url, status)
        
        if status == 200:
            self.part = self.headers
        else:
            self.part = self.ignore
            msg = 'Cannot read, status code %s' % status
            self.listener.error(self.url, msg)
            self.close()
        return version, status, reason

    def headers(self):
        line = self.buffer
        if not line:
            if self.encoding=="chunked":
                self.part = self.chunked_size
            else:
                self.part = self.body
                self.set_terminator(self.length)
        else:
            name, value = line.split(":", 1)
            if name and value:
                name = name.lower()
                value = value.strip()
                if name=="Transfer-Encoding".lower():
                    self.encoding = value
                elif name=="Content-Length".lower():
                    self.length = int(value)
                self.response_header(name, value)

    def response_header(self, name, value):
        self.listener.response_header(self.url, name, value)
    
    def body(self):
        self.done()
        self.close()

    def done(self):
        self.listener.done(self.url)

    def chunked_size(self):
        line = self.buffer
        if not line:
            return
        chunk_size = int(line.split()[0], 16)
        if chunk_size==0:
            self.part = self.trailer
        else:
            self.set_terminator(chunk_size)
            self.part = self.chunked_body            
        self.length += chunk_size
        
    def chunked_body(self):
        line = self.buffer
        self.set_terminator(CRLF)
        self.part = self.chunked_size
        self.feed(line)

    def trailer(self):
        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.6.1
        # trailer        = *(entity-header CRLF)
        line = self.buffer
        if line==CRLF:
            self.done()
            self.close()

if __name__ == '__main__':
    url = sys.argv[1]
    listener = Listener()
    handler = HTTPHandler(listener)
    try:
        handler.get(url)
    except Exception, e:
        listener.error(url, "Error connecting '%s'" % e)

    asyncore.loop()


########NEW FILE########
__FILENAME__ = loggers
"""
Logger implementation loosely modeled on PEP 282.  We don't use the
PEP 282 logger implementation in the stdlib ('logging') because it's
idiosyncratic and a bit slow for our purposes (we don't use threads).
"""

# This module must not depend on any non-stdlib modules to
# avoid circular import problems

import os
import errno
import sys
import time
import traceback

try:
    import syslog
except ImportError:
    # only required when 'syslog' is specified as the log filename
    pass

class LevelsByName:
    CRIT = 50   # messages that probably require immediate user attention
    ERRO = 40   # messages that indicate a potentially ignorable error condition
    WARN = 30   # messages that indicate issues which aren't errors
    INFO = 20   # normal informational output
    DEBG = 10   # messages useful for users trying to debug configurations
    TRAC = 5    # messages useful to developers trying to debug plugins
    BLAT = 3    # messages useful for developers trying to debug supervisor

class LevelsByDescription:
    critical = LevelsByName.CRIT
    error = LevelsByName.ERRO
    warn = LevelsByName.WARN
    info = LevelsByName.INFO
    debug = LevelsByName.DEBG
    trace = LevelsByName.TRAC
    blather = LevelsByName.BLAT

def _levelNumbers():
    bynumber = {}
    for name, number in LevelsByName.__dict__.items():
        if not name.startswith('_'):
            bynumber[number] = name
    return bynumber

LOG_LEVELS_BY_NUM = _levelNumbers()

def getLevelNumByDescription(description):
    num = getattr(LevelsByDescription, description, None)
    return num

class Handler(object):
    fmt = '%(message)s'
    level = LevelsByName.INFO
    def setFormat(self, fmt):
        self.fmt = fmt

    def setLevel(self, level):
        self.level = level

    def flush(self):
        try:
            self.stream.flush()
        except IOError, why:
            # if supervisor output is piped, EPIPE can be raised at exit
            if why[0] != errno.EPIPE:
                raise

    def close(self):
        if hasattr(self.stream, 'fileno'):
            fd = self.stream.fileno()
            if fd < 3: # don't ever close stdout or stderr
                return
        self.stream.close()

    def emit(self, record):
        try:
            msg = self.fmt % record.asdict()
            try:
                self.stream.write(msg)
            except UnicodeError:
                self.stream.write(msg.encode("UTF-8"))
            self.flush()
        except:
            self.handleError(record)

    def handleError(self, record):
        ei = sys.exc_info()
        traceback.print_exception(ei[0], ei[1], ei[2], None, sys.stderr)
        del ei

class FileHandler(Handler):
    """File handler which supports reopening of logs.
    """
    def __init__(self, filename, mode="a"):
        self.stream = open(filename, mode)
        self.baseFilename = filename
        self.mode = mode

    def reopen(self):
        self.close()
        self.stream = open(self.baseFilename, self.mode)

    def remove(self):
        try:
            os.remove(self.baseFilename)
        except OSError, why:
            if why[0] != errno.ENOENT:
                raise

class StreamHandler(Handler):
    def __init__(self, strm=None):
        self.stream = strm

    def remove(self):
        if hasattr(self.stream, 'clear'):
            self.stream.clear()

    def reopen(self):
        pass

class BoundIO:
    def __init__(self, maxbytes, buf=''):
        self.maxbytes = maxbytes
        self.buf = buf

    def flush(self):
        pass

    def close(self):
        self.clear()

    def write(self, s):
        slen = len(s)
        if len(self.buf) + slen > self.maxbytes:
            self.buf = self.buf[slen:]
        self.buf += s

    def getvalue(self):
        return self.buf

    def clear(self):
        self.buf = ''

class RotatingFileHandler(FileHandler):

    open_streams = {}

    def __init__(self, filename, mode='a', maxBytes=512*1024*1024,
                 backupCount=10):
        """
        Open the specified file and use it as the stream for logging.

        By default, the file grows indefinitely. You can specify particular
        values of maxBytes and backupCount to allow the file to rollover at
        a predetermined size.

        Rollover occurs whenever the current log file is nearly maxBytes in
        length. If backupCount is >= 1, the system will successively create
        new files with the same pathname as the base file, but with extensions
        ".1", ".2" etc. appended to it. For example, with a backupCount of 5
        and a base file name of "app.log", you would get "app.log",
        "app.log.1", "app.log.2", ... through to "app.log.5". The file being
        written to is always "app.log" - when it gets filled up, it is closed
        and renamed to "app.log.1", and if files "app.log.1", "app.log.2" etc.
        exist, then they are renamed to "app.log.2", "app.log.3" etc.
        respectively.

        If maxBytes is zero, rollover never occurs.
        """
        if maxBytes > 0:
            mode = 'a' # doesn't make sense otherwise!
        self.mode = mode
        self.baseFilename = filename
        self.stream = self.stream or open(filename, mode)

        self.maxBytes = maxBytes
        self.backupCount = backupCount
        self.counter = 0
        self.every = 10

    class _stream(object):
        """
        Descriptor for managing open filehandles so that only one
        filehandle per file path ever receives logging.
        """
        def __get__(self, obj, objtype):
            """
            Return open filehandle or None
            """
            return objtype.open_streams.get(obj.baseFilename)

        def __set__(self, obj, stream):
            """
            Set open filehandle for filename defined on the
            RotatingFileHandler
            """
            obj.open_streams[obj.baseFilename] = stream

    stream = _stream()

    def close(self):
        if self.stream: self.stream.close()

    def emit(self, record):
        """
        Emit a record.

        Output the record to the file, catering for rollover as described
        in doRollover().
        """
        FileHandler.emit(self, record)
        self.doRollover()

    def doRollover(self):
        """
        Do a rollover, as described in __init__().
        """
        if self.maxBytes <= 0:
            return

        if not (self.stream.tell() >= self.maxBytes):
            return

        self.stream.close()
        if self.backupCount > 0:
            for i in range(self.backupCount - 1, 0, -1):
                sfn = "%s.%d" % (self.baseFilename, i)
                dfn = "%s.%d" % (self.baseFilename, i + 1)
                if os.path.exists(sfn):
                    if os.path.exists(dfn):
                        try:
                            os.remove(dfn)
                        except OSError, why:
                            # catch race condition (already deleted)
                            if why[0] != errno.ENOENT:
                                raise
                    os.rename(sfn, dfn)
            dfn = self.baseFilename + ".1"
            if os.path.exists(dfn):
                try:
                    os.remove(dfn)
                except OSError, why:
                    # catch race condition (already deleted)
                    if why[0] != errno.ENOENT:
                        raise
            os.rename(self.baseFilename, dfn)
        self.stream = open(self.baseFilename, 'w')

class LogRecord:
    def __init__(self, level, msg, **kw):
        self.level = level
        self.msg = msg
        self.kw = kw
        self.dictrepr = None

    def asdict(self):
        if self.dictrepr is None:
            now = time.time()
            msecs = (now - long(now)) * 1000
            part1 = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(now))
            asctime = '%s,%03d' % (part1, msecs)
            levelname = LOG_LEVELS_BY_NUM[self.level]
            if self.kw:
                msg = self.msg % self.kw
            else:
                msg = self.msg
            self.dictrepr = {'message':msg, 'levelname':levelname,
                             'asctime':asctime}
        return self.dictrepr

class Logger:
    def __init__(self, level=None, handlers=None):
        if level is None:
            level = LevelsByName.INFO
        self.level = level

        if handlers is None:
            handlers = []
        self.handlers = handlers

    def close(self):
        for handler in self.handlers:
            handler.close()

    def blather(self, msg, **kw):
        if LevelsByName.BLAT >= self.level:
            self.log(LevelsByName.BLAT, msg, **kw)

    def trace(self, msg, **kw):
        if LevelsByName.TRAC >= self.level:
            self.log(LevelsByName.TRAC, msg, **kw)

    def debug(self, msg, **kw):
        if LevelsByName.DEBG >= self.level:
            self.log(LevelsByName.DEBG, msg, **kw)

    def info(self, msg, **kw):
        if LevelsByName.INFO >= self.level:
            self.log(LevelsByName.INFO, msg, **kw)

    def warn(self, msg, **kw):
        if LevelsByName.WARN >= self.level:
            self.log(LevelsByName.WARN, msg, **kw)

    def error(self, msg, **kw):
        if LevelsByName.ERRO >= self.level:
            self.log(LevelsByName.ERRO, msg, **kw)

    def critical(self, msg, **kw):
        if LevelsByName.CRIT >= self.level:
            self.log(LevelsByName.CRIT, msg, **kw)

    def log(self, level, msg, **kw):
        record = LogRecord(level, msg, **kw)
        for handler in self.handlers:
            if level >= handler.level:
                handler.emit(record)

    def addHandler(self, hdlr):
        self.handlers.append(hdlr)

    def getvalue(self):
        raise NotImplementedError

class SyslogHandler(Handler):
    def __init__(self):
        assert 'syslog' in globals(), "Syslog module not present"

    def close(self):
        pass

    def emit(self, record):
        try:
            params = record.asdict()
            message = params['message']
            for line in message.rstrip('\n').split('\n'):
                params['message'] = line
                msg = self.fmt % params
                try:
                    syslog.syslog(msg)
                except UnicodeError:
                    syslog.syslog(msg.encode("UTF-8"))
        except:
            self.handleError(record)

def getLogger(filename, level, fmt, rotating=False, maxbytes=0, backups=0,
              stdout=False):

    handlers = []

    logger = Logger(level)

    if filename is None:
        if not maxbytes:
            maxbytes = 1<<21 #2MB
        io = BoundIO(maxbytes)
        handlers.append(StreamHandler(io))
        logger.getvalue = io.getvalue

    elif filename == 'syslog':
        handlers.append(SyslogHandler())

    else:
        if rotating is False:
            handlers.append(FileHandler(filename))
        else:
            handlers.append(RotatingFileHandler(filename,'a',maxbytes,backups))

    if stdout:
        handlers.append(StreamHandler(sys.stdout))

    for handler in handlers:
        handler.setFormat(fmt)
        handler.setLevel(level)
        logger.addHandler(handler)

    return logger


########NEW FILE########
__FILENAME__ = asynchat_25
# -*- Mode: Python; tab-width: 4 -*-
#       Id: asynchat.py,v 2.26 2000/09/07 22:29:26 rushing Exp
#       Author: Sam Rushing <rushing@nightmare.com>

# ======================================================================
# Copyright 1996 by Sam Rushing
#
#                         All Rights Reserved
#
# Permission to use, copy, modify, and distribute this software and
# its documentation for any purpose and without fee is hereby
# granted, provided that the above copyright notice appear in all
# copies and that both that copyright notice and this permission
# notice appear in supporting documentation, and that the name of Sam
# Rushing not be used in advertising or publicity pertaining to
# distribution of the software without specific, written prior
# permission.
#
# SAM RUSHING DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
# INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN
# NO EVENT SHALL SAM RUSHING BE LIABLE FOR ANY SPECIAL, INDIRECT OR
# CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS
# OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,
# NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
# CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
# ======================================================================

r"""A class supporting chat-style (command/response) protocols.

This class adds support for 'chat' style protocols - where one side
sends a 'command', and the other sends a response (examples would be
the common internet protocols - smtp, nntp, ftp, etc..).

The handle_read() method looks at the input stream for the current
'terminator' (usually '\r\n' for single-line responses, '\r\n.\r\n'
for multi-line output), calling self.found_terminator() on its
receipt.

for example:
Say you build an async nntp client using this class.  At the start
of the connection, you'll have self.terminator set to '\r\n', in
order to process the single-line greeting.  Just before issuing a
'LIST' command you'll set it to '\r\n.\r\n'.  The output of the LIST
command will be accumulated (using your own 'collect_incoming_data'
method) up to the terminator, and then control will be returned to
you - by calling your self.found_terminator() method.
"""

import socket
from supervisor.medusa import asyncore_25 as asyncore

class async_chat (asyncore.dispatcher):
    """This is an abstract class.  You must derive from this class, and add
    the two methods collect_incoming_data() and found_terminator()"""

    # these are overridable defaults

    ac_in_buffer_size       = 4096
    ac_out_buffer_size      = 4096

    def __init__ (self, conn=None):
        self.ac_in_buffer = ''
        self.ac_out_buffer = ''
        self.producer_fifo = fifo()
        asyncore.dispatcher.__init__ (self, conn)

    def collect_incoming_data(self, data):
        raise NotImplementedError, "must be implemented in subclass"

    def found_terminator(self):
        raise NotImplementedError, "must be implemented in subclass"

    def set_terminator (self, term):
        "Set the input delimiter.  Can be a fixed string of any length, an integer, or None"
        self.terminator = term

    def get_terminator (self):
        return self.terminator

    # grab some more data from the socket,
    # throw it to the collector method,
    # check for the terminator,
    # if found, transition to the next state.

    def handle_read (self):

        try:
            data = self.recv (self.ac_in_buffer_size)
        except socket.error, why:
            self.handle_error()
            return

        self.ac_in_buffer = self.ac_in_buffer + data

        # Continue to search for self.terminator in self.ac_in_buffer,
        # while calling self.collect_incoming_data.  The while loop
        # is necessary because we might read several data+terminator
        # combos with a single recv(1024).

        while self.ac_in_buffer:
            lb = len(self.ac_in_buffer)
            terminator = self.get_terminator()
            if not terminator:
                # no terminator, collect it all
                self.collect_incoming_data (self.ac_in_buffer)
                self.ac_in_buffer = ''
            elif isinstance(terminator, int) or isinstance(terminator, long):
                # numeric terminator
                n = terminator
                if lb < n:
                    self.collect_incoming_data (self.ac_in_buffer)
                    self.ac_in_buffer = ''
                    self.terminator = self.terminator - lb
                else:
                    self.collect_incoming_data (self.ac_in_buffer[:n])
                    self.ac_in_buffer = self.ac_in_buffer[n:]
                    self.terminator = 0
                    self.found_terminator()
            else:
                # 3 cases:
                # 1) end of buffer matches terminator exactly:
                #    collect data, transition
                # 2) end of buffer matches some prefix:
                #    collect data to the prefix
                # 3) end of buffer does not match any prefix:
                #    collect data
                terminator_len = len(terminator)
                index = self.ac_in_buffer.find(terminator)
                if index != -1:
                    # we found the terminator
                    if index > 0:
                        # don't bother reporting the empty string (source of subtle bugs)
                        self.collect_incoming_data (self.ac_in_buffer[:index])
                    self.ac_in_buffer = self.ac_in_buffer[index+terminator_len:]
                    # This does the Right Thing if the terminator is changed here.
                    self.found_terminator()
                else:
                    # check for a prefix of the terminator
                    index = find_prefix_at_end (self.ac_in_buffer, terminator)
                    if index:
                        if index != lb:
                            # we found a prefix, collect up to the prefix
                            self.collect_incoming_data (self.ac_in_buffer[:-index])
                            self.ac_in_buffer = self.ac_in_buffer[-index:]
                        break
                    else:
                        # no prefix, collect it all
                        self.collect_incoming_data (self.ac_in_buffer)
                        self.ac_in_buffer = ''

    def handle_write (self):
        self.initiate_send ()

    def handle_close (self):
        self.close()

    def push (self, data):
        self.producer_fifo.push (simple_producer (data))
        self.initiate_send()

    def push_with_producer (self, producer):
        self.producer_fifo.push (producer)
        self.initiate_send()

    def readable (self):
        "predicate for inclusion in the readable for select()"
        return (len(self.ac_in_buffer) <= self.ac_in_buffer_size)

    def writable (self):
        "predicate for inclusion in the writable for select()"
        # return len(self.ac_out_buffer) or len(self.producer_fifo) or (not self.connected)
        # this is about twice as fast, though not as clear.
        return not (
                (self.ac_out_buffer == '') and
                self.producer_fifo.is_empty() and
                self.connected
                )

    def close_when_done (self):
        "automatically close this channel once the outgoing queue is empty"
        self.producer_fifo.push (None)

    # refill the outgoing buffer by calling the more() method
    # of the first producer in the queue
    def refill_buffer (self):
        while 1:
            if len(self.producer_fifo):
                p = self.producer_fifo.first()
                # a 'None' in the producer fifo is a sentinel,
                # telling us to close the channel.
                if p is None:
                    if not self.ac_out_buffer:
                        self.producer_fifo.pop()
                        self.close()
                    return
                elif isinstance(p, str):
                    self.producer_fifo.pop()
                    self.ac_out_buffer = self.ac_out_buffer + p
                    return
                data = p.more()
                if data:
                    self.ac_out_buffer = self.ac_out_buffer + data
                    return
                else:
                    self.producer_fifo.pop()
            else:
                return

    def initiate_send (self):
        obs = self.ac_out_buffer_size
        # try to refill the buffer
        if (len (self.ac_out_buffer) < obs):
            self.refill_buffer()

        if self.ac_out_buffer and self.connected:
            # try to send the buffer
            try:
                num_sent = self.send (self.ac_out_buffer[:obs])
                if num_sent:
                    self.ac_out_buffer = self.ac_out_buffer[num_sent:]

            except socket.error, why:
                self.handle_error()
                return

    def discard_buffers (self):
        # Emergencies only!
        self.ac_in_buffer = ''
        self.ac_out_buffer = ''
        while self.producer_fifo:
            self.producer_fifo.pop()


class simple_producer:

    def __init__ (self, data, buffer_size=512):
        self.data = data
        self.buffer_size = buffer_size

    def more (self):
        if len (self.data) > self.buffer_size:
            result = self.data[:self.buffer_size]
            self.data = self.data[self.buffer_size:]
            return result
        else:
            result = self.data
            self.data = ''
            return result

class fifo:
    def __init__ (self, list=None):
        if not list:
            self.list = []
        else:
            self.list = list

    def __len__ (self):
        return len(self.list)

    def is_empty (self):
        return self.list == []

    def first (self):
        return self.list[0]

    def push (self, data):
        self.list.append(data)

    def pop (self):
        if self.list:
            return (1, self.list.pop(0))
        else:
            return (0, None)

# Given 'haystack', see if any prefix of 'needle' is at its end.  This
# assumes an exact match has already been checked.  Return the number of
# characters matched.
# for example:
# f_p_a_e ("qwerty\r", "\r\n") => 1
# f_p_a_e ("qwertydkjf", "\r\n") => 0
# f_p_a_e ("qwerty\r\n", "\r\n") => <undefined>

# this could maybe be made faster with a computed regex?
# [answer: no; circa Python-2.0, Jan 2001]
# new python:   28961/s
# old python:   18307/s
# re:        12820/s
# regex:     14035/s

def find_prefix_at_end (haystack, needle):
    l = len(needle) - 1
    while l and not haystack.endswith(needle[:l]):
        l -= 1
    return l

########NEW FILE########
__FILENAME__ = asyncore_25
# -*- Mode: Python -*-
#   Id: asyncore.py,v 2.51 2000/09/07 22:29:26 rushing Exp
#   Author: Sam Rushing <rushing@nightmare.com>

# ======================================================================
# Copyright 1996 by Sam Rushing
#
#                         All Rights Reserved
#
# Permission to use, copy, modify, and distribute this software and
# its documentation for any purpose and without fee is hereby
# granted, provided that the above copyright notice appear in all
# copies and that both that copyright notice and this permission
# notice appear in supporting documentation, and that the name of Sam
# Rushing not be used in advertising or publicity pertaining to
# distribution of the software without specific, written prior
# permission.
#
# SAM RUSHING DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
# INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN
# NO EVENT SHALL SAM RUSHING BE LIABLE FOR ANY SPECIAL, INDIRECT OR
# CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS
# OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,
# NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
# CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
# ======================================================================

"""Basic infrastructure for asynchronous socket service clients and servers.

There are only two ways to have a program on a single processor do "more
than one thing at a time".  Multi-threaded programming is the simplest and
most popular way to do it, but there is another very different technique,
that lets you have nearly all the advantages of multi-threading, without
actually using multiple threads. it's really only practical if your program
is largely I/O bound. If your program is CPU bound, then pre-emptive
scheduled threads are probably what you really need. Network servers are
rarely CPU-bound, however.

If your operating system supports the select() system call in its I/O
library (and nearly all do), then you can use it to juggle multiple
communication channels at once; doing other work while your I/O is taking
place in the "background."  Although this strategy can seem strange and
complex, especially at first, it is in many ways easier to understand and
control than multi-threaded programming. The module documented here solves
many of the difficult problems for you, making the task of building
sophisticated high-performance network servers and clients a snap.
"""

import select
import socket
import sys
import time

import os
from errno import EALREADY, EINPROGRESS, EWOULDBLOCK, ECONNRESET, \
     ENOTCONN, ESHUTDOWN, EINTR, EISCONN, errorcode

try:
    socket_map
except NameError:
    socket_map = {}

class ExitNow(Exception):
    pass

def read(obj):
    try:
        obj.handle_read_event()
    except ExitNow:
        raise
    except:
        obj.handle_error()

def write(obj):
    try:
        obj.handle_write_event()
    except ExitNow:
        raise
    except:
        obj.handle_error()

def _exception (obj):
    try:
        obj.handle_expt_event()
    except ExitNow:
        raise
    except:
        obj.handle_error()

def readwrite(obj, flags):
    try:
        if flags & (select.POLLIN | select.POLLPRI):
            obj.handle_read_event()
        if flags & select.POLLOUT:
            obj.handle_write_event()
        if flags & (select.POLLERR | select.POLLHUP | select.POLLNVAL):
            obj.handle_expt_event()
    except ExitNow:
        raise
    except:
        obj.handle_error()

def poll(timeout=0.0, map=None):
    if map is None:
        map = socket_map
    if map:
        r = []; w = []; e = []
        for fd, obj in map.items():
            is_r = obj.readable()
            is_w = obj.writable()
            if is_r:
                r.append(fd)
            if is_w:
                w.append(fd)
            if is_r or is_w:
                e.append(fd)
        if [] == r == w == e:
            time.sleep(timeout)
        else:
            try:
                r, w, e = select.select(r, w, e, timeout)
            except select.error, err:
                if err[0] != EINTR:
                    raise
                else:
                    return

        for fd in r:
            obj = map.get(fd)
            if obj is None:
                continue
            read(obj)

        for fd in w:
            obj = map.get(fd)
            if obj is None:
                continue
            write(obj)

        for fd in e:
            obj = map.get(fd)
            if obj is None:
                continue
            _exception(obj)

def poll2(timeout=0.0, map=None):
    # Use the poll() support added to the select module in Python 2.0
    if map is None:
        map = socket_map
    if timeout is not None:
        # timeout is in milliseconds
        timeout = int(timeout*1000)
    pollster = select.poll()
    if map:
        for fd, obj in map.items():
            flags = 0
            if obj.readable():
                flags |= select.POLLIN | select.POLLPRI
            if obj.writable():
                flags |= select.POLLOUT
            if flags:
                # Only check for exceptions if object was either readable
                # or writable.
                flags |= select.POLLERR | select.POLLHUP | select.POLLNVAL
                pollster.register(fd, flags)
        try:
            r = pollster.poll(timeout)
        except select.error, err:
            if err[0] != EINTR:
                raise
            r = []
        for fd, flags in r:
            obj = map.get(fd)
            if obj is None:
                continue
            readwrite(obj, flags)

poll3 = poll2                           # Alias for backward compatibility

def loop(timeout=30.0, use_poll=False, map=None, count=None):
    if map is None:
        map = socket_map

    if use_poll and hasattr(select, 'poll'):
        poll_fun = poll2
    else:
        poll_fun = poll

    if count is None:
        while map:
            poll_fun(timeout, map)

    else:
        while map and count > 0:
            poll_fun(timeout, map)
            count = count - 1

class dispatcher:

    debug = False
    connected = False
    accepting = False
    closing = False
    addr = None

    def __init__(self, sock=None, map=None):
        if map is None:
            self._map = socket_map
        else:
            self._map = map

        if sock:
            self.set_socket(sock, map)
            # I think it should inherit this anyway
            self.socket.setblocking(0)
            self.connected = True
            # XXX Does the constructor require that the socket passed
            # be connected?
            try:
                self.addr = sock.getpeername()
            except socket.error:
                # The addr isn't crucial
                pass
        else:
            self.socket = None

    def __repr__(self):
        status = [self.__class__.__module__+"."+self.__class__.__name__]
        if self.accepting and self.addr:
            status.append('listening')
        elif self.connected:
            status.append('connected')
        if self.addr is not None:
            try:
                status.append('%s:%d' % self.addr)
            except TypeError:
                status.append(repr(self.addr))
        return '<%s at %#x>' % (' '.join(status), id(self))

    def add_channel(self, map=None):
        #self.log_info('adding channel %s' % self)
        if map is None:
            map = self._map
        map[self._fileno] = self

    def del_channel(self, map=None):
        fd = self._fileno
        if map is None:
            map = self._map
        if map.has_key(fd):
            #self.log_info('closing channel %d:%s' % (fd, self))
            del map[fd]
        self._fileno = None

    def create_socket(self, family, type):
        self.family_and_type = family, type
        self.socket = socket.socket(family, type)
        self.socket.setblocking(0)
        self._fileno = self.socket.fileno()
        self.add_channel()

    def set_socket(self, sock, map=None):
        self.socket = sock
##        self.__dict__['socket'] = sock
        self._fileno = sock.fileno()
        self.add_channel(map)

    def set_reuse_addr(self):
        # try to re-use a server port if possible
        try:
            self.socket.setsockopt(
                socket.SOL_SOCKET, socket.SO_REUSEADDR,
                self.socket.getsockopt(socket.SOL_SOCKET,
                                       socket.SO_REUSEADDR) | 1
                )
        except socket.error:
            pass

    # ==================================================
    # predicates for select()
    # these are used as filters for the lists of sockets
    # to pass to select().
    # ==================================================

    def readable(self):
        return True

    def writable(self):
        return True

    # ==================================================
    # socket object methods.
    # ==================================================

    def listen(self, num):
        self.accepting = True
        if os.name == 'nt' and num > 5:
            num = 1
        return self.socket.listen(num)

    def bind(self, addr):
        self.addr = addr
        return self.socket.bind(addr)

    def connect(self, address):
        self.connected = False
        err = self.socket.connect_ex(address)
        # XXX Should interpret Winsock return values
        if err in (EINPROGRESS, EALREADY, EWOULDBLOCK):
            return
        if err in (0, EISCONN):
            self.addr = address
            self.connected = True
            self.handle_connect()
        else:
            raise socket.error, (err, errorcode[err])

    def accept(self):
        # XXX can return either an address pair or None
        try:
            conn, addr = self.socket.accept()
            return conn, addr
        except socket.error, why:
            if why[0] == EWOULDBLOCK:
                pass
            else:
                raise

    def send(self, data):
        try:
            result = self.socket.send(data)
            return result
        except socket.error, why:
            if why[0] == EWOULDBLOCK:
                return 0
            else:
                raise
            return 0

    def recv(self, buffer_size):
        try:
            data = self.socket.recv(buffer_size)
            if not data:
                # a closed connection is indicated by signaling
                # a read condition, and having recv() return 0.
                self.handle_close()
                return ''
            else:
                return data
        except socket.error, why:
            # winsock sometimes throws ENOTCONN
            if why[0] in [ECONNRESET, ENOTCONN, ESHUTDOWN]:
                self.handle_close()
                return ''
            else:
                raise

    def close(self):
        self.del_channel()
        self.socket.close()

    # cheap inheritance, used to pass all other attribute
    # references to the underlying socket object.
    def __getattr__(self, attr):
        return getattr(self.socket, attr)

    # log and log_info may be overridden to provide more sophisticated
    # logging and warning methods. In general, log is for 'hit' logging
    # and 'log_info' is for informational, warning and error logging.

    def log(self, message):
        sys.stderr.write('log: %s\n' % str(message))

    def log_info(self, message, type='info'):
        if __debug__ or type != 'info':
            print '%s: %s' % (type, message)

    def handle_read_event(self):
        if self.accepting:
            # for an accepting socket, getting a read implies
            # that we are connected
            if not self.connected:
                self.connected = True
            self.handle_accept()
        elif not self.connected:
            self.handle_connect()
            self.connected = True
            self.handle_read()
        else:
            self.handle_read()

    def handle_write_event(self):
        # getting a write implies that we are connected
        if not self.connected:
            self.handle_connect()
            self.connected = True
        self.handle_write()

    def handle_expt_event(self):
        self.handle_expt()

    def handle_error(self):
        nil, t, v, tbinfo = compact_traceback()

        # sometimes a user repr method will crash.
        try:
            self_repr = repr(self)
        except:
            self_repr = '<__repr__(self) failed for object at %0x>' % id(self)

        self.log_info(
            'uncaptured python exception, closing channel %s (%s:%s %s)' % (
                self_repr,
                t,
                v,
                tbinfo
                ),
            'error'
            )
        self.close()

    def handle_expt(self):
        self.log_info('unhandled exception', 'warning')

    def handle_read(self):
        self.log_info('unhandled read event', 'warning')

    def handle_write(self):
        self.log_info('unhandled write event', 'warning')

    def handle_connect(self):
        self.log_info('unhandled connect event', 'warning')

    def handle_accept(self):
        self.log_info('unhandled accept event', 'warning')

    def handle_close(self):
        self.log_info('unhandled close event', 'warning')
        self.close()

# ---------------------------------------------------------------------------
# adds simple buffered output capability, useful for simple clients.
# [for more sophisticated usage use asynchat.async_chat]
# ---------------------------------------------------------------------------

class dispatcher_with_send(dispatcher):

    def __init__(self, sock=None, map=None):
        dispatcher.__init__(self, sock, map)
        self.out_buffer = ''

    def initiate_send(self):
        num_sent = 0
        num_sent = dispatcher.send(self, self.out_buffer[:512])
        self.out_buffer = self.out_buffer[num_sent:]

    def handle_write(self):
        self.initiate_send()

    def writable(self):
        return (not self.connected) or len(self.out_buffer)

    def send(self, data):
        if self.debug:
            self.log_info('sending %s' % repr(data))
        self.out_buffer = self.out_buffer + data
        self.initiate_send()

# ---------------------------------------------------------------------------
# used for debugging.
# ---------------------------------------------------------------------------

def compact_traceback():
    t, v, tb = sys.exc_info()
    tbinfo = []
    assert tb # Must have a traceback
    while tb:
        tbinfo.append((
            tb.tb_frame.f_code.co_filename,
            tb.tb_frame.f_code.co_name,
            str(tb.tb_lineno)
            ))
        tb = tb.tb_next

    # just to be safe
    del tb

    file, function, line = tbinfo[-1]
    info = ' '.join(['[%s|%s|%s]' % x for x in tbinfo])
    return (file, function, line), t, v, info

def close_all(map=None):
    if map is None:
        map = socket_map
    for x in map.values():
        x.socket.close()
    map.clear()

# Asynchronous File I/O:
#
# After a little research (reading man pages on various unixen, and
# digging through the linux kernel), I've determined that select()
# isn't meant for doing asynchronous file i/o.
# Heartening, though - reading linux/mm/filemap.c shows that linux
# supports asynchronous read-ahead.  So _MOST_ of the time, the data
# will be sitting in memory for us already when we go to read it.
#
# What other OS's (besides NT) support async file i/o?  [VMS?]
#
# Regardless, this is useful for pipes, and stdin/stdout...

if os.name == 'posix':
    import fcntl

    class file_wrapper:
        # here we override just enough to make a file
        # look like a socket for the purposes of asyncore.

        def __init__(self, fd):
            self.fd = fd

        def recv(self, *args):
            return os.read(self.fd, *args)

        def send(self, *args):
            return os.write(self.fd, *args)

        read = recv
        write = send

        def close(self):
            os.close(self.fd)

        def fileno(self):
            return self.fd

    class file_dispatcher(dispatcher):

        def __init__(self, fd, map=None):
            dispatcher.__init__(self, None, map)
            self.connected = True
            self.set_file(fd)
            # set it to non-blocking mode
            flags = fcntl.fcntl(fd, fcntl.F_GETFL, 0)
            flags = flags | os.O_NONBLOCK
            fcntl.fcntl(fd, fcntl.F_SETFL, flags)

        def set_file(self, fd):
            self._fileno = fd
            self.socket = file_wrapper(fd)
            self.add_channel()

########NEW FILE########
__FILENAME__ = auth_handler
# -*- Mode: Python -*-
#
#       Author: Sam Rushing <rushing@nightmare.com>
#       Copyright 1996-2000 by Sam Rushing
#                                                All Rights Reserved.
#

RCS_ID =  '$Id: auth_handler.py,v 1.6 2002/11/25 19:40:23 akuchling Exp $'

# support for 'basic' authenticaion.

import base64
try:
    from hashlib import md5
except ImportError:
    from md5 import new as md5
import re
import string
import time
import counter

import default_handler

get_header = default_handler.get_header

import producers

# This is a 'handler' that wraps an authorization method
# around access to the resources normally served up by
# another handler.

# does anyone support digest authentication? (rfc2069)

class auth_handler:
    def __init__ (self, dict, handler, realm='default'):
        self.authorizer = dictionary_authorizer (dict)
        self.handler = handler
        self.realm = realm
        self.pass_count = counter.counter()
        self.fail_count = counter.counter()

    def match (self, request):
        # by default, use the given handler's matcher
        return self.handler.match (request)

    def handle_request (self, request):
        # authorize a request before handling it...
        scheme = get_header (AUTHORIZATION, request.header)

        if scheme:
            scheme = string.lower (scheme)
            if scheme == 'basic':
                cookie = get_header (AUTHORIZATION, request.header, 2)
                try:
                    decoded = base64.decodestring (cookie)
                except:
                    print 'malformed authorization info <%s>' % cookie
                    request.error (400)
                    return
                auth_info = string.split (decoded, ':')
                if self.authorizer.authorize (auth_info):
                    self.pass_count.increment()
                    request.auth_info = auth_info
                    self.handler.handle_request (request)
                else:
                    self.handle_unauthorized (request)
            #elif scheme == 'digest':
            #       print 'digest: ',AUTHORIZATION.group(2)
            else:
                print 'unknown/unsupported auth method: %s' % scheme
                self.handle_unauthorized(request)
        else:
            # list both?  prefer one or the other?
            # you could also use a 'nonce' here. [see below]
            #auth = 'Basic realm="%s" Digest realm="%s"' % (self.realm, self.realm)
            #nonce = self.make_nonce (request)
            #auth = 'Digest realm="%s" nonce="%s"' % (self.realm, nonce)
            #request['WWW-Authenticate'] = auth
            #print 'sending header: %s' % request['WWW-Authenticate']
            self.handle_unauthorized (request)

    def handle_unauthorized (self, request):
        # We are now going to receive data that we want to ignore.
        # to ignore the file data we're not interested in.
        self.fail_count.increment()
        request.channel.set_terminator (None)
        request['Connection'] = 'close'
        request['WWW-Authenticate'] = 'Basic realm="%s"' % self.realm
        request.error (401)

    def make_nonce (self, request):
        "A digest-authentication <nonce>, constructed as suggested in RFC 2069"
        ip = request.channel.server.ip
        now = str(long(time.time()))
        if now[-1:] == 'L':
            now = now[:-1]
        private_key = str (id (self))
        nonce = string.join ([ip, now, private_key], ':')
        return self.apply_hash (nonce)

    def apply_hash (self, s):
        "Apply MD5 to a string <s>, then wrap it in base64 encoding."
        m = md5()
        m.update (s)
        d = m.digest()
        # base64.encodestring tacks on an extra linefeed.
        return base64.encodestring (d)[:-1]

    def status (self):
        # Thanks to mwm@contessa.phone.net (Mike Meyer)
        r = [
                producers.simple_producer (
                        '<li>Authorization Extension : '
                        '<b>Unauthorized requests:</b> %s<ul>' % self.fail_count
                        )
                ]
        if hasattr (self.handler, 'status'):
            r.append (self.handler.status())
        r.append (
                producers.simple_producer ('</ul>')
                )
        return producers.composite_producer(r)

class dictionary_authorizer:
    def __init__ (self, dict):
        self.dict = dict

    def authorize (self, auth_info):
        [username, password] = auth_info
        if (self.dict.has_key (username)) and (self.dict[username] == password):
            return 1
        else:
            return 0

AUTHORIZATION = re.compile (
        #               scheme  challenge
        'Authorization: ([^ ]+) (.*)',
        re.IGNORECASE
        )

########NEW FILE########
__FILENAME__ = chat_server
# -*- Mode: Python -*-
#
#       Author: Sam Rushing <rushing@nightmare.com>
#       Copyright 1997-2000 by Sam Rushing
#                                                All Rights Reserved.
#

RCS_ID = '$Id: chat_server.py,v 1.4 2002/03/20 17:37:48 amk Exp $'

import string

VERSION = string.split(RCS_ID)[2]

import socket
import asyncore_25 as asyncore
import asynchat_25 as asynchat
import status_handler

class chat_channel (asynchat.async_chat):

    def __init__ (self, server, sock, addr):
        asynchat.async_chat.__init__ (self, sock)
        self.server = server
        self.addr = addr
        self.set_terminator ('\r\n')
        self.data = ''
        self.nick = None
        self.push ('nickname?: ')

    def collect_incoming_data (self, data):
        self.data = self.data + data

    def found_terminator (self):
        line = self.data
        self.data = ''
        if self.nick is None:
            self.nick = string.split (line)[0]
            if not self.nick:
                self.nick = None
                self.push ('huh? gimmee a nickname: ')
            else:
                self.greet()
        else:
            if not line:
                pass
            elif line[0] != '/':
                self.server.push_line (self, line)
            else:
                self.handle_command (line)

    def greet (self):
        self.push ('Hello, %s\r\n' % self.nick)
        num_channels = len(self.server.channels)-1
        if num_channels == 0:
            self.push ('[Kinda lonely in here... you\'re the only caller!]\r\n')
        else:
            self.push ('[There are %d other callers]\r\n' % (len(self.server.channels)-1))
            nicks = map (lambda x: x.get_nick(), self.server.channels.keys())
            self.push (string.join (nicks, '\r\n  ') + '\r\n')
            self.server.push_line (self, '[joined]')

    def handle_command (self, command):
        import types
        command_line = string.split(command)
        name = 'cmd_%s' % command_line[0][1:]
        if hasattr (self, name):
            # make sure it's a method...
            method = getattr (self, name)
            if type(method) == type(self.handle_command):
                method (command_line[1:])
            else:
                self.push ('unknown command: %s' % command_line[0])

    def cmd_quit (self, args):
        self.server.push_line (self, '[left]')
        self.push ('Goodbye!\r\n')
        self.close_when_done()

    # alias for '/quit' - '/q'
    cmd_q = cmd_quit

    def push_line (self, nick, line):
        self.push ('%s: %s\r\n' % (nick, line))

    def handle_close (self):
        self.close()

    def close (self):
        del self.server.channels[self]
        asynchat.async_chat.close (self)

    def get_nick (self):
        if self.nick is not None:
            return self.nick
        else:
            return 'Unknown'

class chat_server (asyncore.dispatcher):

    SERVER_IDENT = 'Chat Server (V%s)' % VERSION

    channel_class = chat_channel

    spy = 1

    def __init__ (self, ip='', port=8518):
        asyncore.dispatcher.__init__(self)
        self.port = port
        self.create_socket (socket.AF_INET, socket.SOCK_STREAM)
        self.bind ((ip, port))
        print '%s started on port %d' % (self.SERVER_IDENT, port)
        self.listen (5)
        self.channels = {}
        self.count = 0

    def handle_accept (self):
        conn, addr = self.accept()
        self.count = self.count + 1
        print 'client #%d - %s:%d' % (self.count, addr[0], addr[1])
        self.channels[self.channel_class (self, conn, addr)] = 1

    def push_line (self, from_channel, line):
        nick = from_channel.get_nick()
        if self.spy:
            print '%s: %s' % (nick, line)
        for c in self.channels.keys():
            if c is not from_channel:
                c.push ('%s: %s\r\n' % (nick, line))

    def status (self):
        lines = [
                '<h2>%s</h2>'                                           % self.SERVER_IDENT,
                '<br>Listening on Port: %d'                     % self.port,
                '<br><b>Total Sessions:</b> %d'         % self.count,
                '<br><b>Current Sessions:</b> %d'       % (len(self.channels))
                ]
        return status_handler.lines_producer (lines)

    def writable (self):
        return 0

if __name__ == '__main__':
    import sys

    if len(sys.argv) > 1:
        port = string.atoi (sys.argv[1])
    else:
        port = 8518

    s = chat_server ('', port)
    asyncore.loop()

########NEW FILE########
__FILENAME__ = counter
# -*- Mode: Python -*-

# It is tempting to add an __int__ method to this class, but it's not
# a good idea.  This class tries to gracefully handle integer
# overflow, and to hide this detail from both the programmer and the
# user.  Note that the __str__ method can be relied on for printing out
# the value of a counter:
#
# >>> print 'Total Client: %s' % self.total_clients
#
# If you need to do arithmetic with the value, then use the 'as_long'
# method, the use of long arithmetic is a reminder that the counter
# will overflow.

class counter:
    "general-purpose counter"

    def __init__ (self, initial_value=0):
        self.value = initial_value

    def increment (self, delta=1):
        result = self.value
        try:
            self.value = self.value + delta
        except OverflowError:
            self.value = long(self.value) + delta
        return result

    def decrement (self, delta=1):
        result = self.value
        try:
            self.value = self.value - delta
        except OverflowError:
            self.value = long(self.value) - delta
        return result

    def as_long (self):
        return long(self.value)

    def __nonzero__ (self):
        return self.value != 0

    def __repr__ (self):
        return '<counter value=%s at %x>' % (self.value, id(self))

    def __str__ (self):
        s = str(long(self.value))
        if s[-1:] == 'L':
            s = s[:-1]
        return s


########NEW FILE########
__FILENAME__ = default_handler
# -*- Mode: Python -*-
#
#       Author: Sam Rushing <rushing@nightmare.com>
#       Copyright 1997 by Sam Rushing
#                                                All Rights Reserved.
#

RCS_ID = '$Id: default_handler.py,v 1.8 2002/08/01 18:15:45 akuchling Exp $'

# standard python modules
import mimetypes
import re
import stat
import string

# medusa modules
import http_date
import http_server
import status_handler
import producers

unquote = http_server.unquote

# This is the 'default' handler.  it implements the base set of
# features expected of a simple file-delivering HTTP server.  file
# services are provided through a 'filesystem' object, the very same
# one used by the FTP server.
#
# You can replace or modify this handler if you want a non-standard
# HTTP server.  You can also derive your own handler classes from
# it.
#
# support for handling POST requests is available in the derived
# class <default_with_post_handler>, defined below.
#

from counter import counter

class default_handler:

    valid_commands = ['GET', 'HEAD']

    IDENT = 'Default HTTP Request Handler'

    # Pathnames that are tried when a URI resolves to a directory name
    directory_defaults = [
            'index.html',
            'default.html'
            ]

    default_file_producer = producers.file_producer

    def __init__ (self, filesystem):
        self.filesystem = filesystem
        # count total hits
        self.hit_counter = counter()
        # count file deliveries
        self.file_counter = counter()
        # count cache hits
        self.cache_counter = counter()

    hit_counter = 0

    def __repr__ (self):
        return '<%s (%s hits) at %x>' % (
                self.IDENT,
                self.hit_counter,
                id (self)
                )

    # always match, since this is a default
    def match (self, request):
        return 1

    # handle a file request, with caching.

    def handle_request (self, request):

        if request.command not in self.valid_commands:
            request.error (400) # bad request
            return

        self.hit_counter.increment()

        path, params, query, fragment = request.split_uri()

        if '%' in path:
            path = unquote (path)

        # strip off all leading slashes
        while path and path[0] == '/':
            path = path[1:]

        if self.filesystem.isdir (path):
            if path and path[-1] != '/':
                request['Location'] = 'http://%s/%s/' % (
                        request.channel.server.server_name,
                        path
                        )
                request.error (301)
                return

            # we could also generate a directory listing here,
            # may want to move this into another method for that
            # purpose
            found = 0
            if path and path[-1] != '/':
                path = path + '/'
            for default in self.directory_defaults:
                p = path + default
                if self.filesystem.isfile (p):
                    path = p
                    found = 1
                    break
            if not found:
                request.error (404) # Not Found
                return

        elif not self.filesystem.isfile (path):
            request.error (404) # Not Found
            return

        file_length = self.filesystem.stat (path)[stat.ST_SIZE]

        ims = get_header_match (IF_MODIFIED_SINCE, request.header)

        length_match = 1
        if ims:
            length = ims.group (4)
            if length:
                try:
                    length = string.atoi (length)
                    if length != file_length:
                        length_match = 0
                except:
                    pass

        ims_date = 0

        if ims:
            ims_date = http_date.parse_http_date (ims.group (1))

        try:
            mtime = self.filesystem.stat (path)[stat.ST_MTIME]
        except:
            request.error (404)
            return

        if length_match and ims_date:
            if mtime <= ims_date:
                request.reply_code = 304
                request.done()
                self.cache_counter.increment()
                return
        try:
            file = self.filesystem.open (path, 'rb')
        except IOError:
            request.error (404)
            return

        request['Last-Modified'] = http_date.build_http_date (mtime)
        request['Content-Length'] = file_length
        self.set_content_type (path, request)

        if request.command == 'GET':
            request.push (self.default_file_producer (file))

        self.file_counter.increment()
        request.done()

    def set_content_type (self, path, request):
        ext = string.lower (get_extension (path))
        typ, encoding = mimetypes.guess_type(path)
        if typ is not None:
            request['Content-Type'] = typ
        else:
            # TODO: test a chunk off the front of the file for 8-bit
            # characters, and use application/octet-stream instead.
            request['Content-Type'] = 'text/plain'

    def status (self):
        return producers.simple_producer (
                '<li>%s' % status_handler.html_repr (self)
                + '<ul>'
                + '  <li><b>Total Hits:</b> %s'                 % self.hit_counter
                + '  <li><b>Files Delivered:</b> %s'    % self.file_counter
                + '  <li><b>Cache Hits:</b> %s'                 % self.cache_counter
                + '</ul>'
                )

# HTTP/1.0 doesn't say anything about the "; length=nnnn" addition
# to this header.  I suppose its purpose is to avoid the overhead
# of parsing dates...
IF_MODIFIED_SINCE = re.compile (
        'If-Modified-Since: ([^;]+)((; length=([0-9]+)$)|$)',
        re.IGNORECASE
        )

USER_AGENT = re.compile ('User-Agent: (.*)', re.IGNORECASE)

CONTENT_TYPE = re.compile (
        r'Content-Type: ([^;]+)((; boundary=([A-Za-z0-9\'\(\)+_,./:=?-]+)$)|$)',
        re.IGNORECASE
        )

get_header = http_server.get_header
get_header_match = http_server.get_header_match

def get_extension (path):
    dirsep = string.rfind (path, '/')
    dotsep = string.rfind (path, '.')
    if dotsep > dirsep:
        return path[dotsep+1:]
    else:
        return ''

########NEW FILE########
__FILENAME__ = publish
# -*- Mode: Python -*-

# Demonstrates use of the auth and put handlers to support publishing
# web pages via HTTP.

# It is also possible to set up the ftp server to do essentially the
# same thing.

# Security Note: Using HTTP with the 'Basic' authentication scheme is
# only slightly more secure than using FTP: both techniques involve
# sending a unencrypted password of the network (http basic auth
# base64-encodes the username and password).  The 'Digest' scheme is
# much more secure, but not widely supported yet. <sigh>

from supervisor.medusa import asyncore_25 as asyncore
from supervisor.medusa import default_handler
from supervisor.medusa import http_server
from supervisor.medusa import put_handler
from supervisor.medusa import auth_handler
from supervisor.medusa import filesys

# For this demo, we'll just use a dictionary of usernames/passwords.
# You can of course use anything that supports the mapping interface,
# and it would be pretty easy to set this up to use the crypt module
# on unix.

users = { 'mozart' : 'jupiter', 'beethoven' : 'pastoral' }

# The filesystem we will be giving access to
fs = filesys.os_filesystem('/home/medusa')

# The 'default' handler - delivers files for the HTTP GET method.
dh = default_handler.default_handler(fs)

# Supports the HTTP PUT method...
ph = put_handler.put_handler(fs, '/.*')

# ... but be sure to wrap it with an auth handler:
ah = auth_handler.auth_handler(users, ph)

# Create a Web Server
hs = http_server.http_server(ip='', port=8080)

# install the handlers we created:

hs.install_handler(dh) # for GET
hs.install_handler(ah) # for PUT

asyncore.loop()

########NEW FILE########
__FILENAME__ = script_server
# -*- Mode: Python -*-

import re, sys
from supervisor.medusa import asyncore_25 as asyncore
from supervisor.medusa import http_server
from supervisor.medusa import default_handler
from supervisor.medusa import logger
from supervisor.medusa import script_handler
from supervisor.medusa import filesys

PUBLISHING_ROOT='/home/medusa'
CONTENT_LENGTH = re.compile ('Content-Length: ([0-9]+)', re.IGNORECASE)

class sample_input_collector:
    def __init__ (self, request, length):
        self.request = request
        self.length = length

    def collect_incoming_data (self, data):
        print 'data from %s: <%s>' % (self.request, repr(data))

class post_script_handler (script_handler.script_handler):

    def handle_request (self, request):
        if request.command == 'post':
            cl = default_handler.get_header(CONTENT_LENGTH, request.header)
            ic = sample_input_collector(request, cl)
            request.collector = ic
            print request.header

        return script_handler.script_handler.handle_request (self, request)

lg = logger.file_logger (sys.stdout)
fs = filesys.os_filesystem (PUBLISHING_ROOT)
dh = default_handler.default_handler (fs)
ph = post_script_handler (fs)
hs = http_server.http_server ('', 8081, logger_object = lg)

hs.install_handler (dh)
hs.install_handler (ph)

asyncore.loop()

########NEW FILE########
__FILENAME__ = simple_anon_ftpd
# -*- Mode: Python -*-

from supervisor.medusa import asyncore_25 as asyncore
from supervisor.medusa import ftp_server

# create a 'dummy' authorizer (one that lets everyone in) that returns
# a read-only filesystem rooted at '/home/ftp'

authorizer = ftp_server.dummy_authorizer('/home/ftp')

# Create an ftp server using this authorizer, running on port 8021
# [the standard port is 21, but you are probably already running
#  a server there]

fs = ftp_server.ftp_server(authorizer, port=8021)

# Run the async main loop
asyncore.loop()

# to test this server, try
# $ ftp myhost 8021
# when using the standard bsd ftp client,
# $ ncftp -p 8021 myhost
# when using ncftp, and
# ftp://myhost:8021/
# from a web browser.


########NEW FILE########
__FILENAME__ = start_medusa
# -*- Mode: Python -*-

#
# Sample/Template Medusa Startup Script.
#
# This file acts as a configuration file and startup script for Medusa.
#
# You should make a copy of this file, then add, change or comment out
# appropriately.  Then you can start up the server by simply typing
#
# $ python start_medusa.py
#

import os
import sys

from supervisor.medusa import asyncore_25 as asyncore
from supervisor.medusa import http_server
from supervisor.medusa import ftp_server
from supervisor.medusa import chat_server
from supervisor.medusa import monitor
from supervisor.medusa import filesys
from supervisor.medusa import default_handler
from supervisor.medusa import status_handler
from supervisor.medusa import resolver
from supervisor.medusa import logger

if len(sys.argv) > 1:
    # process a few convenient arguments
    [HOSTNAME, IP_ADDRESS, PUBLISHING_ROOT] = sys.argv[1:]
else:
    HOSTNAME                        = 'www.nightmare.com'
    # This is the IP address of the network interface you want
    # your servers to be visible from.  This can be changed to ''
    # to listen on all interfaces.
    IP_ADDRESS                      = '205.160.176.5'

    # Root of the http and ftp server's published filesystems.
    PUBLISHING_ROOT         = '/home/www'

HTTP_PORT               = 8080 # The standard port is 80
FTP_PORT                = 8021 # The standard port is 21
CHAT_PORT               = 8888
MONITOR_PORT    = 9999

# ===========================================================================
# Caching DNS Resolver
# ===========================================================================
# The resolver is used to resolve incoming IP address (for logging),
# and also to resolve hostnames for HTTP Proxy requests.  I recommend
# using a nameserver running on the local machine, but you can also
# use a remote nameserver.

rs = resolver.caching_resolver ('127.0.0.1')

# ===========================================================================
# Logging.
# ===========================================================================

# There are several types of logging objects. Multiple loggers may be combined,
# See 'logger.py' for more details.

# This will log to stdout:
lg = logger.file_logger (sys.stdout)

# This will log to syslog:
#lg = logger.syslog_logger ('/dev/log')

# This will wrap the logger so that it will
#  1) keep track of the last 500 entries
#  2) display an entry in the status report with a hyperlink
#     to view these log entries.
#
#  If you decide to comment this out, be sure to remove the
#  logger object from the list of status objects below.
#

lg = status_handler.logger_for_status (lg)

# ===========================================================================
# Filesystem Object.
# ===========================================================================
# An abstraction for the file system.  Filesystem objects can be
# combined and implemented in interesting ways.  The default type
# simply remaps a directory to root.

fs = filesys.os_filesystem (PUBLISHING_ROOT)

# ===========================================================================
# Default HTTP handler
# ===========================================================================

# The 'default' handler for the HTTP server is one that delivers
# files normally - this is the expected behavior of a web server.
# Note that you needn't use it:  Your web server might not want to
# deliver files!

# This default handler uses the filesystem object we just constructed.

dh = default_handler.default_handler (fs)

# ===========================================================================
# HTTP Server
# ===========================================================================
hs = http_server.http_server (IP_ADDRESS, HTTP_PORT, rs, lg)

# Here we install the default handler created above.
hs.install_handler (dh)

# ===========================================================================
# Unix user `public_html' directory support
# ===========================================================================
if os.name == 'posix':
    from supervisor.medusa import unix_user_handler
    uh = unix_user_handler.unix_user_handler ('public_html')
    hs.install_handler (uh)

# ===========================================================================
# FTP Server
# ===========================================================================

# Here we create an 'anonymous' ftp server.
# Note: the ftp server is read-only by default. [in this mode, all
# 'write-capable' commands are unavailable]

ftp = ftp_server.ftp_server (
        ftp_server.anon_authorizer (
                PUBLISHING_ROOT
                ),
        ip=IP_ADDRESS,
        port=FTP_PORT,
        resolver=rs,
        logger_object=lg
        )

# ===========================================================================
# Monitor Server:
# ===========================================================================

# This creates a secure monitor server, binding to the loopback
# address on port 9999, with password 'fnord'.  The monitor server
# can be used to examine and control the server while it is running.
# If you wish to access the server from another machine, you will
# need to use '' or some other IP instead of '127.0.0.1'.
ms = monitor.secure_monitor_server ('fnord', '127.0.0.1', MONITOR_PORT)

# ===========================================================================
# Chat Server
# ===========================================================================

# The chat server is a simple IRC-like server: It is meant as a
# demonstration of how to write new servers and plug them into medusa.
# It's a very simple server (it took about 2 hours to write), but it
# could be easily extended. For example, it could be integrated with
# the web server, perhaps providing navigational tools to browse
# through a series of discussion groups, listing the number of current
# users, authentication, etc...

cs = chat_server.chat_server (IP_ADDRESS, CHAT_PORT)

# ===========================================================================
# Status Handler
# ===========================================================================

# These are objects that can report their status via the HTTP server.
# You may comment out any of these, or add more of your own.  The only
# requirement for a 'status-reporting' object is that it have a method
# 'status' that will return a producer, which will generate an HTML
# description of the status of the object.

status_objects = [
        hs,
        ftp,
        ms,
        cs,
        rs,
        lg
        ]

# Create a status handler.  By default it binds to the URI '/status'...
sh = status_handler.status_extension(status_objects)
# ... and install it on the web server.
hs.install_handler (sh)

# become 'nobody'
if os.name == 'posix':
    if hasattr (os, 'seteuid'):
        import pwd
        [uid, gid] = pwd.getpwnam ('nobody')[2:4]
        os.setegid (gid)
        os.seteuid (uid)

# Finally, start up the server loop!  This loop will not exit until
# all clients and servers are closed.  You may cleanly shut the system
# down by sending SIGINT (a.k.a. KeyboardInterrupt).
asyncore.loop()

########NEW FILE########
__FILENAME__ = winFTPserver
#
# winFTPServer.py -- FTP server that uses Win32 user API
#
# Contributed by John Abel
#
# For it to authenticate users correctly, the user running the
# script must be added to the security policy "Act As Part Of The OS".
# This is needed for the LogonUser to work.  A pain, but something that MS
# forgot to mention in the API.


import win32security, win32con, win32api, win32net
import ntsecuritycon, pywintypes
from supervisor.medusa import asyncore_25 as asyncore
from supervisor.medusa import ftp_server, filesys

class Win32Authorizer:


    def authorize (self, channel, userName, passWord):
        self.AdjustPrivilege( ntsecuritycon.SE_CHANGE_NOTIFY_NAME )
        self.AdjustPrivilege( ntsecuritycon.SE_ASSIGNPRIMARYTOKEN_NAME )
        self.AdjustPrivilege( ntsecuritycon.SE_TCB_NAME )
        try:
            logonHandle = win32security.LogonUser( userName,
                                                   None,
                                                   passWord,
                                                    win32con.LOGON32_LOGON_INTERACTIVE,
                                                    win32con.LOGON32_PROVIDER_DEFAULT )
        except pywintypes.error, ErrorMsg:
            return 0, ErrorMsg[ 2 ], None

        userInfo = win32net.NetUserGetInfo( None, userName, 1 )

        return 1, 'Login successful', filesys.os_filesystem( userInfo[ 'home_dir' ] )

    def AdjustPrivilege( self, priv ):
        flags = ntsecuritycon.TOKEN_ADJUST_PRIVILEGES | ntsecuritycon.TOKEN_QUERY
        htoken =  win32security.OpenProcessToken(win32api.GetCurrentProcess(), flags)
        id = win32security.LookupPrivilegeValue(None, priv)
        newPrivileges = [(id, ntsecuritycon.SE_PRIVILEGE_ENABLED)]
        win32security.AdjustTokenPrivileges(htoken, 0, newPrivileges)

def start_Server():
#    ftpServ = ftp_server.ftp_server( ftp_server.anon_authorizer( "D:\MyDocuments\MyDownloads"), port=21 )
    ftpServ = ftp_server.ftp_server( Win32Authorizer(), port=21 )
    asyncore.loop()

if __name__ == "__main__":
    print "Starting FTP Server"
    start_Server()

########NEW FILE########
__FILENAME__ = event_loop
# -*- Mode: Python -*-

# This is an alternative event loop that supports 'schedulable events'.
# You can specify an event callback to take place after <n> seconds.

# Important usage note: The granularity of the time-check is limited
# by the <timeout> argument to 'go()'; if there is little or no
# activity and you specify a 30-second timeout interval, then the
# schedule of events may only be checked at those 30-second intervals.
# In other words, if you need 1-second resolution, you will have to
# poll at 1-second intervals.  This facility is more useful for longer
# timeouts ("if the channel doesn't close in 5 minutes, then forcibly
# close it" would be a typical usage).

import asyncore_25 as asyncore
import bisect
import time

socket_map = asyncore.socket_map

class event_loop:

    def __init__ (self):
        self.events = []
        self.num_channels = 0
        self.max_channels = 0

    def go (self, timeout=30.0, granularity=15):
        global socket_map
        last_event_check = 0
        while socket_map:
            now = int(time.time())
            if (now - last_event_check) >= granularity:
                last_event_check = now
                fired = []
                # yuck. i want my lisp.
                i = j = 0
                while i < len(self.events):
                    when, what = self.events[i]
                    if now >= when:
                        fired.append (what)
                        j = i + 1
                    else:
                        break
                    i = i + 1
                if fired:
                    self.events = self.events[j:]
                    for what in fired:
                        what (self, now)
            # sample the number of channels
            n = len(asyncore.socket_map)
            self.num_channels = n
            if n > self.max_channels:
                self.max_channels = n
            asyncore.poll (timeout)

    def schedule (self, delta, callback):
        now = int (time.time())
        bisect.insort (self.events, (now + delta, callback))

    def __len__ (self):
        return len(self.events)

class test (asyncore.dispatcher):

    def __init__ (self):
        asyncore.dispatcher.__init__ (self)

    def handle_connect (self):
        print 'Connected!'

    def writable (self):
        return not self.connected

    def connect_timeout_callback (self, event_loop, when):
        if not self.connected:
            print 'Timeout on connect'
            self.close()

    def periodic_thing_callback (self, event_loop, when):
        print 'A Periodic Event has Occurred!'
        # re-schedule it.
        event_loop.schedule (self, 15, self.periodic_thing_callback)

if __name__ == '__main__':
    import socket
    el = event_loop()
    t = test ()
    t.create_socket (socket.AF_INET, socket.SOCK_STREAM)
    el.schedule (10, t.connect_timeout_callback)
    el.schedule (15, t.periodic_thing_callback)
    t.connect (('squirl', 80))
    el.go(1.0)

########NEW FILE########
__FILENAME__ = filesys
# -*- Mode: Python -*-
#       $Id: filesys.py,v 1.9 2003/12/24 16:10:56 akuchling Exp $
#       Author: Sam Rushing <rushing@nightmare.com>
#
# Generic filesystem interface.
#

# We want to provide a complete wrapper around any and all
# filesystem operations.

# this class is really just for documentation,
# identifying the API for a filesystem object.

# opening files for reading, and listing directories, should
# return a producer.

class abstract_filesystem:
    def __init__ (self):
        pass

    def current_directory (self):
        "Return a string representing the current directory."
        pass

    def listdir (self, path, long=0):
        """Return a listing of the directory at 'path' The empty string
        indicates the current directory.  If 'long' is set, instead
        return a list of (name, stat_info) tuples
        """
        pass

    def open (self, path, mode):
        "Return an open file object"
        pass

    def stat (self, path):
        "Return the equivalent of os.stat() on the given path."
        pass

    def isdir (self, path):
        "Does the path represent a directory?"
        pass

    def isfile (self, path):
        "Does the path represent a plain file?"
        pass

    def cwd (self, path):
        "Change the working directory."
        pass

    def cdup (self):
        "Change to the parent of the current directory."
        pass


    def longify (self, path):
        """Return a 'long' representation of the filename
        [for the output of the LIST command]"""
        pass

# standard wrapper around a unix-like filesystem, with a 'false root'
# capability.

# security considerations: can symbolic links be used to 'escape' the
# root?  should we allow it?  if not, then we could scan the
# filesystem on startup, but that would not help if they were added
# later.  We will probably need to check for symlinks in the cwd method.

# what to do if wd is an invalid directory?

import os
import stat
import re
import string

def safe_stat (path):
    try:
        return (path, os.stat (path))
    except:
        return None

import glob

class os_filesystem:
    path_module = os.path

    # set this to zero if you want to disable pathname globbing.
    # [we currently don't glob, anyway]
    do_globbing = 1

    def __init__ (self, root, wd='/'):
        self.root = root
        self.wd = wd

    def current_directory (self):
        return self.wd

    def isfile (self, path):
        p = self.normalize (self.path_module.join (self.wd, path))
        return self.path_module.isfile (self.translate(p))

    def isdir (self, path):
        p = self.normalize (self.path_module.join (self.wd, path))
        return self.path_module.isdir (self.translate(p))

    def cwd (self, path):
        p = self.normalize (self.path_module.join (self.wd, path))
        translated_path = self.translate(p)
        if not self.path_module.isdir (translated_path):
            return 0
        else:
            old_dir = os.getcwd()
            # temporarily change to that directory, in order
            # to see if we have permission to do so.
            try:
                can = 0
                try:
                    os.chdir (translated_path)
                    can = 1
                    self.wd = p
                except:
                    pass
            finally:
                if can:
                    os.chdir (old_dir)
            return can

    def cdup (self):
        return self.cwd ('..')

    def listdir (self, path, long=0):
        p = self.translate (path)
        # I think we should glob, but limit it to the current
        # directory only.
        ld = os.listdir (p)
        if not long:
            return list_producer (ld, None)
        else:
            old_dir = os.getcwd()
            try:
                os.chdir (p)
                # if os.stat fails we ignore that file.
                result = filter (None, map (safe_stat, ld))
            finally:
                os.chdir (old_dir)
            return list_producer (result, self.longify)

    # TODO: implement a cache w/timeout for stat()
    def stat (self, path):
        p = self.translate (path)
        return os.stat (p)

    def open (self, path, mode):
        p = self.translate (path)
        return open (p, mode)

    def unlink (self, path):
        p = self.translate (path)
        return os.unlink (p)

    def mkdir (self, path):
        p = self.translate (path)
        return os.mkdir (p)

    def rmdir (self, path):
        p = self.translate (path)
        return os.rmdir (p)

    def rename(self, src, dst):
        return os.rename(self.translate(src),self.translate(dst))

    # utility methods
    def normalize (self, path):
        # watch for the ever-sneaky '/+' path element
        path = re.sub('/+', '/', path)
        p = self.path_module.normpath (path)
        # remove 'dangling' cdup's.
        if len(p) > 2 and p[:3] == '/..':
            p = '/'
        return p

    def translate (self, path):
        # we need to join together three separate
        # path components, and do it safely.
        # <real_root>/<current_directory>/<path>
        # use the operating system's path separator.
        path = string.join (string.split (path, '/'), os.sep)
        p = self.normalize (self.path_module.join (self.wd, path))
        p = self.normalize (self.path_module.join (self.root, p[1:]))
        return p

    def longify (self, (path, stat_info)):
        return unix_longify (path, stat_info)

    def __repr__ (self):
        return '<unix-style fs root:%s wd:%s>' % (
                self.root,
                self.wd
                )

if os.name == 'posix':

    class unix_filesystem (os_filesystem):
        pass

    class schizophrenic_unix_filesystem (os_filesystem):
        PROCESS_UID             = os.getuid()
        PROCESS_EUID    = os.geteuid()
        PROCESS_GID             = os.getgid()
        PROCESS_EGID    = os.getegid()

        def __init__ (self, root, wd='/', persona=(None, None)):
            os_filesystem.__init__ (self, root, wd)
            self.persona = persona

        def become_persona (self):
            if self.persona is not (None, None):
                uid, gid = self.persona
                # the order of these is important!
                os.setegid (gid)
                os.seteuid (uid)

        def become_nobody (self):
            if self.persona is not (None, None):
                os.seteuid (self.PROCESS_UID)
                os.setegid (self.PROCESS_GID)

        # cwd, cdup, open, listdir
        def cwd (self, path):
            try:
                self.become_persona()
                return os_filesystem.cwd (self, path)
            finally:
                self.become_nobody()

        def cdup (self, path):
            try:
                self.become_persona()
                return os_filesystem.cdup (self)
            finally:
                self.become_nobody()

        def open (self, filename, mode):
            try:
                self.become_persona()
                return os_filesystem.open (self, filename, mode)
            finally:
                self.become_nobody()

        def listdir (self, path, long=0):
            try:
                self.become_persona()
                return os_filesystem.listdir (self, path, long)
            finally:
                self.become_nobody()

# For the 'real' root, we could obtain a list of drives, and then
# use that.  Doesn't win32 provide such a 'real' filesystem?
# [yes, I think something like this "\\.\c\windows"]

class msdos_filesystem (os_filesystem):
    def longify (self, (path, stat_info)):
        return msdos_longify (path, stat_info)

# A merged filesystem will let you plug other filesystems together.
# We really need the equivalent of a 'mount' capability - this seems
# to be the most general idea.  So you'd use a 'mount' method to place
# another filesystem somewhere in the hierarchy.

# Note: this is most likely how I will handle ~user directories
# with the http server.

class merged_filesystem:
    def __init__ (self, *fsys):
        pass

# this matches the output of NT's ftp server (when in
# MSDOS mode) exactly.

def msdos_longify (file, stat_info):
    if stat.S_ISDIR (stat_info[stat.ST_MODE]):
        dir = '<DIR>'
    else:
        dir = '     '
    date = msdos_date (stat_info[stat.ST_MTIME])
    return '%s       %s %8d %s' % (
            date,
            dir,
            stat_info[stat.ST_SIZE],
            file
            )

def msdos_date (t):
    try:
        info = time.gmtime (t)
    except:
        info = time.gmtime (0)
    # year, month, day, hour, minute, second, ...
    hour = info[3]
    if hour > 11:
        merid = 'PM'
        hour = hour - 12
    else:
        merid = 'AM'
    return '%02d-%02d-%02d  %02d:%02d%s' % (
            info[1],
            info[2],
            info[0]%100,
            hour,
            info[4],
            merid
            )

months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']

mode_table = {
        '0':'---',
        '1':'--x',
        '2':'-w-',
        '3':'-wx',
        '4':'r--',
        '5':'r-x',
        '6':'rw-',
        '7':'rwx'
        }

import time

def unix_longify (file, stat_info):
    # for now, only pay attention to the lower bits
    mode = ('%o' % stat_info[stat.ST_MODE])[-3:]
    mode = string.join (map (lambda x: mode_table[x], mode), '')
    if stat.S_ISDIR (stat_info[stat.ST_MODE]):
        dirchar = 'd'
    else:
        dirchar = '-'
    date = ls_date (long(time.time()), stat_info[stat.ST_MTIME])
    return '%s%s %3d %-8d %-8d %8d %s %s' % (
            dirchar,
            mode,
            stat_info[stat.ST_NLINK],
            stat_info[stat.ST_UID],
            stat_info[stat.ST_GID],
            stat_info[stat.ST_SIZE],
            date,
            file
            )

# Emulate the unix 'ls' command's date field.
# it has two formats - if the date is more than 180
# days in the past, then it's like this:
# Oct 19  1995
# otherwise, it looks like this:
# Oct 19 17:33

def ls_date (now, t):
    try:
        info = time.gmtime (t)
    except:
        info = time.gmtime (0)
    # 15,600,000 == 86,400 * 180
    if (now - t) > 15600000:
        return '%s %2d  %d' % (
                months[info[1]-1],
                info[2],
                info[0]
                )
    else:
        return '%s %2d %02d:%02d' % (
                months[info[1]-1],
                info[2],
                info[3],
                info[4]
                )

# ===========================================================================
# Producers
# ===========================================================================

class list_producer:
    def __init__ (self, list, func=None):
        self.list = list
        self.func = func

    # this should do a pushd/popd
    def more (self):
        if not self.list:
            return ''
        else:
            # do a few at a time
            bunch = self.list[:50]
            if self.func is not None:
                bunch = map (self.func, bunch)
            self.list = self.list[50:]
            return string.joinfields (bunch, '\r\n') + '\r\n'


########NEW FILE########
__FILENAME__ = ftp_server
# -*- Mode: Python -*-

#       Author: Sam Rushing <rushing@nightmare.com>
#       Copyright 1996-2000 by Sam Rushing
#                                                All Rights Reserved.
#

RCS_ID =  '$Id: ftp_server.py,v 1.11 2003/12/24 16:05:28 akuchling Exp $'

# An extensible, configurable, asynchronous FTP server.
#
# All socket I/O is non-blocking, however file I/O is currently
# blocking.  Eventually file I/O may be made non-blocking, too, if it
# seems necessary.  Currently the only CPU-intensive operation is
# getting and formatting a directory listing.  [this could be moved
# into another process/directory server, or another thread?]
#
# Only a subset of RFC 959 is implemented, but much of that RFC is
# vestigial anyway.  I've attempted to include the most commonly-used
# commands, using the feature set of wu-ftpd as a guide.

import asyncore_25 as asyncore
import asynchat_25 as asynchat

import os
import socket
import stat
import string
import sys
import time

from supervisor.medusa.producers import file_producer

# TODO: implement a directory listing cache.  On very-high-load
# servers this could save a lot of disk abuse, and possibly the
# work of computing emulated unix ls output.

# Potential security problem with the FTP protocol?  I don't think
# there's any verification of the origin of a data connection.  Not
# really a problem for the server (since it doesn't send the port
# command, except when in PASV mode) But I think a data connection
# could be spoofed by a program with access to a sniffer - it could
# watch for a PORT command to go over a command channel, and then
# connect to that port before the server does.

# Unix user id's:
# In order to support assuming the id of a particular user,
# it seems there are two options:
# 1) fork, and seteuid in the child
# 2) carefully control the effective uid around filesystem accessing
#    methods, using try/finally. [this seems to work]

VERSION = string.split(RCS_ID)[2]

from counter import counter
import producers
import status_handler
import logger

class ftp_channel (asynchat.async_chat):

    # defaults for a reliable __repr__
    addr = ('unknown','0')

    # unset this in a derived class in order
    # to enable the commands in 'self.write_commands'
    read_only = 1
    write_commands = ['appe','dele','mkd','rmd','rnfr','rnto','stor','stou']

    restart_position = 0

    # comply with (possibly troublesome) RFC959 requirements
    # This is necessary to correctly run an active data connection
    # through a firewall that triggers on the source port (expected
    # to be 'L-1', or 20 in the normal case).
    bind_local_minus_one = 0

    def __init__ (self, server, conn, addr):
        self.server = server
        self.current_mode = 'a'
        self.addr = addr
        asynchat.async_chat.__init__ (self, conn)
        self.set_terminator ('\r\n')

        # client data port.  Defaults to 'the same as the control connection'.
        self.client_addr = (addr[0], 21)

        self.client_dc = None
        self.in_buffer = ''
        self.closing = 0
        self.passive_acceptor = None
        self.passive_connection = None
        self.filesystem = None
        self.authorized = 0
        # send the greeting
        self.respond (
                '220 %s FTP server (Medusa Async V%s [experimental]) ready.' % (
                        self.server.hostname,
                        VERSION
                        )
                )

#       def __del__ (self):
#               print 'ftp_channel.__del__()'

    # --------------------------------------------------
    # async-library methods
    # --------------------------------------------------

    def handle_expt (self):
        # this is handled below.  not sure what I could
        # do here to make that code less kludgish.
        pass

    def collect_incoming_data (self, data):
        self.in_buffer = self.in_buffer + data
        if len(self.in_buffer) > 4096:
            # silently truncate really long lines
            # (possible denial-of-service attack)
            self.in_buffer = ''

    def found_terminator (self):

        line = self.in_buffer

        if not len(line):
            return

        sp = string.find (line, ' ')
        if sp != -1:
            line = [line[:sp], line[sp+1:]]
        else:
            line = [line]

        command = string.lower (line[0])
        # watch especially for 'urgent' abort commands.
        if string.find (command, 'abor') != -1:
            # strip off telnet sync chars and the like...
            while command and command[0] not in string.letters:
                command = command[1:]
        fun_name = 'cmd_%s' % command
        if command != 'pass':
            self.log ('<== %s' % repr(self.in_buffer)[1:-1])
        else:
            self.log ('<== %s' % line[0]+' <password>')
        self.in_buffer = ''
        if not hasattr (self, fun_name):
            self.command_not_understood (line[0])
            return
        if hasattr(self,'_rnfr_src') and fun_name!='cmd_rnto':
            del self._rnfr_src
            self.respond ('503 RNTO Command expected!')
            return

        fun = getattr (self, fun_name)
        if (not self.authorized) and (command not in ('user', 'pass', 'help', 'quit')):
            self.respond ('530 Please log in with USER and PASS')
        elif (not self.check_command_authorization (command)):
            self.command_not_authorized (command)
        else:
            try:
                result = apply (fun, (line,))
            except:
                self.server.total_exceptions.increment()
                (file, fun, line), t,v, tbinfo = asyncore.compact_traceback()
                if self.client_dc:
                    try:
                        self.client_dc.close()
                    except:
                        pass
                self.respond (
                        '451 Server Error: %s, %s: file: %s line: %s' % (
                                t,v,file,line,
                                )
                        )

    closed = 0
    def close (self):
        if not self.closed:
            self.closed = 1
            if self.passive_acceptor:
                self.passive_acceptor.close()
            if self.client_dc:
                self.client_dc.close()
            self.server.closed_sessions.increment()
            asynchat.async_chat.close (self)

    # --------------------------------------------------
    # filesystem interface functions.
    # override these to provide access control or perform
    # other functions.
    # --------------------------------------------------

    def cwd (self, line):
        return self.filesystem.cwd (line[1])

    def cdup (self, line):
        return self.filesystem.cdup()

    def open (self, path, mode):
        return self.filesystem.open (path, mode)

    # returns a producer
    def listdir (self, path, long=0):
        return self.filesystem.listdir (path, long)

    def get_dir_list (self, line, long=0):
        # we need to scan the command line for arguments to '/bin/ls'...
        args = line[1:]
        path_args = []
        for arg in args:
            if arg[0] != '-':
                path_args.append (arg)
            else:
                # ignore arguments
                pass
        if len(path_args) < 1:
            dir = '.'
        else:
            dir = path_args[0]
        return self.listdir (dir, long)

    # --------------------------------------------------
    # authorization methods
    # --------------------------------------------------

    def check_command_authorization (self, command):
        if command in self.write_commands and self.read_only:
            return 0
        else:
            return 1

    # --------------------------------------------------
    # utility methods
    # --------------------------------------------------

    def log (self, message):
        self.server.logger.log (
                self.addr[0],
                '%d %s' % (
                        self.addr[1], message
                        )
                )

    def respond (self, resp):
        self.log ('==> %s' % resp)
        self.push (resp + '\r\n')

    def command_not_understood (self, command):
        self.respond ("500 '%s': command not understood." % command)

    def command_not_authorized (self, command):
        self.respond (
                "530 You are not authorized to perform the '%s' command" % (
                        command
                        )
                )

    def make_xmit_channel (self):
        # In PASV mode, the connection may or may _not_ have been made
        # yet.  [although in most cases it is... FTP Explorer being
        # the only exception I've yet seen].  This gets somewhat confusing
        # because things may happen in any order...
        pa = self.passive_acceptor
        if pa:
            if pa.ready:
                # a connection has already been made.
                conn, addr = self.passive_acceptor.ready
                cdc = xmit_channel (self, addr)
                cdc.set_socket (conn)
                cdc.connected = 1
                self.passive_acceptor.close()
                self.passive_acceptor = None
            else:
                # we're still waiting for a connect to the PASV port.
                cdc = xmit_channel (self)
        else:
            # not in PASV mode.
            ip, port = self.client_addr
            cdc = xmit_channel (self, self.client_addr)
            cdc.create_socket (socket.AF_INET, socket.SOCK_STREAM)
            if self.bind_local_minus_one:
                cdc.bind (('', self.server.port - 1))
            try:
                cdc.connect ((ip, port))
            except socket.error, why:
                self.respond ("425 Can't build data connection")
        self.client_dc = cdc

    # pretty much the same as xmit, but only right on the verge of
    # being worth a merge.
    def make_recv_channel (self, fd):
        pa = self.passive_acceptor
        if pa:
            if pa.ready:
                # a connection has already been made.
                conn, addr = pa.ready
                cdc = recv_channel (self, addr, fd)
                cdc.set_socket (conn)
                cdc.connected = 1
                self.passive_acceptor.close()
                self.passive_acceptor = None
            else:
                # we're still waiting for a connect to the PASV port.
                cdc = recv_channel (self, None, fd)
        else:
            # not in PASV mode.
            ip, port = self.client_addr
            cdc = recv_channel (self, self.client_addr, fd)
            cdc.create_socket (socket.AF_INET, socket.SOCK_STREAM)
            try:
                cdc.connect ((ip, port))
            except socket.error, why:
                self.respond ("425 Can't build data connection")
        self.client_dc = cdc

    type_map = {
            'a':'ASCII',
            'i':'Binary',
            'e':'EBCDIC',
            'l':'Binary'
            }

    type_mode_map = {
            'a':'t',
            'i':'b',
            'e':'b',
            'l':'b'
            }

    # --------------------------------------------------
    # command methods
    # --------------------------------------------------

    def cmd_type (self, line):
        'specify data transfer type'
        # ascii, ebcdic, image, local <byte size>
        t = string.lower (line[1])
        # no support for EBCDIC
        # if t not in ['a','e','i','l']:
        if t not in ['a','i','l']:
            self.command_not_understood (string.join (line))
        elif t == 'l' and (len(line) > 2 and line[2] != '8'):
            self.respond ('504 Byte size must be 8')
        else:
            self.current_mode = t
            self.respond ('200 Type set to %s.' % self.type_map[t])


    def cmd_quit (self, line):
        'terminate session'
        self.respond ('221 Goodbye.')
        self.close_when_done()

    def cmd_port (self, line):
        'specify data connection port'
        info = string.split (line[1], ',')
        ip = string.join (info[:4], '.')
        port = string.atoi(info[4])*256 + string.atoi(info[5])
        # how many data connections at a time?
        # I'm assuming one for now...
        # TODO: we should (optionally) verify that the
        # ip number belongs to the client.  [wu-ftpd does this?]
        self.client_addr = (ip, port)
        self.respond ('200 PORT command successful.')

    def new_passive_acceptor (self):
        # ensure that only one of these exists at a time.
        if self.passive_acceptor is not None:
            self.passive_acceptor.close()
            self.passive_acceptor = None
        self.passive_acceptor = passive_acceptor (self)
        return self.passive_acceptor

    def cmd_pasv (self, line):
        'prepare for server-to-server transfer'
        pc = self.new_passive_acceptor()
        port = pc.addr[1]
        ip_addr = pc.control_channel.getsockname()[0]
        self.respond (
                '227 Entering Passive Mode (%s,%d,%d)' % (
                        string.replace(ip_addr, '.', ','),
                        port/256,
                        port%256
                        )
                )
        self.client_dc = None

    def cmd_nlst (self, line):
        'give name list of files in directory'
        # ncftp adds the -FC argument for the user-visible 'nlist'
        # command.  We could try to emulate ls flags, but not just yet.
        if '-FC' in line:
            line.remove ('-FC')
        try:
            dir_list_producer = self.get_dir_list (line, 0)
        except os.error, why:
            self.respond ('550 Could not list directory: %s' % why)
            return
        self.respond (
                '150 Opening %s mode data connection for file list' % (
                        self.type_map[self.current_mode]
                        )
                )
        self.make_xmit_channel()
        self.client_dc.push_with_producer (dir_list_producer)
        self.client_dc.close_when_done()

    def cmd_list (self, line):
        'give a list of files in a directory'
        try:
            dir_list_producer = self.get_dir_list (line, 1)
        except os.error, why:
            self.respond ('550 Could not list directory: %s' % why)
            return
        self.respond (
                '150 Opening %s mode data connection for file list' % (
                        self.type_map[self.current_mode]
                        )
                )
        self.make_xmit_channel()
        self.client_dc.push_with_producer (dir_list_producer)
        self.client_dc.close_when_done()

    def cmd_cwd (self, line):
        'change working directory'
        if self.cwd (line):
            self.respond ('250 CWD command successful.')
        else:
            self.respond ('550 No such directory.')

    def cmd_cdup (self, line):
        'change to parent of current working directory'
        if self.cdup(line):
            self.respond ('250 CDUP command successful.')
        else:
            self.respond ('550 No such directory.')

    def cmd_pwd (self, line):
        'print the current working directory'
        self.respond (
                '257 "%s" is the current directory.' % (
                        self.filesystem.current_directory()
                        )
                )

    # modification time
    # example output:
    # 213 19960301204320
    def cmd_mdtm (self, line):
        'show last modification time of file'
        filename = line[1]
        if not self.filesystem.isfile (filename):
            self.respond ('550 "%s" is not a file' % filename)
        else:
            mtime = time.gmtime(self.filesystem.stat(filename)[stat.ST_MTIME])
            self.respond (
                    '213 %4d%02d%02d%02d%02d%02d' % (
                            mtime[0],
                            mtime[1],
                            mtime[2],
                            mtime[3],
                            mtime[4],
                            mtime[5]
                            )
                    )

    def cmd_noop (self, line):
        'do nothing'
        self.respond ('200 NOOP command successful.')

    def cmd_size (self, line):
        'return size of file'
        filename = line[1]
        if not self.filesystem.isfile (filename):
            self.respond ('550 "%s" is not a file' % filename)
        else:
            self.respond (
                    '213 %d' % (self.filesystem.stat(filename)[stat.ST_SIZE])
                    )

    def cmd_retr (self, line):
        'retrieve a file'
        if len(line) < 2:
            self.command_not_understood (string.join (line))
        else:
            file = line[1]
            if not self.filesystem.isfile (file):
                self.log_info ('checking %s' % file)
                self.respond ('550 No such file')
            else:
                try:
                    # FIXME: for some reason, 'rt' isn't working on win95
                    mode = 'r'+self.type_mode_map[self.current_mode]
                    fd = self.open (file, mode)
                except IOError, why:
                    self.respond ('553 could not open file for reading: %s' % (repr(why)))
                    return
                self.respond (
                        "150 Opening %s mode data connection for file '%s'" % (
                                self.type_map[self.current_mode],
                                file
                                )
                        )
                self.make_xmit_channel()

                if self.restart_position:
                    # try to position the file as requested, but
                    # give up silently on failure (the 'file object'
                    # may not support seek())
                    try:
                        fd.seek (self.restart_position)
                    except:
                        pass
                    self.restart_position = 0

                self.client_dc.push_with_producer (
                        file_producer (fd)
                        )
                self.client_dc.close_when_done()

    def cmd_stor (self, line, mode='wb'):
        'store a file'
        if len (line) < 2:
            self.command_not_understood (string.join (line))
        else:
            if self.restart_position:
                restart_position = 0
                self.respond ('553 restart on STOR not yet supported')
                return
            file = line[1]
            # todo: handle that type flag
            try:
                fd = self.open (file, mode)
            except IOError, why:
                self.respond ('553 could not open file for writing: %s' % (repr(why)))
                return
            self.respond (
                    '150 Opening %s connection for %s' % (
                            self.type_map[self.current_mode],
                            file
                            )
                    )
            self.make_recv_channel (fd)

    def cmd_abor (self, line):
        'abort operation'
        if self.client_dc:
            self.client_dc.close()
        self.respond ('226 ABOR command successful.')

    def cmd_appe (self, line):
        'append to a file'
        return self.cmd_stor (line, 'ab')

    def cmd_dele (self, line):
        if len (line) != 2:
            self.command_not_understood (string.join (line))
        else:
            file = line[1]
            if self.filesystem.isfile (file):
                try:
                    self.filesystem.unlink (file)
                    self.respond ('250 DELE command successful.')
                except:
                    self.respond ('550 error deleting file.')
            else:
                self.respond ('550 %s: No such file.' % file)

    def cmd_mkd (self, line):
        if len (line) != 2:
            self.command_not_understood (string.join (line))
        else:
            path = line[1]
            try:
                self.filesystem.mkdir (path)
                self.respond ('257 MKD command successful.')
            except:
                self.respond ('550 error creating directory.')

    def cmd_rnfr (self, line):
        if not hasattr(self.filesystem,'rename'):
            self.respond('502 RNFR not implemented.' % src)
            return

        if len(line)!=2:
            self.command_not_understood (string.join (line))
        else:
            src = line[1]
            try:
                assert self.filesystem.isfile(src)
                self._rfnr_src = src
                self.respond('350 RNFR file exists, ready for destination name.')
            except:
                self.respond('550 %s: No such file.' % src)

    def cmd_rnto (self, line):
        src = getattr(self,'_rfnr_src',None)
        if not src:
            self.respond('503 RNTO command unexpected.')
            return

        if len(line)!=2:
            self.command_not_understood (string.join (line))
        else:
            dst = line[1]
            try:
                self.filesystem.rename(src,dst)
                self.respond('250 RNTO command successful.')
            except:
                t, v = sys.exc_info[:2]
                self.respond('550 %s: %s.' % (str(t),str(v)))
        try:
            del self._rfnr_src
        except:
            pass

    def cmd_rmd (self, line):
        if len (line) != 2:
            self.command_not_understood (string.join (line))
        else:
            path = line[1]
            try:
                self.filesystem.rmdir (path)
                self.respond ('250 RMD command successful.')
            except:
                self.respond ('550 error removing directory.')

    def cmd_user (self, line):
        'specify user name'
        if len(line) > 1:
            self.user = line[1]
            self.respond ('331 Password required.')
        else:
            self.command_not_understood (string.join (line))

    def cmd_pass (self, line):
        'specify password'
        if len(line) < 2:
            pw = ''
        else:
            pw = line[1]
        result, message, fs = self.server.authorizer.authorize (self, self.user, pw)
        if result:
            self.respond ('230 %s' % message)
            self.filesystem = fs
            self.authorized = 1
            self.log_info('Successful login: Filesystem=%s' % repr(fs))
        else:
            self.respond ('530 %s' % message)

    def cmd_rest (self, line):
        'restart incomplete transfer'
        try:
            pos = string.atoi (line[1])
        except ValueError:
            self.command_not_understood (string.join (line))
        self.restart_position = pos
        self.respond (
                '350 Restarting at %d. Send STORE or RETRIEVE to initiate transfer.' % pos
                )

    def cmd_stru (self, line):
        'obsolete - set file transfer structure'
        if line[1] in 'fF':
            # f == 'file'
            self.respond ('200 STRU F Ok')
        else:
            self.respond ('504 Unimplemented STRU type')

    def cmd_mode (self, line):
        'obsolete - set file transfer mode'
        if line[1] in 'sS':
            # f == 'file'
            self.respond ('200 MODE S Ok')
        else:
            self.respond ('502 Unimplemented MODE type')

# The stat command has two personalities.  Normally it returns status
# information about the current connection.  But if given an argument,
# it is equivalent to the LIST command, with the data sent over the
# control connection.  Strange.  But wuftpd, ftpd, and nt's ftp server
# all support it.
#
##      def cmd_stat (self, line):
##              'return status of server'
##              pass

    def cmd_syst (self, line):
        'show operating system type of server system'
        # Replying to this command is of questionable utility, because
        # this server does not behave in a predictable way w.r.t. the
        # output of the LIST command.  We emulate Unix ls output, but
        # on win32 the pathname can contain drive information at the front
        # Currently, the combination of ensuring that os.sep == '/'
        # and removing the leading slash when necessary seems to work.
        # [cd'ing to another drive also works]
        #
        # This is how wuftpd responds, and is probably
        # the most expected.  The main purpose of this reply is so that
        # the client knows to expect Unix ls-style LIST output.
        self.respond ('215 UNIX Type: L8')
        # one disadvantage to this is that some client programs
        # assume they can pass args to /bin/ls.
        # a few typical responses:
        # 215 UNIX Type: L8 (wuftpd)
        # 215 Windows_NT version 3.51
        # 215 VMS MultiNet V3.3
        # 500 'SYST': command not understood. (SVR4)

    def cmd_help (self, line):
        'give help information'
        # find all the methods that match 'cmd_xxxx',
        # use their docstrings for the help response.
        attrs = dir(self.__class__)
        help_lines = []
        for attr in attrs:
            if attr[:4] == 'cmd_':
                x = getattr (self, attr)
                if type(x) == type(self.cmd_help):
                    if x.__doc__:
                        help_lines.append ('\t%s\t%s' % (attr[4:], x.__doc__))
        if help_lines:
            self.push ('214-The following commands are recognized\r\n')
            self.push_with_producer (producers.lines_producer (help_lines))
            self.push ('214\r\n')
        else:
            self.push ('214-\r\n\tHelp Unavailable\r\n214\r\n')

class ftp_server (asyncore.dispatcher):
    # override this to spawn a different FTP channel class.
    ftp_channel_class = ftp_channel

    SERVER_IDENT = 'FTP Server (V%s)' % VERSION

    def __init__ (
            self,
            authorizer,
            hostname        =None,
            ip              ='',
            port            =21,
            resolver        =None,
            logger_object=logger.file_logger (sys.stdout)
            ):
        self.ip = ip
        self.port = port
        self.authorizer = authorizer

        if hostname is None:
            self.hostname = socket.gethostname()
        else:
            self.hostname = hostname

        # statistics
        self.total_sessions = counter()
        self.closed_sessions = counter()
        self.total_files_out = counter()
        self.total_files_in = counter()
        self.total_bytes_out = counter()
        self.total_bytes_in = counter()
        self.total_exceptions = counter()
        #
        asyncore.dispatcher.__init__ (self)
        self.create_socket (socket.AF_INET, socket.SOCK_STREAM)

        self.set_reuse_addr()
        self.bind ((self.ip, self.port))
        self.listen (5)

        if not logger_object:
            logger_object = sys.stdout

        if resolver:
            self.logger = logger.resolving_logger (resolver, logger_object)
        else:
            self.logger = logger.unresolving_logger (logger_object)

        self.log_info('FTP server started at %s\n\tAuthorizer:%s\n\tHostname: %s\n\tPort: %d' % (
                time.ctime(time.time()),
                repr (self.authorizer),
                self.hostname,
                self.port)
                )

    def writable (self):
        return 0

    def handle_read (self):
        pass

    def handle_connect (self):
        pass

    def handle_accept (self):
        conn, addr = self.accept()
        self.total_sessions.increment()
        self.log_info('Incoming connection from %s:%d' % (addr[0], addr[1]))
        self.ftp_channel_class (self, conn, addr)

    # return a producer describing the state of the server
    def status (self):

        def nice_bytes (n):
            return string.join (status_handler.english_bytes (n))

        return producers.lines_producer (
                ['<h2>%s</h2>'                          % self.SERVER_IDENT,
                 '<br>Listening on <b>Host:</b> %s' % self.hostname,
                 '<b>Port:</b> %d'                      % self.port,
                 '<br>Sessions',
                 '<b>Total:</b> %s'                     % self.total_sessions,
                 '<b>Current:</b> %d'           % (self.total_sessions.as_long() - self.closed_sessions.as_long()),
                 '<br>Files',
                 '<b>Sent:</b> %s'                      % self.total_files_out,
                 '<b>Received:</b> %s'          % self.total_files_in,
                 '<br>Bytes',
                 '<b>Sent:</b> %s'                      % nice_bytes (self.total_bytes_out.as_long()),
                 '<b>Received:</b> %s'          % nice_bytes (self.total_bytes_in.as_long()),
                 '<br>Exceptions: %s'           % self.total_exceptions,
                 ]
                )

# ======================================================================
#                                                Data Channel Classes
# ======================================================================

# This socket accepts a data connection, used when the server has been
# placed in passive mode.  Although the RFC implies that we ought to
# be able to use the same acceptor over and over again, this presents
# a problem: how do we shut it off, so that we are accepting
# connections only when we expect them?  [we can't]
#
# wuftpd, and probably all the other servers, solve this by allowing
# only one connection to hit this acceptor.  They then close it.  Any
# subsequent data-connection command will then try for the default
# port on the client side [which is of course never there].  So the
# 'always-send-PORT/PASV' behavior seems required.
#
# Another note: wuftpd will also be listening on the channel as soon
# as the PASV command is sent.  It does not wait for a data command
# first.

# --- we need to queue up a particular behavior:
#  1) xmit : queue up producer[s]
#  2) recv : the file object
#
# It would be nice if we could make both channels the same.  Hmmm..
#

class passive_acceptor (asyncore.dispatcher):
    ready = None

    def __init__ (self, control_channel):
        # connect_fun (conn, addr)
        asyncore.dispatcher.__init__ (self)
        self.control_channel = control_channel
        self.create_socket (socket.AF_INET, socket.SOCK_STREAM)
        # bind to an address on the interface that the
        # control connection is coming from.
        self.bind ((
                self.control_channel.getsockname()[0],
                0
                ))
        self.addr = self.getsockname()
        self.listen (1)

#       def __del__ (self):
#               print 'passive_acceptor.__del__()'

    def log (self, *ignore):
        pass

    def handle_accept (self):
        conn, addr = self.accept()
        dc = self.control_channel.client_dc
        if dc is not None:
            dc.set_socket (conn)
            dc.addr = addr
            dc.connected = 1
            self.control_channel.passive_acceptor = None
        else:
            self.ready = conn, addr
        self.close()


class xmit_channel (asynchat.async_chat):

    # for an ethernet, you want this to be fairly large, in fact, it
    # _must_ be large for performance comparable to an ftpd.  [64k] we
    # ought to investigate automatically-sized buffers...

    ac_out_buffer_size = 16384
    bytes_out = 0

    def __init__ (self, channel, client_addr=None):
        self.channel = channel
        self.client_addr = client_addr
        asynchat.async_chat.__init__ (self)

#       def __del__ (self):
#               print 'xmit_channel.__del__()'

    def log (self, *args):
        pass

    def readable (self):
        return not self.connected

    def writable (self):
        return 1

    def send (self, data):
        result = asynchat.async_chat.send (self, data)
        self.bytes_out = self.bytes_out + result
        return result

    def handle_error (self):
        # usually this is to catch an unexpected disconnect.
        self.log_info ('unexpected disconnect on data xmit channel', 'error')
        try:
            self.close()
        except:
            pass

    # TODO: there's a better way to do this.  we need to be able to
    # put 'events' in the producer fifo.  to do this cleanly we need
    # to reposition the 'producer' fifo as an 'event' fifo.

    def close (self):
        c = self.channel
        s = c.server
        c.client_dc = None
        s.total_files_out.increment()
        s.total_bytes_out.increment (self.bytes_out)
        if not len(self.producer_fifo):
            c.respond ('226 Transfer complete')
        elif not c.closed:
            c.respond ('426 Connection closed; transfer aborted')
        del c
        del s
        del self.channel
        asynchat.async_chat.close (self)

class recv_channel (asyncore.dispatcher):
    def __init__ (self, channel, client_addr, fd):
        self.channel = channel
        self.client_addr = client_addr
        self.fd = fd
        asyncore.dispatcher.__init__ (self)
        self.bytes_in = counter()

    def log (self, *ignore):
        pass

    def handle_connect (self):
        pass

    def writable (self):
        return 0

    def recv (*args):
        result = apply (asyncore.dispatcher.recv, args)
        self = args[0]
        self.bytes_in.increment(len(result))
        return result

    buffer_size = 8192

    def handle_read (self):
        block = self.recv (self.buffer_size)
        if block:
            try:
                self.fd.write (block)
            except IOError:
                self.log_info ('got exception writing block...', 'error')

    def handle_close (self):
        s = self.channel.server
        s.total_files_in.increment()
        s.total_bytes_in.increment(self.bytes_in.as_long())
        self.fd.close()
        self.channel.respond ('226 Transfer complete.')
        self.close()

import filesys

# not much of a doorman! 8^)
class dummy_authorizer:
    def __init__ (self, root='/'):
        self.root = root
    def authorize (self, channel, username, password):
        channel.persona = -1, -1
        channel.read_only = 1
        return 1, 'Ok.', filesys.os_filesystem (self.root)

class anon_authorizer:
    def __init__ (self, root='/'):
        self.root = root

    def authorize (self, channel, username, password):
        if username in ('ftp', 'anonymous'):
            channel.persona = -1, -1
            channel.read_only = 1
            return 1, 'Ok.', filesys.os_filesystem (self.root)
        else:
            return 0, 'Password invalid.', None

# ===========================================================================
# Unix-specific improvements
# ===========================================================================

if os.name == 'posix':

    class unix_authorizer:
        # return a trio of (success, reply_string, filesystem)
        def authorize (self, channel, username, password):
            import crypt
            import pwd
            try:
                info = pwd.getpwnam (username)
            except KeyError:
                return 0, 'No such user.', None
            mangled = info[1]
            if crypt.crypt (password, mangled[:2]) == mangled:
                channel.read_only = 0
                fs = filesys.schizophrenic_unix_filesystem (
                        '/',
                        info[5],
                        persona = (info[2], info[3])
                        )
                return 1, 'Login successful.', fs
            else:
                return 0, 'Password invalid.', None

        def __repr__ (self):
            return '<standard unix authorizer>'

    # simple anonymous ftp support
    class unix_authorizer_with_anonymous (unix_authorizer):
        def __init__ (self, root=None, real_users=0):
            self.root = root
            self.real_users = real_users

        def authorize (self, channel, username, password):
            if string.lower(username) in ['anonymous', 'ftp']:
                import pwd
                try:
                    # ok, here we run into lots of confusion.
                    # on some os', anon runs under user 'nobody',
                    # on others as 'ftp'.  ownership is also critical.
                    # need to investigate.
                    # linux: new linuxen seem to have nobody's UID=-1,
                    #    which is an illegal value.  Use ftp.
                    ftp_user_info = pwd.getpwnam ('ftp')
                    if string.lower(os.uname()[0]) == 'linux':
                        nobody_user_info = pwd.getpwnam ('ftp')
                    else:
                        nobody_user_info = pwd.getpwnam ('nobody')
                    channel.read_only = 1
                    if self.root is None:
                        self.root = ftp_user_info[5]
                    fs = filesys.unix_filesystem (self.root, '/')
                    return 1, 'Anonymous Login Successful', fs
                except KeyError:
                    return 0, 'Anonymous account not set up', None
            elif self.real_users:
                return unix_authorizer.authorize (
                        self,
                        channel,
                        username,
                        password
                        )
            else:
                return 0, 'User logins not allowed', None

# usage: ftp_server /PATH/TO/FTP/ROOT PORT
# for example:
# $ ftp_server /home/users/ftp 8021

if os.name == 'posix':
    def test (port='8021'):
        fs = ftp_server (
                unix_authorizer(),
                port=string.atoi (port)
                )
        try:
            asyncore.loop()
        except KeyboardInterrupt:
            fs.log_info('FTP server shutting down. (received SIGINT)', 'warning')
            # close everything down on SIGINT.
            # of course this should be a cleaner shutdown.
            asyncore.close_all()

    if __name__ == '__main__':
        test (sys.argv[1])
# not unix
else:
    def test ():
        fs = ftp_server (dummy_authorizer())
    if __name__ == '__main__':
        test ()

# this is the command list from the wuftpd man page
# '*' means we've implemented it.
# '!' requires write access
#
command_documentation = {
        'abor': 'abort previous command',                                                       #*
        'acct': 'specify account (ignored)',
        'allo': 'allocate storage (vacuously)',
        'appe': 'append to a file',                                                                     #*!
        'cdup': 'change to parent of current working directory',        #*
        'cwd':  'change working directory',                                                     #*
        'dele': 'delete a file',                                                                        #!
        'help': 'give help information',                                                        #*
        'list': 'give list files in a directory',                                       #*
        'mkd':  'make a directory',                                                                     #!
        'mdtm': 'show last modification time of file',                          #*
        'mode': 'specify data transfer mode',
        'nlst': 'give name list of files in directory',                         #*
        'noop': 'do nothing',                                                                           #*
        'pass': 'specify password',                                                                     #*
        'pasv': 'prepare for server-to-server transfer',                        #*
        'port': 'specify data connection port',                                         #*
        'pwd':  'print the current working directory',                          #*
        'quit': 'terminate session',                                                            #*
        'rest': 'restart incomplete transfer',                                          #*
        'retr': 'retrieve a file',                                                                      #*
        'rmd':  'remove a directory',                                                           #!
        'rnfr': 'specify rename-from file name',                                        #*!
        'rnto': 'specify rename-to file name',                                          #*!
        'site': 'non-standard commands (see next section)',
        'size': 'return size of file',                                                          #*
        'stat': 'return status of server',                                                      #*
        'stor': 'store a file',                                                                         #*!
        'stou': 'store a file with a unique name',                                      #!
        'stru': 'specify data transfer structure',
        'syst': 'show operating system type of server system',          #*
        'type': 'specify data transfer type',                                           #*
        'user': 'specify user name',                                                            #*
        'xcup': 'change to parent of current working directory (deprecated)',
        'xcwd': 'change working directory (deprecated)',
        'xmkd': 'make a directory (deprecated)',                                        #!
        'xpwd': 'print the current working directory (deprecated)',
        'xrmd': 'remove a directory (deprecated)',                                      #!
}


# debugging aid (linux)
def get_vm_size ():
    return string.atoi (string.split(open ('/proc/self/stat').readline())[22])

def print_vm():
    print 'vm: %8dk' % (get_vm_size()/1024)

########NEW FILE########
__FILENAME__ = http_date
# -*- Mode: Python -*-

import re
import string
import time

def concat (*args):
    return ''.join (args)

def join (seq, field=' '):
    return field.join (seq)

def group (s):
    return '(' + s + ')'

short_days = ['sun','mon','tue','wed','thu','fri','sat']
long_days = ['sunday','monday','tuesday','wednesday','thursday','friday','saturday']

short_day_reg = group (join (short_days, '|'))
long_day_reg = group (join (long_days, '|'))

daymap = {}
for i in range(7):
    daymap[short_days[i]] = i
    daymap[long_days[i]] = i

hms_reg = join (3 * [group('[0-9][0-9]')], ':')

months = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']

monmap = {}
for i in range(12):
    monmap[months[i]] = i+1

months_reg = group (join (months, '|'))

# From draft-ietf-http-v11-spec-07.txt/3.3.1
#       Sun, 06 Nov 1994 08:49:37 GMT  ; RFC 822, updated by RFC 1123
#       Sunday, 06-Nov-94 08:49:37 GMT ; RFC 850, obsoleted by RFC 1036
#       Sun Nov  6 08:49:37 1994       ; ANSI C's asctime() format

# rfc822 format
rfc822_date = join (
        [concat (short_day_reg,','),    # day
         group('[0-9][0-9]?'),                  # date
         months_reg,                                    # month
         group('[0-9]+'),                               # year
         hms_reg,                                               # hour minute second
         'gmt'
         ],
        ' '
        )

rfc822_reg = re.compile (rfc822_date)

def unpack_rfc822 (m):
    g = m.group
    a = string.atoi
    return (
            a(g(4)),                # year
            monmap[g(3)],   # month
            a(g(2)),                # day
            a(g(5)),                # hour
            a(g(6)),                # minute
            a(g(7)),                # second
            0,
            0,
            0
            )

# rfc850 format
rfc850_date = join (
        [concat (long_day_reg,','),
         join (
                 [group ('[0-9][0-9]?'),
                  months_reg,
                  group ('[0-9]+')
                  ],
                 '-'
                 ),
         hms_reg,
         'gmt'
         ],
        ' '
        )

rfc850_reg = re.compile (rfc850_date)
# they actually unpack the same way
def unpack_rfc850 (m):
    g = m.group
    a = string.atoi
    return (
            a(g(4)),                # year
            monmap[g(3)],   # month
            a(g(2)),                # day
            a(g(5)),                # hour
            a(g(6)),                # minute
            a(g(7)),                # second
            0,
            0,
            0
            )

# parsdate.parsedate    - ~700/sec.
# parse_http_date       - ~1333/sec.

def build_http_date (when):
    return time.strftime ('%a, %d %b %Y %H:%M:%S GMT', time.gmtime(when))

def parse_http_date (d):
    d = string.lower (d)
    tz = time.timezone
    m = rfc850_reg.match (d)
    if m and m.end() == len(d):
        retval = int (time.mktime (unpack_rfc850(m)) - tz)
    else:
        m = rfc822_reg.match (d)
        if m and m.end() == len(d):
            retval = int (time.mktime (unpack_rfc822(m)) - tz)
        else:
            return 0
    # Thanks to Craig Silverstein <csilvers@google.com> for pointing
    # out the DST discrepancy
    if time.daylight and time.localtime(retval)[-1] == 1: # DST correction
        retval = retval + (tz - time.altzone)
    return retval

########NEW FILE########
__FILENAME__ = http_server
# -*- Mode: Python -*-
#
#       Author: Sam Rushing <rushing@nightmare.com>
#       Copyright 1996-2000 by Sam Rushing
#                                                All Rights Reserved.
#

RCS_ID =  '$Id: http_server.py,v 1.12 2004/04/21 15:11:44 akuchling Exp $'

# python modules
import os
import re
import socket
import string
import sys
import time

# async modules
import asyncore_25 as asyncore
import asynchat_25 as asynchat

# medusa modules
import http_date
import producers
import status_handler
import logger

VERSION_STRING = string.split(RCS_ID)[2]

from counter import counter
from urllib import unquote, splitquery

# ===========================================================================
#                                                       Request Object
# ===========================================================================

class http_request:

    # default reply code
    reply_code = 200

    request_counter = counter()

    # Whether to automatically use chunked encoding when
    #
    #   HTTP version is 1.1
    #   Content-Length is not set
    #   Chunked encoding is not already in effect
    #
    # If your clients are having trouble, you might want to disable this.
    use_chunked = 1

    # by default, this request object ignores user data.
    collector = None

    def __init__ (self, *args):
        # unpack information about the request
        (self.channel, self.request,
         self.command, self.uri, self.version,
         self.header) = args

        self.outgoing = []
        self.reply_headers = {
                'Server'        : 'Medusa/%s' % VERSION_STRING,
                'Date'          : http_date.build_http_date (time.time())
                }
        
        # New reply header list (to support multiple 
        # headers with same name)
        self.__reply_header_list = []
        
        self.request_number = http_request.request_counter.increment()
        self._split_uri = None
        self._header_cache = {}

    # --------------------------------------------------
    # reply header management
    # --------------------------------------------------
    def __setitem__ (self, key, value):
        self.reply_headers[key] = value

    def __getitem__ (self, key):
        return self.reply_headers[key]

    def has_key (self, key):
        return self.reply_headers.has_key (key)

    def build_reply_header (self):
        return string.join (
                [self.response(self.reply_code)] + map (
                        lambda x: '%s: %s' % x,
                        self.reply_headers.items()
                        ),
                '\r\n'
                ) + '\r\n\r\n'

    ####################################################
    # multiple reply header management
    ####################################################
    # These are intended for allowing multiple occurrences
    # of the same header.
    # Usually you can fold such headers together, separating
    # their contents by a comma (e.g. Accept: text/html, text/plain)
    # but the big exception is the Set-Cookie header.
    # dictionary centric.
    #---------------------------------------------------
    
    def add_header(self, name, value):
        """ Adds a header to the reply headers """
        self.__reply_header_list.append((name, value))
    
    def clear_headers(self):
        """ Clears the reply header list """

        # Remove things from the old dict as well
        self.reply_headers.clear()

        self.__reply_header_list[:] = []
    
    def remove_header(self, name, value=None):
        """ Removes the specified header.
        If a value is provided, the name and 
        value must match to remove the header.  
        If the value is None, removes all headers
        with that name."""

        found_it = 0
        
        # Remove things from the old dict as well
        if (self.reply_headers.has_key(name) and
            (value is None or 
             self.reply_headers[name] == value)):
            del self.reply_headers[name]
            found_it = 1


        if not value is None:
            if (name, value) in self.__reply_header_list:
                removed_headers = [(name, value)]
                found_it = 1
        else:
            removed_headers = []
            for h in self.__reply_header_list:
                if h[0] == name:
                    removed_headers.append(h)
                    found_it = 1

        if not found_it:
            if value is None:
                search_value = "%s" % name
            else:
                search_value = "%s: %s" % (name, value)

            raise LookupError("Header '%s' not found" % search_value)
        
        for h in removed_headers:
            self.__reply_header_list.remove(h)


    def get_reply_headers(self):
        """ Get the tuple of headers that will be used
        for generating reply headers"""
        header_tuples = self.__reply_header_list[:]
       
        # The idea here is to insert the headers from 
        # the old header dict into the new header list, 
        # UNLESS there's already an entry in the list
        # that would have overwritten the dict entry
        # if the dict was the only storage...
        header_names = [n for n,v in header_tuples]
        for n,v in self.reply_headers.items():
            if n not in header_names:
                header_tuples.append((n,v))
                header_names.append(n)
        # Ok, that should do it.  Now, if there were any
        # headers in the dict that weren't in the list,
        # they should have been copied in.  If the name
        # was already in the list, we didn't copy it,
        # because the value from the dict has been 
        # 'overwritten' by the one in the list.

        return header_tuples        

    def get_reply_header_text(self):
        """ Gets the reply header (including status and 
        additional crlf)"""

        header_tuples = self.get_reply_headers()

        headers = [self.response(self.reply_code)]
        headers += ["%s: %s" % h for h in header_tuples]
        
        return string.join(headers, '\r\n') + '\r\n\r\n'

    #---------------------------------------------------
    # This is the end of the new reply header 
    # management section.
    ####################################################


    # --------------------------------------------------
    # split a uri
    # --------------------------------------------------

    # <path>;<params>?<query>#<fragment>
    path_regex = re.compile (
    #      path      params    query   fragment
            r'([^;?#]*)(;[^?#]*)?(\?[^#]*)?(#.*)?'
            )

    def split_uri (self):
        if self._split_uri is None:
            m = self.path_regex.match (self.uri)
            if m.end() != len(self.uri):
                raise ValueError, "Broken URI"
            else:
                self._split_uri = m.groups()
        return self._split_uri

    def get_header_with_regex (self, head_reg, group):
        for line in self.header:
            m = head_reg.match (line)
            if m.end() == len(line):
                return m.group (group)
        return ''

    def get_header (self, header):
        header = string.lower (header)
        hc = self._header_cache
        if not hc.has_key (header):
            h = header + ': '
            hl = len(h)
            for line in self.header:
                if string.lower (line[:hl]) == h:
                    r = line[hl:]
                    hc[header] = r
                    return r
            hc[header] = None
            return None
        else:
            return hc[header]

    # --------------------------------------------------
    # user data
    # --------------------------------------------------

    def collect_incoming_data (self, data):
        if self.collector:
            self.collector.collect_incoming_data (data)
        else:
            self.log_info(
                    'Dropping %d bytes of incoming request data' % len(data),
                    'warning'
                    )

    def found_terminator (self):
        if self.collector:
            self.collector.found_terminator()
        else:
            self.log_info (
                    'Unexpected end-of-record for incoming request',
                    'warning'
                    )

    def push (self, thing):
        if type(thing) == type(''):
            self.outgoing.append(producers.simple_producer(thing,
              buffer_size=len(thing)))
        else:
            self.outgoing.append(thing)

    def response (self, code=200):
        message = self.responses[code]
        self.reply_code = code
        return 'HTTP/%s %d %s' % (self.version, code, message)

    def error (self, code):
        self.reply_code = code
        message = self.responses[code]
        s = self.DEFAULT_ERROR_MESSAGE % {
                'code': code,
                'message': message,
                }
        self['Content-Length'] = len(s)
        self['Content-Type'] = 'text/html'
        # make an error reply
        self.push (s)
        self.done()

    # can also be used for empty replies
    reply_now = error

    def done (self):
        "finalize this transaction - send output to the http channel"

        # ----------------------------------------
        # persistent connection management
        # ----------------------------------------

        #  --- BUCKLE UP! ----

        connection = string.lower (get_header (CONNECTION, self.header))

        close_it = 0
        wrap_in_chunking = 0

        if self.version == '1.0':
            if connection == 'keep-alive':
                if not self.has_key ('Content-Length'):
                    close_it = 1
                else:
                    self['Connection'] = 'Keep-Alive'
            else:
                close_it = 1
        elif self.version == '1.1':
            if connection == 'close':
                close_it = 1
            elif not self.has_key ('Content-Length'):
                if self.has_key ('Transfer-Encoding'):
                    if not self['Transfer-Encoding'] == 'chunked':
                        close_it = 1
                elif self.use_chunked:
                    self['Transfer-Encoding'] = 'chunked'
                    wrap_in_chunking = 1
                else:
                    close_it = 1
        elif self.version is None:
            # Although we don't *really* support http/0.9 (because we'd have to
            # use \r\n as a terminator, and it would just yuck up a lot of stuff)
            # it's very common for developers to not want to type a version number
            # when using telnet to debug a server.
            close_it = 1

        outgoing_header = producers.simple_producer(self.get_reply_header_text())

        if close_it:
            self['Connection'] = 'close'

        if wrap_in_chunking:
            outgoing_producer = producers.chunked_producer (
                    producers.composite_producer (self.outgoing)
                    )
            # prepend the header
            outgoing_producer = producers.composite_producer(
                [outgoing_header, outgoing_producer]
                )
        else:
            # prepend the header
            self.outgoing.insert(0, outgoing_header)
            outgoing_producer = producers.composite_producer (self.outgoing)

        # apply a few final transformations to the output
        self.channel.push_with_producer (
                # globbing gives us large packets
                producers.globbing_producer (
                        # hooking lets us log the number of bytes sent
                        producers.hooked_producer (
                                outgoing_producer,
                                self.log
                                )
                        )
                )

        self.channel.current_request = None

        if close_it:
            self.channel.close_when_done()

    def log_date_string (self, when):
        gmt = time.gmtime(when)
        if time.daylight and gmt[8]:
            tz = time.altzone
        else:
            tz = time.timezone
        if tz > 0:
            neg = 1
        else:
            neg = 0
            tz = -tz
        h, rem = divmod (tz, 3600)
        m, rem = divmod (rem, 60)
        if neg:
            offset = '-%02d%02d' % (h, m)
        else:
            offset = '+%02d%02d' % (h, m)

        return time.strftime ( '%d/%b/%Y:%H:%M:%S ', gmt) + offset

    def log (self, bytes):
        self.channel.server.logger.log (
                self.channel.addr[0],
                '%d - - [%s] "%s" %d %d\n' % (
                        self.channel.addr[1],
                        self.log_date_string (time.time()),
                        self.request,
                        self.reply_code,
                        bytes
                        )
                )

    responses = {
            100: "Continue",
            101: "Switching Protocols",
            200: "OK",
            201: "Created",
            202: "Accepted",
            203: "Non-Authoritative Information",
            204: "No Content",
            205: "Reset Content",
            206: "Partial Content",
            300: "Multiple Choices",
            301: "Moved Permanently",
            302: "Moved Temporarily",
            303: "See Other",
            304: "Not Modified",
            305: "Use Proxy",
            400: "Bad Request",
            401: "Unauthorized",
            402: "Payment Required",
            403: "Forbidden",
            404: "Not Found",
            405: "Method Not Allowed",
            406: "Not Acceptable",
            407: "Proxy Authentication Required",
            408: "Request Time-out",
            409: "Conflict",
            410: "Gone",
            411: "Length Required",
            412: "Precondition Failed",
            413: "Request Entity Too Large",
            414: "Request-URI Too Large",
            415: "Unsupported Media Type",
            500: "Internal Server Error",
            501: "Not Implemented",
            502: "Bad Gateway",
            503: "Service Unavailable",
            504: "Gateway Time-out",
            505: "HTTP Version not supported"
            }

    # Default error message
    DEFAULT_ERROR_MESSAGE = string.join (
            ['<head>',
             '<title>Error response</title>',
             '</head>',
             '<body>',
             '<h1>Error response</h1>',
             '<p>Error code %(code)d.',
             '<p>Message: %(message)s.',
             '</body>',
             ''
             ],
            '\r\n'
            )


# ===========================================================================
#                                                HTTP Channel Object
# ===========================================================================

class http_channel (asynchat.async_chat):

    # use a larger default output buffer
    ac_out_buffer_size = 1<<16

    current_request = None
    channel_counter = counter()

    def __init__ (self, server, conn, addr):
        self.channel_number = http_channel.channel_counter.increment()
        self.request_counter = counter()
        asynchat.async_chat.__init__ (self, conn)
        self.server = server
        self.addr = addr
        self.set_terminator ('\r\n\r\n')
        self.in_buffer = ''
        self.creation_time = int (time.time())
        self.check_maintenance()

    def __repr__ (self):
        ar = asynchat.async_chat.__repr__(self)[1:-1]
        return '<%s channel#: %s requests:%s>' % (
                ar,
                self.channel_number,
                self.request_counter
                )

    # Channel Counter, Maintenance Interval...
    maintenance_interval = 500

    def check_maintenance (self):
        if not self.channel_number % self.maintenance_interval:
            self.maintenance()

    def maintenance (self):
        self.kill_zombies()

    # 30-minute zombie timeout.  status_handler also knows how to kill zombies.
    zombie_timeout = 30 * 60

    def kill_zombies (self):
        now = int (time.time())
        for channel in asyncore.socket_map.values():
            if channel.__class__ == self.__class__:
                if (now - channel.creation_time) > channel.zombie_timeout:
                    channel.close()

    # --------------------------------------------------
    # send/recv overrides, good place for instrumentation.
    # --------------------------------------------------

    # this information needs to get into the request object,
    # so that it may log correctly.
    def send (self, data):
        result = asynchat.async_chat.send (self, data)
        self.server.bytes_out.increment (len(data))
        return result

    def recv (self, buffer_size):
        try:
            result = asynchat.async_chat.recv (self, buffer_size)
            self.server.bytes_in.increment (len(result))
            return result
        except MemoryError:
            # --- Save a Trip to Your Service Provider ---
            # It's possible for a process to eat up all the memory of
            # the machine, and put it in an extremely wedged state,
            # where medusa keeps running and can't be shut down.  This
            # is where MemoryError tends to get thrown, though of
            # course it could get thrown elsewhere.
            sys.exit ("Out of Memory!")

    def handle_error (self):
        t, v = sys.exc_info()[:2]
        if t is SystemExit:
            raise t, v
        else:
            asynchat.async_chat.handle_error (self)

    def log (self, *args):
        pass

    # --------------------------------------------------
    # async_chat methods
    # --------------------------------------------------

    def collect_incoming_data (self, data):
        if self.current_request:
            # we are receiving data (probably POST data) for a request
            self.current_request.collect_incoming_data (data)
        else:
            # we are receiving header (request) data
            self.in_buffer = self.in_buffer + data

    def found_terminator (self):
        if self.current_request:
            self.current_request.found_terminator()
        else:
            header = self.in_buffer
            self.in_buffer = ''
            lines = string.split (header, '\r\n')

            # --------------------------------------------------
            # crack the request header
            # --------------------------------------------------

            while lines and not lines[0]:
                # as per the suggestion of http-1.1 section 4.1, (and
                # Eric Parker <eparker@zyvex.com>), ignore a leading
                # blank lines (buggy browsers tack it onto the end of
                # POST requests)
                lines = lines[1:]

            if not lines:
                self.close_when_done()
                return

            request = lines[0]

            command, uri, version = crack_request (request)
            header = join_headers (lines[1:])

            # unquote path if necessary (thanks to Skip Montanaro for pointing
            # out that we must unquote in piecemeal fashion).
            rpath, rquery = splitquery(uri)
            if '%' in rpath:
                if rquery:
                    uri = unquote (rpath) + '?' + rquery
                else:
                    uri = unquote (rpath)

            r = http_request (self, request, command, uri, version, header)
            self.request_counter.increment()
            self.server.total_requests.increment()

            if command is None:
                self.log_info ('Bad HTTP request: %s' % repr(request), 'error')
                r.error (400)
                return

            # --------------------------------------------------
            # handler selection and dispatch
            # --------------------------------------------------
            for h in self.server.handlers:
                if h.match (r):
                    try:
                        self.current_request = r
                        # This isn't used anywhere.
                        # r.handler = h # CYCLE
                        h.handle_request (r)
                    except:
                        self.server.exceptions.increment()
                        (file, fun, line), t, v, tbinfo = asyncore.compact_traceback()
                        self.log_info(
                                        'Server Error: %s, %s: file: %s line: %s' % (t,v,file,line),
                                        'error')
                        try:
                            r.error (500)
                        except:
                            pass
                    return

            # no handlers, so complain
            r.error (404)

    def writable_for_proxy (self):
        # this version of writable supports the idea of a 'stalled' producer
        # [i.e., it's not ready to produce any output yet] This is needed by
        # the proxy, which will be waiting for the magic combination of
        # 1) hostname resolved
        # 2) connection made
        # 3) data available.
        if self.ac_out_buffer:
            return 1
        elif len(self.producer_fifo):
            p = self.producer_fifo.first()
            if hasattr (p, 'stalled'):
                return not p.stalled()
            else:
                return 1

# ===========================================================================
#                                                HTTP Server Object
# ===========================================================================

class http_server (asyncore.dispatcher):

    SERVER_IDENT = 'HTTP Server (V%s)' % VERSION_STRING

    channel_class = http_channel

    def __init__ (self, ip, port, resolver=None, logger_object=None):
        self.ip = ip
        self.port = port
        asyncore.dispatcher.__init__ (self)
        self.create_socket (socket.AF_INET, socket.SOCK_STREAM)

        self.handlers = []

        if not logger_object:
            logger_object = logger.file_logger (sys.stdout)

        self.set_reuse_addr()
        self.bind ((ip, port))

        # lower this to 5 if your OS complains
        self.listen (1024)

        host, port = self.socket.getsockname()
        if not ip:
            self.log_info('Computing default hostname', 'warning')
            ip = socket.gethostbyname (socket.gethostname())
        try:
            self.server_name = socket.gethostbyaddr (ip)[0]
        except socket.error:
            self.log_info('Cannot do reverse lookup', 'warning')
            self.server_name = ip       # use the IP address as the "hostname"

        self.server_port = port
        self.total_clients = counter()
        self.total_requests = counter()
        self.exceptions = counter()
        self.bytes_out = counter()
        self.bytes_in  = counter()

        if not logger_object:
            logger_object = logger.file_logger (sys.stdout)

        if resolver:
            self.logger = logger.resolving_logger (resolver, logger_object)
        else:
            self.logger = logger.unresolving_logger (logger_object)

        self.log_info (
                'Medusa (V%s) started at %s'
                '\n\tHostname: %s'
                '\n\tPort:%d'
                '\n' % (
                        VERSION_STRING,
                        time.ctime(time.time()),
                        self.server_name,
                        port,
                        )
                )

    def writable (self):
        return 0

    def handle_read (self):
        pass

    def readable (self):
        return self.accepting

    def handle_connect (self):
        pass

    def handle_accept (self):
        self.total_clients.increment()
        try:
            conn, addr = self.accept()
        except socket.error:
            # linux: on rare occasions we get a bogus socket back from
            # accept.  socketmodule.c:makesockaddr complains that the
            # address family is unknown.  We don't want the whole server
            # to shut down because of this.
            self.log_info ('warning: server accept() threw an exception', 'warning')
            return
        except TypeError:
            # unpack non-sequence.  this can happen when a read event
            # fires on a listening socket, but when we call accept()
            # we get EWOULDBLOCK, so dispatcher.accept() returns None.
            # Seen on FreeBSD3.
            self.log_info ('warning: server accept() threw EWOULDBLOCK', 'warning')
            return

        self.channel_class (self, conn, addr)

    def install_handler (self, handler, back=0):
        if back:
            self.handlers.append (handler)
        else:
            self.handlers.insert (0, handler)

    def remove_handler (self, handler):
        self.handlers.remove (handler)

    def status (self):
        def nice_bytes (n):
            return string.join (status_handler.english_bytes (n))

        handler_stats = filter (None, map (maybe_status, self.handlers))

        if self.total_clients:
            ratio = self.total_requests.as_long() / float(self.total_clients.as_long())
        else:
            ratio = 0.0

        return producers.composite_producer (
                [producers.lines_producer (
                        ['<h2>%s</h2>'                                                  % self.SERVER_IDENT,
                        '<br>Listening on: <b>Host:</b> %s'             % self.server_name,
                        '<b>Port:</b> %d'                                               % self.port,
                         '<p><ul>'
                         '<li>Total <b>Clients:</b> %s'                 % self.total_clients,
                         '<b>Requests:</b> %s'                                  % self.total_requests,
                         '<b>Requests/Client:</b> %.1f'                 % (ratio),
                         '<li>Total <b>Bytes In:</b> %s'        % (nice_bytes (self.bytes_in.as_long())),
                         '<b>Bytes Out:</b> %s'                         % (nice_bytes (self.bytes_out.as_long())),
                         '<li>Total <b>Exceptions:</b> %s'              % self.exceptions,
                         '</ul><p>'
                         '<b>Extension List</b><ul>',
                         ])] + handler_stats + [producers.simple_producer('</ul>')]
                )

def maybe_status (thing):
    if hasattr (thing, 'status'):
        return thing.status()
    else:
        return None

CONNECTION = re.compile ('Connection: (.*)', re.IGNORECASE)

# merge multi-line headers
# [486dx2: ~500/sec]
def join_headers (headers):
    r = []
    for i in range(len(headers)):
        if headers[i][0] in ' \t':
            r[-1] = r[-1] + headers[i][1:]
        else:
            r.append (headers[i])
    return r

def get_header (head_reg, lines, group=1):
    for line in lines:
        m = head_reg.match (line)
        if m and m.end() == len(line):
            return m.group (group)
    return ''

def get_header_match (head_reg, lines):
    for line in lines:
        m = head_reg.match (line)
        if m and m.end() == len(line):
            return m
    return ''

REQUEST = re.compile ('([^ ]+) ([^ ]+)(( HTTP/([0-9.]+))$|$)')

def crack_request (r):
    m = REQUEST.match (r)
    if m and m.end() == len(r):
        if m.group(3):
            version = m.group(5)
        else:
            version = None
        return m.group(1), m.group(2), version
    else:
        return None, None, None

if __name__ == '__main__':
    import sys
    if len(sys.argv) < 2:
        print 'usage: %s <root> <port>' % (sys.argv[0])
    else:
        import monitor
        import filesys
        import default_handler
        import status_handler
        import ftp_server
        import chat_server
        import resolver
        import logger
        rs = resolver.caching_resolver ('127.0.0.1')
        lg = logger.file_logger (sys.stdout)
        ms = monitor.secure_monitor_server ('fnord', '127.0.0.1', 9999)
        fs = filesys.os_filesystem (sys.argv[1])
        dh = default_handler.default_handler (fs)
        hs = http_server ('', string.atoi (sys.argv[2]), rs, lg)
        hs.install_handler (dh)
        ftp = ftp_server.ftp_server (
                ftp_server.dummy_authorizer(sys.argv[1]),
                port=8021,
                resolver=rs,
                logger_object=lg
                )
        cs = chat_server.chat_server ('', 7777)
        sh = status_handler.status_extension([hs,ms,ftp,cs,rs])
        hs.install_handler (sh)
        if ('-p' in sys.argv):
            def profile_loop ():
                try:
                    asyncore.loop()
                except KeyboardInterrupt:
                    pass
            import profile
            profile.run ('profile_loop()', 'profile.out')
        else:
            asyncore.loop()

########NEW FILE########
__FILENAME__ = logger
# -*- Mode: Python -*-

import asynchat_25 as asynchat
import socket
import time         # these three are for the rotating logger
import os           # |
import stat         # v

#
# three types of log:
# 1) file
#    with optional flushing.  Also, one that rotates the log.
# 2) socket
#    dump output directly to a socket connection. [how do we
#    keep it open?]
# 3) syslog
#    log to syslog via tcp.  this is a per-line protocol.
#

#
# The 'standard' interface to a logging object is simply
# log_object.log (message)
#

# a file-like object that captures output, and
# makes sure to flush it always...  this could
# be connected to:
#  o    stdio file
#  o    low-level file
#  o    socket channel
#  o    syslog output...

class file_logger:

    # pass this either a path or a file object.
    def __init__ (self, file, flush=1, mode='a'):
        if type(file) == type(''):
            if (file == '-'):
                import sys
                self.file = sys.stdout
            else:
                self.file = open (file, mode)
        else:
            self.file = file
        self.do_flush = flush

    def __repr__ (self):
        return '<file logger: %s>' % self.file

    def write (self, data):
        self.file.write (data)
        self.maybe_flush()

    def writeline (self, line):
        self.file.writeline (line)
        self.maybe_flush()

    def writelines (self, lines):
        self.file.writelines (lines)
        self.maybe_flush()

    def maybe_flush (self):
        if self.do_flush:
            self.file.flush()

    def flush (self):
        self.file.flush()

    def softspace (self, *args):
        pass

    def log (self, message):
        if message[-1] not in ('\r', '\n'):
            self.write (message + '\n')
        else:
            self.write (message)

# like a file_logger, but it must be attached to a filename.
# When the log gets too full, or a certain time has passed,
# it backs up the log and starts a new one.  Note that backing
# up the log is done via "mv" because anything else (cp, gzip)
# would take time, during which medusa would do nothing else.

class rotating_file_logger (file_logger):

    # If freq is non-None we back up "daily", "weekly", or "monthly".
    # Else if maxsize is non-None we back up whenever the log gets
    # to big.  If both are None we never back up.
    def __init__ (self, file, freq=None, maxsize=None, flush=1, mode='a'):
        self.filename = file
        self.mode = mode
        self.file = open (file, mode)
        self.freq = freq
        self.maxsize = maxsize
        self.rotate_when = self.next_backup(self.freq)
        self.do_flush = flush

    def __repr__ (self):
        return '<rotating-file logger: %s>' % self.file

    # We back up at midnight every 1) day, 2) monday, or 3) 1st of month
    def next_backup (self, freq):
        (yr, mo, day, hr, min, sec, wd, jday, dst) = time.localtime(time.time())
        if freq == 'daily':
            return time.mktime((yr,mo,day+1, 0,0,0, 0,0,-1))
        elif freq == 'weekly':
            return time.mktime((yr,mo,day-wd+7, 0,0,0, 0,0,-1)) # wd(monday)==0
        elif freq == 'monthly':
            return time.mktime((yr,mo+1,1, 0,0,0, 0,0,-1))
        else:
            return None                  # not a date-based backup

    def maybe_flush (self):              # rotate first if necessary
        self.maybe_rotate()
        if self.do_flush:                # from file_logger()
            self.file.flush()

    def maybe_rotate (self):
        if self.freq and time.time() > self.rotate_when:
            self.rotate()
            self.rotate_when = self.next_backup(self.freq)
        elif self.maxsize:               # rotate when we get too big
            try:
                if os.stat(self.filename)[stat.ST_SIZE] > self.maxsize:
                    self.rotate()
            except os.error:             # file not found, probably
                self.rotate()            # will create a new file

    def rotate (self):
        (yr, mo, day, hr, min, sec, wd, jday, dst) = time.localtime(time.time())
        try:
            self.file.close()
            newname = '%s.ends%04d%02d%02d' % (self.filename, yr, mo, day)
            try:
                open(newname, "r").close()      # check if file exists
                newname = newname + "-%02d%02d%02d" % (hr, min, sec)
            except:                             # YEARMODY is unique
                pass
            os.rename(self.filename, newname)
            self.file = open(self.filename, self.mode)
        except:
            pass

# syslog is a line-oriented log protocol - this class would be
# appropriate for FTP or HTTP logs, but not for dumping stderr to.

# TODO: a simple safety wrapper that will ensure that the line sent
# to syslog is reasonable.

# TODO: async version of syslog_client: now, log entries use blocking
# send()

import m_syslog
syslog_logger = m_syslog.syslog_client

class syslog_logger (m_syslog.syslog_client):
    def __init__ (self, address, facility='user'):
        m_syslog.syslog_client.__init__ (self, address)
        self.facility = m_syslog.facility_names[facility]
        self.address=address

    def __repr__ (self):
        return '<syslog logger address=%s>' % (repr(self.address))

    def log (self, message):
        m_syslog.syslog_client.log (
                self,
                message,
                facility=self.facility,
                priority=m_syslog.LOG_INFO
                )

# log to a stream socket, asynchronously

class socket_logger (asynchat.async_chat):

    def __init__ (self, address):

        if type(address) == type(''):
            self.create_socket (socket.AF_UNIX, socket.SOCK_STREAM)
        else:
            self.create_socket (socket.AF_INET, socket.SOCK_STREAM)

        self.connect (address)
        self.address = address

    def __repr__ (self):
        return '<socket logger: address=%s>' % (self.address)

    def log (self, message):
        if message[-2:] != '\r\n':
            self.socket.push (message + '\r\n')
        else:
            self.socket.push (message)

# log to multiple places
class multi_logger:
    def __init__ (self, loggers):
        self.loggers = loggers

    def __repr__ (self):
        return '<multi logger: %s>' % (repr(self.loggers))

    def log (self, message):
        for logger in self.loggers:
            logger.log (message)

class resolving_logger:
    """Feed (ip, message) combinations into this logger to get a
    resolved hostname in front of the message.  The message will not
    be logged until the PTR request finishes (or fails)."""

    def __init__ (self, resolver, logger):
        self.resolver = resolver
        self.logger = logger

    class logger_thunk:
        def __init__ (self, message, logger):
            self.message = message
            self.logger = logger

        def __call__ (self, host, ttl, answer):
            if not answer:
                answer = host
            self.logger.log ('%s:%s' % (answer, self.message))

    def log (self, ip, message):
        self.resolver.resolve_ptr (
                ip,
                self.logger_thunk (
                        message,
                        self.logger
                        )
                )

class unresolving_logger:
    "Just in case you don't want to resolve"
    def __init__ (self, logger):
        self.logger = logger

    def log (self, ip, message):
        self.logger.log ('%s:%s' % (ip, message))


def strip_eol (line):
    while line and line[-1] in '\r\n':
        line = line[:-1]
    return line

class tail_logger:
    "Keep track of the last <size> log messages"
    def __init__ (self, logger, size=500):
        self.size = size
        self.logger = logger
        self.messages = []

    def log (self, message):
        self.messages.append (strip_eol (message))
        if len (self.messages) > self.size:
            del self.messages[0]
        self.logger.log (message)

########NEW FILE########
__FILENAME__ = medusa_gif
# -*- Mode: Python -*-

# the medusa icon as a python source file.

width = 97
height = 61

data = 'GIF89aa\000=\000\204\000\000\000\000\000\255\255\255\245\245\245ssskkkccc111)))\326\326\326!!!\316\316\316\300\300\300\204\204\000\224\224\224\214\214\214\200\200\200RRR\377\377\377JJJ\367\367\367BBB\347\347\347\000\204\000\020\020\020\265\265\265\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000!\371\004\001\000\000\021\000,\000\000\000\000a\000=\000\000\005\376`$\216di\236h\252\256l\353\276p,\317tm\337x\256\357|m\001@\240E\305\000\364\2164\206R)$\005\201\214\007r\012{X\255\312a\004\260\\>\026\3240\353)\224n\001W+X\334\373\231~\344.\303b\216\024\027x<\273\307\255G,rJiWN\014{S}k"?ti\013EdPQ\207G@_%\000\026yy\\\201\202\227\224<\221Fs$pOjWz\241<r@vO\236\231\233k\247M\2544\203F\177\235\236L#\247\256Z\270,\266BxJ[\276\256A]iE\304\305\262\273E\313\201\275i#\\\303\321\'h\203V\\\177\326\276\216\220P~\335\230_\264\013\342\275\344KF\233\360Q\212\352\246\000\367\274s\361\236\334\347T\341;\341\246\2202\177\3142\211`\242o\325@S\202\264\031\252\207\260\323\256\205\311\036\236\270\002\'\013\302\177\274H\010\324X\002\0176\212\037\376\321\360\032\226\207\244\2674(+^\202\346r\205J\0211\375\241Y#\256f\0127\315>\272\002\325\307g\012(\007\205\312#j\317(\012A\200\224.\241\003\346GS\247\033\245\344\264\366\015L\'PXQl]\266\263\243\232\260?\245\316\371\362\225\035\332\243J\273\332Q\263\357-D\241T\327\270\265\013W&\330\010u\371b\322IW0\214\261]\003\033Va\365Z#\207\213a\030k\2647\262\014p\354\024[n\321N\363\346\317\003\037P\000\235C\302\000\3228(\244\363YaA\005\022\255_\237@\260\000A\212\326\256qbp\321\332\266\011\334=T\023\010"!B\005\003A\010\224\020\220 H\002\337#\020 O\276E\357h\221\327\003\\\000b@v\004\351A.h\365\354\342B\002\011\257\025\\ \220\340\301\353\006\000\024\214\200pA\300\353\012\364\241k/\340\033C\202\003\000\310fZ\011\003V\240R\005\007\354\376\026A\000\000\360\'\202\177\024\004\210\003\000\305\215\360\000\000\015\220\240\332\203\027@\'\202\004\025VpA\000%\210x\321\206\032J\341\316\010\262\211H"l\333\341\200\200>"]P\002\212\011\010`\002\0066FP\200\001\'\024p]\004\027(8B\221\306]\000\201w>\002iB\001\007\340\260"v7J1\343(\257\020\251\243\011\242i\263\017\215\337\035\220\200\221\365m4d\015\016D\251\341iN\354\346Ng\253\200I\240\031\35609\245\2057\311I\302\2007t\231"&`\314\310\244\011e\226(\236\010w\212\300\234\011\012HX(\214\253\311@\001\233^\222pg{% \340\035\224&H\000\246\201\362\215`@\001"L\340\004\030\234\022\250\'\015(V:\302\235\030\240q\337\205\224\212h@\177\006\000\250\210\004\007\310\207\337\005\257-P\346\257\367]p\353\203\271\256:\203\236\211F\340\247\010\3329g\244\010\307*=A\000\203\260y\012\304s#\014\007D\207,N\007\304\265\027\021C\233\207%B\366[m\353\006\006\034j\360\306+\357\274a\204\000\000;'

########NEW FILE########
__FILENAME__ = monitor
# -*- Mode: Python -*-
#       Author: Sam Rushing <rushing@nightmare.com>

#
# python REPL channel.
#

RCS_ID = '$Id: monitor.py,v 1.5 2002/03/23 15:08:06 amk Exp $'

import md5
import socket
import string
import sys
import time

VERSION = string.split(RCS_ID)[2]

import asyncore_25 as asyncore
import asynchat_25 as asynchat

from counter import counter
import producers

class monitor_channel (asynchat.async_chat):
    try_linemode = 1

    def __init__ (self, server, sock, addr):
        asynchat.async_chat.__init__ (self, sock)
        self.server = server
        self.addr = addr
        self.set_terminator ('\r\n')
        self.data = ''
        # local bindings specific to this channel
        self.local_env = sys.modules['__main__'].__dict__.copy()
        self.push ('Python ' + sys.version + '\r\n')
        self.push (sys.copyright+'\r\n')
        self.push ('Welcome to %s\r\n' % self)
        self.push ("[Hint: try 'from __main__ import *']\r\n")
        self.prompt()
        self.number = server.total_sessions.as_long()
        self.line_counter = counter()
        self.multi_line = []

    def handle_connect (self):
        # send IAC DO LINEMODE
        self.push ('\377\375\"')

    def close (self):
        self.server.closed_sessions.increment()
        asynchat.async_chat.close(self)

    def prompt (self):
        self.push ('>>> ')

    def collect_incoming_data (self, data):
        self.data = self.data + data
        if len(self.data) > 1024:
            # denial of service.
            self.push ('BCNU\r\n')
            self.close_when_done()

    def found_terminator (self):
        line = self.clean_line (self.data)
        self.data = ''
        self.line_counter.increment()
        # check for special case inputs...
        if not line and not self.multi_line:
            self.prompt()
            return
        if line in ['\004', 'exit']:
            self.push ('BCNU\r\n')
            self.close_when_done()
            return
        oldout = sys.stdout
        olderr = sys.stderr
        try:
            p = output_producer(self, olderr)
            sys.stdout = p
            sys.stderr = p
            try:
                # this is, of course, a blocking operation.
                # if you wanted to thread this, you would have
                # to synchronize, etc... and treat the output
                # like a pipe.  Not Fun.
                #
                # try eval first.  If that fails, try exec.  If that fails,
                # hurl.
                try:
                    if self.multi_line:
                        # oh, this is horrible...
                        raise SyntaxError
                    co = compile (line, repr(self), 'eval')
                    result = eval (co, self.local_env)
                    method = 'eval'
                    if result is not None:
                        print repr(result)
                    self.local_env['_'] = result
                except SyntaxError:
                    try:
                        if self.multi_line:
                            if line and line[0] in [' ','\t']:
                                self.multi_line.append (line)
                                self.push ('... ')
                                return
                            else:
                                self.multi_line.append (line)
                                line =  string.join (self.multi_line, '\n')
                                co = compile (line, repr(self), 'exec')
                                self.multi_line = []
                        else:
                            co = compile (line, repr(self), 'exec')
                    except SyntaxError, why:
                        if why[0] == 'unexpected EOF while parsing':
                            self.push ('... ')
                            self.multi_line.append (line)
                            return
                        else:
                            t,v,tb = sys.exc_info()
                            del tb
                            raise t,v
                    exec co in self.local_env
                    method = 'exec'
            except:
                method = 'exception'
                self.multi_line = []
                (file, fun, line), t, v, tbinfo = asyncore.compact_traceback()
                self.log_info('%s %s %s' %(t, v, tbinfo), 'warning')
        finally:
            sys.stdout = oldout
            sys.stderr = olderr
        self.log_info('%s:%s (%s)> %s' % (
                self.number,
                self.line_counter,
                method,
                repr(line))
                )
        self.push_with_producer (p)
        self.prompt()

    # for now, we ignore any telnet option stuff sent to
    # us, and we process the backspace key ourselves.
    # gee, it would be fun to write a full-blown line-editing
    # environment, etc...
    def clean_line (self, line):
        chars = []
        for ch in line:
            oc = ord(ch)
            if oc < 127:
                if oc in [8,177]:
                    # backspace
                    chars = chars[:-1]
                else:
                    chars.append (ch)
        return string.join (chars, '')

class monitor_server (asyncore.dispatcher):

    SERVER_IDENT = 'Monitor Server (V%s)' % VERSION

    channel_class = monitor_channel

    def __init__ (self, hostname='127.0.0.1', port=8023):
        asyncore.dispatcher.__init__(self)
        self.hostname = hostname
        self.port = port
        self.create_socket (socket.AF_INET, socket.SOCK_STREAM)
        self.set_reuse_addr()
        self.bind ((hostname, port))
        self.log_info('%s started on port %d' % (self.SERVER_IDENT, port))
        self.listen (5)
        self.closed             = 0
        self.failed_auths = 0
        self.total_sessions = counter()
        self.closed_sessions = counter()

    def writable (self):
        return 0

    def handle_accept (self):
        conn, addr = self.accept()
        self.log_info('Incoming monitor connection from %s:%d' % addr)
        self.channel_class (self, conn, addr)
        self.total_sessions.increment()

    def status (self):
        return producers.simple_producer (
                '<h2>%s</h2>'                                           % self.SERVER_IDENT
                + '<br><b>Total Sessions:</b> %s'               % self.total_sessions
                + '<br><b>Current Sessions:</b> %d'     % (
                        self.total_sessions.as_long()-self.closed_sessions.as_long()
                        )
                )

def hex_digest (s):
    m = md5.md5()
    m.update (s)
    return string.joinfields (
            map (lambda x: hex (ord (x))[2:], map (None, m.digest())),
            '',
            )

class secure_monitor_channel (monitor_channel):
    authorized = 0

    def __init__ (self, server, sock, addr):
        asynchat.async_chat.__init__ (self, sock)
        self.server = server
        self.addr = addr
        self.set_terminator ('\r\n')
        self.data = ''
        # local bindings specific to this channel
        self.local_env = {}
        # send timestamp string
        self.timestamp = str(time.time())
        self.count = 0
        self.line_counter = counter()
        self.number = int(server.total_sessions.as_long())
        self.multi_line = []
        self.push (self.timestamp + '\r\n')

    def found_terminator (self):
        if not self.authorized:
            if hex_digest ('%s%s' % (self.timestamp, self.server.password)) != self.data:
                self.log_info ('%s: failed authorization' % self, 'warning')
                self.server.failed_auths = self.server.failed_auths + 1
                self.close()
            else:
                self.authorized = 1
                self.push ('Python ' + sys.version + '\r\n')
                self.push (sys.copyright+'\r\n')
                self.push ('Welcome to %s\r\n' % self)
                self.prompt()
                self.data = ''
        else:
            monitor_channel.found_terminator (self)

class secure_encrypted_monitor_channel (secure_monitor_channel):
    "Wrap send() and recv() with a stream cipher"

    def __init__ (self, server, conn, addr):
        key = server.password
        self.outgoing = server.cipher.new (key)
        self.incoming = server.cipher.new (key)
        secure_monitor_channel.__init__ (self, server, conn, addr)

    def send (self, data):
        # send the encrypted data instead
        ed = self.outgoing.encrypt (data)
        return secure_monitor_channel.send (self, ed)

    def recv (self, block_size):
        data = secure_monitor_channel.recv (self, block_size)
        if data:
            dd = self.incoming.decrypt (data)
            return dd
        else:
            return data

class secure_monitor_server (monitor_server):
    channel_class = secure_monitor_channel

    def __init__ (self, password, hostname='', port=8023):
        monitor_server.__init__ (self, hostname, port)
        self.password = password

    def status (self):
        p = monitor_server.status (self)
        # kludge
        p.data = p.data + ('<br><b>Failed Authorizations:</b> %d' % self.failed_auths)
        return p

# don't try to print from within any of the methods
# of this object. 8^)

class output_producer:
    def __init__ (self, channel, real_stderr):
        self.channel = channel
        self.data = ''
        # use _this_ for debug output
        self.stderr = real_stderr

    def check_data (self):
        if len(self.data) > 1<<16:
            # runaway output, close it.
            self.channel.close()

    def write (self, data):
        lines = string.splitfields (data, '\n')
        data = string.join (lines, '\r\n')
        self.data = self.data + data
        self.check_data()

    def writeline (self, line):
        self.data = self.data + line + '\r\n'
        self.check_data()

    def writelines (self, lines):
        self.data = self.data + string.joinfields (
                lines,
                '\r\n'
                ) + '\r\n'
        self.check_data()

    def flush (self):
        pass

    def softspace (self, *args):
        pass

    def more (self):
        if self.data:
            result = self.data[:512]
            self.data = self.data[512:]
            return result
        else:
            return ''

if __name__ == '__main__':
    if '-s' in sys.argv:
        sys.argv.remove ('-s')
        print 'Enter password: ',
        password = raw_input()
    else:
        password = None

    if '-e' in sys.argv:
        sys.argv.remove ('-e')
        encrypt = 1
    else:
        encrypt = 0

    if len(sys.argv) > 1:
        port = string.atoi (sys.argv[1])
    else:
        port = 8023

    if password is not None:
        s = secure_monitor_server (password, '', port)
        if encrypt:
            s.channel_class = secure_encrypted_monitor_channel
            import sapphire
            s.cipher = sapphire
    else:
        s = monitor_server ('', port)

    asyncore.loop(use_poll=1)

########NEW FILE########
__FILENAME__ = monitor_client
# -*- Mode: Python -*-

# monitor client, unix version.

import asyncore_25 as asyncore
import asynchat_25 as asynchat
import socket
import string
import sys
import os

import md5

class stdin_channel (asyncore.file_dispatcher):
    def handle_read (self):
        data = self.recv(512)
        if not data:
            print '\nclosed.'
            self.sock_channel.close()
            try:
                self.close()
            except:
                pass

        data = string.replace(data, '\n', '\r\n')
        self.sock_channel.push (data)

    def writable (self):
        return 0

    def log (self, *ignore):
        pass

class monitor_client (asynchat.async_chat):
    def __init__ (self, password, addr=('',8023), socket_type=socket.AF_INET):
        asynchat.async_chat.__init__ (self)
        self.create_socket (socket_type, socket.SOCK_STREAM)
        self.terminator = '\r\n'
        self.connect (addr)
        self.sent_auth = 0
        self.timestamp = ''
        self.password = password

    def collect_incoming_data (self, data):
        if not self.sent_auth:
            self.timestamp = self.timestamp + data
        else:
            sys.stdout.write (data)
            sys.stdout.flush()

    def found_terminator (self):
        if not self.sent_auth:
            self.push (hex_digest (self.timestamp + self.password) + '\r\n')
            self.sent_auth = 1
        else:
            print

    def handle_close (self):
        # close all the channels, which will make the standard main
        # loop exit.
        map (lambda x: x.close(), asyncore.socket_map.values())

    def log (self, *ignore):
        pass

class encrypted_monitor_client (monitor_client):
    "Wrap push() and recv() with a stream cipher"

    def init_cipher (self, cipher, key):
        self.outgoing = cipher.new (key)
        self.incoming = cipher.new (key)

    def push (self, data):
        # push the encrypted data instead
        return monitor_client.push (self, self.outgoing.encrypt (data))

    def recv (self, block_size):
        data = monitor_client.recv (self, block_size)
        if data:
            return self.incoming.decrypt (data)
        else:
            return data

def hex_digest (s):
    m = md5.md5()
    m.update (s)
    return string.join (
            map (lambda x: hex (ord (x))[2:], map (None, m.digest())),
            '',
            )

if __name__ == '__main__':
    if len(sys.argv) == 1:
        print 'Usage: %s host port' % sys.argv[0]
        sys.exit(0)

    if ('-e' in sys.argv):
        encrypt = 1
        sys.argv.remove ('-e')
    else:
        encrypt = 0

    sys.stderr.write ('Enter Password: ')
    sys.stderr.flush()
    try:
        os.system ('stty -echo')
        p = raw_input()
        print
    finally:
        os.system ('stty echo')
    stdin = stdin_channel (0)
    if len(sys.argv) > 1:
        if encrypt:
            client = encrypted_monitor_client (p, (sys.argv[1], string.atoi (sys.argv[2])))
            import sapphire
            client.init_cipher (sapphire, p)
        else:
            client = monitor_client (p, (sys.argv[1], string.atoi (sys.argv[2])))
    else:
        # default to local host, 'standard' port
        client = monitor_client (p)
    stdin.sock_channel = client
    asyncore.loop()

########NEW FILE########
__FILENAME__ = monitor_client_win32
# -*- Mode: Python -*-

# monitor client, win32 version

# since we can't do select() on stdin/stdout, we simply
# use threads and blocking sockets.  <sigh>

import socket
import string
import sys
import thread
import md5

def hex_digest (s):
    m = md5.md5()
    m.update (s)
    return string.join (
            map (lambda x: hex (ord (x))[2:], map (None, m.digest())),
            '',
            )

def reader (lock, sock, password):
    # first grab the timestamp
    ts = sock.recv (1024)[:-2]
    sock.send (hex_digest (ts+password) + '\r\n')
    while 1:
        d = sock.recv (1024)
        if not d:
            lock.release()
            print 'Connection closed.  Hit <return> to exit'
            thread.exit()
        sys.stdout.write (d)
        sys.stdout.flush()

def writer (lock, sock, barrel="just kidding"):
    while lock.locked():
        sock.send (
                sys.stdin.readline()[:-1] + '\r\n'
                )

if __name__ == '__main__':
    if len(sys.argv) == 1:
        print 'Usage: %s host port'
        sys.exit(0)
    print 'Enter Password: ',
    p = raw_input()
    s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
    s.connect ((sys.argv[1], string.atoi(sys.argv[2])))
    l = thread.allocate_lock()
    l.acquire()
    thread.start_new_thread (reader, (l, s, p))
    writer (l, s)

########NEW FILE########
__FILENAME__ = m_syslog
# -*- Mode: Python -*-

# ======================================================================
# Copyright 1997 by Sam Rushing
#
#                         All Rights Reserved
#
# Permission to use, copy, modify, and distribute this software and
# its documentation for any purpose and without fee is hereby
# granted, provided that the above copyright notice appear in all
# copies and that both that copyright notice and this permission
# notice appear in supporting documentation, and that the name of Sam
# Rushing not be used in advertising or publicity pertaining to
# distribution of the software without specific, written prior
# permission.
#
# SAM RUSHING DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
# INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN
# NO EVENT SHALL SAM RUSHING BE LIABLE FOR ANY SPECIAL, INDIRECT OR
# CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS
# OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,
# NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
# CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
# ======================================================================

"""socket interface to unix syslog.
On Unix, there are usually two ways of getting to syslog: via a
local unix-domain socket, or via the TCP service.

Usually "/dev/log" is the unix domain socket.  This may be different
for other systems.

>>> my_client = syslog_client ('/dev/log')

Otherwise, just use the UDP version, port 514.

>>> my_client = syslog_client (('my_log_host', 514))

On win32, you will have to use the UDP version.  Note that
you can use this to log to other hosts (and indeed, multiple
hosts).

This module is not a drop-in replacement for the python
<syslog> extension module - the interface is different.

Usage:

>>> c = syslog_client()
>>> c = syslog_client ('/strange/non_standard_log_location')
>>> c = syslog_client (('other_host.com', 514))
>>> c.log ('testing', facility='local0', priority='debug')

"""

# TODO: support named-pipe syslog.
# [see ftp://sunsite.unc.edu/pub/Linux/system/Daemons/syslog-fifo.tar.z]

# from <linux/sys/syslog.h>:
# ===========================================================================
# priorities/facilities are encoded into a single 32-bit quantity, where the
# bottom 3 bits are the priority (0-7) and the top 28 bits are the facility
# (0-big number).  Both the priorities and the facilities map roughly
# one-to-one to strings in the syslogd(8) source code.  This mapping is
# included in this file.
#
# priorities (these are ordered)

LOG_EMERG               = 0             #  system is unusable
LOG_ALERT               = 1             #  action must be taken immediately
LOG_CRIT                = 2             #  critical conditions
LOG_ERR                 = 3             #  error conditions
LOG_WARNING             = 4             #  warning conditions
LOG_NOTICE              = 5             #  normal but significant condition
LOG_INFO                = 6             #  informational
LOG_DEBUG               = 7             #  debug-level messages

#  facility codes
LOG_KERN                = 0             #  kernel messages
LOG_USER                = 1             #  random user-level messages
LOG_MAIL                = 2             #  mail system
LOG_DAEMON              = 3             #  system daemons
LOG_AUTH                = 4             #  security/authorization messages
LOG_SYSLOG              = 5             #  messages generated internally by syslogd
LOG_LPR                 = 6             #  line printer subsystem
LOG_NEWS                = 7             #  network news subsystem
LOG_UUCP                = 8             #  UUCP subsystem
LOG_CRON                = 9             #  clock daemon
LOG_AUTHPRIV    = 10    #  security/authorization messages (private)

#  other codes through 15 reserved for system use
LOG_LOCAL0              = 16            #  reserved for local use
LOG_LOCAL1              = 17            #  reserved for local use
LOG_LOCAL2              = 18            #  reserved for local use
LOG_LOCAL3              = 19            #  reserved for local use
LOG_LOCAL4              = 20            #  reserved for local use
LOG_LOCAL5              = 21            #  reserved for local use
LOG_LOCAL6              = 22            #  reserved for local use
LOG_LOCAL7              = 23            #  reserved for local use

priority_names = {
        "alert":        LOG_ALERT,
        "crit":         LOG_CRIT,
        "debug":        LOG_DEBUG,
        "emerg":        LOG_EMERG,
        "err":          LOG_ERR,
        "error":        LOG_ERR,                #  DEPRECATED
        "info":         LOG_INFO,
        "notice":       LOG_NOTICE,
        "panic":        LOG_EMERG,              #  DEPRECATED
        "warn":         LOG_WARNING,            #  DEPRECATED
        "warning":      LOG_WARNING,
        }

facility_names = {
        "auth":         LOG_AUTH,
        "authpriv":     LOG_AUTHPRIV,
        "cron":         LOG_CRON,
        "daemon":       LOG_DAEMON,
        "kern":         LOG_KERN,
        "lpr":          LOG_LPR,
        "mail":         LOG_MAIL,
        "news":         LOG_NEWS,
        "security":     LOG_AUTH,               #  DEPRECATED
        "syslog":       LOG_SYSLOG,
        "user":         LOG_USER,
        "uucp":         LOG_UUCP,
        "local0":       LOG_LOCAL0,
        "local1":       LOG_LOCAL1,
        "local2":       LOG_LOCAL2,
        "local3":       LOG_LOCAL3,
        "local4":       LOG_LOCAL4,
        "local5":       LOG_LOCAL5,
        "local6":       LOG_LOCAL6,
        "local7":       LOG_LOCAL7,
        }

import socket

class syslog_client:
    def __init__ (self, address='/dev/log'):
        self.address = address
        self.stream = 0
        if isinstance(address, type('')):
            try:
                self.socket = socket.socket(socket.AF_UNIX, socket.SOCK_DGRAM)
                self.socket.connect(address)
            except socket.error:
                # Some Linux installations have /dev/log
                # a stream socket instead of a datagram socket.
                self.socket = socket.socket (socket.AF_UNIX,
                                             socket.SOCK_STREAM)
                self.stream = 1
        else:
            self.socket = socket.socket (socket.AF_INET, socket.SOCK_DGRAM)

    # curious: when talking to the unix-domain '/dev/log' socket, a
    #   zero-terminator seems to be required.  this string is placed
    #   into a class variable so that it can be overridden if
    #   necessary.

    log_format_string = '<%d>%s\000'

    def log (self, message, facility=LOG_USER, priority=LOG_INFO):
        message = self.log_format_string % (
                self.encode_priority (facility, priority),
                message
                )
        if self.stream:
            self.socket.send (message)
        else:
            self.socket.sendto (message, self.address)

    def encode_priority (self, facility, priority):
        if type(facility) == type(''):
            facility = facility_names[facility]
        if type(priority) == type(''):
            priority = priority_names[priority]
        return (facility<<3) | priority

    def close (self):
        if self.stream:
            self.socket.close()

########NEW FILE########
__FILENAME__ = producers
# -*- Mode: Python -*-

RCS_ID = '$Id: producers.py,v 1.9 2004/04/21 13:56:28 akuchling Exp $'

"""
A collection of producers.
Each producer implements a particular feature:  They can be combined
in various ways to get interesting and useful behaviors.

For example, you can feed dynamically-produced output into the compressing
producer, then wrap this with the 'chunked' transfer-encoding producer.
"""

import string
from asynchat import find_prefix_at_end

class simple_producer:
    "producer for a string"
    def __init__ (self, data, buffer_size=1024):
        self.data = data
        self.buffer_size = buffer_size

    def more (self):
        if len (self.data) > self.buffer_size:
            result = self.data[:self.buffer_size]
            self.data = self.data[self.buffer_size:]
            return result
        else:
            result = self.data
            self.data = ''
            return result

class scanning_producer:
    "like simple_producer, but more efficient for large strings"
    def __init__ (self, data, buffer_size=1024):
        self.data = data
        self.buffer_size = buffer_size
        self.pos = 0

    def more (self):
        if self.pos < len(self.data):
            lp = self.pos
            rp = min (
                    len(self.data),
                    self.pos + self.buffer_size
                    )
            result = self.data[lp:rp]
            self.pos = self.pos + len(result)
            return result
        else:
            return ''

class lines_producer:
    "producer for a list of lines"

    def __init__ (self, lines):
        self.lines = lines

    def more (self):
        if self.lines:
            chunk = self.lines[:50]
            self.lines = self.lines[50:]
            return string.join (chunk, '\r\n') + '\r\n'
        else:
            return ''

class buffer_list_producer:
    "producer for a list of strings"

    # i.e., data == string.join (buffers, '')

    def __init__ (self, buffers):

        self.index = 0
        self.buffers = buffers

    def more (self):
        if self.index >= len(self.buffers):
            return ''
        else:
            data = self.buffers[self.index]
            self.index = self.index + 1
            return data

class file_producer:
    "producer wrapper for file[-like] objects"

    # match http_channel's outgoing buffer size
    out_buffer_size = 1<<16

    def __init__ (self, file):
        self.done = 0
        self.file = file

    def more (self):
        if self.done:
            return ''
        else:
            data = self.file.read (self.out_buffer_size)
            if not data:
                self.file.close()
                del self.file
                self.done = 1
                return ''
            else:
                return data

# A simple output producer.  This one does not [yet] have
# the safety feature builtin to the monitor channel:  runaway
# output will not be caught.

# don't try to print from within any of the methods
# of this object.

class output_producer:
    "Acts like an output file; suitable for capturing sys.stdout"
    def __init__ (self):
        self.data = ''

    def write (self, data):
        lines = string.splitfields (data, '\n')
        data = string.join (lines, '\r\n')
        self.data = self.data + data

    def writeline (self, line):
        self.data = self.data + line + '\r\n'

    def writelines (self, lines):
        self.data = self.data + string.joinfields (
                lines,
                '\r\n'
                ) + '\r\n'

    def flush (self):
        pass

    def softspace (self, *args):
        pass

    def more (self):
        if self.data:
            result = self.data[:512]
            self.data = self.data[512:]
            return result
        else:
            return ''

class composite_producer:
    "combine a fifo of producers into one"
    def __init__ (self, producers):
        self.producers = producers

    def more (self):
        while len(self.producers):
            p = self.producers[0]
            d = p.more()
            if d:
                return d
            else:
                self.producers.pop(0)
        else:
            return ''


class globbing_producer:
    """
    'glob' the output from a producer into a particular buffer size.
    helps reduce the number of calls to send().  [this appears to
    gain about 30% performance on requests to a single channel]
    """

    def __init__ (self, producer, buffer_size=1<<16):
        self.producer = producer
        self.buffer = ''
        self.buffer_size = buffer_size

    def more (self):
        while len(self.buffer) < self.buffer_size:
            data = self.producer.more()
            if data:
                self.buffer = self.buffer + data
            else:
                break
        r = self.buffer
        self.buffer = ''
        return r


class hooked_producer:
    """
    A producer that will call <function> when it empties,.
    with an argument of the number of bytes produced.  Useful
    for logging/instrumentation purposes.
    """

    def __init__ (self, producer, function):
        self.producer = producer
        self.function = function
        self.bytes = 0

    def more (self):
        if self.producer:
            result = self.producer.more()
            if not result:
                self.producer = None
                self.function (self.bytes)
            else:
                self.bytes = self.bytes + len(result)
            return result
        else:
            return ''

# HTTP 1.1 emphasizes that an advertised Content-Length header MUST be
# correct.  In the face of Strange Files, it is conceivable that
# reading a 'file' may produce an amount of data not matching that
# reported by os.stat() [text/binary mode issues, perhaps the file is
# being appended to, etc..]  This makes the chunked encoding a True
# Blessing, and it really ought to be used even with normal files.
# How beautifully it blends with the concept of the producer.

class chunked_producer:
    """A producer that implements the 'chunked' transfer coding for HTTP/1.1.
    Here is a sample usage:
            request['Transfer-Encoding'] = 'chunked'
            request.push (
                    producers.chunked_producer (your_producer)
                    )
            request.done()
    """

    def __init__ (self, producer, footers=None):
        self.producer = producer
        self.footers = footers

    def more (self):
        if self.producer:
            data = self.producer.more()
            if data:
                return '%x\r\n%s\r\n' % (len(data), data)
            else:
                self.producer = None
                if self.footers:
                    return string.join (
                            ['0'] + self.footers,
                            '\r\n'
                            ) + '\r\n\r\n'
                else:
                    return '0\r\n\r\n'
        else:
            return ''

try:
    import zlib
except ImportError:
    zlib = None

class compressed_producer:
    """
    Compress another producer on-the-fly, using ZLIB
    """

    # Note: It's not very efficient to have the server repeatedly
    # compressing your outgoing files: compress them ahead of time, or
    # use a compress-once-and-store scheme.  However, if you have low
    # bandwidth and low traffic, this may make more sense than
    # maintaining your source files compressed.
    #
    # Can also be used for compressing dynamically-produced output.

    def __init__ (self, producer, level=5):
        self.producer = producer
        self.compressor = zlib.compressobj (level)

    def more (self):
        if self.producer:
            cdata = ''
            # feed until we get some output
            while not cdata:
                data = self.producer.more()
                if not data:
                    self.producer = None
                    return self.compressor.flush()
                else:
                    cdata = self.compressor.compress (data)
            return cdata
        else:
            return ''

class escaping_producer:

    "A producer that escapes a sequence of characters"
    " Common usage: escaping the CRLF.CRLF sequence in SMTP, NNTP, etc..."

    def __init__ (self, producer, esc_from='\r\n.', esc_to='\r\n..'):
        self.producer = producer
        self.esc_from = esc_from
        self.esc_to = esc_to
        self.buffer = ''
        self.find_prefix_at_end = find_prefix_at_end

    def more (self):
        esc_from = self.esc_from
        esc_to   = self.esc_to

        buffer = self.buffer + self.producer.more()

        if buffer:
            buffer = string.replace (buffer, esc_from, esc_to)
            i = self.find_prefix_at_end (buffer, esc_from)
            if i:
                # we found a prefix
                self.buffer = buffer[-i:]
                return buffer[:-i]
            else:
                # no prefix, return it all
                self.buffer = ''
                return buffer
        else:
            return buffer

########NEW FILE########
__FILENAME__ = put_handler
# -*- Mode: Python -*-
#
#       Author: Sam Rushing <rushing@nightmare.com>
#       Copyright 1996-2000 by Sam Rushing
#                                                All Rights Reserved.
#

RCS_ID =  '$Id: put_handler.py,v 1.4 2002/08/01 18:15:45 akuchling Exp $'

import re
import string

import default_handler
unquote         = default_handler.unquote
get_header      = default_handler.get_header

last_request = None

class put_handler:
    def __init__ (self, filesystem, uri_regex):
        self.filesystem = filesystem
        if type (uri_regex) == type(''):
            self.uri_regex = re.compile (uri_regex)
        else:
            self.uri_regex = uri_regex

    def match (self, request):
        uri = request.uri
        if request.command == 'PUT':
            m = self.uri_regex.match (uri)
            if m and m.end() == len(uri):
                return 1
        return 0

    def handle_request (self, request):

        path, params, query, fragment = request.split_uri()

        # strip off leading slashes
        while path and path[0] == '/':
            path = path[1:]

        if '%' in path:
            path = unquote (path)

        # make sure there's a content-length header
        cl = get_header (CONTENT_LENGTH, request.header)
        if not cl:
            request.error (411)
            return
        else:
            cl = string.atoi (cl)

        # don't let the try to overwrite a directory
        if self.filesystem.isdir (path):
            request.error (405)
            return

        is_update = self.filesystem.isfile (path)

        try:
            output_file = self.filesystem.open (path, 'wb')
        except:
            request.error (405)
            return

        request.collector = put_collector (output_file, cl, request, is_update)

        # no terminator while receiving PUT data
        request.channel.set_terminator (None)

        # don't respond yet, wait until we've received the data...

class put_collector:
    def __init__ (self, file, length, request, is_update):
        self.file               = file
        self.length             = length
        self.request    = request
        self.is_update  = is_update
        self.bytes_in   = 0

    def collect_incoming_data (self, data):
        ld = len(data)
        bi = self.bytes_in
        if (bi + ld) >= self.length:
            # last bit of data
            chunk = self.length - bi
            self.file.write (data[:chunk])
            self.file.close()

            if chunk != ld:
                print 'orphaned %d bytes: <%s>' % (ld - chunk, repr(data[chunk:]))

            # do some housekeeping
            r = self.request
            ch = r.channel
            ch.current_request = None
            # set the terminator back to the default
            ch.set_terminator ('\r\n\r\n')
            if self.is_update:
                r.reply_code = 204 # No content
                r.done()
            else:
                r.reply_now (201) # Created
            # avoid circular reference
            del self.request
        else:
            self.file.write (data)
            self.bytes_in = self.bytes_in + ld

    def found_terminator (self):
        # shouldn't be called
        pass

CONTENT_LENGTH = re.compile ('Content-Length: ([0-9]+)', re.IGNORECASE)

########NEW FILE########
__FILENAME__ = redirecting_handler
# -*- Mode: Python -*-
#
#       Author: Sam Rushing <rushing@nightmare.com>
#       Copyright 1996-2000 by Sam Rushing
#                                                All Rights Reserved.
#

RCS_ID =  '$Id: redirecting_handler.py,v 1.4 2002/03/20 17:37:48 amk Exp $'

import re
import counter

class redirecting_handler:

    def __init__ (self, pattern, redirect, regex_flag=re.IGNORECASE):
        self.pattern = pattern
        self.redirect = redirect
        self.patreg = re.compile (pattern, regex_flag)
        self.hits = counter.counter()

    def match (self, request):
        m = self.patreg.match (request.uri)
        return (m and (m.end() == len(request.uri)))

    def handle_request (self, request):
        self.hits.increment()
        m = self.patreg.match (request.uri)
        part = m.group(1)

        request['Location'] = self.redirect % part
        request.error (302) # moved temporarily

    def __repr__ (self):
        return '<Redirecting Handler at %08x [%s => %s]>' % (
                id(self),
                repr(self.pattern),
                repr(self.redirect)
                )

    def status (self):
        import producers
        return producers.simple_producer (
                '<li> Redirecting Handler %s => %s <b>Hits</b>: %s' % (
                        self.pattern, self.redirect, self.hits
                        )
                )

########NEW FILE########
__FILENAME__ = resolver
# -*- Mode: Python -*-

#
#       Author: Sam Rushing <rushing@nightmare.com>
#

RCS_ID =  '$Id: resolver.py,v 1.4 2002/03/20 17:37:48 amk Exp $'


# Fast, low-overhead asynchronous name resolver.  uses 'pre-cooked'
# DNS requests, unpacks only as much as it needs of the reply.

# see rfc1035 for details

import string
import asyncore_25 as asyncore
import socket
import sys
import time
from counter import counter

VERSION = string.split(RCS_ID)[2]

# header
#                                    1  1  1  1  1  1
#      0  1  2  3  4  5  6  7  8  9  0  1  2  3  4  5
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                      ID                       |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |QR|   Opcode  |AA|TC|RD|RA|   Z    |   RCODE   |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                    QDCOUNT                    |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                    ANCOUNT                    |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                    NSCOUNT                    |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                    ARCOUNT                    |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+


# question
#                                    1  1  1  1  1  1
#      0  1  2  3  4  5  6  7  8  9  0  1  2  3  4  5
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                                               |
#    /                     QNAME                     /
#    /                                               /
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                     QTYPE                     |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                     QCLASS                    |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+

# build a DNS address request, _quickly_
def fast_address_request (host, id=0):
    return (
            '%c%c' % (chr((id>>8)&0xff),chr(id&0xff))
            + '\001\000\000\001\000\000\000\000\000\000%s\000\000\001\000\001' % (
                    string.join (
                            map (
                                    lambda part: '%c%s' % (chr(len(part)),part),
                                    string.split (host, '.')
                                    ), ''
                            )
                    )
            )

def fast_ptr_request (host, id=0):
    return (
            '%c%c' % (chr((id>>8)&0xff),chr(id&0xff))
            + '\001\000\000\001\000\000\000\000\000\000%s\000\000\014\000\001' % (
                    string.join (
                            map (
                                    lambda part: '%c%s' % (chr(len(part)),part),
                                    string.split (host, '.')
                                    ), ''
                            )
                    )
            )

def unpack_name (r,pos):
    n = []
    while 1:
        ll = ord(r[pos])
        if (ll&0xc0):
            # compression
            pos = (ll&0x3f << 8) + (ord(r[pos+1]))
        elif ll == 0:
            break
        else:
            pos = pos + 1
            n.append (r[pos:pos+ll])
            pos = pos + ll
    return string.join (n,'.')

def skip_name (r,pos):
    s = pos
    while 1:
        ll = ord(r[pos])
        if (ll&0xc0):
            # compression
            return pos + 2
        elif ll == 0:
            pos = pos + 1
            break
        else:
            pos = pos + ll + 1
    return pos

def unpack_ttl (r,pos):
    return reduce (
            lambda x,y: (x<<8)|y,
            map (ord, r[pos:pos+4])
            )

# resource record
#                                    1  1  1  1  1  1
#      0  1  2  3  4  5  6  7  8  9  0  1  2  3  4  5
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                                               |
#    /                                               /
#    /                      NAME                     /
#    |                                               |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                      TYPE                     |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                     CLASS                     |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                      TTL                      |
#    |                                               |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
#    |                   RDLENGTH                    |
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--|
#    /                     RDATA                     /
#    /                                               /
#    +--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+

def unpack_address_reply (r):
    ancount = (ord(r[6])<<8) + (ord(r[7]))
    # skip question, first name starts at 12,
    # this is followed by QTYPE and QCLASS
    pos = skip_name (r, 12) + 4
    if ancount:
        # we are looking very specifically for
        # an answer with TYPE=A, CLASS=IN (\000\001\000\001)
        for an in range(ancount):
            pos = skip_name (r, pos)
            if r[pos:pos+4] == '\000\001\000\001':
                return (
                        unpack_ttl (r,pos+4),
                        '%d.%d.%d.%d' % tuple(map(ord,r[pos+10:pos+14]))
                        )
            # skip over TYPE, CLASS, TTL, RDLENGTH, RDATA
            pos = pos + 8
            rdlength = (ord(r[pos])<<8) + (ord(r[pos+1]))
            pos = pos + 2 + rdlength
        return 0, None
    else:
        return 0, None

def unpack_ptr_reply (r):
    ancount = (ord(r[6])<<8) + (ord(r[7]))
    # skip question, first name starts at 12,
    # this is followed by QTYPE and QCLASS
    pos = skip_name (r, 12) + 4
    if ancount:
        # we are looking very specifically for
        # an answer with TYPE=PTR, CLASS=IN (\000\014\000\001)
        for an in range(ancount):
            pos = skip_name (r, pos)
            if r[pos:pos+4] == '\000\014\000\001':
                return (
                        unpack_ttl (r,pos+4),
                        unpack_name (r, pos+10)
                        )
            # skip over TYPE, CLASS, TTL, RDLENGTH, RDATA
            pos = pos + 8
            rdlength = (ord(r[pos])<<8) + (ord(r[pos+1]))
            pos = pos + 2 + rdlength
        return 0, None
    else:
        return 0, None


# This is a UDP (datagram) resolver.

#
# It may be useful to implement a TCP resolver.  This would presumably
# give us more reliable behavior when things get too busy.  A TCP
# client would have to manage the connection carefully, since the
# server is allowed to close it at will (the RFC recommends closing
# after 2 minutes of idle time).
#
# Note also that the TCP client will have to prepend each request
# with a 2-byte length indicator (see rfc1035).
#

class resolver (asyncore.dispatcher):
    id = counter()
    def __init__ (self, server='127.0.0.1'):
        asyncore.dispatcher.__init__ (self)
        self.create_socket (socket.AF_INET, socket.SOCK_DGRAM)
        self.server = server
        self.request_map = {}
        self.last_reap_time = int(time.time())      # reap every few minutes

    def writable (self):
        return 0

    def log (self, *args):
        pass

    def handle_close (self):
        self.log_info('closing!')
        self.close()

    def handle_error (self):      # don't close the connection on error
        (file,fun,line), t, v, tbinfo = asyncore.compact_traceback()
        self.log_info(
                        'Problem with DNS lookup (%s:%s %s)' % (t, v, tbinfo),
                        'error')

    def get_id (self):
        return (self.id.as_long() % (1<<16))

    def reap (self):          # find DNS requests that have timed out
        now = int(time.time())
        if now - self.last_reap_time > 180:        # reap every 3 minutes
            self.last_reap_time = now              # update before we forget
            for k,(host,unpack,callback,when) in self.request_map.items():
                if now - when > 180:               # over 3 minutes old
                    del self.request_map[k]
                    try:                           # same code as in handle_read
                        callback (host, 0, None)   # timeout val is (0,None)
                    except:
                        (file,fun,line), t, v, tbinfo = asyncore.compact_traceback()
                        self.log_info('%s %s %s' % (t,v,tbinfo), 'error')

    def resolve (self, host, callback):
        self.reap()                                # first, get rid of old guys
        self.socket.sendto (
                fast_address_request (host, self.get_id()),
                (self.server, 53)
                )
        self.request_map [self.get_id()] = (
                host, unpack_address_reply, callback, int(time.time()))
        self.id.increment()

    def resolve_ptr (self, host, callback):
        self.reap()                                # first, get rid of old guys
        ip = string.split (host, '.')
        ip.reverse()
        ip = string.join (ip, '.') + '.in-addr.arpa'
        self.socket.sendto (
                fast_ptr_request (ip, self.get_id()),
                (self.server, 53)
                )
        self.request_map [self.get_id()] = (
                host, unpack_ptr_reply, callback, int(time.time()))
        self.id.increment()

    def handle_read (self):
        reply, whence = self.socket.recvfrom (512)
        # for security reasons we may want to double-check
        # that <whence> is the server we sent the request to.
        id = (ord(reply[0])<<8) + ord(reply[1])
        if self.request_map.has_key (id):
            host, unpack, callback, when = self.request_map[id]
            del self.request_map[id]
            ttl, answer = unpack (reply)
            try:
                callback (host, ttl, answer)
            except:
                (file,fun,line), t, v, tbinfo = asyncore.compact_traceback()
                self.log_info('%s %s %s' % ( t,v,tbinfo), 'error')

class rbl (resolver):

    def resolve_maps (self, host, callback):
        ip = string.split (host, '.')
        ip.reverse()
        ip = string.join (ip, '.') + '.rbl.maps.vix.com'
        self.socket.sendto (
                fast_ptr_request (ip, self.get_id()),
                (self.server, 53)
                )
        self.request_map [self.get_id()] = host, self.check_reply, callback
        self.id.increment()

    def check_reply (self, r):
        # we only need to check RCODE.
        rcode = (ord(r[3])&0xf)
        self.log_info('MAPS RBL; RCODE =%02x\n %s' % (rcode, repr(r)))
        return 0, rcode # (ttl, answer)


class hooked_callback:
    def __init__ (self, hook, callback):
        self.hook, self.callback = hook, callback

    def __call__ (self, *args):
        apply (self.hook, args)
        apply (self.callback, args)

class caching_resolver (resolver):
    "Cache DNS queries.  Will need to honor the TTL value in the replies"

    def __init__ (*args):
        apply (resolver.__init__, args)
        self = args[0]
        self.cache = {}
        self.forward_requests = counter()
        self.reverse_requests = counter()
        self.cache_hits = counter()

    def resolve (self, host, callback):
        self.forward_requests.increment()
        if self.cache.has_key (host):
            when, ttl, answer = self.cache[host]
            # ignore TTL for now
            callback (host, ttl, answer)
            self.cache_hits.increment()
        else:
            resolver.resolve (
                    self,
                    host,
                    hooked_callback (
                            self.callback_hook,
                            callback
                            )
                    )

    def resolve_ptr (self, host, callback):
        self.reverse_requests.increment()
        if self.cache.has_key (host):
            when, ttl, answer = self.cache[host]
            # ignore TTL for now
            callback (host, ttl, answer)
            self.cache_hits.increment()
        else:
            resolver.resolve_ptr (
                    self,
                    host,
                    hooked_callback (
                            self.callback_hook,
                            callback
                            )
                    )

    def callback_hook (self, host, ttl, answer):
        self.cache[host] = time.time(), ttl, answer

    SERVER_IDENT = 'Caching DNS Resolver (V%s)' % VERSION

    def status (self):
        import producers
        return producers.simple_producer (
                '<h2>%s</h2>'                                   % self.SERVER_IDENT
                + '<br>Server: %s'                              % self.server
                + '<br>Cache Entries: %d'               % len(self.cache)
                + '<br>Outstanding Requests: %d' % len(self.request_map)
                + '<br>Forward Requests: %s'    % self.forward_requests
                + '<br>Reverse Requests: %s'    % self.reverse_requests
                + '<br>Cache Hits: %s'                  % self.cache_hits
                )

#test_reply = """\000\000\205\200\000\001\000\001\000\002\000\002\006squirl\011nightmare\003com\000\000\001\000\001\300\014\000\001\000\001\000\001Q\200\000\004\315\240\260\005\011nightmare\003com\000\000\002\000\001\000\001Q\200\000\002\300\014\3006\000\002\000\001\000\001Q\200\000\015\003ns1\003iag\003net\000\300\014\000\001\000\001\000\001Q\200\000\004\315\240\260\005\300]\000\001\000\001\000\000\350\227\000\004\314\033\322\005"""
# def test_unpacker ():
#       print unpack_address_reply (test_reply)
#
# import time
# class timer:
#       def __init__ (self):
#               self.start = time.time()
#       def end (self):
#               return time.time() - self.start
#
# # I get ~290 unpacks per second for the typical case, compared to ~48
# # using dnslib directly.  also, that latter number does not include
# # picking the actual data out.
#
# def benchmark_unpacker():
#
#       r = range(1000)
#       t = timer()
#       for i in r:
#               unpack_address_reply (test_reply)
#       print '%.2f unpacks per second' % (1000.0 / t.end())

if __name__ == '__main__':
    if len(sys.argv) == 1:
        print 'usage: %s [-r] [-s <server_IP>] host [host ...]' % sys.argv[0]
        sys.exit(0)
    elif ('-s' in sys.argv):
        i = sys.argv.index('-s')
        server = sys.argv[i+1]
        del sys.argv[i:i+2]
    else:
        server = '127.0.0.1'

    if ('-r' in sys.argv):
        reverse = 1
        i = sys.argv.index('-r')
        del sys.argv[i]
    else:
        reverse = 0

    if ('-m' in sys.argv):
        maps = 1
        sys.argv.remove ('-m')
    else:
        maps = 0

    if maps:
        r = rbl (server)
    else:
        r = caching_resolver(server)

    count = len(sys.argv) - 1

    def print_it (host, ttl, answer):
        global count
        print '%s: %s' % (host, answer)
        count = count - 1
        if not count:
            r.close()

    for host in sys.argv[1:]:
        if reverse:
            r.resolve_ptr (host, print_it)
        elif maps:
            r.resolve_maps (host, print_it)
        else:
            r.resolve (host, print_it)

    # hooked asyncore.loop()
    while asyncore.socket_map:
        asyncore.poll (30.0)
        print 'requests outstanding: %d' % len(r.request_map)

########NEW FILE########
__FILENAME__ = rpc_client
# -*- Mode: Python -*-

# Copyright 1999, 2000 by eGroups, Inc.
#
#                         All Rights Reserved
#
# Permission to use, copy, modify, and distribute this software and
# its documentation for any purpose and without fee is hereby
# granted, provided that the above copyright notice appear in all
# copies and that both that copyright notice and this permission
# notice appear in supporting documentation, and that the name of
# eGroups not be used in advertising or publicity pertaining to
# distribution of the software without specific, written prior
# permission.
#
# EGROUPS DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
# INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN
# NO EVENT SHALL EGROUPS BE LIABLE FOR ANY SPECIAL, INDIRECT OR
# CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS
# OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,
# NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
# CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

import marshal
import socket
import string
import exceptions
import string
import sys

#
# there are three clients in here.
#
# 1) rpc client
# 2) fastrpc client
# 3) async fastrpc client
#
# we hope that *whichever* choice you make, that you will enjoy the
# excellent hand-made construction, and return to do business with us
# again in the near future.
#

class RPC_Error (exceptions.StandardError):
    pass

# ===========================================================================
#                                                         RPC Client
# ===========================================================================

# request types:
# 0 call
# 1 getattr
# 2 setattr
# 3 repr
# 4 del


class rpc_proxy:

    DEBUG = 0

    def __init__ (self, conn, oid):
        # route around __setattr__
        self.__dict__['conn'] = conn
        self.__dict__['oid'] = oid

    # Warning: be VERY CAREFUL with attribute references, keep
    #             this __getattr__ in mind!

    def __getattr__ (self, attr):
        # __getattr__ and __call__
        if attr == '__call__':
            # 0 == __call__
            return self.__remote_call__
        elif attr == '__repr__':
            # 3 == __repr__
            return self.__remote_repr__
        elif attr == '__getitem__':
            return self.__remote_getitem__
        elif attr == '__setitem__':
            return self.__remote_setitem__
        elif attr == '__len__':
            return self.__remote_len__
        else:
            # 1 == __getattr__
            return self.__send_request__ (1, attr)

    def __setattr__ (self, attr, value):
        return self.__send_request__ (2, (attr, value))

    def __del__ (self):
        try:
            self.__send_request__ (4, None)
        except:
            import who_calls
            info = who_calls.compact_traceback()
            print info

    def __remote_repr__ (self):
        r = self.__send_request__ (3, None)
        return '<remote object [%s]>' % r[1:-1]

    def __remote_call__ (self, *args):
        return self.__send_request__ (0, args)

    def __remote_getitem__ (self, key):
        return self.__send_request__ (5, key)

    def __remote_setitem__ (self, key, value):
        return self.__send_request__ (6, (key, value))

    def __remote_len__ (self):
        return self.__send_request__ (7, None)

    _request_types_ = ['call', 'getattr', 'setattr', 'repr', 'del', 'getitem', 'setitem', 'len']

    def __send_request__ (self, *args):
        if self.DEBUG:
            kind = args[0]
            print (
                    'RPC: ==> %s:%08x:%s:%s' % (
                            self.conn.address,
                            self.oid,
                            self._request_types_[kind],
                            repr(args[1:])
                            )
                    )
        packet = marshal.dumps ((self.oid,)+args)
        # send request
        self.conn.send_packet (packet)
        # get response
        data = self.conn.receive_packet()
        # types of response:
        # 0: proxy
        # 1: error
        # 2: marshal'd data

        kind, value = marshal.loads (data)

        if kind == 0:
            # proxy (value == oid)
            if self.DEBUG:
                print 'RPC: <== proxy(%08x)' % (value)
            return rpc_proxy (self.conn, value)
        elif kind == 1:
            raise RPC_Error, value
        else:
            if self.DEBUG:
                print 'RPC: <== %s' % (repr(value))
            return value

class rpc_connection:

    cache = {}

    def __init__ (self, address):
        self.address = address
        self.connect ()

    def connect (self):
        s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
        s.connect (self.address)
        self.socket = s

    def receive_packet (self):
        packet_len = string.atoi (self.socket.recv (8), 16)
        packet = []
        while packet_len:
            data = self.socket.recv (8192)
            packet.append (data)
            packet_len = packet_len - len(data)
        return string.join (packet, '')

    def send_packet (self, packet):
        self.socket.send ('%08x%s' % (len(packet), packet))

def rpc_connect (address = ('localhost', 8746)):
    if not rpc_connection.cache.has_key (address):
        conn = rpc_connection (address)
        # get oid of remote object
        data = conn.receive_packet()
        (oid,) = marshal.loads (data)
        rpc_connection.cache[address] = rpc_proxy (conn, oid)
    return rpc_connection.cache[address]

# ===========================================================================
#                       fastrpc client
# ===========================================================================

class fastrpc_proxy:

    def __init__ (self, conn, path=()):
        self.conn = conn
        self.path = path

    def __getattr__ (self, attr):
        if attr == '__call__':
            return self.__method_caller__
        else:
            return fastrpc_proxy (self.conn, self.path + (attr,))

    def __method_caller__ (self, *args):
        # send request
        packet = marshal.dumps ((self.path, args))
        self.conn.send_packet (packet)
        # get response
        data = self.conn.receive_packet()
        error, result = marshal.loads (data)
        if error is None:
            return result
        else:
            raise RPC_Error, error

    def __repr__ (self):
        return '<remote-method-%s at %x>' % (string.join (self.path, '.'), id (self))

def fastrpc_connect (address = ('localhost', 8748)):
    if not rpc_connection.cache.has_key (address):
        conn = rpc_connection (address)
        rpc_connection.cache[address] = fastrpc_proxy (conn)
    return rpc_connection.cache[address]

# ===========================================================================
#                                                async fastrpc client
# ===========================================================================

import asynchat_25 as asynchat

class async_fastrpc_client (asynchat.async_chat):

    STATE_LENGTH = 'length state'
    STATE_PACKET = 'packet state'

    def __init__ (self, address=('idb', 3001)):

        asynchat.async_chat.__init__ (self)

        if type(address) is type(''):
            family = socket.AF_UNIX
        else:
            family = socket.AF_INET

        self.create_socket (family, socket.SOCK_STREAM)
        self.address = address
        self.request_fifo = []
        self.buffer = []
        self.pstate = self.STATE_LENGTH
        self.set_terminator (8)
        self._connected = 0
        self.connect (self.address)

    def log (self, *args):
        pass

    def handle_connect (self):
        self._connected = 1

    def close (self):
        self._connected = 0
        self.flush_pending_requests ('lost connection to rpc server')
        asynchat.async_chat.close(self)

    def flush_pending_requests (self, why):
        f = self.request_fifo
        while len(f):
            callback = f.pop(0)
            callback (why, None)

    def collect_incoming_data (self, data):
        self.buffer.append (data)

    def found_terminator (self):
        self.buffer, data = [], string.join (self.buffer, '')

        if self.pstate is self.STATE_LENGTH:
            packet_length = string.atoi (data, 16)
            self.set_terminator (packet_length)
            self.pstate = self.STATE_PACKET
        else:
            # modified to fix socket leak in chat server, 2000-01-27, schiller@eGroups.net
            #self.set_terminator (8)
            #self.pstate = self.STATE_LENGTH
            error, result = marshal.loads (data)
            callback = self.request_fifo.pop(0)
            callback (error, result)
            self.close()    # for chat server

    def call_method (self, method, args, callback):
        if not self._connected:
            # might be a unix socket...
            family, type = self.family_and_type
            self.create_socket (family, type)
            self.connect (self.address)
        # push the request out the socket
        path = string.split (method, '.')
        packet = marshal.dumps ((path, args))
        self.push ('%08x%s' % (len(packet), packet))
        self.request_fifo.append(callback)


if __name__ == '__main__':
    if '-f' in sys.argv:
        connect = fastrpc_connect
    else:
        connect = rpc_connect

    print 'connecting...'
    c = connect()
    print 'calling <remote>.calc.sum (1,2,3)'
    print c.calc.sum (1,2,3)
    print 'calling <remote>.calc.nonexistent(), expect an exception!'
    print c.calc.nonexistent()

########NEW FILE########
__FILENAME__ = rpc_server
# -*- Mode: Python -*-

# Copyright 1999, 2000 by eGroups, Inc.
#
#                         All Rights Reserved
#
# Permission to use, copy, modify, and distribute this software and
# its documentation for any purpose and without fee is hereby
# granted, provided that the above copyright notice appear in all
# copies and that both that copyright notice and this permission
# notice appear in supporting documentation, and that the name of
# eGroups not be used in advertising or publicity pertaining to
# distribution of the software without specific, written prior
# permission.
#
# EGROUPS DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
# INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS, IN
# NO EVENT SHALL EGROUPS BE LIABLE FOR ANY SPECIAL, INDIRECT OR
# CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS
# OF USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT,
# NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF OR IN
# CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

# There are two RPC implementations here.

# The first ('rpc') attempts to be as transparent as possible, and
# passes along 'internal' methods like __getattr__, __getitem__, and
# __del__.  It is rather 'chatty', and may not be suitable for a
# high-performance system.

# The second ('fastrpc') is less flexible, but has much less overhead,
# and is easier to use from an asynchronous client.

import marshal
import socket
import string
import sys
import types

import asyncore_25 as asyncore
import asynchat_25 as asynchat

from producers import scanning_producer
from counter import counter

MY_NAME = string.split (socket.gethostname(), '.')[0]

# ===========================================================================
#                                                         RPC server
# ===========================================================================

# marshal is good for low-level data structures.
# but when passing an 'object' (any non-marshallable object)
# we really want to pass a 'reference', which will act on
# the other side as a proxy.  How transparent can we make this?

class rpc_channel (asynchat.async_chat):

    'Simple RPC server.'

    # a 'packet': NNNNNNNNmmmmmmmmmmmmmmmm
    # (hex length in 8 bytes, followed by marshal'd packet data)
    # same protocol used in both directions.

    STATE_LENGTH = 'length state'
    STATE_PACKET = 'packet state'

    ac_out_buffer_size = 65536

    request_counter = counter()
    exception_counter = counter()
    client_counter = counter()

    def __init__ (self, root, conn, addr):
        self.root = root
        self.addr = addr
        asynchat.async_chat.__init__ (self, conn)
        self.pstate = self.STATE_LENGTH
        self.set_terminator (8)
        self.buffer = []
        self.proxies = {}
        rid = id(root)
        self.new_reference (root)
        p = marshal.dumps ((rid,))
        # send root oid to the other side
        self.push ('%08x%s' % (len(p), p))
        self.client_counter.increment()

    def new_reference (self, object):
        oid = id(object)
        ignore, refcnt = self.proxies.get (oid, (None, 0))
        self.proxies[oid] = (object, refcnt + 1)

    def forget_reference (self, oid):
        object, refcnt = self.proxies.get (oid, (None, 0))
        if refcnt > 1:
            self.proxies[oid] = (object, refcnt - 1)
        else:
            del self.proxies[oid]

    def log (self, *ignore):
        pass

    def collect_incoming_data (self, data):
        self.buffer.append (data)

    def found_terminator (self):
        self.buffer, data = [], string.join (self.buffer, '')

        if self.pstate is self.STATE_LENGTH:
            packet_length = string.atoi (data, 16)
            self.set_terminator (packet_length)
            self.pstate = self.STATE_PACKET
        else:

            self.set_terminator (8)
            self.pstate = self.STATE_LENGTH

            oid, kind, arg = marshal.loads (data)

            obj, refcnt = self.proxies[oid]
            e = None
            reply_kind = 2

            try:
                if kind == 0:
                    # __call__
                    result = apply (obj, arg)
                elif kind == 1:
                    # __getattr__
                    result = getattr (obj, arg)
                elif kind == 2:
                    # __setattr__
                    key, value = arg
                    setattr (obj, key, value)
                    result = None
                elif kind == 3:
                    # __repr__
                    result = repr(obj)
                elif kind == 4:
                    # __del__
                    self.forget_reference (oid)
                    result = None
                elif kind == 5:
                    # __getitem__
                    result = obj[arg]
                elif kind == 6:
                    # __setitem__
                    (key, value) = arg
                    obj[key] = value
                    result = None
                elif kind == 7:
                    # __len__
                    result = len(obj)

            except:
                reply_kind = 1
                (file,fun,line), t, v, tbinfo = asyncore.compact_traceback()
                result = '%s:%s:%s:%s (%s:%s)' % (MY_NAME, file, fun, line, t, str(v))
                self.log_info (result, 'error')
                self.exception_counter.increment()

            self.request_counter.increment()

            # optimize a common case
            if type(result) is types.InstanceType:
                can_marshal = 0
            else:
                can_marshal = 1

            try:
                rb = marshal.dumps ((reply_kind, result))
            except ValueError:
                can_marshal = 0

            if not can_marshal:
                # unmarshallable object, return a reference
                rid = id(result)
                self.new_reference (result)
                rb = marshal.dumps ((0, rid))

            self.push_with_producer (
                    scanning_producer (
                            ('%08x' % len(rb)) + rb,
                            buffer_size = 65536
                            )
                    )

class rpc_server_root:
    pass

class rpc_server (asyncore.dispatcher):

    def __init__ (self, root, address = ('', 8746)):
        self.create_socket (socket.AF_INET, socket.SOCK_STREAM)
        self.set_reuse_addr()
        self.bind (address)
        self.listen (128)
        self.root = root

    def handle_accept (self):
        conn, addr = self.accept()
        rpc_channel (self.root, conn, addr)


# ===========================================================================
#                                                  Fast RPC server
# ===========================================================================

# no proxies, request consists
# of a 'chain' of getattrs terminated by a __call__.

# Protocol:
# <path>.<to>.<object> ( <param1>, <param2>, ... )
# => ( <value1>, <value2>, ... )
#
#
# (<path>, <params>)
# path: tuple of strings
# params: tuple of objects

class fastrpc_channel (asynchat.async_chat):

    'Simple RPC server'

    # a 'packet': NNNNNNNNmmmmmmmmmmmmmmmm
    # (hex length in 8 bytes, followed by marshal'd packet data)
    # same protocol used in both directions.

    # A request consists of (<path-tuple>, <args-tuple>)
    # where <path-tuple> is a list of strings (eqv to string.split ('a.b.c', '.'))

    STATE_LENGTH = 'length state'
    STATE_PACKET = 'packet state'

    def __init__ (self, root, conn, addr):
        self.root = root
        self.addr = addr
        asynchat.async_chat.__init__ (self, conn)
        self.pstate = self.STATE_LENGTH
        self.set_terminator (8)
        self.buffer = []

    def log (*ignore):
        pass

    def collect_incoming_data (self, data):
        self.buffer.append (data)

    def found_terminator (self):
        self.buffer, data = [], string.join (self.buffer, '')

        if self.pstate is self.STATE_LENGTH:
            packet_length = string.atoi (data, 16)
            self.set_terminator (packet_length)
            self.pstate = self.STATE_PACKET
        else:
            self.set_terminator (8)
            self.pstate = self.STATE_LENGTH
            (path, params) = marshal.loads (data)
            o = self.root

            e = None

            try:
                for p in path:
                    o = getattr (o, p)
                result = apply (o, params)
            except:
                e = repr (asyncore.compact_traceback())
                result = None

            rb = marshal.dumps ((e,result))
            self.push (('%08x' % len(rb)) + rb)

class fastrpc_server (asyncore.dispatcher):

    def __init__ (self, root, address = ('', 8748)):
        self.create_socket (socket.AF_INET, socket.SOCK_STREAM)
        self.set_reuse_addr()
        self.bind (address)
        self.listen (128)
        self.root = root

    def handle_accept (self):
        conn, addr = self.accept()
        fastrpc_channel (self.root, conn, addr)

# ===========================================================================

if __name__ == '__main__':

    class thing:
        def __del__ (self):
            print 'a thing has gone away %08x' % id(self)

    class sample_calc:

        def product (self, *values):
            return reduce (lambda a,b: a*b, values, 1)

        def sum (self, *values):
            return reduce (lambda a,b: a+b, values, 0)

        def eval (self, string):
            return eval (string)

        def make_a_thing (self):
            return thing()

    if '-f' in sys.argv:
        server_class = fastrpc_server
        address = ('', 8748)
    else:
        server_class = rpc_server
        address = ('', 8746)

    root = rpc_server_root()
    root.calc = sample_calc()
    root.sys = sys
    rs = server_class (root, address)
    asyncore.loop()

########NEW FILE########
__FILENAME__ = script_handler
# -*- Mode: Python -*-

# This is a simple python server-side script handler.

# A note about performance: This is really only suited for 'fast'
# scripts: The script should generate its output quickly, since the
# whole web server will stall otherwise.  This doesn't mean you have
# to write 'fast code' or anything, it simply means that you shouldn't
# call any long-running code, [like say something that opens up an
# internet connection, or a database query that will hold up the
# server].  If you need this sort of feature, you can support it using
# the asynchronous I/O 'api' that the rest of medusa is built on.  [or
# you could probably use threads]

# Put your script into your web docs directory (like a cgi-bin
# script), make sure it has the correct extension [see the overridable
# script_handler.extension member below].
#
# There's lots of things that can be done to tweak the restricted
# execution model.  Also, of course you could just use 'execfile'
# instead (this is now the default, see class variable
# script_handler.restricted)

import rexec
import re
import string
import StringIO
import sys

import counter
import default_handler
import producers

unquote    = default_handler.unquote

class script_handler:

    extension = 'mpy'
    restricted = 0

    script_regex = re.compile (
            r'.*/([^/]+\.%s)' % extension,
            re.IGNORECASE
            )

    def __init__ (self, filesystem):
        self.filesystem = filesystem
        self.hits = counter.counter()
        self.exceptions = counter.counter()

    def match (self, request):
        [path, params, query, fragment] = request.split_uri()
        m = self.script_regex.match (path)
        return (m and (m.end() == len(path)))

    def handle_request (self, request):

        [path, params, query, fragment] = request.split_uri()

        while path and path[0] == '/':
            path = path[1:]

        if '%' in path:
            path = unquote (path)

        if not self.filesystem.isfile (path):
            request.error (404)
            return
        else:

            self.hits.increment()

            request.script_filename = self.filesystem.translate (path)

            if request.command in ('PUT', 'POST'):
                # look for a Content-Length header.
                cl = request.get_header ('content-length')
                length = int(cl)
                if not cl:
                    request.error (411)
                else:
                    collector (self, length, request)
            else:
                self.continue_request (
                        request,
                        StringIO.StringIO() # empty stdin
                        )

    def continue_request (self, request, stdin):
        temp_files = stdin, StringIO.StringIO(), StringIO.StringIO()
        old_files = sys.stdin, sys.stdout, sys.stderr

        if self.restricted:
            r = rexec.RExec()

        try:
            sys.request = request
            sys.stdin, sys.stdout, sys.stderr = temp_files
            try:
                if self.restricted:
                    r.s_execfile (request.script_filename)
                else:
                    execfile (request.script_filename)
                request.reply_code = 200
            except:
                request.reply_code = 500
                self.exceptions.increment()
        finally:
            sys.stdin, sys.stdout, sys.stderr = old_files
            del sys.request

        i,o,e = temp_files

        if request.reply_code != 200:
            s = e.getvalue()
        else:
            s = o.getvalue()

        request['Content-Length'] = len(s)
        request.push (s)
        request.done()

    def status (self):
        return producers.simple_producer (
                '<li>Server-Side Script Handler'
                + '<ul>'
                + '  <li><b>Hits:</b> %s' % self.hits
                + '  <li><b>Exceptions:</b> %s' % self.exceptions
                + '</ul>'
                )


class persistent_script_handler:

    def __init__ (self):
        self.modules = {}
        self.hits = counter.counter()
        self.exceptions = counter.counter()

    def add_module (self, name, module):
        self.modules[name] = module

    def del_module (self, name):
        del self.modules[name]

    def match (self, request):
        [path, params, query, fragment] = request.split_uri()
        parts = string.split (path, '/')
        if (len(parts)>1) and self.modules.has_key (parts[1]):
            module = self.modules[parts[1]]
            request.module = module
            return 1
        else:
            return 0

    def handle_request (self, request):
        if request.command in ('PUT', 'POST'):
            # look for a Content-Length header.
            cl = request.get_header ('content-length')
            length = int(cl)
            if not cl:
                request.error (411)
            else:
                collector (self, length, request)
        else:
            self.continue_request (request, StringIO.StringIO())

    def continue_request (self, request, input_data):
        temp_files = input_data, StringIO.StringIO(), StringIO.StringIO()
        old_files = sys.stdin, sys.stdout, sys.stderr

        try:
            sys.stdin, sys.stdout, sys.stderr = temp_files
            # provide a default
            request['Content-Type'] = 'text/html'
            try:
                request.module.main (request)
                request.reply_code = 200
            except:
                request.reply_code = 500
                self.exceptions.increment()
        finally:
            sys.stdin, sys.stdout, sys.stderr = old_files

        i,o,e = temp_files

        if request.reply_code != 200:
            s = e.getvalue()
        else:
            s = o.getvalue()

        request['Content-Length'] = len(s)
        request.push (s)
        request.done()

class collector:

    def __init__ (self, handler, length, request):
        self.handler = handler
        self.request = request
        self.request.collector = self
        self.request.channel.set_terminator (length)
        self.buffer = StringIO.StringIO()

    def collect_incoming_data (self, data):
        self.buffer.write (data)

    def found_terminator (self):
        self.buffer.seek(0)
        self.request.collector = None
        self.request.channel.set_terminator ('\r\n\r\n')
        self.handler.continue_request (
                self.request,
                self.buffer
                )

########NEW FILE########
__FILENAME__ = status_handler
# -*- Mode: Python -*-

VERSION_STRING = "$Id: status_handler.py,v 1.7 2003/12/24 16:08:16 akuchling Exp $"

#
# medusa status extension
#

import string
import time
import re
from cgi import escape

import asyncore_25 as asyncore
import http_server
import medusa_gif
import producers
from counter import counter

START_TIME = long(time.time())

class status_extension:
    hit_counter = counter()

    def __init__ (self, objects, statusdir='/status', allow_emergency_debug=0):
        self.objects = objects
        self.statusdir = statusdir
        self.allow_emergency_debug = allow_emergency_debug
        # We use /status instead of statusdir here because it's too
        # hard to pass statusdir to the logger, who makes the HREF
        # to the object dir.  We don't need the security-through-
        # obscurity here in any case, because the id is obscurity enough
        self.hyper_regex = re.compile('/status/object/([0-9]+)/.*')
        self.hyper_objects = []
        for object in objects:
            self.register_hyper_object (object)

    def __repr__ (self):
        return '<Status Extension (%s hits) at %x>' % (
                self.hit_counter,
                id(self)
                )

    def match (self, request):
        path, params, query, fragment = request.split_uri()
        # For reasons explained above, we don't use statusdir for /object
        return (path[:len(self.statusdir)] == self.statusdir or
                        path[:len("/status/object/")] == '/status/object/')

    # Possible Targets:
    # /status
    # /status/channel_list
    # /status/medusa.gif

    # can we have 'clickable' objects?
    # [yes, we can use id(x) and do a linear search]

    # Dynamic producers:
    # HTTP/1.0: we must close the channel, because it's dynamic output
    # HTTP/1.1: we can use the chunked transfer-encoding, and leave
    #   it open.

    def handle_request (self, request):
        [path, params, query, fragment] = request.split_uri()
        self.hit_counter.increment()
        if path == self.statusdir:          # and not a subdirectory
            up_time = string.join (english_time (long(time.time()) - START_TIME))
            request['Content-Type'] = 'text/html'
            request.push (
                    '<html>'
                    '<title>Medusa Status Reports</title>'
                    '<body bgcolor="#ffffff">'
                    '<h1>Medusa Status Reports</h1>'
                    '<b>Up:</b> %s' % up_time
                    )
            for i in range(len(self.objects)):
                try:
                    request.push (self.objects[i].status())
                except:
                    import traceback, StringIO
                    stream = StringIO.StringIO()
                    traceback.print_exc(None,stream)
                    request.push('<h2><font color="red">Error in Channel %3d: %s</font><pre>%s</pre>' % (i,escape(repr(self.objects[i])), escape(stream.getvalue())))
                request.push ('<hr>\r\n')
            request.push (
                    '<p><a href="%s/channel_list">Channel List</a>'
                    '<hr>'
                    '<img src="%s/medusa.gif" align=right width=%d height=%d>'
                    '</body></html>' % (
                            self.statusdir,
                            self.statusdir,
                            medusa_gif.width,
                            medusa_gif.height
                            )
                    )
            request.done()
        elif path == self.statusdir + '/channel_list':
            request['Content-Type'] = 'text/html'
            request.push ('<html><body>')
            request.push(channel_list_producer(self.statusdir))
            request.push (
                    '<hr>'
                    '<img src="%s/medusa.gif" align=right width=%d height=%d>' % (
                            self.statusdir,
                            medusa_gif.width,
                            medusa_gif.height
                            ) +
                    '</body></html>'
                    )
            request.done()

        elif path == self.statusdir + '/medusa.gif':
            request['Content-Type'] = 'image/gif'
            request['Content-Length'] = len(medusa_gif.data)
            request.push (medusa_gif.data)
            request.done()

        elif path == self.statusdir + '/close_zombies':
            message = (
                    '<h2>Closing all zombie http client connections...</h2>'
                    '<p><a href="%s">Back to the status page</a>' % self.statusdir
                    )
            request['Content-Type'] = 'text/html'
            request['Content-Length'] = len (message)
            request.push (message)
            now = int (time.time())
            for channel in asyncore.socket_map.keys():
                if channel.__class__ == http_server.http_channel:
                    if channel != request.channel:
                        if (now - channel.creation_time) > channel.zombie_timeout:
                            channel.close()
            request.done()

        # Emergency Debug Mode
        # If a server is running away from you, don't KILL it!
        # Move all the AF_INET server ports and perform an autopsy...
        # [disabled by default to protect the innocent]
        elif self.allow_emergency_debug and path == self.statusdir + '/emergency_debug':
            request.push ('<html>Moving All Servers...</html>')
            request.done()
            for channel in asyncore.socket_map.keys():
                if channel.accepting:
                    if type(channel.addr) is type(()):
                        ip, port = channel.addr
                        channel.socket.close()
                        channel.del_channel()
                        channel.addr = (ip, port+10000)
                        fam, typ = channel.family_and_type
                        channel.create_socket (fam, typ)
                        channel.set_reuse_addr()
                        channel.bind (channel.addr)
                        channel.listen(5)

        else:
            m = self.hyper_regex.match (path)
            if m:
                oid = string.atoi (m.group (1))
                for object in self.hyper_objects:
                    if id (object) == oid:
                        if hasattr (object, 'hyper_respond'):
                            object.hyper_respond (self, path, request)
            else:
                request.error (404)
                return

    def status (self):
        return producers.simple_producer (
                '<li>Status Extension <b>Hits</b> : %s' % self.hit_counter
                )

    def register_hyper_object (self, object):
        if not object in self.hyper_objects:
            self.hyper_objects.append (object)

import logger

class logger_for_status (logger.tail_logger):

    def status (self):
        return 'Last %d log entries for: %s' % (
                len (self.messages),
                html_repr (self)
                )

    def hyper_respond (self, sh, path, request):
        request['Content-Type'] = 'text/plain'
        messages = self.messages[:]
        messages.reverse()
        request.push (lines_producer (messages))
        request.done()

class lines_producer:
    def __init__ (self, lines):
        self.lines = lines

    def more (self):
        if self.lines:
            chunk = self.lines[:50]
            self.lines = self.lines[50:]
            return string.join (chunk, '\r\n') + '\r\n'
        else:
            return ''

class channel_list_producer (lines_producer):
    def __init__ (self, statusdir):
        channel_reprs = map (
                lambda x: '&lt;' + repr(x)[1:-1] + '&gt;',
                asyncore.socket_map.values()
                )
        channel_reprs.sort()
        lines_producer.__init__ (
                self,
                ['<h1>Active Channel List</h1>',
                 '<pre>'
                 ] + channel_reprs + [
                         '</pre>',
                         '<p><a href="%s">Status Report</a>' % statusdir
                         ]
                )


def html_repr (object):
    so = escape (repr (object))
    if hasattr (object, 'hyper_respond'):
        return '<a href="/status/object/%d/">%s</a>' % (id (object), so)
    else:
        return so

def html_reprs (list, front='', back=''):
    reprs = map (
            lambda x,f=front,b=back: '%s%s%s' % (f,x,b),
            map (lambda x: escape (html_repr(x)), list)
            )
    reprs.sort()
    return reprs

# for example, tera, giga, mega, kilo
# p_d (n, (1024, 1024, 1024, 1024))
# smallest divider goes first - for example
# minutes, hours, days
# p_d (n, (60, 60, 24))

def progressive_divide (n, parts):
    result = []
    for part in parts:
        n, rem = divmod (n, part)
        result.append (rem)
    result.append (n)
    return result

# b,k,m,g,t
def split_by_units (n, units, dividers, format_string):
    divs = progressive_divide (n, dividers)
    result = []
    for i in range(len(units)):
        if divs[i]:
            result.append (format_string % (divs[i], units[i]))
    result.reverse()
    if not result:
        return [format_string % (0, units[0])]
    else:
        return result

def english_bytes (n):
    return split_by_units (
            n,
            ('','K','M','G','T'),
            (1024, 1024, 1024, 1024, 1024),
            '%d %sB'
            )

def english_time (n):
    return split_by_units (
            n,
            ('secs', 'mins', 'hours', 'days', 'weeks', 'years'),
            (         60,     60,      24,     7,       52),
            '%d %s'
            )

########NEW FILE########
__FILENAME__ = asyn_http_bench
# -*- Mode: Python -*-

import asyncore
import socket
import string
import sys

def blurt (thing):
    sys.stdout.write (thing)
    sys.stdout.flush ()

total_sessions = 0

class http_client (asyncore.dispatcher_with_send):
    def __init__ (self, host='127.0.0.1', port=80, uri='/', num=10):
        asyncore.dispatcher_with_send.__init__ (self)
        self.create_socket (socket.AF_INET, socket.SOCK_STREAM)
        self.host = host
        self.port = port
        self.uri = uri
        self.num = num
        self.bytes = 0
        self.connect ((host, port))

    def log (self, *info):
        pass

    def handle_connect (self):
        self.connected = 1
#               blurt ('o')
        self.send ('GET %s HTTP/1.0\r\n\r\n' % self.uri)

    def handle_read (self):
#               blurt ('.')
        d = self.recv (8192)
        self.bytes = self.bytes + len(d)

    def handle_close (self):
        global total_sessions
#               blurt ('(%d)' % (self.bytes))
        self.close()
        total_sessions = total_sessions + 1
        if self.num:
            http_client (self.host, self.port, self.uri, self.num-1)

import time
class timer:
    def __init__ (self):
        self.start = time.time()
    def end (self):
        return time.time() - self.start

from asyncore import socket_map, poll

MAX = 0

def loop (timeout=30.0):
    global MAX
    while socket_map:
        if len(socket_map) > MAX:
            MAX = len(socket_map)
        poll (timeout)

if __name__ == '__main__':
    if len(sys.argv) < 6:
        print 'usage: %s <host> <port> <uri> <hits> <num_clients>' % sys.argv[0]
    else:
        [host, port, uri, hits, num] = sys.argv[1:]
        hits = string.atoi (hits)
        num = string.atoi (num)
        port = string.atoi (port)
        t = timer()
        clients = map (lambda x: http_client (host, port, uri, hits-1), range(num))
        #import profile
        #profile.run ('loop')
        loop()
        total_time = t.end()
        print (
                '\n%d clients\n%d hits/client\n'
                'total_hits:%d\n%.3f seconds\ntotal hits/sec:%.3f' % (
                        num,
                        hits,
                        total_sessions,
                        total_time,
                        total_sessions / total_time
                        )
                )
        print 'Max. number of concurrent sessions: %d' % (MAX)


# linux 2.x, talking to medusa
# 50 clients
# 1000 hits/client
# total_hits:50000
# 2255.858 seconds
# total hits/sec:22.165
# Max. number of concurrent sessions: 50

########NEW FILE########
__FILENAME__ = bench
# -*- Mode: Python -*-

# benchmark a single channel, pipelined

request = 'GET /index.html HTTP/1.0\r\nConnection: Keep-Alive\r\n\r\n'
last_request = 'GET /index.html HTTP/1.0\r\nConnection: close\r\n\r\n'

import socket
import time

class timer:
    def __init__ (self):
        self.start = time.time()
    def end (self):
        return time.time() - self.start

def bench (host, port=80, n=100):
    s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
    s.connect ((host, port))
    t = timer()
    s.send ((request * n) + last_request)
    while 1:
        d = s.recv(65536)
        if not d:
            break
    total = t.end()
    print 'time: %.2f seconds  (%.2f hits/sec)' % (total, n/total)

if __name__ == '__main__':
    import sys
    import string
    if len(sys.argv) < 3:
        print 'usage: %s <host> <port> <count>' % (sys.argv[0])
    else:
        bench (sys.argv[1], string.atoi (sys.argv[2]), string.atoi (sys.argv[3]))

########NEW FILE########
__FILENAME__ = max_sockets
# -*- Mode: Python -*-

import socket
import select

# several factors here we might want to test:
# 1) max we can create
# 2) max we can bind
# 3) max we can listen on
# 4) max we can connect

def max_server_sockets():
    sl = []
    while 1:
        try:
            s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
            s.bind (('',0))
            s.listen(5)
            sl.append (s)
        except:
            break
    num = len(sl)
    for s in sl:
        s.close()
    del sl
    return num

def max_client_sockets():
    # make a server socket
    server = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
    server.bind (('', 9999))
    server.listen (5)
    sl = []
    while 1:
        try:
            s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
            s.connect (('', 9999))
            conn, addr = server.accept()
            sl.append ((s,conn))
        except:
            break
    num = len(sl)
    for s,c in sl:
        s.close()
        c.close()
    del sl
    return num

def max_select_sockets():
    sl = []
    while 1:
        try:
            s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
            s.bind (('',0))
            s.listen(5)
            sl.append (s)
            select.select(sl,[],[],0)
        except:
            break
    num = len(sl) - 1
    for s in sl:
        s.close()
    del sl
    return num

########NEW FILE########
__FILENAME__ = test_11
# -*- Mode: Python -*-

import socket
import string
from supervisor.medusa import asyncore_25 as asyncore
from supervisor.medusa import asynchat_25 as asynchat

# get some performance figures for an HTTP/1.1 server.
# use pipelining.

class test_client (asynchat.async_chat):

    ac_in_buffer_size = 16384
    ac_out_buffer_size = 16384

    total_in = 0

    concurrent = 0
    max_concurrent = 0

    def __init__ (self, addr, chain):
        asynchat.async_chat.__init__ (self)
        self.create_socket (socket.AF_INET, socket.SOCK_STREAM)
        self.set_terminator ('\r\n\r\n')
        self.connect (addr)
        self.push (chain)

    def handle_connect (self):
        test_client.concurrent = test_client.concurrent + 1
        if (test_client.concurrent > test_client.max_concurrent):
            test_client.max_concurrent = test_client.concurrent

    def handle_expt (self):
        print 'unexpected FD_EXPT thrown.  closing()'
        self.close()

    def close (self):
        test_client.concurrent = test_client.concurrent - 1
        asynchat.async_chat.close(self)

    def collect_incoming_data (self, data):
        test_client.total_in = test_client.total_in + len(data)

    def found_terminator (self):
        pass

    def log (self, *args):
        pass


import time

class timer:
    def __init__ (self):
        self.start = time.time()

    def end (self):
        return time.time() - self.start

def build_request_chain (num, host, request_size):
    s = 'GET /test%d.html HTTP/1.1\r\nHost: %s\r\n\r\n' % (request_size, host)
    sl = [s] * (num-1)
    sl.append (
            'GET /test%d.html HTTP/1.1\r\nHost: %s\r\nConnection: close\r\n\r\n' % (
                    request_size, host
                    )
            )
    return string.join (sl, '')

if __name__ == '__main__':
    import string
    import sys
    if len(sys.argv) != 6:
        print 'usage: %s <host> <port> <request-size> <num-requests> <num-connections>\n' % sys.argv[0]
    else:
        host = sys.argv[1]

        ip = socket.gethostbyname (host)

        [port, request_size, num_requests, num_conns] = map (
                string.atoi, sys.argv[2:]
                )

        chain = build_request_chain (num_requests, host, request_size)

        t = timer()
        for i in range (num_conns):
            test_client ((host,port), chain)
        asyncore.loop()
        total_time = t.end()

        # ok, now do some numbers
        total_bytes = test_client.total_in
        num_trans = num_requests * num_conns
        throughput = float (total_bytes) / total_time
        trans_per_sec = num_trans / total_time

        sys.stderr.write ('total time: %.2f\n' % total_time)
        sys.stderr.write ('number of transactions: %d\n' % num_trans)
        sys.stderr.write ('total bytes sent: %d\n' % total_bytes)
        sys.stderr.write ('total throughput (bytes/sec): %.2f\n' % throughput)
        sys.stderr.write ('transactions/second: %.2f\n' % trans_per_sec)
        sys.stderr.write ('max concurrent connections: %d\n' % test_client.max_concurrent)

        sys.stdout.write (
                string.join (
                        map (str, (num_conns, num_requests, request_size, throughput, trans_per_sec)),
                        ','
                        ) + '\n'
                )

########NEW FILE########
__FILENAME__ = test_lb
# -*- Mode: Python -*-

# Get a lower bound for Medusa performance with a simple async
# client/server benchmark built on the async lib.  The idea is to test
# all the underlying machinery [select, asyncore, asynchat, etc...] in
# a context where there is virtually no processing of the data.

import socket
import select
import sys

# ==================================================
# server
# ==================================================

from supervisor.medusa import asyncore_25 as asyncore
from supervisor.medusa import asynchat_25 as asynchat

class test_channel (asynchat.async_chat):

    ac_in_buffer_size = 16384
    ac_out_buffer_size = 16384

    total_in = 0

    def __init__ (self, conn, addr):
        asynchat.async_chat.__init__ (self, conn)
        self.set_terminator ('\r\n\r\n')
        self.buffer = ''

    def collect_incoming_data (self, data):
        self.buffer = self.buffer + data
        test_channel.total_in = test_channel.total_in + len(data)

    def found_terminator (self):
        # we've gotten the data, now send it back
        data = self.buffer
        self.buffer = ''
        self.push (data+'\r\n\r\n')

    def handle_close (self):
        sys.stdout.write ('.'); sys.stdout.flush()
        self.close()

    def log (self, *args):
        pass

class test_server (asyncore.dispatcher):
    def __init__ (self, addr):

        if type(addr) == type(''):
            f = socket.AF_UNIX
        else:
            f = socket.AF_INET

        self.create_socket (f, socket.SOCK_STREAM)
        self.bind (addr)
        self.listen (5)
        print 'server started on',addr

    def handle_accept (self):
        conn, addr = self.accept()
        test_channel (conn, addr)

# ==================================================
# client
# ==================================================

# pretty much the same behavior, except that we kick
# off the exchange and decide when to quit

class test_client (test_channel):

    def __init__ (self, addr, packet, number):
        if type(addr) == type(''):
            f = socket.AF_UNIX
        else:
            f = socket.AF_INET

        asynchat.async_chat.__init__ (self)
        self.create_socket (f, socket.SOCK_STREAM)
        self.set_terminator ('\r\n\r\n')
        self.buffer = ''
        self.connect (addr)
        self.push (packet + '\r\n\r\n')
        self.number = number
        self.count = 0

    def handle_connect (self):
        pass

    def found_terminator (self):
        self.count = self.count + 1
        if self.count == self.number:
            sys.stdout.write('.'); sys.stdout.flush()
            self.close()
        else:
            test_channel.found_terminator (self)

import time

class timer:
    def __init__ (self):
        self.start = time.time()

    def end (self):
        return time.time() - self.start

if __name__ == '__main__':
    import string

    if '--poll' in sys.argv:
        sys.argv.remove ('--poll')
        use_poll=1
    else:
        use_poll=0

    if len(sys.argv) == 1:
        print 'usage: %s\n' \
        '  (as a server) [--poll] -s <ip> <port>\n' \
        '  (as a client) [--poll] -c <ip> <port> <packet-size> <num-packets> <num-connections>\n' % sys.argv[0]
        sys.exit(0)
    if sys.argv[1] == '-s':
        s = test_server ((sys.argv[2], string.atoi (sys.argv[3])))
        asyncore.loop(use_poll=use_poll)
    elif sys.argv[1] == '-c':
        # create the packet
        packet = string.atoi(sys.argv[4]) * 'B'
        host = sys.argv[2]
        port = string.atoi (sys.argv[3])
        num_packets = string.atoi (sys.argv[5])
        num_conns = string.atoi (sys.argv[6])

        t = timer()
        for i in range (num_conns):
            test_client ((host,port), packet, num_packets)
        asyncore.loop(use_poll=use_poll)
        total_time = t.end()

        # ok, now do some numbers
        bytes = test_client.total_in
        num_trans = num_packets * num_conns
        total_bytes = num_trans * len(packet)
        throughput = float (total_bytes) / total_time
        trans_per_sec = num_trans / total_time

        sys.stderr.write ('total time: %.2f\n' % total_time)
        sys.stderr.write ( 'number of transactions: %d\n' % num_trans)
        sys.stderr.write ( 'total bytes sent: %d\n' % total_bytes)
        sys.stderr.write ( 'total throughput (bytes/sec): %.2f\n' % throughput)
        sys.stderr.write ( ' [note, throughput is this amount in each direction]\n')
        sys.stderr.write ( 'transactions/second: %.2f\n' % trans_per_sec)

        sys.stdout.write (
                string.join (
                        map (str, (num_conns, num_packets, len(packet), throughput, trans_per_sec)),
                        ','
                        ) + '\n'
                )

########NEW FILE########
__FILENAME__ = test_medusa
# -*- Mode: Python -*-

import socket
import string
import time
from supervisor.medusa import http_date

now = http_date.build_http_date (time.time())

cache_request = string.joinfields (
        ['GET / HTTP/1.0',
         'If-Modified-Since: %s' % now,
         ],
        '\r\n'
        ) + '\r\n\r\n'

nocache_request = 'GET / HTTP/1.0\r\n\r\n'

def get (request, host='', port=80):
    s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
    s.connect((host, port))
    s.send (request)
    while 1:
        d = s.recv (8192)
        if not d:
            break
    s.close()

class timer:
    def __init__ (self):
        self.start = time.time()
    def end (self):
        return time.time() - self.start

def test_cache (n=1000):
    t = timer()
    for i in xrange (n):
        get(cache_request)
    end = t.end()
    print 'cache: %d requests, %.2f seconds, %.2f hits/sec' % (n, end, n/end)

def test_nocache (n=1000):
    t = timer()
    for i in xrange (n):
        get(nocache_request)
    end = t.end()
    print 'nocache: %d requests, %.2f seconds, %.2f hits/sec' % (n, end, n/end)

if __name__ == '__main__':
    test_cache()
    test_nocache()

########NEW FILE########
__FILENAME__ = test_producers
#
# Test script for producers.py
#

__revision__ = "$Id: test_producers.py,v 1.2 2002/09/18 20:16:40 akuchling Exp $"

import StringIO, zlib
from sancho.unittest import TestScenario, parse_args, run_scenarios

tested_modules = ["supervisor.medusa.producers"]


from supervisor.medusa import producers

test_string = ''
for i in range(16385):
    test_string += chr(48 + (i%10))

class ProducerTest (TestScenario):

    def setup (self):
        pass
    
    def shutdown (self):
        pass

    def _check_all (self, p, expected_string):
        # Check that a producer returns all of the string,
        # and that it's the unchanged string.
        count = 0
        data = ""
        while 1:
            s = p.more()
            if s == "":
                break
            count += len(s)
            data += s
        self.test_val('count', len(expected_string))
        self.test_val('data', expected_string)
        self.test_val('p.more()', '')
        return data
        
    def check_simple (self):
        p = producers.simple_producer(test_string)
        self.test_val('p.more()', test_string[:1024])

        p = producers.simple_producer(test_string, buffer_size = 5)
        self._check_all(p, test_string)

    def check_scanning (self):
        p = producers.scanning_producer(test_string)
        self.test_val('p.more()', test_string[:1024])

        p = producers.scanning_producer(test_string, buffer_size = 5)
        self._check_all(p, test_string)

    def check_lines (self):
        p = producers.lines_producer(['a']* 65)
        self._check_all(p, 'a\r\n'*65)

    def check_buffer (self):
        p = producers.buffer_list_producer(['a']* 1027)
        self._check_all(p, 'a'*1027)

    def check_file (self):
        f = StringIO.StringIO(test_string)
        p = producers.file_producer(f)
        self._check_all(p, test_string)

    def check_output (self):
        p = producers.output_producer()
        for i in range(0,66):
            p.write('a')
        for i in range(0,65):
            p.write('b\n')
        self._check_all(p, 'a'*66 + 'b\r\n'*65)

    def check_composite (self):
        p1 = producers.simple_producer('a'*66, buffer_size = 5)
        p2 = producers.lines_producer(['b']*65)
        p = producers.composite_producer([p1, p2])
        self._check_all(p, 'a'*66 + 'b\r\n'*65)

    def check_glob (self):
        p1 = producers.simple_producer(test_string, buffer_size = 5)
        p = producers.globbing_producer(p1, buffer_size = 1024)
        self.test_true('1024 <= len(p.more())')

    def check_hooked (self):
        def f (num_bytes):
            self.test_val('num_bytes', len(test_string))
        p1 = producers.simple_producer(test_string, buffer_size = 5)
        p = producers.hooked_producer(p1, f)
        self._check_all(p, test_string)

    def check_chunked (self):
        p1 = producers.simple_producer('the quick brown fox', buffer_size = 5)
        p = producers.chunked_producer(p1, footers=['FOOTER'])
        self._check_all(p, """5\r
the q\r
5\r
uick \r
5\r
brown\r
4\r
 fox\r
0\r
FOOTER\r
\r\n""")

    def check_compressed (self):
        p1 = producers.simple_producer(test_string, buffer_size = 5)
        p = producers.compressed_producer(p1)
        compr_data = self._check_all(p, zlib.compress(test_string, 5))
        self.test_val('zlib.decompress(compr_data)', test_string)

    def check_escaping (self):
        p1 = producers.simple_producer('the quick brown fox', buffer_size = 5)
        p = producers.escaping_producer(p1,
                                        esc_from = ' ',
                                        esc_to = '_')
        self._check_all(p, 'the_quick_brown_fox')
        
# class ProducerTest


if __name__ == "__main__":
    (scenarios, options) = parse_args()
    run_scenarios(scenarios, options)

########NEW FILE########
__FILENAME__ = test_single_11
# -*- Mode: Python -*-

# no-holds barred, test a single channel's pipelining speed

import string
import socket

def build_request_chain (num, host, request_size):
    s = 'GET /test%d.html HTTP/1.1\r\nHost: %s\r\n\r\n' % (request_size, host)
    sl = [s] * (num-1)
    sl.append (
            'GET /test%d.html HTTP/1.1\r\nHost: %s\r\nConnection: close\r\n\r\n' % (
                    request_size, host
                    )
            )
    return string.join (sl, '')

import time

class timer:
    def __init__ (self):
        self.start = time.time()

    def end (self):
        return time.time() - self.start

if __name__ == '__main__':
    import sys
    if len(sys.argv) != 5:
        print 'usage: %s <host> <port> <request-size> <num-requests>' % (sys.argv[0])
    else:
        host = sys.argv[1]
        [port, request_size, num_requests] = map (
                string.atoi,
                sys.argv[2:]
                )
        chain = build_request_chain (num_requests, host, request_size)
        import socket
        s = socket.socket (socket.AF_INET, socket.SOCK_STREAM)
        s.connect ((host,port))
        t = timer()
        s.send (chain)
        num_bytes = 0
        while 1:
            data = s.recv(16384)
            if not data:
                break
            else:
                num_bytes = num_bytes + len(data)
        total_time = t.end()
        print 'total bytes received: %d' % num_bytes
        print 'total time: %.2f sec' % (total_time)
        print 'transactions/sec: %.2f' % (num_requests/total_time)

########NEW FILE########
__FILENAME__ = pi_module
# -*- Mode: Python -*-

# [reworking of the version in Python-1.5.1/Demo/scripts/pi.py]

# Print digits of pi forever.
#
# The algorithm, using Python's 'long' integers ("bignums"), works
# with continued fractions, and was conceived by Lambert Meertens.
#
# See also the ABC Programmer's Handbook, by Geurts, Meertens & Pemberton,
# published by Prentice-Hall (UK) Ltd., 1990.

import string

StopException = "Stop!"

def go (file):
    try:
        k, a, b, a1, b1 = 2L, 4L, 1L, 12L, 4L
        while 1:
            # Next approximation
            p, q, k = k*k, 2L*k+1L, k+1L
            a, b, a1, b1 = a1, b1, p*a+q*a1, p*b+q*b1
            # Print common digits
            d, d1 = a/b, a1/b1
            while d == d1:
                if file.write (str(int(d))):
                    raise StopException
                a, a1 = 10L*(a%b), 10L*(a1%b1)
                d, d1 = a/b, a1/b1
    except StopException:
        return

class line_writer:

    "partition the endless line into 80-character ones"

    def __init__ (self, file, digit_limit=10000):
        self.file = file
        self.buffer = ''
        self.count = 0
        self.digit_limit = digit_limit

    def write (self, data):
        self.buffer = self.buffer + data
        if len(self.buffer) > 80:
            line, self.buffer = self.buffer[:80], self.buffer[80:]
            self.file.write (line+'\r\n')
            self.count = self.count + 80
        if self.count > self.digit_limit:
            return 1
        else:
            return 0

def main (env, stdin, stdout):
    parts = string.split (env['REQUEST_URI'], '/')
    if len(parts) >= 3:
        ndigits = string.atoi (parts[2])
    else:
        ndigits = 5000
    stdout.write ('Content-Type: text/plain\r\n\r\n')
    go (line_writer (stdout, ndigits))

########NEW FILE########
__FILENAME__ = select_trigger
# -*- Mode: Python -*-

##############################################################################
#
# Copyright (c) 2001, 2002 Zope Corporation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.0 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE
#
##############################################################################

__revision__ = "$Id: select_trigger.py,v 1.4 2003/01/09 15:49:15 akuchling Exp $"

import asyncore_25 as asyncore
import asynchat_25 as asynchat

import os
import socket
import string
import thread

if os.name == 'posix':

    class trigger (asyncore.file_dispatcher):

        "Wake up a call to select() running in the main thread"

        # This is useful in a context where you are using Medusa's I/O
        # subsystem to deliver data, but the data is generated by another
        # thread.  Normally, if Medusa is in the middle of a call to
        # select(), new output data generated by another thread will have
        # to sit until the call to select() either times out or returns.
        # If the trigger is 'pulled' by another thread, it should immediately
        # generate a READ event on the trigger object, which will force the
        # select() invocation to return.

        # A common use for this facility: letting Medusa manage I/O for a
        # large number of connections; but routing each request through a
        # thread chosen from a fixed-size thread pool.  When a thread is
        # acquired, a transaction is performed, but output data is
        # accumulated into buffers that will be emptied more efficiently
        # by Medusa. [picture a server that can process database queries
        # rapidly, but doesn't want to tie up threads waiting to send data
        # to low-bandwidth connections]

        # The other major feature provided by this class is the ability to
        # move work back into the main thread: if you call pull_trigger()
        # with a thunk argument, when select() wakes up and receives the
        # event it will call your thunk from within that thread.  The main
        # purpose of this is to remove the need to wrap thread locks around
        # Medusa's data structures, which normally do not need them.  [To see
        # why this is true, imagine this scenario: A thread tries to push some
        # new data onto a channel's outgoing data queue at the same time that
        # the main thread is trying to remove some]

        def __init__ (self):
            r, w = self._fds = os.pipe()
            self.trigger = w
            asyncore.file_dispatcher.__init__(self, r)
            self.lock = thread.allocate_lock()
            self.thunks = []
            self._closed = 0

        # Override the asyncore close() method, because it seems that
        # it would only close the r file descriptor and not w.  The
        # constructor calls file_dispatcher.__init__ and passes r,
        # which would get stored in a file_wrapper and get closed by
        # the default close.  But that would leave w open...

        def close(self):
            if not self._closed:
                self._closed = 1
                self.del_channel()
                for fd in self._fds:
                    os.close(fd)
                self._fds = []
 
        def __repr__ (self):
            return '<select-trigger (pipe) at %x>' % id(self)

        def readable (self):
            return 1

        def writable (self):
            return 0

        def handle_connect (self):
            pass

        def handle_close(self):
            self.close()

        def pull_trigger (self, thunk=None):
            # print 'PULL_TRIGGER: ', len(self.thunks)
            if thunk:
                self.lock.acquire()
                try:
                    self.thunks.append(thunk)
                finally:
                    self.lock.release()
            os.write(self.trigger, 'x')

        def handle_read (self):
            try:
                self.recv(8192)
            except socket.error:
                return
            self.lock.acquire()
            try:
                for thunk in self.thunks:
                    try:
                        thunk()
                    except:
                        nil, t, v, tbinfo = asyncore.compact_traceback()
                        print ('exception in trigger thunk:'
                               ' (%s:%s %s)' % (t, v, tbinfo))
                self.thunks = []
            finally:
                self.lock.release()

else:

    # win32-safe version

    # XXX Should define a base class that has the common methods and
    # then put the platform-specific in a subclass named trigger.

    HOST = '127.0.0.1'
    MINPORT = 19950
    NPORTS = 50

    class trigger (asyncore.dispatcher):
        portoffset = 0

        def __init__ (self):
            a = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            w = socket.socket(socket.AF_INET, socket.SOCK_STREAM)

            # set TCP_NODELAY to true to avoid buffering
            w.setsockopt(socket.IPPROTO_TCP, 1, 1)

            # tricky: get a pair of connected sockets
            for i in range(NPORTS):
                trigger.portoffset = (trigger.portoffset + 1) % NPORTS
                port = MINPORT + trigger.portoffset
                address = (HOST, port)
                try:
                    a.bind(address)
                except socket.error:
                    continue
                else:
                    break
            else:
                raise RuntimeError, 'Cannot bind trigger!'

            a.listen(1)
            w.setblocking(0)
            try:
                w.connect(address)
            except:
                pass
            r, addr = a.accept()
            a.close()
            w.setblocking(1)
            self.trigger = w

            asyncore.dispatcher.__init__(self, r)
            self.lock = thread.allocate_lock()
            self.thunks = []
            self._trigger_connected = 0

        def __repr__ (self):
            return '<select-trigger (loopback) at %x>' % id(self)

        def readable (self):
            return 1

        def writable (self):
            return 0

        def handle_connect (self):
            pass

        def pull_trigger (self, thunk=None):
            if thunk:
                self.lock.acquire()
                try:
                    self.thunks.append(thunk)
                finally:
                    self.lock.release()
            self.trigger.send('x')

        def handle_read (self):
            try:
                self.recv(8192)
            except socket.error:
                return
            self.lock.acquire()
            try:
                for thunk in self.thunks:
                    try:
                        thunk()
                    except:
                        nil, t, v, tbinfo = asyncore.compact_traceback()
                        print ('exception in trigger thunk:'
                               ' (%s:%s %s)' % (t, v, tbinfo))
                self.thunks = []
            finally:
                self.lock.release()


the_trigger = None

class trigger_file:

    "A 'triggered' file object"

    buffer_size = 4096

    def __init__ (self, parent):
        global the_trigger
        if the_trigger is None:
            the_trigger = trigger()
        self.parent = parent
        self.buffer = ''

    def write (self, data):
        self.buffer = self.buffer + data
        if len(self.buffer) > self.buffer_size:
            d, self.buffer = self.buffer, ''
            the_trigger.pull_trigger (
                    lambda d=d,p=self.parent: p.push (d)
                    )

    def writeline (self, line):
        self.write (line+'\r\n')

    def writelines (self, lines):
        self.write (
                string.joinfields (
                        lines,
                        '\r\n'
                        ) + '\r\n'
                )

    def flush (self):
        if self.buffer:
            d, self.buffer = self.buffer, ''
            the_trigger.pull_trigger (
                    lambda p=self.parent,d=d: p.push (d)
                    )

    def softspace (self, *args):
        pass

    def close (self):
        # in a derived class, you may want to call trigger_close() instead.
        self.flush()
        self.parent = None

    def trigger_close (self):
        d, self.buffer = self.buffer, ''
        p, self.parent = self.parent, None
        the_trigger.pull_trigger (
                lambda p=p,d=d: (p.push(d), p.close_when_done())
                )

if __name__ == '__main__':

    import time

    def thread_function (output_file, i, n):
        print 'entering thread_function'
        while n:
            time.sleep (5)
            output_file.write ('%2d.%2d %s\r\n' % (i, n, output_file))
            output_file.flush()
            n = n - 1
        output_file.close()
        print 'exiting thread_function'

    class thread_parent (asynchat.async_chat):

        def __init__ (self, conn, addr):
            self.addr = addr
            asynchat.async_chat.__init__ (self, conn)
            self.set_terminator ('\r\n')
            self.buffer = ''
            self.count = 0

        def collect_incoming_data (self, data):
            self.buffer = self.buffer + data

        def found_terminator (self):
            data, self.buffer = self.buffer, ''
            if not data:
                asyncore.close_all()
                print "done"
                return
            n = string.atoi (string.split (data)[0])
            tf = trigger_file (self)
            self.count = self.count + 1
            thread.start_new_thread (thread_function, (tf, self.count, n))

    class thread_server (asyncore.dispatcher):

        def __init__ (self, family=socket.AF_INET, address=('', 9003)):
            asyncore.dispatcher.__init__ (self)
            self.create_socket (family, socket.SOCK_STREAM)
            self.set_reuse_addr()
            self.bind (address)
            self.listen (5)

        def handle_accept (self):
            conn, addr = self.accept()
            tp = thread_parent (conn, addr)

    thread_server()
    #asyncore.loop(1.0, use_poll=1)
    try:
        asyncore.loop ()
    except:
        asyncore.close_all()

########NEW FILE########
__FILENAME__ = test_module
# -*- Mode: Python -*-

import pprint

def main (env, stdin, stdout):

    stdout.write (
            '<html><body><h1>Test CGI Module</h1>\r\n'
            '<br>The Environment:<pre>\r\n'
            )
    pprint.pprint (env, stdout)
    stdout.write ('</pre></body></html>\r\n')

########NEW FILE########
__FILENAME__ = thread_channel
# -*- Mode: Python -*-

VERSION_STRING = "$Id: thread_channel.py,v 1.3 2002/03/19 22:49:40 amk Exp $"

# This will probably only work on Unix.

# The disadvantage to this technique is that it wastes file
# descriptors (especially when compared to select_trigger.py)

# May be possible to do it on Win32, using TCP localhost sockets.
# [does winsock support 'socketpair'?]

import asyncore_25 as asyncore
import asynchat_25 as asynchat

import fcntl
import FCNTL
import os
import socket
import string
import thread

# this channel slaves off of another one.  it starts a thread which
# pumps its output through the 'write' side of the pipe.  The 'read'
# side of the pipe will then notify us when data is ready.  We push
# this data on the owning data channel's output queue.

class thread_channel (asyncore.file_dispatcher):

    buffer_size = 8192

    def __init__ (self, channel, function, *args):
        self.parent = channel
        self.function = function
        self.args = args
        self.pipe = rfd, wfd = os.pipe()
        asyncore.file_dispatcher.__init__ (self, rfd)

    def start (self):
        rfd, wfd = self.pipe

        # The read side of the pipe is set to non-blocking I/O; it is
        # 'owned' by medusa.

        flags = fcntl.fcntl (rfd, FCNTL.F_GETFL, 0)
        fcntl.fcntl (rfd, FCNTL.F_SETFL, flags | FCNTL.O_NDELAY)

        # The write side of the pipe is left in blocking mode; it is
        # 'owned' by the thread.  However, we wrap it up as a file object.
        # [who wants to 'write()' to a number?]

        of = os.fdopen (wfd, 'w')

        thread.start_new_thread (
                self.function,
                # put the output file in front of the other arguments
                (of,) + self.args
                )

    def writable (self):
        return 0

    def readable (self):
        return 1

    def handle_read (self):
        data = self.recv (self.buffer_size)
        self.parent.push (data)

    def handle_close (self):
        # Depending on your intentions, you may want to close
        # the parent channel here.
        self.close()

# Yeah, it's bad when the test code is bigger than the library code.

if __name__ == '__main__':

    import time

    def thread_function (output_file, i, n):
        print 'entering thread_function'
        while n:
            time.sleep (5)
            output_file.write ('%2d.%2d %s\r\n' % (i, n, output_file))
            output_file.flush()
            n = n - 1
        output_file.close()
        print 'exiting thread_function'

    class thread_parent (asynchat.async_chat):

        def __init__ (self, conn, addr):
            self.addr = addr
            asynchat.async_chat.__init__ (self, conn)
            self.set_terminator ('\r\n')
            self.buffer = ''
            self.count = 0

        def collect_incoming_data (self, data):
            self.buffer = self.buffer + data

        def found_terminator (self):
            data, self.buffer = self.buffer, ''
            n = string.atoi (string.split (data)[0])
            tc = thread_channel (self, thread_function, self.count, n)
            self.count = self.count + 1
            tc.start()

    class thread_server (asyncore.dispatcher):

        def __init__ (self, family=socket.AF_INET, address=('127.0.0.1', 9003)):
            asyncore.dispatcher.__init__ (self)
            self.create_socket (family, socket.SOCK_STREAM)
            self.set_reuse_addr()
            self.bind (address)
            self.listen (5)

        def handle_accept (self):
            conn, addr = self.accept()
            tp = thread_parent (conn, addr)

    thread_server()
    #asyncore.loop(1.0, use_poll=1)
    asyncore.loop ()

########NEW FILE########
__FILENAME__ = thread_handler
# -*- Mode: Python -*-

import re
import string
import StringIO
import sys

import os
import sys
import time

import select_trigger
from supervisor.medusa import counter
from supervisor.medusa import producers

from supervisor.medusa.default_handler import unquote, get_header

import threading

class request_queue:

    def __init__ (self):
        self.mon = threading.RLock()
        self.cv = threading.Condition (self.mon)
        self.queue = []

    def put (self, item):
        self.cv.acquire()
        self.queue.append(item)
        self.cv.notify()
        self.cv.release()

    def get(self):
        self.cv.acquire()
        while not self.queue:
            self.cv.wait()
        result = self.queue.pop(0)
        self.cv.release()
        return result

header2env= {
        'Content-Length'        : 'CONTENT_LENGTH',
        'Content-Type'          : 'CONTENT_TYPE',
        'Referer'                       : 'HTTP_REFERER',
        'User-Agent'            : 'HTTP_USER_AGENT',
        'Accept'                        : 'HTTP_ACCEPT',
        'Accept-Charset'        : 'HTTP_ACCEPT_CHARSET',
        'Accept-Language'       : 'HTTP_ACCEPT_LANGUAGE',
        'Host'                          : 'HTTP_HOST',
        'Connection'            : 'CONNECTION_TYPE',
        'Authorization'         : 'HTTP_AUTHORIZATION',
        'Cookie'                        : 'HTTP_COOKIE',
        }

# convert keys to lower case for case-insensitive matching
for (key,value) in header2env.items():
    del header2env[key]
    key=string.lower(key)
    header2env[key]=value

class thread_output_file (select_trigger.trigger_file):

    def close (self):
        self.trigger_close()

class script_handler:

    def __init__ (self, queue, document_root=""):
        self.modules = {}
        self.document_root = document_root
        self.queue = queue

    def add_module (self, module, *names):
        if not names:
            names = ["/%s" % module.__name__]
        for name in names:
            self.modules['/'+name] = module

    def match (self, request):
        uri = request.uri

        i = string.find(uri, "/", 1)
        if i != -1:
            uri = uri[:i]

        i = string.find(uri, "?", 1)
        if i != -1:
            uri = uri[:i]

        if self.modules.has_key (uri):
            request.module = self.modules[uri]
            return 1
        else:
            return 0

    def handle_request (self, request):

        [path, params, query, fragment] = request.split_uri()

        while path and path[0] == '/':
            path = path[1:]

        if '%' in path:
            path = unquote (path)

        env = {}

        env['REQUEST_URI'] = "/" + path
        env['REQUEST_METHOD']   = string.upper(request.command)
        env['SERVER_PORT']       = str(request.channel.server.port)
        env['SERVER_NAME']       = request.channel.server.server_name
        env['SERVER_SOFTWARE'] = request['Server']
        env['DOCUMENT_ROOT']     = self.document_root

        parts = string.split(path, "/")

        # are script_name and path_info ok?

        env['SCRIPT_NAME']      = "/" + parts[0]

        if query and query[0] == "?":
            query = query[1:]

        env['QUERY_STRING']     = query

        try:
            path_info = "/" + string.join(parts[1:], "/")
        except:
            path_info = ''

        env['PATH_INFO']                = path_info
        env['GATEWAY_INTERFACE']='CGI/1.1'                                      # what should this really be?
        env['REMOTE_ADDR']              =request.channel.addr[0]
        env['REMOTE_HOST']              =request.channel.addr[0]        # TODO: connect to resolver

        for header in request.header:
            [key,value]=string.split(header,": ",1)
            key=string.lower(key)

            if header2env.has_key(key):
                if header2env[key]:
                    env[header2env[key]]=value
            else:
                key = 'HTTP_' + string.upper(
                        string.join(
                                string.split (key,"-"),
                                "_"
                                )
                        )
                env[key]=value

        ## remove empty environment variables
        for key in env.keys():
            if env[key]=="" or env[key]==None:
                del env[key]

        try:
            httphost = env['HTTP_HOST']
            parts = string.split(httphost,":")
            env['HTTP_HOST'] = parts[0]
        except KeyError:
            pass

        if request.command in ('put', 'post'):
            # PUT data requires a correct Content-Length: header
            # (though I bet with http/1.1 we can expect chunked encoding)
            request.collector = collector (self, request, env)
            request.channel.set_terminator (None)
        else:
            sin = StringIO.StringIO ('')
            self.continue_request (sin, request, env)

    def continue_request (self, stdin, request, env):
        stdout = header_scanning_file (
                request,
                thread_output_file (request.channel)
                )
        self.queue.put (
                (request.module.main, (env, stdin, stdout))
                )

HEADER_LINE = re.compile ('([A-Za-z0-9-]+): ([^\r\n]+)')

# A file wrapper that handles the CGI 'Status:' header hack
# by scanning the output.

class header_scanning_file:

    def __init__ (self, request, file):
        self.buffer = ''
        self.request = request
        self.file = file
        self.got_header = 0
        self.bytes_out = counter.counter()

    def write (self, data):
        if self.got_header:
            self._write (data)
        else:
            # CGI scripts may optionally provide extra headers.
            #
            # If they do not, then the output is assumed to be
            # text/html, with an HTTP reply code of '200 OK'.
            #
            # If they do, we need to scan those headers for one in
            # particular: the 'Status:' header, which will tell us
            # to use a different HTTP reply code [like '302 Moved']
            #
            self.buffer = self.buffer + data
            lines = string.split (self.buffer, '\n')
            # ignore the last piece, it is either empty, or a partial line
            lines = lines[:-1]
            # look for something un-header-like
            for i in range(len(lines)):
                li = lines[i]
                if (not li) or (HEADER_LINE.match (li) is None):
                    # this is either the header separator, or it
                    # is not a header line.
                    self.got_header = 1
                    h = self.build_header (lines[:i])
                    self._write (h)
                    # rejoin the rest of the data
                    d = string.join (lines[i:], '\n')
                    self._write (d)
                    self.buffer = ''
                    break

    def build_header (self, lines):
        status = '200 OK'
        saw_content_type = 0
        hl = HEADER_LINE
        for line in lines:
            mo = hl.match (line)
            if mo is not None:
                h = string.lower (mo.group(1))
                if h == 'status':
                    status = mo.group(2)
                elif h == 'content-type':
                    saw_content_type = 1
        lines.insert (0, 'HTTP/1.0 %s' % status)
        lines.append ('Server: ' + self.request['Server'])
        lines.append ('Date: ' + self.request['Date'])
        if not saw_content_type:
            lines.append ('Content-Type: text/html')
        lines.append ('Connection: close')
        return string.join (lines, '\r\n')+'\r\n\r\n'

    def _write (self, data):
        self.bytes_out.increment (len(data))
        self.file.write (data)

    def writelines(self, list):
        self.write (string.join (list, ''))

    def flush(self):
        pass

    def close (self):
        if not self.got_header:
            # managed to slip through our header detectors
            self._write (self.build_header (['Status: 502', 'Content-Type: text/html']))
            self._write (
                    '<html><h1>Server Error</h1>\r\n'
                    '<b>Bad Gateway:</b> No Header from CGI Script\r\n'
                    '<pre>Data: %s</pre>'
                    '</html>\r\n' % (repr(self.buffer))
                    )
        self.request.log (int(self.bytes_out.as_long()))
        self.file.close()
        self.request.channel.current_request = None


class collector:

    "gathers input for PUT requests"

    def __init__ (self, handler, request, env):
        self.handler    = handler
        self.env = env
        self.request    = request
        self.data = StringIO.StringIO()

        # make sure there's a content-length header
        self.cl = request.get_header ('content-length')

        if not self.cl:
            request.error (411)
            return
        else:
            self.cl = string.atoi(self.cl)

    def collect_incoming_data (self, data):
        self.data.write (data)
        if self.data.tell() >= self.cl:
            self.data.seek(0)

            h=self.handler
            r=self.request

            # set the terminator back to the default
            self.request.channel.set_terminator ('\r\n\r\n')
            del self.handler
            del self.request

            h.continue_request (self.data, r, self.env)


class request_loop_thread (threading.Thread):

    def __init__ (self, queue):
        threading.Thread.__init__ (self)
        self.setDaemon(1)
        self.queue = queue

    def run (self):
        while 1:
            function, (env, stdin, stdout) = self.queue.get()
            function (env, stdin, stdout)
            stdout.close()

# ===========================================================================
#                                                          Testing
# ===========================================================================

if __name__ == '__main__':

    import sys

    if len(sys.argv) < 2:
        print 'Usage: %s <worker_threads>' % sys.argv[0]
    else:
        nthreads = string.atoi (sys.argv[1])

        import asyncore_25 as asyncore
        from supervisor.medusa import http_server
        # create a generic web server
        hs = http_server.http_server ('', 7080)

        # create a request queue
        q = request_queue()

        # create a script handler
        sh = script_handler (q)

        # install the script handler on the web server
        hs.install_handler (sh)

        # get a couple of CGI modules
        import test_module
        import pi_module

        # install the module on the script handler
        sh.add_module (test_module, 'test')
        sh.add_module (pi_module, 'pi')

        # fire up the worker threads
        for i in range (nthreads):
            rt = request_loop_thread (q)
            rt.start()

        # start the main event loop
        asyncore.loop()

########NEW FILE########
__FILENAME__ = unix_user_handler
# -*- Mode: Python -*-
#
#       Author: Sam Rushing <rushing@nightmare.com>
#       Copyright 1996, 1997 by Sam Rushing
#                                                All Rights Reserved.
#

RCS_ID =  '$Id: unix_user_handler.py,v 1.4 2002/11/25 00:09:23 akuchling Exp $'

# support for `~user/public_html'.

import re
import string
import default_handler
import filesys
import os
import pwd

get_header = default_handler.get_header

user_dir = re.compile ('/~([^/]+)(.*)')

class unix_user_handler (default_handler.default_handler):

    def __init__ (self, public_html = 'public_html'):
        self.public_html = public_html
        default_handler.default_handler.__init__ (self, None)

    # cache userdir-filesystem objects
    fs_cache = {}

    def match (self, request):
        m = user_dir.match (request.uri)
        return m and (m.end() == len (request.uri))

    def handle_request (self, request):
        # get the user name
        m = user_dir.match (request.uri)
        user = m.group(1)
        rest = m.group(2)

        # special hack to catch those lazy URL typers
        if not rest:
            request['Location'] = '/~%s/' % user
            request.error (301)
            return

        # have we already built a userdir fs for this user?
        if self.fs_cache.has_key (user):
            fs = self.fs_cache[user]
        else:
            # no, well then, let's build one.
            # first, find out where the user directory is
            try:
                info = pwd.getpwnam (user)
            except KeyError:
                request.error (404)
                return
            ud = info[5] + '/' + self.public_html
            if os.path.isdir (ud):
                fs = filesys.os_filesystem (ud)
                self.fs_cache[user] = fs
            else:
                request.error (404)
                return

        # fake out default_handler
        self.filesystem = fs
        # massage the request URI
        request.uri = '/' + rest
        return default_handler.default_handler.handle_request (self, request)

    def __repr__ (self):
        return '<Unix User Directory Handler at %08x [~user/%s, %d filesystems loaded]>' % (
                id(self),
                self.public_html,
                len(self.fs_cache)
                )

########NEW FILE########
__FILENAME__ = virtual_handler
# -*- Mode: Python -*-

import socket
import default_handler
import re

HOST = re.compile ('Host: ([^:/]+).*', re.IGNORECASE)

get_header = default_handler.get_header

class virtual_handler:

    """HTTP request handler for an HTTP/1.0-style virtual host.  Each
    Virtual host must have a different IP"""

    def __init__ (self, handler, hostname):
        self.handler = handler
        self.hostname = hostname
        try:
            self.ip = socket.gethostbyname (hostname)
        except socket.error:
            raise ValueError, "Virtual Hostname %s does not appear to be registered in the DNS" % hostname

    def match (self, request):
        if (request.channel.addr[0] == self.ip):
            return 1
        else:
            return 0

    def handle_request (self, request):
        return self.handler.handle_request (request)

    def __repr__ (self):
        return '<virtual request handler for %s>' % self.hostname


class virtual_handler_with_host:

    """HTTP request handler for HTTP/1.1-style virtual hosts.  This
    matches by checking the value of the 'Host' header in the request.
    You actually don't _have_ to support HTTP/1.1 to use this, since
    many browsers now send the 'Host' header.  This is a Good Thing."""

    def __init__ (self, handler, hostname):
        self.handler = handler
        self.hostname = hostname

    def match (self, request):
        host = get_header (HOST, request.header)
        if host == self.hostname:
            return 1
        else:
            return 0

    def handle_request (self, request):
        return self.handler.handle_request (request)

    def __repr__ (self):
        return '<virtual request handler for %s>' % self.hostname

########NEW FILE########
__FILENAME__ = xmlrpc_handler
# -*- Mode: Python -*-

# See http://www.xml-rpc.com/
#     http://www.pythonware.com/products/xmlrpc/

# Based on "xmlrpcserver.py" by Fredrik Lundh (fredrik@pythonware.com)

VERSION = "$Id: xmlrpc_handler.py,v 1.6 2004/04/21 14:09:24 akuchling Exp $"

import http_server
import xmlrpclib

import string
import sys

class xmlrpc_handler:

    def match (self, request):
        # Note: /RPC2 is not required by the spec, so you may override this method.
        if request.uri[:5] == '/RPC2':
            return 1
        else:
            return 0

    def handle_request (self, request):
        [path, params, query, fragment] = request.split_uri()

        if request.command == 'POST':
            request.collector = collector (self, request)
        else:
            request.error (400)

    def continue_request (self, data, request):
        params, method = xmlrpclib.loads (data)
        try:
            # generate response
            try:
                response = self.call (method, params)
                if type(response) != type(()):
                    response = (response,)
            except:
                # report exception back to server
                response = xmlrpclib.dumps (
                        xmlrpclib.Fault (1, "%s:%s" % (sys.exc_type, sys.exc_value))
                        )
            else:
                response = xmlrpclib.dumps (response, methodresponse=1)
        except:
            # internal error, report as HTTP server error
            request.error (500)
        else:
            # got a valid XML RPC response
            request['Content-Type'] = 'text/xml'
            request.push (response)
            request.done()

    def call (self, method, params):
        # override this method to implement RPC methods
        raise "NotYetImplemented"

class collector:

    "gathers input for POST and PUT requests"

    def __init__ (self, handler, request):

        self.handler = handler
        self.request = request
        self.data = []

        # make sure there's a content-length header
        cl = request.get_header ('content-length')

        if not cl:
            request.error (411)
        else:
            cl = string.atoi (cl)
            # using a 'numeric' terminator
            self.request.channel.set_terminator (cl)

    def collect_incoming_data (self, data):
        self.data.append(data)

    def found_terminator (self):
        # set the terminator back to the default
        self.request.channel.set_terminator ('\r\n\r\n')
        self.handler.continue_request ("".join(self.data), self.request)

if __name__ == '__main__':

    class rpc_demo (xmlrpc_handler):

        def call (self, method, params):
            print 'method="%s" params=%s' % (method, params)
            return "Sure, that works"

    import asyncore_25 as asyncore

    hs = http_server.http_server ('', 8000)
    rpc = rpc_demo()
    hs.install_handler (rpc)

    asyncore.loop()

########NEW FILE########
__FILENAME__ = options
# This file was modified by Xiaomi.com on 2013-6-27.

import ConfigParser
import socket
import getopt
import os
import sys
import tempfile
import errno
import signal
import re
import xmlrpclib
import pwd
import grp
import resource
import stat
import pkg_resources
import select
import glob
import platform
import warnings

from fcntl import fcntl
from fcntl import F_SETFL, F_GETFL

from supervisor.medusa import asyncore_25 as asyncore

from supervisor.datatypes import boolean
from supervisor.datatypes import integer
from supervisor.datatypes import name_to_uid
from supervisor.datatypes import gid_for_uid
from supervisor.datatypes import existing_dirpath
from supervisor.datatypes import byte_size
from supervisor.datatypes import signal_number
from supervisor.datatypes import list_of_exitcodes
from supervisor.datatypes import dict_of_key_value_pairs
from supervisor.datatypes import logfile_name
from supervisor.datatypes import list_of_strings
from supervisor.datatypes import octal_type
from supervisor.datatypes import existing_directory
from supervisor.datatypes import logging_level
from supervisor.datatypes import colon_separated_user_group
from supervisor.datatypes import inet_address
from supervisor.datatypes import InetStreamSocketConfig
from supervisor.datatypes import UnixStreamSocketConfig
from supervisor.datatypes import url
from supervisor.datatypes import Automatic
from supervisor.datatypes import auto_restart
from supervisor.datatypes import profile_options
from supervisor.datatypes import set_here

from supervisor.states import ProcessStates

from supervisor import loggers
from supervisor import states
from supervisor import xmlrpc

mydir = os.path.abspath(os.path.dirname(__file__))
version_txt = os.path.join(mydir, 'version.txt')
VERSION = open(version_txt).read().strip()

def normalize_path(v):
    return os.path.normpath(os.path.abspath(os.path.expanduser(v)))

class Dummy:
    pass

class Options:
    stderr = sys.stderr
    stdout = sys.stdout
    exit = sys.exit
    warnings = warnings

    uid = gid = None

    progname = sys.argv[0]
    configfile = None
    schemadir = None
    configroot = None
    here = None

    # Class variable deciding whether positional arguments are allowed.
    # If you want positional arguments, set this to 1 in your subclass.
    positional_args_allowed = 0

    def __init__(self):
        self.names_list = []
        self.short_options = []
        self.long_options = []
        self.options_map = {}
        self.default_map = {}
        self.required_map = {}
        self.environ_map = {}
        self.attr_priorities = {}
        self.add(None, None, "h", "help", self.help)
        self.add("configfile", None, "c:", "configuration=")

    def default_configfile(self):
        """Return the name of the found config file or raise. """
        here = os.path.dirname(os.path.dirname(sys.argv[0]))
        paths = [os.path.join(here, 'etc', 'supervisord.conf'),
                 os.path.join(here, 'supervisord.conf'),
                 'supervisord.conf', 'etc/supervisord.conf',
                 '/etc/supervisord.conf']
        config = None
        for path in paths:
            if os.path.exists(path):
                config = path
                break
        if config is None:
            self.usage('No config file found at default paths (%s); '
                       'use the -c option to specify a config file '
                       'at a different path' % ', '.join(paths))
        return config

    def help(self, dummy):
        """Print a long help message to stdout and exit(0).

        Occurrences of "%s" in are replaced by self.progname.
        """
        help = self.doc
        if help.find("%s") > 0:
            help = help.replace("%s", self.progname)
        print help,
        self.exit(0)

    def usage(self, msg):
        """Print a brief error message to stderr and exit(2)."""
        self.stderr.write("Error: %s\n" % str(msg))
        self.stderr.write("For help, use %s -h\n" % self.progname)
        self.exit(2)

    def add(self,
            name=None,                  # attribute name on self
            confname=None,              # dotted config path name
            short=None,                 # short option name
            long=None,                  # long option name
            handler=None,               # handler (defaults to string)
            default=None,               # default value
            required=None,              # message if not provided
            flag=None,                  # if not None, flag value
            env=None,                   # if not None, environment variable
            ):
        """Add information about a configuration option.

        This can take several forms:

        add(name, confname)
            Configuration option 'confname' maps to attribute 'name'
        add(name, None, short, long)
            Command line option '-short' or '--long' maps to 'name'
        add(None, None, short, long, handler)
            Command line option calls handler
        add(name, None, short, long, handler)
            Assign handler return value to attribute 'name'

        In addition, one of the following keyword arguments may be given:

        default=...  -- if not None, the default value
        required=... -- if nonempty, an error message if no value provided
        flag=...     -- if not None, flag value for command line option
        env=...      -- if not None, name of environment variable that
                        overrides the configuration file or default
        """
        if flag is not None:
            if handler is not None:
                raise ValueError, "use at most one of flag= and handler="
            if not long and not short:
                raise ValueError, "flag= requires a command line flag"
            if short and short.endswith(":"):
                raise ValueError, "flag= requires a command line flag"
            if long and long.endswith("="):
                raise ValueError, "flag= requires a command line flag"
            handler = lambda arg, flag=flag: flag

        if short and long:
            if short.endswith(":") != long.endswith("="):
                raise ValueError, "inconsistent short/long options: %r %r" % (
                    short, long)

        if short:
            if short[0] == "-":
                raise ValueError, "short option should not start with '-'"
            key, rest = short[:1], short[1:]
            if rest not in ("", ":"):
                raise ValueError, "short option should be 'x' or 'x:'"
            key = "-" + key
            if self.options_map.has_key(key):
                raise ValueError, "duplicate short option key '%s'" % key
            self.options_map[key] = (name, handler)
            self.short_options.append(short)

        if long:
            if long[0] == "-":
                raise ValueError, "long option should not start with '-'"
            key = long
            if key[-1] == "=":
                key = key[:-1]
            key = "--" + key
            if self.options_map.has_key(key):
                raise ValueError, "duplicate long option key '%s'" % key
            self.options_map[key] = (name, handler)
            self.long_options.append(long)

        if env:
            self.environ_map[env] = (name, handler)

        if name:
            if not hasattr(self, name):
                setattr(self, name, None)
            self.names_list.append((name, confname))
            if default is not None:
                self.default_map[name] = default
            if required:
                self.required_map[name] = required

    def _set(self, attr, value, prio):
        current = self.attr_priorities.get(attr, -1)
        if prio >= current:
            setattr(self, attr, value)
            self.attr_priorities[attr] = prio

    def realize(self, args=None, doc=None,
                progname=None, raise_getopt_errs=True):
        """Realize a configuration.

        Optional arguments:

        args     -- the command line arguments, less the program name
                    (default is sys.argv[1:])

        doc      -- usage message (default is __main__.__doc__)
        """
        # Provide dynamic default method arguments
        if args is None:
            args = sys.argv[1:]
        if progname is None:
            progname = sys.argv[0]
        if doc is None:
            import __main__
            doc = __main__.__doc__
        self.progname = progname
        self.doc = doc

        self.options = []
        self.args = []

        # Call getopt
        try:
            self.options, self.args = getopt.getopt(
                args, "".join(self.short_options), self.long_options)
        except getopt.error, msg:
            if raise_getopt_errs:
                self.usage(msg)

        # Check for positional args
        if self.args and not self.positional_args_allowed:
            self.usage("positional arguments are not supported")

        # Process options returned by getopt
        for opt, arg in self.options:
            name, handler = self.options_map[opt]
            if handler is not None:
                try:
                    arg = handler(arg)
                except ValueError, msg:
                    self.usage("invalid value for %s %r: %s" % (opt, arg, msg))
            if name and arg is not None:
                if getattr(self, name) is not None:
                    self.usage("conflicting command line option %r" % opt)
                self._set(name, arg, 2)

        # Process environment variables
        for envvar in self.environ_map.keys():
            name, handler = self.environ_map[envvar]
            if os.environ.has_key(envvar):
                value = os.environ[envvar]
                if handler is not None:
                    try:
                        value = handler(value)
                    except ValueError, msg:
                        self.usage("invalid environment value for %s %r: %s"
                                   % (envvar, value, msg))
                if name and value is not None:
                    self._set(name, value, 1)

        if self.configfile is None:
            if os.getuid() == 0 and self.progname.find("supervisord") > -1: # pragma: no cover
                self.warnings.warn(
                    'Supervisord is running as root and it is searching '
                    'for its configuration file in default locations '
                    '(including its current working directory); you '
                    'probably want to specify a "-c" argument specifying an '
                    'absolute path to a configuration file for improved '
                    'security.'
                    )

            self.configfile = self.default_configfile()

        self.process_config_file()

    def process_config_file(self, do_usage=True):
        """Process config file."""
        if not hasattr(self.configfile, 'read'):
            self.here = os.path.abspath(os.path.dirname(self.configfile))
            set_here(self.here)
        try:
            self.read_config(self.configfile)
        except ValueError, msg:
            if do_usage:
                # if this is not called from an RPC method, run usage and exit.
                self.usage(str(msg))
            else:
                # if this is called from an RPC method, raise an error
                raise ValueError(msg)

        # Copy config options to attributes of self.  This only fills
        # in options that aren't already set from the command line.
        for name, confname in self.names_list:
            if confname:
                parts = confname.split(".")
                obj = self.configroot
                for part in parts:
                    if obj is None:
                        break
                    # Here AttributeError is not a user error!
                    obj = getattr(obj, part)
                self._set(name, obj, 0)

        # Process defaults
        for name, value in self.default_map.items():
            if getattr(self, name) is None:
                setattr(self, name, value)

        # Process required options
        for name, message in self.required_map.items():
            if getattr(self, name) is None:
                self.usage(message)

    def get_plugins(self, parser, factory_key, section_prefix):
        factories = []

        for section in parser.sections():
            if not section.startswith(section_prefix):
                continue
            name = section.split(':', 1)[1]
            factory_spec = parser.saneget(section, factory_key, None)
            if factory_spec is None:
                raise ValueError('section [%s] does not specify a %s'  %
                                 (section, factory_key))
            try:
                factory = self.import_spec(factory_spec)
            except ImportError:
                raise ValueError('%s cannot be resolved within [%s]' % (
                    factory_spec, section))
            items = parser.items(section)
            items.remove((factory_key, factory_spec))
            factories.append((name, factory, dict(items)))

        return factories

    def import_spec(self, spec):
        return pkg_resources.EntryPoint.parse("x="+spec).load(False)


class ServerOptions(Options):
    user = None
    sockchown = None
    sockchmod = None
    logfile = None
    loglevel = None
    pidfile = None
    subprocpidfile = None
    passwdfile = None
    nodaemon = None
    environment = None
    httpservers = ()
    unlink_socketfiles = True
    mood = states.SupervisorStates.RUNNING

    def __init__(self):
        Options.__init__(self)
        self.configroot = Dummy()
        self.configroot.supervisord = Dummy()

        self.add(None, None, "v", "version", self.version)
        self.add("nodaemon", "supervisord.nodaemon", "n", "nodaemon", flag=1,
                 default=0)
        self.add("user", "supervisord.user", "u:", "user=")
        self.add("umask", "supervisord.umask", "m:", "umask=",
                 octal_type, default='022')
        self.add("directory", "supervisord.directory", "d:", "directory=",
                 existing_directory)
        self.add("logfile", "supervisord.logfile", "l:", "logfile=",
                 existing_dirpath, default="supervisord.log")
        self.add("logfile_maxbytes", "supervisord.logfile_maxbytes",
                 "y:", "logfile_maxbytes=", byte_size,
                 default=50 * 1024 * 1024) # 50MB
        self.add("logfile_backups", "supervisord.logfile_backups",
                 "z:", "logfile_backups=", integer, default=10)
        self.add("loglevel", "supervisord.loglevel", "e:", "loglevel=",
                 logging_level, default="info")
        self.add("pidfile", "supervisord.pidfile", "j:", "pidfile=",
                 existing_dirpath, default="supervisord.pid")
        self.add("subprocpidfile", "supervisord.subprocpidfile",
                 "s:", "subprocpidfile=", existing_dirpath)
        self.add("identifier", "supervisord.identifier", "i:", "identifier=",
                 str, default="supervisor")
        self.add("childlogdir", "supervisord.childlogdir", "q:", "childlogdir=",
                 existing_directory, default=tempfile.gettempdir())
        self.add("minfds", "supervisord.minfds",
                 "a:", "minfds=", int, default=1024)
        self.add("minprocs", "supervisord.minprocs",
                 "", "minprocs=", int, default=200)
        self.add("nocleanup", "supervisord.nocleanup",
                 "k", "nocleanup", flag=1, default=0)
        self.add("strip_ansi", "supervisord.strip_ansi",
                 "t", "strip_ansi", flag=1, default=0)
        self.add("profile_options", "supervisord.profile_options",
                 "", "profile_options=", profile_options, default=None)
        self.pidhistory = {}
        self.process_group_configs = []
        self.parse_warnings = []
        self.signal_receiver = SignalReceiver()
        self.resumed_pids = set()

    def version(self, dummy):
        """Print version to stdout and exit(0).
        """
        self.stdout.write('%s\n' % VERSION)
        self.exit(0)

    def getLogger(self, filename, level, fmt, rotating=False, maxbytes=0,
                  backups=0, stdout=False):
        return loggers.getLogger(filename, level, fmt, rotating, maxbytes,
                                 backups, stdout)

    def realize(self, *arg, **kw):
        Options.realize(self, *arg, **kw)
        section = self.configroot.supervisord

        # Additional checking of user option; set uid and gid
        if self.user is not None:
            uid = name_to_uid(self.user)
            if uid is None:
                self.usage("No such user %s" % self.user)
            self.uid = uid
            self.gid = gid_for_uid(uid)

        if not self.loglevel:
            self.loglevel = section.loglevel

        if self.logfile:
            logfile = self.logfile
        else:
            logfile = section.logfile

        self.logfile = normalize_path(logfile)

        if self.pidfile:
            pidfile = self.pidfile
        else:
            pidfile = section.pidfile

        self.pidfile = normalize_path(pidfile)

        if self.subprocpidfile:
            subprocpidfile = self.subprocpidfile
        else:
            subprocpidfile = section.subprocpidfile

        if subprocpidfile:
            self.subprocpidfile = normalize_path(subprocpidfile)

        self.rpcinterface_factories = section.rpcinterface_factories

        self.serverurl = None

        self.server_configs = sconfigs = section.server_configs

        # we need to set a fallback serverurl that process.spawn can use

        # prefer a unix domain socket
        for config in [ config for config in sconfigs if
                        config['family'] is socket.AF_UNIX ]:
            path = config['file']
            self.serverurl = 'unix://%s' % path
            break

        # fall back to an inet socket
        if self.serverurl is None:
            for config in [ config for config in sconfigs if
                            config['family'] is socket.AF_INET]:
                host = config['host']
                port = config['port']
                if not host:
                    host = 'localhost'
                self.serverurl = 'http://%s:%s' % (host, port)

        # self.serverurl may still be None if no servers at all are
        # configured in the config file

        self.identifier = section.identifier

    def process_config_file(self, do_usage=True):
        Options.process_config_file(self, do_usage=do_usage)

        new = self.configroot.supervisord.process_group_configs
        self.process_group_configs = new

    def read_config(self, fp):
        # Clear parse warnings, since we may be re-reading the
        # config a second time after a reload.
        self.parse_warnings = []

        section = self.configroot.supervisord
        if not hasattr(fp, 'read'):
            try:
                fp = open(fp, 'r')
            except (IOError, OSError):
                raise ValueError("could not find config file %s" % fp)
        parser = UnhosedConfigParser()
        try:
            parser.readfp(fp)
        except ConfigParser.ParsingError, why:
            raise ValueError(str(why))

        if parser.has_section('include'):
            if not parser.has_option('include', 'files'):
                raise ValueError(".ini file has [include] section, but no "
                "files setting")
            files = parser.get('include', 'files')
            files = files.split()
            if hasattr(fp, 'name'):
                base = os.path.dirname(os.path.abspath(fp.name))
            else:
                base = '.'
            for pattern in files:
                pattern = os.path.join(base, pattern)
                for filename in glob.glob(pattern):
                    self.parse_warnings.append(
                        'Included extra file "%s" during parsing' % filename)
                    try:
                        parser.read(filename)
                    except ConfigParser.ParsingError, why:
                        raise ValueError(str(why))

        sections = parser.sections()
        if not 'supervisord' in sections:
            raise ValueError, '.ini file does not include supervisord section'
        get = parser.getdefault
        section.minfds = integer(get('minfds', 1024))
        section.minprocs = integer(get('minprocs', 200))

        directory = get('directory', None)
        if directory is None:
            section.directory = None
        else:
            section.directory = existing_directory(directory)

        section.user = get('user', None)
        section.umask = octal_type(get('umask', '022'))
        section.logfile = existing_dirpath(get('logfile', 'supervisord.log'))
        section.logfile_maxbytes = byte_size(get('logfile_maxbytes', '50MB'))
        section.logfile_backups = integer(get('logfile_backups', 10))
        section.loglevel = logging_level(get('loglevel', 'info'))
        section.pidfile = existing_dirpath(get('pidfile', 'supervisord.pid'))
        section.subprocpidfile = existing_dirpath(get('subprocpidfile', ''))
        section.identifier = get('identifier', 'supervisor')
        section.nodaemon = boolean(get('nodaemon', 'false'))

        tempdir = tempfile.gettempdir()
        section.childlogdir = existing_directory(get('childlogdir', tempdir))
        section.nocleanup = boolean(get('nocleanup', 'false'))
        section.strip_ansi = boolean(get('strip_ansi', 'false'))

        expansions = {'here':self.here}
        expansions.update(environ_expansions())
        environ_str = get('environment', '')
        environ_str = expand(environ_str, expansions, 'environment')
        section.environment = dict_of_key_value_pairs(environ_str)
        # Process rpcinterface plugins before groups to allow custom events to
        # be registered.
        section.rpcinterface_factories = self.get_plugins(
            parser,
            'supervisor.rpcinterface_factory',
            'rpcinterface:'
            )
        section.process_group_configs = self.process_groups_from_parser(parser)
        for group in section.process_group_configs:
            for proc in group.process_configs:
                env = section.environment.copy()
                env.update(proc.environment)
                proc.environment = env
        section.server_configs = self.server_configs_from_parser(parser)
        section.profile_options = None
        return section

    def process_groups_from_parser(self, parser):
        groups = []
        all_sections = parser.sections()
        homogeneous_exclude = []
        get = parser.saneget

        # process heterogeneous groups
        for section in all_sections:
            if not section.startswith('group:'):
                continue
            group_name = section.split(':', 1)[1]
            programs = list_of_strings(get(section, 'programs', None))
            priority = integer(get(section, 'priority', 999))
            group_processes = []
            for program in programs:
                program_section = "program:%s" % program
                if not program_section in all_sections:
                    raise ValueError(
                        '[%s] names unknown program %s' % (section, program))
                homogeneous_exclude.append(program_section)
                processes = self.processes_from_section(parser, program_section,
                                                        group_name,
                                                        ProcessConfig)
                group_processes.extend(processes)
            groups.append(
                ProcessGroupConfig(self, group_name, priority, group_processes)
                )

        # process "normal" homogeneous groups
        for section in all_sections:
            if ( (not section.startswith('program:') )
                 or section in homogeneous_exclude ):
                continue
            program_name = section.split(':', 1)[1]
            priority = integer(get(section, 'priority', 999))
            processes=self.processes_from_section(parser, section, program_name,
                                                  ProcessConfig)
            groups.append(
                ProcessGroupConfig(self, program_name, priority, processes)
                )

        # process "event listener" homogeneous groups
        for section in all_sections:
            if not section.startswith('eventlistener:'):
                continue
            pool_name = section.split(':', 1)[1]
            # give listeners a "high" default priority so they are started first
            # and stopped last at mainloop exit
            priority = integer(get(section, 'priority', -1))
            buffer_size = integer(get(section, 'buffer_size', 10))
            result_handler = get(section, 'result_handler',
                                       'supervisor.dispatchers:default_handler')
            try:
                result_handler = self.import_spec(result_handler)
            except ImportError:
                raise ValueError('%s cannot be resolved within [%s]' % (
                    result_handler, section))
            pool_event_names = [x.upper() for x in
                                list_of_strings(get(section, 'events', ''))]
            pool_event_names = set(pool_event_names)
            if not pool_event_names:
                raise ValueError('[%s] section requires an "events" line' %
                                 section)
            from supervisor.events import EventTypes
            pool_events = []
            for pool_event_name in pool_event_names:
                pool_event = getattr(EventTypes, pool_event_name, None)
                if pool_event is None:
                    raise ValueError('Unknown event type %s in [%s] events' %
                                     (pool_event_name, section))
                pool_events.append(pool_event)
            processes=self.processes_from_section(parser, section, pool_name,
                                                  EventListenerConfig)

            groups.append(
                EventListenerPoolConfig(self, pool_name, priority, processes,
                                        buffer_size, pool_events,
                                        result_handler)
                )

        # process fastcgi homogeneous groups
        for section in all_sections:
            if ( (not section.startswith('fcgi-program:') )
                 or section in homogeneous_exclude ):
                continue
            program_name = section.split(':', 1)[1]
            priority = integer(get(section, 'priority', 999))

            proc_uid = name_to_uid(get(section, 'user', None))

            socket_owner = get(section, 'socket_owner', None)
            if socket_owner is not None:
                try:
                    socket_owner = colon_separated_user_group(socket_owner)
                except ValueError:
                    raise ValueError('Invalid socket_owner value %s'
                                                                % socket_owner)

            socket_mode = get(section, 'socket_mode', None)
            if socket_mode is not None:
                try:
                    socket_mode = octal_type(socket_mode)
                except (TypeError, ValueError):
                    raise ValueError('Invalid socket_mode value %s'
                                                                % socket_mode)

            socket = get(section, 'socket', None)
            if not socket:
                raise ValueError('[%s] section requires a "socket" line' %
                                 section)

            expansions = {'here':self.here,
                          'program_name':program_name}
            expansions.update(environ_expansions())
            socket = expand(socket, expansions, 'socket')
            try:
                socket_config = self.parse_fcgi_socket(socket, proc_uid,
                                                    socket_owner, socket_mode)
            except ValueError, e:
                raise ValueError('%s in [%s] socket' % (str(e), section))

            processes=self.processes_from_section(parser, section, program_name,
                                                  FastCGIProcessConfig)
            groups.append(
                FastCGIGroupConfig(self, program_name, priority, processes,
                                   socket_config)
                )

        groups.sort()
        return groups

    def parse_fcgi_socket(self, sock, proc_uid, socket_owner, socket_mode):
        if sock.startswith('unix://'):
            path = sock[7:]
            #Check it's an absolute path
            if not os.path.isabs(path):
                raise ValueError("Unix socket path %s is not an absolute path",
                                 path)
            path = normalize_path(path)

            if socket_owner is None:
                uid = os.getuid()
                if proc_uid is not None and proc_uid != uid:
                    socket_owner = (proc_uid, self.get_gid_for_uid(proc_uid))

            if socket_mode is None:
                socket_mode = 0700

            return UnixStreamSocketConfig(path, owner=socket_owner,
                                                mode=socket_mode)

        if socket_owner is not None or socket_mode is not None:
            raise ValueError("socket_owner and socket_mode params should"
                    + " only be used with a Unix domain socket")

        m = re.match(r'tcp://([^\s:]+):(\d+)$', sock)
        if m:
            host = m.group(1)
            port = int(m.group(2))
            return InetStreamSocketConfig(host, port)

        raise ValueError("Bad socket format %s", sock)

    def get_gid_for_uid(self, uid):
        pwrec = pwd.getpwuid(uid)
        return pwrec[3]

    def processes_from_section(self, parser, section, group_name,
                               klass=None):
        if klass is None:
            klass = ProcessConfig
        programs = []
        get = parser.saneget
        program_name = section.split(':', 1)[1]

        priority = integer(get(section, 'priority', 999))
        autostart = boolean(get(section, 'autostart', 'false'))
        autorestart = auto_restart(get(section, 'autorestart', 'unexpected'))
        startsecs = integer(get(section, 'startsecs', 1))
        startretries = integer(get(section, 'startretries', 3))
        uid = name_to_uid(get(section, 'user', None))
        stopsignal = signal_number(get(section, 'stopsignal', 'TERM'))
        stopwaitsecs = integer(get(section, 'stopwaitsecs', 10))
        stopasgroup = boolean(get(section, 'stopasgroup', 'false'))
        killasgroup = boolean(get(section, 'killasgroup', stopasgroup))
        exitcodes = list_of_exitcodes(get(section, 'exitcodes', '0,2'))
        redirect_stderr = boolean(get(section, 'redirect_stderr','false'))
        numprocs = integer(get(section, 'numprocs', 1))
        numprocs_start = integer(get(section, 'numprocs_start', 0))
        process_name = get(section, 'process_name', '%(program_name)s')
        environment_str = get(section, 'environment', '')
        stdout_cmaxbytes = byte_size(get(section,'stdout_capture_maxbytes','0'))
        stdout_events = boolean(get(section, 'stdout_events_enabled','false'))
        stderr_cmaxbytes = byte_size(get(section,'stderr_capture_maxbytes','0'))
        stderr_events = boolean(get(section, 'stderr_events_enabled','false'))
        directory = get(section, 'directory', None)
        serverurl = get(section, 'serverurl', None)
        http_url = get(section, 'http_url', None)
        if serverurl and serverurl.strip().upper() == 'AUTO':
            serverurl = None

        umask = get(section, 'umask', None)
        if umask is not None:
            umask = octal_type(umask)

        command = get(section, 'command', None)
        if command is None:
            raise ValueError, (
                'program section %s does not specify a command' % section)

        if numprocs > 1:
            if process_name.find('%(process_num)') == -1:
                # process_name needs to include process_num when we
                # represent a group of processes
                raise ValueError(
                    '%(process_num) must be present within process_name when '
                    'numprocs > 1')
                    
        if stopasgroup and not killasgroup:
            raise ValueError("Cannot set stopasgroup=true and killasgroup=false")

        host_node_name = platform.node()
        for process_num in range(numprocs_start, numprocs + numprocs_start):
            expansions = {'here':self.here,
                          'process_num':process_num,
                          'program_name':program_name,
                          'host_node_name':host_node_name,
                          'group_name':group_name}
            expansions.update(environ_expansions())

            environment = dict_of_key_value_pairs(
                expand(environment_str, expansions, 'environment'))

            if directory:
                directory = expand(directory, expansions, 'directory')

            logfiles = {}

            for k in ('stdout', 'stderr'):
                n = '%s_logfile' % k
                lf_val = get(section, n, Automatic)
                if isinstance(lf_val, basestring):
                    lf_val = expand(lf_val, expansions, n)
                lf_val = logfile_name(lf_val)
                logfiles[n] = lf_val

                bu_key = '%s_logfile_backups' % k
                backups = integer(get(section, bu_key, 10))
                logfiles[bu_key] = backups

                mb_key = '%s_logfile_maxbytes' % k
                maxbytes = byte_size(get(section, mb_key, '50MB'))
                logfiles[mb_key] = maxbytes

                if lf_val is Automatic and not maxbytes:
                    self.parse_warnings.append(
                        'For [%s], AUTO logging used for %s without '
                        'rollover, set maxbytes > 0 to avoid filling up '
                        'filesystem unintentionally' % (section, n))

            pconfig = klass(
                self,
                name=expand(process_name, expansions, 'process_name'),
                command=expand(command, expansions, 'command'),
                directory=directory,
                umask=umask,
                priority=priority,
                autostart=autostart,
                autorestart=autorestart,
                startsecs=startsecs,
                startretries=startretries,
                uid=uid,
                stdout_logfile=logfiles['stdout_logfile'],
                stdout_capture_maxbytes = stdout_cmaxbytes,
                stdout_events_enabled = stdout_events,
                stdout_logfile_backups=logfiles['stdout_logfile_backups'],
                stdout_logfile_maxbytes=logfiles['stdout_logfile_maxbytes'],
                stderr_logfile=logfiles['stderr_logfile'],
                stderr_capture_maxbytes = stderr_cmaxbytes,
                stderr_events_enabled = stderr_events,
                stderr_logfile_backups=logfiles['stderr_logfile_backups'],
                stderr_logfile_maxbytes=logfiles['stderr_logfile_maxbytes'],
                stopsignal=stopsignal,
                stopwaitsecs=stopwaitsecs,
                stopasgroup=stopasgroup,
                killasgroup=killasgroup,
                exitcodes=exitcodes,
                redirect_stderr=redirect_stderr,
                environment=environment,
                serverurl=serverurl,
                http_url=http_url)

            programs.append(pconfig)

        programs.sort() # asc by priority
        return programs

    def _parse_servernames(self, parser, stype):
        options = []
        for section in parser.sections():
            if section.startswith(stype):
                parts = section.split(':', 1)
                if len(parts) > 1:
                    name = parts[1]
                else:
                    name = None # default sentinel
                options.append((name, section))
        return options

    def _parse_username_and_password(self, parser, section):
        get = parser.saneget
        username = get(section, 'username', None)
        password = get(section, 'password', None)
        if username is None and password is not None:
            raise ValueError(
                'Must specify username if password is specified in [%s]'
                % section)
        return {'username':username, 'password':password}

    def server_configs_from_parser(self, parser):
        configs = []
        inet_serverdefs = self._parse_servernames(parser, 'inet_http_server')
        for name, section in inet_serverdefs:
            config = {}
            get = parser.saneget
            config.update(self._parse_username_and_password(parser, section))
            config['name'] = name
            config['family'] = socket.AF_INET
            port = get(section, 'port', None)
            if port is None:
                raise ValueError('section [%s] has no port value' % section)
            host, port = inet_address(port)
            config['host'] = host
            config['port'] = port
            config['section'] = section
            configs.append(config)

        unix_serverdefs = self._parse_servernames(parser, 'unix_http_server')
        for name, section in unix_serverdefs:
            config = {}
            get = parser.saneget
            sfile = get(section, 'file', None)
            if sfile is None:
                raise ValueError('section [%s] has no file value' % section)
            sfile = sfile.strip()
            config['name'] = name
            config['family'] = socket.AF_UNIX
            sfile = expand(sfile, {'here':self.here}, 'socket file')
            config['file'] = normalize_path(sfile)
            config.update(self._parse_username_and_password(parser, section))
            chown = get(section, 'chown', None)
            if chown is not None:
                try:
                    chown = colon_separated_user_group(chown)
                except ValueError:
                    raise ValueError('Invalid sockchown value %s' % chown)
            else:
                chown = (-1, -1)
            config['chown'] = chown
            chmod = get(section, 'chmod', None)
            if chmod is not None:
                try:
                    chmod = octal_type(chmod)
                except (TypeError, ValueError):
                    raise ValueError('Invalid chmod value %s' % chmod)
            else:
                chmod = 0700
            config['chmod'] = chmod
            config['section'] = section
            configs.append(config)

        return configs

    def daemonize(self):
        # To daemonize, we need to become the leader of our own session
        # (process) group.  If we do not, signals sent to our
        # parent process will also be sent to us.   This might be bad because
        # signals such as SIGINT can be sent to our parent process during
        # normal (uninteresting) operations such as when we press Ctrl-C in the
        # parent terminal window to escape from a logtail command.
        # To disassociate ourselves from our parent's session group we use
        # os.setsid.  It means "set session id", which has the effect of
        # disassociating a process from is current session and process group
        # and setting itself up as a new session leader.
        #
        # Unfortunately we cannot call setsid if we're already a session group
        # leader, so we use "fork" to make a copy of ourselves that is
        # guaranteed to not be a session group leader.
        #
        # We also change directories, set stderr and stdout to null, and
        # change our umask.
        #
        # This explanation was (gratefully) garnered from
        # http://www.hawklord.uklinux.net/system/daemons/d3.htm

        pid = os.fork()
        if pid != 0:
            # Parent
            self.logger.blather("supervisord forked; parent exiting")
            os._exit(0)
        # Child
        self.logger.info("daemonizing the supervisord process")
        if self.directory:
            try:
                os.chdir(self.directory)
            except OSError, err:
                self.logger.critical("can't chdir into %r: %s"
                                     % (self.directory, err))
            else:
                self.logger.info("set current directory: %r"
                                 % self.directory)
        os.close(0)
        self.stdin = sys.stdin = sys.__stdin__ = open("/dev/null")
        os.close(1)
        self.stdout = sys.stdout = sys.__stdout__ = open("/dev/null", "w")
        os.close(2)
        self.stderr = sys.stderr = sys.__stderr__ = open("/dev/null", "w")
        os.setsid()
        os.umask(self.umask)
        # XXX Stevens, in his Advanced Unix book, section 13.3 (page
        # 417) recommends calling umask(0) and closing unused
        # file descriptors.  In his Network Programming book, he
        # additionally recommends ignoring SIGHUP and forking again
        # after the setsid() call, for obscure SVR4 reasons.

    def write_pidfile(self):
        pid = os.getpid()
        try:
            f = open(self.pidfile, 'w')
            f.write('%s\n' % pid)
            f.close()
        except (IOError, OSError):
            self.logger.critical('could not write pidfile %s' % self.pidfile)
        else:
            self.logger.info('supervisord started with pid %s' % pid)

    def add_process(self, process):
        self.pidhistory[process.pid] = process
        self.write_subproc_pidfile()

    def del_process(self, pid):
        del self.pidhistory[pid]
        self.write_subproc_pidfile()

    def get_process(self, pid):
        return self.pidhistory.get(pid, None)

    def write_subproc_pidfile(self):
        if not self.subprocpidfile: return
        try:
            f = open(self.subprocpidfile, 'w')
            for pid, process in self.pidhistory.iteritems():
                f.write('%s %d %d\n' %
                        (process.config.name, pid, process.laststart))
            f.close()
        except (IOError, OSError):
            self.logger.critical('could not write sub-process pidfile %s' %
                                 self.subprocpidfile)
        else:
            self.logger.info('supervisord wrote sub-process pidfile')

    def load_subproc_pidfile(self, process_groups):
        if not self.subprocpidfile: return
        resumed_processes = {}
        try:
            f = open(self.subprocpidfile, 'r')
            for line in f:
                process_name, pid, laststart = line.split()
                pid = int(pid)
                laststart = int(laststart)
                try:
                    os.kill(pid, 0)
                except:
                    self.logger.info(
                        "pid doesn't exist, can't resume '%s' with pid %d" %
                        (process_name, pid))
                else:
                    self.logger.info(
                        "would resume process '%s' with pid %d later" %
                        (process_name, pid))
                    resumed_processes[process_name] = (pid, laststart)
            f.close()
        except (IOError, OSError, ValueError) as e:
            self.logger.warn('could not load sub-process pidfile %s' %
                             self.subprocpidfile)
            print type(e)
        else:
            self.logger.info('supervisord load sub-process pidfile')

        for group in process_groups.itervalues():
            for process in group.processes.itervalues():
                process_name = process.config.name
                if process_name in resumed_processes:
                    process.pid, process.laststart = resumed_processes[process_name]
                    process.resumed = True
                    process.change_state(ProcessStates.RUNNING)
                    self.add_process(process)

                    del resumed_processes[process_name]
                    self.resumed_pids.add(process.pid)

                    self.logger.info(
                        "success: resumed process '%s' with pid %d" %
                        (process_name, process.pid))

    def cleanup(self):
        try:
            for config, server in self.httpservers:
                if config['family'] == socket.AF_UNIX:
                    if self.unlink_socketfiles:
                        socketname = config['file']
                        try:
                            os.unlink(socketname)
                        except OSError:
                            pass
        except OSError:
            pass
        try:
            os.unlink(self.pidfile)
        except OSError:
            pass

    def close_httpservers(self):
        for config, server in self.httpservers:
            server.close()
            map = self.get_socket_map()
            # server._map is a reference to the asyncore socket_map
            for dispatcher in map.values():
                # For unknown reasons, sometimes an http_channel
                # dispatcher in the socket map related to servers
                # remains open *during a reload*.  If one of these
                # exists at this point, we need to close it by hand
                # (thus removing it from the asyncore.socket_map).  If
                # we don't do this, 'cleanup_fds' will cause its file
                # descriptor to be closed, but it will still remain in
                # the socket_map, and eventually its file descriptor
                # will be passed to # select(), which will bomb.  See
                # also http://www.plope.com/software/collector/253
                dispatcher_server = getattr(dispatcher, 'server', None)
                if dispatcher_server is server:
                    dispatcher.close()

    def close_logger(self):
        self.logger.close()

    def setsignals(self):
        receive = self.signal_receiver.receive
        signal.signal(signal.SIGTERM, receive)
        signal.signal(signal.SIGINT, receive)
        signal.signal(signal.SIGQUIT, receive)
        signal.signal(signal.SIGHUP, receive)
        signal.signal(signal.SIGCHLD, receive)
        signal.signal(signal.SIGUSR2, receive)

    def get_signal(self):
        return self.signal_receiver.get_signal()

    def openhttpservers(self, supervisord):
        try:
            self.httpservers = self.make_http_servers(supervisord)
        except socket.error, why:
            if why[0] == errno.EADDRINUSE:
                self.usage('Another program is already listening on '
                           'a port that one of our HTTP servers is '
                           'configured to use.  Shut this program '
                           'down first before starting supervisord.')
            else:
                help = 'Cannot open an HTTP server: socket.error reported'
                errorname = errno.errorcode.get(why[0])
                if errorname is None:
                    self.usage('%s %s' % (help, why[0]))
                else:
                    self.usage('%s errno.%s (%d)' %
                               (help, errorname, why[0]))
            self.unlink_socketfiles = False
        except ValueError, why:
            self.usage(why[0])

    def get_autochildlog_name(self, name, identifier, channel):
        prefix='%s-%s---%s-' % (name, channel, identifier)
        logfile = self.mktempfile(
            suffix='.log',
            prefix=prefix,
            dir=self.childlogdir)
        return logfile

    def clear_autochildlogdir(self):
        # must be called after realize()
        childlogdir = self.childlogdir
        fnre = re.compile(r'.+?---%s-\S+\.log\.{0,1}\d{0,4}' % self.identifier)
        try:
            filenames = os.listdir(childlogdir)
        except (IOError, OSError):
            self.logger.warn('Could not clear childlog dir')
            return

        for filename in filenames:
            if fnre.match(filename):
                pathname = os.path.join(childlogdir, filename)
                try:
                    os.remove(pathname)
                except (OSError, IOError):
                    self.logger.warn('Failed to clean up %r' % pathname)

    def get_socket_map(self):
        return asyncore.socket_map

    def cleanup_fds(self):
        # try to close any leaked file descriptors (for reload)
        start = 5
        for x in range(start, self.minfds):
            try:
                os.close(x)
            except OSError:
                pass

    def select(self, r, w, x, timeout):
        return select.select(r, w, x, timeout)

    def kill(self, pid, signal):
        os.kill(pid, signal)

    def set_uid(self):
        if self.uid is None:
            if os.getuid() == 0:
                return 'Supervisor running as root (no user in config file)'
            return None
        msg = self.dropPrivileges(self.uid)
        if msg is None:
            return 'Set uid to user %s' % self.uid
        return msg

    def dropPrivileges(self, user):
        # Drop root privileges if we have them
        if user is None:
            return "No user specified to setuid to!"
        if os.getuid() != 0:
            return "Can't drop privilege as nonroot user"
        try:
            uid = int(user)
        except ValueError:
            try:
                pwrec = pwd.getpwnam(user)
            except KeyError:
                return "Can't find username %r" % user
            uid = pwrec[2]
        else:
            try:
                pwrec = pwd.getpwuid(uid)
            except KeyError:
                return "Can't find uid %r" % uid
        gid = pwrec[3]
        if hasattr(os, 'setgroups'):
            user = pwrec[0]
            groups = [grprec[2] for grprec in grp.getgrall() if user in
                      grprec[3]]

            # always put our primary gid first in this list, otherwise we can
            # lose group info since sometimes the first group in the setgroups
            # list gets overwritten on the subsequent setgid call (at least on 
            # freebsd 9 with python 2.7 - this will be safe though for all unix
            # /python version combos)
            groups.insert(0, gid)
            try:
                os.setgroups(groups)
            except OSError:
                return 'Could not set groups of effective user'
        try:
            os.setgid(gid)
        except OSError:
            return 'Could not set group id of effective user'
        os.setuid(uid)

    def waitpid(self):
        # firstly send a signal to all resumed processes to check if they are
        # still running. resumed process is NOT spawned child process of
        # supervisord, so the os.waitpid doesn't work.
        for pid in self.resumed_pids:
            try:
                os.kill(pid, 0)
            except:
                # got an exception, we blindly consider the process has exited.
                self.resumed_pids.remove(pid)
                return pid, 0

        # need pthread_sigmask here to avoid concurrent sigchild, but
        # Python doesn't offer it as it's not standard across UNIX versions.
        # there is still a race condition here; we can get a sigchild while
        # we're sitting in the waitpid call.
        try:
            pid, sts = os.waitpid(-1, os.WNOHANG)
        except OSError, why:
            err = why[0]
            if err not in (errno.ECHILD, errno.EINTR):
                self.logger.critical(
                    'waitpid error; a process may not be cleaned up properly')
            if err == errno.EINTR:
                self.logger.blather('EINTR during reap')
            pid, sts = None, None
        return pid, sts

    def set_rlimits(self):
        limits = []
        if hasattr(resource, 'RLIMIT_NOFILE'):
            limits.append(
                {
                'msg':('The minimum number of file descriptors required '
                       'to run this process is %(min)s as per the "minfds" '
                       'command-line argument or config file setting. '
                       'The current environment will only allow you '
                       'to open %(hard)s file descriptors.  Either raise '
                       'the number of usable file descriptors in your '
                       'environment (see README.rst) or lower the '
                       'minfds setting in the config file to allow '
                       'the process to start.'),
                'min':self.minfds,
                'resource':resource.RLIMIT_NOFILE,
                'name':'RLIMIT_NOFILE',
                })
        if hasattr(resource, 'RLIMIT_NPROC'):
            limits.append(
                {
                'msg':('The minimum number of available processes required '
                       'to run this program is %(min)s as per the "minprocs" '
                       'command-line argument or config file setting. '
                       'The current environment will only allow you '
                       'to open %(hard)s processes.  Either raise '
                       'the number of usable processes in your '
                       'environment (see README.rst) or lower the '
                       'minprocs setting in the config file to allow '
                       'the program to start.'),
                'min':self.minprocs,
                'resource':resource.RLIMIT_NPROC,
                'name':'RLIMIT_NPROC',
                })

        msgs = []

        for limit in limits:

            min = limit['min']
            res = limit['resource']
            msg = limit['msg']
            name = limit['name']

            soft, hard = resource.getrlimit(res)

            if (soft < min) and (soft != -1): # -1 means unlimited
                if (hard < min) and (hard != -1):
                    # setrlimit should increase the hard limit if we are
                    # root, if not then setrlimit raises and we print usage
                    hard = min

                try:
                    resource.setrlimit(res, (min, hard))
                    msgs.append('Increased %(name)s limit to %(min)s' %
                                locals())
                except (resource.error, ValueError):
                    self.usage(msg % locals())
        return msgs

    def make_logger(self, critical_messages, warn_messages, info_messages):
        # must be called after realize() and after supervisor does setuid()
        format =  '%(asctime)s %(levelname)s %(message)s\n'
        self.logger = loggers.getLogger(
            self.logfile,
            self.loglevel,
            format,
            rotating=True,
            maxbytes=self.logfile_maxbytes,
            backups=self.logfile_backups,
            stdout = self.nodaemon,
            )
        for msg in critical_messages:
            self.logger.critical(msg)
        for msg in warn_messages:
            self.logger.warn(msg)
        for msg in info_messages:
            self.logger.info(msg)

    def make_http_servers(self, supervisord):
        from supervisor.http import make_http_servers
        return make_http_servers(self, supervisord)

    def close_fd(self, fd):
        try:
            os.close(fd)
        except OSError:
            pass

    def fork(self):
        return os.fork()

    def dup2(self, frm, to):
        return os.dup2(frm, to)

    def setpgrp(self):
        return os.setpgrp()

    def stat(self, filename):
        return os.stat(filename)

    def write(self, fd, data):
        return os.write(fd, data)

    def execve(self, filename, argv, env):
        return os.execve(filename, argv, env)

    def mktempfile(self, suffix, prefix, dir):
        # set os._urandomfd as a hack around bad file descriptor bug
        # seen in the wild, see
        # http://www.plope.com/software/collector/252
        os._urandomfd = None
        fd, filename = tempfile.mkstemp(suffix, prefix, dir)
        os.close(fd)
        return filename

    def remove(self, path):
        os.remove(path)

    def exists(self, path):
        return os.path.exists(path)

    def _exit(self, code):
        os._exit(code)

    def setumask(self, mask):
        os.umask(mask)

    def get_path(self):
        """Return a list corresponding to $PATH, or a default."""
        path = ["/bin", "/usr/bin", "/usr/local/bin"]
        if os.environ.has_key("PATH"):
            p = os.environ["PATH"]
            if p:
                path = p.split(os.pathsep)
        return path

    def get_pid(self):
        return os.getpid()

    def check_execv_args(self, filename, argv, st):
        if st is None:
            raise NotFound("can't find command %r" % filename)

        elif stat.S_ISDIR(st[stat.ST_MODE]):
            raise NotExecutable("command at %r is a directory" % filename)

        elif not (stat.S_IMODE(st[stat.ST_MODE]) & 0111):
            raise NotExecutable("command at %r is not executable" % filename)

        elif not os.access(filename, os.X_OK):
            raise NoPermission("no permission to run command %r" % filename)

    def reopenlogs(self):
        self.logger.info('supervisord logreopen')
        for handler in self.logger.handlers:
            if hasattr(handler, 'reopen'):
                handler.reopen()

    def readfd(self, fd):
        try:
            data = os.read(fd, 2 << 16) # 128K
        except OSError, why:
            if why[0] not in (errno.EWOULDBLOCK, errno.EBADF, errno.EINTR):
                raise
            data = ''
        return data

    def process_environment(self):
        os.environ.update(self.environment or {})

    def open(self, fn, mode='r'):
        return open(fn, mode)

    def chdir(self, dir):
        os.chdir(dir)

    def make_pipes(self, stderr=True):
        """ Create pipes for parent to child stdin/stdout/stderr
        communications.  Open fd in nonblocking mode so we can read them
        in the mainloop without blocking.  If stderr is False, don't
        create a pipe for stderr. """

        pipes = {'child_stdin':None,
                 'stdin':None,
                 'stdout':None,
                 'child_stdout':None,
                 'stderr':None,
                 'child_stderr':None}
        try:
            stdin, child_stdin = os.pipe()
            pipes['child_stdin'], pipes['stdin'] = stdin, child_stdin
            stdout, child_stdout = os.pipe()
            pipes['stdout'], pipes['child_stdout'] = stdout, child_stdout
            if stderr:
                stderr, child_stderr = os.pipe()
                pipes['stderr'], pipes['child_stderr'] = stderr, child_stderr
            for fd in (pipes['stdout'], pipes['stderr'], pipes['stdin']):
                if fd is not None:
                    fcntl(fd, F_SETFL, fcntl(fd, F_GETFL) | os.O_NDELAY)
            return pipes
        except OSError:
            for fd in pipes.values():
                if fd is not None:
                    self.close_fd(fd)

    def close_parent_pipes(self, pipes):
        for fdname in ('stdin', 'stdout', 'stderr'):
            fd = pipes[fdname]
            if fd is not None:
                self.close_fd(fd)

    def close_child_pipes(self, pipes):
        for fdname in ('child_stdin', 'child_stdout', 'child_stderr'):
            fd = pipes[fdname]
            if fd is not None:
                self.close_fd(fd)

class ClientOptions(Options):
    positional_args_allowed = 1

    interactive = None
    prompt = None
    serverurl = None
    username = None
    password = None
    history_file = None

    def __init__(self):
        Options.__init__(self)
        self.configroot = Dummy()
        self.configroot.supervisorctl = Dummy()
        self.configroot.supervisorctl.interactive = None
        self.configroot.supervisorctl.prompt = 'supervisor'
        self.configroot.supervisorctl.serverurl = None
        self.configroot.supervisorctl.username = None
        self.configroot.supervisorctl.password = None
        self.configroot.supervisorctl.history_file = None


        self.add("interactive", "supervisorctl.interactive", "i",
                 "interactive", flag=1, default=0)
        self.add("prompt", "supervisorctl.prompt", default="supervisor")
        self.add("serverurl", "supervisorctl.serverurl", "s:", "serverurl=",
                 url, default="http://localhost:9001")
        self.add("username", "supervisorctl.username", "u:", "username=")
        self.add("password", "supervisorctl.password", "p:", "password=")
        self.add("history", "supervisorctl.history_file", "r:", "history_file=")

    def realize(self, *arg, **kw):
        Options.realize(self, *arg, **kw)
        if not self.args:
            self.interactive = 1

    def read_config(self, fp):
        section = self.configroot.supervisorctl
        if not hasattr(fp, 'read'):
            self.here = os.path.dirname(normalize_path(fp))
            try:
                fp = open(fp, 'r')
            except (IOError, OSError):
                raise ValueError("could not find config file %s" % fp)
        config = UnhosedConfigParser()
        config.mysection = 'supervisorctl'
        config.readfp(fp)
        sections = config.sections()
        if not 'supervisorctl' in sections:
            raise ValueError,'.ini file does not include supervisorctl section'
        serverurl = config.getdefault('serverurl', 'http://localhost:9001')
        if serverurl.startswith('unix://'):
            sf = serverurl[7:]
            path = expand(sf, {'here':self.here}, 'serverurl')
            path = normalize_path(path)
            serverurl = 'unix://%s' % path
        section.serverurl = serverurl

        # The defaults used below are really set in __init__ (since
        # section==self.configroot.supervisorctl)
        section.prompt = config.getdefault('prompt', section.prompt)
        section.username = config.getdefault('username', section.username)
        section.password = config.getdefault('password', section.password)
        history_file = config.getdefault('history_file', section.history_file)

        if history_file:
            history_file = normalize_path(history_file)
            section.history_file = history_file
            self.history_file = history_file
        else:
            section.history_file = None
            self.history_file = None

        from supervisor.supervisorctl import DefaultControllerPlugin
        self.plugin_factories = self.get_plugins(
            config,
            'supervisor.ctl_factory',
            'ctlplugin:'
            )
        default_factory = ('default', DefaultControllerPlugin, {})
        # if you want to a supervisorctl without the default plugin,
        # please write your own supervisorctl.
        self.plugin_factories.insert(0, default_factory)

        return section

    def getServerProxy(self):
        # mostly put here for unit testing
        return xmlrpclib.ServerProxy(
            # dumbass ServerProxy won't allow us to pass in a non-HTTP url,
            # so we fake the url we pass into it and always use the transport's
            # 'serverurl' to figure out what to attach to
            'http://127.0.0.1',
            transport = xmlrpc.SupervisorTransport(self.username,
                                                   self.password,
                                                   self.serverurl)
            )

_marker = []

class UnhosedConfigParser(ConfigParser.RawConfigParser):
    mysection = 'supervisord'
    def read_string(self, s):
        from StringIO import StringIO
        s = StringIO(s)
        return self.readfp(s)

    def getdefault(self, option, default=_marker):
        try:
            return self.get(self.mysection, option)
        except ConfigParser.NoOptionError:
            if default is _marker:
                raise
            else:
                return default

    def saneget(self, section, option, default=_marker):
        try:
            return self.get(section, option)
        except ConfigParser.NoOptionError:
            if default is _marker:
                raise
            else:
                return default

class Config(object):
    def __ne__(self, other):
        return not self.__eq__(other)

    def __lt__(self, other):
        if self.priority == other.priority:
            return self.name < other.name

        return self.priority < other.priority

    def __le__(self, other):
        if self.priority == other.priority:
            return self.name <= other.name

        return self.priority <= other.priority

    def __gt__(self, other):
        if self.priority == other.priority:
            return self.name > other.name

        return self.priority > other.priority

    def __ge__(self, other):
        if self.priority == other.priority:
            return self.name >= other.name

        return self.priority >= other.priority

    def __repr__(self):
        return '<%s instance at %s named %s>' % (self.__class__, id(self),
                                                 self.name)

class ProcessConfig(Config):
    req_param_names = [
        'name', 'uid', 'command', 'directory', 'umask', 'priority',
        'autostart', 'autorestart', 'startsecs', 'startretries',
        'stdout_logfile', 'stdout_capture_maxbytes',
        'stdout_events_enabled',
        'stdout_logfile_backups', 'stdout_logfile_maxbytes',
        'stderr_logfile', 'stderr_capture_maxbytes',
        'stderr_logfile_backups', 'stderr_logfile_maxbytes',
        'stderr_events_enabled',
        'stopsignal', 'stopwaitsecs', 'stopasgroup', 'killasgroup',
        'exitcodes', 'redirect_stderr' ]
    optional_param_names = [ 'environment', 'serverurl', 'http_url']

    def __init__(self, options, **params):
        self.options = options
        for name in self.req_param_names:
            setattr(self, name, params[name])
        for name in self.optional_param_names:
            setattr(self, name, params.get(name, None))

    def __eq__(self, other):
        if not isinstance(other, ProcessConfig):
            return False

        for name in self.req_param_names + self.optional_param_names:
            if Automatic in [getattr(self, name), getattr(other, name)] :
                continue
            if getattr(self, name) != getattr(other, name):
                return False

        return True

    def create_autochildlogs(self):
        # temporary logfiles which are erased at start time
        get_autoname = self.options.get_autochildlog_name
        sid = self.options.identifier
        name = self.name
        if self.stdout_logfile is Automatic:
            self.stdout_logfile = get_autoname(name, sid, 'stdout')
        if self.stderr_logfile is Automatic:
            self.stderr_logfile = get_autoname(name, sid, 'stderr')

    def make_process(self, group=None):
        from supervisor.process import Subprocess
        process = Subprocess(self)
        process.group = group
        return process

    def make_dispatchers(self, proc):
        use_stderr = not self.redirect_stderr
        p = self.options.make_pipes(use_stderr)
        stdout_fd,stderr_fd,stdin_fd = p['stdout'],p['stderr'],p['stdin']
        dispatchers = {}
        from supervisor.dispatchers import POutputDispatcher
        from supervisor.dispatchers import PInputDispatcher
        from supervisor import events
        if stdout_fd is not None:
            etype = events.ProcessCommunicationStdoutEvent
            dispatchers[stdout_fd] = POutputDispatcher(proc, etype, stdout_fd)
        if stderr_fd is not None:
            etype = events.ProcessCommunicationStderrEvent
            dispatchers[stderr_fd] = POutputDispatcher(proc,etype, stderr_fd)
        if stdin_fd is not None:
            dispatchers[stdin_fd] = PInputDispatcher(proc, 'stdin', stdin_fd)
        return dispatchers, p

class EventListenerConfig(ProcessConfig):
    def make_dispatchers(self, proc):
        use_stderr = not self.redirect_stderr
        p = self.options.make_pipes(use_stderr)
        stdout_fd,stderr_fd,stdin_fd = p['stdout'],p['stderr'],p['stdin']
        dispatchers = {}
        from supervisor.dispatchers import PEventListenerDispatcher
        from supervisor.dispatchers import PInputDispatcher
        from supervisor.dispatchers import POutputDispatcher
        from supervisor import events
        if stdout_fd is not None:
            dispatchers[stdout_fd] = PEventListenerDispatcher(proc, 'stdout',
                                                              stdout_fd)
        if stderr_fd is not None:
            etype = events.ProcessCommunicationStderrEvent
            dispatchers[stderr_fd] = POutputDispatcher(proc, etype, stderr_fd)
        if stdin_fd is not None:
            dispatchers[stdin_fd] = PInputDispatcher(proc, 'stdin', stdin_fd)
        return dispatchers, p

class FastCGIProcessConfig(ProcessConfig):

    def make_process(self, group=None):
        if group is None:
            raise NotImplementedError('FastCGI programs require a group')
        from supervisor.process import FastCGISubprocess
        process = FastCGISubprocess(self)
        process.group = group
        return process

    def make_dispatchers(self, proc):
        dispatchers, p = ProcessConfig.make_dispatchers(self, proc)
        #FastCGI child processes expect the FastCGI socket set to
        #file descriptor 0, so supervisord cannot use stdin
        #to communicate with the child process
        stdin_fd = p['stdin']
        if stdin_fd is not None:
            dispatchers[stdin_fd].close()
        return dispatchers, p

class ProcessGroupConfig(Config):
    def __init__(self, options, name, priority, process_configs):
        self.options = options
        self.name = name
        self.priority = priority
        self.process_configs = process_configs

    def __eq__(self, other):
        if not isinstance(other, ProcessGroupConfig):
            return False

        if self.name != other.name:
            return False
        if self.priority != other.priority:
            return False
        if self.process_configs != other.process_configs:
            return False

        return True

    def after_setuid(self):
        for config in self.process_configs:
            config.create_autochildlogs()

    def make_group(self):
        from supervisor.process import ProcessGroup
        return ProcessGroup(self)

class EventListenerPoolConfig(Config):
    def __init__(self, options, name, priority, process_configs, buffer_size,
                 pool_events, result_handler):
        self.options = options
        self.name = name
        self.priority = priority
        self.process_configs = process_configs
        self.buffer_size = buffer_size
        self.pool_events = pool_events
        self.result_handler = result_handler

    def __eq__(self, other):
        if not isinstance(other, EventListenerPoolConfig):
            return False

        if (self.name == other.name) and (self.priority == other.priority):
            return True

        return False

    def after_setuid(self):
        for config in self.process_configs:
            config.create_autochildlogs()

    def make_group(self):
        from supervisor.process import EventListenerPool
        return EventListenerPool(self)

class FastCGIGroupConfig(ProcessGroupConfig):
    def __init__(self, options, name, priority, process_configs,
                 socket_config):
        self.options = options
        self.name = name
        self.priority = priority
        self.process_configs = process_configs
        self.socket_config = socket_config

    def __eq__(self, other):
        if not isinstance(other, FastCGIGroupConfig):
            return False

        if self.socket_config != other.socket_config:
            return False

        return ProcessGroupConfig.__eq__(self, other)

    def make_group(self):
        from supervisor.process import FastCGIProcessGroup
        return FastCGIProcessGroup(self)

def readFile(filename, offset, length):
    """ Read length bytes from the file named by filename starting at
    offset """

    absoffset = abs(offset)
    abslength = abs(length)

    try:
        f = open(filename, 'rb')
        if absoffset != offset:
            # negative offset returns offset bytes from tail of the file
            if length:
                raise ValueError('BAD_ARGUMENTS')
            f.seek(0, 2)
            sz = f.tell()
            pos = int(sz - absoffset)
            if pos < 0:
                pos = 0
            f.seek(pos)
            data = f.read(absoffset)
        else:
            if abslength != length:
                raise ValueError('BAD_ARGUMENTS')
            if length == 0:
                f.seek(offset)
                data = f.read()
            else:
                sz = f.seek(offset)
                data = f.read(length)
    except (OSError, IOError):
        raise ValueError('FAILED')

    return data

def tailFile(filename, offset, length):
    """
    Read length bytes from the file named by filename starting at
    offset, automatically increasing offset and setting overflow
    flag if log size has grown beyond (offset + length).  If length
    bytes are not available, as many bytes as are available are returned.
    """

    overflow = False
    try:
        f = open(filename, 'rb')
        f.seek(0, 2)
        sz = f.tell()

        if sz > (offset + length):
            overflow = True
            offset   = sz - 1

        if (offset + length) > sz:
            if (offset > (sz - 1)):
                length = 0
            offset = sz - length

        if offset < 0: offset = 0
        if length < 0: length = 0

        if length == 0:
            data = ''
        else:
            f.seek(offset)
            data = f.read(length)

        offset = sz
        return [data, offset, overflow]

    except (OSError, IOError):
        return ['', offset, False]

# Helpers for dealing with signals and exit status

def decode_wait_status(sts):
    """Decode the status returned by wait() or waitpid().

    Return a tuple (exitstatus, message) where exitstatus is the exit
    status, or -1 if the process was killed by a signal; and message
    is a message telling what happened.  It is the caller's
    responsibility to display the message.
    """
    if os.WIFEXITED(sts):
        es = os.WEXITSTATUS(sts) & 0xffff
        msg = "exit status %s" % es
        return es, msg
    elif os.WIFSIGNALED(sts):
        sig = os.WTERMSIG(sts)
        msg = "terminated by %s" % signame(sig)
        if hasattr(os, "WCOREDUMP"):
            iscore = os.WCOREDUMP(sts)
        else:
            iscore = sts & 0x80
        if iscore:
            msg += " (core dumped)"
        return -1, msg
    else:
        msg = "unknown termination cause 0x%04x" % sts
        return -1, msg

_signames = None

def signame(sig):
    """Return a symbolic name for a signal.

    Return "signal NNN" if there is no corresponding SIG name in the
    signal module.
    """

    if _signames is None:
        _init_signames()
    return _signames.get(sig) or "signal %d" % sig

def _init_signames():
    global _signames
    d = {}
    for k, v in signal.__dict__.items():
        k_startswith = getattr(k, "startswith", None)
        if k_startswith is None:
            continue
        if k_startswith("SIG") and not k_startswith("SIG_"):
            d[v] = k
    _signames = d

class SignalReceiver:
    def __init__(self):
        self._signals_recvd = []

    def receive(self, sig, frame):
        if sig not in self._signals_recvd:
            self._signals_recvd.append(sig)

    def get_signal(self):
        if self._signals_recvd:
            sig = self._signals_recvd.pop(0)
        else:
            sig = None
        return sig

# miscellaneous utility functions

def expand(s, expansions, name):
    try:
        return s % expansions
    except KeyError:
        raise ValueError(
            'Format string %r for %r contains names which cannot be '
            'expanded' % (s, name))
    except:
        raise ValueError(
            'Format string %r for %r is badly formatted' % (s, name)
            )

_environ_expansions = None

def environ_expansions():
    """Return dict of environment variables, suitable for use in string
    expansions.

    Every environment variable is prefixed by 'ENV_'.
    """
    global _environ_expansions

    if _environ_expansions:
        return _environ_expansions

    _environ_expansions = {}
    for key, value in os.environ.iteritems():
        _environ_expansions['ENV_%s' % key] = value

    return _environ_expansions

def make_namespec(group_name, process_name):
    # we want to refer to the process by its "short name" (a process named
    # process1 in the group process1 has a name "process1").  This is for
    # backwards compatibility
    if group_name == process_name:
        name = process_name
    else:
        name = '%s:%s' % (group_name, process_name)
    return name

def split_namespec(namespec):
    names = namespec.split(':', 1)
    if len(names) == 2:
        # group and and process name differ
        group_name, process_name = names
        if not process_name or process_name == '*':
            process_name = None
    else:
        # group name is same as process name
        group_name, process_name = namespec, namespec
    return group_name, process_name

# exceptions

class ProcessException(Exception):
    """ Specialized exceptions used when attempting to start a process """

class NotExecutable(ProcessException):
    """ Indicates that the filespec cannot be executed because its path
    resolves to a file which is not executable, or which is a directory. """

class NotFound(ProcessException):
    """ Indicates that the filespec cannot be executed because it could not
    be found """

class NoPermission(ProcessException):
    """ Indicates that the file cannot be executed because the supervisor
    process does not possess the appropriate UNIX filesystem permission
    to execute the file. """


########NEW FILE########
__FILENAME__ = pidproxy
#!/usr/bin/env python

""" An executable which proxies for a subprocess; upon a signal, it sends that
signal to the process identified by a pidfile. """

import os
import sys
import signal
import time

class PidProxy:
    pid = None
    def __init__(self, args):
        self.setsignals()
        try:
            self.pidfile, cmdargs = args[1], args[2:]
            self.command = os.path.abspath(cmdargs[0])
            self.cmdargs = cmdargs
        except (ValueError, IndexError):
            self.usage()
            sys.exit(1)

    def go(self):
        self.pid = os.spawnv(os.P_NOWAIT, self.command, self.cmdargs)
        while 1:
            time.sleep(5)
            try:
                pid, sts = os.waitpid(-1, os.WNOHANG)
            except OSError:
                pid, sts = None, None
            if pid:
                break

    def usage(self):
        print "pidproxy.py <pidfile name> <command> [<cmdarg1> ...]"

    def setsignals(self):
        signal.signal(signal.SIGTERM, self.passtochild)
        signal.signal(signal.SIGHUP, self.passtochild)
        signal.signal(signal.SIGINT, self.passtochild)
        signal.signal(signal.SIGUSR1, self.passtochild)
        signal.signal(signal.SIGUSR2, self.passtochild)
        signal.signal(signal.SIGCHLD, self.reap)

    def reap(self, sig, frame):
        # do nothing, we reap our child synchronously
        pass

    def passtochild(self, sig, frame):
        try:
            pid = int(open(self.pidfile, 'r').read().strip())
        except:
            pid = None
            print "Can't read child pidfile %s!" % self.pidfile
            return
        os.kill(pid, sig)
        if sig in [signal.SIGTERM, signal.SIGINT, signal.SIGQUIT]:
            sys.exit(0)

def main():
    pp = PidProxy(sys.argv)
    pp.go()

if __name__ == '__main__':
    main()
    
    
    

########NEW FILE########
__FILENAME__ = process
import os
import sys
import time
import errno
import shlex
import StringIO
import traceback
import signal

from supervisor.medusa import asyncore_25 as asyncore

from supervisor.states import ProcessStates
from supervisor.states import SupervisorStates
from supervisor.states import getProcessStateDescription
from supervisor.states import STOPPED_STATES

from supervisor.options import decode_wait_status
from supervisor.options import signame
from supervisor.options import ProcessException

from supervisor.dispatchers import EventListenerStates

from supervisor import events

from supervisor.datatypes import RestartUnconditionally

from supervisor.socket_manager import SocketManager

class Subprocess:

    """A class to manage a subprocess."""

    # Initial state; overridden by instance variables

    pid = 0 # Subprocess pid; 0 when not running
    config = None # ProcessConfig instance
    state = None # process state code
    listener_state = None # listener state code (if we're an event listener)
    event = None # event currently being processed (if we're an event listener)
    laststart = 0 # Last time the subprocess was started; 0 if never
    laststop = 0  # Last time the subprocess was stopped; 0 if never
    delay = 0 # If nonzero, delay starting or killing until this time
    administrative_stop = 0 # true if the process has been stopped by an admin
    system_stop = 0 # true if the process has been stopped by the system
    killing = 0 # flag determining whether we are trying to kill this proc
    backoff = 0 # backoff counter (to startretries)
    dispatchers = None # asnycore output dispatchers (keyed by fd)
    pipes = None # map of channel name to file descriptor #
    exitstatus = None # status attached to dead process by finsh()
    spawnerr = None # error message attached by spawn() if any
    group = None # ProcessGroup instance if process is in the group
    resumed = False # whether the process is created by a previous supervisord
                    # instance and is resumed by current supervisord
    
    def __init__(self, config):
        """Constructor.

        Argument is a ProcessConfig instance.
        """
        self.config = config
        self.dispatchers = {}
        self.pipes = {}
        self.state = ProcessStates.STOPPED

    def removelogs(self):
        for dispatcher in self.dispatchers.values():
            if hasattr(dispatcher, 'removelogs'):
                dispatcher.removelogs()

    def reopenlogs(self):
        for dispatcher in self.dispatchers.values():
            if hasattr(dispatcher, 'reopenlogs'):
                dispatcher.reopenlogs()

    def drain(self):
        for dispatcher in self.dispatchers.values():
            # note that we *must* call readable() for every
            # dispatcher, as it may have side effects for a given
            # dispatcher (eg. call handle_listener_state_change for
            # event listener processes)
            if dispatcher.readable():
                dispatcher.handle_read_event()
            if dispatcher.writable():
                dispatcher.handle_write_event()
                
    def write(self, chars):
        if not self.pid or self.killing:
            raise OSError(errno.EPIPE, "Process already closed")

        stdin_fd = self.pipes['stdin']
        if stdin_fd is None:
            raise OSError(errno.EPIPE, "Process has no stdin channel")

        dispatcher = self.dispatchers[stdin_fd]
        if dispatcher.closed:
            raise OSError(errno.EPIPE, "Process' stdin channel is closed")
            
        dispatcher.input_buffer += chars
        dispatcher.flush() # this must raise EPIPE if the pipe is closed

    def get_execv_args(self):
        """Internal: turn a program name into a file name, using $PATH,
        make sure it exists / is executable, raising a ProcessException
        if not """
        commandargs = shlex.split(self.config.command)

        program = commandargs[0]

        if "/" in program:
            filename = program
            try:
                st = self.config.options.stat(filename)
            except OSError:
                st = None
            
        else:
            path = self.config.options.get_path()
            found = None
            st = None
            for dir in path:
                found = os.path.join(dir, program)
                try:
                    st = self.config.options.stat(found)
                except OSError:
                    pass
                else:
                    break
            if st is None:
                filename = program
            else:
                filename = found

        # check_execv_args will raise a ProcessException if the execv
        # args are bogus, we break it out into a separate options
        # method call here only to service unit tests
        self.config.options.check_execv_args(filename, commandargs, st)

        return filename, commandargs

    event_map = {
        ProcessStates.BACKOFF: events.ProcessStateBackoffEvent,
        ProcessStates.FATAL:   events.ProcessStateFatalEvent,
        ProcessStates.UNKNOWN: events.ProcessStateUnknownEvent,
        ProcessStates.STOPPED: events.ProcessStateStoppedEvent,
        ProcessStates.EXITED:  events.ProcessStateExitedEvent,
        ProcessStates.RUNNING: events.ProcessStateRunningEvent,
        ProcessStates.STARTING: events.ProcessStateStartingEvent,
        ProcessStates.STOPPING: events.ProcessStateStoppingEvent,
        }

    def change_state(self, new_state, expected=True):
        old_state = self.state
        if new_state is old_state:
            # exists for unit tests
            return False

        event_class = self.event_map.get(new_state)
        if event_class is not None:
            event = event_class(self, old_state, expected)
            events.notify(event)

        if new_state == ProcessStates.BACKOFF:
            now = time.time()
            self.backoff = self.backoff + 1
            self.delay = now + self.backoff

        self.state = new_state

    def _assertInState(self, *states):
        if self.state not in states:
            current_state = getProcessStateDescription(self.state)
            allowable_states = ' '.join(map(getProcessStateDescription, states))
            raise AssertionError('Assertion failed for %s: %s not in %s' %  (
                self.config.name, current_state, allowable_states))

    def record_spawnerr(self, msg):
        self.spawnerr = msg
        self.config.options.logger.info("spawnerr: %s" % msg)

    def spawn(self):
        """Start the subprocess.  It must not be running already.

        Return the process id.  If the fork() call fails, return None.
        """
        options = self.config.options

        if self.pid:
            msg = 'process %r already running' % self.config.name
            options.logger.warn(msg)
            return

        self.killing = 0
        self.spawnerr = None
        self.exitstatus = None
        self.system_stop = 0
        self.administrative_stop = 0
        
        self.laststart = time.time()

        self._assertInState(ProcessStates.EXITED, ProcessStates.FATAL,
                            ProcessStates.BACKOFF, ProcessStates.STOPPED)

        self.change_state(ProcessStates.STARTING)

        try:
            filename, argv = self.get_execv_args()
        except ProcessException, what:
            self.record_spawnerr(what.args[0])
            self._assertInState(ProcessStates.STARTING)
            self.change_state(ProcessStates.BACKOFF)
            return

        try:
            self.dispatchers, self.pipes = self.config.make_dispatchers(self)
        except OSError, why:
            code = why[0]
            if code == errno.EMFILE:
                # too many file descriptors open
                msg = 'too many open files to spawn %r' % self.config.name
            else:
                msg = 'unknown error: %s' % errno.errorcode.get(code, code)
            self.record_spawnerr(msg)
            self._assertInState(ProcessStates.STARTING)
            self.change_state(ProcessStates.BACKOFF)
            return
        
        try:
            pid = options.fork()
        except OSError, why:
            code = why[0]
            if code == errno.EAGAIN:
                # process table full
                msg  = ('Too many processes in process table to spawn %r' %
                        self.config.name)
            else:
                msg = 'unknown error: %s' % errno.errorcode.get(code, code)

            self.record_spawnerr(msg)
            self._assertInState(ProcessStates.STARTING)
            self.change_state(ProcessStates.BACKOFF)
            options.close_parent_pipes(self.pipes)
            options.close_child_pipes(self.pipes)
            return

        if pid != 0:
            return self._spawn_as_parent(pid)
        
        else:
            return self._spawn_as_child(filename, argv)

    def _spawn_as_parent(self, pid):
        # Parent
        self.pid = pid
        options = self.config.options
        options.close_child_pipes(self.pipes)
        options.logger.info('spawned: %r with pid %s' % (self.config.name, pid))
        self.spawnerr = None
        self.delay = time.time() + self.config.startsecs
        self.resumed = False
        options.add_process(self)
        return pid

    def _prepare_child_fds(self):
        options = self.config.options
        options.dup2(self.pipes['child_stdin'], 0)
        options.dup2(self.pipes['child_stdout'], 1)
        if self.config.redirect_stderr:
            options.dup2(self.pipes['child_stdout'], 2)
        else:
            options.dup2(self.pipes['child_stderr'], 2)
        for i in range(3, options.minfds):
            options.close_fd(i)        

    def _spawn_as_child(self, filename, argv):
        options = self.config.options
        try:
            # prevent child from receiving signals sent to the
            # parent by calling os.setpgrp to create a new process
            # group for the child; this prevents, for instance,
            # the case of child processes being sent a SIGINT when
            # running supervisor in foreground mode and Ctrl-C in
            # the terminal window running supervisord is pressed.
            # Presumably it also prevents HUP, etc received by
            # supervisord from being sent to children.
            options.setpgrp()
            self._prepare_child_fds()
            # sending to fd 2 will put this output in the stderr log
            msg = self.set_uid()
            if msg:
                uid = self.config.uid
                s = 'supervisor: error trying to setuid to %s ' % uid
                options.write(2, s)
                options.write(2, "(%s)\n" % msg)
            env = os.environ.copy()
            env['SUPERVISOR_ENABLED'] = '1'
            serverurl = self.config.serverurl
            if serverurl is None: # unset
                serverurl = self.config.options.serverurl # might still be None
            if serverurl:
                env['SUPERVISOR_SERVER_URL'] = serverurl
            env['SUPERVISOR_PROCESS_NAME'] = self.config.name
            if self.group:
                env['SUPERVISOR_GROUP_NAME'] = self.group.config.name
            if self.config.environment is not None:
                env.update(self.config.environment)
            try:
                cwd = self.config.directory
                if cwd is not None:
                    options.chdir(cwd)
            except OSError, why:
                code = errno.errorcode.get(why[0], why[0])
                msg = "couldn't chdir to %s: %s\n" % (cwd, code)
                options.write(2, msg)
            else:
                try:
                    if self.config.umask is not None:
                        options.setumask(self.config.umask)
                    options.execve(filename, argv, env)
                except OSError, why:
                    code = errno.errorcode.get(why[0], why[0])
                    msg = "couldn't exec %s: %s\n" % (argv[0], code)
                    options.write(2, msg)
                except:
                    (file, fun, line), t,v,tbinfo = asyncore.compact_traceback()
                    error = '%s, %s: file: %s line: %s' % (t, v, file, line)
                    options.write(2, "couldn't exec %s: %s\n" % (filename,
                                                                 error))
        finally:
            options._exit(127)

    def stop(self):
        """ Administrative stop """
        self.administrative_stop = 1
        return self.kill(self.config.stopsignal)

    def give_up(self):
        self.delay = 0
        self.backoff = 0
        self.system_stop = 1
        self._assertInState(ProcessStates.BACKOFF)
        self.change_state(ProcessStates.FATAL)

    def kill(self, sig):
        """Send a signal to the subprocess.  This may or may not kill it.

        Return None if the signal was sent, or an error message string
        if an error occurred or if the subprocess is not running.
        """
        now = time.time()
        options = self.config.options
        if not self.pid:
            msg = ("attempted to kill %s with sig %s but it wasn't running" %
                   (self.config.name, signame(sig)))
            options.logger.debug(msg)
            return msg

        #If we're in the stopping state, then we've already sent the stop
        #signal and this is the kill signal
        if self.state == ProcessStates.STOPPING:
            killasgroup = self.config.killasgroup
        else:
            killasgroup = self.config.stopasgroup

        as_group = ""
        if killasgroup:
            as_group = "process group "

        options.logger.debug('killing %s (pid %s) %swith signal %s'
                             % (self.config.name,
                                self.pid,
                                as_group,
                                signame(sig))
                             )

        # RUNNING/STARTING/STOPPING -> STOPPING
        self.killing = 1
        self.delay = now + self.config.stopwaitsecs
        # we will already be in the STOPPING state if we're doing a
        # SIGKILL as a result of overrunning stopwaitsecs
        self._assertInState(ProcessStates.RUNNING,ProcessStates.STARTING,
                            ProcessStates.STOPPING)
        self.change_state(ProcessStates.STOPPING)

        pid = self.pid
        if killasgroup:
            # send to the whole process group instead
            pid = -self.pid

        try:
            options.kill(pid, sig)
        except:
            io = StringIO.StringIO()
            traceback.print_exc(file=io)
            tb = io.getvalue()
            msg = 'unknown problem killing %s (%s):%s' % (self.config.name,
                                                          self.pid, tb)
            options.logger.critical(msg)
            self.change_state(ProcessStates.UNKNOWN)
            self.pid = 0
            self.killing = 0
            self.delay = 0
            return msg
            
        return None

    def finish(self, pid, sts):
        """ The process was reaped and we need to report and manage its state
        """
        self.drain()

        es, msg = decode_wait_status(sts)

        now = time.time()
        self.laststop = now
        processname = self.config.name

        tooquickly = now - self.laststart < self.config.startsecs
        exit_expected = es in self.config.exitcodes

        if self.killing:
            # likely the result of a stop request
            # implies STOPPING -> STOPPED
            self.killing = 0
            self.delay = 0
            self.exitstatus = es

            msg = "stopped: %s (%s)" % (processname, msg)
            self._assertInState(ProcessStates.STOPPING)
            self.change_state(ProcessStates.STOPPED)

        elif tooquickly:
            # the program did not stay up long enough to make it to RUNNING
            # implies STARTING -> BACKOFF
            self.exitstatus = None
            self.spawnerr = 'Exited too quickly (process log may have details)'
            msg = "exited: %s (%s)" % (processname, msg + "; not expected")
            self._assertInState(ProcessStates.STARTING)
            self.change_state(ProcessStates.BACKOFF)

        else:
            # this finish was not the result of a stop request, the
            # program was in the RUNNING state but exited implies
            # RUNNING -> EXITED
            self.delay = 0
            self.backoff = 0
            self.exitstatus = es

            if self.state == ProcessStates.STARTING:
                # XXX I dont know under which circumstances this
                # happens, but in the wild, there is a transition that
                # subverts the RUNNING state (directly from STARTING
                # to EXITED), so we perform the correct transition
                # here.
                self.change_state(ProcessStates.RUNNING)

            self._assertInState(ProcessStates.RUNNING)

            if exit_expected:
                # expected exit code
                msg = "exited: %s (%s)" % (processname, msg + "; expected")
                self.change_state(ProcessStates.EXITED, expected=True)
            else:
                # unexpected exit code
                self.spawnerr = 'Bad exit code %s' % es
                msg = "exited: %s (%s)" % (processname, msg + "; not expected")
                self.change_state(ProcessStates.EXITED, expected=False)

        self.config.options.logger.info(msg)

        self.pid = 0
        if not self.resumed:
            self.config.options.close_parent_pipes(self.pipes)
        self.pipes = {}
        self.dispatchers = {}

        # if we died before we processed the current event (only happens
        # if we're an event listener), notify the event system that this
        # event was rejected so it can be processed again.
        if self.event is not None:
            # Note: this should only be true if we were in the BUSY
            # state when finish() was called.
            events.notify(events.EventRejectedEvent(self, self.event))
            self.event = None

    def set_uid(self):
        if self.config.uid is None:
            return
        msg = self.config.options.dropPrivileges(self.config.uid)
        return msg

    def __cmp__(self, other):
        # sort by priority
        return cmp(self.config.priority, other.config.priority)

    def __repr__(self):
        return '<Subprocess at %s with name %s in state %s>' % (
            id(self),
            self.config.name,
            getProcessStateDescription(self.get_state()))

    def get_state(self):
        return self.state

    def transition(self):
        now = time.time()
        state = self.state

        logger = self.config.options.logger

        if self.config.options.mood > SupervisorStates.RESTARTING:
            # dont start any processes if supervisor is shutting down
            if state == ProcessStates.EXITED:
                if self.config.autorestart:
                    if self.config.autorestart is RestartUnconditionally:
                        # EXITED -> STARTING
                        self.spawn()
                    else: # autorestart is RestartWhenExitUnexpected
                        if self.exitstatus not in self.config.exitcodes:
                            # EXITED -> STARTING
                            self.spawn()
            elif state == ProcessStates.STOPPED and not self.laststart:
                if self.config.autostart:
                    # STOPPED -> STARTING
                    self.spawn()
            elif state == ProcessStates.BACKOFF:
                if self.backoff <= self.config.startretries:
                    if now > self.delay:
                        # BACKOFF -> STARTING
                        self.spawn()

        if state == ProcessStates.STARTING:
            if now - self.laststart > self.config.startsecs:
                # STARTING -> RUNNING if the proc has started
                # successfully and it has stayed up for at least
                # proc.config.startsecs,
                self.delay = 0
                self.backoff = 0
                self._assertInState(ProcessStates.STARTING)
                self.change_state(ProcessStates.RUNNING)
                msg = (
                    'entered RUNNING state, process has stayed up for '
                    '> than %s seconds (startsecs)' % self.config.startsecs)
                logger.info('success: %s %s' % (self.config.name, msg))

        if state == ProcessStates.BACKOFF:
            if self.backoff > self.config.startretries:
                # BACKOFF -> FATAL if the proc has exceeded its number
                # of retries
                self.give_up()
                msg = ('entered FATAL state, too many start retries too '
                       'quickly')
                logger.info('gave up: %s %s' % (self.config.name, msg))

        elif state == ProcessStates.STOPPING:
            time_left = self.delay - now
            if time_left <= 0:
                # kill processes which are taking too long to stop with a final
                # sigkill.  if this doesn't kill it, the process will be stuck
                # in the STOPPING state forever.
                self.config.options.logger.warn(
                    'killing %r (%s) with SIGKILL' % (self.config.name,
                                                      self.pid))
                self.kill(signal.SIGKILL)

class FastCGISubprocess(Subprocess):
    """Extends Subprocess class to handle FastCGI subprocesses"""

    def __init__(self, config):
        Subprocess.__init__(self, config)
        self.fcgi_sock = None

    def before_spawn(self):
        """
        The FastCGI socket needs to be created by the parent before we fork
        """
        if self.group is None:
            raise NotImplementedError('No group set for FastCGISubprocess')
        if not hasattr(self.group, 'socket_manager'):
            raise NotImplementedError('No SocketManager set for '
                                      '%s:%s' % (self.group, dir(self.group)))
        self.fcgi_sock = self.group.socket_manager.get_socket()

    def spawn(self):
        """
        Overrides Subprocess.spawn() so we can hook in before it happens
        """
        self.before_spawn()
        pid = Subprocess.spawn(self)
        if pid is None:
            #Remove object reference to decrement the reference count on error
            self.fcgi_sock = None
        return pid
        
    def after_finish(self):
        """
        Releases reference to FastCGI socket when process is reaped
        """
        #Remove object reference to decrement the reference count
        self.fcgi_sock = None
        
    def finish(self, pid, sts):
        """
        Overrides Subprocess.finish() so we can hook in after it happens
        """
        retval = Subprocess.finish(self, pid, sts)
        self.after_finish()
        return retval

    def _prepare_child_fds(self):
        """
        Overrides Subprocess._prepare_child_fds()
        The FastCGI socket needs to be set to file descriptor 0 in the child
        """
        sock_fd = self.fcgi_sock.fileno()
        
        options = self.config.options
        options.dup2(sock_fd, 0)
        options.dup2(self.pipes['child_stdout'], 1)
        if self.config.redirect_stderr:
            options.dup2(self.pipes['child_stdout'], 2)
        else:
            options.dup2(self.pipes['child_stderr'], 2)
        for i in range(3, options.minfds):
            options.close_fd(i)
                
class ProcessGroupBase:
    def __init__(self, config):
        self.config = config
        self.processes = {}
        for pconfig in self.config.process_configs:
            self.processes[pconfig.name] = pconfig.make_process(self)
        

    def __cmp__(self, other):
        return cmp(self.config.priority, other.config.priority)

    def __repr__(self):
        return '<%s instance at %s named %s>' % (self.__class__, id(self),
                                                 self.config.name)

    def removelogs(self):
        for process in self.processes.values():
            process.removelogs()

    def reopenlogs(self):
        for process in self.processes.values():
            process.reopenlogs()

    def stop_all(self):
        processes = self.processes.values()
        processes.sort()
        processes.reverse() # stop in desc priority order

        for proc in processes:
            state = proc.get_state()
            if state == ProcessStates.RUNNING:
                # RUNNING -> STOPPING
                proc.stop()
            elif state == ProcessStates.STARTING:
                # STARTING -> STOPPING
                proc.stop()
            elif state == ProcessStates.BACKOFF:
                # BACKOFF -> FATAL
                proc.give_up()

    def get_unstopped_processes(self):
        """ Processes which aren't in a state that is considered 'stopped' """
        return [ x for x in self.processes.values() if x.get_state() not in
                 STOPPED_STATES ]

    def get_dispatchers(self):
        dispatchers = {}
        for process in self.processes.values():
            dispatchers.update(process.dispatchers)
        return dispatchers

class ProcessGroup(ProcessGroupBase):
    def transition(self):
        for proc in self.processes.values():
            proc.transition()
            
class FastCGIProcessGroup(ProcessGroup):

    def __init__(self, config, **kwargs):
        ProcessGroup.__init__(self, config)
        sockManagerKlass = kwargs.get('socketManager', SocketManager)
        self.socket_manager = sockManagerKlass(config.socket_config, 
                                               logger=config.options.logger)
        #It's not required to call get_socket() here but we want
        #to fail early during start up if there is a config error
        try:
            sock = self.socket_manager.get_socket()
        except Exception, e:
            raise ValueError('Could not create FastCGI socket %s: %s' % (self.socket_manager.config(), e))

class EventListenerPool(ProcessGroupBase):
    def __init__(self, config):
        ProcessGroupBase.__init__(self, config)
        self.event_buffer = []
        for event_type in self.config.pool_events:
            events.subscribe(event_type, self._acceptEvent)
        events.subscribe(events.EventRejectedEvent, self.handle_rejected)
        self.serial = -1
        self.last_dispatch = 0
        self.dispatch_throttle = 0 # in seconds: .00195 is an interesting one

    def handle_rejected(self, event):
        process = event.process
        procs = self.processes.values()
        if process in procs: # this is one of our processes
            # rebuffer the event
            self._acceptEvent(event.event, head=True)

    def transition(self):
        processes = self.processes.values()
        dispatch_capable = False
        for process in processes:
            process.transition()
            # this is redundant, we do it in _dispatchEvent too, but we
            # want to reduce function call overhead
            if process.state == ProcessStates.RUNNING:
                if process.listener_state == EventListenerStates.READY:
                    dispatch_capable = True
        if dispatch_capable:
            if self.dispatch_throttle:
                now = time.time()
                if now - self.last_dispatch < self.dispatch_throttle:
                    return
            self.dispatch()

    def dispatch(self):
        while self.event_buffer:
            # dispatch the oldest event
            event = self.event_buffer.pop(0)
            ok = self._dispatchEvent(event)
            if not ok:
                # if we can't dispatch an event, rebuffer it and stop trying
                # to process any further events in the buffer
                self._acceptEvent(event, head=True)
                break
        self.last_dispatch = time.time()

    def _acceptEvent(self, event, head=False):
        # events are required to be instances
        event_type = event.__class__
        if not hasattr(event, 'serial'):
            event.serial = new_serial(GlobalSerial)
        if not hasattr(event, 'pool_serials'):
            event.pool_serials = {}
        if not event.pool_serials.has_key(self.config.name):
            event.pool_serials[self.config.name] = new_serial(self)
        else:
            self.config.options.logger.debug(
                'rebuffering event %s for pool %s (bufsize %s)' % (
                (event.serial, self.config.name, len(self.event_buffer))))

        if len(self.event_buffer) >= self.config.buffer_size:
            if self.event_buffer:
                # discard the oldest event
                discarded_event = self.event_buffer.pop(0)
                self.config.options.logger.error(
                    'pool %s event buffer overflowed, discarding event %s' % (
                    (self.config.name, discarded_event.serial)))
        if head:
            self.event_buffer.insert(0, event)
        else:
            self.event_buffer.append(event)

    def _dispatchEvent(self, event):
        pool_serial = event.pool_serials[self.config.name]
            
        for process in self.processes.values():
            if process.state != ProcessStates.RUNNING:
                continue
            if process.listener_state == EventListenerStates.READY:
                payload = str(event)
                try:
                    event_type = event.__class__
                    serial = event.serial
                    envelope = self._eventEnvelope(event_type, serial,
                                                   pool_serial, payload)
                    process.write(envelope)
                except OSError, why:
                    if why[0] != errno.EPIPE:
                        raise
                    continue
                
                process.listener_state = EventListenerStates.BUSY
                process.event = event
                self.config.options.logger.debug(
                    'event %s sent to listener %s' % (
                    event.serial, process.config.name))
                return True

        return False

    def _eventEnvelope(self, event_type, serial, pool_serial, payload):
        event_name = events.getEventNameByType(event_type)
        payload_len = len(payload)
        D = {
            'ver':'3.0',
            'sid':self.config.options.identifier,
            'serial':serial,
            'pool_name':self.config.name,
            'pool_serial':pool_serial,
            'event_name':event_name,
            'len':payload_len,
            'payload':payload,
             }
        return ('ver:%(ver)s server:%(sid)s serial:%(serial)s '
                'pool:%(pool_name)s poolserial:%(pool_serial)s '
                'eventname:%(event_name)s len:%(len)s\n%(payload)s' % D)

class GlobalSerial:
    def __init__(self):
        self.serial = -1

GlobalSerial = GlobalSerial() # singleton

def new_serial(inst):
    if inst.serial == sys.maxint:
        inst.serial = -1
    inst.serial += 1
    return inst.serial

            
    

########NEW FILE########
__FILENAME__ = rpcinterface
import os
import time
import datetime
import errno

from supervisor.options import readFile
from supervisor.options import tailFile
from supervisor.options import NotExecutable
from supervisor.options import NotFound
from supervisor.options import NoPermission
from supervisor.options import make_namespec
from supervisor.options import split_namespec
from supervisor.options import VERSION

from supervisor.events import notify
from supervisor.events import RemoteCommunicationEvent

from supervisor.http import NOT_DONE_YET
from supervisor.xmlrpc import Faults
from supervisor.xmlrpc import RPCError

from supervisor.states import SupervisorStates
from supervisor.states import getSupervisorStateDescription
from supervisor.states import ProcessStates
from supervisor.states import getProcessStateDescription
from supervisor.states import RUNNING_STATES

API_VERSION  = '3.0'

class SupervisorNamespaceRPCInterface:
    def __init__(self, supervisord):
        self.supervisord = supervisord

    def _update(self, text):
        self.update_text = text # for unit tests, mainly
        if self.supervisord.options.mood < SupervisorStates.RUNNING:
            raise RPCError(Faults.SHUTDOWN_STATE)

    # RPC API methods

    def getAPIVersion(self):
        """ Return the version of the RPC API used by supervisord

        @return string version version id
        """
        self._update('getAPIVersion')
        return API_VERSION

    getVersion = getAPIVersion # b/w compatibility with releases before 3.0

    def getSupervisorVersion(self):
        """ Return the version of the supervisor package in use by supervisord

        @return string version version id
        """
        self._update('getSupervisorVersion')
        return VERSION

    def getIdentification(self):
        """ Return identifiying string of supervisord

        @return string identifier identifying string
        """
        self._update('getIdentification')
        return self.supervisord.options.identifier

    def getState(self):
        """ Return current state of supervisord as a struct

        @return struct A struct with keys string statecode, int statename
        """
        self._update('getState')

        state = self.supervisord.options.mood
        statename = getSupervisorStateDescription(state)
        data =  {
            'statecode':state,
            'statename':statename,
            }
        return data

    def getPID(self):
        """ Return the PID of supervisord

        @return int PID
        """
        self._update('getPID')
        return self.supervisord.options.get_pid()

    def readLog(self, offset, length):
        """ Read length bytes from the main log starting at offset

        @param int offset         offset to start reading from.
        @param int length         number of bytes to read from the log.
        @return string result     Bytes of log
        """
        self._update('readLog')

        logfile = self.supervisord.options.logfile

        if logfile is None or not os.path.exists(logfile):
            raise RPCError(Faults.NO_FILE, logfile)

        try:
            return readFile(logfile, int(offset), int(length))
        except ValueError, inst:
            why = inst.args[0]
            raise RPCError(getattr(Faults, why))

    readMainLog = readLog # b/w compatibility with releases before 2.1

    def clearLog(self):
        """ Clear the main log.

        @return boolean result always returns True unless error
        """
        self._update('clearLog')

        logfile = self.supervisord.options.logfile
        if  logfile is None or not self.supervisord.options.exists(logfile):
            raise RPCError(Faults.NO_FILE)

        # there is a race condition here, but ignore it.
        try:
            self.supervisord.options.remove(logfile)
        except (OSError, IOError):
            raise RPCError(Faults.FAILED)

        for handler in self.supervisord.options.logger.handlers:
            if hasattr(handler, 'reopen'):
                self.supervisord.options.logger.info('reopening log file')
                handler.reopen()
        return True

    def shutdown(self):
        """ Shut down the supervisor process

        @return boolean result always returns True unless error
        """
        self._update('shutdown')
        self.supervisord.options.mood = SupervisorStates.SHUTDOWN
        return True

    def restart(self):
        """ Restart the supervisor process

        @return boolean result  always return True unless error
        """
        self._update('restart')

        self.supervisord.options.mood = SupervisorStates.RESTARTING
        return True

    def reloadConfig(self):
        """
        Reload configuration

        @return boolean result  always return True unless error
        """
        self._update('reloadConfig')
        try:
            self.supervisord.options.process_config_file(do_usage=False)
        except ValueError, msg:
            raise RPCError(Faults.CANT_REREAD, msg)

        added, changed, removed = self.supervisord.diff_to_active()

        added = [group.name for group in added]
        changed = [group.name for group in changed]
        removed = [group.name for group in removed]
        return [[added, changed, removed]] # cannot return len > 1, apparently

    def addProcessGroup(self, name):
        """ Update the config for a running process from config file.

        @param string name         name of process group to add
        @return boolean result     true if successful
        """
        self._update('addProcessGroup')

        for config in self.supervisord.options.process_group_configs:
            if config.name == name:
                result = self.supervisord.add_process_group(config)
                if not result:
                    raise RPCError(Faults.ALREADY_ADDED, name)
                return True
        raise RPCError(Faults.BAD_NAME, name)

    def removeProcessGroup(self, name):
        """ Remove a stopped process from the active configuration.

        @param string name         name of process group to remove
        @return boolean result     Indicates wether the removal was successful
        """
        self._update('removeProcessGroup')
        if name not in self.supervisord.process_groups:
            raise RPCError(Faults.BAD_NAME, name)

        result = self.supervisord.remove_process_group(name)
        if not result:
            raise RPCError(Faults.STILL_RUNNING)
        return True

    def _getAllProcesses(self, lexical=False):
        # if lexical is true, return processes sorted in lexical order,
        # otherwise, sort in priority order
        all_processes = []

        if lexical:
            group_names = self.supervisord.process_groups.keys()
            group_names.sort()
            for group_name in group_names:
                group = self.supervisord.process_groups[group_name]
                process_names = group.processes.keys()
                process_names.sort()
                for process_name in process_names:
                    process = group.processes[process_name]
                    all_processes.append((group, process))
        else:
            groups = self.supervisord.process_groups.values()
            groups.sort() # asc by priority

            for group in groups:
                processes = group.processes.values()
                processes.sort() # asc by priority
                for process in processes:
                    all_processes.append((group, process))

        return all_processes

    def _getGroupAndProcess(self, name):
        # get process to start from name
        group_name, process_name = split_namespec(name)

        group = self.supervisord.process_groups.get(group_name)
        if group is None:
            raise RPCError(Faults.BAD_NAME, name)

        if process_name is None:
            return group, None

        process = group.processes.get(process_name)
        if process is None:
            raise RPCError(Faults.BAD_NAME, name)

        return group, process

    def startProcess(self, name, wait=True):
        """ Start a process

        @param string name Process name (or 'group:name', or 'group:*')
        @param boolean wait Wait for process to be fully started
        @return boolean result     Always true unless error

        """
        self._update('startProcess')
        group, process = self._getGroupAndProcess(name)
        if process is None:
            group_name, process_name = split_namespec(name)
            return self.startProcessGroup(group_name, wait)

        # test filespec, don't bother trying to spawn if we know it will
        # eventually fail
        try:
            filename, argv = process.get_execv_args()
        except NotFound, why:
            raise RPCError(Faults.NO_FILE, why.args[0])
        except (NotExecutable, NoPermission), why:
            raise RPCError(Faults.NOT_EXECUTABLE, why.args[0])

        started = []

        startsecs = process.config.startsecs

        def startit():
            if not started:

                if process.get_state() in RUNNING_STATES:
                    raise RPCError(Faults.ALREADY_STARTED, name)

                process.spawn()

                if process.spawnerr:
                    raise RPCError(Faults.SPAWN_ERROR, name)

                # we use a list here to fake out lexical scoping;
                # using a direct assignment to 'started' in the
                # function appears to not work (symptom: 2nd or 3rd
                # call through, it forgets about 'started', claiming
                # it's undeclared).
                started.append(time.time())

            if not wait or not startsecs:
                return True

            t = time.time()
            runtime = (t - started[0])
            state = process.get_state()

            if state not in (ProcessStates.STARTING, ProcessStates.RUNNING):
                raise RPCError(Faults.ABNORMAL_TERMINATION, name)

            if runtime < startsecs:
                return NOT_DONE_YET

            if state == ProcessStates.RUNNING:
                return True

            raise RPCError(Faults.ABNORMAL_TERMINATION, name)

        startit.delay = 0.05
        startit.rpcinterface = self
        return startit # deferred

    def startProcessGroup(self, name, wait=True):
        """ Start all processes in the group named 'name'

        @param string name        The group name
        @param boolean wait       Wait for each process to be fully started
        @return struct result     A structure containing start statuses
        """
        self._update('startProcessGroup')

        group = self.supervisord.process_groups.get(name)

        if group is None:
            raise RPCError(Faults.BAD_NAME, name)

        processes = group.processes.values()
        processes.sort()
        processes = [ (group, process) for process in processes ]

        startall = make_allfunc(processes, isNotRunning, self.startProcess,
                                wait=wait)

        startall.delay = 0.05
        startall.rpcinterface = self
        return startall # deferred

    def startAllProcesses(self, wait=True):
        """ Start all processes listed in the configuration file

        @param boolean wait Wait for each process to be fully started
        @return struct result     A structure containing start statuses
        """
        self._update('startAllProcesses')

        processes = self._getAllProcesses()
        startall = make_allfunc(processes, isNotRunning, self.startProcess,
                                wait=wait)

        startall.delay = 0.05
        startall.rpcinterface = self
        return startall # deferred

    def stopProcess(self, name, wait=True):
        """ Stop a process named by name

        @param string name  The name of the process to stop (or 'group:name')
        @param boolean wait        Wait for the process to be fully stopped
        @return boolean result     Always return True unless error
        """
        self._update('stopProcess')

        group, process = self._getGroupAndProcess(name)

        if process is None:
            group_name, process_name = split_namespec(name)
            return self.stopProcessGroup(group_name, wait)

        stopped = []
        called  = []

        def killit():
            if not called:
                if process.get_state() not in RUNNING_STATES:
                    raise RPCError(Faults.NOT_RUNNING)
                # use a mutable for lexical scoping; see startProcess
                called.append(1)

            if not stopped:
                msg = process.stop()
                if msg is not None:
                    raise RPCError(Faults.FAILED, msg)
                stopped.append(1)

                if wait:
                    return NOT_DONE_YET
                else:
                    return True

            if process.get_state() not in (ProcessStates.STOPPED,
                                           ProcessStates.EXITED):
                return NOT_DONE_YET
            else:
                return True

        killit.delay = 0.2
        killit.rpcinterface = self
        return killit # deferred

    def stopProcessGroup(self, name, wait=True):
        """ Stop all processes in the process group named 'name'

        @param string name  The group name
        @param boolean wait    Wait for each process to be fully stopped
        @return boolean result Always return true unless error.
        """
        self._update('stopProcessGroup')

        group = self.supervisord.process_groups.get(name)

        if group is None:
            raise RPCError(Faults.BAD_NAME, name)

        processes = group.processes.values()
        processes.sort()
        processes = [ (group, process) for process in processes ]

        killall = make_allfunc(processes, isRunning, self.stopProcess,
                               wait=wait)

        killall.delay = 0.05
        killall.rpcinterface = self
        return killall # deferred

    def stopAllProcesses(self, wait=True):
        """ Stop all processes in the process list

        @param boolean wait    Wait for each process to be fully stopped
        @return boolean result Always return true unless error.
        """
        self._update('stopAllProcesses')

        processes = self._getAllProcesses()

        killall = make_allfunc(processes, isRunning, self.stopProcess,
                               wait=wait)

        killall.delay = 0.05
        killall.rpcinterface = self
        return killall # deferred

    def getAllConfigInfo(self):
        """ Get info about all availible process configurations. Each record
        represents a single process (i.e. groups get flattened).

        @return array result  An array of process config info records
        """
        self._update('getAllConfigInfo')

        configinfo = []
        for gconfig in self.supervisord.options.process_group_configs:
            inuse = gconfig.name in self.supervisord.process_groups
            for pconfig in gconfig.process_configs:
                configinfo.append(
                    { 'name': pconfig.name,
                      'group': gconfig.name,
                      'inuse': inuse,
                      'autostart': pconfig.autostart,
                      'group_prio': gconfig.priority,
                      'process_prio': pconfig.priority })

        configinfo.sort()
        return configinfo

    def _interpretProcessInfo(self, info):
        state = info['state']

        if state == ProcessStates.RUNNING:
            start = info['start']
            now = info['now']
            start_dt = datetime.datetime(*time.gmtime(start)[:6])
            now_dt = datetime.datetime(*time.gmtime(now)[:6])
            uptime = now_dt - start_dt
            desc = 'pid %s, uptime %s' % (info['pid'], uptime)
            if info['resumed']:
                desc += ' [resumed]'

        elif state in (ProcessStates.FATAL, ProcessStates.BACKOFF):
            desc = info['spawnerr']
            if not desc:
                desc = 'unknown error (try "tail %s")' % info['name']

        elif state in (ProcessStates.STOPPED, ProcessStates.EXITED):
            if info['start']:
                stop = info['stop']
                stop_dt = datetime.datetime(*time.localtime(stop)[:7])
                desc = stop_dt.strftime('%b %d %I:%M %p')
            else:
                desc = 'Not started'

        else:
            desc = ''

        return desc

    def getProcessInfo(self, name):
        """ Get info about a process named name

        @param string name The name of the process (or 'group:name')
        @return struct result     A structure containing data about the process
        """
        self._update('getProcessInfo')

        group, process = self._getGroupAndProcess(name)

        start = int(process.laststart)
        stop = int(process.laststop)
        now = int(time.time())

        state = process.get_state()
        spawnerr = process.spawnerr or ''
        exitstatus = process.exitstatus or 0
        stdout_logfile = process.config.stdout_logfile or ''
        stderr_logfile = process.config.stderr_logfile or ''

        info = {
            'name':process.config.name,
            'group':group.config.name,
            'start':start,
            'stop':stop,
            'now':now,
            'state':state,
            'statename':getProcessStateDescription(state),
            'spawnerr':spawnerr,
            'exitstatus':exitstatus,
            'logfile':stdout_logfile, # b/c alias
            'stdout_logfile':stdout_logfile,
            'stderr_logfile':stderr_logfile,
            'pid':process.pid,
            'resumed':process.resumed,
            }

        description = self._interpretProcessInfo(info)
        info['description'] = description
        return info

    def getAllProcessInfo(self):
        """ Get info about all processes

        @return array result  An array of process status results
        """
        self._update('getAllProcessInfo')

        all_processes = self._getAllProcesses(lexical=True)

        output = []
        for group, process in all_processes:
            name = make_namespec(group.config.name, process.config.name)
            output.append(self.getProcessInfo(name))
        return output

    def _readProcessLog(self, name, offset, length, channel):
        group, process = self._getGroupAndProcess(name)

        logfile = getattr(process.config, '%s_logfile' % channel)

        if logfile is None or not os.path.exists(logfile):
            raise RPCError(Faults.NO_FILE, logfile)

        try:
            return readFile(logfile, int(offset), int(length))
        except ValueError, inst:
            why = inst.args[0]
            raise RPCError(getattr(Faults, why))

    def readProcessStdoutLog(self, name, offset, length):
        """ Read length bytes from name's stdout log starting at offset

        @param string name        the name of the process (or 'group:name')
        @param int offset         offset to start reading from.
        @param int length         number of bytes to read from the log.
        @return string result     Bytes of log
        """
        self._update('readProcessStdoutLog')
        return self._readProcessLog(name, offset, length, 'stdout')

    readProcessLog = readProcessStdoutLog # b/c alias

    def readProcessStderrLog(self, name, offset, length):
        """ Read length bytes from name's stderr log starting at offset

        @param string name        the name of the process (or 'group:name')
        @param int offset         offset to start reading from.
        @param int length         number of bytes to read from the log.
        @return string result     Bytes of log
        """
        self._update('readProcessStderrLog')
        return self._readProcessLog(name, offset, length, 'stderr')

    def _tailProcessLog(self, name, offset, length, channel):
        group, process = self._getGroupAndProcess(name)

        logfile = getattr(process.config, '%s_logfile' % channel)

        if logfile is None or not os.path.exists(logfile):
            return ['', 0, False]

        return tailFile(logfile, int(offset), int(length))

    def tailProcessStdoutLog(self, name, offset, length):
        """
        Provides a more efficient way to tail the (stdout) log than
        readProcessStdoutLog().  Use readProcessStdoutLog() to read
        chunks and tailProcessStdoutLog() to tail.

        Requests (length) bytes from the (name)'s log, starting at
        (offset).  If the total log size is greater than (offset +
        length), the overflow flag is set and the (offset) is
        automatically increased to position the buffer at the end of
        the log.  If less than (length) bytes are available, the
        maximum number of available bytes will be returned.  (offset)
        returned is always the last offset in the log +1.

        @param string name         the name of the process (or 'group:name')
        @param int offset          offset to start reading from
        @param int length          maximum number of bytes to return
        @return array result       [string bytes, int offset, bool overflow]
        """
        self._update('tailProcessStdoutLog')
        return self._tailProcessLog(name, offset, length, 'stdout')

    tailProcessLog = tailProcessStdoutLog # b/c alias

    def tailProcessStderrLog(self, name, offset, length):
        """
        Provides a more efficient way to tail the (stderr) log than
        readProcessStderrLog().  Use readProcessStderrLog() to read
        chunks and tailProcessStderrLog() to tail.

        Requests (length) bytes from the (name)'s log, starting at
        (offset).  If the total log size is greater than (offset +
        length), the overflow flag is set and the (offset) is
        automatically increased to position the buffer at the end of
        the log.  If less than (length) bytes are available, the
        maximum number of available bytes will be returned.  (offset)
        returned is always the last offset in the log +1.

        @param string name         the name of the process (or 'group:name')
        @param int offset          offset to start reading from
        @param int length          maximum number of bytes to return
        @return array result       [string bytes, int offset, bool overflow]
        """
        self._update('tailProcessStderrLog')
        return self._tailProcessLog(name, offset, length, 'stderr')

    def clearProcessLogs(self, name):
        """ Clear the stdout and stderr logs for the named process and
        reopen them.

        @param string name   The name of the process (or 'group:name')
        @return boolean result      Always True unless error
        """
        self._update('clearProcessLogs')

        group, process = self._getGroupAndProcess(name)

        try:
            # implies a reopen
            process.removelogs()
        except (IOError, OSError):
            raise RPCError(Faults.FAILED, name)

        return True

    clearProcessLog = clearProcessLogs # b/c alias

    def clearAllProcessLogs(self):
        """ Clear all process log files

        @return boolean result      Always return true
        """
        self._update('clearAllProcessLogs')
        results  = []
        callbacks = []

        all_processes = self._getAllProcesses()

        for group, process in all_processes:
            callbacks.append((group, process, self.clearProcessLog))

        def clearall():
            if not callbacks:
                return results

            group, process, callback = callbacks.pop(0)
            name = make_namespec(group.config.name, process.config.name)
            try:
                callback(name)
            except RPCError, e:
                results.append(
                    {'name':process.config.name,
                     'group':group.config.name,
                     'status':e.code,
                     'description':e.text})
            else:
                results.append(
                    {'name':process.config.name,
                     'group':group.config.name,
                     'status':Faults.SUCCESS,
                     'description':'OK'}
                    )

            if callbacks:
                return NOT_DONE_YET

            return results

        clearall.delay = 0.05
        clearall.rpcinterface = self
        return clearall # deferred

    def sendProcessStdin(self, name, chars):
        """ Send a string of chars to the stdin of the process name.
        If non-7-bit data is sent (unicode), it is encoded to utf-8
        before being sent to the process' stdin.  If chars is not a
        string or is not unicode, raise INCORRECT_PARAMETERS.  If the
        process is not running, raise NOT_RUNNING.  If the process'
        stdin cannot accept input (e.g. it was closed by the child
        process), raise NO_FILE.

        @param string name        The process name to send to (or 'group:name')
        @param string chars       The character data to send to the process
        @return boolean result    Always return True unless error
        """
        self._update('sendProcessStdin')

        if isinstance(chars, unicode):
            chars = chars.encode('utf-8')

        if not isinstance(chars, basestring):
            raise RPCError(Faults.INCORRECT_PARAMETERS, chars)

        group, process = self._getGroupAndProcess(name)

        if process is None:
            raise RPCError(Faults.BAD_NAME, name)

        if not process.pid or process.killing:
            raise RPCError(Faults.NOT_RUNNING, name)

        try:
            process.write(chars)
        except OSError, why:
            if why[0] == errno.EPIPE:
                raise RPCError(Faults.NO_FILE, name)
            else:
                raise

        return True

    def sendRemoteCommEvent(self, type, data):
        """ Send an event that will be received by event listener
        subprocesses subscribing to the RemoteCommunicationEvent.

        @param  string  type  String for the "type" key in the event header
        @param  string  data  Data for the event body
        @return boolean       Always return True unless error
        """
        if isinstance(type, unicode):
            type = type.encode('utf-8')
        if isinstance(data, unicode):
            data = data.encode('utf-8')

        notify(
            RemoteCommunicationEvent(type, data)
        )

        return True

def make_allfunc(processes, predicate, func, **extra_kwargs):
    """ Return a closure representing a function that calls a
    function for every process, and returns a result """

    callbacks = []
    results = []

    def allfunc(processes=processes, predicate=predicate, func=func,
                extra_kwargs=extra_kwargs, callbacks=callbacks,
                results=results):
        if not callbacks:

            for group, process in processes:
                name = make_namespec(group.config.name, process.config.name)
                if predicate(process):
                    try:
                        callback = func(name, **extra_kwargs)
                        callbacks.append((group, process, callback))
                    except RPCError, e:
                        results.append({'name':process.config.name,
                                        'group':group.config.name,
                                        'status':e.code,
                                        'description':e.text})
                        continue

        if not callbacks:
            return results

        group, process, callback = callbacks.pop(0)

        try:
            value = callback()
        except RPCError, e:
            results.append(
                {'name':process.config.name,
                 'group':group.config.name,
                 'status':e.code,
                 'description':e.text})
            return NOT_DONE_YET

        if value is NOT_DONE_YET:
            # push it back into the queue; it will finish eventually
            callbacks.append((group, process, callback))
        else:
            results.append(
                {'name':process.config.name,
                 'group':group.config.name,
                 'status':Faults.SUCCESS,
                 'description':'OK'}
                )

        if callbacks:
            return NOT_DONE_YET

        return results

    # XXX the above implementation has a weakness inasmuch as the
    # first call into each individual process callback will always
    # return NOT_DONE_YET, so they need to be called twice.  The
    # symptom of this is that calling this method causes the
    # client to block for much longer than it actually requires to
    # kill all of the running processes.  After the first call to
    # the killit callback, the process is actually dead, but the
    # above killall method processes the callbacks one at a time
    # during the select loop, which, because there is no output
    # from child processes after e.g. stopAllProcesses is called,
    # is not busy, so hits the timeout for each callback.  I
    # attempted to make this better, but the only way to make it
    # better assumes totally synchronous reaping of child
    # processes, which requires infrastructure changes to
    # supervisord that are scary at the moment as it could take a
    # while to pin down all of the platform differences and might
    # require a C extension to the Python signal module to allow
    # the setting of ignore flags to signals.
    return allfunc

def isRunning(process):
    if process.get_state() in RUNNING_STATES:
        return True

def isNotRunning(process):
    return not isRunning(process)

# this is not used in code but referenced via an entry point in the conf file
def make_main_rpcinterface(supervisord):
    return SupervisorNamespaceRPCInterface(supervisord)


########NEW FILE########
__FILENAME__ = loop_eventgen
#!/usr/bin/env python

# A process which emits a process communications event on its stdout,
# and subsequently waits for a line to be sent back to its stdin by
# loop_listener.py.

import sys
import time
from supervisor import childutils

def main(max):
    start = time.time()
    report = open('/tmp/report', 'w')
    i = 0
    while 1:
        childutils.pcomm.stdout('the_data')
        data = sys.stdin.readline()
        report.write(str(i) + ' @ %s\n' % childutils.get_asctime())
        report.flush()
        i+=1
        if max and i >= max:
            end = time.time()
            report.write('%s per second\n' % (i / (end - start)))
            sys.exit(0)

if __name__ == '__main__':
    max = 0
    if len(sys.argv) > 1:
        max = int(sys.argv[1])
    main(max)
        


########NEW FILE########
__FILENAME__ = loop_listener
#!/usr/bin/env python -u

# An event listener that listens for process communications events
# from loop_eventgen.py and uses RPC to write data to the event
# generator's stdin.

import os
from supervisor import childutils

def main():
    rpcinterface = childutils.getRPCInterface(os.environ)
    while 1:
        headers, payload = childutils.listener.wait()
        if headers['eventname'].startswith('PROCESS_COMMUNICATION'):
            pheaders, pdata = childutils.eventdata(payload)
            pname = '%s:%s' % (pheaders['processname'], pheaders['groupname'])
            rpcinterface.supervisor.sendProcessStdin(pname, 'Got it yo\n')
        childutils.listener.ok()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = sample_commevent
#!/usr/bin/env python

# An example process which emits a stdout process communication event every
# second (or every number of seconds specified as a single argument).

import sys
import time

def write_stdout(s):
    sys.stdout.write(s)
    sys.stdout.flush()

def main(sleep):
    while 1:
        write_stdout('<!--XSUPERVISOR:BEGIN-->')
        write_stdout('the data')
        write_stdout('<!--XSUPERVISOR:END-->')
        time.sleep(sleep)

if __name__ == '__main__':
    if len(sys.argv) > 1:
        main(float(sys.argv[1]))
    else:
        main(1)


########NEW FILE########
__FILENAME__ = sample_eventlistener
#!/usr/bin/env python -u

# A sample long-running supervisor event listener which demonstrates
# how to accept event notifications from supervisor and how to respond
# properly.  This demonstration does *not* use the
# supervisor.childutils module, which wraps the specifics of
# communications in higher-level API functions.  If your listeners are
# implemented using Python, it is recommended that you use the
# childutils module API instead of modeling your scripts on the
# lower-level protocol example below.

import sys

def write_stdout(s):
    sys.stdout.write(s)
    sys.stdout.flush()

def write_stderr(s):
    sys.stderr.write(s)
    sys.stderr.flush()

def main():
    while 1:
        write_stdout('READY\n') # transition from ACKNOWLEDGED to READY
        line = sys.stdin.readline()  # read header line from stdin 
        write_stderr(line) # print it out to stderr (testing only)
        headers = dict([ x.split(':') for x in line.split() ])
        data = sys.stdin.read(int(headers['len'])) # read the event payload
        write_stderr(data) # print the event payload to stderr (testing only)
        write_stdout('RESULT 2\nOK') # transition from BUSY to ACKNOWLEDGED
        #write_stdout('RESULT 4\nFAIL') # transition from BUSY TO ACKNOWLEDGED

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = sample_exiting_eventlistener
#!/usr/bin/env python

# A sample long-running supervisor event listener which demonstrates
# how to accept event notifications from supervisor and how to respond
# properly.  It is the same as the sample_eventlistener.py script
# except it exits after each request (presumably to be restarted by
# supervisor).  This demonstration does *not* use the
# supervisor.childutils module, which wraps the specifics of
# communications in higher-level API functions.  If your listeners are
# implemented using Python, it is recommended that you use the
# childutils module API instead of modeling your scripts on the
# lower-level protocol example below.

import sys

def write_stdout(s):
    sys.stdout.write(s)
    sys.stdout.flush()

def write_stderr(s):
    sys.stderr.write(s)
    sys.stderr.flush()

def main():
    write_stdout('READY\n') # transition from ACKNOWLEDGED to READY
    line = sys.stdin.readline()  # read a line from stdin from supervisord
    write_stderr(line) # print it out to stderr (testing only)
    headers = dict([ x.split(':') for x in line.split() ])
    data = sys.stdin.read(int(headers['len'])) # read the event payload
    write_stderr(data) # print the event payload to stderr (testing only)
    write_stdout('RESULT 2\nOK') # transition from READY to ACKNOWLEDGED
    # exit, if the eventlistener process config has autorestart=true,
    # it will be restarted by supervisord.

if __name__ == '__main__':
    main()
    

########NEW FILE########
__FILENAME__ = socket_manager
import socket

class Proxy:
    """ Class for wrapping a shared resource object and getting
        notified when it's deleted
    """
    
    def __init__(self, object, **kwargs):
        self.object = object
        self.on_delete = kwargs.get('on_delete', None)

    def __del__(self):
        if self.on_delete:
            self.on_delete()
    
    def __getattr__(self, name):
        return getattr(self.object, name)
        
    def _get(self):
        return self.object
        
class ReferenceCounter:
    """ Class for tracking references to a shared resource
    """
    
    def __init__(self, **kwargs):
        self.on_non_zero = kwargs['on_non_zero']
        self.on_zero = kwargs['on_zero']
        self.ref_count = 0
    
    def get_count(self):
        return self.ref_count
    
    def increment(self):
        if self.ref_count == 0:
            self.on_non_zero()
        self.ref_count = self.ref_count + 1
        
    def decrement(self):
        if self.ref_count <= 0:
            raise Exception('Illegal operation: cannot decrement below zero')
        self.ref_count -= 1
        if self.ref_count == 0:
            self.on_zero()

class SocketManager:
    """ Class for managing sockets in servers that create/bind/listen
        before forking multiple child processes to accept() 
        Sockets are managed at the process group level and referenced counted
        at the process level b/c that's really the only place to hook in
    """
    
    def __init__(self, socket_config, **kwargs):
        self.logger = kwargs.get('logger', None)
        self.socket = None
        self.prepared = False
        self.socket_config = socket_config
        self.ref_ctr = ReferenceCounter(on_zero=self._close, on_non_zero=self._prepare_socket)
        
    def __repr__(self):
        return '<%s at %s for %s>' % (self.__class__,
                                      id(self),
                                      self.socket_config.url)

    def config(self):
        return self.socket_config
        
    def is_prepared(self):
        return self.prepared

    def get_socket(self):
        self.ref_ctr.increment()
        self._require_prepared()
        return Proxy(self.socket, on_delete=self.ref_ctr.decrement)
        
    def get_socket_ref_count(self):
        self._require_prepared()
        return self.ref_ctr.get_count()
        
    def _require_prepared(self):
        if not self.prepared:
            raise Exception('Socket has not been prepared')
    
    def _prepare_socket(self):
        if not self.prepared:
            if self.logger:
                self.logger.info('Creating socket %s' % self.socket_config)
            self.socket = self.socket_config.create_and_bind()
            self.socket.listen(socket.SOMAXCONN)
            self.prepared = True

    def _close(self):
        self._require_prepared()
        if self.logger:
            self.logger.info('Closing socket %s' % self.socket_config)
        self.socket.close()
        self.prepared = False

########NEW FILE########
__FILENAME__ = states
# This module must not depend on any other non-stdlib module to prevent
# circular import problems.

class ProcessStates:
    STOPPED = 0
    STARTING = 10
    RUNNING = 20
    BACKOFF = 30
    STOPPING = 40
    EXITED = 100
    FATAL = 200
    UNKNOWN = 1000

STOPPED_STATES = (ProcessStates.STOPPED,
                  ProcessStates.EXITED,
                  ProcessStates.FATAL,
                  ProcessStates.UNKNOWN)

RUNNING_STATES = (ProcessStates.RUNNING,
                  ProcessStates.BACKOFF,
                  ProcessStates.STARTING)



def getProcessStateDescription(code):
    for statename in ProcessStates.__dict__:
        if getattr(ProcessStates, statename) == code:
            return statename

class SupervisorStates:
    FATAL = 2
    RUNNING = 1
    RESTARTING = 0
    SHUTDOWN = -1

def getSupervisorStateDescription(code):
    for statename in SupervisorStates.__dict__:
        if getattr(SupervisorStates, statename) == code:
            return statename


class EventListenerStates:
    READY = 10 # the process ready to be sent an event from supervisor
    BUSY = 20 # event listener is processing an event sent to it by supervisor
    ACKNOWLEDGED = 30 # the event listener processed an event
    UNKNOWN = 40 # the event listener is in an unknown state

def getEventListenerStateDescription(code):
    for statename in EventListenerStates.__dict__:
        if getattr(EventListenerStates, statename) == code:
            return statename


########NEW FILE########
__FILENAME__ = supervisorctl
#!/usr/bin/env python -u

"""supervisorctl -- control applications run by supervisord from the cmd line.

Usage: %s [options] [action [arguments]]

Options:
-c/--configuration -- configuration file path (default /etc/supervisor.conf)
-h/--help -- print usage message and exit
-i/--interactive -- start an interactive shell after executing commands
-s/--serverurl URL -- URL on which supervisord server is listening
     (default "http://localhost:9001").  
-u/--username -- username to use for authentication with server
-p/--password -- password to use for authentication with server
-r/--history-file -- keep a readline history (if readline is available)

action [arguments] -- see below

Actions are commands like "tail" or "stop".  If -i is specified or no action is
specified on the command line, a "shell" interpreting actions typed
interactively is started.  Use the action "help" to find out about available
actions.
"""

import cmd
import sys
import getpass
import xmlrpclib
import socket
import errno
import urlparse
import threading

from supervisor.medusa import asyncore_25 as asyncore

from supervisor.options import ClientOptions
from supervisor.options import split_namespec
from supervisor import xmlrpc
from supervisor import states

class fgthread(threading.Thread):
    """ A subclass of threading.Thread, with a kill() method.
    To be used for foreground output/error streaming.
    http://mail.python.org/pipermail/python-list/2004-May/260937.html
    """
  
    def __init__(self, program, ctl):
        threading.Thread.__init__(self)
        import http_client
        self.killed = False
        self.program = program
        self.ctl = ctl
        self.listener = http_client.Listener()
        self.output_handler = http_client.HTTPHandler(self.listener,
                                                      self.ctl.options.username,
                                                      self.ctl.options.password)
        self.error_handler = http_client.HTTPHandler(self.listener,
                                                     self.ctl.options.username,
                                                     self.ctl.options.password)

    def start(self):
        # Start the thread
        self.__run_backup = self.run
        self.run = self.__run
        threading.Thread.start(self)

    def run(self):
        self.output_handler.get(self.ctl.options.serverurl,
                                '/logtail/%s/stdout'%self.program)
        self.error_handler.get(self.ctl.options.serverurl,
                               '/logtail/%s/stderr'%self.program)
        asyncore.loop()

    def __run(self):
        # Hacked run function, which installs the trace
        sys.settrace(self.globaltrace)
        self.__run_backup()
        self.run = self.__run_backup

    def globaltrace(self, frame, why, arg):
        if why == 'call':
            return self.localtrace
        else:
            return None

    def localtrace(self, frame, why, arg):
        if self.killed:
            if why == 'line':
                raise SystemExit()
        return self.localtrace

    def kill(self):
        self.output_handler.close()
        self.error_handler.close()
        self.killed = True

class Controller(cmd.Cmd):

    def __init__(self, options, completekey='tab', stdin=None,
                 stdout=None):
        self.options = options
        self.prompt = self.options.prompt + '> '
        self.options.plugins = []
        self.vocab = ['add','exit','maintail','pid','reload',
                      'restart','start','stop','version','clear',
                      'fg','open','quit','remove','shutdown','status',
                      'tail','help']
        cmd.Cmd.__init__(self, completekey, stdin, stdout)
        for name, factory, kwargs in self.options.plugin_factories:
            plugin = factory(self, **kwargs)
            self.options.plugins.append(plugin)
            plugin.name = name

    def emptyline(self):
        # We don't want a blank line to repeat the last command.
        return

    def onecmd(self, line):
        """ Override the onecmd method to:
          - catch and print all exceptions
          - allow for composite commands in interactive mode (foo; bar)
          - call 'do_foo' on plugins rather than ourself
        """
        origline = line
        lines = line.split(';') # don't filter(None, line.split), as we pop
        line = lines.pop(0)
        # stuffing the remainder into cmdqueue will cause cmdloop to
        # call us again for each command.
        self.cmdqueue.extend(lines)
        cmd, arg, line = self.parseline(line)
        if not line:
            return self.emptyline()
        if cmd is None:
            return self.default(line)
        self.lastcmd = line
        if cmd == '':
            return self.default(line)
        else:
            do_func = self._get_do_func(cmd)
            if do_func is None:
                return self.default(line)
            try:
                try:
                    return do_func(arg)
                except xmlrpclib.ProtocolError, e:
                    if e.errcode == 401:
                        if self.options.interactive:
                            self.output('Server requires authentication')
                            username = raw_input('Username:')
                            password = getpass.getpass(prompt='Password:')
                            self.output('')
                            self.options.username = username
                            self.options.password = password
                            return self.onecmd(origline)
                        else:
                            self.options.usage('Server requires authentication')
                    else:
                        raise
                do_func(arg)
            except SystemExit:
                raise
            except Exception, e:
                (file, fun, line), t, v, tbinfo = asyncore.compact_traceback()
                error = 'error: %s, %s: file: %s line: %s' % (t, v, file, line)
                self.output(error)
                if not self.options.interactive:
                    sys.exit(2)

    def _get_do_func(self, cmd):
        func_name = 'do_' + cmd
        func = getattr(self, func_name, None)
        if not func:
            for plugin in self.options.plugins:
                func = getattr(plugin, func_name, None)
                if func is not None:
                    break
        return func

    def output(self, stuff):
        if stuff is not None:
            if isinstance(stuff, unicode):
                stuff = stuff.encode('utf-8')
            self.stdout.write(stuff + '\n')
    
    def get_supervisor(self):
        return self.get_server_proxy('supervisor')

    def get_server_proxy(self, namespace=None):
        proxy = self.options.getServerProxy()
        if namespace is None:
            return proxy
        else:
            return getattr(proxy, namespace)

    def upcheck(self):
        try:
            supervisor = self.get_supervisor()
            api = supervisor.getVersion() # deprecated
            from supervisor import rpcinterface
            if api != rpcinterface.API_VERSION:
                self.output(
                    'Sorry, this version of supervisorctl expects to '
                    'talk to a server with API version %s, but the '
                    'remote version is %s.' % (rpcinterface.API_VERSION, api))
                return False
        except xmlrpclib.Fault, e:
            if e.faultCode == xmlrpc.Faults.UNKNOWN_METHOD:
                self.output(
                    'Sorry, supervisord responded but did not recognize '
                    'the supervisor namespace commands that supervisorctl '
                    'uses to control it.  Please check that the '
                    '[rpcinterface:supervisor] section is enabled in the '
                    'configuration file (see sample.conf).')
                return False
            raise 
        except socket.error, why:
            if why[0] == errno.ECONNREFUSED:
                self.output('%s refused connection' % self.options.serverurl)
                return False
            elif why[0] == errno.ENOENT:
                self.output('%s no such file' % self.options.serverurl)
                return False
            raise
        return True

    def completionmatches(self,text,line,flag=0):
        groups=[]
        programs=[]
        groupwiseprograms={}
        info = self.get_supervisor().getAllProcessInfo()
        for i in info:
            programs.append(i['name'])
            if i['group'] not in groups:
                groups.append(i['group'])
                groupwiseprograms[i['group']]=[]
            groupwiseprograms[i['group']].append(i['name'])
        total=[]
        for i in groups:
            if i in programs:
                total.append(i+' ')
            else:
                for n in groupwiseprograms[i]:
                    total.append(i+':'+n+' ')
        if flag:
            # add/remove require only the group name
            return [i+' ' for i in groups if i.startswith(text)]
        if len(line.split()) == 1:
            return total
        else:
            current=line.split()[-1]
            if line.endswith(' ') and len(line.split()) > 1:
                results=[i for i in total if i.startswith(text)]
                return results
            if ':' in current:
                g=current.split(':')[0]
                results = [i+' ' for i in groupwiseprograms[g]
                           if i.startswith(text)]
                return results
            results = [i for i in total if i.startswith(text)]
            return results

    def complete(self,text,state):
        try:
            import readline
        except ImportError:
            return None
        line = readline.get_line_buffer()
        if line == '':
            results = [i+' ' for i in self.vocab if i.startswith(text)]+[None]
            return results[state]
        else:
            exp = line.split()[0]
            if exp in ['start','stop','restart','clear','status','tail','fg','pid']:
                if not line.endswith(' ') and len(line.split()) == 1:
                    return [text + ' ', None][state]
                if exp == 'fg':
                    if line.endswith(' ') and len(line.split()) > 1:
                        return None
                results = self.completionmatches(text,line)+[None]
                return results[state]
            elif exp in ['maintail','pid','reload','shutdown','exit','open',
                         'quit','version','EOF']:
                return None
            elif exp == 'help':
                if line.endswith(' ') and len(line.split()) > 1:
                    return None
                results=[i+' ' for i in self.vocab if i.startswith(text)]+[None]
                return results[state]
            elif exp in ['add','remove']:
                results=self.completionmatches(text,line,flag=1)+[None]
                return results[state]
            else:
                results=[i+' ' for i in self.vocab if i.startswith(text)]+[None]
                return results[state]

    def do_help(self, arg):
        for plugin in self.options.plugins:
            plugin.do_help(arg)

    def help_help(self):
        self.output("help\t\tPrint a list of available actions")
        self.output("help <action>\tPrint help for <action>")

    def do_EOF(self, arg):
        self.output('')
        return 1

    def help_EOF(self):
        self.output("To quit, type ^D or use the quit command")

def get_names(inst):
    names = []
    classes = [inst.__class__]
    while classes:
        aclass = classes.pop(0)
        if aclass.__bases__:
            classes = classes + list(aclass.__bases__)
        names = names + dir(aclass)
    return names

class ControllerPluginBase:
    name = 'unnamed'

    def __init__(self, controller):
        self.ctl = controller

    def _doc_header(self):
        return "%s commands (type help <topic>):" % self.name
    doc_header = property(_doc_header)

    def do_help(self, arg):
        if arg:
            # XXX check arg syntax
            try:
                func = getattr(self, 'help_' + arg)
            except AttributeError:
                try:
                    doc=getattr(self, 'do_' + arg).__doc__
                    if doc:
                        self.ctl.stdout.write("%s\n"%str(doc))
                        return
                except AttributeError:
                    pass
                self.ctl.stdout.write("%s\n"%str(self.ctl.nohelp % (arg,)))
                return
            func()
        else:
            names = get_names(self)
            cmds_doc = []
            cmds_undoc = []
            help = {}
            for name in names:
                if name[:5] == 'help_':
                    help[name[5:]]=1
            names.sort()
            # There can be duplicates if routines overridden
            prevname = ''
            for name in names:
                if name[:3] == 'do_':
                    if name == prevname:
                        continue
                    prevname = name
                    cmd=name[3:]
                    if cmd in help:
                        cmds_doc.append(cmd)
                        del help[cmd]
                    elif getattr(self, name).__doc__:
                        cmds_doc.append(cmd)
                    else:
                        cmds_undoc.append(cmd)
            self.ctl.stdout.write("\n")
            self.ctl.print_topics(self.doc_header,   cmds_doc,   15,80)

class DefaultControllerPlugin(ControllerPluginBase):
    name = 'default'
    listener = None # for unit tests
    def _tailf(self, path):
        if not self.ctl.upcheck():
            return

        self.ctl.output('==> Press Ctrl-C to exit <==')

        username = self.ctl.options.username
        password = self.ctl.options.password
        try:
            # Python's urllib2 (at least as of Python 2.4.2) isn't up
            # to this task; it doesn't actually implement a proper
            # HTTP/1.1 client that deals with chunked responses (it
            # always sends a Connection: close header).  We use a
            # homegrown client based on asyncore instead.  This makes
            # me sad.
            import http_client
            if self.listener is None:
                listener = http_client.Listener()
            else:
                listener = self.listener # for unit tests
            handler = http_client.HTTPHandler(listener, username, password)
            handler.get(self.ctl.options.serverurl, path)
            asyncore.loop()
        except KeyboardInterrupt:
            handler.close()
            self.ctl.output('')
            return

    def do_tail(self, arg):
        if not self.ctl.upcheck():
            return
        
        args = arg.strip().split()

        if len(args) < 1:
            self.ctl.output('Error: too few arguments')
            self.help_tail()
            return

        elif len(args) > 3:
            self.ctl.output('Error: too many arguments')
            self.help_tail()
            return

        modifier = None

        if args[0].startswith('-'):
            modifier = args.pop(0)

        if len(args) == 1:
            name = args[-1]
            channel = 'stdout'
        else:
            if args:
                name = args[0]
                channel = args[-1].lower()
                if channel not in ('stderr', 'stdout'):
                    self.ctl.output('Error: bad channel %r' % channel)
                    return
            else:
                self.ctl.output('Error: tail requires process name')
                return

        bytes = 1600

        if modifier is not None:
            what = modifier[1:]
            if what == 'f':
                bytes = None
            else:
                try:
                    bytes = int(what)
                except:
                    self.ctl.output('Error: bad argument %s' % modifier)
                    return

        supervisor = self.ctl.get_supervisor()

        if bytes is None:
            return self._tailf('/logtail/%s/%s' % (name, channel))

        else:
            try:
                if channel is 'stdout':
                    output = supervisor.readProcessStdoutLog(name,
                                                             -bytes, 0)
                else: # if channel is 'stderr'
                    output = supervisor.readProcessStderrLog(name,
                                                             -bytes, 0)
            except xmlrpclib.Fault, e:
                template = '%s: ERROR (%s)'
                if e.faultCode == xmlrpc.Faults.NO_FILE:
                    self.ctl.output(template % (name, 'no log file'))
                elif e.faultCode == xmlrpc.Faults.FAILED:
                    self.ctl.output(template % (name,
                                             'unknown error reading log'))
                elif e.faultCode == xmlrpc.Faults.BAD_NAME:
                    self.ctl.output(template % (name,
                                             'no such process name'))
            else:
                self.ctl.output(output)

    def help_tail(self):
        self.ctl.output(
            "tail [-f] <name> [stdout|stderr] (default stdout)\n"
            "Ex:\n"
            "tail -f <name>\t\tContinuous tail of named process stdout\n"
            "\t\t\tCtrl-C to exit.\n"
            "tail -100 <name>\tlast 100 *bytes* of process stdout\n"
            "tail <name> stderr\tlast 1600 *bytes* of process stderr"
            )

    def do_maintail(self, arg):
        if not self.ctl.upcheck():
            return
        
        args = arg.strip().split()

        if len(args) > 1:
            self.ctl.output('Error: too many arguments')
            self.help_maintail()
            return

        elif len(args) == 1:
            if args[0].startswith('-'):
                what = args[0][1:]
                if what == 'f':
                    path = '/mainlogtail'
                    return self._tailf(path)
                try:
                    what = int(what)
                except:
                    self.ctl.output('Error: bad argument %s' % args[0])
                    return
                else:
                    bytes = what
            else:
                self.ctl.output('Error: bad argument %s' % args[0])
                return
            
        else:
            bytes = 1600

        supervisor = self.ctl.get_supervisor()

        try:
            output = supervisor.readLog(-bytes, 0)
        except xmlrpclib.Fault, e:
            template = '%s: ERROR (%s)'
            if e.faultCode == xmlrpc.Faults.NO_FILE:
                self.ctl.output(template % ('supervisord', 'no log file'))
            elif e.faultCode == xmlrpc.Faults.FAILED:
                self.ctl.output(template % ('supervisord',
                                         'unknown error reading log'))
        else:
            self.ctl.output(output)

    def help_maintail(self):
        self.ctl.output(
            "maintail -f \tContinuous tail of supervisor main log file"
            " (Ctrl-C to exit)\n"
            "maintail -100\tlast 100 *bytes* of supervisord main log file\n"
            "maintail\tlast 1600 *bytes* of supervisor main log file\n"
            )

    def do_quit(self, arg):
        sys.exit(0)

    def help_quit(self):
        self.ctl.output("quit\tExit the supervisor shell.")

    do_exit = do_quit

    def help_exit(self):
        self.ctl.output("exit\tExit the supervisor shell.")

    def _procrepr(self, info):
        template = '%(name)-32s %(state)-10s %(desc)s'
        if info['name'] == info['group']:
            name = info['name']
        else:
            name = '%s:%s' % (info['group'], info['name'])
                    
        return template % {'name':name, 'state':info['statename'],
                           'desc':info['description']}

    def do_status(self, arg):
        if not self.ctl.upcheck():
            return
        
        supervisor = self.ctl.get_supervisor()

        names = arg.strip().split()

        if names:
            for name in names:
                try:
                    info = supervisor.getProcessInfo(name)
                except xmlrpclib.Fault, e:
                    if e.faultCode == xmlrpc.Faults.BAD_NAME:
                        self.ctl.output('No such process %s' % name)
                    else:
                        raise
                    continue
                self.ctl.output(self._procrepr(info))
        else:
            for info in supervisor.getAllProcessInfo():
                self.ctl.output(self._procrepr(info))

    def help_status(self):
        self.ctl.output("status\t\t\tGet all process status info.")
        self.ctl.output(
            "status <name>\t\tGet status on a single process by name.")
        self.ctl.output("status <name> <name>\tGet status on multiple named "
                     "processes.")

    def do_pid(self, arg):
        supervisor = self.ctl.get_supervisor()
        if not self.ctl.upcheck():
            return
        names = arg.strip().split()
        if not names:
            pid = supervisor.getPID()
            self.ctl.output(str(pid))
        elif 'all' in names:
            for info in supervisor.getAllProcessInfo():
                self.ctl.output(str(info['pid']))
        else:
            for name in names:
                try:
                    info = supervisor.getProcessInfo(name)
                except xmlrpclib.Fault, e:
                    if e.faultCode == xmlrpc.Faults.BAD_NAME:
                        self.ctl.output('No such process %s' % name)
                    else:
                        raise
                    continue
                self.ctl.output(str(info['pid']))

    def help_pid(self):
        self.ctl.output("pid\t\t\tGet the PID of supervisord.")    
        self.ctl.output("pid <name>\t\tGet the PID of a single "
            "child process by name.")
        self.ctl.output("pid all\t\t\tGet the PID of every child "
            "process, one per line.")

    def _startresult(self, result):
        name = result['name']
        code = result['status']
        template = '%s: ERROR (%s)'
        if code == xmlrpc.Faults.BAD_NAME:
            return template % (name, 'no such process')
        elif code == xmlrpc.Faults.NO_FILE:
            return template % (name, 'no such file')
        elif code == xmlrpc.Faults.NOT_EXECUTABLE:
            return template % (name, 'file is not executable')
        elif code == xmlrpc.Faults.ALREADY_STARTED:
            return template % (name, 'already started')
        elif code == xmlrpc.Faults.SPAWN_ERROR:
            return template % (name, 'spawn error')
        elif code == xmlrpc.Faults.ABNORMAL_TERMINATION:
            return template % (name, 'abnormal termination')
        elif code == xmlrpc.Faults.SUCCESS:
            return '%s: started' % name
        # assertion
        raise ValueError('Unknown result code %s for %s' % (code, name))

    def do_start(self, arg):
        if not self.ctl.upcheck():
            return

        names = arg.strip().split()
        supervisor = self.ctl.get_supervisor()

        if not names:
            self.ctl.output("Error: start requires a process name")
            self.help_start()
            return

        if 'all' in names:
            results = supervisor.startAllProcesses()
            for result in results:
                result = self._startresult(result)
                self.ctl.output(result)
                
        else:
            for name in names:
                group_name, process_name = split_namespec(name)
                if process_name is None:
                    results = supervisor.startProcessGroup(group_name)
                    for result in results:
                        result = self._startresult(result)
                        self.ctl.output(result)
                else:
                    try:
                        result = supervisor.startProcess(name)
                    except xmlrpclib.Fault, e:
                        error = self._startresult({'status':e.faultCode,
                                                   'name':name,
                                                   'description':e.faultString})
                        self.ctl.output(error)
                    else:
                        self.ctl.output('%s: started' % name)

    def help_start(self):
        self.ctl.output("start <name>\t\tStart a process")
        self.ctl.output("start <gname>:*\t\tStart all processes in a group")
        self.ctl.output(
            "start <name> <name>\tStart multiple processes or groups")
        self.ctl.output("start all\t\tStart all processes")

    def _stopresult(self, result):
        name = result['name']
        code = result['status']
        fault_string = result['description']
        template = '%s: ERROR (%s)'
        if code == xmlrpc.Faults.BAD_NAME:
            return template % (name, 'no such process')
        elif code == xmlrpc.Faults.NOT_RUNNING:
            return template % (name, 'not running')
        elif code == xmlrpc.Faults.SUCCESS:
            return '%s: stopped' % name
        elif code == xmlrpc.Faults.FAILED:
            return fault_string
        # assertion
        raise ValueError('Unknown result code %s for %s' % (code, name))

    def do_stop(self, arg):
        if not self.ctl.upcheck():
            return

        names = arg.strip().split()
        supervisor = self.ctl.get_supervisor()

        if not names:
            self.ctl.output('Error: stop requires a process name')
            self.help_stop()
            return

        if 'all' in names:
            results = supervisor.stopAllProcesses()
            for result in results:
                result = self._stopresult(result)
                self.ctl.output(result)

        else:
            for name in names:
                group_name, process_name = split_namespec(name)
                if process_name is None:
                    results = supervisor.stopProcessGroup(group_name)
                    for result in results:
                        result = self._stopresult(result)
                        self.ctl.output(result)
                else:
                    try:
                        result = supervisor.stopProcess(name)
                    except xmlrpclib.Fault, e:
                        error = self._stopresult({'status':e.faultCode,
                                                  'name':name,
                                                  'description':e.faultString})
                        self.ctl.output(error)
                    else:
                        self.ctl.output('%s: stopped' % name)

    def help_stop(self):
        self.ctl.output("stop <name>\t\tStop a process")
        self.ctl.output("stop <gname>:*\t\tStop all processes in a group")
        self.ctl.output("stop <name> <name>\tStop multiple processes or groups")
        self.ctl.output("stop all\t\tStop all processes")

    def do_restart(self, arg):
        if not self.ctl.upcheck():
            return

        names = arg.strip().split()

        if not names:
            self.ctl.output('Error: restart requires a process name')
            self.help_restart()
            return

        self.do_stop(arg)
        self.do_start(arg)

    def help_restart(self):
        self.ctl.output("restart <name>\t\tRestart a process")
        self.ctl.output("restart <gname>:*\tRestart all processes in a group")
        self.ctl.output("restart <name> <name>\tRestart multiple processes or "
                     "groups")
        self.ctl.output("restart all\t\tRestart all processes")
        self.ctl.output("Note: restart does not reread config files. For that,"
                        " see reread and update.")

    def do_shutdown(self, arg):
        if self.ctl.options.interactive:
            yesno = raw_input('Really shut the remote supervisord process '
                              'down y/N? ')
            really = yesno.lower().startswith('y')
        else:
            really = 1

        if really:
            supervisor = self.ctl.get_supervisor()
            try:
                supervisor.shutdown()
            except xmlrpclib.Fault, e:
                if e.faultCode == xmlrpc.Faults.SHUTDOWN_STATE:
                    self.ctl.output('ERROR: already shutting down')
                else:
                    raise
            except socket.error, e:
                if e[0] == errno.ECONNREFUSED:
                    msg = 'ERROR: %s refused connection (already shut down?)'
                    self.ctl.output(msg % self.ctl.options.serverurl)
                elif e[0] == errno.ENOENT:
                    msg = 'ERROR: %s no such file (already shut down?)'
                    self.ctl.output(msg % self.ctl.options.serverurl)
                else:
                    raise
            else:
                self.ctl.output('Shut down')

    def help_shutdown(self):
        self.ctl.output("shutdown \tShut the remote supervisord down.")

    def do_reload(self, arg):
        if self.ctl.options.interactive:
            yesno = raw_input('Really restart the remote supervisord process '
                              'y/N? ')
            really = yesno.lower().startswith('y')
        else:
            really = 1
        if really:
            supervisor = self.ctl.get_supervisor()
            try:
                supervisor.restart()
            except xmlrpclib.Fault, e:
                if e.faultCode == xmlrpc.Faults.SHUTDOWN_STATE:
                    self.ctl.output('ERROR: already shutting down')
            else:
                self.ctl.output('Restarted supervisord')

    def help_reload(self):
        self.ctl.output("reload \t\tRestart the remote supervisord.")

    def _formatChanges(self, (added, changed, dropped)):
        changedict = {}
        for n, t in [(added, 'available'),
                     (changed, 'changed'),
                     (dropped, 'disappeared')]:
            changedict.update(dict(zip(n, [t] * len(n))))

        if changedict:
            names = changedict.keys()
            names.sort()
            for name in names:
                self.ctl.output("%s: %s" % (name, changedict[name]))
        else:
            self.ctl.output("No config updates to processes")

    def _formatConfigInfo(self, configinfo):
        if configinfo['group'] == configinfo['name']:
            name = configinfo['group']
        else:
            name = "%s:%s" % (configinfo['group'], configinfo['name'])
        formatted = { 'name': name }
        if configinfo['inuse']:
            formatted['inuse'] = 'in use'
        else:
            formatted['inuse'] = 'avail'
        if configinfo['autostart']:
            formatted['autostart'] = 'auto'
        else:
            formatted['autostart'] = 'manual'
        formatted['priority'] = "%s:%s" % (configinfo['group_prio'],
                                           configinfo['process_prio'])

        template = '%(name)-32s %(inuse)-9s %(autostart)-9s %(priority)s'
        return template % formatted

    def do_avail(self, arg):
        supervisor = self.ctl.get_supervisor()
        try:
            configinfo = supervisor.getAllConfigInfo()
        except xmlrpclib.Fault, e:
            if e.faultCode == xmlrpc.Faults.SHUTDOWN_STATE:
                self.ctl.output('ERROR: supervisor shutting down')
        else:
            for pinfo in configinfo:
                self.ctl.output(self._formatConfigInfo(pinfo))

    def help_avail(self):
        self.ctl.output("avail\t\t\tDisplay all configured processes")

    def do_reread(self, arg):
        supervisor = self.ctl.get_supervisor()
        try:
            result = supervisor.reloadConfig()
        except xmlrpclib.Fault, e:
            if e.faultCode == xmlrpc.Faults.SHUTDOWN_STATE:
                self.ctl.output('ERROR: supervisor shutting down')
            elif e.faultCode == xmlrpc.Faults.CANT_REREAD:
                self.ctl.output('ERROR: %s' % e.faultString)
            else:
                raise
        else:
            self._formatChanges(result[0])

    def help_reread(self):
        self.ctl.output("reread \t\t\tReload the daemon's configuration files")

    def do_add(self, arg):
        names = arg.strip().split()

        supervisor = self.ctl.get_supervisor()
        for name in names:
            try:
                supervisor.addProcessGroup(name)
            except xmlrpclib.Fault, e:
                if e.faultCode == xmlrpc.Faults.SHUTDOWN_STATE:
                    self.ctl.output('ERROR: shutting down')
                elif e.faultCode == xmlrpc.Faults.ALREADY_ADDED:
                    self.ctl.output('ERROR: process group already active')
                elif e.faultCode == xmlrpc.Faults.BAD_NAME:
                    self.ctl.output(
                        "ERROR: no such process/group: %s" % name)
                else:
                    raise
            else:
                self.ctl.output("%s: added process group" % name)

    def help_add(self):
        self.ctl.output("add <name> [...]\tActivates any updates in config "
                        "for process/group")

    def do_remove(self, arg):
        names = arg.strip().split()

        supervisor = self.ctl.get_supervisor()
        for name in names:
            try:
                result = supervisor.removeProcessGroup(name)
            except xmlrpclib.Fault, e:
                if e.faultCode == xmlrpc.Faults.STILL_RUNNING:
                    self.ctl.output('ERROR: process/group still running: %s'
                                    % name)
                elif e.faultCode == xmlrpc.Faults.BAD_NAME:
                    self.ctl.output(
                        "ERROR: no such process/group: %s" % name)
                else:
                    raise
            else:
                self.ctl.output("%s: removed process group" % name)

    def help_remove(self):
        self.ctl.output("remove <name> [...]\tRemoves process/group from "
                        "active config")

    def do_update(self, arg):
        def log(name, message):
            self.ctl.output("%s: %s" % (name, message))

        supervisor = self.ctl.get_supervisor()
        try:
            result = supervisor.reloadConfig()
        except xmlrpclib.Fault, e:
            if e.faultCode == xmlrpc.Faults.SHUTDOWN_STATE:
                self.ctl.output('ERROR: already shutting down')
                return
            else:
                raise e

        added, changed, removed = result[0]

        for gname in removed:
            results = supervisor.stopProcessGroup(gname)
            log(gname, "stopped")

            fails = [res for res in results
                     if res['status'] == xmlrpc.Faults.FAILED]
            if fails:
                log(gname, "has problems; not removing")
                continue
            supervisor.removeProcessGroup(gname)
            log(gname, "removed process group")

        for gname in changed:
            results = supervisor.stopProcessGroup(gname)
            log(gname, "stopped")

            supervisor.removeProcessGroup(gname)
            supervisor.addProcessGroup(gname)
            log(gname, "updated process group")

        for gname in added:
            supervisor.addProcessGroup(gname)
            log(gname, "added process group")

    def help_update(self):
        self.ctl.output("update\t\tReload config and add/remove as necessary")

    def _clearresult(self, result):
        name = result['name']
        code = result['status']
        template = '%s: ERROR (%s)'
        if code == xmlrpc.Faults.BAD_NAME:
            return template % (name, 'no such process')
        elif code == xmlrpc.Faults.FAILED:
            return template % (name, 'failed')
        elif code == xmlrpc.Faults.SUCCESS:
            return '%s: cleared' % name
        raise ValueError('Unknown result code %s for %s' % (code, name))

    def do_clear(self, arg):
        if not self.ctl.upcheck():
            return

        names = arg.strip().split()

        if not names:
            self.ctl.output('Error: clear requires a process name')
            self.help_clear()
            return

        supervisor = self.ctl.get_supervisor()

        if 'all' in names:
            results = supervisor.clearAllProcessLogs()
            for result in results:
                result = self._clearresult(result)
                self.ctl.output(result)

        else:

            for name in names:
                try:
                    result = supervisor.clearProcessLogs(name)
                except xmlrpclib.Fault, e:
                    error = self._clearresult({'status':e.faultCode,
                                               'name':name,
                                               'description':e.faultString})
                    self.ctl.output(error)
                else:
                    self.ctl.output('%s: cleared' % name)

    def help_clear(self):
        self.ctl.output("clear <name>\t\tClear a process' log files.")
        self.ctl.output(
            "clear <name> <name>\tClear multiple process' log files")
        self.ctl.output("clear all\t\tClear all process' log files")

    def do_open(self, arg):
        url = arg.strip()
        parts = urlparse.urlparse(url)
        if parts[0] not in ('unix', 'http'):
            self.ctl.output('ERROR: url must be http:// or unix://')
            return
        self.ctl.options.serverurl = url
        self.do_status('')

    def help_open(self):
        self.ctl.output("open <url>\tConnect to a remote supervisord process.")
        self.ctl.output("\t\t(for UNIX domain socket, use unix:///socket/path)")

    def do_version(self, arg):
        if not self.ctl.upcheck():
            return
        supervisor = self.ctl.get_supervisor()
        self.ctl.output(supervisor.getSupervisorVersion())

    def help_version(self):
        self.ctl.output(
            "version\t\t\tShow the version of the remote supervisord "
            "process")

    def do_fg(self,args=None):
        if not self.ctl.upcheck():
            return
        if not args:
            self.ctl.output('Error: no process name supplied')
            self.help_fg()
            return
        args = args.split()
        if len(args) > 1:
            self.ctl.output('Error: too many process names supplied')
            return
        program = args[0]
        supervisor = self.ctl.get_supervisor()
        try:
            info = supervisor.getProcessInfo(program)
        except xmlrpclib.Fault, msg:
            if msg.faultCode == xmlrpc.Faults.BAD_NAME:
                self.ctl.output('Error: bad process name supplied')
                return
            # for any other fault
            self.ctl.output(str(msg))
            return
        if not info['state'] == states.ProcessStates.RUNNING:
            self.ctl.output('Error: process not running')
            return
        # everything good; continue
        try:
            a = fgthread(program,self.ctl)
            # this thread takes care of
            # the output/error messages
            a.start()
            while True:
                # this takes care of the user input
                inp = raw_input() + '\n'
                try:
                    supervisor.sendProcessStdin(program, inp)
                except xmlrpclib.Fault, msg:
                    if msg.faultCode == xmlrpc.Faults.NOT_RUNNING:
                        self.ctl.output('Process got killed')
                        self.ctl.output('Exiting foreground')
                        a.kill()
                        return
                info = supervisor.getProcessInfo(program)
                if not info['state'] == states.ProcessStates.RUNNING:
                    self.ctl.output('Process got killed')
                    self.ctl.output('Exiting foreground')
                    a.kill()
                    return
                continue
        except (KeyboardInterrupt, EOFError):
            a.kill()
            self.ctl.output('Exiting foreground')
        return

    def help_fg(self,args=None):
        self.ctl.output('fg <process>\tConnect to a process in foreground mode')
        self.ctl.output('Press Ctrl+C to exit foreground')

def main(args=None, options=None):
    if options is None:
        options = ClientOptions()
    options.realize(args, doc=__doc__)
    c = Controller(options)
    if options.args:
        c.onecmd(" ".join(options.args))
    if options.interactive:
        try:
            import readline
            if options.history_file:
                try:
                    readline.read_history_file(options.history_file)
                except IOError:
                    pass
                def save():
                    try:
                        readline.write_history_file(options.history_file)
                    except IOError:
                        pass
                import atexit
                atexit.register(save)
        except ImportError:
            pass
        try:
            c.cmdqueue.append('status')
            c.cmdloop()
        except KeyboardInterrupt:
            c.output('')
            pass

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = supervisord
#!/usr/bin/env python

"""supervisord -- run a set of applications as daemons.

Usage: %s [options]

Options:
-c/--configuration FILENAME -- configuration file
-n/--nodaemon -- run in the foreground (same as 'nodaemon true' in config file)
-h/--help -- print this usage message and exit
-v/--version -- print supervisord version number and exit
-u/--user USER -- run supervisord as this user (or numeric uid)
-m/--umask UMASK -- use this umask for daemon subprocess (default is 022)
-d/--directory DIRECTORY -- directory to chdir to when daemonized
-l/--logfile FILENAME -- use FILENAME as logfile path
-y/--logfile_maxbytes BYTES -- use BYTES to limit the max size of logfile
-z/--logfile_backups NUM -- number of backups to keep when max bytes reached
-e/--loglevel LEVEL -- use LEVEL as log level (debug,info,warn,error,critical)
-j/--pidfile FILENAME -- write a pid file for the daemon process to FILENAME
-i/--identifier STR -- identifier used for this instance of supervisord
-q/--childlogdir DIRECTORY -- the log directory for child process logs
-k/--nocleanup --  prevent the process from performing cleanup (removal of
                   old automatic child log files) at startup.
-a/--minfds NUM -- the minimum number of file descriptors for start success
-t/--strip_ansi -- strip ansi escape codes from process output
--minprocs NUM  -- the minimum number of processes available for start success
--profile_options OPTIONS -- run supervisord under profiler and output
                             results based on OPTIONS, which  is a comma-sep'd
                             list of 'cumulative', 'calls', and/or 'callers',
                             e.g. 'cumulative,callers')
"""

import os
import time
import errno
import select
import signal

from supervisor.medusa import asyncore_25 as asyncore

from supervisor.options import ServerOptions
from supervisor.options import signame
from supervisor import events
from supervisor.states import SupervisorStates
from supervisor.states import getProcessStateDescription

class Supervisor:
    stopping = False # set after we detect that we are handling a stop request
    lastshutdownreport = 0 # throttle for delayed process error reports at stop
    process_groups = None # map of process group name to process group object
    stop_groups = None # list used for priority ordered shutdown

    def __init__(self, options):
        self.options = options
        self.process_groups = {}
        self.ticks = {}

    def main(self):
        if not self.options.first:
            # prevent crash on libdispatch-based systems, at least for the
            # first request
            self.options.cleanup_fds()
        info_messages = []
        critical_messages = []
        warn_messages = []
        setuid_msg = self.options.set_uid()
        if setuid_msg:
            critical_messages.append(setuid_msg)
        if self.options.first:
            rlimit_messages = self.options.set_rlimits()
            info_messages.extend(rlimit_messages)
        warn_messages.extend(self.options.parse_warnings)

        # this sets the options.logger object
        # delay logger instantiation until after setuid
        self.options.make_logger(critical_messages, warn_messages,
                                 info_messages)

        if not self.options.nocleanup:
            # clean up old automatic logs
            self.options.clear_autochildlogdir()

        self.run()

    def run(self):
        self.process_groups = {} # clear
        self.stop_groups = None # clear
        events.clear()
        try:
            for config in self.options.process_group_configs:
                self.add_process_group(config)
            self.options.process_environment()
            self.options.openhttpservers(self)
            self.options.setsignals()
            if (not self.options.nodaemon) and self.options.first:
                self.options.daemonize()
            # writing pid file needs to come *after* daemonizing or pid
            # will be wrong
            self.options.write_pidfile()
            self.options.load_subproc_pidfile(self.process_groups)
            self.runforever()
        finally:
            self.options.cleanup()

    def diff_to_active(self, new=None):
        if not new:
            new = self.options.process_group_configs
        cur = [group.config for group in self.process_groups.values()]

        curdict = dict(zip([cfg.name for cfg in cur], cur))
        newdict = dict(zip([cfg.name for cfg in new], new))

        added   = [cand for cand in new if cand.name not in curdict]
        removed = [cand for cand in cur if cand.name not in newdict]

        changed = [cand for cand in new
                   if cand != curdict.get(cand.name, cand)]

        return added, changed, removed

    def add_process_group(self, config):
        name = config.name
        if name not in self.process_groups:
            config.after_setuid()
            self.process_groups[name] = config.make_group()
            return True
        return False

    def remove_process_group(self, name):
        if self.process_groups[name].get_unstopped_processes():
            return False
        del self.process_groups[name]
        return True

    def get_process_map(self):
        process_map = {}
        pgroups = self.process_groups.values()
        for group in pgroups:
            process_map.update(group.get_dispatchers())
        return process_map

    def shutdown_report(self):
        unstopped = []

        pgroups = self.process_groups.values()
        for group in pgroups:
            unstopped.extend(group.get_unstopped_processes())

        if unstopped:
            # throttle 'waiting for x to die' reports
            now = time.time()
            if now > (self.lastshutdownreport + 3): # every 3 secs
                names = [ p.config.name for p in unstopped ]
                namestr = ', '.join(names)
                self.options.logger.info('waiting for %s to die' % namestr)
                self.lastshutdownreport = now
                for proc in unstopped:
                    state = getProcessStateDescription(proc.get_state())
                    self.options.logger.blather(
                        '%s state: %s' % (proc.config.name, state))
        return unstopped

    def ordered_stop_groups_phase_1(self):
        if self.stop_groups:
            # stop the last group (the one with the "highest" priority)
            self.stop_groups[-1].stop_all()

    def ordered_stop_groups_phase_2(self):
        # after phase 1 we've transitioned and reaped, let's see if we
        # can remove the group we stopped from the stop_groups queue.
        if self.stop_groups:
            # pop the last group (the one with the "highest" priority)
            group = self.stop_groups.pop()
            if group.get_unstopped_processes():
                # if any processes in the group aren't yet in a
                # stopped state, we're not yet done shutting this
                # group down, so push it back on to the end of the
                # stop group queue
                self.stop_groups.append(group)

    def runforever(self):
        events.notify(events.SupervisorRunningEvent())
        timeout = 1 # this cannot be fewer than the smallest TickEvent (5)

        socket_map = self.options.get_socket_map()

        while 1:
            combined_map = {}
            combined_map.update(socket_map)
            combined_map.update(self.get_process_map())

            pgroups = self.process_groups.values()
            pgroups.sort()

            if self.options.mood < SupervisorStates.RUNNING:
                if self.options.subprocpidfile:
                    # 'subprocpidfile' option is set, which implies that all
                    # managed sub-processes should NOT be killed and would
                    # continue to run even supervisord exits. the supervisord
                    # would also continue to manage these sub-processes after
                    # it restarts.
                    self.options.logger.info('exiting without killing managed '
                                             'sub-processes')
                    raise asyncore.ExitNow

                if not self.stopping:
                    # first time, set the stopping flag, do a
                    # notification and set stop_groups
                    self.stopping = True
                    self.stop_groups = pgroups[:]
                    events.notify(events.SupervisorStoppingEvent())

                self.ordered_stop_groups_phase_1()

                if not self.shutdown_report():
                    # if there are no unstopped processes (we're done
                    # killing everything), it's OK to swtop or reload
                    raise asyncore.ExitNow

            r, w, x = [], [], []

            for fd, dispatcher in combined_map.items():
                if dispatcher.readable():
                    r.append(fd)
                if dispatcher.writable():
                    w.append(fd)

            try:
                r, w, x = self.options.select(r, w, x, timeout)
            except select.error, err:
                r = w = x = []
                if err[0] == errno.EINTR:
                    self.options.logger.blather('EINTR encountered in select')
                else:
                    raise

            for fd in r:
                if combined_map.has_key(fd):
                    try:
                        dispatcher = combined_map[fd]
                        self.options.logger.blather(
                            'read event caused by %(dispatcher)s',
                            dispatcher=dispatcher)
                        dispatcher.handle_read_event()
                    except asyncore.ExitNow:
                        raise
                    except:
                        combined_map[fd].handle_error()

            for fd in w:
                if combined_map.has_key(fd):
                    try:
                        dispatcher = combined_map[fd]
                        self.options.logger.blather(
                            'write event caused by %(dispatcher)s',
                            dispatcher=dispatcher)
                        dispatcher.handle_write_event()
                    except asyncore.ExitNow:
                        raise
                    except:
                        combined_map[fd].handle_error()

            [ group.transition() for group  in pgroups ]

            self.reap()
            self.handle_signal()
            self.tick()

            if self.options.mood < SupervisorStates.RUNNING:
                self.ordered_stop_groups_phase_2()

            if self.options.test:
                break

    def tick(self, now=None):
        """ Send one or more 'tick' events when the timeslice related to
        the period for the event type rolls over """
        if now is None:
            # now won't be None in unit tests
            now = time.time()
        for event in events.TICK_EVENTS:
            period = event.period
            last_tick = self.ticks.get(period)
            if last_tick is None:
                # we just started up
                last_tick = self.ticks[period] = timeslice(period, now)
            this_tick = timeslice(period, now)
            if this_tick != last_tick:
                self.ticks[period] = this_tick
                events.notify(event(this_tick, self))

    def reap(self, once=False):
        pid, sts = self.options.waitpid()
        if pid:
            process = self.options.get_process(pid)
            if process is None:
                self.options.logger.critical('reaped unknown pid %s)' % pid)
            else:
                process.finish(pid, sts)
                self.options.del_process(pid)
            if not once:
                self.reap() # keep reaping until no more kids to reap

    def handle_signal(self):
        sig = self.options.get_signal()
        if sig:
            if sig in (signal.SIGTERM, signal.SIGINT, signal.SIGQUIT):
                self.options.logger.warn(
                    'received %s indicating exit request' % signame(sig))
                self.options.mood = SupervisorStates.SHUTDOWN
            elif sig == signal.SIGHUP:
                self.options.logger.warn(
                    'received %s indicating restart request' % signame(sig))
                self.options.mood = SupervisorStates.RESTARTING
            elif sig == signal.SIGCHLD:
                self.options.logger.debug(
                    'received %s indicating a child quit' % signame(sig))
            elif sig == signal.SIGUSR2:
                self.options.logger.info(
                    'received %s indicating log reopen request' % signame(sig))
                self.options.reopenlogs()
                for group in self.process_groups.values():
                    group.reopenlogs()
            else:
                self.options.logger.blather(
                    'received %s indicating nothing' % signame(sig))

    def get_state(self):
        return self.options.mood

def timeslice(period, when):
    return int(when - (when % period))

# profile entry point
def profile(cmd, globals, locals, sort_order, callers):
    try:
        import cProfile as profile
    except ImportError:
        import profile # python < 2.5
    import pstats
    import tempfile
    fd, fn = tempfile.mkstemp()
    try:
        profile.runctx(cmd, globals, locals, fn)
        stats = pstats.Stats(fn)
        stats.strip_dirs()
        # calls,time,cumulative and cumulative,calls,time are useful
        stats.sort_stats(*sort_order or ('cumulative', 'calls', 'time'))
        if callers:
            stats.print_callers(.3)
        else:
            stats.print_stats(.3)
    finally:
        os.remove(fn)


# Main program
def main(args=None, test=False):
    assert os.name == "posix", "This code makes Unix-specific assumptions"
    # if we hup, restart by making a new Supervisor()
    first = True
    while 1:
        options = ServerOptions()
        options.realize(args, doc=__doc__)
        options.first = first
        options.test = test
        if options.profile_options:
            sort_order, callers = options.profile_options
            profile('go(options)', globals(), locals(), sort_order, callers)
        else:
            go(options)
        if test or (options.mood < SupervisorStates.RESTARTING):
            break
        options.close_httpservers()
        options.close_logger()
        first = False

def go(options):
    d = Supervisor(options)
    try:
        d.main()
    except asyncore.ExitNow:
        pass

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = base
_NOW = 1151365354
_TIMEFORMAT = '%b %d %I:%M %p'

class DummyOptions:

    make_pipes_error = None
    fork_error = None
    execv_error = None
    kill_error = None
    minfds = 5
    loglevel = 20

    def __init__(self):
        self.identifier = 'supervisor'
        self.childlogdir = '/tmp'
        self.uid = 999
        self.logger = self.getLogger()
        self.backofflimit = 10
        self.logfile = '/tmp/logfile'
        self.nocleanup = False
        self.strip_ansi = False
        self.pidhistory = {}
        self.process_group_configs = []
        self.nodaemon = False
        self.socket_map = {}
        self.mood = 1
        self.mustreopen = False
        self.realizeargs = None
        self.fds_cleaned_up = False
        self.rlimit_set = False
        self.setuid_called = False
        self.httpservers_opened = False
        self.signals_set = False
        self.daemonized = False
        self.make_logger_messages = None
        self.autochildlogdir_cleared = False
        self.cleaned_up = False
        self.pidfile_written = False
        self.directory = None
        self.waitpid_return = None, None
        self.kills = {}
        self._signal = None
        self.parent_pipes_closed = None
        self.child_pipes_closed = None
        self.forkpid = 0
        self.pgrp_set = None
        self.duped = {}
        self.written = {}
        self.fds_closed = []
        self._exitcode = None
        self.execv_args = None
        self.setuid_msg = None
        self.privsdropped = None
        self.logs_reopened = False
        self.environment_processed = False
        self.select_result = [], [], []
        self.select_error = None
        self.write_accept = None
        self.write_error = None
        self.tempfile_name = '/foo/bar'
        self.remove_error = None
        self.removed = []
        self.existing = []
        self.openreturn = None
        self.readfd_result = ''
        self.parse_warnings = []
        self.serverurl = 'http://localhost:9001'
        self.changed_directory = False
        self.chdir_error = None
        self.umaskset = None

    def getLogger(self, *args, **kw):
        logger = DummyLogger()
        logger.handlers = [DummyLogger()]
        logger.args = args, kw
        return logger

    def realize(self, args, **kw):
        self.realizeargs = args
        self.realizekw = kw

    def process_config_file(self, do_usage=True):
        pass

    def cleanup_fds(self):
        self.fds_cleaned_up = True

    def set_rlimits(self):
        self.rlimits_set = True
        return ['rlimits_set']

    def set_uid(self):
        self.setuid_called = True
        return 'setuid_called'

    def openhttpservers(self, supervisord):
        self.httpservers_opened = True

    def daemonize(self):
        self.daemonized = True

    def setsignals(self):
        self.signals_set = True

    def get_signal(self):
        return self._signal

    def get_socket_map(self):
        return self.socket_map

    def make_logger(self, critical_msgs, warn_msgs, info_msgs):
        self.make_logger_messages = critical_msgs, warn_msgs, info_msgs

    def clear_autochildlogdir(self):
        self.autochildlogdir_cleared = True

    def get_autochildlog_name(self, *ignored):
        return self.tempfile_name

    def cleanup(self):
        self.cleaned_up = True

    def write_pidfile(self):
        self.pidfile_written = True

    def waitpid(self):
        return self.waitpid_return

    def kill(self, pid, sig):
        if self.kill_error:
            raise OSError(self.kill_error)
        self.kills[pid] = sig

    def stat(self, filename):
        import os
        return os.stat(filename)

    def get_path(self):
        return ["/bin", "/usr/bin", "/usr/local/bin"]

    def get_pid(self):
        import os
        return os.getpid()
        
    def check_execv_args(self, filename, argv, st):
        if filename == '/bad/filename':
            from supervisor.options import NotFound
            raise NotFound('bad filename')

    def make_pipes(self, stderr=True):
        if self.make_pipes_error:
            raise OSError(self.make_pipes_error)
        pipes = {}
        pipes['child_stdin'], pipes['stdin'] = (3, 4)
        pipes['stdout'], pipes['child_stdout'] = (5, 6)
        if stderr:
            pipes['stderr'], pipes['child_stderr'] = (7, 8)
        else:
            pipes['stderr'], pipes['child_stderr'] = None, None
        return pipes

    def write(self, fd, chars):
        if self.write_error:
            raise OSError(self.write_error)
        if self.write_accept:
            chars = chars[self.write_accept]
        data = self.written.setdefault(fd, '')
        data += chars
        self.written[fd] = data
        return len(chars)

    def fork(self):
        if self.fork_error:
            raise OSError(self.fork_error)
        return self.forkpid

    def close_fd(self, fd):
        self.fds_closed.append(fd)

    def close_parent_pipes(self, pipes):
        self.parent_pipes_closed = pipes

    def close_child_pipes(self, pipes):
        self.child_pipes_closed = pipes

    def setpgrp(self):
        self.pgrp_set = True

    def dup2(self, frm, to):
        self.duped[frm] = to

    def _exit(self, code):
        self._exitcode = code

    def execve(self, filename, argv, environment):
        if self.execv_error:
            if self.execv_error == 1:
                raise OSError(self.execv_error)
            else:
                raise RuntimeError(self.execv_error)
        self.execv_args = (filename, argv)
        self.execv_environment = environment

    def dropPrivileges(self, uid):
        if self.setuid_msg:
            return self.setuid_msg
        self.privsdropped = uid

    def readfd(self, fd):
        return self.readfd_result

    def reopenlogs(self):
        self.logs_reopened = True

    def process_environment(self):
        self.environment_processed = True

    def mktempfile(self, prefix, suffix, dir):
        return self.tempfile_name

    def select(self, r, w, x, timeout):
        import select
        if self.select_error:
            raise select.error(self.select_error)
        return self.select_result

    def remove(self, path):
        import os
        if self.remove_error:
            raise os.error(self.remove_error)
        self.removed.append(path)

    def exists(self, path):
        if path in self.existing:
            return True
        return False

    def open(self, name, mode='r'):
        if self.openreturn:
            return self.openreturn
        return open(name, mode)

    def chdir(self, dir):
        if self.chdir_error:
            raise OSError(self.chdir_error)
        self.changed_directory = True

    def setumask(self, mask):
        self.umaskset = mask

class DummyLogger:
    def __init__(self):
        self.reopened = False
        self.removed = False
        self.closed = False
        self.data = []

    def info(self, msg, **kw):
        if kw:
            msg = msg % kw
        self.data.append(msg)
    warn = debug = critical = trace = error = blather = info

    def log(self, level, msg, **kw):
        if kw:
            msg = msg % kw
        self.data.append(msg)
        
    def reopen(self):
        self.reopened = True
    def close(self):
        self.closed = True
    def remove(self):
        self.removed = True
    def flush(self):
        self.flushed = True
    def getvalue(self):
        return ''.join(self.data)

class DummySupervisor:
    def __init__(self, options=None, state=None, process_groups=None):
        if options is None:
            self.options = DummyOptions()
        else:
            self.options = options
        if state is None:
            from supervisor.supervisord import SupervisorStates
            self.options.mood = SupervisorStates.RUNNING
        else:
            self.options.mood = state
        if process_groups is None:
            self.process_groups = {}
        else:
            self.process_groups = process_groups

    def get_state(self):
        return self.options.mood

class DummySocket:
    bind_called = False
    bind_addr = None
    listen_called = False
    listen_backlog = None
    close_called = False

    def __init__(self, fd):
        self.fd = fd

    def fileno(self):
        return self.fd

    def bind(self, addr):
        self.bind_called = True
        self.bind_addr = addr

    def listen(self, backlog):
        self.listen_called = True
        self.listen_backlog = backlog

    def close(self):
        self.close_called = True

    def __str__(self):
        return 'dummy socket'

class DummySocketConfig:
    def __init__(self, fd):
        self.fd = fd

    def addr(self):
        return 'dummy addr'

    def __eq__(self, other):
        return self.fd == other.fd

    def __ne__(self, other):
        return not self.__eq__(other)

    def create_and_bind(self):
        return DummySocket(self.fd)

class DummySocketManager:
    def __init__(self, config, **kwargs):
        self._config = config

    def config(self):
        return self._config

    def get_socket(self):
        return DummySocket(self._config.fd)
        
class DummyProcess:
    # Initial state; overridden by instance variables
    pid = 0 # Subprocess pid; 0 when not running
    laststart = 0 # Last time the subprocess was started; 0 if never
    laststop = 0  # Last time the subprocess was stopped; 0 if never
    delay = 0 # If nonzero, delay starting or killing until this time
    administrative_stop = 0 # true if the process has been stopped by an admin
    system_stop = 0 # true if the process has been stopped by the system
    killing = 0 # flag determining whether we are trying to kill this proc
    backoff = 0 # backoff counter (to backofflimit)
    waitstatus = None
    exitstatus = None
    pipes = None
    rpipes = None
    dispatchers = None
    stdout_logged = ''
    stderr_logged = ''
    spawnerr = None
    stdout_buffer = '' # buffer of characters from child stdout output to log
    stderr_buffer = '' # buffer of characters from child stderr output to log
    stdin_buffer = '' # buffer of characters to send to child process' stdin
    listener_state = None
    group = None

    def __init__(self, config, state=None):
        self.config = config
        self.logsremoved = False
        self.stop_called = False
        self.backoff_secs = None
        self.spawned = False
        if state is None:
            from supervisor.process import ProcessStates
            state = ProcessStates.RUNNING
        self.state = state
        self.error_at_clear = False
        self.killed_with = None
        self.drained = False
        self.stdout_buffer = ''
        self.stderr_buffer = ''
        self.stdout_logged = ''
        self.stderr_logged = ''
        self.stdin_buffer = ''
        self.pipes = {}
        self.rpipes = {}
        self.dispatchers = {}
        self.finished = None
        self.logs_reopened = False
        self.execv_arg_exception = None
        self.input_fd_drained = None
        self.output_fd_drained = None
        self.transitioned = False
        self.write_error = None

    def reopenlogs(self):
        self.logs_reopened = True

    def removelogs(self):
        if self.error_at_clear:
            raise IOError('whatever')
        self.logsremoved = True

    def get_state(self):
        return self.state

    def stop(self):
        self.stop_called = True
        self.killing = False
        from supervisor.process import ProcessStates
        self.state = ProcessStates.STOPPED

    def kill(self, signal):
        self.killed_with = signal

    def spawn(self):
        self.spawned = True
        from supervisor.process import ProcessStates
        self.state = ProcessStates.RUNNING

    def drain(self):
        self.drained = True

    def __cmp__(self, other):
        return cmp(self.config.priority, other.config.priority)

    def readable_fds(self):
        return []

    def record_output(self):
        self.stdout_logged += self.stdout_buffer
        self.stdout_buffer = ''

        self.stderr_logged += self.stderr_buffer
        self.stderr_buffer = ''

    def finish(self, pid, sts):
        self.finished = pid, sts

    def give_up(self):
        from supervisor.process import ProcessStates
        self.state = ProcessStates.FATAL

    def get_execv_args(self):
        if self.execv_arg_exception:
            raise self.execv_arg_exception('whatever')
        import shlex
        commandargs = shlex.split(self.config.command)
        program = commandargs[0]
        return program, commandargs

    def drain_output_fd(self, fd):
        self.output_fd_drained = fd

    def drain_input_fd(self, fd):
        self.input_fd_drained = fd

    def write(self, chars):
        if self.write_error:
            raise OSError(self.write_error)
        self.stdin_buffer += chars

    def transition(self):
        self.transitioned = True

class DummyPConfig:
    def __init__(self, options, name, command, directory=None, umask=None,
                 priority=999, autostart=True,
                 autorestart=True, startsecs=10, startretries=999,
                 uid=None, stdout_logfile=None, stdout_capture_maxbytes=0,
                 stdout_events_enabled=False,
                 stdout_logfile_backups=0, stdout_logfile_maxbytes=0,
                 stderr_logfile=None, stderr_capture_maxbytes=0,
                 stderr_events_enabled=False,
                 stderr_logfile_backups=0, stderr_logfile_maxbytes=0,
                 redirect_stderr=False,
                 stopsignal=None, stopwaitsecs=10, stopasgroup=False, killasgroup=False,
                 exitcodes=(0,2), environment=None, serverurl=None):
        self.options = options
        self.name = name
        self.command = command
        self.priority = priority
        self.autostart = autostart
        self.autorestart = autorestart
        self.startsecs = startsecs
        self.startretries = startretries
        self.uid = uid
        self.stdout_logfile = stdout_logfile
        self.stdout_capture_maxbytes = stdout_capture_maxbytes
        self.stdout_events_enabled = stdout_events_enabled
        self.stdout_logfile_backups = stdout_logfile_backups
        self.stdout_logfile_maxbytes = stdout_logfile_maxbytes
        self.stderr_logfile = stderr_logfile
        self.stderr_capture_maxbytes = stderr_capture_maxbytes
        self.stderr_events_enabled = stderr_events_enabled
        self.stderr_logfile_backups = stderr_logfile_backups
        self.stderr_logfile_maxbytes = stderr_logfile_maxbytes
        self.redirect_stderr = redirect_stderr
        if stopsignal is None:
            import signal
            stopsignal = signal.SIGTERM
        self.stopsignal = stopsignal
        self.stopwaitsecs = stopwaitsecs
        self.stopasgroup = stopasgroup
        self.killasgroup = killasgroup
        self.exitcodes = exitcodes
        self.environment = environment
        self.directory = directory
        self.umask = umask
        self.autochildlogs_created = False
        self.serverurl = serverurl

    def create_autochildlogs(self):
        self.autochildlogs_created = True

    def make_process(self, group=None):
        process = DummyProcess(self)
        process.group = group
        return process

    def make_dispatchers(self, proc):
        use_stderr = not self.redirect_stderr
        pipes = self.options.make_pipes(use_stderr)
        stdout_fd,stderr_fd,stdin_fd = (pipes['stdout'],pipes['stderr'],
                                        pipes['stdin'])
        dispatchers = {}
        if stdout_fd is not None:
            dispatchers[stdout_fd] = DummyDispatcher(readable=True)
        if stderr_fd is not None:
            dispatchers[stderr_fd] = DummyDispatcher(readable=True)
        if stdin_fd is not None:
            dispatchers[stdin_fd] = DummyDispatcher(writable=True)
        return dispatchers, pipes

def makeExecutable(file, substitutions=None):
    import os
    import sys
    import tempfile
    
    if substitutions is None:
        substitutions = {}
    data = open(file).read()
    last = os.path.split(file)[1]

    substitutions['PYTHON'] = sys.executable
    for key in substitutions.keys():
        data = data.replace('<<%s>>' % key.upper(), substitutions[key])
    
    tmpnam = tempfile.mktemp(prefix=last)
    f = open(tmpnam, 'w')
    f.write(data)
    f.close()
    os.chmod(tmpnam, 0755)
    return tmpnam

def makeSpew(unkillable=False):
    import os
    here = os.path.dirname(__file__)
    if not unkillable:
        return makeExecutable(os.path.join(here, 'fixtures/spew.py'))
    return makeExecutable(os.path.join(here, 'fixtures/unkillable_spew.py'))

class DummyMedusaServerLogger:
    def __init__(self):
        self.logged = []
    def log(self, category, msg):
        self.logged.append((category, msg))

class DummyMedusaServer:
    def __init__(self):
        self.logger = DummyMedusaServerLogger()

class DummyMedusaChannel:
    def __init__(self):
        self.server = DummyMedusaServer()
        self.producer = None

    def push_with_producer(self, producer):
        self.producer = producer

    def close_when_done(self):
        pass

class DummyRequest:
    command = 'GET'
    _error = None
    _done = False
    version = '1.0'
    def __init__(self, path, params, query, fragment, env=None):
        self.args = path, params, query, fragment
        self.producers = []
        self.headers = {}
        self.header = []
        self.outgoing = []
        self.channel = DummyMedusaChannel()
        if env is None:
            self.env = {}
        else:
            self.env = env

    def split_uri(self):
        return self.args

    def error(self, code):
        self._error = code

    def push(self, producer):
        self.producers.append(producer)

    def __setitem__(self, header, value):
        self.headers[header] = value

    def has_key(self, header):
        return self.headers.has_key(header)

    def done(self):
        self._done = True

    def build_reply_header(self):
        return ''

    def log(self, *arg, **kw):
        pass

    def cgi_environment(self):
        return self.env

    def get_server_url(self):
        return 'http://example.com'
        

class DummyRPCInterfaceFactory:
    def __init__(self, supervisord, **config):
        self.supervisord = supervisord
        self.config = config

class DummyRPCServer:
    def __init__(self):
        self.supervisor = DummySupervisorRPCNamespace()
        self.system = DummySystemRPCNamespace()

class DummySystemRPCNamespace:
    pass

class DummySupervisorRPCNamespace:
    _restartable = True
    _restarted = False
    _shutdown = False
    _readlog_error = False


    from supervisor.process import ProcessStates
    all_process_info = [
        {
        'name':'foo',
        'group':'foo',
        'pid':11,
        'state':ProcessStates.RUNNING,
        'statename':'RUNNING',
        'start':_NOW - 100,
        'stop':0,
        'spawnerr':'',
        'now':_NOW,
        'description':'foo description',
        },
        {
        'name':'bar',
        'group':'bar',
        'pid':12,
        'state':ProcessStates.FATAL,
        'statename':'FATAL',
        'start':_NOW - 100,
        'stop':_NOW - 50,
        'spawnerr':'screwed',
        'now':_NOW,
        'description':'bar description',
        },
        {
        'name':'baz_01',
        'group':'baz',
        'pid':13,
        'state':ProcessStates.STOPPED,
        'statename':'STOPPED',
        'start':_NOW - 100,
        'stop':_NOW - 25,
        'spawnerr':'',
        'now':_NOW,
        'description':'baz description',
        },
        ]

    def getAPIVersion(self):
        return '3.0'

    getVersion = getAPIVersion # deprecated

    def getPID(self):
        return 42

    def readProcessStdoutLog(self, name, offset, length):
        from supervisor import xmlrpc
        import xmlrpclib
        if name == 'BAD_NAME':
            raise xmlrpclib.Fault(xmlrpc.Faults.BAD_NAME, 'BAD_NAME')
        elif name == 'FAILED':
            raise xmlrpclib.Fault(xmlrpc.Faults.FAILED, 'FAILED')
        elif name == 'NO_FILE':
            raise xmlrpclib.Fault(xmlrpc.Faults.NO_FILE, 'NO_FILE')
        a = 'output line\n' * 10
        return a[offset:]

    readProcessLog = readProcessStdoutLog
    readProcessStderrLog = readProcessStdoutLog

    def getAllProcessInfo(self):
        return self.all_process_info

    def getProcessInfo(self, name):
        from supervisor import xmlrpc
        import xmlrpclib
        from supervisor.process import ProcessStates
        for i in self.all_process_info:
            if i['name']==name:
                info=i
                return info
        if name == 'BAD_NAME':
            raise xmlrpclib.Fault(xmlrpc.Faults.BAD_NAME, 'BAD_NAME')
        if name == 'FAILED':
            raise xmlrpclib.Fault(xmlrpc.Faults.FAILED, 'FAILED')
        if name == 'NO_FILE':
            raise xmlrpclib.Fault(xmlrpc.Faults.NO_FILE, 'NO_FILE')

    def startProcess(self, name):
        from supervisor import xmlrpc
        from xmlrpclib import Fault
        if name == 'BAD_NAME:BAD_NAME':
            raise Fault(xmlrpc.Faults.BAD_NAME, 'BAD_NAME:BAD_NAME')
        if name == 'BAD_NAME':
            raise Fault(xmlrpc.Faults.BAD_NAME, 'BAD_NAME')
        if name == 'NO_FILE':
            raise Fault(xmlrpc.Faults.NO_FILE, 'NO_FILE')
        if name == 'NOT_EXECUTABLE':
            raise Fault(xmlrpc.Faults.NOT_EXECUTABLE, 'NOT_EXECUTABLE')
        if name == 'ALREADY_STARTED':
            raise Fault(xmlrpc.Faults.ALREADY_STARTED, 'ALREADY_STARTED')
        if name == 'SPAWN_ERROR':
            raise Fault(xmlrpc.Faults.SPAWN_ERROR, 'SPAWN_ERROR')
        return True

    def startProcessGroup(self, name):
        from supervisor import xmlrpc
        return [
            {'name':'foo_00', 'group':'foo',
             'status': xmlrpc.Faults.SUCCESS,
             'description': 'OK'},
            {'name':'foo_01', 'group':'foo',
             'status':xmlrpc.Faults.SUCCESS,
             'description': 'OK'},
            ]

    def startAllProcesses(self):
        from supervisor import xmlrpc
        return [
            {'name':'foo', 'group':'foo',
             'status': xmlrpc.Faults.SUCCESS,
             'description': 'OK'},
            {'name':'foo2', 'group':'foo2',
             'status':xmlrpc.Faults.SUCCESS,
             'description': 'OK'},
            {'name':'failed', 'group':'failed_group',
             'status':xmlrpc.Faults.SPAWN_ERROR,
             'description':'SPAWN_ERROR'}
            ]

    def stopProcessGroup(self, name):
        from supervisor import xmlrpc
        return [
            {'name':'foo_00', 'group':'foo',
             'status': xmlrpc.Faults.SUCCESS,
             'description': 'OK'},
            {'name':'foo_01', 'group':'foo',
             'status':xmlrpc.Faults.SUCCESS,
             'description': 'OK'},
            ]

    def stopProcess(self, name):
        from supervisor import xmlrpc
        from xmlrpclib import Fault
        if name == 'BAD_NAME:BAD_NAME':
            raise Fault(xmlrpc.Faults.BAD_NAME, 'BAD_NAME:BAD_NAME')
        if name == 'BAD_NAME':
            raise Fault(xmlrpc.Faults.BAD_NAME, 'BAD_NAME')
        if name == 'NOT_RUNNING':
            raise Fault(xmlrpc.Faults.NOT_RUNNING, 'NOT_RUNNING')
        if name == 'FAILED':
            raise Fault(xmlrpc.Faults.FAILED, 'FAILED')
        
        return True
    
    def stopAllProcesses(self):
        from supervisor import xmlrpc
        return [
            {'name':'foo','group':'foo',
             'status': xmlrpc.Faults.SUCCESS,
             'description': 'OK'},
            {'name':'foo2', 'group':'foo2',
             'status':xmlrpc.Faults.SUCCESS,'description': 'OK'},
            {'name':'failed', 'group':'failed_group',
             'status':xmlrpc.Faults.BAD_NAME,
             'description':'FAILED'}
            ]

    def restart(self):
        if self._restartable:
            self._restarted = True
            return
        from xmlrpclib import Fault
        from supervisor import xmlrpc
        raise Fault(xmlrpc.Faults.SHUTDOWN_STATE, '')

    def shutdown(self):
        if self._restartable:
            self._shutdown = True
            return
        from xmlrpclib import Fault
        from supervisor import xmlrpc
        raise Fault(xmlrpc.Faults.SHUTDOWN_STATE, '')

    def reloadConfig(self):
        return [[['added'], ['changed'], ['removed']]]

    def addProcessGroup(self, name):
        from xmlrpclib import Fault
        from supervisor import xmlrpc
        if name == 'ALREADY_ADDED':
            raise Fault(xmlrpc.Faults.ALREADY_ADDED, '')
        if name == 'BAD_NAME':
            raise Fault(xmlrpc.Faults.BAD_NAME, '')
        if hasattr(self, 'processes'):
            self.processes.append(name)
        else:
            self.processes = [name]

    def removeProcessGroup(self, name):
        from xmlrpclib import Fault
        from supervisor import xmlrpc
        if name == 'STILL_RUNNING':
            raise Fault(xmlrpc.Faults.STILL_RUNNING, '')
        if name == 'BAD_NAME':
            raise Fault(xmlrpc.Faults.BAD_NAME, '')
        self.processes.remove(name)

    def clearProcessStdoutLog(self, name):
        from xmlrpclib import Fault
        from supervisor import xmlrpc
        if name == 'BAD_NAME':
            raise Fault(xmlrpc.Faults.BAD_NAME, 'BAD_NAME')
        return True

    clearProcessLog = clearProcessStdoutLog
    clearProcessStderrLog = clearProcessStdoutLog
    clearProcessLogs = clearProcessStdoutLog

    def clearAllProcessLogs(self):
        from supervisor import xmlrpc
        return [
            {'name':'foo', 'group':'foo',
             'status':xmlrpc.Faults.SUCCESS,
             'description': 'OK'},
            {'name':'foo2', 'group':'foo2',
             'status':xmlrpc.Faults.SUCCESS,
             'description': 'OK'},
            {'name':'failed', 'group':'failed_group',
             'status':xmlrpc.Faults.FAILED,
             'description':'FAILED'}
            ]

    def raiseError(self):
        raise ValueError('error')

    def getSupervisorVersion(self):
        return '3000'

    def readLog(self, whence, offset):
        if self._readlog_error:
            from xmlrpclib import Fault
            raise Fault(self._readlog_error, '')
        return 'mainlogdata'

class DummyPGroupConfig:
    def __init__(self, options, name='whatever', priority=999, pconfigs=None):
        self.options = options
        self.name = name
        self.priority = priority
        if pconfigs is None:
            pconfigs = []
        self.process_configs = pconfigs
        self.after_setuid_called = False
        self.pool_events = []
        self.buffer_size = 10

    def after_setuid(self):
        self.after_setuid_called = True

    def make_group(self):
        return DummyProcessGroup(self)

    def __repr__(self):
        return '<%s instance at %s named %s>' % (self.__class__, id(self),
                                                 self.name)

class DummyFCGIGroupConfig(DummyPGroupConfig):
    def __init__(self, options, name='whatever', priority=999, pconfigs=None, socket_config=DummySocketConfig(1)):
        DummyPGroupConfig.__init__(self, options, name, priority, pconfigs)
        self.socket_config = socket_config

class DummyProcessGroup:
    def __init__(self, config):
        self.config = config
        self.transitioned = False
        self.all_stopped = False
        self.dispatchers = {}
        self.unstopped_processes = []

    def transition(self):
        self.transitioned = True

    def stop_all(self):
        self.all_stopped = True
        
    def get_unstopped_processes(self):
        return self.unstopped_processes

    def get_dispatchers(self):
        return self.dispatchers
        
class DummyFCGIProcessGroup(DummyProcessGroup):
    
    def __init__(self, config):
        DummyProcessGroup.__init__(self, config)
        self.socket_manager = DummySocketManager(config.socket_config)

class PopulatedDummySupervisor(DummySupervisor):
    def __init__(self, options, group_name, *pconfigs):
        DummySupervisor.__init__(self, options)
        self.process_groups = {}
        processes = {}
        self.group_name = group_name
        gconfig = DummyPGroupConfig(options, group_name, pconfigs=pconfigs)
        pgroup = DummyProcessGroup(gconfig)
        self.process_groups[group_name] = pgroup
        for pconfig in pconfigs:
            process = DummyProcess(pconfig)
            processes[pconfig.name] = process
        pgroup.processes = processes

    def set_procattr(self, process_name, attr_name, val, group_name=None):
        if group_name is None:
            group_name = self.group_name
        process = self.process_groups[group_name].processes[process_name]
        setattr(process, attr_name, val)

class DummyDispatcher:
    write_event_handled = False
    read_event_handled = False
    error_handled = False
    logs_reopened = False
    logs_removed = False
    closed = False
    flush_error = None
    flushed = False
    def __init__(self, readable=False, writable=False, error=False):
        self._readable = readable
        self._writable = writable
        self._error = error
        self.input_buffer = ''
        if readable:
            # only readable dispatchers should have these methods
            def reopenlogs():
                self.logs_reopened = True
            self.reopenlogs = reopenlogs
            def removelogs():
                self.logs_removed = True
            self.removelogs = removelogs

    def readable(self):
        return self._readable
    def writable(self):
        return self._writable
    def handle_write_event(self):
        if self._error:
            raise self._error
        self.write_event_handled = True
    def handle_read_event(self):
        if self._error:
            raise self._error
        self.read_event_handled = True
    def handle_error(self):
        self.error_handled = True
    def close(self):
        self.closed = True
    def flush(self):
        if self.flush_error:
            raise OSError(self.flush_error)
        self.flushed = True
                
class DummyStream:
    def __init__(self, error=None):
        self.error = error
        self.closed = False
        self.flushed = False
        self.written = ''
    def close(self):
        if self.error:
            raise self.error
        self.closed = True
    def flush(self):
        self.flushed = True
    def write(self, msg):
        if self.error:
            raise self.error
        self.written +=msg
    def seek(self, num, whence=0):
        pass
    def tell(self):
        return len(self.written)
        
class DummyEvent:
    def __init__(self, serial='abc'):
        if serial is not None:
            self.serial = serial

    def __str__(self):
        return 'dummy event'
        
def dummy_handler(event, result):
    pass

def rejecting_handler(event, result):
    from supervisor.dispatchers import RejectEvent
    raise RejectEvent(result)

def exception_handler(event, result):
    raise ValueError(result)

def lstrip(s):
    strings = [x.strip() for x in s.split('\n')]
    return '\n'.join(strings)

########NEW FILE########
__FILENAME__ = fakeos
from os import *
from os import _exit
import os

class FakeOS:
    def __init__(self):
        self.orig_uid = os.getuid()
        self.orig_gid = os.getgid()

    def setgroups(*args):
        return

    def getuid():
        return 0

    def setuid(arg):
        self.uid = arg
        self.setuid_called = 1

    def setgid(arg):
        self.gid = arg
        self.setgid_called = 1

    def clear():
        self.uid = orig_uid
        self.gid = orig_gid
        self.setuid_called = 0
        self.setgid_called = 0

fake = FakeOS()

setgroups = fake.setgroups
getuid = fake.getuid
setuid = fake.setuid
setgid = fake.setgid
clear = fake.clear

########NEW FILE########
__FILENAME__ = spew
#!<<PYTHON>>
import time

counter = 0

while 1:
   time.sleep(0.01)
   print "more spewage %s" % counter
   counter += 1
   

########NEW FILE########
__FILENAME__ = unkillable_spew
#!<<PYTHON>>
import time
import signal
signal.signal(signal.SIGTERM, signal.SIG_IGN)

counter = 0

while 1:
   time.sleep(0.01)
   print "more spewage %s" % counter
   counter += 1
   

########NEW FILE########
__FILENAME__ = test_childutils
import sys
import time
import unittest
from StringIO import StringIO

class ChildUtilsTests(unittest.TestCase):
    def test_getRPCInterface(self):
        from supervisor.childutils import getRPCInterface
        rpc = getRPCInterface({'SUPERVISOR_SERVER_URL':'http://localhost:9001'})
        # we can't really test this thing; its a magic object

    def test_getRPCTransport_no_uname_pass(self):
        from supervisor.childutils import getRPCTransport
        t = getRPCTransport({'SUPERVISOR_SERVER_URL':'http://localhost:9001'})
        self.assertEqual(t.username, '')
        self.assertEqual(t.password, '')
        self.assertEqual(t.serverurl, 'http://localhost:9001')

    def test_getRPCTransport_with_uname_pass(self):
        from supervisor.childutils import getRPCTransport
        env = {'SUPERVISOR_SERVER_URL':'http://localhost:9001',
               'SUPERVISOR_USERNAME':'chrism',
               'SUPERVISOR_PASSWORD':'abc123'}
        t = getRPCTransport(env)
        self.assertEqual(t.username, 'chrism')
        self.assertEqual(t.password, 'abc123')
        self.assertEqual(t.serverurl, 'http://localhost:9001')

    def test_get_headers(self):
        from supervisor.childutils import get_headers
        line = 'a:1 b:2'
        result = get_headers(line)
        self.assertEqual(result, {'a':'1', 'b':'2'})

    def test_eventdata(self):
        from supervisor.childutils import eventdata
        payload = 'a:1 b:2\nthedata\n'
        headers, data = eventdata(payload)
        self.assertEqual(headers, {'a':'1', 'b':'2'})
        self.assertEqual(data, 'thedata\n')

    def test_get_asctime(self):
        from supervisor.childutils import get_asctime
        timestamp = time.mktime((2009, 1, 18, 22, 14, 7, 0, 0, 0))
        result = get_asctime(timestamp)
        self.assertEqual(result, '2009-01-18 22:14:07,000')

class TestProcessCommunicationsProtocol(unittest.TestCase):
    def test_send(self):
        from supervisor.childutils import pcomm
        stdout = StringIO()
        pcomm.send('hello', stdout)
        from supervisor.events import ProcessCommunicationEvent
        begin = ProcessCommunicationEvent.BEGIN_TOKEN
        end = ProcessCommunicationEvent.END_TOKEN
        self.assertEqual(stdout.getvalue(), '%s%s%s' % (begin, 'hello', end))

    def test_stdout(self):
        from supervisor.childutils import pcomm
        old = sys.stdout
        try:
            io = sys.stdout = StringIO()
            pcomm.stdout('hello')
            from supervisor.events import ProcessCommunicationEvent
            begin = ProcessCommunicationEvent.BEGIN_TOKEN
            end = ProcessCommunicationEvent.END_TOKEN
            self.assertEqual(io.getvalue(), '%s%s%s' % (begin, 'hello', end))
        finally:
            sys.stdout = old
        
    def test_stderr(self):
        from supervisor.childutils import pcomm
        old = sys.stderr
        try:
            io = sys.stderr = StringIO()
            pcomm.stderr('hello')
            from supervisor.events import ProcessCommunicationEvent
            begin = ProcessCommunicationEvent.BEGIN_TOKEN
            end = ProcessCommunicationEvent.END_TOKEN
            self.assertEqual(io.getvalue(), '%s%s%s' % (begin, 'hello', end))
        finally:
            sys.stderr = old

class TestEventListenerProtocol(unittest.TestCase):
    def test_wait(self):
        from supervisor.childutils import listener
        from supervisor.dispatchers import PEventListenerDispatcher
        token = PEventListenerDispatcher.READY_FOR_EVENTS_TOKEN
        class Dummy:
            def readline(self):
                return 'len:5'
            def read(self, *ignored):
                return 'hello'
        stdin = Dummy()
        stdout = StringIO()
        headers, payload = listener.wait(stdin, stdout)
        self.assertEqual(headers, {'len':'5'})
        self.assertEqual(payload, 'hello')
        self.assertEqual(stdout.getvalue(), 'READY\n')

    def test_token(self):
        from supervisor.childutils import listener
        from supervisor.dispatchers import PEventListenerDispatcher
        token = PEventListenerDispatcher.READY_FOR_EVENTS_TOKEN
        stdout = StringIO()
        listener.ready(stdout)
        self.assertEqual(stdout.getvalue(), token)

    def test_ok(self):
        from supervisor.childutils import listener
        from supervisor.dispatchers import PEventListenerDispatcher
        begin = PEventListenerDispatcher.RESULT_TOKEN_START
        stdout = StringIO()
        listener.ok(stdout)
        self.assertEqual(stdout.getvalue(), begin + '2\nOK')

    def test_fail(self):
        from supervisor.childutils import listener
        from supervisor.dispatchers import PEventListenerDispatcher
        begin = PEventListenerDispatcher.RESULT_TOKEN_START
        stdout = StringIO()
        listener.fail(stdout)
        self.assertEqual(stdout.getvalue(), begin + '4\nFAIL')

    def test_send(self):
        from supervisor.childutils import listener
        from supervisor.dispatchers import PEventListenerDispatcher
        begin = PEventListenerDispatcher.RESULT_TOKEN_START
        stdout = StringIO()
        msg = 'the body data ya fool\n'
        listener.send(msg, stdout)
        expected = '%s%s\n%s' % (begin, len(msg), msg)
        self.assertEqual(stdout.getvalue(), expected)
        

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = test_confecho
"""Test suite for supervisor.confecho"""

import sys
import unittest
from StringIO import StringIO
from supervisor import confecho

class TopLevelFunctionTests(unittest.TestCase):
    def test_main_writes_data_out_that_looks_like_a_config_file(self):
        sio = StringIO()
        confecho.main(out=sio)

        output = sio.getvalue()
        self.assertTrue("[supervisord]" in output)


def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = test_datatypes
"""Test suite for supervisor.datatypes"""

import sys
import os
import unittest
import socket
import tempfile
from mock import Mock, patch, sentinel
from supervisor import datatypes

class DatatypesTest(unittest.TestCase):
    def test_boolean_returns_true_for_truthy_values(self):
        for s in datatypes.TRUTHY_STRINGS:
            actual = datatypes.boolean(s)
            self.assertEqual(actual, True)

    def test_boolean_returns_true_for_upper_truthy_values(self):
        for s in map(str.upper, datatypes.TRUTHY_STRINGS):
            actual = datatypes.boolean(s)
            self.assert_(actual, True)

    def test_boolean_returns_false_for_falsy_values(self):
        for s in datatypes.FALSY_STRINGS:
            actual = datatypes.boolean(s)
            self.assertEqual(actual, False)

    def test_boolean_returns_false_for_upper_falsy_values(self):
        for s in map(str.upper, datatypes.FALSY_STRINGS):
            actual = datatypes.boolean(s)
            self.assertEqual(actual, False)

    def test_boolean_raises_value_error_for_bad_value(self):
        self.assertRaises(ValueError,
                          datatypes.boolean, 'not-a-value')

    def test_list_of_strings_returns_empty_list_for_empty_string(self):
        actual = datatypes.list_of_strings('')
        self.assertEqual(actual, [])

    def test_list_of_strings_returns_list_of_strings_by_comma_split(self):
        actual = datatypes.list_of_strings('foo,bar')
        self.assertEqual(actual, ['foo', 'bar'])

    def test_list_of_strings_returns_strings_with_whitespace_stripped(self):
        actual = datatypes.list_of_strings(' foo , bar ')
        self.assertEqual(actual, ['foo', 'bar'])

    def test_list_of_strings_raises_value_error_when_comma_split_fails(self):
        self.assertRaises(ValueError,
                          datatypes.list_of_strings, 42)

    def test_list_of_ints_returns_empty_list_for_empty_string(self):
        actual = datatypes.list_of_ints('')
        self.assertEqual(actual, [])

    def test_list_of_ints_returns_list_of_ints_by_comma_split(self):
        actual = datatypes.list_of_ints('1,42')
        self.assertEqual(actual, [1,42])

    def test_list_of_ints_returns_ints_even_if_whitespace_in_string(self):
        actual = datatypes.list_of_ints(' 1 , 42 ')
        self.assertEqual(actual, [1,42])

    def test_list_of_ints_raises_value_error_when_comma_split_fails(self):
        self.assertRaises(ValueError,
                          datatypes.list_of_ints, 42)

    def test_list_of_ints_raises_value_error_when_one_value_is_bad(self):
        self.assertRaises(ValueError,
                          datatypes.list_of_ints, '1, bad, 42')

    def test_list_of_exitcodes(self):
        vals = datatypes.list_of_exitcodes('1,2,3')
        self.assertEqual(vals, [1,2,3])
        vals = datatypes.list_of_exitcodes('1')
        self.assertEqual(vals, [1])
        self.assertRaises(ValueError, datatypes.list_of_exitcodes, 'a,b,c')
        self.assertRaises(ValueError, datatypes.list_of_exitcodes, '1024')
        self.assertRaises(ValueError, datatypes.list_of_exitcodes, '-1,1')

    def test_hasattr_automatic(self):
        datatypes.Automatic

    def test_dict_of_key_value_pairs_returns_empty_dict_for_empty_str(self):
        actual = datatypes.dict_of_key_value_pairs('')
        self.assertEqual({}, actual)

    def test_dict_of_key_value_pairs_returns_dict_from_single_pair_str(self):
        actual = datatypes.dict_of_key_value_pairs('foo=bar')
        expected = {'foo': 'bar'}
        self.assertEqual(actual, expected)

    def test_dict_of_key_value_pairs_returns_dict_from_multi_pair_str(self):
        actual = datatypes.dict_of_key_value_pairs('foo=bar,baz=qux')
        expected = {'foo': 'bar', 'baz': 'qux'}
        self.assertEqual(actual, expected)

    def test_dict_of_key_value_pairs_returns_dict_even_if_whitespace(self):
        actual = datatypes.dict_of_key_value_pairs(' foo = bar , baz = qux ')
        expected = {'foo': 'bar', 'baz': 'qux'}
        self.assertEqual(actual, expected)

    def test_dict_of_key_value_pairs_returns_dict_even_if_newlines(self):
        actual = datatypes.dict_of_key_value_pairs('foo\n=\nbar\n,\nbaz\n=\nqux')
        expected = {'foo': 'bar', 'baz': 'qux'}
        self.assertEqual(actual, expected)

    def test_dict_of_key_value_pairs_handles_commas_inside_apostrophes(self):
        actual = datatypes.dict_of_key_value_pairs("foo='bar,baz',baz='q,ux'")
        expected = {'foo': 'bar,baz', 'baz': 'q,ux'}
        self.assertEqual(actual, expected)

    def test_dict_of_key_value_pairs_handles_commas_inside_quotes(self):
        actual = datatypes.dict_of_key_value_pairs('foo="bar,baz",baz="q,ux"')
        expected = {'foo': 'bar,baz', 'baz': 'q,ux'}
        self.assertEqual(actual, expected)

    def test_dict_of_key_value_pairs_handles_unquoted_non_alphanum(self):
        actual = datatypes.dict_of_key_value_pairs(
            'HOME=/home/auser,FOO=/.foo+(1.2)-_/,'
            'SUPERVISOR_SERVER_URL=http://127.0.0.1:9001')
        expected = {'HOME': '/home/auser', 'FOO': '/.foo+(1.2)-_/',
                    'SUPERVISOR_SERVER_URL': 'http://127.0.0.1:9001'}
        self.assertEqual(actual, expected)

    def test_dict_of_key_value_pairs_allows_trailing_comma(self):
        actual = datatypes.dict_of_key_value_pairs('foo=bar,')
        expected = {'foo': 'bar'}
        self.assertEqual(actual, expected)

    def test_dict_of_key_value_pairs_raises_value_error_on_too_short(self):
        self.assertRaises(ValueError,
                          datatypes.dict_of_key_value_pairs, 'foo')
        self.assertRaises(ValueError,
                          datatypes.dict_of_key_value_pairs, 'foo=')
        self.assertRaises(ValueError,
                          datatypes.dict_of_key_value_pairs, 'foo=bar,baz')
        self.assertRaises(ValueError,
                          datatypes.dict_of_key_value_pairs, 'foo=bar,baz=')

    def test_dict_of_key_value_pairs_raises_when_comma_is_missing(self):
        kvp = 'KEY1=no-comma KEY2=ends-with-comma,'
        self.assertRaises(ValueError,
                          datatypes.dict_of_key_value_pairs, kvp)

    def test_logfile_name_returns_none_for_none_values(self):
        for thing in datatypes.LOGFILE_NONES:
            actual = datatypes.logfile_name(thing)
            self.assertEqual(actual, None)

    def test_logfile_name_returns_none_for_uppered_none_values(self):
        for thing in datatypes.LOGFILE_NONES:
            if hasattr(thing, 'upper'):
                thing = thing.upper()
            actual = datatypes.logfile_name(thing)
            self.assertEqual(actual, None)

    def test_logfile_name_returns_automatic_for_auto_values(self):
        for thing in datatypes.LOGFILE_AUTOS:
            actual = datatypes.logfile_name(thing)
            self.assertEqual(actual, datatypes.Automatic)

    def test_logfile_name_returns_automatic_for_uppered_auto_values(self):
        for thing in datatypes.LOGFILE_AUTOS:
            if hasattr(thing, 'upper'):
                thing = thing.upper()
            actual = datatypes.logfile_name(thing)
            self.assertEqual(actual, datatypes.Automatic)

    def test_logfile_name_returns_existing_dirpath_for_other_values(self):
        func = datatypes.existing_dirpath
        datatypes.existing_dirpath = lambda path: path
        try:
            path = '/path/to/logfile/With/Case/Preserved'
            actual = datatypes.logfile_name(path)
            self.assertEqual(actual, path)
        finally:
            datatypes.existing_dirpath = func

    def test_integer(self):
        from supervisor.datatypes import integer
        self.assertRaises(ValueError, integer, 'abc')
        self.assertEqual(integer('1'), 1)
        self.assertEqual(integer(str(sys.maxint+1)), sys.maxint+1)

    def test_url_accepts_urlparse_recognized_scheme_with_netloc(self):
        good_url = 'http://localhost:9001'
        self.assertEqual(datatypes.url(good_url), good_url)

    def test_url_rejects_urlparse_recognized_scheme_but_no_netloc(self):
        bad_url = 'http://'
        self.assertRaises(ValueError, datatypes.url, bad_url)

    def test_url_accepts_unix_scheme_with_path(self):
        good_url = "unix://somepath"
        self.assertEqual(good_url, datatypes.url(good_url))

    def test_url_rejects_unix_scheme_with_no_slashes_or_path(self):
        bad_url = "unix:"
        self.assertRaises(ValueError, datatypes.url, bad_url)

    def test_url_rejects_unix_scheme_with_slashes_but_no_path(self):
        bad_url = "unix://"
        self.assertRaises(ValueError, datatypes.url, bad_url)

class InetStreamSocketConfigTests(unittest.TestCase):
    def _getTargetClass(self):
        return datatypes.InetStreamSocketConfig

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_url(self):
        conf = self._makeOne('127.0.0.1', 8675)
        self.assertEqual(conf.url, 'tcp://127.0.0.1:8675')

    def test___str__(self):
        cfg = self._makeOne('localhost', 65531)
        self.assertEqual(str(cfg), 'tcp://localhost:65531')

    def test_repr(self):
        conf = self._makeOne('127.0.0.1', 8675)
        s = repr(conf)
        self.assertTrue(s.startswith(
            '<supervisor.datatypes.InetStreamSocketConfig at'), s)
        self.assertTrue(s.endswith('for tcp://127.0.0.1:8675>'), s)

    def test_addr(self):
        conf = self._makeOne('127.0.0.1', 8675)
        addr = conf.addr()
        self.assertEqual(addr, ('127.0.0.1', 8675))

    def test_port_as_string(self):
        conf = self._makeOne('localhost', '5001')
        addr = conf.addr()
        self.assertEqual(addr, ('localhost', 5001))

    def test_create_and_bind(self):
        conf = self._makeOne('127.0.0.1', 8675)
        sock = conf.create_and_bind()
        reuse = sock.getsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR)
        self.assertTrue(reuse)
        self.assertEquals(conf.addr(), sock.getsockname()) #verifies that bind was called
        sock.close()

    def test_same_urls_are_equal(self):
        conf1 = self._makeOne('localhost', 5001)
        conf2 = self._makeOne('localhost', 5001)
        self.assertTrue(conf1 == conf2)
        self.assertFalse(conf1 != conf2)

    def test_diff_urls_are_not_equal(self):
        conf1 = self._makeOne('localhost', 5001)
        conf2 = self._makeOne('localhost', 5002)
        self.assertTrue(conf1 != conf2)
        self.assertFalse(conf1 == conf2)

    def test_diff_objs_are_not_equal(self):
        conf1 = self._makeOne('localhost', 5001)
        conf2 = 'blah'
        self.assertTrue(conf1 != conf2)
        self.assertFalse(conf1 == conf2)

class UnixStreamSocketConfigTests(unittest.TestCase):
    def _getTargetClass(self):
        return datatypes.UnixStreamSocketConfig

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_url(self):
        conf = self._makeOne('/tmp/foo.sock')
        self.assertEqual(conf.url, 'unix:///tmp/foo.sock')

    def test___str__(self):
        cfg = self._makeOne('foo/bar')
        self.assertEqual(str(cfg), 'unix://foo/bar')

    def test_repr(self):
        conf = self._makeOne('/tmp/foo.sock')
        s = repr(conf)
        self.assertTrue(s.startswith(
            '<supervisor.datatypes.UnixStreamSocketConfig at'), s)
        self.assertTrue(s.endswith('for unix:///tmp/foo.sock>'), s)

    def test_get_addr(self):
        conf = self._makeOne('/tmp/foo.sock')
        addr = conf.addr()
        self.assertEqual(addr, '/tmp/foo.sock')

    def test_create_and_bind(self):
        (tf_fd, tf_name) = tempfile.mkstemp()
        owner = (sentinel.uid, sentinel.gid)
        mode = sentinel.mode
        conf = self._makeOne(tf_name, owner=owner, mode=mode)

        #Patch os.chmod and os.chown functions with mocks
        #objects so that the test does not depend on
        #any specific system users or permissions
        chown_mock = Mock()
        chmod_mock = Mock()
        @patch('os.chown', chown_mock)
        @patch('os.chmod', chmod_mock)
        def call_create_and_bind(conf):
            return conf.create_and_bind()

        sock = call_create_and_bind(conf)
        self.assertTrue(os.path.exists(tf_name))
        self.assertEquals(conf.addr(), sock.getsockname()) #verifies that bind was called
        sock.close()
        self.assertTrue(os.path.exists(tf_name))
        os.unlink(tf_name)
        #Verify that os.chown was called with correct args
        self.assertEquals(1, chown_mock.call_count)
        path_arg = chown_mock.call_args[0][0]
        uid_arg = chown_mock.call_args[0][1]
        gid_arg = chown_mock.call_args[0][2]
        self.assertEquals(tf_name, path_arg)
        self.assertEquals(owner[0], uid_arg)
        self.assertEquals(owner[1], gid_arg)
        #Verify that os.chmod was called with correct args
        self.assertEquals(1, chmod_mock.call_count)
        path_arg = chmod_mock.call_args[0][0]
        mode_arg = chmod_mock.call_args[0][1]
        self.assertEquals(tf_name, path_arg)
        self.assertEquals(mode, mode_arg)

    def test_same_paths_are_equal(self):
        conf1 = self._makeOne('/tmp/foo.sock')
        conf2 = self._makeOne('/tmp/foo.sock')
        self.assertTrue(conf1 == conf2)
        self.assertFalse(conf1 != conf2)

    def test_diff_paths_are_not_equal(self):
        conf1 = self._makeOne('/tmp/foo.sock')
        conf2 = self._makeOne('/tmp/bar.sock')
        self.assertTrue(conf1 != conf2)
        self.assertFalse(conf1 == conf2)

    def test_diff_objs_are_not_equal(self):
        conf1 = self._makeOne('/tmp/foo.sock')
        conf2 = 'blah'
        self.assertTrue(conf1 != conf2)
        self.assertFalse(conf1 == conf2)

class RangeCheckedConversionTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.datatypes import RangeCheckedConversion
        return RangeCheckedConversion

    def _makeOne(self, conversion, min=None, max=None):
        return self._getTargetClass()(conversion, min, max)

    def test_below_lower_bound(self):
        conversion = self._makeOne(lambda *arg: -1, 0)
        self.assertRaises(ValueError, conversion, None)

    def test_above_upper_lower_bound(self):
        conversion = self._makeOne(lambda *arg: 1, 0, 0)
        self.assertRaises(ValueError, conversion, None)

    def test_passes(self):
        conversion = self._makeOne(lambda *arg: 0, 0, 0)
        self.assertEqual(conversion(0), 0)

class InetAddressTests(unittest.TestCase):
    def _callFUT(self, s):
        from supervisor.datatypes import inet_address
        return inet_address(s)

    def test_no_port_number(self):
        self.assertRaises(ValueError, self._callFUT, 'a:')

    def test_bad_port_number(self):
        self.assertRaises(ValueError, self._callFUT, 'a')

    def test_default_host(self):
        host, port = self._callFUT('*:8080')
        self.assertEqual(host, '')
        self.assertEqual(port, 8080)

    def test_boring(self):
        host, port = self._callFUT('localhost:80')
        self.assertEqual(host, 'localhost')
        self.assertEqual(port, 80)

class TestSocketAddress(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.datatypes import SocketAddress
        return SocketAddress

    def _makeOne(self, s):
        return self._getTargetClass()(s)

    def test_unix_socket(self):
        import socket
        addr = self._makeOne('/foo/bar')
        self.assertEqual(addr.family, socket.AF_UNIX)
        self.assertEqual(addr.address, '/foo/bar')

    def test_inet_socket(self):
        import socket
        addr = self._makeOne('localhost:8080')
        self.assertEqual(addr.family, socket.AF_INET)
        self.assertEqual(addr.address, ('localhost', 8080))

class TestColonSeparatedUserGroup(unittest.TestCase):
    def _callFUT(self, arg):
        from supervisor.datatypes import colon_separated_user_group
        return colon_separated_user_group(arg)

    def test_ok_username(self):
        self.assertEqual(self._callFUT('root')[0], 0)

    def test_missinguser_username(self):
        self.assertRaises(ValueError,
                          self._callFUT, 'godihopethisuserdoesntexist')

    def test_missinguser_username_and_groupname(self):
        self.assertRaises(ValueError,
                          self._callFUT, 'godihopethisuserdoesntexist:foo')

class TestOctalType(unittest.TestCase):
    def _callFUT(self, arg):
        from supervisor.datatypes import octal_type
        return octal_type(arg)

    def test_it_success(self):
        self.assertEqual(self._callFUT('10'), 8)

    def test_test_it_failure(self):
        self.assertRaises(ValueError, self._callFUT, 'noo')

########NEW FILE########
__FILENAME__ = test_dispatchers
import unittest
import os
import sys

from supervisor.tests.base import DummyOptions
from supervisor.tests.base import DummyProcess
from supervisor.tests.base import DummyPConfig
from supervisor.tests.base import DummyLogger
from supervisor.tests.base import DummyEvent

class POutputDispatcherTests(unittest.TestCase):
    def setUp(self):
        from supervisor.events import clear
        clear()

    def tearDown(self):
        from supervisor.events import clear
        clear()

    def _getTargetClass(self):
        from supervisor.dispatchers import POutputDispatcher
        return POutputDispatcher

    def _makeOne(self, process, channel='stdout'):
        from supervisor import events
        events = {'stdout': events.ProcessCommunicationStdoutEvent,
                  'stderr': events.ProcessCommunicationStderrEvent}
        # dispatcher derives its channel from event class
        return self._getTargetClass()(process, events[channel], 0)

    def test_writable(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.writable(), False)
        
    def test_readable_open(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.closed = False
        self.assertEqual(dispatcher.readable(), True)

    def test_readable_closed(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.closed = True
        self.assertEqual(dispatcher.readable(), False)

    def test_handle_write_event(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertRaises(NotImplementedError, dispatcher.handle_write_event)

    def test_handle_read_event(self):
        options = DummyOptions()
        options.readfd_result = 'abc'
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_capture_maxbytes=100)
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.handle_read_event(), None)
        self.assertEqual(dispatcher.output_buffer, 'abc')
        
    def test_handle_error(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        try:
            raise ValueError('foo')
        except:
            dispatcher.handle_error()
        result = options.logger.data[0]
        self.assertTrue(result.startswith(
            'uncaptured python exception, closing channel'),result)

    def test_toggle_capturemode_sends_event(self):
        executable = '/bin/cat'
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo',
                              stdout_capture_maxbytes=500)
        process = DummyProcess(config)
        process.pid = 4000
        dispatcher = self._makeOne(process)
        dispatcher.capturemode = True
        dispatcher.capturelog.data = ['hallooo']
        L = []
        def doit(event):
            L.append(event)
        from supervisor import events
        events.subscribe(events.EventTypes.PROCESS_COMMUNICATION, doit)
        dispatcher.toggle_capturemode()
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.process, process)
        self.assertEqual(event.pid, 4000)
        self.assertEqual(event.data, 'hallooo')

    def test_removelogs(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.removelogs()
        self.assertEqual(dispatcher.mainlog.handlers[0].reopened, True)
        self.assertEqual(dispatcher.mainlog.handlers[0].removed, True)
        self.assertEqual(dispatcher.childlog.handlers[0].reopened, True)
        self.assertEqual(dispatcher.childlog.handlers[0].removed, True)

    def test_reopenlogs(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.reopenlogs()
        self.assertEqual(dispatcher.childlog.handlers[0].reopened, True)
        self.assertEqual(dispatcher.mainlog.handlers[0].reopened, True)

    def test_record_output_log_non_capturemode(self):
        # stdout/stderr goes to the process log and the main log,
        # in non-capturemode, the data length doesn't matter
        options = DummyOptions()
        from supervisor import loggers
        options.loglevel = loggers.LevelsByName.TRAC
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.output_buffer = 'a'
        dispatcher.record_output()
        self.assertEqual(dispatcher.childlog.data, ['a'])
        self.assertEqual(options.logger.data[0],
             "'process1' stdout output:\na")
        self.assertEqual(dispatcher.output_buffer, '')

    def test_record_output_emits_stdout_event_when_enabled(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_events_enabled=True)
        process = DummyProcess(config)
        dispatcher = self._makeOne(process, 'stdout')
        dispatcher.output_buffer = 'hello from stdout'

        L = []
        def doit(event):
            L.append(event)
        from supervisor import events
        events.subscribe(events.EventTypes.PROCESS_LOG_STDOUT, doit)
        dispatcher.record_output()

        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.process, process)
        self.assertEqual(event.data, 'hello from stdout') 

    def test_record_output_does_not_emit_stdout_event_when_disabled(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_events_enabled=False)
        process = DummyProcess(config)
        dispatcher = self._makeOne(process, 'stdout')
        dispatcher.output_buffer = 'hello from stdout'

        L = []
        def doit(event):
            L.append(event)
        from supervisor import events
        events.subscribe(events.EventTypes.PROCESS_LOG_STDOUT, doit)
        dispatcher.record_output()

        self.assertEqual(len(L), 0)

    def test_record_output_emits_stderr_event_when_enabled(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stderr_events_enabled=True)
        process = DummyProcess(config)
        dispatcher = self._makeOne(process, 'stderr')
        dispatcher.output_buffer = 'hello from stderr'

        L = []
        def doit(event):
            L.append(event)
        from supervisor import events
        events.subscribe(events.EventTypes.PROCESS_LOG_STDERR, doit)
        dispatcher.record_output()

        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.process, process)
        self.assertEqual(event.data, 'hello from stderr') 

    def test_record_output_does_not_emit_stderr_event_when_disabled(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stderr_events_enabled=False)
        process = DummyProcess(config)
        dispatcher = self._makeOne(process, 'stderr')
        dispatcher.output_buffer = 'hello from stderr'

        L = []
        def doit(event):
            L.append(event)
        from supervisor import events
        events.subscribe(events.EventTypes.PROCESS_LOG_STDERR, doit)
        dispatcher.record_output()

        self.assertEqual(len(L), 0)

    def test_record_output_capturemode_string_longer_than_token(self):
        # stdout/stderr goes to the process log and the main log,
        # in capturemode, the length of the data needs to be longer
        # than the capture token to make it out.
        options = DummyOptions()
        from supervisor import loggers
        options.loglevel = loggers.LevelsByName.TRAC
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo',
                              stdout_capture_maxbytes=100)
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.output_buffer = 'stdout string longer than a token'
        dispatcher.record_output()
        self.assertEqual(dispatcher.childlog.data,
                         ['stdout string longer than a token'])
        self.assertEqual(options.logger.data[0],
             "'process1' stdout output:\nstdout string longer than a token")

    def test_record_output_capturemode_string_not_longer_than_token(self):
        # stdout/stderr goes to the process log and the main log,
        # in capturemode, the length of the data needs to be longer
        # than the capture token to make it out.
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo',
                              stdout_capture_maxbytes=100)
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.output_buffer = 'a'
        dispatcher.record_output()
        self.assertEqual(dispatcher.childlog.data, [])
        self.assertEqual(dispatcher.output_buffer, 'a')

    def test_stdout_capturemode_single_buffer(self):
        # mike reported that comm events that took place within a single
        # output buffer were broken 8/20/2007
        from supervisor.events import ProcessCommunicationEvent
        from supervisor.events import subscribe
        events = []
        def doit(event):
            events.append(event)
        subscribe(ProcessCommunicationEvent, doit)
        BEGIN_TOKEN = ProcessCommunicationEvent.BEGIN_TOKEN
        END_TOKEN = ProcessCommunicationEvent.END_TOKEN
        data = BEGIN_TOKEN + 'hello' + END_TOKEN
        options = DummyOptions()
        from supervisor.loggers import getLogger
        options.getLogger = getLogger # actually use real logger
        logfile = '/tmp/log'
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile=logfile,
                              stdout_capture_maxbytes=1000)
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)

        try:
            dispatcher.output_buffer = data
            dispatcher.record_output()
            self.assertEqual(open(logfile, 'r').read(), '')
            self.assertEqual(dispatcher.output_buffer, '')
            self.assertEqual(len(events), 1)

            event = events[0]
            from supervisor.events import ProcessCommunicationStdoutEvent
            self.assertEqual(event.__class__, ProcessCommunicationStdoutEvent)
            self.assertEqual(event.process, process)
            self.assertEqual(event.channel, 'stdout')
            self.assertEqual(event.data, 'hello')

        finally:
            try:
                os.remove(logfile)
            except (OSError, IOError):
                pass
        
    def test_stdout_capturemode_multiple_buffers(self):
        from supervisor.events import ProcessCommunicationEvent
        from supervisor.events import subscribe
        events = []
        def doit(event):
            events.append(event)
        subscribe(ProcessCommunicationEvent, doit)
        import string
        letters = string.letters
        digits = string.digits * 4
        BEGIN_TOKEN = ProcessCommunicationEvent.BEGIN_TOKEN
        END_TOKEN = ProcessCommunicationEvent.END_TOKEN
        data = (letters +  BEGIN_TOKEN + digits + END_TOKEN + letters)

        # boundaries that split tokens
        broken = data.split(':')
        first = broken[0] + ':'
        second = broken[1] + ':'
        third = broken[2]

        options = DummyOptions()
        from supervisor.loggers import getLogger
        options.getLogger = getLogger # actually use real logger
        logfile = '/tmp/log'
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile=logfile,
                              stdout_capture_maxbytes=10000)
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        try:
            dispatcher.output_buffer = first
            dispatcher.record_output()
            [ x.flush() for x in dispatcher.childlog.handlers]
            self.assertEqual(open(logfile, 'r').read(), letters)
            self.assertEqual(dispatcher.output_buffer, first[len(letters):])
            self.assertEqual(len(events), 0)

            dispatcher.output_buffer += second
            dispatcher.record_output()
            self.assertEqual(len(events), 0)
            [ x.flush() for x in dispatcher.childlog.handlers]
            self.assertEqual(open(logfile, 'r').read(), letters)
            self.assertEqual(dispatcher.output_buffer, first[len(letters):])
            self.assertEqual(len(events), 0)

            dispatcher.output_buffer += third
            dispatcher.record_output()
            [ x.flush() for x in dispatcher.childlog.handlers]
            self.assertEqual(open(logfile, 'r').read(), letters *2)
            self.assertEqual(len(events), 1)
            event = events[0]
            from supervisor.events import ProcessCommunicationStdoutEvent
            self.assertEqual(event.__class__, ProcessCommunicationStdoutEvent)
            self.assertEqual(event.process, process)
            self.assertEqual(event.channel, 'stdout')
            self.assertEqual(event.data, digits)

        finally:
            try:
                os.remove(logfile)
            except (OSError, IOError):
                pass

    def test_strip_ansi(self):
        options = DummyOptions()
        options.strip_ansi = True
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        ansi = '\x1b[34mHello world... this is longer than a token!\x1b[0m'
        noansi = 'Hello world... this is longer than a token!'

        dispatcher.output_buffer = ansi
        dispatcher.record_output()
        self.assertEqual(len(dispatcher.childlog.data), 1)
        self.assertEqual(dispatcher.childlog.data[0], noansi)

        options.strip_ansi = False

        dispatcher.output_buffer = ansi
        dispatcher.record_output()
        self.assertEqual(len(dispatcher.childlog.data), 2)
        self.assertEqual(dispatcher.childlog.data[1], ansi)

    def test_ctor_nologfiles(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.process, process)
        self.assertEqual(dispatcher.channel, 'stdout')
        self.assertEqual(dispatcher.fd, 0)
        self.assertEqual(dispatcher.capturelog, None)
        self.assertEqual(dispatcher.mainlog, None)
        self.assertEqual(dispatcher.childlog, None)

    def test_ctor_logfile_only(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.process, process)
        self.assertEqual(dispatcher.channel, 'stdout')
        self.assertEqual(dispatcher.fd, 0)
        self.assertEqual(dispatcher.capturelog, None)
        self.assertEqual(dispatcher.mainlog.__class__, DummyLogger)
        self.assertEqual(dispatcher.childlog, dispatcher.mainlog)

    def test_ctor_capturelog_only(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_capture_maxbytes=300)
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.process, process)
        self.assertEqual(dispatcher.channel, 'stdout')
        self.assertEqual(dispatcher.fd, 0)
        self.assertEqual(dispatcher.capturelog.__class__,DummyLogger)
        self.assertEqual(dispatcher.mainlog, None)
        self.assertEqual(dispatcher.childlog, None)

    def test_ctor_nologs(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.process, process)
        self.assertEqual(dispatcher.channel, 'stdout')
        self.assertEqual(dispatcher.fd, 0)
        self.assertEqual(dispatcher.capturelog, None)
        self.assertEqual(dispatcher.mainlog, None)
        self.assertEqual(dispatcher.childlog, None)

    def test_repr(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        drepr = repr(dispatcher)
        self.assertTrue(drepr.startswith('<POutputDispatcher at'), drepr)
        self.assertNotEqual(
            drepr.find('<supervisor.tests.base.DummyProcess instance at'),
            -1)
        self.assertTrue(drepr.endswith('(stdout)>'), drepr)

    def test_close(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.close()
        self.assertEqual(dispatcher.closed, True)
        dispatcher.close() # make sure we don't error if we try to close twice
        self.assertEqual(dispatcher.closed, True)
        
                        
class PInputDispatcherTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.dispatchers import PInputDispatcher
        return PInputDispatcher

    def _makeOne(self, process):
        channel = 'stdin'
        return self._getTargetClass()(process, channel, 0)

    def test_writable_open_nodata(self):
        process = DummyProcess(None)
        dispatcher = self._makeOne(process)
        dispatcher.input_buffer = 'a'
        dispatcher.closed = False
        self.assertEqual(dispatcher.writable(), True)

    def test_writable_open_withdata(self):
        process = DummyProcess(None)
        dispatcher = self._makeOne(process)
        dispatcher.input_buffer = ''
        dispatcher.closed = False
        self.assertEqual(dispatcher.writable(), False)

    def test_writable_closed_nodata(self):
        process = DummyProcess(None)
        dispatcher = self._makeOne(process)
        dispatcher.input_buffer = 'a'
        dispatcher.closed = True
        self.assertEqual(dispatcher.writable(), False)

    def test_writable_closed_withdata(self):
        process = DummyProcess(None)
        dispatcher = self._makeOne(process)
        dispatcher.input_buffer = ''
        dispatcher.closed = True
        self.assertEqual(dispatcher.writable(), False)

    def test_readable(self):
        process = DummyProcess(None)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.readable(), False)

    def test_handle_write_event(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.input_buffer = 'halloooo'
        self.assertEqual(dispatcher.handle_write_event(), None)
        self.assertEqual(options.written[0], 'halloooo')

    def test_handle_write_event_nodata(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.input_buffer, '')
        dispatcher.handle_write_event
        self.assertEqual(dispatcher.input_buffer, '')
        self.assertEqual(options.written, {})

    def test_handle_write_event_epipe_raised(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.input_buffer = 'halloooo'
        import errno
        options.write_error = errno.EPIPE
        dispatcher.handle_write_event()
        self.assertEqual(dispatcher.input_buffer, '')
        self.assertTrue(options.logger.data[0].startswith(
            'fd 0 closed, stopped monitoring'))
        self.assertTrue(options.logger.data[0].endswith('(stdin)>'))

    def test_handle_write_event_uncaught_raised(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.input_buffer = 'halloooo'
        import errno
        options.write_error = errno.EBADF
        self.assertRaises(OSError, dispatcher.handle_write_event)

    def test_handle_write_event_over_os_limit(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        options.write_accept = 1
        dispatcher.input_buffer = 'a' * 50
        dispatcher.handle_write_event()
        self.assertEqual(len(dispatcher.input_buffer), 49)
        self.assertEqual(options.written[0], 'a')

    def test_handle_read_event(self):
        process = DummyProcess(None)
        dispatcher = self._makeOne(process)
        self.assertRaises(NotImplementedError, dispatcher.handle_read_event)
        
    def test_handle_error(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        try:
            raise ValueError('foo')
        except:
            dispatcher.handle_error()
        result = options.logger.data[0]
        self.assertTrue(result.startswith(
            'uncaptured python exception, closing channel'),result)

    def test_repr(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        drepr = repr(dispatcher)
        self.assertTrue(drepr.startswith('<PInputDispatcher at'), drepr)
        self.assertNotEqual(
            drepr.find('<supervisor.tests.base.DummyProcess instance at'),
            -1)
        self.assertTrue(drepr.endswith('(stdin)>'), drepr)

    def test_close(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.close()
        self.assertEqual(dispatcher.closed, True)
        dispatcher.close() # make sure we don't error if we try to close twice
        self.assertEqual(dispatcher.closed, True)

class PEventListenerDispatcherTests(unittest.TestCase):
    def setUp(self):
        from supervisor.events import clear
        clear()

    def tearDown(self):
        from supervisor.events import clear
        clear()

    def _getTargetClass(self):
        from supervisor.dispatchers import PEventListenerDispatcher
        return PEventListenerDispatcher

    def _makeOne(self, process):
        channel = 'stdout'
        return self._getTargetClass()(process, channel, 0)

    def test_writable(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.writable(), False)
        
    def test_readable_open(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.closed = False
        self.assertEqual(dispatcher.readable(), True)

    def test_readable_closed(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.closed = True
        self.assertEqual(dispatcher.readable(), False)

    def test_handle_write_event(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertRaises(NotImplementedError, dispatcher.handle_write_event)

    def test_handle_read_event_calls_handle_listener_state_change(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        process.listener_state = EventListenerStates.ACKNOWLEDGED
        dispatcher = self._makeOne(process)
        options.readfd_result = dispatcher.READY_FOR_EVENTS_TOKEN
        self.assertEqual(dispatcher.handle_read_event(), None)
        self.assertEqual(process.listener_state, EventListenerStates.READY)
        self.assertEqual(dispatcher.state_buffer, '')
        self.assertEqual(len(dispatcher.childlog.data), 1)
        self.assertEqual(dispatcher.childlog.data[0],
                         dispatcher.READY_FOR_EVENTS_TOKEN)

    def test_handle_read_event_nodata(self):
        options = DummyOptions()
        options.readfd_result = ''
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.handle_read_event(), None)
        self.assertEqual(dispatcher.state_buffer, '')
        from supervisor.dispatchers import EventListenerStates
        self.assertEqual(dispatcher.process.listener_state,
                         EventListenerStates.ACKNOWLEDGED)

    def test_handle_read_event_logging_nologs(self):
        options = DummyOptions()
        options.readfd_result = 'supercalifragilisticexpialidocious'
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        # just make sure there are no errors if a child logger doesnt
        # exist
        self.assertEqual(dispatcher.handle_read_event(), None)
        self.assertEqual(dispatcher.childlog, None)

    def test_handle_read_event_logging_childlog(self):
        options = DummyOptions()
        options.readfd_result = 'supercalifragilisticexpialidocious'
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.handle_read_event(), None)
        self.assertEqual(len(dispatcher.childlog.data), 1)
        self.assertEqual(dispatcher.childlog.data[0],
                         'supercalifragilisticexpialidocious')

    def test_handle_listener_state_change_from_unknown(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.UNKNOWN
        dispatcher.state_buffer = 'whatever'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, '')
        self.assertEqual(options.logger.data, [])
        self.assertEqual(process.listener_state, EventListenerStates.UNKNOWN)

    def test_handle_listener_state_change_acknowledged_to_ready(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.ACKNOWLEDGED
        dispatcher.state_buffer = 'READY\n'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, '')
        self.assertEqual(options.logger.data[0],
                         'process1: ACKNOWLEDGED -> READY')
        self.assertEqual(process.listener_state, EventListenerStates.READY)

    def test_handle_listener_state_change_acknowledged_gobbles(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.ACKNOWLEDGED
        dispatcher.state_buffer = 'READY\ngarbage\n'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, '')
        self.assertEqual(options.logger.data[0],
                         'process1: ACKNOWLEDGED -> READY')
        self.assertEqual(options.logger.data[1],
                         'process1: READY -> UNKNOWN')
        self.assertEqual(process.listener_state, EventListenerStates.UNKNOWN)

    def test_handle_listener_state_change_acknowledged_to_insufficient(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.ACKNOWLEDGED
        dispatcher.state_buffer = 'RE'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, 'RE')
        self.assertEqual(options.logger.data, [])
        self.assertEqual(process.listener_state,
                         EventListenerStates.ACKNOWLEDGED)

    def test_handle_listener_state_change_acknowledged_to_unknown(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.ACKNOWLEDGED
        dispatcher.state_buffer = 'bogus data yo'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, '')
        self.assertEqual(options.logger.data[0],
                         'process1: ACKNOWLEDGED -> UNKNOWN')
        self.assertEqual(process.listener_state, EventListenerStates.UNKNOWN)

    def test_handle_listener_state_change_ready_to_unknown(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.READY
        dispatcher.state_buffer = 'bogus data yo'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, '')
        self.assertEqual(options.logger.data[0],
                         'process1: READY -> UNKNOWN')
        self.assertEqual(process.listener_state, EventListenerStates.UNKNOWN)

    def test_handle_listener_state_change_busy_to_insufficient(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.BUSY
        dispatcher.state_buffer = 'bogus data yo'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, 'bogus data yo')
        self.assertEqual(process.listener_state, EventListenerStates.BUSY)

    def test_handle_listener_state_change_busy_to_acknowledged_procd(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.BUSY
        class Dummy:
            pass
        process.group = Dummy()
        process.group.config = Dummy()
        from supervisor.dispatchers import default_handler
        process.group.config.result_handler = default_handler
        dispatcher.state_buffer = 'RESULT 2\nOKabc'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, 'abc')
        self.assertEqual(options.logger.data[0],
                         'process1: BUSY -> ACKNOWLEDGED (processed)')
        self.assertEqual(process.listener_state,
                         EventListenerStates.ACKNOWLEDGED)

    def test_handle_listener_state_change_busy_to_acknowledged_rejected(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.BUSY
        class Dummy:
            pass
        process.group = Dummy()
        process.group.config = Dummy()
        from supervisor.dispatchers import default_handler
        process.group.config.result_handler = default_handler
        dispatcher.state_buffer = 'RESULT 4\nFAILabc'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, 'abc')
        self.assertEqual(options.logger.data[0],
                         'process1: BUSY -> ACKNOWLEDGED (rejected)')
        self.assertEqual(process.listener_state,
                         EventListenerStates.ACKNOWLEDGED)

    def test_handle_listener_state_change_busy_to_unknown(self):
        from supervisor.events import EventRejectedEvent
        from supervisor.events import subscribe
        events = []
        def doit(event):
            events.append(event)
        subscribe(EventRejectedEvent, doit)
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.BUSY
        current_event = DummyEvent()
        process.event = current_event
        dispatcher.state_buffer = 'bogus data\n'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, '')
        self.assertEqual(options.logger.data[0],
                'process1: BUSY -> UNKNOWN (bad result line \'bogus data\')')
        self.assertEqual(process.listener_state,
                         EventListenerStates.UNKNOWN)
        self.assertEqual(events[0].process, process)
        self.assertEqual(events[0].event, current_event)

    def test_handle_listener_state_busy_gobbles(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        process.listener_state = EventListenerStates.BUSY
        class Dummy:
            pass
        process.group = Dummy()
        process.group.config = Dummy()
        from supervisor.dispatchers import default_handler
        process.group.config.result_handler = default_handler
        dispatcher.state_buffer = 'RESULT 2\nOKbogus data\n'
        self.assertEqual(dispatcher.handle_listener_state_change(), None)
        self.assertEqual(dispatcher.state_buffer, '')
        self.assertEqual(options.logger.data[0],
                         'process1: BUSY -> ACKNOWLEDGED (processed)')
        self.assertEqual(options.logger.data[1],
                         'process1: ACKNOWLEDGED -> UNKNOWN')
        self.assertEqual(process.listener_state,
                         EventListenerStates.UNKNOWN)

    def test_handle_result_accept(self):
        from supervisor.events import subscribe
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        L = []
        def doit(event):
            L.append(event)
        from supervisor import events
        subscribe(events.EventRejectedEvent, doit)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        def handle(event, result):
            pass
        class Dummy:
            pass
        process.group = Dummy()
        process.group.config = Dummy()
        process.group.config.result_handler = handle
        process.listener_state = EventListenerStates.BUSY
        dispatcher.handle_result('foo')
        self.assertEqual(len(L), 0)
        self.assertEqual(process.listener_state,
                         EventListenerStates.ACKNOWLEDGED)
        result = options.logger.data[0]
        self.assertTrue(result.endswith('BUSY -> ACKNOWLEDGED (processed)'))

    def test_handle_result_rejectevent(self):
        from supervisor.events import subscribe
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        L = []
        def doit(event):
            L.append(event)
        from supervisor import events
        subscribe(events.EventRejectedEvent, doit)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        def rejected(event, result):
            from supervisor.dispatchers import RejectEvent
            raise RejectEvent(result)
        class Dummy:
            pass
        process.group = Dummy()
        process.group.config = Dummy()
        process.group.config.result_handler = rejected
        process.listener_state = EventListenerStates.BUSY
        dispatcher.handle_result('foo')
        self.assertEqual(len(L), 1)
        self.assertEqual(L[0].__class__, events.EventRejectedEvent)
        self.assertEqual(process.listener_state,
                         EventListenerStates.ACKNOWLEDGED)
        result = options.logger.data[0]
        self.assertTrue(result.endswith('BUSY -> ACKNOWLEDGED (rejected)'))

    def test_handle_result_exception(self):
        from supervisor.events import subscribe
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        L = []
        def doit(event):
            L.append(event)
        from supervisor import events
        subscribe(events.EventRejectedEvent, doit)
        from supervisor.dispatchers import EventListenerStates
        dispatcher = self._makeOne(process)
        def exception(event, result):
            raise ValueError
        class Dummy:
            pass
        process.group = Dummy()
        process.group.config = Dummy()
        process.group.config.result_handler = exception
        process.group.result_handler = exception
        process.listener_state = EventListenerStates.BUSY
        dispatcher.handle_result('foo')
        self.assertEqual(len(L), 1)
        self.assertEqual(L[0].__class__, events.EventRejectedEvent)
        self.assertEqual(process.listener_state,
                         EventListenerStates.UNKNOWN)
        result = options.logger.data[0]
        self.assertTrue(result.endswith('BUSY -> UNKNOWN'))

    def test_handle_error(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        try:
            raise ValueError('foo')
        except:
            dispatcher.handle_error()
        result = options.logger.data[0]
        self.assertTrue(result.startswith(
            'uncaptured python exception, closing channel'),result)

    def test_removelogs(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.removelogs()
        self.assertEqual(dispatcher.childlog.handlers[0].reopened, True)
        self.assertEqual(dispatcher.childlog.handlers[0].removed, True)

    def test_reopenlogs(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.reopenlogs()
        self.assertEqual(dispatcher.childlog.handlers[0].reopened, True)

    def test_strip_ansi(self):
        options = DummyOptions()
        options.strip_ansi = True
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        ansi = '\x1b[34mHello world... this is longer than a token!\x1b[0m'
        noansi = 'Hello world... this is longer than a token!'

        options.readfd_result = ansi
        dispatcher.handle_read_event()
        self.assertEqual(len(dispatcher.childlog.data), 1)
        self.assertEqual(dispatcher.childlog.data[0], noansi)

        options.strip_ansi = False

        options.readfd_result = ansi
        dispatcher.handle_read_event()
        self.assertEqual(len(dispatcher.childlog.data), 2)
        self.assertEqual(dispatcher.childlog.data[1], ansi)

    def test_ctor_nologfiles(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.process, process)
        self.assertEqual(dispatcher.channel, 'stdout')
        self.assertEqual(dispatcher.fd, 0)
        self.assertEqual(dispatcher.childlog, None)

    def test_ctor_logfile_only(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1',
                              stdout_logfile='/tmp/foo')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        self.assertEqual(dispatcher.process, process)
        self.assertEqual(dispatcher.channel, 'stdout')
        self.assertEqual(dispatcher.fd, 0)
        self.assertEqual(dispatcher.childlog.__class__, DummyLogger)

    def test_repr(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        drepr = repr(dispatcher)
        self.assertTrue(drepr.startswith('<PEventListenerDispatcher at'), drepr)
        self.assertNotEqual(
            drepr.find('<supervisor.tests.base.DummyProcess instance at'),
            -1)
        self.assertTrue(drepr.endswith('(stdout)>'), drepr)
    
    def test_close(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'process1', '/bin/process1')
        process = DummyProcess(config)
        dispatcher = self._makeOne(process)
        dispatcher.close()
        self.assertEqual(dispatcher.closed, True)
        dispatcher.close() # make sure we don't error if we try to close twice
        self.assertEqual(dispatcher.closed, True)


def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = test_events
import sys
import unittest

from supervisor.tests.base import DummyOptions
from supervisor.tests.base import DummyPConfig
from supervisor.tests.base import DummyProcess
from supervisor.tests.base import DummyEvent

class EventSubscriptionNotificationTests(unittest.TestCase):
    def setUp(self):
        from supervisor import events
        events.callbacks[:] = []

    def tearDown(self):
        from supervisor import events
        events.callbacks[:] = []

    def test_subscribe(self):
        from supervisor import events
        events.subscribe(None, None)
        self.assertEqual(events.callbacks, [(None, None)])

    def test_clear(self):
        from supervisor import events
        events.callbacks[:] = [(None, None)]
        events.clear()
        self.assertEqual(events.callbacks, [])

    def test_notify_true(self):
        from supervisor import events
        L = []
        def callback(event):
            L.append(1)
        class DummyEvent:
            pass
        events.callbacks[:] = [(DummyEvent, callback)]
        events.notify(DummyEvent())
        self.assertEqual(L, [1])

    def test_notify_false(self):
        from supervisor import events
        L = []
        def callback(event):
            L.append(1)
        class DummyEvent:
            pass
        class AnotherEvent:
            pass
        events.callbacks[:] = [(AnotherEvent, callback)]
        events.notify(DummyEvent())
        self.assertEqual(L, [])

    def test_notify_via_subclass(self):
        from supervisor import events
        L = []
        def callback(event):
            L.append(1)
        class DummyEvent:
            pass
        class ASubclassEvent(DummyEvent):
            pass
        events.callbacks[:] = [(DummyEvent, callback)]
        events.notify(ASubclassEvent())
        self.assertEqual(L, [1])
        

class TestEventTypes(unittest.TestCase):
    def test_ProcessLogEvent_attributes(self):
        from supervisor.events import ProcessLogEvent
        inst = ProcessLogEvent(1, 2, 3)
        self.assertEqual(inst.process, 1)
        self.assertEqual(inst.pid, 2)
        self.assertEqual(inst.data, 3)

    def test_ProcessLogEvent_inheritence(self):
        from supervisor.events import ProcessLogEvent
        from supervisor.events import Event
        self.assertTrue(
            issubclass(ProcessLogEvent, Event)
        )

    def test_ProcessLogStdoutEvent_attributes(self):
        from supervisor.events import ProcessLogStdoutEvent
        inst = ProcessLogStdoutEvent(1, 2, 3)
        self.assertEqual(inst.process, 1)
        self.assertEqual(inst.pid, 2)
        self.assertEqual(inst.data, 3)
        self.assertEqual(inst.channel, 'stdout')

    def test_ProcessLogStdoutEvent_inheritence(self):
        from supervisor.events import ProcessLogStdoutEvent
        from supervisor.events import ProcessLogEvent
        self.assertTrue(
            issubclass(ProcessLogStdoutEvent, ProcessLogEvent)
        )

    def test_ProcessLogStderrEvent_attributes(self):
        from supervisor.events import ProcessLogStderrEvent
        inst = ProcessLogStderrEvent(1, 2, 3)
        self.assertEqual(inst.process, 1)
        self.assertEqual(inst.pid, 2)
        self.assertEqual(inst.data, 3)
        self.assertEqual(inst.channel, 'stderr')

    def test_ProcessLogStderrEvent_inheritence(self):
        from supervisor.events import ProcessLogStderrEvent
        from supervisor.events import ProcessLogEvent
        self.assertTrue(
            issubclass(ProcessLogStderrEvent, ProcessLogEvent)
        )

    def test_ProcessCommunicationEvent_attributes(self):
        from supervisor.events import ProcessCommunicationEvent
        inst = ProcessCommunicationEvent(1, 2, 3)
        self.assertEqual(inst.process, 1)
        self.assertEqual(inst.pid, 2)
        self.assertEqual(inst.data, 3)

    def test_ProcessCommunicationEvent_inheritence(self):
        from supervisor.events import ProcessCommunicationEvent
        from supervisor.events import Event
        self.assertTrue(
            issubclass(ProcessCommunicationEvent, Event)
        )

    def test_ProcessCommunicationStdoutEvent_attributes(self):
        from supervisor.events import ProcessCommunicationStdoutEvent
        inst = ProcessCommunicationStdoutEvent(1, 2, 3)
        self.assertEqual(inst.process, 1)
        self.assertEqual(inst.pid, 2)
        self.assertEqual(inst.data, 3)
        self.assertEqual(inst.channel, 'stdout')

    def test_ProcessCommunicationStdoutEvent_inheritence(self):
        from supervisor.events import ProcessCommunicationStdoutEvent
        from supervisor.events import ProcessCommunicationEvent
        self.assertTrue(
            issubclass(ProcessCommunicationStdoutEvent, 
                       ProcessCommunicationEvent)
        )
        
    def test_ProcessCommunicationStderrEvent_attributes(self):
        from supervisor.events import ProcessCommunicationStderrEvent
        inst = ProcessCommunicationStderrEvent(1, 2, 3)
        self.assertEqual(inst.process, 1)
        self.assertEqual(inst.pid, 2)
        self.assertEqual(inst.data, 3)
        self.assertEqual(inst.channel, 'stderr')

    def test_ProcessCommunicationStderrEvent_inheritence(self):
        from supervisor.events import ProcessCommunicationStderrEvent
        from supervisor.events import ProcessCommunicationEvent
        self.assertTrue(
            issubclass(ProcessCommunicationStderrEvent, 
                       ProcessCommunicationEvent)
        )

    def test_RemoteCommunicationEvent_attributes(self):
        from supervisor.events import RemoteCommunicationEvent
        inst = RemoteCommunicationEvent(1, 2)
        self.assertEqual(inst.type, 1)
        self.assertEqual(inst.data, 2)

    def test_RemoteCommunicationEvent_inheritence(self):
        from supervisor.events import RemoteCommunicationEvent
        from supervisor.events import Event
        self.assertTrue(
            issubclass(RemoteCommunicationEvent, Event)
        )

    def test_EventRejectedEvent_attributes(self):
        from supervisor.events import EventRejectedEvent
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process = DummyProcess(pconfig1)
        rejected_event = DummyEvent()
        event = EventRejectedEvent(process, rejected_event)
        self.assertEqual(event.process, process)
        self.assertEqual(event.event, rejected_event)

    def test_EventRejectedEvent_does_not_inherit_from_event(self):
        from supervisor.events import EventRejectedEvent
        from supervisor.events import Event
        self.assertFalse(
            issubclass(EventRejectedEvent, Event)
        )

    def test_all_SupervisorStateChangeEvents(self):
        from supervisor import events
        for klass in (
            events.SupervisorStateChangeEvent,
            events.SupervisorRunningEvent,
            events.SupervisorStoppingEvent        
            ):
            self._test_one_SupervisorStateChangeEvent(klass)

    def _test_one_SupervisorStateChangeEvent(self, klass):
        from supervisor.events import SupervisorStateChangeEvent
        self.assertTrue(issubclass(klass, SupervisorStateChangeEvent))

    def test_all_ProcessStateEvents(self):
        from supervisor import events
        for klass in (
            events.ProcessStateEvent,
            events.ProcessStateStoppedEvent,
            events.ProcessStateExitedEvent,
            events.ProcessStateFatalEvent,
            events.ProcessStateBackoffEvent,
            events.ProcessStateRunningEvent,
            events.ProcessStateUnknownEvent,
            events.ProcessStateStoppingEvent,
            events.ProcessStateStartingEvent,
            ):
            self._test_one_ProcessStateEvent(klass)

    def _test_one_ProcessStateEvent(self, klass):
        from supervisor.states import ProcessStates
        from supervisor.events import ProcessStateEvent
        self.assertTrue(issubclass(klass, ProcessStateEvent))
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process = DummyProcess(pconfig1)
        inst = klass(process, ProcessStates.STARTING)
        self.assertEqual(inst.process, process)
        self.assertEqual(inst.from_state, ProcessStates.STARTING)
        self.assertEqual(inst.expected, True)

    def test_all_TickEvents(self):
        from supervisor import events
        for klass in (
           events.TickEvent,
           events.Tick5Event,
           events.Tick60Event,
           events.Tick3600Event
           ):
           self._test_one_TickEvent(klass)

    def _test_one_TickEvent(self, klass):
        from supervisor.events import TickEvent
        self.assertTrue(issubclass(klass, TickEvent))
        
        inst = klass(1, 2)
        self.assertEqual(inst.when, 1)
        self.assertEqual(inst.supervisord, 2)
        
class TestSerializations(unittest.TestCase):
    def _deserialize(self, serialization):
        data = serialization.split('\n')
        headerdata = data[0]
        payload = ''
        headers = {}
        if len(data) > 1:
            payload = data[1]
        if headerdata:
            try:
                headers = dict( [ x.split(':',1) for x in
                                  headerdata.split()] )
            except ValueError:
                raise AssertionError('headerdata %r could not be deserialized' %
                                     headerdata)
        return headers, payload

    def test_plog_stdout_event(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1)
        from supervisor.events import ProcessLogStdoutEvent
        class DummyGroup:
            config = pconfig1
        process1.group = DummyGroup
        event = ProcessLogStdoutEvent(process1, 1, 'yo')
        headers, payload = self._deserialize(str(event))
        self.assertEqual(headers['processname'], 'process1', headers)
        self.assertEqual(headers['groupname'], 'process1', headers)
        self.assertEqual(headers['pid'], '1', headers)
        self.assertEqual(payload, 'yo')

    def test_plog_stderr_event(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1)
        from supervisor.events import ProcessLogStderrEvent
        class DummyGroup:
            config = pconfig1
        process1.group = DummyGroup
        event = ProcessLogStderrEvent(process1, 1, 'yo')
        headers, payload = self._deserialize(str(event))
        self.assertEqual(headers['processname'], 'process1', headers)
        self.assertEqual(headers['groupname'], 'process1', headers)
        self.assertEqual(headers['pid'], '1', headers)
        self.assertEqual(payload, 'yo')
            
    def test_pcomm_stdout_event(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1)
        from supervisor.events import ProcessCommunicationStdoutEvent
        class DummyGroup:
            config = pconfig1
        process1.group = DummyGroup
        event = ProcessCommunicationStdoutEvent(process1, 1, 'yo')
        headers, payload = self._deserialize(str(event))
        self.assertEqual(headers['processname'], 'process1', headers)
        self.assertEqual(headers['groupname'], 'process1', headers)
        self.assertEqual(headers['pid'], '1', headers)
        self.assertEqual(payload, 'yo')
            
    def test_pcomm_stdout_event(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1)
        from supervisor.events import ProcessCommunicationStdoutEvent
        class DummyGroup:
            config = pconfig1
        process1.group = DummyGroup
        event = ProcessCommunicationStdoutEvent(process1, 1, 'yo')
        headers, payload = self._deserialize(str(event))
        self.assertEqual(headers['processname'], 'process1', headers)
        self.assertEqual(headers['groupname'], 'process1', headers)
        self.assertEqual(headers['pid'], '1', headers)
        self.assertEqual(payload, 'yo')
            
    def test_pcomm_stderr_event(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1)
        class DummyGroup:
            config = pconfig1
        process1.group = DummyGroup
        from supervisor.events import ProcessCommunicationStderrEvent
        event = ProcessCommunicationStderrEvent(process1, 1, 'yo')
        headers, payload = self._deserialize(str(event))
        self.assertEqual(headers['processname'], 'process1', headers)
        self.assertEqual(headers['groupname'], 'process1', headers)
        self.assertEqual(headers['pid'], '1', headers)
        self.assertEqual(payload, 'yo')

    def test_remote_comm_event(self):
        from supervisor.events import RemoteCommunicationEvent
        event = RemoteCommunicationEvent('foo', 'bar')
        headers, payload = self._deserialize(str(event))
        self.assertEqual(headers['type'], 'foo', headers)
        self.assertEqual(payload, 'bar')

    def test_process_state_events_without_extra_values(self):
        from supervisor.states import ProcessStates
        from supervisor import events
        for klass in (
            events.ProcessStateFatalEvent,
            events.ProcessStateUnknownEvent,
            ):
            options = DummyOptions()
            pconfig1 = DummyPConfig(options, 'process1', 'process1',
                                    '/bin/process1')
            class DummyGroup:
                config = pconfig1
            process1 = DummyProcess(pconfig1)
            process1.group = DummyGroup
            event = klass(process1, ProcessStates.STARTING)
            headers, payload = self._deserialize(str(event))
            self.assertEqual(len(headers), 3)
            self.assertEqual(headers['processname'], 'process1')
            self.assertEqual(headers['groupname'], 'process1')
            self.assertEqual(headers['from_state'], 'STARTING')
            self.assertEqual(payload, '')

    def test_process_state_events_with_pid(self):
        from supervisor.states import ProcessStates
        from supervisor import events
        for klass in (
            events.ProcessStateRunningEvent,
            events.ProcessStateStoppedEvent,
            events.ProcessStateStoppingEvent,
            ):
            options = DummyOptions()
            pconfig1 = DummyPConfig(options, 'process1', 'process1',
                                    '/bin/process1')
            class DummyGroup:
                config = pconfig1
            process1 = DummyProcess(pconfig1)
            process1.group = DummyGroup
            process1.pid = 1
            event = klass(process1, ProcessStates.STARTING)
            headers, payload = self._deserialize(str(event))
            self.assertEqual(len(headers), 4)
            self.assertEqual(headers['processname'], 'process1')
            self.assertEqual(headers['groupname'], 'process1')
            self.assertEqual(headers['from_state'], 'STARTING')
            self.assertEqual(headers['pid'], '1')
            self.assertEqual(payload, '')

    def test_process_state_events_starting_and_backoff(self):
        from supervisor.states import ProcessStates
        from supervisor import events
        for klass in (
            events.ProcessStateStartingEvent,
            events.ProcessStateBackoffEvent,
            ):
            options = DummyOptions()
            pconfig1 = DummyPConfig(options, 'process1', 'process1',
                                    '/bin/process1')
            class DummyGroup:
                config = pconfig1
            process1 = DummyProcess(pconfig1)
            process1.group = DummyGroup
            event = klass(process1, ProcessStates.STARTING)
            headers, payload = self._deserialize(str(event))
            self.assertEqual(len(headers), 4)
            self.assertEqual(headers['processname'], 'process1')
            self.assertEqual(headers['groupname'], 'process1')
            self.assertEqual(headers['from_state'], 'STARTING')
            self.assertEqual(headers['tries'], '0')
            self.assertEqual(payload, '')
            process1.backoff = 1
            event = klass(process1, ProcessStates.STARTING)
            headers, payload = self._deserialize(str(event))
            self.assertEqual(headers['tries'], '1')
            process1.backoff = 2
            event = klass(process1, ProcessStates.STARTING)
            headers, payload = self._deserialize(str(event))
            self.assertEqual(headers['tries'], '2')
        
    def test_process_state_exited_event_expected(self):
        from supervisor import events
        from supervisor.states import ProcessStates
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1)
        class DummyGroup:
            config = pconfig1
        process1.group = DummyGroup
        process1.pid = 1
        event = events.ProcessStateExitedEvent(process1,
                                               ProcessStates.STARTING,
                                               expected=True)
        headers, payload = self._deserialize(str(event))
        self.assertEqual(len(headers), 5)
        self.assertEqual(headers['processname'], 'process1')
        self.assertEqual(headers['groupname'], 'process1')
        self.assertEqual(headers['pid'], '1')
        self.assertEqual(headers['from_state'], 'STARTING')
        self.assertEqual(headers['expected'], '1')
        self.assertEqual(payload, '')

    def test_process_state_exited_event_unexpected(self):
        from supervisor import events
        from supervisor.states import ProcessStates
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1)
        class DummyGroup:
            config = pconfig1
        process1.group = DummyGroup
        process1.pid = 1
        event = events.ProcessStateExitedEvent(process1,
                                               ProcessStates.STARTING,
                                               expected=False)
        headers, payload = self._deserialize(str(event))
        self.assertEqual(len(headers), 5)
        self.assertEqual(headers['processname'], 'process1')
        self.assertEqual(headers['groupname'], 'process1')
        self.assertEqual(headers['pid'], '1')
        self.assertEqual(headers['from_state'], 'STARTING')
        self.assertEqual(headers['expected'], '0')
        self.assertEqual(payload, '')

    def test_supervisor_sc_event(self):
        from supervisor import events
        event = events.SupervisorRunningEvent()
        headers, payload = self._deserialize(str(event))
        self.assertEqual(headers, {})
        self.assertEqual(payload, '')

    def test_tick_events(self):
        from supervisor import events
        for klass in (
            events.Tick5Event,
            events.Tick60Event,
            events.Tick3600Event,
            ):
            event = klass(1, 2)
            headers, payload = self._deserialize(str(event))
            self.assertEqual(headers, {'when':'1'})
            self.assertEqual(payload, '')

class TestUtilityFunctions(unittest.TestCase):
    def test_getEventNameByType(self):
        from supervisor import events
        for name, value in events.EventTypes.__dict__.items():
            self.assertEqual(events.getEventNameByType(value), name)

    def _assertStateChange(self, old, new, expected):
        from supervisor.events import getProcessStateChangeEventType
        klass = getProcessStateChangeEventType(old, new)
        self.assertEqual(expected, klass)


def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')


########NEW FILE########
__FILENAME__ = test_http
import sys
import os
import socket
import tempfile
import unittest

try:
    from hashlib import sha1
except ImportError:
    from sha import new as sha1

from supervisor.tests.base import DummySupervisor
from supervisor.tests.base import PopulatedDummySupervisor
from supervisor.tests.base import DummyRPCInterfaceFactory
from supervisor.tests.base import DummyPConfig
from supervisor.tests.base import DummyOptions
from supervisor.tests.base import DummyRequest

from supervisor.http import NOT_DONE_YET

class HandlerTests:
    def _makeOne(self, supervisord):
        return self._getTargetClass()(supervisord)

    def test_match(self):
        class DummyRequest:
            def __init__(self, uri):
                self.uri = uri
        supervisor = DummySupervisor()
        handler = self._makeOne(supervisor)
        self.assertEqual(handler.match(DummyRequest(handler.path)), True)

class LogtailHandlerTests(HandlerTests, unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.http import logtail_handler
        return logtail_handler

    def test_handle_request_stdout_logfile_none(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'process1', '/bin/process1', priority=1,
                               stdout_logfile='/tmp/process1.log')
        supervisord = PopulatedDummySupervisor(options, 'process1', pconfig)
        handler = self._makeOne(supervisord)
        request = DummyRequest('/logtail/process1', None, None, None)
        handler.handle_request(request)
        self.assertEqual(request._error, 410)

    def test_handle_request_stdout_logfile_missing(self):
        supervisor = DummySupervisor()
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', 'foo', 'it/is/missing')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        handler = self._makeOne(supervisord)
        request = DummyRequest('/logtail/foo', None, None, None)
        handler.handle_request(request)
        self.assertEqual(request._error, 410)

    def test_handle_request(self):
        supervisor = DummySupervisor()
        import tempfile
        import os
        import stat
        f = tempfile.NamedTemporaryFile()
        t = f.name
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', 'foo', stdout_logfile=t)
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        handler = self._makeOne(supervisord)
        request = DummyRequest('/logtail/foo', None, None, None)
        handler.handle_request(request)
        self.assertEqual(request._error, None)
        from supervisor.medusa import http_date
        self.assertEqual(request.headers['Last-Modified'],
                         http_date.build_http_date(os.stat(t)[stat.ST_MTIME]))
        self.assertEqual(request.headers['Content-Type'], 'text/plain')
        self.assertEqual(len(request.producers), 1)
        self.assertEqual(request._done, True)

class MainLogTailHandlerTests(HandlerTests, unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.http import mainlogtail_handler
        return mainlogtail_handler

    def test_handle_request_stdout_logfile_none(self):
        supervisor = DummySupervisor()
        handler = self._makeOne(supervisor)
        request = DummyRequest('/mainlogtail', None, None, None)
        handler.handle_request(request)
        self.assertEqual(request._error, 410)

    def test_handle_request_stdout_logfile_missing(self):
        supervisor = DummySupervisor()
        supervisor.options.logfile = '/not/there'
        request = DummyRequest('/mainlogtail', None, None, None)
        handler = self._makeOne(supervisor)
        handler.handle_request(request)
        self.assertEqual(request._error, 410)

    def test_handle_request(self):
        supervisor = DummySupervisor()
        import tempfile
        import os
        import stat
        f = tempfile.NamedTemporaryFile()
        t = f.name
        supervisor.options.logfile = t
        handler = self._makeOne(supervisor)
        request = DummyRequest('/mainlogtail', None, None, None)
        handler.handle_request(request)
        self.assertEqual(request._error, None)
        from supervisor.medusa import http_date
        self.assertEqual(request.headers['Last-Modified'],
                         http_date.build_http_date(os.stat(t)[stat.ST_MTIME]))
        self.assertEqual(request.headers['Content-Type'], 'text/plain')
        self.assertEqual(len(request.producers), 1)
        self.assertEqual(request._done, True)
    

class TailFProducerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.http import tail_f_producer
        return tail_f_producer

    def _makeOne(self, request, filename, head):
        return self._getTargetClass()(request, filename, head)

    def test_handle_more(self):
        request = DummyRequest('/logtail/foo', None, None, None)
        import tempfile
        from supervisor import http
        f = tempfile.NamedTemporaryFile()
        f.write('a' * 80)
        f.flush()
        t = f.name
        producer = self._makeOne(request, t, 80)
        result = producer.more()
        self.assertEqual(result, 'a' * 80)
        f.write('w' * 100)
        f.flush()
        result = producer.more()
        self.assertEqual(result, 'w' * 100)
        result = producer.more()
        self.assertEqual(result, http.NOT_DONE_YET)
        f.truncate(0)
        f.flush()
        result = producer.more()
        self.assertEqual(result, '==> File truncated <==\n')

class DeferringChunkedProducerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.http import deferring_chunked_producer
        return deferring_chunked_producer

    def _makeOne(self, producer, footers=None):
        return self._getTargetClass()(producer, footers)

    def test_more_not_done_yet(self):
        wrapped = DummyProducer(NOT_DONE_YET)
        producer = self._makeOne(wrapped)
        self.assertEqual(producer.more(), NOT_DONE_YET)

    def test_more_string(self):
        wrapped = DummyProducer('hello')
        producer = self._makeOne(wrapped)
        self.assertEqual(producer.more(), '5\r\nhello\r\n')

    def test_more_nodata(self):
        wrapped = DummyProducer()
        producer = self._makeOne(wrapped, footers=['a', 'b'])
        self.assertEqual(producer.more(), '0\r\na\r\nb\r\n\r\n')

class DeferringCompositeProducerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.http import deferring_composite_producer
        return deferring_composite_producer

    def _makeOne(self, producers):
        return self._getTargetClass()(producers)

    def test_more_not_done_yet(self):
        wrapped = DummyProducer(NOT_DONE_YET)
        producer = self._makeOne([wrapped])
        self.assertEqual(producer.more(), NOT_DONE_YET)

    def test_more_string(self):
        wrapped1 = DummyProducer('hello')
        wrapped2 = DummyProducer('goodbye')
        producer = self._makeOne([wrapped1, wrapped2])
        self.assertEqual(producer.more(), 'hello')
        self.assertEqual(producer.more(), 'goodbye')
        self.assertEqual(producer.more(), '')

    def test_more_nodata(self):
        wrapped = DummyProducer()
        producer = self._makeOne([wrapped])
        self.assertEqual(producer.more(), '')

class DeferringGlobbingProducerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.http import deferring_globbing_producer
        return deferring_globbing_producer

    def _makeOne(self, producer, buffer_size=1<<16):
        return self._getTargetClass()(producer, buffer_size)

    def test_more_not_done_yet(self):
        wrapped = DummyProducer(NOT_DONE_YET)
        producer = self._makeOne(wrapped)
        self.assertEqual(producer.more(), NOT_DONE_YET)

    def test_more_string(self):
        wrapped = DummyProducer('hello', 'there', 'guy')
        producer = self._makeOne(wrapped, buffer_size=1)
        self.assertEqual(producer.more(), 'hello')

        wrapped = DummyProducer('hello', 'there', 'guy')
        producer = self._makeOne(wrapped, buffer_size=50)
        self.assertEqual(producer.more(), 'hellothereguy')

    def test_more_nodata(self):
        wrapped = DummyProducer()
        producer = self._makeOne(wrapped)
        self.assertEqual(producer.more(), '')

class DeferringHookedProducerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.http import deferring_hooked_producer
        return deferring_hooked_producer

    def _makeOne(self, producer, function):
        return self._getTargetClass()(producer, function)

    def test_more_not_done_yet(self):
        wrapped = DummyProducer(NOT_DONE_YET)
        producer = self._makeOne(wrapped, None)
        self.assertEqual(producer.more(), NOT_DONE_YET)

    def test_more_string(self):
        wrapped = DummyProducer('hello')
        L = []
        def callback(bytes):
            L.append(bytes)
        producer = self._makeOne(wrapped, callback)
        self.assertEqual(producer.more(), 'hello')
        self.assertEqual(L, [])
        producer.more()
        self.assertEqual(L, [5])

    def test_more_nodata(self):
        wrapped = DummyProducer()
        L = []
        def callback(bytes):
            L.append(bytes)
        producer = self._makeOne(wrapped, callback)
        self.assertEqual(producer.more(), '')
        self.assertEqual(L, [0])

class EncryptedDictionaryAuthorizedTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.http import encrypted_dictionary_authorizer
        return encrypted_dictionary_authorizer

    def _makeOne(self, dict):
        return self._getTargetClass()(dict)

    def test_authorize_baduser(self):
        authorizer = self._makeOne({})
        self.assertEqual(authorizer.authorize(('foo', 'bar')), False)
        
    def test_authorize_gooduser_badpassword(self):
        authorizer = self._makeOne({'foo':'password'})
        self.assertEqual(authorizer.authorize(('foo', 'bar')), False)

    def test_authorize_gooduser_goodpassword(self):
        authorizer = self._makeOne({'foo':'password'})
        self.assertEqual(authorizer.authorize(('foo', 'password')), True)
    
    def test_authorize_gooduser_badpassword_sha(self):
        password = '{SHA}' + sha1('password').hexdigest()
        authorizer = self._makeOne({'foo':password})
        self.assertEqual(authorizer.authorize(('foo', 'bar')), False)

    def test_authorize_gooduser_goodpassword_sha(self):
        password = '{SHA}' + sha1('password').hexdigest()
        authorizer = self._makeOne({'foo':password})
        self.assertEqual(authorizer.authorize(('foo', 'password')), True)

class SupervisorAuthHandlerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.http import supervisor_auth_handler
        return supervisor_auth_handler

    def _makeOne(self, dict, handler):
        return self._getTargetClass()(dict, handler)

    def test_ctor(self):
        handler = self._makeOne({'a':1}, None)
        from supervisor.http import encrypted_dictionary_authorizer
        self.assertEqual(handler.authorizer.__class__,
                         encrypted_dictionary_authorizer)
    

class TopLevelFunctionTests(unittest.TestCase):
    def _make_http_servers(self, sconfigs):
        options = DummyOptions()
        options.server_configs = sconfigs
        options.rpcinterface_factories = [('dummy',DummyRPCInterfaceFactory,{})]
        supervisord = DummySupervisor()
        from supervisor.http import make_http_servers
        servers = make_http_servers(options, supervisord)
        try:
            for config, s in servers:
                s.close()
                socketfile = config.get('file')
                if socketfile is not None:
                    os.unlink(socketfile)
        finally:
            from asyncore import socket_map
            socket_map.clear()
        return servers
        
    def test_make_http_servers_noauth(self):
        socketfile = tempfile.mktemp()
        inet = {'family':socket.AF_INET, 'host':'localhost', 'port':17735,
                'username':None, 'password':None, 'section':'inet_http_server'}
        unix = {'family':socket.AF_UNIX, 'file':socketfile, 'chmod':0700,
                'chown':(-1, -1), 'username':None, 'password':None,
                'section':'unix_http_server'}
        servers = self._make_http_servers([inet, unix])
        self.assertEqual(len(servers), 2)

        inetdata = servers[0]
        self.assertEqual(inetdata[0], inet)
        server = inetdata[1]
        idents = [
            'Supervisor XML-RPC Handler',
            'Logtail HTTP Request Handler',
            'Main Logtail HTTP Request Handler',
            'Supervisor Web UI HTTP Request Handler',
            'Default HTTP Request Handler'
            ]
        self.assertEqual([x.IDENT for x in server.handlers], idents)

        unixdata = servers[1]
        self.assertEqual(unixdata[0], unix)
        server = unixdata[1]
        self.assertEqual([x.IDENT for x in server.handlers], idents)

    def test_make_http_servers_withauth(self):
        socketfile = tempfile.mktemp()
        inet = {'family':socket.AF_INET, 'host':'localhost', 'port':17736,
                'username':'username', 'password':'password',
                'section':'inet_http_server'}
        unix = {'family':socket.AF_UNIX, 'file':socketfile, 'chmod':0700,
                'chown':(-1, -1), 'username':'username', 'password':'password',
                'section':'unix_http_server'}
        servers = self._make_http_servers([inet, unix])
        self.assertEqual(len(servers), 2)
        from supervisor.http import supervisor_auth_handler
        for config, server in servers:
            for handler in server.handlers:
                self.failUnless(isinstance(handler, supervisor_auth_handler),
                                handler)

class DummyProducer:
    def __init__(self, *data):
        self.data = list(data)

    def more(self):
        if self.data:
            return self.data.pop(0)
        else:
            return ''

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = test_loggers
import sys
import unittest
import tempfile
import shutil
import os
import syslog

import mock

from supervisor.tests.base import DummyStream

class LevelTests(unittest.TestCase):
    def test_LOG_LEVELS_BY_NUM_doesnt_include_builtins(self):
        from supervisor import loggers
        for level_name in loggers.LOG_LEVELS_BY_NUM.values():
            self.assertFalse(level_name.startswith('_'))

class HandlerTests:
    def setUp(self):
        self.basedir = tempfile.mkdtemp()
        self.filename = os.path.join(self.basedir, 'thelog')

    def tearDown(self):
        try:
            shutil.rmtree(self.basedir)
        except OSError:
            pass

    def _makeOne(self, *arg, **kw):
        klass = self._getTargetClass()
        return klass(*arg, **kw)

    def _makeLogRecord(self, msg):
        from supervisor import loggers
        record = loggers.LogRecord(level=loggers.LevelsByName.INFO,
                                   msg=msg,
                                   exc_info=None)
        return record

class FileHandlerTests(HandlerTests, unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.loggers import FileHandler
        return FileHandler

    def test_ctor(self):
        handler = self._makeOne(self.filename)
        self.assertTrue(os.path.exists(self.filename), self.filename)
        self.assertEqual(handler.mode, 'a')
        self.assertEqual(handler.baseFilename, self.filename)
        self.assertEqual(handler.stream.name, self.filename)

    def test_close(self):
        handler = self._makeOne(self.filename)
        handler.stream = DummyStream()
        handler.close()
        self.assertEqual(handler.stream.closed, True)

    def test_close_raises(self):
        handler = self._makeOne(self.filename)
        handler.stream = DummyStream(OSError)
        self.assertRaises(OSError, handler.close)
        self.assertEqual(handler.stream.closed, False)

    def test_reopen(self):
        handler = self._makeOne(self.filename)
        stream = DummyStream()
        handler.stream = stream
        handler.reopen()
        self.assertEqual(stream.closed, True)
        self.assertEqual(handler.stream.name, self.filename)

    def test_reopen_raises(self):
        handler = self._makeOne(self.filename)
        stream = DummyStream()
        handler.stream = stream
        handler.baseFilename = os.path.join(self.basedir, 'notthere', 'a.log')
        self.assertRaises(IOError, handler.reopen)
        self.assertEqual(stream.closed, True)

    def test_remove_exists(self):
        handler = self._makeOne(self.filename)
        self.assertTrue(os.path.exists(self.filename), self.filename)
        handler.remove()
        self.assertFalse(os.path.exists(self.filename), self.filename)

    def test_remove_doesntexist(self):
        handler = self._makeOne(self.filename)
        os.remove(self.filename)
        self.assertFalse(os.path.exists(self.filename), self.filename)
        handler.remove() # should not raise
        self.assertFalse(os.path.exists(self.filename), self.filename)

    def test_remove_raises(self):
        handler = self._makeOne(self.filename)
        os.remove(self.filename)
        os.mkdir(self.filename)
        self.assertTrue(os.path.exists(self.filename), self.filename)
        self.assertRaises(OSError, handler.remove)

    def test_emit_ascii_noerror(self):
        handler = self._makeOne(self.filename)
        record = self._makeLogRecord('hello!')
        handler.emit(record)
        content = open(self.filename, 'r').read()
        self.assertEqual(content, 'hello!')

    def test_emit_unicode_noerror(self):
        handler = self._makeOne(self.filename)
        record = self._makeLogRecord(u'fi\xed')
        handler.emit(record)
        content = open(self.filename, 'r').read()
        self.assertEqual(content, 'fi\xc3\xad')

    def test_emit_error(self):
        handler = self._makeOne(self.filename)
        handler.stream = DummyStream(error=OSError)
        record = self._makeLogRecord('hello!')
        try:
            old_stderr = sys.stderr
            dummy_stderr = DummyStream()
            sys.stderr = dummy_stderr
            handler.emit(record)
        finally:
            sys.stderr = old_stderr

        self.assertTrue(dummy_stderr.written.endswith('OSError\n'),
                        dummy_stderr.written)

class RotatingFileHandlerTests(FileHandlerTests):

    def _getTargetClass(self):
        from supervisor.loggers import RotatingFileHandler
        return RotatingFileHandler

    def test_ctor(self):
        handler = self._makeOne(self.filename)
        self.assertEqual(handler.mode, 'a')
        self.assertEqual(handler.maxBytes, 512*1024*1024)
        self.assertEqual(handler.backupCount, 10)

    def test_emit_tracks_correct_file_for_multiple_handlers(self):
        """
        Rollovers should roll for all handlers of the same file.

        When more than one process logs to a singlefile, we want to
        make sure that files get rotated properly.

        When the file rotates, all handlers should start writing to
        the file specified by handler.baseFilename.
        """
        handler1 = self._makeOne(self.filename, maxBytes=10, backupCount=2)
        handler2 = self._makeOne(self.filename, maxBytes=10, backupCount=2)
        record = self._makeLogRecord('a' * 4)
        handler1.emit(record) #4 bytes
        handler2.emit(record) #8 bytes
        self.assertFalse(os.path.exists(self.filename + '.1'))
        handler1.emit(record) #12 bytes
        self.assertTrue(os.path.exists(self.filename + '.1'))
        self.assertTrue(handler1.stream == handler2.stream)
        new_record = self._makeLogRecord("NEW")
        handler2.emit(new_record)
        self.assertTrue(open(self.filename).read().endswith("NEW"))
        handler1.emit(record)
        self.assertTrue(open(self.filename).read().endswith("aaaa"))
        handler2.emit(new_record)
        self.assertTrue(open(self.filename).read().endswith(""))

    def test_reopen_raises(self):
        handler = self._makeOne(self.filename)
        stream = DummyStream()
        handler.baseFilename = os.path.join(self.basedir, 'notthere', 'a.log')
        handler.open_streams[handler.baseFilename] = stream
        self.assertRaises(IOError, handler.reopen)
        self.assertEqual(stream.closed, True)

    def test_emit_does_rollover(self):
        handler = self._makeOne(self.filename, maxBytes=10, backupCount=2)
        record = self._makeLogRecord('a' * 4)

        handler.emit(record) # 4 bytes
        self.assertFalse(os.path.exists(self.filename + '.1'))
        self.assertFalse(os.path.exists(self.filename + '.2'))

        handler.emit(record) # 8 bytes
        self.assertFalse(os.path.exists(self.filename + '.1'))
        self.assertFalse(os.path.exists(self.filename + '.2'))

        handler.emit(record) # 12 bytes, do rollover
        self.assertTrue(os.path.exists(self.filename + '.1'))
        self.assertFalse(os.path.exists(self.filename + '.2'))

        handler.emit(record) # 16 bytes
        self.assertTrue(os.path.exists(self.filename + '.1'))
        self.assertFalse(os.path.exists(self.filename + '.2'))

        handler.emit(record) # 20 bytes
        self.assertTrue(os.path.exists(self.filename + '.1'))
        self.assertFalse(os.path.exists(self.filename + '.2'))

        handler.emit(record) # 24 bytes, do rollover
        self.assertTrue(os.path.exists(self.filename + '.1'))
        self.assertTrue(os.path.exists(self.filename + '.2'))

        handler.emit(record) # 28 bytes
        self.assertTrue(os.path.exists(self.filename + '.1'))
        self.assertTrue(os.path.exists(self.filename + '.2'))

        current = open(self.filename,'r').read()
        self.assertEqual(current, 'a' * 4)
        one = open(self.filename+ '.1','r').read()
        self.assertEqual(one, 'a'*12)
        two = open(self.filename+ '.2','r').read()
        self.assertEqual(two, 'a'*12)

class BoundIOTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.loggers import BoundIO
        return BoundIO

    def _makeOne(self, maxbytes, buf=''):
        klass = self._getTargetClass()
        return klass(maxbytes, buf)

    def test_write_overflow(self):
        io = self._makeOne(1, 'a')
        io.write('b')
        self.assertEqual(io.buf, 'b')

    def test_getvalue(self):
        io = self._makeOne(1, 'a')
        self.assertEqual(io.getvalue(), 'a')

    def test_clear(self):
        io = self._makeOne(1, 'a')
        io.clear()
        self.assertEqual(io.buf, '')

    def test_close(self):
        io = self._makeOne(1, 'a')
        io.close()
        self.assertEqual(io.buf, '')

class LoggerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.loggers import Logger
        return Logger

    def _makeOne(self, level=None, handlers=None):
        klass = self._getTargetClass()
        return klass(level, handlers)

    def test_blather(self):
        from supervisor.loggers import LevelsByName
        handler = DummyHandler(LevelsByName.BLAT)
        logger = self._makeOne(LevelsByName.BLAT, (handler,))
        logger.blather('hello')
        self.assertEqual(len(handler.records), 1)
        logger.level = LevelsByName.TRAC
        logger.blather('hello')
        self.assertEqual(len(handler.records), 1)

    def test_trace(self):
        from supervisor.loggers import LevelsByName
        handler = DummyHandler(LevelsByName.TRAC)
        logger = self._makeOne(LevelsByName.TRAC, (handler,))
        logger.trace('hello')
        self.assertEqual(len(handler.records), 1)
        logger.level = LevelsByName.DEBG
        logger.trace('hello')
        self.assertEqual(len(handler.records), 1)

    def test_debug(self):
        from supervisor.loggers import LevelsByName
        handler = DummyHandler(LevelsByName.DEBG)
        logger = self._makeOne(LevelsByName.DEBG, (handler,))
        logger.debug('hello')
        self.assertEqual(len(handler.records), 1)
        logger.level = LevelsByName.INFO
        logger.debug('hello')
        self.assertEqual(len(handler.records), 1)

    def test_info(self):
        from supervisor.loggers import LevelsByName
        handler = DummyHandler(LevelsByName.INFO)
        logger = self._makeOne(LevelsByName.INFO, (handler,))
        logger.info('hello')
        self.assertEqual(len(handler.records), 1)
        logger.level = LevelsByName.WARN
        logger.info('hello')
        self.assertEqual(len(handler.records), 1)

    def test_warn(self):
        from supervisor.loggers import LevelsByName
        handler = DummyHandler(LevelsByName.WARN)
        logger = self._makeOne(LevelsByName.WARN, (handler,))
        logger.warn('hello')
        self.assertEqual(len(handler.records), 1)
        logger.level = LevelsByName.ERRO
        logger.warn('hello')
        self.assertEqual(len(handler.records), 1)

    def test_error(self):
        from supervisor.loggers import LevelsByName
        handler = DummyHandler(LevelsByName.ERRO)
        logger = self._makeOne(LevelsByName.ERRO, (handler,))
        logger.error('hello')
        self.assertEqual(len(handler.records), 1)
        logger.level = LevelsByName.CRIT
        logger.error('hello')
        self.assertEqual(len(handler.records), 1)

    def test_critical(self):
        from supervisor.loggers import LevelsByName
        handler = DummyHandler(LevelsByName.CRIT)
        logger = self._makeOne(LevelsByName.CRIT, (handler,))
        logger.critical('hello')
        self.assertEqual(len(handler.records), 1)

    def test_close(self):
        from supervisor.loggers import LevelsByName
        handler = DummyHandler(LevelsByName.CRIT)
        logger = self._makeOne(LevelsByName.CRIT, (handler,))
        logger.close()
        self.assertEqual(handler.closed, True)

class MockSysLog(mock.Mock):
    def __call__(self, *args, **kwargs):
        message = args[-1]
        if sys.version_info < (3, 0) and isinstance(message, unicode):
            # Python 2.x raises a UnicodeEncodeError when attempting to
            #  transmit unicode characters that don't encode in the
            #  default encoding.
            message.encode()
        super(MockSysLog, self).__call__(*args, **kwargs)

class SyslogHandlerTests(HandlerTests, unittest.TestCase):
    def setUp(self):
        pass

    def tearDown(self):
        pass

    def _getTargetClass(self):
        return __import__('supervisor.loggers').loggers.SyslogHandler

    def _makeOne(self):
        return self._getTargetClass()()

    @mock.patch('syslog.syslog', MockSysLog())
    def test_emit_ascii_noerror(self):
        handler = self._makeOne()
        record = self._makeLogRecord('hello!')
        handler.emit(record)
        syslog.syslog.assert_called_with('hello!')

    @mock.patch('syslog.syslog', MockSysLog())
    def test_emit_unicode_noerror(self):
        handler = self._makeOne()
        record = self._makeLogRecord(u'fi\xed')
        handler.emit(record)
        if sys.version_info < (3, 0):
            syslog.syslog.assert_called_with('fi\xc3\xad')
        else:
            syslog.syslog.assert_called_with(u'fi\xed')

class DummyHandler:
    close = False
    def __init__(self, level):
        self.level = level
        self.records = []
    def emit(self, record):
        self.records.append(record)
    def close(self):
        self.closed = True

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = test_options
"""Test suite for supervisor.options"""

import os
import sys
import tempfile
import socket
import unittest
import signal
import shutil
import errno
from mock import Mock, patch, sentinel

from supervisor.tests.base import DummySupervisor
from supervisor.tests.base import DummyLogger
from supervisor.tests.base import DummyOptions
from supervisor.tests.base import DummyPConfig
from supervisor.tests.base import DummyProcess
from supervisor.tests.base import DummySocketConfig
from supervisor.tests.base import lstrip

class OptionTests(unittest.TestCase):

    def _makeOptions(self, read_error=False):
        from cStringIO import StringIO
        from supervisor.options import Options
        from supervisor.datatypes import integer

        class MyOptions(Options):
            master = {
                'other': 41 }
            def __init__(self, read_error=read_error):
                self.read_error = read_error
                Options.__init__(self)
                class Foo(object): pass
                self.configroot = Foo()

            def read_config(self, fp):
                if self.read_error:
                    raise ValueError(self.read_error)
                # Pretend we read it from file:
                self.configroot.__dict__.update(self.default_map)
                self.configroot.__dict__.update(self.master)

        options = MyOptions()
        options.configfile = StringIO()
        options.add(name='anoption', confname='anoption',
                    short='o', long='option', default='default')
        options.add(name='other', confname='other', env='OTHER',
                    short='p:', long='other=', handler=integer)
        return options

    def test_options_and_args_order(self):
        # Only config file exists
        options = self._makeOptions()
        options.realize([])
        self.assertEquals(options.anoption, 'default')
        self.assertEquals(options.other, 41)

        # Env should trump config
        options = self._makeOptions()
        os.environ['OTHER'] = '42'
        options.realize([])
        self.assertEquals(options.other, 42)

        # Opt should trump both env (still set) and config
        options = self._makeOptions()
        options.realize(['-p', '43'])
        self.assertEquals(options.other, 43)
        del os.environ['OTHER']

    def test_config_reload(self):
        options = self._makeOptions()
        options.realize([])
        self.assertEquals(options.other, 41)
        options.master['other'] = 42
        options.process_config_file()
        self.assertEquals(options.other, 42)

    def test_config_reload_do_usage_false(self):
        options = self._makeOptions(read_error='error')
        self.assertRaises(ValueError, options.process_config_file,
                          False)

    def test_config_reload_do_usage_true(self):
        options = self._makeOptions(read_error='error')
        from StringIO import StringIO
        L = []
        def exit(num):
            L.append(num)
        options.stderr = options.stdout = StringIO()
        options.exit = exit
        options.configroot.anoption = 1
        options.configroot.other = 1
        options.process_config_file(True)
        self.assertEqual(L, [2])

    def test__set(self):
        from supervisor.options import Options
        options = Options()
        options._set('foo', 'bar', 0)
        self.assertEquals(options.foo, 'bar')
        self.assertEquals(options.attr_priorities['foo'], 0)
        options._set('foo', 'baz', 1)
        self.assertEquals(options.foo, 'baz')
        self.assertEquals(options.attr_priorities['foo'], 1)
        options._set('foo', 'gazonk', 0)
        self.assertEquals(options.foo, 'baz')
        self.assertEquals(options.attr_priorities['foo'], 1)
        options._set('foo', 'gazonk', 1)
        self.assertEquals(options.foo, 'gazonk')

class ClientOptionsTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.options import ClientOptions
        return ClientOptions

    def _makeOne(self):
        return self._getTargetClass()()

    def test_options(self):
        tempdir = tempfile.gettempdir()
        s = lstrip("""[supervisorctl]
        serverurl=http://localhost:9001
        username=chris
        password=123
        prompt=mysupervisor
        history_file=%s/sc_history
        """ % tempdir)

        from StringIO import StringIO
        fp = StringIO(s)
        instance = self._makeOne()
        instance.configfile = fp
        instance.realize(args=[])
        self.assertEqual(instance.interactive, True)
        history_file = os.path.join(tempdir, 'sc_history')
        self.assertEqual(instance.history_file, history_file)
        options = instance.configroot.supervisorctl
        self.assertEqual(options.prompt, 'mysupervisor')
        self.assertEqual(options.serverurl, 'http://localhost:9001')
        self.assertEqual(options.username, 'chris')
        self.assertEqual(options.password, '123')
        self.assertEqual(options.history_file, history_file)

    def test_options_unixsocket_cli(self):
        from StringIO import StringIO
        fp = StringIO('[supervisorctl]')
        instance = self._makeOne()
        instance.configfile = fp
        instance.realize(args=['--serverurl', 'unix:///dev/null'])
        self.assertEqual(instance.serverurl, 'unix:///dev/null')

class ServerOptionsTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.options import ServerOptions
        return ServerOptions

    def _makeOne(self):
        return self._getTargetClass()()

    def test_version(self):
        from supervisor.options import VERSION
        options = self._makeOne()
        from StringIO import StringIO
        options.stdout = StringIO()
        self.assertRaises(SystemExit, options.version, None)
        self.assertEqual(options.stdout.getvalue(), VERSION + '\n')

    def test_options(self):
        s = lstrip("""[inet_http_server]
        port=127.0.0.1:8999
        username=chrism
        password=foo

        [supervisord]
        directory=%(tempdir)s
        backofflimit=10
        user=root
        umask=022
        logfile=supervisord.log
        logfile_maxbytes=1000MB
        logfile_backups=5
        loglevel=error
        pidfile=supervisord.pid
        nodaemon=true
        identifier=fleeb
        childlogdir=%(tempdir)s
        nocleanup=true
        minfds=2048
        minprocs=300
        environment=FAKE_ENV_VAR=/some/path

        [program:cat1]
        command=/bin/cat
        priority=1
        autostart=true
        user=root
        stdout_logfile=/tmp/cat.log
        stopsignal=KILL
        stopwaitsecs=5
        startsecs=5
        startretries=10
        directory=/tmp
        umask=002

        [program:cat2]
        priority=2
        command=/bin/cat
        autostart=true
        autorestart=false
        stdout_logfile_maxbytes = 1024
        stdout_logfile_backups = 2
        stdout_logfile = /tmp/cat2.log

        [program:cat3]
        priority=3
        process_name = replaced
        command=/bin/cat
        autorestart=true
        exitcodes=0,1,127
        stopasgroup=true
        killasgroup=true

        [program:cat4]
        priority=4
        process_name = fleeb_%%(process_num)s
        numprocs = 2
        command = /bin/cat
        autorestart=unexpected
        """ % {'tempdir':tempfile.gettempdir()})

        from supervisor import datatypes

        from StringIO import StringIO
        fp = StringIO(s)
        instance = self._makeOne()
        instance.configfile = fp
        instance.realize(args=[])
        options = instance.configroot.supervisord
        self.assertEqual(options.directory, tempfile.gettempdir())
        self.assertEqual(options.umask, 022)
        self.assertEqual(options.logfile, 'supervisord.log')
        self.assertEqual(options.logfile_maxbytes, 1000 * 1024 * 1024)
        self.assertEqual(options.logfile_backups, 5)
        self.assertEqual(options.loglevel, 40)
        self.assertEqual(options.pidfile, 'supervisord.pid')
        self.assertEqual(options.nodaemon, True)
        self.assertEqual(options.identifier, 'fleeb')
        self.assertEqual(options.childlogdir, tempfile.gettempdir())
        self.assertEqual(len(options.server_configs), 1)
        self.assertEqual(options.server_configs[0]['family'], socket.AF_INET)
        self.assertEqual(options.server_configs[0]['host'], '127.0.0.1')
        self.assertEqual(options.server_configs[0]['port'], 8999)
        self.assertEqual(options.server_configs[0]['username'], 'chrism')
        self.assertEqual(options.server_configs[0]['password'], 'foo')
        self.assertEqual(options.nocleanup, True)
        self.assertEqual(options.minfds, 2048)
        self.assertEqual(options.minprocs, 300)
        self.assertEqual(options.nocleanup, True)
        self.assertEqual(len(options.process_group_configs), 4)
        self.assertEqual(options.environment, dict(FAKE_ENV_VAR='/some/path'))

        cat1 = options.process_group_configs[0]
        self.assertEqual(cat1.name, 'cat1')
        self.assertEqual(cat1.priority, 1)
        self.assertEqual(len(cat1.process_configs), 1)

        proc1 = cat1.process_configs[0]
        self.assertEqual(proc1.name, 'cat1')
        self.assertEqual(proc1.command, '/bin/cat')
        self.assertEqual(proc1.priority, 1)
        self.assertEqual(proc1.autostart, True)
        self.assertEqual(proc1.autorestart, datatypes.RestartWhenExitUnexpected)
        self.assertEqual(proc1.startsecs, 5)
        self.assertEqual(proc1.startretries, 10)
        self.assertEqual(proc1.uid, 0)
        self.assertEqual(proc1.stdout_logfile, '/tmp/cat.log')
        self.assertEqual(proc1.stopsignal, signal.SIGKILL)
        self.assertEqual(proc1.stopwaitsecs, 5)
        self.assertEqual(proc1.stopasgroup, False)
        self.assertEqual(proc1.killasgroup, False)
        self.assertEqual(proc1.stdout_logfile_maxbytes,
                         datatypes.byte_size('50MB'))
        self.assertEqual(proc1.stdout_logfile_backups, 10)
        self.assertEqual(proc1.exitcodes, [0,2])
        self.assertEqual(proc1.directory, '/tmp')
        self.assertEqual(proc1.umask, 002)
        self.assertEqual(proc1.environment, dict(FAKE_ENV_VAR='/some/path'))

        cat2 = options.process_group_configs[1]
        self.assertEqual(cat2.name, 'cat2')
        self.assertEqual(cat2.priority, 2)
        self.assertEqual(len(cat2.process_configs), 1)

        proc2 = cat2.process_configs[0]
        self.assertEqual(proc2.name, 'cat2')
        self.assertEqual(proc2.command, '/bin/cat')
        self.assertEqual(proc2.priority, 2)
        self.assertEqual(proc2.autostart, True)
        self.assertEqual(proc2.autorestart, False)
        self.assertEqual(proc2.uid, None)
        self.assertEqual(proc2.stdout_logfile, '/tmp/cat2.log')
        self.assertEqual(proc2.stopsignal, signal.SIGTERM)
        self.assertEqual(proc2.stopasgroup, False)
        self.assertEqual(proc2.killasgroup, False)
        self.assertEqual(proc2.stdout_logfile_maxbytes, 1024)
        self.assertEqual(proc2.stdout_logfile_backups, 2)
        self.assertEqual(proc2.exitcodes, [0,2])
        self.assertEqual(proc2.directory, None)

        cat3 = options.process_group_configs[2]
        self.assertEqual(cat3.name, 'cat3')
        self.assertEqual(cat3.priority, 3)
        self.assertEqual(len(cat3.process_configs), 1)

        proc3 = cat3.process_configs[0]
        self.assertEqual(proc3.name, 'replaced')
        self.assertEqual(proc3.command, '/bin/cat')
        self.assertEqual(proc3.priority, 3)
        self.assertEqual(proc3.autostart, True)
        self.assertEqual(proc3.autorestart, datatypes.RestartUnconditionally)
        self.assertEqual(proc3.uid, None)
        self.assertEqual(proc3.stdout_logfile, datatypes.Automatic)
        self.assertEqual(proc3.stdout_logfile_maxbytes,
                         datatypes.byte_size('50MB'))
        self.assertEqual(proc3.stdout_logfile_backups, 10)
        self.assertEqual(proc3.exitcodes, [0,1,127])
        self.assertEqual(proc3.stopsignal, signal.SIGTERM)
        self.assertEqual(proc3.stopasgroup, True)
        self.assertEqual(proc3.killasgroup, True)

        cat4 = options.process_group_configs[3]
        self.assertEqual(cat4.name, 'cat4')
        self.assertEqual(cat4.priority, 4)
        self.assertEqual(len(cat4.process_configs), 2)

        proc4_a = cat4.process_configs[0]
        self.assertEqual(proc4_a.name, 'fleeb_0')
        self.assertEqual(proc4_a.command, '/bin/cat')
        self.assertEqual(proc4_a.priority, 4)
        self.assertEqual(proc4_a.autostart, True)
        self.assertEqual(proc4_a.autorestart,
                         datatypes.RestartWhenExitUnexpected)
        self.assertEqual(proc4_a.uid, None)
        self.assertEqual(proc4_a.stdout_logfile, datatypes.Automatic)
        self.assertEqual(proc4_a.stdout_logfile_maxbytes,
                         datatypes.byte_size('50MB'))
        self.assertEqual(proc4_a.stdout_logfile_backups, 10)
        self.assertEqual(proc4_a.exitcodes, [0,2])
        self.assertEqual(proc4_a.stopsignal, signal.SIGTERM)
        self.assertEqual(proc4_a.stopasgroup, False)
        self.assertEqual(proc4_a.killasgroup, False)

        proc4_b = cat4.process_configs[1]
        self.assertEqual(proc4_b.name, 'fleeb_1')
        self.assertEqual(proc4_b.command, '/bin/cat')
        self.assertEqual(proc4_b.priority, 4)
        self.assertEqual(proc4_b.autostart, True)
        self.assertEqual(proc4_b.autorestart,
                         datatypes.RestartWhenExitUnexpected)
        self.assertEqual(proc4_b.uid, None)
        self.assertEqual(proc4_b.stdout_logfile, datatypes.Automatic)
        self.assertEqual(proc4_b.stdout_logfile_maxbytes,
                         datatypes.byte_size('50MB'))
        self.assertEqual(proc4_b.stdout_logfile_backups, 10)
        self.assertEqual(proc4_b.exitcodes, [0,2])
        self.assertEqual(proc4_b.stopsignal, signal.SIGTERM)
        self.assertEqual(proc4_b.stopasgroup, False)
        self.assertEqual(proc4_b.killasgroup, False)

        here = os.path.abspath(os.getcwd())
        self.assertEqual(instance.uid, 0)
        self.assertEqual(instance.gid, 0)
        self.assertEqual(instance.directory, tempfile.gettempdir())
        self.assertEqual(instance.umask, 022)
        self.assertEqual(instance.logfile, os.path.join(here,'supervisord.log'))
        self.assertEqual(instance.logfile_maxbytes, 1000 * 1024 * 1024)
        self.assertEqual(instance.logfile_backups, 5)
        self.assertEqual(instance.loglevel, 40)
        self.assertEqual(instance.pidfile, os.path.join(here,'supervisord.pid'))
        self.assertEqual(instance.nodaemon, True)
        self.assertEqual(instance.passwdfile, None)
        self.assertEqual(instance.identifier, 'fleeb')
        self.assertEqual(instance.childlogdir, tempfile.gettempdir())

        self.assertEqual(len(instance.server_configs), 1)
        self.assertEqual(instance.server_configs[0]['family'], socket.AF_INET)
        self.assertEqual(instance.server_configs[0]['host'], '127.0.0.1')
        self.assertEqual(instance.server_configs[0]['port'], 8999)
        self.assertEqual(instance.server_configs[0]['username'], 'chrism')
        self.assertEqual(instance.server_configs[0]['password'], 'foo')

        self.assertEqual(instance.nocleanup, True)
        self.assertEqual(instance.minfds, 2048)
        self.assertEqual(instance.minprocs, 300)

    def test_reload(self):
        from cStringIO import StringIO
        text = lstrip("""\
        [supervisord]
        user=root

        [program:one]
        command = /bin/cat

        [program:two]
        command = /bin/dog

        [program:four]
        command = /bin/sheep

        [group:thegroup]
        programs = one,two
        """)

        instance = self._makeOne()
        instance.configfile = StringIO(text)
        instance.realize(args=[])

        section = instance.configroot.supervisord

        self.assertEqual(len(section.process_group_configs), 2)

        cat = section.process_group_configs[0]
        self.assertEqual(len(cat.process_configs), 1)

        cat = section.process_group_configs[1]
        self.assertEqual(len(cat.process_configs), 2)
        self.assertTrue(section.process_group_configs is
                        instance.process_group_configs)

        text = lstrip("""\
        [supervisord]
        user=root

        [program:one]
        command = /bin/cat

        [program:three]
        command = /bin/pig

        [group:thegroup]
        programs = three
        """)
        instance.configfile = StringIO(text)
        instance.process_config_file()

        section = instance.configroot.supervisord

        self.assertEqual(len(section.process_group_configs), 2)

        cat = section.process_group_configs[0]
        self.assertEqual(len(cat.process_configs), 1)
        proc = cat.process_configs[0]
        self.assertEqual(proc.name, 'one')
        self.assertEqual(proc.command, '/bin/cat')
        self.assertTrue(section.process_group_configs is
                        instance.process_group_configs)

        cat = section.process_group_configs[1]
        self.assertEqual(len(cat.process_configs), 1)
        proc = cat.process_configs[0]
        self.assertEqual(proc.name, 'three')
        self.assertEqual(proc.command, '/bin/pig')

    def test_reload_clears_parse_warnings(self):
        instance = self._makeOne()
        old_warning = "Warning from a prior config read"
        instance.parse_warnings = [old_warning]

        from cStringIO import StringIO
        text = lstrip("""\
        [supervisord]
        user=root

        [program:cat]
        command = /bin/cat
        """)
        instance.configfile = StringIO(text)
        instance.realize(args=[])
        self.assertFalse(old_warning in instance.parse_warnings)

    def test_readFile_failed(self):
        from supervisor.options import readFile
        try:
            readFile('/notthere', 0, 10)
        except ValueError, inst:
            self.assertEqual(inst.args[0], 'FAILED')
        else:
            raise AssertionError("Didn't raise")

    def test_get_pid(self):
        instance = self._makeOne()
        self.assertEqual(os.getpid(), instance.get_pid())

    def test_get_signal_delegates_to_signal_receiver(self):
        instance = self._makeOne()
        instance.signal_receiver.receive(signal.SIGTERM, None)
        instance.signal_receiver.receive(signal.SIGCHLD, None)
        self.assertEqual(instance.get_signal(), signal.SIGTERM)
        self.assertEqual(instance.get_signal(), signal.SIGCHLD)
        self.assertEqual(instance.get_signal(), None)

    def test_check_execv_args_cant_find_command(self):
        instance = self._makeOne()
        from supervisor.options import NotFound
        self.assertRaises(NotFound, instance.check_execv_args,
                          '/not/there', None, None)

    def test_check_execv_args_notexecutable(self):
        instance = self._makeOne()
        from supervisor.options import NotExecutable
        self.assertRaises(NotExecutable,
                          instance.check_execv_args, '/etc/passwd',
                          ['etc/passwd'], os.stat('/etc/passwd'))

    def test_check_execv_args_isdir(self):
        instance = self._makeOne()
        from supervisor.options import NotExecutable
        self.assertRaises(NotExecutable,
                          instance.check_execv_args, '/',
                          ['/'], os.stat('/'))

    def test_cleanup_afunix_unlink(self):
        fn = tempfile.mktemp()
        f = open(fn, 'w')
        f.write('foo')
        f.close()
        instance = self._makeOne()
        class Port:
            family = socket.AF_UNIX
            address = fn
        class Server:
            pass
        instance.httpservers = [({'family':socket.AF_UNIX, 'file':fn},
                                 Server())]
        instance.pidfile = ''
        instance.cleanup()
        self.failIf(os.path.exists(fn))

    def test_cleanup_afunix_nounlink(self):
        fn = tempfile.mktemp()
        try:
            f = open(fn, 'w')
            f.write('foo')
            f.close()
            instance = self._makeOne()
            class Port:
                family = socket.AF_UNIX
                address = fn
            class Server:
                pass
            instance.httpservers = [({'family':socket.AF_UNIX, 'file':fn},
                                     Server())]
            instance.pidfile = ''
            instance.unlink_socketfiles = False
            instance.cleanup()
            self.failUnless(os.path.exists(fn))
        finally:
            try:
                os.unlink(fn)
            except OSError:
                pass

    def test_close_httpservers(self):
        instance = self._makeOne()
        class Server:
            closed = False
            def close(self):
                self.closed = True
        server = Server()
        instance.httpservers = [({}, server)]
        instance.close_httpservers()
        self.assertEqual(server.closed, True)

    def test_close_logger(self):
        instance = self._makeOne()
        logger = DummyLogger()
        instance.logger = logger
        instance.close_logger()
        self.assertEqual(logger.closed, True)

    def test_write_pidfile_ok(self):
        fn = tempfile.mktemp()
        try:
            instance = self._makeOne()
            instance.logger = DummyLogger()
            instance.pidfile = fn
            instance.write_pidfile()
            self.failUnless(os.path.exists(fn))
            pid = int(open(fn, 'r').read()[:-1])
            self.assertEqual(pid, os.getpid())
            msg = instance.logger.data[0]
            self.failUnless(msg.startswith('supervisord started with pid'))
        finally:
            try:
                os.unlink(fn)
            except OSError:
                pass

    def test_write_pidfile_fail(self):
        fn = '/cannot/possibly/exist'
        instance = self._makeOne()
        instance.logger = DummyLogger()
        instance.pidfile = fn
        instance.write_pidfile()
        msg = instance.logger.data[0]
        self.failUnless(msg.startswith('could not write pidfile'))

    def test_close_fd(self):
        instance = self._makeOne()
        innie, outie = os.pipe()
        os.read(innie, 0) # we can read it while its open
        os.write(outie, 'foo') # we can write to it while its open
        instance.close_fd(innie)
        self.assertRaises(OSError, os.read, innie, 0)
        instance.close_fd(outie)
        self.assertRaises(OSError, os.write, outie, 'foo')

    def test_processes_from_section(self):
        instance = self._makeOne()
        text = lstrip("""\
        [program:foo]
        command = /bin/cat
        priority = 1
        autostart = false
        autorestart = false
        startsecs = 100
        startretries = 100
        user = root
        stdout_logfile = NONE
        stdout_logfile_backups = 1
        stdout_logfile_maxbytes = 100MB
        stdout_events_enabled = true
        stopsignal = KILL
        stopwaitsecs = 100
        killasgroup = true
        exitcodes = 1,4
        redirect_stderr = false
        environment = KEY1=val1,KEY2=val2,KEY3=%(process_num)s
        numprocs = 2
        process_name = %(group_name)s_%(program_name)s_%(process_num)02d
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        pconfigs = instance.processes_from_section(config, 'program:foo', 'bar')
        self.assertEqual(len(pconfigs), 2)
        pconfig = pconfigs[0]
        self.assertEqual(pconfig.name, 'bar_foo_00')
        self.assertEqual(pconfig.command, '/bin/cat')
        self.assertEqual(pconfig.autostart, False)
        self.assertEqual(pconfig.autorestart, False)
        self.assertEqual(pconfig.startsecs, 100)
        self.assertEqual(pconfig.startretries, 100)
        self.assertEqual(pconfig.uid, 0)
        self.assertEqual(pconfig.stdout_logfile, None)
        self.assertEqual(pconfig.stdout_capture_maxbytes, 0)
        self.assertEqual(pconfig.stdout_logfile_maxbytes, 104857600)
        self.assertEqual(pconfig.stdout_events_enabled, True)
        self.assertEqual(pconfig.stopsignal, signal.SIGKILL)
        self.assertEqual(pconfig.stopasgroup, False)
        self.assertEqual(pconfig.killasgroup, True)
        self.assertEqual(pconfig.stopwaitsecs, 100)
        self.assertEqual(pconfig.exitcodes, [1,4])
        self.assertEqual(pconfig.redirect_stderr, False)
        self.assertEqual(pconfig.environment,
                         {'KEY1':'val1', 'KEY2':'val2', 'KEY3':'0'})

    def test_processes_from_section_host_node_name_expansion(self):
        instance = self._makeOne()
        text = lstrip("""\
        [program:foo]
        command = /bin/foo --host=%(host_node_name)s
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        pconfigs = instance.processes_from_section(config, 'program:foo', 'bar')
        import platform
        expected = "/bin/foo --host=" + platform.node()
        self.assertEqual(pconfigs[0].command, expected)

    def test_processes_from_section_environment_variables_expansion(self):
        instance = self._makeOne()
        text = lstrip("""\
        [program:foo]
        command = /bin/foo --path='%(ENV_PATH)s'
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        pconfigs = instance.processes_from_section(config, 'program:foo', 'bar')
        expected = "/bin/foo --path='%s'" % os.environ['PATH']
        self.assertEqual(pconfigs[0].command, expected)

    def test_processes_from_section_no_procnum_in_processname(self):
        instance = self._makeOne()
        text = lstrip("""\
        [program:foo]
        command = /bin/cat
        numprocs = 2
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        self.assertRaises(ValueError, instance.processes_from_section,
                          config, 'program:foo', None)

    def test_processes_from_section_no_command(self):
        instance = self._makeOne()
        text = lstrip("""\
        [program:foo]
        numprocs = 2
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        self.assertRaises(ValueError, instance.processes_from_section,
                          config, 'program:foo', None)

    def test_processes_from_section_missing_replacement_in_process_name(self):
        instance = self._makeOne()
        text = lstrip("""\
        [program:foo]
        command = /bin/cat
        process_name = %(not_there)s
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        self.assertRaises(ValueError, instance.processes_from_section,
                          config, 'program:foo', None)

    def test_processes_from_section_bad_expression_in_process_name(self):
        instance = self._makeOne()
        text = lstrip("""\
        [program:foo]
        command = /bin/cat
        process_name = %(program_name)
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        self.assertRaises(ValueError, instance.processes_from_section,
                          config, 'program:foo', None)

    def test_processes_from_section_stopasgroup_implies_killasgroup(self):
        instance = self._makeOne()
        text = lstrip("""\
        [program:foo]
        command = /bin/cat
        process_name = %(program_name)s
        stopasgroup = true
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        pconfigs = instance.processes_from_section(config, 'program:foo', 'bar')
        self.assertEqual(len(pconfigs), 1)
        pconfig = pconfigs[0]
        self.assertEqual(pconfig.stopasgroup, True)
        self.assertEqual(pconfig.killasgroup, True)
    
    def test_processes_from_section_killasgroup_mismatch_w_stopasgroup(self):
        instance = self._makeOne()
        text = lstrip("""\
        [program:foo]
        command = /bin/cat
        process_name = %(program_name)s
        stopasgroup = true
        killasgroup = false
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        self.assertRaises(ValueError, instance.processes_from_section,
                          config, 'program:foo', None)

    def test_processes_from_autolog_without_rollover(self):
        instance = self._makeOne()
        text = lstrip("""\
        [program:foo]
        command = /bin/foo
        stdout_logfile = AUTO
        stdout_logfile_maxbytes = 0
        stderr_logfile = AUTO
        stderr_logfile_maxbytes = 0
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        instance.logger = DummyLogger()
        config.read_string(text)
        instance.processes_from_section(config, 'program:foo', None)
        self.assertEqual(instance.parse_warnings[0],
             'For [program:foo], AUTO logging used for stdout_logfile '
             'without rollover, set maxbytes > 0 to avoid filling up '
              'filesystem unintentionally')
        self.assertEqual(instance.parse_warnings[1],
             'For [program:foo], AUTO logging used for stderr_logfile '
             'without rollover, set maxbytes > 0 to avoid filling up '
              'filesystem unintentionally')

    def test_homogeneous_process_groups_from_parser(self):
        text = lstrip("""\
        [program:many]
        process_name = %(program_name)s_%(process_num)s
        command = /bin/cat
        numprocs = 2
        priority = 1
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        gconfigs = instance.process_groups_from_parser(config)
        self.assertEqual(len(gconfigs), 1)
        gconfig = gconfigs[0]
        self.assertEqual(gconfig.name, 'many')
        self.assertEqual(gconfig.priority, 1)
        self.assertEqual(len(gconfig.process_configs), 2)

    def test_event_listener_pools_from_parser(self):
        text = lstrip("""\
        [eventlistener:dog]
        events=PROCESS_COMMUNICATION
        process_name = %(program_name)s_%(process_num)s
        command = /bin/dog
        numprocs = 2
        priority = 1

        [eventlistener:cat]
        events=PROCESS_COMMUNICATION
        process_name = %(program_name)s_%(process_num)s
        command = /bin/cat
        numprocs = 3

        [eventlistener:biz]
        events=PROCESS_COMMUNICATION
        process_name = %(program_name)s_%(process_num)s
        command = /bin/biz
        numprocs = 2
        """)
        from supervisor.options import UnhosedConfigParser
        from supervisor.dispatchers import default_handler
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        gconfigs = instance.process_groups_from_parser(config)
        self.assertEqual(len(gconfigs), 3)

        gconfig1 = gconfigs[0]
        self.assertEqual(gconfig1.name, 'biz')
        self.assertEqual(gconfig1.result_handler, default_handler)
        self.assertEqual(len(gconfig1.process_configs), 2)

        gconfig1 = gconfigs[1]
        self.assertEqual(gconfig1.name, 'cat')
        self.assertEqual(gconfig1.priority, -1)
        self.assertEqual(gconfig1.result_handler, default_handler)
        self.assertEqual(len(gconfig1.process_configs), 3)

        gconfig1 = gconfigs[2]
        self.assertEqual(gconfig1.name, 'dog')
        self.assertEqual(gconfig1.priority, 1)
        self.assertEqual(gconfig1.result_handler, default_handler)
        self.assertEqual(len(gconfig1.process_configs), 2)

    def test_event_listener_pool_with_event_results_handler(self):
        text = lstrip("""\
        [eventlistener:dog]
        events=PROCESS_COMMUNICATION
        command = /bin/dog
        result_handler = supervisor.tests.base:dummy_handler
        """)
        from supervisor.options import UnhosedConfigParser
        from supervisor.tests.base import dummy_handler
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        gconfigs = instance.process_groups_from_parser(config)
        self.assertEqual(len(gconfigs), 1)

        gconfig1 = gconfigs[0]
        self.assertEqual(gconfig1.result_handler, dummy_handler)

    def test_event_listener_pool_noeventsline(self):
        text = lstrip("""\
        [eventlistener:dog]
        process_name = %(program_name)s_%(process_num)s
        command = /bin/dog
        numprocs = 2
        priority = 1
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_event_listener_pool_unknown_eventtype(self):
        text = lstrip("""\
        [eventlistener:dog]
        events=PROCESS_COMMUNICATION,THIS_EVENT_TYPE_DOESNT_EXIST
        process_name = %(program_name)s_%(process_num)s
        command = /bin/dog
        numprocs = 2
        priority = 1
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_fcgi_programs_from_parser(self):
        from supervisor.options import FastCGIGroupConfig
        from supervisor.options import FastCGIProcessConfig
        text = lstrip("""\
        [fcgi-program:foo]
        socket = unix:///tmp/%(program_name)s.sock
        socket_owner = testuser:testgroup
        socket_mode = 0666
        process_name = %(program_name)s_%(process_num)s
        command = /bin/foo
        numprocs = 2
        priority = 1

        [fcgi-program:bar]
        socket = unix:///tmp/%(program_name)s.sock
        process_name = %(program_name)s_%(process_num)s
        command = /bin/bar
        user = testuser
        numprocs = 3

        [fcgi-program:flub]
        socket = unix:///tmp/%(program_name)s.sock
        command = /bin/flub

        [fcgi-program:cub]
        socket = tcp://localhost:6000
        command = /bin/cub
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()

        #Patch pwd and grp module functions to give us sentinel
        #uid/gid values so that the test does not depend on
        #any specific system users
        pwd_mock = Mock()
        pwd_mock.return_value = (None, None, sentinel.uid, sentinel.gid)
        grp_mock = Mock()
        grp_mock.return_value = (None, None, sentinel.gid)
        @patch('pwd.getpwuid', pwd_mock)
        @patch('pwd.getpwnam', pwd_mock)
        @patch('grp.getgrnam', grp_mock)
        def get_process_groups(instance, config):
            return instance.process_groups_from_parser(config)

        gconfigs = get_process_groups(instance, config)

        exp_owner = (sentinel.uid, sentinel.gid)

        self.assertEqual(len(gconfigs), 4)

        gconf_foo = gconfigs[0]
        self.assertEqual(gconf_foo.__class__, FastCGIGroupConfig)
        self.assertEqual(gconf_foo.name, 'foo')
        self.assertEqual(gconf_foo.priority, 1)
        self.assertEqual(gconf_foo.socket_config.url,
                                'unix:///tmp/foo.sock')
        self.assertEqual(exp_owner, gconf_foo.socket_config.get_owner())
        self.assertEqual(0666, gconf_foo.socket_config.get_mode())
        self.assertEqual(len(gconf_foo.process_configs), 2)
        pconfig_foo = gconf_foo.process_configs[0]
        self.assertEqual(pconfig_foo.__class__, FastCGIProcessConfig)

        gconf_bar = gconfigs[1]
        self.assertEqual(gconf_bar.name, 'bar')
        self.assertEqual(gconf_bar.priority, 999)
        self.assertEqual(gconf_bar.socket_config.url,
                         'unix:///tmp/bar.sock')
        self.assertEqual(exp_owner, gconf_bar.socket_config.get_owner())
        self.assertEqual(0700, gconf_bar.socket_config.get_mode())
        self.assertEqual(len(gconf_bar.process_configs), 3)

        gconf_cub = gconfigs[2]
        self.assertEqual(gconf_cub.name, 'cub')
        self.assertEqual(gconf_cub.socket_config.url,
                         'tcp://localhost:6000')
        self.assertEqual(len(gconf_cub.process_configs), 1)

        gconf_flub = gconfigs[3]
        self.assertEqual(gconf_flub.name, 'flub')
        self.assertEqual(gconf_flub.socket_config.url,
                         'unix:///tmp/flub.sock')
        self.assertEqual(None, gconf_flub.socket_config.get_owner())
        self.assertEqual(0700, gconf_flub.socket_config.get_mode())
        self.assertEqual(len(gconf_flub.process_configs), 1)



    def test_fcgi_program_no_socket(self):
        text = lstrip("""\
        [fcgi-program:foo]
        process_name = %(program_name)s_%(process_num)s
        command = /bin/foo
        numprocs = 2
        priority = 1
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_fcgi_program_unknown_socket_protocol(self):
        text = lstrip("""\
        [fcgi-program:foo]
        socket=junk://blah
        process_name = %(program_name)s_%(process_num)s
        command = /bin/foo
        numprocs = 2
        priority = 1
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_fcgi_program_rel_unix_sock_path(self):
        text = lstrip("""\
        [fcgi-program:foo]
        socket=unix://relative/path
        process_name = %(program_name)s_%(process_num)s
        command = /bin/foo
        numprocs = 2
        priority = 1
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_fcgi_program_bad_tcp_sock_format(self):
        text = lstrip("""\
        [fcgi-program:foo]
        socket=tcp://missingport
        process_name = %(program_name)s_%(process_num)s
        command = /bin/foo
        numprocs = 2
        priority = 1
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_fcgi_program_bad_expansion_proc_num(self):
        text = lstrip("""\
        [fcgi-program:foo]
        socket=unix:///tmp/%(process_num)s.sock
        process_name = %(program_name)s_%(process_num)s
        command = /bin/foo
        numprocs = 2
        priority = 1
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_fcgi_program_socket_owner_set_for_tcp(self):
        text = lstrip("""\
        [fcgi-program:foo]
        socket=tcp://localhost:8000
        socket_owner=nobody:nobody
        command = /bin/foo
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_fcgi_program_socket_mode_set_for_tcp(self):
        text = lstrip("""\
        [fcgi-program:foo]
        socket = tcp://localhost:8000
        socket_mode = 0777
        command = /bin/foo
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_fcgi_program_bad_socket_owner(self):
        text = lstrip("""\
        [fcgi-program:foo]
        socket = unix:///tmp/foo.sock
        socket_owner = sometotaljunkuserthatshouldnobethere
        command = /bin/foo
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_fcgi_program_bad_socket_mode(self):
        text = lstrip("""\
        [fcgi-program:foo]
        socket = unix:///tmp/foo.sock
        socket_mode = junk
        command = /bin/foo
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError,instance.process_groups_from_parser,config)

    def test_heterogeneous_process_groups_from_parser(self):
        text = lstrip("""\
        [program:one]
        command = /bin/cat

        [program:two]
        command = /bin/cat

        [group:thegroup]
        programs = one,two
        priority = 5
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        gconfigs = instance.process_groups_from_parser(config)
        self.assertEqual(len(gconfigs), 1)
        gconfig = gconfigs[0]
        self.assertEqual(gconfig.name, 'thegroup')
        self.assertEqual(gconfig.priority, 5)
        self.assertEqual(len(gconfig.process_configs), 2)

    def test_mixed_process_groups_from_parser1(self):
        text = lstrip("""\
        [program:one]
        command = /bin/cat

        [program:two]
        command = /bin/cat

        [program:many]
        process_name = %(program_name)s_%(process_num)s
        command = /bin/cat
        numprocs = 2
        priority = 1

        [group:thegroup]
        programs = one,two
        priority = 5
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        gconfigs = instance.process_groups_from_parser(config)
        self.assertEqual(len(gconfigs), 2)

        manyconfig = gconfigs[0]
        self.assertEqual(manyconfig.name, 'many')
        self.assertEqual(manyconfig.priority, 1)
        self.assertEqual(len(manyconfig.process_configs), 2)

        gconfig = gconfigs[1]
        self.assertEqual(gconfig.name, 'thegroup')
        self.assertEqual(gconfig.priority, 5)
        self.assertEqual(len(gconfig.process_configs), 2)

    def test_mixed_process_groups_from_parser2(self):
        text = lstrip("""\
        [program:one]
        command = /bin/cat

        [program:two]
        command = /bin/cat

        [program:many]
        process_name = %(program_name)s_%(process_num)s
        command = /bin/cat
        numprocs = 2
        priority = 1

        [group:thegroup]
        programs = one,two, many
        priority = 5
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        gconfigs = instance.process_groups_from_parser(config)
        self.assertEqual(len(gconfigs), 1)

        gconfig = gconfigs[0]
        self.assertEqual(gconfig.name, 'thegroup')
        self.assertEqual(gconfig.priority, 5)
        self.assertEqual(len(gconfig.process_configs), 4)

    def test_unknown_program_in_heterogeneous_group(self):
        text = lstrip("""\
        [program:one]
        command = /bin/cat

        [group:foo]
        programs = notthere
        """)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        self.assertRaises(ValueError, instance.process_groups_from_parser,
                          config)

    def test_rpcinterfaces_from_parser(self):
        text = lstrip("""\
        [rpcinterface:dummy]
        supervisor.rpcinterface_factory = %s
        foo = bar
        """ % __name__)
        from supervisor.options import UnhosedConfigParser
        config = UnhosedConfigParser()
        config.read_string(text)
        instance = self._makeOne()
        factories = instance.get_plugins(config,
                                         'supervisor.rpcinterface_factory',
                                         'rpcinterface:')
        self.assertEqual(len(factories), 1)
        factory = factories[0]
        self.assertEqual(factory[0], 'dummy')
        self.assertEqual(factory[1], sys.modules[__name__])
        self.assertEqual(factory[2], {'foo':'bar'})

    def test_clear_autochildlogdir(self):
        dn = tempfile.mkdtemp()
        try:
            instance = self._makeOne()
            instance.childlogdir = dn
            sid = 'supervisor'
            instance.identifier = sid
            logfn = instance.get_autochildlog_name('foo', sid,'stdout')
            first = logfn + '.1'
            second = logfn + '.2'
            open(first, 'w')
            open(second, 'w')
            instance.clear_autochildlogdir()
            self.failIf(os.path.exists(logfn))
            self.failIf(os.path.exists(first))
            self.failIf(os.path.exists(second))
        finally:
            shutil.rmtree(dn)

    def test_clear_autochildlog_oserror(self):
        instance = self._makeOne()
        instance.childlogdir = '/tmp/this/cant/possibly/existjjjj'
        instance.logger = DummyLogger()
        instance.clear_autochildlogdir()
        self.assertEqual(instance.logger.data, ['Could not clear childlog dir'])

    def test_openhttpservers_reports_friendly_usage_when_eaddrinuse(self):
        supervisord = DummySupervisor()
        instance = self._makeOne()

        def raise_eaddrinuse(supervisord):
            raise socket.error(errno.EADDRINUSE)
        instance.make_http_servers = raise_eaddrinuse

        recorder = []
        def record_usage(message):
            recorder.append(message)
        instance.usage = record_usage

        instance.openhttpservers(supervisord)
        self.assertEqual(len(recorder), 1)
        expected = 'Another program is already listening'
        self.assertTrue(recorder[0].startswith(expected))

    def test_openhttpservers_reports_socket_error_with_errno(self):
        supervisord = DummySupervisor()
        instance = self._makeOne()

        def make_http_servers(supervisord):
            raise socket.error(errno.EPERM)
        instance.make_http_servers = make_http_servers

        recorder = []
        def record_usage(message):
            recorder.append(message)
        instance.usage = record_usage

        instance.openhttpservers(supervisord)
        self.assertEqual(len(recorder), 1)
        expected = ('Cannot open an HTTP server: socket.error '
                    'reported errno.EPERM (%d)' % errno.EPERM)
        self.assertEqual(recorder[0], expected)

    def test_openhttpservers_reports_other_socket_errors(self):
        supervisord = DummySupervisor()
        instance = self._makeOne()

        def make_http_servers(supervisord):
            raise socket.error('uh oh')
        instance.make_http_servers = make_http_servers

        recorder = []
        def record_usage(message):
            recorder.append(message)
        instance.usage = record_usage

        instance.openhttpservers(supervisord)
        self.assertEqual(len(recorder), 1)
        expected = ('Cannot open an HTTP server: socket.error '
                    'reported uh oh')
        self.assertEqual(recorder[0], expected)

    def test_openhttpservers_reports_value_errors(self):
        supervisord = DummySupervisor()
        instance = self._makeOne()

        def make_http_servers(supervisord):
            raise ValueError('not prefixed with help')
        instance.make_http_servers = make_http_servers

        recorder = []
        def record_usage(message):
            recorder.append(message)
        instance.usage = record_usage

        instance.openhttpservers(supervisord)
        self.assertEqual(len(recorder), 1)
        expected = 'not prefixed with help'
        self.assertEqual(recorder[0], expected)

    def test_openhttpservers_does_not_catch_other_exception_types(self):
        supervisord = DummySupervisor()
        instance = self._makeOne()

        def make_http_servers(supervisord):
            raise OverflowError
        instance.make_http_servers = make_http_servers

        # this scenario probably means a bug in supervisor.  we dump
        # all the gory details on the poor user for troubleshooting
        self.assertRaises(OverflowError,
                          instance.openhttpservers, supervisord)


class TestProcessConfig(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.options import ProcessConfig
        return ProcessConfig

    def _makeOne(self, *arg, **kw):
        defaults = {}
        for name in ('name', 'command', 'directory', 'umask',
                     'priority', 'autostart', 'autorestart',
                     'startsecs', 'startretries', 'uid',
                     'stdout_logfile', 'stdout_capture_maxbytes',
                     'stdout_events_enabled',
                     'stdout_logfile_backups', 'stdout_logfile_maxbytes',
                     'stderr_logfile', 'stderr_capture_maxbytes',
                     'stderr_events_enabled',
                     'stderr_logfile_backups', 'stderr_logfile_maxbytes',
                     'stopsignal', 'stopwaitsecs', 'stopasgroup', 'killasgroup', 'exitcodes',
                     'redirect_stderr', 'environment'):
            defaults[name] = name
        defaults.update(kw)
        return self._getTargetClass()(*arg, **defaults)

    def test_create_autochildlogs(self):
        options = DummyOptions()
        instance = self._makeOne(options)
        from supervisor.datatypes import Automatic
        instance.stdout_logfile = Automatic
        instance.stderr_logfile = Automatic
        instance.create_autochildlogs()
        self.assertEqual(instance.stdout_logfile, options.tempfile_name)
        self.assertEqual(instance.stderr_logfile, options.tempfile_name)

    def test_make_process(self):
        options = DummyOptions()
        instance = self._makeOne(options)
        process = instance.make_process()
        from supervisor.process import Subprocess
        self.assertEqual(process.__class__, Subprocess)
        self.assertEqual(process.group, None)

    def test_make_process_with_group(self):
        options = DummyOptions()
        instance = self._makeOne(options)
        process = instance.make_process('abc')
        from supervisor.process import Subprocess
        self.assertEqual(process.__class__, Subprocess)
        self.assertEqual(process.group, 'abc')

    def test_make_dispatchers_stderr_not_redirected(self):
        options = DummyOptions()
        instance = self._makeOne(options)
        instance.redirect_stderr = False
        process1 = DummyProcess(instance)
        dispatchers, pipes = instance.make_dispatchers(process1)
        self.assertEqual(dispatchers[5].channel, 'stdout')
        from supervisor.events import ProcessCommunicationStdoutEvent
        self.assertEqual(dispatchers[5].event_type,
                         ProcessCommunicationStdoutEvent)
        self.assertEqual(pipes['stdout'], 5)
        self.assertEqual(dispatchers[7].channel, 'stderr')
        from supervisor.events import ProcessCommunicationStderrEvent
        self.assertEqual(dispatchers[7].event_type,
                         ProcessCommunicationStderrEvent)
        self.assertEqual(pipes['stderr'], 7)

    def test_make_dispatchers_stderr_redirected(self):
        options = DummyOptions()
        instance = self._makeOne(options)
        process1 = DummyProcess(instance)
        dispatchers, pipes = instance.make_dispatchers(process1)
        self.assertEqual(dispatchers[5].channel, 'stdout')
        self.assertEqual(pipes['stdout'], 5)
        self.assertEqual(pipes['stderr'], None)

class FastCGIProcessConfigTest(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.options import FastCGIProcessConfig
        return FastCGIProcessConfig

    def _makeOne(self, *arg, **kw):
        defaults = {}
        for name in ('name', 'command', 'directory', 'umask',
                     'priority', 'autostart', 'autorestart',
                     'startsecs', 'startretries', 'uid',
                     'stdout_logfile', 'stdout_capture_maxbytes',
                     'stdout_events_enabled',
                     'stdout_logfile_backups', 'stdout_logfile_maxbytes',
                     'stderr_logfile', 'stderr_capture_maxbytes',
                     'stderr_events_enabled',
                     'stderr_logfile_backups', 'stderr_logfile_maxbytes',
                     'stopsignal', 'stopwaitsecs', 'stopasgroup', 'killasgroup', 'exitcodes',
                     'redirect_stderr', 'environment'):
            defaults[name] = name
        defaults.update(kw)
        return self._getTargetClass()(*arg, **defaults)

    def test_make_process(self):
        options = DummyOptions()
        instance = self._makeOne(options)
        self.assertRaises(NotImplementedError, instance.make_process)

    def test_make_process_with_group(self):
        options = DummyOptions()
        instance = self._makeOne(options)
        process = instance.make_process('abc')
        from supervisor.process import FastCGISubprocess
        self.assertEqual(process.__class__, FastCGISubprocess)
        self.assertEqual(process.group, 'abc')

    def test_make_dispatchers(self):
        options = DummyOptions()
        instance = self._makeOne(options)
        instance.redirect_stderr = False
        process1 = DummyProcess(instance)
        dispatchers, pipes = instance.make_dispatchers(process1)
        self.assertEqual(dispatchers[4].channel, 'stdin')
        self.assertEqual(dispatchers[4].closed, True)
        self.assertEqual(dispatchers[5].channel, 'stdout')
        from supervisor.events import ProcessCommunicationStdoutEvent
        self.assertEqual(dispatchers[5].event_type,
                         ProcessCommunicationStdoutEvent)
        self.assertEqual(pipes['stdout'], 5)
        self.assertEqual(dispatchers[7].channel, 'stderr')
        from supervisor.events import ProcessCommunicationStderrEvent
        self.assertEqual(dispatchers[7].event_type,
                         ProcessCommunicationStderrEvent)
        self.assertEqual(pipes['stderr'], 7)

class ProcessGroupConfigTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.options import ProcessGroupConfig
        return ProcessGroupConfig

    def _makeOne(self, options, name, priority, pconfigs):
        return self._getTargetClass()(options, name, priority, pconfigs)

    def test_ctor(self):
        options = DummyOptions()
        instance = self._makeOne(options, 'whatever', 999, [])
        self.assertEqual(instance.options, options)
        self.assertEqual(instance.name, 'whatever')
        self.assertEqual(instance.priority, 999)
        self.assertEqual(instance.process_configs, [])

    def test_after_setuid(self):
        options = DummyOptions()
        pconfigs = [DummyPConfig(options, 'process1', '/bin/process1')]
        instance = self._makeOne(options, 'whatever', 999, pconfigs)
        instance.after_setuid()
        self.assertEqual(pconfigs[0].autochildlogs_created, True)

    def test_make_group(self):
        options = DummyOptions()
        pconfigs = [DummyPConfig(options, 'process1', '/bin/process1')]
        instance = self._makeOne(options, 'whatever', 999, pconfigs)
        group = instance.make_group()
        from supervisor.process import ProcessGroup
        self.assertEqual(group.__class__, ProcessGroup)

class FastCGIGroupConfigTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.options import FastCGIGroupConfig
        return FastCGIGroupConfig

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_ctor(self):
        options = DummyOptions()
        sock_config = DummySocketConfig(6)
        instance = self._makeOne(options, 'whatever', 999, [], sock_config)
        self.assertEqual(instance.options, options)
        self.assertEqual(instance.name, 'whatever')
        self.assertEqual(instance.priority, 999)
        self.assertEqual(instance.process_configs, [])
        self.assertEqual(instance.socket_config, sock_config)

    def test_same_sockets_are_equal(self):
        options = DummyOptions()
        sock_config1 = DummySocketConfig(6)
        instance1 = self._makeOne(options, 'whatever', 999, [], sock_config1)

        sock_config2 = DummySocketConfig(6)
        instance2 = self._makeOne(options, 'whatever', 999, [], sock_config2)

        self.assertTrue(instance1 == instance2)
        self.assertFalse(instance1 != instance2)

    def test_diff_sockets_are_not_equal(self):
        options = DummyOptions()
        sock_config1 = DummySocketConfig(6)
        instance1 = self._makeOne(options, 'whatever', 999, [], sock_config1)

        sock_config2 = DummySocketConfig(7)
        instance2 = self._makeOne(options, 'whatever', 999, [], sock_config2)

        self.assertTrue(instance1 != instance2)
        self.assertFalse(instance1 == instance2)

class SignalReceiverTests(unittest.TestCase):
    def test_returns_None_initially(self):
        from supervisor.options import SignalReceiver
        sr = SignalReceiver()
        self.assertEquals(sr.get_signal(), None)

    def test_returns_signals_in_order_received(self):
        from supervisor.options import SignalReceiver
        sr = SignalReceiver()
        sr.receive(signal.SIGTERM, 'frame')
        sr.receive(signal.SIGCHLD, 'frame')
        self.assertEquals(sr.get_signal(), signal.SIGTERM)
        self.assertEquals(sr.get_signal(), signal.SIGCHLD)
        self.assertEquals(sr.get_signal(), None)

    def test_does_not_queue_duplicate_signals(self):
        from supervisor.options import SignalReceiver
        sr = SignalReceiver()
        sr.receive(signal.SIGTERM, 'frame')
        sr.receive(signal.SIGTERM, 'frame')
        self.assertEquals(sr.get_signal(), signal.SIGTERM)
        self.assertEquals(sr.get_signal(), None)

    def test_queues_again_after_being_emptied(self):
        from supervisor.options import SignalReceiver
        sr = SignalReceiver()
        sr.receive(signal.SIGTERM, 'frame')
        self.assertEquals(sr.get_signal(), signal.SIGTERM)
        self.assertEquals(sr.get_signal(), None)
        sr.receive(signal.SIGCHLD, 'frame')
        self.assertEquals(sr.get_signal(), signal.SIGCHLD)
        self.assertEquals(sr.get_signal(), None)

class UtilFunctionsTests(unittest.TestCase):
    def test_make_namespec(self):
        from supervisor.options import make_namespec
        self.assertEquals(make_namespec('group', 'process'), 'group:process')
        self.assertEquals(make_namespec('process', 'process'), 'process')

    def test_split_namespec(self):
        from supervisor.options import split_namespec
        s = split_namespec
        self.assertEquals(s('process:group'), ('process', 'group'))
        self.assertEquals(s('process'), ('process', 'process'))
        self.assertEquals(s('group:'), ('group', None))
        self.assertEquals(s('group:*'), ('group', None))

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')


########NEW FILE########
__FILENAME__ = test_process
import os
import signal
import time
import unittest
import sys
import errno
from mock import Mock, patch, sentinel

from supervisor.tests.base import DummyOptions
from supervisor.tests.base import DummyPConfig
from supervisor.tests.base import DummyProcess
from supervisor.tests.base import DummyPGroupConfig
from supervisor.tests.base import DummyDispatcher
from supervisor.tests.base import DummyEvent
from supervisor.tests.base import DummyFCGIGroupConfig
from supervisor.tests.base import DummySocketConfig
from supervisor.tests.base import DummyProcessGroup
from supervisor.tests.base import DummyFCGIProcessGroup
from supervisor.tests.base import DummySocketManager

from supervisor.process import Subprocess

class SubprocessTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.process import Subprocess
        return Subprocess

    def _makeOne(self, *arg, **kw):
        return self._getTargetClass()(*arg, **kw)

    def tearDown(self):
        from supervisor.events import clear
        clear()

    def test_getProcessStateDescription(self):
        from supervisor.states import ProcessStates
        from supervisor.process import getProcessStateDescription
        for statename, code in ProcessStates.__dict__.items():
            self.assertEqual(getProcessStateDescription(code), statename)

    def test_ctor(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'cat', 'bin/cat',
                              stdout_logfile='/tmp/temp123.log',
                              stderr_logfile='/tmp/temp456.log')
        instance = self._makeOne(config)
        self.assertEqual(instance.config, config)
        self.assertEqual(instance.config.options, options)
        self.assertEqual(instance.laststart, 0)
        self.assertEqual(instance.pid, 0)
        self.assertEqual(instance.laststart, 0)
        self.assertEqual(instance.laststop, 0)
        self.assertEqual(instance.delay, 0)
        self.assertEqual(instance.administrative_stop, 0)
        self.assertEqual(instance.killing, 0)
        self.assertEqual(instance.backoff, 0)
        self.assertEqual(instance.pipes, {})
        self.assertEqual(instance.dispatchers, {})
        self.assertEqual(instance.spawnerr, None)

    def test_repr(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'cat', 'bin/cat')
        instance = self._makeOne(config)
        s = repr(instance)
        self.assertTrue(s.startswith('<Subprocess at'))
        self.assertTrue(s.endswith('with name cat in state STOPPED>'))

    def test_reopenlogs(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.dispatchers = {0:DummyDispatcher(readable=True),
                                1:DummyDispatcher(writable=True)}
        instance.reopenlogs()
        self.assertEqual(instance.dispatchers[0].logs_reopened, True)
        self.assertEqual(instance.dispatchers[1].logs_reopened, False)
        
    def test_removelogs(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.dispatchers = {0:DummyDispatcher(readable=True),
                                1:DummyDispatcher(writable=True)}
        instance.removelogs()
        self.assertEqual(instance.dispatchers[0].logs_removed, True)
        self.assertEqual(instance.dispatchers[1].logs_removed, False)

    def test_drain(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test',
                              stdout_logfile='/tmp/foo',
                              stderr_logfile='/tmp/bar')
        instance = self._makeOne(config)
        instance.dispatchers = {0:DummyDispatcher(readable=True),
                                1:DummyDispatcher(writable=True)}
        instance.drain()
        self.assertTrue(instance.dispatchers[0].read_event_handled)
        self.assertTrue(instance.dispatchers[1].write_event_handled)
        
    def test_get_execv_args_abs_missing(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'notthere', '/notthere')
        instance = self._makeOne(config)
        args = instance.get_execv_args()
        self.assertEqual(args, ('/notthere', ['/notthere']))

    def test_get_execv_args_abs_withquotes_missing(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'notthere', '/notthere "an argument"')
        instance = self._makeOne(config)
        args = instance.get_execv_args()
        self.assertEqual(args, ('/notthere', ['/notthere', 'an argument']))

    def test_get_execv_args_rel_missing(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'notthere', 'notthere')
        instance = self._makeOne(config)
        args = instance.get_execv_args()
        self.assertEqual(args, ('notthere', ['notthere']))

    def test_get_execv_args_rel_withquotes_missing(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'notthere', 'notthere "an argument"')
        instance = self._makeOne(config)
        args = instance.get_execv_args()
        self.assertEqual(args, ('notthere', ['notthere', 'an argument']))

    def test_get_execv_args_abs(self):
        executable = '/bin/sh foo'
        options = DummyOptions()
        config = DummyPConfig(options, 'sh', executable)
        instance = self._makeOne(config)
        args = instance.get_execv_args()
        self.assertEqual(len(args), 2)
        self.assertEqual(args[0], '/bin/sh')
        self.assertEqual(args[1], ['/bin/sh', 'foo'])

    def test_get_execv_args_rel(self):
        executable = 'sh foo'
        options = DummyOptions()
        config = DummyPConfig(options, 'sh', executable)
        instance = self._makeOne(config)
        args = instance.get_execv_args()
        self.assertEqual(len(args), 2)
        self.assertEqual(args[0], '/bin/sh')
        self.assertEqual(args[1], ['sh', 'foo'])

    def test_record_spawnerr(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.record_spawnerr('foo')
        self.assertEqual(instance.spawnerr, 'foo')
        self.assertEqual(options.logger.data[0], 'spawnerr: foo')

    def test_spawn_already_running(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'sh', '/bin/sh')
        instance = self._makeOne(config)
        instance.pid = True
        from supervisor.states import ProcessStates
        instance.state = ProcessStates.RUNNING
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.logger.data[0], "process 'sh' already running")
        self.assertEqual(instance.state, ProcessStates.RUNNING)

    def test_spawn_fail_check_execv_args(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'bad', '/bad/filename')
        instance = self._makeOne(config)
        from supervisor.states import ProcessStates
        instance.state = ProcessStates.BACKOFF
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(instance.spawnerr, 'bad filename')
        self.assertEqual(options.logger.data[0], "spawnerr: bad filename")
        self.failUnless(instance.delay)
        self.failUnless(instance.backoff)
        from supervisor.states import ProcessStates
        self.assertEqual(instance.state, ProcessStates.BACKOFF)
        self.assertEqual(len(L), 2)
        event1 = L[0]
        event2 = L[1]
        self.assertEqual(event1.__class__, events.ProcessStateStartingEvent)
        self.assertEqual(event2.__class__, events.ProcessStateBackoffEvent)

    def test_spawn_fail_make_pipes_emfile(self):
        options = DummyOptions()
        import errno
        options.make_pipes_error = errno.EMFILE
        config = DummyPConfig(options, 'good', '/good/filename')
        instance = self._makeOne(config)
        from supervisor.states import ProcessStates
        instance.state = ProcessStates.BACKOFF
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(instance.spawnerr,
                         "too many open files to spawn 'good'")
        self.assertEqual(options.logger.data[0],
                         "spawnerr: too many open files to spawn 'good'")
        self.failUnless(instance.delay)
        self.failUnless(instance.backoff)
        from supervisor.states import ProcessStates
        self.assertEqual(instance.state, ProcessStates.BACKOFF)
        self.assertEqual(len(L), 2)
        event1, event2 = L
        self.assertEqual(event1.__class__, events.ProcessStateStartingEvent)
        self.assertEqual(event2.__class__, events.ProcessStateBackoffEvent)

    def test_spawn_fail_make_pipes_other(self):
        options = DummyOptions()
        options.make_pipes_error = 1
        config = DummyPConfig(options, 'good', '/good/filename')
        instance = self._makeOne(config)
        from supervisor.states import ProcessStates
        instance.state = ProcessStates.BACKOFF
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(instance.spawnerr, 'unknown error: EPERM')
        self.assertEqual(options.logger.data[0],
                         "spawnerr: unknown error: EPERM")
        self.failUnless(instance.delay)
        self.failUnless(instance.backoff)
        from supervisor.states import ProcessStates
        self.assertEqual(instance.state, ProcessStates.BACKOFF)
        self.assertEqual(len(L), 2)
        event1, event2 = L
        self.assertEqual(event1.__class__, events.ProcessStateStartingEvent)
        self.assertEqual(event2.__class__, events.ProcessStateBackoffEvent)

    def test_spawn_fork_fail_eagain(self):
        options = DummyOptions()
        import errno
        options.fork_error = errno.EAGAIN
        config = DummyPConfig(options, 'good', '/good/filename')
        instance = self._makeOne(config)
        from supervisor.states import ProcessStates
        instance.state = ProcessStates.BACKOFF
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(instance.spawnerr,
                         "Too many processes in process table to spawn 'good'")
        self.assertEqual(options.logger.data[0],
             "spawnerr: Too many processes in process table to spawn 'good'")
        self.assertEqual(len(options.parent_pipes_closed), 6)
        self.assertEqual(len(options.child_pipes_closed), 6)
        self.failUnless(instance.delay)
        self.failUnless(instance.backoff)
        from supervisor.states import ProcessStates
        self.assertEqual(instance.state, ProcessStates.BACKOFF)
        self.assertEqual(len(L), 2)
        event1, event2 = L
        self.assertEqual(event1.__class__, events.ProcessStateStartingEvent)
        self.assertEqual(event2.__class__, events.ProcessStateBackoffEvent)

    def test_spawn_fork_fail_other(self):
        options = DummyOptions()
        options.fork_error = 1
        config = DummyPConfig(options, 'good', '/good/filename')
        instance = self._makeOne(config)
        from supervisor.states import ProcessStates
        instance.state = ProcessStates.BACKOFF
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(instance.spawnerr, 'unknown error: EPERM')
        self.assertEqual(options.logger.data[0],
                         "spawnerr: unknown error: EPERM")
        self.assertEqual(len(options.parent_pipes_closed), 6)
        self.assertEqual(len(options.child_pipes_closed), 6)
        self.failUnless(instance.delay)
        self.failUnless(instance.backoff)
        from supervisor.states import ProcessStates
        self.assertEqual(instance.state, ProcessStates.BACKOFF)
        self.assertEqual(len(L), 2)
        event1, event2 = L
        self.assertEqual(event1.__class__, events.ProcessStateStartingEvent)
        self.assertEqual(event2.__class__, events.ProcessStateBackoffEvent)

    def test_spawn_as_child_setuid_ok(self):
        options = DummyOptions()
        options.forkpid = 0
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.parent_pipes_closed, None)
        self.assertEqual(options.child_pipes_closed, None)
        self.assertEqual(options.pgrp_set, True)
        self.assertEqual(len(options.duped), 3)
        self.assertEqual(len(options.fds_closed), options.minfds - 3)
        self.assertEqual(options.written, {})
        self.assertEqual(options.privsdropped, 1)
        self.assertEqual(options.execv_args,
                         ('/good/filename', ['/good/filename']) )
        self.assertEqual(options._exitcode, 127)

    def test_spawn_as_child_setuid_fail(self):
        options = DummyOptions()
        options.forkpid = 0
        options.setuid_msg = 'screwed'
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.parent_pipes_closed, None)
        self.assertEqual(options.child_pipes_closed, None)
        self.assertEqual(options.pgrp_set, True)
        self.assertEqual(len(options.duped), 3)
        self.assertEqual(len(options.fds_closed), options.minfds - 3)
        self.assertEqual(options.written,
             {2: 'supervisor: error trying to setuid to 1 (screwed)\n'})
        self.assertEqual(options.privsdropped, None)
        self.assertEqual(options.execv_args,
                         ('/good/filename', ['/good/filename']) )
        self.assertEqual(options._exitcode, 127)

    def test_spawn_as_child_cwd_ok(self):
        options = DummyOptions()
        options.forkpid = 0
        config = DummyPConfig(options, 'good', '/good/filename',
                              directory='/tmp')
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.parent_pipes_closed, None)
        self.assertEqual(options.child_pipes_closed, None)
        self.assertEqual(options.pgrp_set, True)
        self.assertEqual(len(options.duped), 3)
        self.assertEqual(len(options.fds_closed), options.minfds - 3)
        self.assertEqual(options.written, {})
        self.assertEqual(options.execv_args,
                         ('/good/filename', ['/good/filename']) )
        self.assertEqual(options._exitcode, 127)
        self.assertEqual(options.changed_directory, True)

    def test_spawn_as_child_sets_umask(self):
        options = DummyOptions()
        options.forkpid = 0
        config = DummyPConfig(options, 'good', '/good/filename', umask=002)
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.written, {})
        self.assertEqual(options.execv_args,
                         ('/good/filename', ['/good/filename']) )
        self.assertEqual(options._exitcode, 127)
        self.assertEqual(options.umaskset, 002)

    def test_spawn_as_child_cwd_fail(self):
        options = DummyOptions()
        options.forkpid = 0
        options.chdir_error = 2
        config = DummyPConfig(options, 'good', '/good/filename',
                              directory='/tmp')
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.parent_pipes_closed, None)
        self.assertEqual(options.child_pipes_closed, None)
        self.assertEqual(options.pgrp_set, True)
        self.assertEqual(len(options.duped), 3)
        self.assertEqual(len(options.fds_closed), options.minfds - 3)
        self.assertEqual(options.execv_args, None)
        self.assertEqual(options.written,
                         {2: "couldn't chdir to /tmp: ENOENT\n"})
        self.assertEqual(options._exitcode, 127)
        self.assertEqual(options.changed_directory, False)

    def test_spawn_as_child_execv_fail_oserror(self):
        options = DummyOptions()
        options.forkpid = 0
        options.execv_error = 1
        config = DummyPConfig(options, 'good', '/good/filename')
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.parent_pipes_closed, None)
        self.assertEqual(options.child_pipes_closed, None)
        self.assertEqual(options.pgrp_set, True)
        self.assertEqual(len(options.duped), 3)
        self.assertEqual(len(options.fds_closed), options.minfds - 3)
        self.assertEqual(options.written,
                         {2: "couldn't exec /good/filename: EPERM\n"})
        self.assertEqual(options.privsdropped, None)
        self.assertEqual(options._exitcode, 127)

    def test_spawn_as_child_execv_fail_runtime_error(self):
        options = DummyOptions()
        options.forkpid = 0
        options.execv_error = 2
        config = DummyPConfig(options, 'good', '/good/filename')
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.parent_pipes_closed, None)
        self.assertEqual(options.child_pipes_closed, None)
        self.assertEqual(options.pgrp_set, True)
        self.assertEqual(len(options.duped), 3)
        self.assertEqual(len(options.fds_closed), options.minfds - 3)
        msg = options.written[2] # dict, 2 is fd #
        self.failUnless(msg.startswith("couldn't exec /good/filename:"))
        self.failUnless("exceptions.RuntimeError" in msg)
        self.assertEqual(options.privsdropped, None)
        self.assertEqual(options._exitcode, 127)

    def test_spawn_as_child_uses_pconfig_environment(self):
        options = DummyOptions()
        options.forkpid = 0
        config = DummyPConfig(options, 'cat', '/bin/cat',
                              environment={'_TEST_':'1'})
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.execv_args, ('/bin/cat', ['/bin/cat']) )
        self.assertEqual(options.execv_environment['_TEST_'], '1')

    def test_spawn_as_child_environment_supervisor_envvars(self):
        options = DummyOptions()
        options.forkpid = 0
        config = DummyPConfig(options, 'cat', '/bin/cat')
        instance = self._makeOne(config)
        class Dummy:
            name = 'dummy'
        instance.group = Dummy()
        instance.group.config = Dummy()
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.execv_args, ('/bin/cat', ['/bin/cat']) )
        self.assertEqual(
            options.execv_environment['SUPERVISOR_ENABLED'], '1')
        self.assertEqual(
            options.execv_environment['SUPERVISOR_PROCESS_NAME'], 'cat')
        self.assertEqual(
            options.execv_environment['SUPERVISOR_GROUP_NAME'], 'dummy')
        self.assertEqual(
            options.execv_environment['SUPERVISOR_SERVER_URL'],
            'http://localhost:9001')

    def test_spawn_as_child_stderr_redirected(self):
        options = DummyOptions()
        options.forkpid = 0
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        config.redirect_stderr = True
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(options.parent_pipes_closed, None)
        self.assertEqual(options.child_pipes_closed, None)
        self.assertEqual(options.pgrp_set, True)
        self.assertEqual(len(options.duped), 2)
        self.assertEqual(len(options.fds_closed), options.minfds - 3)
        self.assertEqual(options.written, {})
        self.assertEqual(options.privsdropped, 1)
        self.assertEqual(options.execv_args,
                         ('/good/filename', ['/good/filename']) )
        self.assertEqual(options._exitcode, 127)

    def test_spawn_as_parent(self):
        options = DummyOptions()
        options.forkpid = 10
        config = DummyPConfig(options, 'good', '/good/filename')
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, 10)
        self.assertEqual(instance.dispatchers[4].__class__, DummyDispatcher)
        self.assertEqual(instance.dispatchers[5].__class__, DummyDispatcher)
        self.assertEqual(instance.dispatchers[7].__class__, DummyDispatcher)
        self.assertEqual(instance.pipes['stdin'], 4)
        self.assertEqual(instance.pipes['stdout'], 5)
        self.assertEqual(instance.pipes['stderr'], 7)
        self.assertEqual(options.parent_pipes_closed, None)
        self.assertEqual(len(options.child_pipes_closed), 6)
        self.assertEqual(options.logger.data[0], "spawned: 'good' with pid 10")
        self.assertEqual(instance.spawnerr, None)
        self.failUnless(instance.delay)
        self.assertEqual(instance.config.options.pidhistory[10], instance)
        from supervisor.states import ProcessStates
        self.assertEqual(instance.state, ProcessStates.STARTING)

    def test_spawn_redirect_stderr(self):
        options = DummyOptions()
        options.forkpid = 10
        config = DummyPConfig(options, 'good', '/good/filename',
                              redirect_stderr=True)
        instance = self._makeOne(config)
        result = instance.spawn()
        self.assertEqual(result, 10)
        self.assertEqual(instance.dispatchers[4].__class__, DummyDispatcher)
        self.assertEqual(instance.dispatchers[5].__class__, DummyDispatcher)
        self.assertEqual(instance.pipes['stdin'], 4)
        self.assertEqual(instance.pipes['stdout'], 5)
        self.assertEqual(instance.pipes['stderr'], None)

    def test_write(self):
        executable = '/bin/cat'
        options = DummyOptions()
        config = DummyPConfig(options, 'output', executable)
        instance = self._makeOne(config)
        sent = 'a' * (1 << 13)
        self.assertRaises(OSError, instance.write, sent)
        options.forkpid = 1
        result = instance.spawn()
        instance.write(sent)
        stdin_fd = instance.pipes['stdin']
        self.assertEqual(sent, instance.dispatchers[stdin_fd].input_buffer)
        instance.killing = True
        self.assertRaises(OSError, instance.write, sent)

    def test_write_dispatcher_closed(self):
        executable = '/bin/cat'
        options = DummyOptions()
        config = DummyPConfig(options, 'output', executable)
        instance = self._makeOne(config)
        sent = 'a' * (1 << 13)
        self.assertRaises(OSError, instance.write, sent)
        options.forkpid = 1
        result = instance.spawn()
        stdin_fd = instance.pipes['stdin']
        instance.dispatchers[stdin_fd].close()
        self.assertRaises(OSError, instance.write, sent)

    def test_write_dispatcher_flush_raises_epipe(self):
        executable = '/bin/cat'
        options = DummyOptions()
        config = DummyPConfig(options, 'output', executable)
        instance = self._makeOne(config)
        sent = 'a' * (1 << 13)
        self.assertRaises(OSError, instance.write, sent)
        options.forkpid = 1
        result = instance.spawn()
        stdin_fd = instance.pipes['stdin']
        instance.dispatchers[stdin_fd].flush_error = errno.EPIPE
        self.assertRaises(OSError, instance.write, sent)

    def dont_test_spawn_and_kill(self):
        # this is a functional test
        from supervisor.tests.base import makeSpew
        try:
            called = 0
            def foo(*args):
                called = 1
            signal.signal(signal.SIGCHLD, foo)
            executable = makeSpew()
            options = DummyOptions()
            config = DummyPConfig(options, 'spew', executable)
            instance = self._makeOne(config)
            result = instance.spawn()
            msg = options.logger.data[0]
            self.failUnless(msg.startswith("spawned: 'spew' with pid"))
            self.assertEqual(len(instance.pipes), 6)
            self.failUnless(instance.pid)
            self.failUnlessEqual(instance.pid, result)
            origpid = instance.pid
            import errno
            while 1:
                try:
                    data = os.popen('ps').read()
                    break
                except IOError, why:
                    if why[0] != errno.EINTR:
                        raise
                        # try again ;-)
            time.sleep(0.1) # arbitrary, race condition possible
            self.failUnless(data.find(`origpid`) != -1 )
            msg = instance.kill(signal.SIGTERM)
            time.sleep(0.1) # arbitrary, race condition possible
            self.assertEqual(msg, None)
            pid, sts = os.waitpid(-1, os.WNOHANG)
            data = os.popen('ps').read()
            self.assertEqual(data.find(`origpid`), -1) # dubious
        finally:
            try:
                os.remove(executable)
            except:
                pass
            signal.signal(signal.SIGCHLD, signal.SIG_DFL)

    def test_stop(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.pid = 11
        dispatcher = DummyDispatcher(writable=True)
        instance.dispatchers = {'foo':dispatcher}
        from supervisor.states import ProcessStates
        instance.state = ProcessStates.RUNNING
        instance.stop()
        self.assertEqual(instance.administrative_stop, 1)
        self.failUnless(instance.delay)
        self.assertEqual(options.logger.data[0], 'killing test (pid 11) with '
                         'signal SIGTERM')
        self.assertEqual(instance.killing, 1)
        self.assertEqual(options.kills[11], signal.SIGTERM)

    def test_give_up(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        L = []
        from supervisor.states import ProcessStates
        from supervisor import events
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        instance.state = ProcessStates.BACKOFF
        instance.give_up()
        self.assertEqual(instance.system_stop, 1)
        self.assertFalse(instance.delay)
        self.assertFalse(instance.backoff)
        self.assertEqual(instance.state, ProcessStates.FATAL)
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateFatalEvent)

    def test_kill_nopid(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.kill(signal.SIGTERM)
        self.assertEqual(options.logger.data[0],
              'attempted to kill test with sig SIGTERM but it wasn\'t running')
        self.assertEqual(instance.killing, 0)

    def test_kill_error(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        options.kill_error = 1
        instance = self._makeOne(config)
        L = []
        from supervisor.states import ProcessStates
        from supervisor import events
        events.subscribe(events.ProcessStateEvent,
                         lambda x: L.append(x))
        instance.pid = 11
        instance.state = ProcessStates.RUNNING
        instance.kill(signal.SIGTERM)
        self.assertEqual(options.logger.data[0], 'killing test (pid 11) with '
                         'signal SIGTERM')
        self.failUnless(options.logger.data[1].startswith(
            'unknown problem killing test'))
        self.assertEqual(instance.killing, 0)
        self.assertEqual(len(L), 2)
        event1 = L[0]
        event2 = L[1]
        self.assertEqual(event1.__class__, events.ProcessStateStoppingEvent)
        self.assertEqual(event2.__class__, events.ProcessStateUnknownEvent)

    def test_kill_from_starting(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.pid = 11
        L = []
        from supervisor.states import ProcessStates
        from supervisor import events
        events.subscribe(events.ProcessStateEvent,lambda x: L.append(x))
        instance.state = ProcessStates.STARTING
        instance.kill(signal.SIGTERM)
        self.assertEqual(options.logger.data[0], 'killing test (pid 11) with '
                         'signal SIGTERM')
        self.assertEqual(instance.killing, 1)
        self.assertEqual(options.kills[11], signal.SIGTERM)
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateStoppingEvent)

    def test_kill_from_running(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.pid = 11
        L = []
        from supervisor.states import ProcessStates
        from supervisor import events
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        instance.state = ProcessStates.RUNNING
        instance.kill(signal.SIGTERM)
        self.assertEqual(options.logger.data[0], 'killing test (pid 11) with '
                         'signal SIGTERM')
        self.assertEqual(instance.killing, 1)
        self.assertEqual(options.kills[11], signal.SIGTERM)
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateStoppingEvent)

    def test_kill_from_stopping(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.pid = 11
        L = []
        from supervisor.states import ProcessStates
        from supervisor import events
        events.subscribe(events.Event,lambda x: L.append(x))
        instance.state = ProcessStates.STOPPING
        instance.kill(signal.SIGKILL)
        self.assertEqual(options.logger.data[0], 'killing test (pid 11) with '
                         'signal SIGKILL')
        self.assertEqual(instance.killing, 1)
        self.assertEqual(options.kills[11], signal.SIGKILL)
        self.assertEqual(L, []) # no event because we didn't change state

    def test_kill_from_stopping_w_killasgroup(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test', killasgroup=True)
        instance = self._makeOne(config)
        instance.pid = 11
        L = []
        from supervisor.states import ProcessStates
        from supervisor import events
        events.subscribe(events.Event,lambda x: L.append(x))
        instance.state = ProcessStates.STOPPING
        instance.kill(signal.SIGKILL)
        self.assertEqual(options.logger.data[0], 'killing test (pid 11) '
                         'process group with signal SIGKILL')
        self.assertEqual(instance.killing, 1)
        self.assertEqual(options.kills[-11], signal.SIGKILL)
        self.assertEqual(L, []) # no event because we didn't change state

    def test_stopasgroup(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test', stopasgroup=True)
        instance = self._makeOne(config)
        instance.pid = 11
        L = []
        from supervisor.states import ProcessStates
        from supervisor import events
        events.subscribe(events.Event,lambda x: L.append(x))
        instance.state = ProcessStates.RUNNING
        instance.kill(signal.SIGTERM)
        self.assertEqual(options.logger.data[0], 'killing test (pid 11) '
                         'process group with signal SIGTERM')
        self.assertEqual(instance.killing, 1)
        self.assertEqual(options.kills[-11], signal.SIGTERM)
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateStoppingEvent)
        self.assertEqual(event.extra_values, [('pid', 11)])
        self.assertEqual(event.from_state, ProcessStates.RUNNING)

    def test_finish(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'notthere', '/notthere',
                              stdout_logfile='/tmp/foo')
        instance = self._makeOne(config)
        instance.waitstatus = (123, 1) # pid, waitstatus
        instance.config.options.pidhistory[123] = instance
        instance.killing = 1
        pipes = {'stdout':'','stderr':''}
        instance.pipes = pipes
        from supervisor.states import ProcessStates
        from supervisor import events
        instance.state = ProcessStates.STOPPING
        L = []
        events.subscribe(events.ProcessStateStoppedEvent, lambda x: L.append(x))
        instance.pid = 123
        instance.finish(123, 1)
        self.assertEqual(instance.killing, 0)
        self.assertEqual(instance.pid, 0)
        self.assertEqual(options.parent_pipes_closed, pipes)
        self.assertEqual(instance.pipes, {})
        self.assertEqual(instance.dispatchers, {})
        self.assertEqual(options.logger.data[0], 'stopped: notthere '
                         '(terminated by SIGHUP)')
        self.assertEqual(instance.exitstatus, -1)
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateStoppedEvent)
        self.assertEqual(event.extra_values, [('pid', 123)])
        self.assertEqual(event.from_state, ProcessStates.STOPPING)

    def test_finish_expected(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'notthere', '/notthere',
                              stdout_logfile='/tmp/foo')
        instance = self._makeOne(config)
        instance.config.options.pidhistory[123] = instance
        pipes = {'stdout':'','stderr':''}
        instance.pipes = pipes
        instance.config.exitcodes =[-1]
        from supervisor.states import ProcessStates
        from supervisor import events
        instance.state = ProcessStates.RUNNING
        L = []
        events.subscribe(events.ProcessStateExitedEvent, lambda x: L.append(x))
        instance.pid = 123
        instance.finish(123, 1)
        self.assertEqual(instance.killing, 0)
        self.assertEqual(instance.pid, 0)
        self.assertEqual(options.parent_pipes_closed, pipes)
        self.assertEqual(instance.pipes, {})
        self.assertEqual(instance.dispatchers, {})
        self.assertEqual(options.logger.data[0],
                         'exited: notthere (terminated by SIGHUP; expected)')
        self.assertEqual(instance.exitstatus, -1)
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__,
                         events.ProcessStateExitedEvent)
        self.assertEqual(event.expected, True)
        self.assertEqual(event.extra_values, [('expected', True), ('pid', 123)])
        self.assertEqual(event.from_state, ProcessStates.RUNNING)

    def test_finish_tooquickly(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'notthere', '/notthere',
                              stdout_logfile='/tmp/foo', startsecs=10)
        instance = self._makeOne(config)
        instance.config.options.pidhistory[123] = instance
        pipes = {'stdout':'','stderr':''}
        instance.pipes = pipes
        instance.config.exitcodes =[-1]
        import time
        instance.laststart = time.time()
        from supervisor.states import ProcessStates
        from supervisor import events
        instance.state = ProcessStates.STARTING
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        instance.pid = 123
        instance.finish(123, 1)
        self.assertEqual(instance.killing, 0)
        self.assertEqual(instance.pid, 0)
        self.assertEqual(options.parent_pipes_closed, pipes)
        self.assertEqual(instance.pipes, {})
        self.assertEqual(instance.dispatchers, {})
        self.assertEqual(options.logger.data[0],
                      'exited: notthere (terminated by SIGHUP; not expected)')
        self.assertEqual(instance.exitstatus, None)
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateBackoffEvent)
        self.assertEqual(event.from_state, ProcessStates.STARTING)

    def test_finish_with_current_event_sends_rejected(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        events.subscribe(events.EventRejectedEvent, lambda x: L.append(x))
        options = DummyOptions()
        config = DummyPConfig(options, 'notthere', '/notthere',
                              stdout_logfile='/tmp/foo', startsecs=10)
        instance = self._makeOne(config)
        from supervisor.states import ProcessStates
        instance.state = ProcessStates.RUNNING
        event = DummyEvent()
        instance.event = event
        instance.finish(123, 1)
        self.assertEqual(len(L), 2)
        event1, event2 = L
        self.assertEqual(event1.__class__,
                         events.ProcessStateExitedEvent)
        self.assertEqual(event2.__class__, events.EventRejectedEvent)
        self.assertEqual(event2.process, instance)
        self.assertEqual(event2.event, event)
        self.assertEqual(instance.event, None)

    def test_set_uid_no_uid(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.set_uid()
        self.assertEqual(options.privsdropped, None)

    def test_set_uid(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test', uid=1)
        instance = self._makeOne(config)
        msg = instance.set_uid()
        self.assertEqual(options.privsdropped, 1)
        self.assertEqual(msg, None)

    def test_cmp_bypriority(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'notthere', '/notthere',
                              stdout_logfile='/tmp/foo',
                              priority=1)
        instance = self._makeOne(config)

        config = DummyPConfig(options, 'notthere1', '/notthere',
                              stdout_logfile='/tmp/foo',
                              priority=2)
        instance1 = self._makeOne(config)

        config = DummyPConfig(options, 'notthere2', '/notthere',
                              stdout_logfile='/tmp/foo',
                              priority=3)
        instance2 = self._makeOne(config)

        L = [instance2, instance, instance1]
        L.sort()

        self.assertEqual(L, [instance, instance1, instance2])

    def test_transition_stopped_to_starting_supervisor_stopping(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates, SupervisorStates
        options = DummyOptions()
        options.mood = SupervisorStates.SHUTDOWN

        # this should not be spawned, as supervisor is shutting down
        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        process = self._makeOne(pconfig)
        process.laststart = 0
        process.state = ProcessStates.STOPPED
        process.transition()
        self.assertEqual(process.state, ProcessStates.STOPPED)
        self.assertEqual(L, [])

    def test_transition_stopped_to_starting_supervisor_running(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates, SupervisorStates
        options = DummyOptions()
        options.mood = SupervisorStates.RUNNING

        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        process = self._makeOne(pconfig)
        process.laststart = 0
        process.state = ProcessStates.STOPPED
        process.transition()
        self.assertEqual(process.state, ProcessStates.STARTING)
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateStartingEvent)

    def test_transition_exited_to_starting_supervisor_stopping(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates, SupervisorStates
        options = DummyOptions()
        options.mood = SupervisorStates.SHUTDOWN

        # this should not be spawned, as supervisor is shutting down
        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        from supervisor.datatypes import RestartUnconditionally
        pconfig.autorestart = RestartUnconditionally
        process = self._makeOne(pconfig)
        process.laststart = 1
        process.system_stop = 1
        process.state = ProcessStates.EXITED
        process.transition()
        self.assertEqual(process.state, ProcessStates.EXITED)
        self.assertEqual(process.system_stop, 1)
        self.assertEqual(L, [])

    def test_transition_exited_to_starting_uncond_supervisor_running(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates
        options = DummyOptions()

        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        from supervisor.datatypes import RestartUnconditionally
        pconfig.autorestart = RestartUnconditionally
        process = self._makeOne(pconfig)
        process.laststart = 1
        process.state = ProcessStates.EXITED
        process.transition()
        self.assertEqual(process.state, ProcessStates.STARTING)
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateStartingEvent)

    def test_transition_exited_to_starting_condit_supervisor_running(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates
        options = DummyOptions()

        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        from supervisor.datatypes import RestartWhenExitUnexpected
        pconfig.autorestart = RestartWhenExitUnexpected
        process = self._makeOne(pconfig)
        process.laststart = 1
        process.state = ProcessStates.EXITED
        process.exitstatus = 'bogus'
        process.transition()
        self.assertEqual(process.state, ProcessStates.STARTING)
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateStartingEvent)

    def test_transition_exited_to_starting_condit_fls_supervisor_running(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates
        options = DummyOptions()

        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        from supervisor.datatypes import RestartWhenExitUnexpected
        pconfig.autorestart = RestartWhenExitUnexpected
        process = self._makeOne(pconfig)
        process.laststart = 1
        process.state = ProcessStates.EXITED
        process.exitstatus = 0
        process.transition()
        self.assertEqual(process.state, ProcessStates.EXITED)
        self.assertEqual(L, [])

    def test_transition_backoff_to_starting_supervisor_stopping(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates, SupervisorStates
        options = DummyOptions()
        options.mood = SupervisorStates.SHUTDOWN

        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        process = self._makeOne(pconfig)
        process.laststart = 1
        process.delay = 0
        process.backoff = 0
        process.state = ProcessStates.BACKOFF
        process.transition()
        self.assertEqual(process.state, ProcessStates.BACKOFF)
        self.assertEqual(L, [])

    def test_transition_backoff_to_starting_supervisor_running(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates, SupervisorStates
        options = DummyOptions()
        options.mood = SupervisorStates.RUNNING

        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        process = self._makeOne(pconfig)
        process.laststart = 1
        process.delay = 0
        process.backoff = 0
        process.state = ProcessStates.BACKOFF
        process.transition()
        self.assertEqual(process.state, ProcessStates.STARTING)
        self.assertEqual(len(L), 1)
        self.assertEqual(L[0].__class__, events.ProcessStateStartingEvent)

    def test_transition_backoff_to_starting_supervisor_running_notyet(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates, SupervisorStates
        options = DummyOptions()
        options.mood = SupervisorStates.RUNNING

        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        process = self._makeOne(pconfig)
        process.laststart = 1
        process.delay = sys.maxint
        process.backoff = 0
        process.state = ProcessStates.BACKOFF
        process.transition()
        self.assertEqual(process.state, ProcessStates.BACKOFF)
        self.assertEqual(L, [])

    def test_transition_starting_to_running(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates

        options = DummyOptions()

        # this should go from STARTING to RUNNING via transition()
        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        process = self._makeOne(pconfig)
        process.backoff = 1
        process.delay = 1
        process.system_stop = 0
        process.laststart = 1
        process.pid = 1
        process.stdout_buffer = 'abc'
        process.stderr_buffer = 'def'
        process.state = ProcessStates.STARTING
        process.transition()

        # this implies RUNNING
        self.assertEqual(process.backoff, 0)
        self.assertEqual(process.delay, 0)
        self.assertEqual(process.system_stop, 0)
        self.assertEqual(options.logger.data[0],
                         'success: process entered RUNNING state, process has '
                         'stayed up for > than 10 seconds (startsecs)')
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateRunningEvent)

    def test_transition_backoff_to_fatal(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates
        options = DummyOptions()

        # this should go from BACKOFF to FATAL via transition()
        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        process = self._makeOne(pconfig)
        process.laststart = 1
        process.backoff = 10000
        process.delay = 1
        process.system_stop = 0
        process.stdout_buffer = 'abc'
        process.stderr_buffer = 'def'
        process.state = ProcessStates.BACKOFF

        process.transition()

        # this implies FATAL
        self.assertEqual(process.backoff, 0)
        self.assertEqual(process.delay, 0)
        self.assertEqual(process.system_stop, 1)
        self.assertEqual(options.logger.data[0],
                         'gave up: process entered FATAL state, too many start'
                         ' retries too quickly')
        self.assertEqual(len(L), 1)
        event = L[0]
        self.assertEqual(event.__class__, events.ProcessStateFatalEvent)

    def test_transition_stops_unkillable_notyet(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates
        options = DummyOptions()

        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        process = self._makeOne(pconfig)
        process.delay = sys.maxint
        process.state = ProcessStates.STOPPING

        process.transition()
        self.assertEqual(process.state, ProcessStates.STOPPING)
        self.assertEqual(L, [])

    def test_transition_stops_unkillable(self):
        from supervisor import events
        L = []
        events.subscribe(events.ProcessStateEvent, lambda x: L.append(x))
        from supervisor.states import ProcessStates
        options = DummyOptions()

        pconfig = DummyPConfig(options, 'process', 'process','/bin/process')
        process = self._makeOne(pconfig)
        process.delay = 0
        process.pid = 1
        process.killing = 0
        process.state = ProcessStates.STOPPING

        process.transition()
        self.assertEqual(process.killing, 1)
        self.assertNotEqual(process.delay, 0)
        self.assertEqual(process.state, ProcessStates.STOPPING)
        self.assertEqual(options.logger.data[0],
                         "killing 'process' (1) with SIGKILL")
        import signal
        self.assertEqual(options.kills[1], signal.SIGKILL)
        self.assertEqual(L, [])

    def test_change_state_doesnt_notify_if_no_state_change(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.state = 10
        self.assertEqual(instance.change_state(10), False)

    def test_change_state_sets_backoff_and_delay(self):
        from supervisor.states import ProcessStates
        options = DummyOptions()
        config = DummyPConfig(options, 'test', '/test')
        instance = self._makeOne(config)
        instance.state = 10
        instance.change_state(ProcessStates.BACKOFF)
        self.assertEqual(instance.backoff, 1)
        self.failUnless(instance.delay > 0)

class FastCGISubprocessTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.process import FastCGISubprocess
        return FastCGISubprocess

    def _makeOne(self, *arg, **kw):
        return self._getTargetClass()(*arg, **kw)

    def tearDown(self):
        from supervisor.events import clear
        clear()

    def test_no_group(self):
        options = DummyOptions()
        options.forkpid = 0
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        instance = self._makeOne(config)
        self.assertRaises(NotImplementedError, instance.spawn)

    def test_no_socket_manager(self):
        options = DummyOptions()
        options.forkpid = 0
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        instance = self._makeOne(config)
        instance.group = DummyProcessGroup(DummyPGroupConfig(options))
        self.assertRaises(NotImplementedError, instance.spawn)
        
    def test_prepare_child_fds(self):
        options = DummyOptions()
        options.forkpid = 0
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        instance = self._makeOne(config)
        sock_config = DummySocketConfig(7)
        gconfig = DummyFCGIGroupConfig(options, 'whatever', 999, None, 
                                       sock_config)
        instance.group = DummyFCGIProcessGroup(gconfig)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(len(options.duped), 3)
        self.assertEqual(options.duped[7], 0)
        self.assertEqual(options.duped[instance.pipes['child_stdout']], 1)
        self.assertEqual(options.duped[instance.pipes['child_stderr']], 2)
        self.assertEqual(len(options.fds_closed), options.minfds - 3)

    def test_prepare_child_fds_stderr_redirected(self):
        options = DummyOptions()
        options.forkpid = 0
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        config.redirect_stderr = True
        instance = self._makeOne(config)
        sock_config = DummySocketConfig(13)
        gconfig = DummyFCGIGroupConfig(options, 'whatever', 999, None, 
                                       sock_config)
        instance.group = DummyFCGIProcessGroup(gconfig)
        result = instance.spawn()
        self.assertEqual(result, None)
        self.assertEqual(len(options.duped), 2)
        self.assertEqual(options.duped[13], 0)
        self.assertEqual(len(options.fds_closed), options.minfds - 3)
        
    def test_before_spawn_gets_socket_ref(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        instance = self._makeOne(config)
        sock_config = DummySocketConfig(7)
        gconfig = DummyFCGIGroupConfig(options, 'whatever', 999, None, 
                                       sock_config)
        instance.group = DummyFCGIProcessGroup(gconfig)
        self.assertTrue(instance.fcgi_sock is None)
        instance.before_spawn()
        self.assertFalse(instance.fcgi_sock is None)
        
    def test_after_finish_removes_socket_ref(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        instance = self._makeOne(config)
        instance.fcgi_sock = 'hello'
        instance.after_finish()
        self.assertTrue(instance.fcgi_sock is None)
    
    #Patch Subprocess.finish() method for this test to verify override
    @patch.object(Subprocess, 'finish', Mock(return_value=sentinel.finish_result))
    def test_finish_override(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        instance = self._makeOne(config)
        instance.after_finish = Mock()
        result = instance.finish(sentinel.pid, sentinel.sts)
        self.assertEqual(sentinel.finish_result, result,
                        'FastCGISubprocess.finish() did not pass thru result')
        self.assertEqual(1, instance.after_finish.call_count,
                            'FastCGISubprocess.after_finish() not called once')
        finish_mock = Subprocess.finish
        self.assertEqual(1, finish_mock.call_count,
                            'Subprocess.finish() not called once')
        pid_arg = finish_mock.call_args[0][1]
        sts_arg = finish_mock.call_args[0][2]
        self.assertEqual(sentinel.pid, pid_arg,
                            'Subprocess.finish() pid arg was not passed')
        self.assertEqual(sentinel.sts, sts_arg,
                            'Subprocess.finish() sts arg was not passed')
                            
    #Patch Subprocess.spawn() method for this test to verify override
    @patch.object(Subprocess, 'spawn', Mock(return_value=sentinel.ppid))
    def test_spawn_override_success(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        instance = self._makeOne(config)
        instance.before_spawn = Mock()
        result = instance.spawn()
        self.assertEqual(sentinel.ppid, result,
                        'FastCGISubprocess.spawn() did not pass thru result')
        self.assertEqual(1, instance.before_spawn.call_count,
                            'FastCGISubprocess.before_spawn() not called once')
        spawn_mock = Subprocess.spawn
        self.assertEqual(1, spawn_mock.call_count,
                            'Subprocess.spawn() not called once')

    #Patch Subprocess.spawn() method for this test to verify error handling
    @patch.object(Subprocess, 'spawn', Mock(return_value=None))
    def test_spawn_error(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'good', '/good/filename', uid=1)
        instance = self._makeOne(config)
        instance.before_spawn = Mock()
        instance.fcgi_sock = 'nuke me on error'
        result = instance.spawn()
        self.assertEqual(None, result,
                        'FastCGISubprocess.spawn() did return None on error')
        self.assertEqual(1, instance.before_spawn.call_count,
                            'FastCGISubprocess.before_spawn() not called once')
        self.assertEqual(None, instance.fcgi_sock,
                'FastCGISubprocess.spawn() did not remove sock ref on error')    

class ProcessGroupBaseTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.process import ProcessGroupBase
        return ProcessGroupBase

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_get_unstopped_processes(self):
        options = DummyOptions()
        from supervisor.states import ProcessStates
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1, state=ProcessStates.STOPPING)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        group = self._makeOne(gconfig)
        group.processes = { 'process1': process1 }
        unstopped = group.get_unstopped_processes()
        self.assertEqual(unstopped, [process1])

    def test_stop_all(self):
        from supervisor.states import ProcessStates
        options = DummyOptions()

        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1, state=ProcessStates.STOPPED)

        pconfig2 = DummyPConfig(options, 'process2', 'process2','/bin/process2')
        process2 = DummyProcess(pconfig2, state=ProcessStates.RUNNING)

        pconfig3 = DummyPConfig(options, 'process3', 'process3','/bin/process3')
        process3 = DummyProcess(pconfig3, state=ProcessStates.STARTING)
        pconfig4 = DummyPConfig(options, 'process4', 'process4','/bin/process4')
        process4 = DummyProcess(pconfig4, state=ProcessStates.BACKOFF)
        process4.delay = 1000
        process4.backoff = 10
        gconfig = DummyPGroupConfig(
            options,
            pconfigs=[pconfig1, pconfig2, pconfig3, pconfig4])
        group = self._makeOne(gconfig)
        group.processes = {'process1': process1, 'process2': process2,
                           'process3':process3, 'process4':process4}

        group.stop_all()
        self.assertEqual(process1.stop_called, False)
        self.assertEqual(process2.stop_called, True)
        self.assertEqual(process3.stop_called, True)
        self.assertEqual(process4.stop_called, False)
        self.assertEqual(process4.state, ProcessStates.FATAL)

    def test_get_dispatchers(self):
        options = DummyOptions()
        from supervisor.states import ProcessStates
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1, state=ProcessStates.STOPPING)
        process1.dispatchers = {4:None}
        pconfig2 = DummyPConfig(options, 'process2', 'process2','/bin/process2')
        process2 = DummyProcess(pconfig2, state=ProcessStates.STOPPING)
        process2.dispatchers = {5:None}
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1, pconfig2])
        group = self._makeOne(gconfig)
        group.processes = { 'process1': process1, 'process2': process2 }
        result= group.get_dispatchers()
        self.assertEqual(result, {4:None, 5:None})
        
    def test_reopenlogs(self):
        options = DummyOptions()
        from supervisor.states import ProcessStates
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1, state=ProcessStates.STOPPING)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        group = self._makeOne(gconfig)
        group.processes = {'process1': process1}
        group.reopenlogs()
        self.assertEqual(process1.logs_reopened, True)

    def test_removelogs(self):
        options = DummyOptions()
        from supervisor.states import ProcessStates
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1, state=ProcessStates.STOPPING)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        group = self._makeOne(gconfig)
        group.processes = {'process1': process1}
        group.removelogs()
        self.assertEqual(process1.logsremoved, True)

    def test_cmp(self):
        options = DummyOptions()
        gconfig1 = DummyPGroupConfig(options)
        group1 = self._makeOne(gconfig1)
        
        gconfig2 = DummyPGroupConfig(options)
        group2 = self._makeOne(gconfig2)

        group1.priority = 5
        group2.priority = 1

        L = [group1, group2]
        L.sort()

        self.assertEqual(L, [group2, group1])

class ProcessGroupTests(ProcessGroupBaseTests):
    def _getTargetClass(self):
        from supervisor.process import ProcessGroup
        return ProcessGroup

    def test_repr(self):
        options = DummyOptions()
        gconfig = DummyPGroupConfig(options)
        group = self._makeOne(gconfig)
        s = repr(group)
        self.assertTrue(s.startswith(
            '<supervisor.process.ProcessGroup instance at'), s)
        self.assertTrue(s.endswith('named whatever>'), s)

    def test_transition(self):
        options = DummyOptions()
        from supervisor.states import ProcessStates
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1, state=ProcessStates.STOPPING)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        group = self._makeOne(gconfig)
        group.processes = {'process1': process1}
        group.transition()
        self.assertEqual(process1.transitioned, True)
                
class EventListenerPoolTests(ProcessGroupBaseTests):
    def setUp(self):
        from supervisor.events import clear
        clear()

    def tearDown(self):
        from supervisor.events import clear
        clear()
        
    def _getTargetClass(self):
        from supervisor.process import EventListenerPool
        return EventListenerPool

    def test_ctor(self):
        options = DummyOptions()
        gconfig = DummyPGroupConfig(options)
        class EventType:
            pass
        gconfig.pool_events = (EventType,)
        pool = self._makeOne(gconfig)
        from supervisor import events
        self.assertEqual(len(events.callbacks), 2)
        self.assertEqual(events.callbacks[0], 
            (EventType, pool._acceptEvent))
        self.assertEqual(events.callbacks[1], 
            (events.EventRejectedEvent, pool.handle_rejected))
        self.assertEqual(pool.serial, -1)

    def test__eventEnvelope(self):
        options = DummyOptions()
        options.identifier = 'thesupervisorname'
        gconfig = DummyPGroupConfig(options)
        gconfig.name = 'thepoolname'
        pool = self._makeOne(gconfig)
        from supervisor import events
        result = pool._eventEnvelope(
            events.EventTypes.PROCESS_COMMUNICATION_STDOUT, 80, 20, 'payload\n')
        header, payload = result.split('\n', 1)
        headers = header.split()
        self.assertEqual(headers[0], 'ver:3.0')
        self.assertEqual(headers[1], 'server:thesupervisorname')
        self.assertEqual(headers[2], 'serial:80')
        self.assertEqual(headers[3], 'pool:thepoolname')
        self.assertEqual(headers[4], 'poolserial:20')
        self.assertEqual(headers[5], 'eventname:PROCESS_COMMUNICATION_STDOUT')
        self.assertEqual(headers[6], 'len:8')
        self.assertEqual(payload, 'payload\n')

    def test_handle_rejected_no_overflow(self):
        options = DummyOptions()
        gconfig = DummyPGroupConfig(options)
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        pool = self._makeOne(gconfig)
        pool.processes = {'process1': process1}
        pool.event_buffer = [None, None]
        class DummyEvent1:
            serial = 'abc'
        class DummyEvent2:
            process = process1
            event = DummyEvent1()
        dummyevent = DummyEvent2()
        dummyevent.serial = 1
        pool.handle_rejected(dummyevent)
        self.assertEqual(pool.event_buffer, [dummyevent.event, None, None])
        
    def test_handle_rejected_event_buffer_overflowed(self):
        options = DummyOptions()
        gconfig = DummyPGroupConfig(options)
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        gconfig.buffer_size = 3
        pool = self._makeOne(gconfig)
        pool.processes = {'process1': process1}
        class DummyEvent:
            def __init__(self, serial):
                self.serial = serial
        class DummyRejectedEvent:
            def __init__(self, serial):
                self.process = process1
                self.event = DummyEvent(serial)
        event_a = DummyEvent('a')
        event_b = DummyEvent('b')
        event_c = DummyEvent('c')
        rej_event = DummyRejectedEvent('rejected')
        pool.event_buffer = [event_a, event_b, event_c]
        pool.handle_rejected(rej_event)
        serials = [ x.serial for x in pool.event_buffer ]
        # we popped a, and we inserted the rejected event into the 1st pos
        self.assertEqual(serials, ['rejected', 'b', 'c'])
        self.assertEqual(pool.config.options.logger.data[0],
            'pool whatever event buffer overflowed, discarding event a')

    def test_dispatch_pipe_error(self):
        options = DummyOptions()
        gconfig = DummyPGroupConfig(options)
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        from supervisor.states import EventListenerStates
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        pool = self._makeOne(gconfig)
        process1 = pool.processes['process1']
        import errno
        process1.write_error = errno.EPIPE
        process1.listener_state = EventListenerStates.READY
        event = DummyEvent()
        pool._acceptEvent(event)
        pool.dispatch()
        self.assertEqual(process1.listener_state, EventListenerStates.READY)
        self.assertEqual(pool.event_buffer, [event])
        self.assertEqual(options.logger.data[0],
                         'rebuffering event abc for pool whatever (bufsize 0)')

    def test__acceptEvent_attaches_pool_serial_and_serial(self):
        from supervisor.process import GlobalSerial
        options = DummyOptions()
        gconfig = DummyPGroupConfig(options)
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        pool = self._makeOne(gconfig)
        process1 = pool.processes['process1']
        from supervisor.states import EventListenerStates
        process1.listener_state = EventListenerStates.READY
        event = DummyEvent(None)
        pool._acceptEvent(event)
        self.assertEqual(event.serial, GlobalSerial.serial)
        self.assertEqual(event.pool_serials['whatever'], pool.serial)

    def test_repr(self):
        options = DummyOptions()
        gconfig = DummyPGroupConfig(options)
        pool = self._makeOne(gconfig)
        s = repr(pool)
        self.assertTrue(s.startswith(
            '<supervisor.process.EventListenerPool instance at'))
        self.assertTrue(s.endswith('named whatever>'))

    def test_transition_nobody_ready(self):
        options = DummyOptions()
        from supervisor.states import ProcessStates
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1, state=ProcessStates.STARTING)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        pool = self._makeOne(gconfig)
        pool.processes = {'process1': process1}
        event = DummyEvent()
        event.serial = 'a'
        from supervisor.states import EventListenerStates
        process1.listener_state = EventListenerStates.BUSY
        pool._acceptEvent(event)
        pool.transition()
        self.assertEqual(process1.transitioned, True)
        self.assertEqual(pool.event_buffer, [event])
        data = pool.config.options.logger.data
    
    def test_transition_event_proc_not_running(self):
        options = DummyOptions()
        from supervisor.states import ProcessStates
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1, state=ProcessStates.STARTING)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        pool = self._makeOne(gconfig)
        pool.processes = {'process1': process1}
        event = DummyEvent()
        from supervisor.states import EventListenerStates
        event.serial = 1
        process1.listener_state = EventListenerStates.READY
        pool._acceptEvent(event)
        pool.transition()
        self.assertEqual(process1.transitioned, True)
        self.assertEqual(pool.event_buffer, [event])
        self.assertEqual(process1.stdin_buffer, '')
        self.assertEqual(process1.listener_state, EventListenerStates.READY)

    def test_transition_event_proc_running(self):
        options = DummyOptions()
        from supervisor.states import ProcessStates
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        process1 = DummyProcess(pconfig1, state=ProcessStates.RUNNING)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig1])
        pool = self._makeOne(gconfig)
        pool.processes = {'process1': process1}
        event = DummyEvent()
        from supervisor.states import EventListenerStates
        process1.listener_state = EventListenerStates.READY
        class DummyGroup:
            config = gconfig
        process1.group = DummyGroup
        pool._acceptEvent(event)
        pool.transition()
        self.assertEqual(process1.transitioned, True)
        self.assertEqual(pool.event_buffer, [])
        header, payload = process1.stdin_buffer.split('\n', 1)
        self.assertEquals(payload, 'dummy event', payload)
        self.assertEqual(process1.listener_state, EventListenerStates.BUSY)
        self.assertEqual(process1.event, event)

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')


########NEW FILE########
__FILENAME__ = test_rpcinterfaces
import unittest
import sys
import os
import time
import errno

from supervisor.tests.base import DummyOptions
from supervisor.tests.base import DummySupervisor
from supervisor.tests.base import DummyProcess
from supervisor.tests.base import DummyPConfig
from supervisor.tests.base import DummyPGroupConfig
from supervisor.tests.base import DummyProcessGroup
from supervisor.tests.base import PopulatedDummySupervisor
from supervisor.tests.base import _NOW
from supervisor.tests.base import _TIMEFORMAT

class TestBase(unittest.TestCase):
    def setUp(self):
        pass

    def tearDown(self):
        pass

    def _assertRPCError(self, code, callable, *args, **kw):
        from supervisor import xmlrpc
        try:
            callable(*args, **kw)
        except xmlrpc.RPCError, inst:
            self.assertEqual(inst.code, code)
        else:
            raise AssertionError("Didnt raise")

class MainXMLRPCInterfaceTests(TestBase):

    def _getTargetClass(self):
        from supervisor import xmlrpc
        return xmlrpc.RootRPCInterface

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_ctor(self):
        interface = self._makeOne([('supervisor', None)])
        self.assertEqual(interface.supervisor, None)

    def test_traverse(self):
        dummy = DummyRPCInterface()
        interface = self._makeOne([('dummy', dummy)])
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.UNKNOWN_METHOD,
                             xmlrpc.traverse, interface, 'notthere.hello', [])
        self._assertRPCError(xmlrpc.Faults.UNKNOWN_METHOD,
                             xmlrpc.traverse, interface,
                             'supervisor._readFile', [])
        self._assertRPCError(xmlrpc.Faults.INCORRECT_PARAMETERS,
                             xmlrpc.traverse, interface,
                             'dummy.hello', [1])
        self.assertEqual(xmlrpc.traverse(
            interface, 'dummy.hello', []), 'Hello!')
            
class SupervisorNamespaceXMLRPCInterfaceTests(TestBase):
    def _getTargetClass(self):
        from supervisor import rpcinterface
        return rpcinterface.SupervisorNamespaceRPCInterface

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_update(self):
        from supervisor import xmlrpc
        from supervisor.supervisord import SupervisorStates
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        interface._update('foo')
        self.assertEqual(interface.update_text, 'foo')
        supervisord.options.mood = SupervisorStates.SHUTDOWN
        self._assertRPCError(xmlrpc.Faults.SHUTDOWN_STATE, interface._update,
                             'foo')

    def test_getAPIVersion(self):
        from supervisor import rpcinterface
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        version = interface.getAPIVersion()
        self.assertEqual(version, rpcinterface.API_VERSION)
        self.assertEqual(interface.update_text, 'getAPIVersion')

    def test_getAPIVersion_aliased_to_deprecated_getVersion(self):
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        self.assertEqual(interface.getAPIVersion, interface.getVersion)

    def test_getSupervisorVersion(self):
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        version = interface.getSupervisorVersion()
        from supervisor import options
        self.assertEqual(version, options.VERSION)
        self.assertEqual(interface.update_text, 'getSupervisorVersion')
        

    def test_getIdentification(self):
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        identifier = interface.getIdentification()
        self.assertEqual(identifier, supervisord.options.identifier)
        self.assertEqual(interface.update_text, 'getIdentification')

    def test_getState(self):
        from supervisor.states import getSupervisorStateDescription
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        stateinfo = interface.getState()
        statecode = supervisord.options.mood
        statename = getSupervisorStateDescription(statecode)
        self.assertEqual(stateinfo['statecode'], statecode)
        self.assertEqual(stateinfo['statename'], statename)
        self.assertEqual(interface.update_text, 'getState')

    def test_getPID(self):
        options = DummyOptions()
        supervisord = DummySupervisor(options)
        interface = self._makeOne(supervisord)
        self.assertEqual(interface.getPID(), options.get_pid())
        self.assertEqual(interface.update_text, 'getPID')

    def test_readLog_aliased_to_deprecated_readMainLog(self):
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        self.assertEqual(interface.readMainLog, interface.readLog)

    def test_readLog_unreadable(self):
        from supervisor import xmlrpc
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.NO_FILE, interface.readLog,
                             offset=0, length=1)

    def test_readLog_badargs(self):
        from supervisor import xmlrpc
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        try:
            logfile = supervisord.options.logfile
            f = open(logfile, 'w+')
            f.write('x' * 2048)
            f.close()
            self._assertRPCError(xmlrpc.Faults.BAD_ARGUMENTS,
                                 interface.readLog, offset=-1, length=1)
            self._assertRPCError(xmlrpc.Faults.BAD_ARGUMENTS,
                                 interface.readLog, offset=-1,
                                 length=-1)
        finally:
            os.remove(logfile)

    def test_readLog(self):
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        logfile = supervisord.options.logfile
        import os
        try:
            f = open(logfile, 'w+')
            f.write('x' * 2048)
            f.write('y' * 2048)
            f.close()
            data = interface.readLog(offset=0, length=0)
            self.assertEqual(interface.update_text, 'readLog')
            self.assertEqual(data, ('x' * 2048) + ('y' * 2048))
            data = interface.readLog(offset=2048, length=0)
            self.assertEqual(data, 'y' * 2048)
            data = interface.readLog(offset=0, length=2048)
            self.assertEqual(data, 'x' * 2048)
            data = interface.readLog(offset=-4, length=0)
            self.assertEqual(data, 'y' * 4)
        finally:
            os.remove(logfile)

    def test_clearLog_unreadable(self):
        from supervisor import xmlrpc
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.NO_FILE, interface.clearLog)

    def test_clearLog_unremoveable(self):
        from supervisor import xmlrpc
        options = DummyOptions()
        options.existing = [options.logfile]
        options.remove_error = 1
        supervisord = DummySupervisor(options)
        interface = self._makeOne(supervisord)
        logfile = supervisord.options.logfile
        self.assertRaises(xmlrpc.RPCError, interface.clearLog)
        self.assertEqual(interface.update_text, 'clearLog')

    def test_clearLog(self):
        options = DummyOptions()
        options.existing = [options.logfile]
        supervisord = DummySupervisor(options)
        interface = self._makeOne(supervisord)
        result = interface.clearLog()
        self.assertEqual(interface.update_text, 'clearLog')
        self.assertEqual(result, True)
        self.assertEqual(options.removed[0], options.logfile)
        for handler in supervisord.options.logger.handlers:
            self.assertEqual(handler.reopened, True)

    def test_shutdown(self):
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        value = interface.shutdown()
        self.assertEqual(value, True)
        self.assertEqual(supervisord.options.mood, -1)

    def test_restart(self):
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        value = interface.restart()
        self.assertEqual(value, True)
        self.assertEqual(supervisord.options.mood, 0)

    def test_reloadConfig(self):
        options = DummyOptions()
        supervisord = DummySupervisor(options)
        interface = self._makeOne(supervisord)

        changes = [ [DummyPGroupConfig(options, 'added')],
                    [DummyPGroupConfig(options, 'changed')],
                    [DummyPGroupConfig(options, 'dropped')] ]

        supervisord.diff_to_active = lambda : changes

        value = interface.reloadConfig()
        self.assertEqual(value, [[['added'], ['changed'], ['dropped']]])

    def test_reloadConfig_process_config_file_raises_ValueError(self):
        from supervisor import xmlrpc
        options = DummyOptions()
        def raise_exc(*arg, **kw):
            raise ValueError('foo')
        options.process_config_file = raise_exc
        supervisord = DummySupervisor(options)
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.CANT_REREAD, interface.reloadConfig)

    def test_addProcessGroup(self):
        from supervisor.supervisord import Supervisor
        from supervisor import xmlrpc
        options = DummyOptions()
        supervisord = Supervisor(options)
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False)
        gconfig = DummyPGroupConfig(options, 'group1', pconfigs=[pconfig])
        supervisord.options.process_group_configs = [gconfig]

        interface = self._makeOne(supervisord)

        result = interface.addProcessGroup('group1')
        self.assertTrue(result)
        self.assertEqual(supervisord.process_groups.keys(), ['group1'])

        self._assertRPCError(xmlrpc.Faults.ALREADY_ADDED,
                             interface.addProcessGroup, 'group1')
        self.assertEqual(supervisord.process_groups.keys(), ['group1'])

        self._assertRPCError(xmlrpc.Faults.BAD_NAME,
                             interface.addProcessGroup, 'asdf')
        self.assertEqual(supervisord.process_groups.keys(), ['group1'])

    def test_removeProcessGroup(self):
        from supervisor.supervisord import Supervisor
        options = DummyOptions()
        supervisord = Supervisor(options)
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False)
        gconfig = DummyPGroupConfig(options, 'group1', pconfigs=[pconfig])
        supervisord.options.process_group_configs = [gconfig]

        interface = self._makeOne(supervisord)

        interface.addProcessGroup('group1')
        result = interface.removeProcessGroup('group1')
        self.assertTrue(result)
        self.assertEqual(supervisord.process_groups.keys(), [])

    def test_removeProcessGroup_bad_name(self):
        from supervisor.supervisord import Supervisor
        from supervisor import xmlrpc
        options = DummyOptions()
        supervisord = Supervisor(options)
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False)
        gconfig = DummyPGroupConfig(options, 'group1', pconfigs=[pconfig])
        supervisord.options.process_group_configs = [gconfig]

        interface = self._makeOne(supervisord)

        self._assertRPCError(xmlrpc.Faults.BAD_NAME,
                             interface.removeProcessGroup, 'asdf')

    def test_removeProcessGroup_still_running(self):
        from supervisor.supervisord import Supervisor
        from supervisor import xmlrpc
        options = DummyOptions()
        supervisord = Supervisor(options)
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False)
        gconfig = DummyPGroupConfig(options, 'group1', pconfigs=[pconfig])
        supervisord.options.process_group_configs = [gconfig]
        process = DummyProcessGroup(gconfig)
        process.unstopped_processes = [123]
        supervisord.process_groups = {'group1':process}
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.STILL_RUNNING,
                             interface.removeProcessGroup, 'group1')


    def test_startProcess_already_started(self):
        from supervisor import xmlrpc
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False)
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        supervisord.set_procattr('foo', 'pid', 10)
        interface = self._makeOne(supervisord)
        callback = interface.startProcess('foo')
        self._assertRPCError(xmlrpc.Faults.ALREADY_STARTED,
                             callback)

    def test_startProcess_bad_group_name(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False)
        supervisord = PopulatedDummySupervisor(options, 'group1', pconfig)
        interface = self._makeOne(supervisord)
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.BAD_NAME,
                             interface.startProcess, 'group2:foo')

    def test_startProcess_bad_process_name(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False)
        supervisord = PopulatedDummySupervisor(options, 'group1', pconfig)
        interface = self._makeOne(supervisord)
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.BAD_NAME,
                             interface.startProcess, 'group1:bar')

    def test_startProcess_file_not_found(self):
        options = DummyOptions()
        pconfig  = DummyPConfig(options, 'foo', '/foo/bar', autostart=False)
        from supervisor.options import NotFound
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        process = supervisord.process_groups['foo'].processes['foo']
        process.execv_arg_exception = NotFound
        interface = self._makeOne(supervisord)
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.NO_FILE,
                             interface.startProcess, 'foo')

    def test_startProcess_file_not_executable(self):
        options = DummyOptions()
        pconfig  = DummyPConfig(options, 'foo', '/foo/bar', autostart=False)
        from supervisor.options import NotExecutable
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        process = supervisord.process_groups['foo'].processes['foo']
        process.execv_arg_exception = NotExecutable
        interface = self._makeOne(supervisord)
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.NOT_EXECUTABLE,
                             interface.startProcess, 'foo')

    def test_startProcess_spawnerr(self):
        from supervisor import xmlrpc
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False)
        from supervisor.process import ProcessStates
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        supervisord.set_procattr('foo', 'state', ProcessStates.STOPPED)
        process = supervisord.process_groups['foo'].processes['foo']
        process.spawnerr = 'abc'
        interface = self._makeOne(supervisord)
        callback = interface.startProcess('foo')
        self._assertRPCError(xmlrpc.Faults.SPAWN_ERROR, callback)

    def test_startProcess(self):
        from supervisor import http
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False,
                               startsecs=.01)
        from supervisor.process import ProcessStates
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        supervisord.set_procattr('foo', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.startProcess('foo')
        self.assertEqual(callback(), http.NOT_DONE_YET)
        process = supervisord.process_groups['foo'].processes['foo']
        self.assertEqual(process.spawned, True)
        self.assertEqual(interface.update_text, 'startProcess')
        from supervisor.process import ProcessStates
        process.state = ProcessStates.RUNNING
        time.sleep(.02)
        result = callback()
        self.assertEqual(result, True)

    def test_startProcess_nowait(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False)
        from supervisor.process import ProcessStates
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        supervisord.set_procattr('foo', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.startProcess('foo', wait=False)
        self.assertEqual(callback(), True)
        process = supervisord.process_groups['foo'].processes['foo']
        self.assertEqual(process.spawned, True)
        self.assertEqual(interface.update_text, 'startProcess')

    def test_startProcess_nostartsecs(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False,
                               startsecs=0)
        from supervisor.process import ProcessStates
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        supervisord.set_procattr('foo', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.startProcess('foo', wait=True)
        self.assertEqual(callback(), True)
        process = supervisord.process_groups['foo'].processes['foo']
        self.assertEqual(process.spawned, True)
        self.assertEqual(interface.update_text, 'startProcess')

    def test_startProcess_abnormal_term_process_not_running(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False)
        from supervisor.process import ProcessStates
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        supervisord.set_procattr('foo', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.startProcess('foo', 100) # milliseconds
        result = callback()
        from supervisor import http
        self.assertEqual(result, http.NOT_DONE_YET)
        process = supervisord.process_groups['foo'].processes['foo']
        self.assertEqual(process.spawned, True)
        self.assertEqual(interface.update_text, 'startProcess')
        from supervisor.process import ProcessStates
        process.state = ProcessStates.BACKOFF

        time.sleep(.1)
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.ABNORMAL_TERMINATION, callback)
    
    def test_startProcess_abormal_term_startsecs_exceeded(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', __file__, autostart=False,
                               startsecs=.01)
        from supervisor.process import ProcessStates
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        supervisord.set_procattr('foo', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.startProcess('foo', 100) # milliseconds
        result = callback()
        from supervisor import http
        self.assertEqual(result, http.NOT_DONE_YET)
        supervisord.set_procattr('foo', 'state', ProcessStates.STARTING)
        process = supervisord.process_groups['foo'].processes['foo']
        self.assertEqual(process.spawned, True)
        self.assertEqual(interface.update_text, 'startProcess')
        from supervisor.process import ProcessStates
        process.state = ProcessStates.STARTING

        time.sleep(.2)
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.ABNORMAL_TERMINATION, callback)

    def test_startProcess_splat_calls_startProcessGroup(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', __file__, autostart=False,
                               startsecs=.01)
        pconfig2 = DummyPConfig(options, 'process2', __file__, priority=2,
                                startsecs=.01)
        supervisord = PopulatedDummySupervisor(options, 'foo',
                                               pconfig1, pconfig2)
        from supervisor.process import ProcessStates
        supervisord.set_procattr('process1', 'state', ProcessStates.STOPPED)
        supervisord.set_procattr('process2', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.startProcess('foo:*')
        self.assertEqual(interface.update_text, 'startProcessGroup')

    def test_startProcessGroup(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', __file__, priority=1,
                                startsecs=.01)
        pconfig2 = DummyPConfig(options, 'process2', __file__, priority=2,
                                startsecs=.01)
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1,
                                               pconfig2)
        from supervisor.process import ProcessStates
        supervisord.set_procattr('process1', 'state', ProcessStates.STOPPED)
        supervisord.set_procattr('process2', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.startProcessGroup('foo')

        from supervisor.http import NOT_DONE_YET

        # create callbacks in startall()
        self.assertEqual(callback(), NOT_DONE_YET)
        # start first process
        self.assertEqual(callback(), NOT_DONE_YET)
        # start second process
        self.assertEqual(callback(), NOT_DONE_YET)

        # wait for timeout 1
        time.sleep(.02)
        self.assertEqual(callback(), NOT_DONE_YET)

        # wait for timeout 2
        time.sleep(.02)
        result = callback()

        self.assertEqual(len(result), 2)

        from supervisor.xmlrpc import Faults

        # XXX not sure about this ordering, I think process1 should
        # probably show up first
        self.assertEqual(result[0]['name'], 'process2')
        self.assertEqual(result[0]['group'], 'foo')
        self.assertEqual(result[0]['status'],  Faults.SUCCESS)
        self.assertEqual(result[0]['description'], 'OK')

        self.assertEqual(result[1]['name'], 'process1')
        self.assertEqual(result[1]['group'], 'foo')
        self.assertEqual(result[1]['status'],  Faults.SUCCESS)
        self.assertEqual(result[1]['description'], 'OK')

        self.assertEqual(interface.update_text, 'startProcess')

        process1 = supervisord.process_groups['foo'].processes['process1']
        self.assertEqual(process1.spawned, True)
        process2 = supervisord.process_groups['foo'].processes['process2']
        self.assertEqual(process2.spawned, True)

    def test_startProcessGroup_nowait(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', __file__, priority=1,
                                startsecs=.01)
        pconfig2 = DummyPConfig(options, 'process2', __file__, priority=2,
                                startsecs=.01)
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1,
                                               pconfig2)
        from supervisor.process import ProcessStates
        supervisord.set_procattr('process1', 'state', ProcessStates.STOPPED)
        supervisord.set_procattr('process2', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.startProcessGroup('foo', wait=False)
        from supervisor.http import NOT_DONE_YET
        from supervisor.xmlrpc import Faults

        # create callbacks in startall()
        self.assertEqual(callback(), NOT_DONE_YET)

        # get a result
        result = callback()

        self.assertEqual(len(result), 2)
        self.assertEqual(result[0]['name'], 'process1')
        self.assertEqual(result[0]['group'], 'foo')
        self.assertEqual(result[0]['status'],  Faults.SUCCESS)
        self.assertEqual(result[0]['description'], 'OK')

        self.assertEqual(result[1]['name'], 'process2')
        self.assertEqual(result[1]['group'], 'foo')
        self.assertEqual(result[1]['status'],  Faults.SUCCESS)
        self.assertEqual(result[1]['description'], 'OK')

    def test_startProcessGroup_badname(self):
        from supervisor import xmlrpc
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.BAD_NAME,
                             interface.startProcessGroup, 'foo')


    def test_startAllProcesses(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', __file__, priority=1,
                                startsecs=.01)
        pconfig2 = DummyPConfig(options, 'process2', __file__, priority=2,
                                startsecs=.01)
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1,
                                               pconfig2)
        from supervisor.process import ProcessStates
        supervisord.set_procattr('process1', 'state', ProcessStates.STOPPED)
        supervisord.set_procattr('process2', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.startAllProcesses()

        from supervisor.http import NOT_DONE_YET

        # create callbacks in startall()
        self.assertEqual(callback(), NOT_DONE_YET)
        # start first process
        self.assertEqual(callback(), NOT_DONE_YET)
        # start second process
        self.assertEqual(callback(), NOT_DONE_YET)

        # wait for timeout 1
        time.sleep(.02)
        self.assertEqual(callback(), NOT_DONE_YET)

        # wait for timeout 2
        time.sleep(.02)
        result = callback()

        self.assertEqual(len(result), 2)

        from supervisor.xmlrpc import Faults

        # XXX not sure about this ordering, I think process1 should
        # probably show up first
        self.assertEqual(result[0]['name'], 'process2')
        self.assertEqual(result[0]['group'], 'foo')
        self.assertEqual(result[0]['status'],  Faults.SUCCESS)
        self.assertEqual(result[0]['description'], 'OK')

        self.assertEqual(result[1]['name'], 'process1')
        self.assertEqual(result[1]['group'], 'foo')
        self.assertEqual(result[1]['status'],  Faults.SUCCESS)
        self.assertEqual(result[1]['description'], 'OK')

        self.assertEqual(interface.update_text, 'startProcess')

        process1 = supervisord.process_groups['foo'].processes['process1']
        self.assertEqual(process1.spawned, True)
        process2 = supervisord.process_groups['foo'].processes['process2']
        self.assertEqual(process2.spawned, True)

    def test_startAllProcesses_nowait(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', __file__, priority=1,
                                startsecs=.01)
        pconfig2 = DummyPConfig(options, 'process2', __file__, priority=2,
                                startsecs=.01)
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1,
                                               pconfig2)
        from supervisor.process import ProcessStates
        supervisord.set_procattr('process1', 'state', ProcessStates.STOPPED)
        supervisord.set_procattr('process2', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.startAllProcesses(wait=False)
        from supervisor.http import NOT_DONE_YET
        from supervisor.xmlrpc import Faults

        # create callbacks in startall()
        self.assertEqual(callback(), NOT_DONE_YET)

        # get a result
        result = callback()

        self.assertEqual(len(result), 2)
        self.assertEqual(result[0]['name'], 'process1')
        self.assertEqual(result[0]['group'], 'foo')
        self.assertEqual(result[0]['status'],  Faults.SUCCESS)
        self.assertEqual(result[0]['description'], 'OK')

        self.assertEqual(result[1]['name'], 'process2')
        self.assertEqual(result[1]['group'], 'foo')
        self.assertEqual(result[1]['status'],  Faults.SUCCESS)
        self.assertEqual(result[1]['description'], 'OK')

    def test_stopProcess_badname(self):
        from supervisor import xmlrpc
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.BAD_NAME,
                             interface.stopProcess, 'foo')

    def test_stopProcess(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo')
        from supervisor.process import ProcessStates
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        supervisord.set_procattr('foo', 'state', ProcessStates.RUNNING)
        interface = self._makeOne(supervisord)
        callback = interface.stopProcess('foo')
        self.assertEqual(interface.update_text, 'stopProcess')
        process = supervisord.process_groups['foo'].processes['foo']
        self.assertEqual(process.backoff, 0)
        self.assertEqual(process.delay, 0)
        self.assertEqual(process.killing, 0)
        from supervisor import http
        self.assertEqual(callback(), http.NOT_DONE_YET)
        from supervisor.process import ProcessStates
        self.assertEqual(process.state, ProcessStates.STOPPED)
        self.assertEqual(callback(), True)
        self.assertEqual(len(supervisord.process_groups['foo'].processes), 1)
        self.assertEqual(interface.update_text, 'stopProcess')

    def test_stopProcess_nowait(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', __file__)
        from supervisor.process import ProcessStates
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        supervisord.set_procattr('foo', 'state', ProcessStates.RUNNING)
        interface = self._makeOne(supervisord)
        callback = interface.stopProcess('foo', wait=False)
        self.assertEqual(callback(), True)
        process = supervisord.process_groups['foo'].processes['foo']
        self.assertEqual(process.stop_called, True)
        self.assertEqual(interface.update_text, 'stopProcess')

    def test_stopProcessGroup(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', '/bin/foo')
        pconfig2 = DummyPConfig(options, 'process2', '/bin/foo2')
        from supervisor.process import ProcessStates
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1,
                                               pconfig2)
        supervisord.set_procattr('process1', 'state', ProcessStates.RUNNING)
        supervisord.set_procattr('process2', 'state', ProcessStates.RUNNING)
        interface = self._makeOne(supervisord)
        callback = interface.stopProcessGroup('foo')
        self.assertEqual(interface.update_text, 'stopProcessGroup')
        from supervisor import http
        value = http.NOT_DONE_YET
        while 1:
            value = callback()
            if value is not http.NOT_DONE_YET:
                break

        self.assertEqual(value, [
            {'status':80,'group':'foo','name': 'process1','description': 'OK'},
            {'status':80,'group':'foo','name': 'process2','description': 'OK'},
            ] )
        process1 = supervisord.process_groups['foo'].processes['process1']
        self.assertEqual(process1.stop_called, True)
        process2 = supervisord.process_groups['foo'].processes['process2']
        self.assertEqual(process2.stop_called, True)

    def test_stopProcessGroup_nowait(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', __file__)
        pconfig2 = DummyPConfig(options, 'process2', __file__)
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1,
                                               pconfig2)
        from supervisor.process import ProcessStates
        supervisord.set_procattr('process1', 'state', ProcessStates.RUNNING)
        supervisord.set_procattr('process2', 'state', ProcessStates.RUNNING)
        interface = self._makeOne(supervisord)
        callback = interface.stopProcessGroup('foo', wait=False)
        from supervisor.http import NOT_DONE_YET
        from supervisor.xmlrpc import Faults

        # create callbacks in killall()
        self.assertEqual(callback(), NOT_DONE_YET)

        # get a result
        result = callback()

        self.assertEqual(len(result), 2)
        self.assertEqual(result[0]['name'], 'process1')
        self.assertEqual(result[0]['group'], 'foo')
        self.assertEqual(result[0]['status'],  Faults.SUCCESS)
        self.assertEqual(result[0]['description'], 'OK')

        self.assertEqual(result[1]['name'], 'process2')
        self.assertEqual(result[1]['group'], 'foo')
        self.assertEqual(result[1]['status'],  Faults.SUCCESS)
        self.assertEqual(result[1]['description'], 'OK')

    def test_stopProcessGroup_badname(self):
        from supervisor import xmlrpc
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.BAD_NAME,
                             interface.stopProcessGroup, 'foo')

    def test_stopProcess_splat_calls_stopProcessGroup(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', __file__, autostart=False,
                               startsecs=.01)
        pconfig2 = DummyPConfig(options, 'process2', __file__, priority=2,
                                startsecs=.01)
        supervisord = PopulatedDummySupervisor(options, 'foo',
                                               pconfig1, pconfig2)
        from supervisor.process import ProcessStates
        supervisord.set_procattr('process1', 'state', ProcessStates.STOPPED)
        supervisord.set_procattr('process2', 'state', ProcessStates.STOPPED)
        interface = self._makeOne(supervisord)
        callback = interface.stopProcess('foo:*')
        self.assertEqual(interface.update_text, 'stopProcessGroup')

    def test_stopAllProcesses(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', '/bin/foo')
        pconfig2 = DummyPConfig(options, 'process2', '/bin/foo2')
        from supervisor.process import ProcessStates
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1,
                                               pconfig2)
        supervisord.set_procattr('process1', 'state', ProcessStates.RUNNING)
        supervisord.set_procattr('process2', 'state', ProcessStates.RUNNING)
        interface = self._makeOne(supervisord)
        callback = interface.stopAllProcesses()
        self.assertEqual(interface.update_text, 'stopAllProcesses')
        from supervisor import http
        value = http.NOT_DONE_YET
        while 1:
            value = callback()
            if value is not http.NOT_DONE_YET:
                break

        self.assertEqual(value, [
            {'status':80,'group':'foo','name': 'process1','description': 'OK'},
            {'status':80,'group':'foo','name': 'process2','description': 'OK'},
            ] )
        process1 = supervisord.process_groups['foo'].processes['process1']
        self.assertEqual(process1.stop_called, True)
        process2 = supervisord.process_groups['foo'].processes['process2']
        self.assertEqual(process2.stop_called, True)

    def test_stopAllProcesses_nowait(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', __file__)
        pconfig2 = DummyPConfig(options, 'process2', __file__)
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1,
                                               pconfig2)
        from supervisor.process import ProcessStates
        supervisord.set_procattr('process1', 'state', ProcessStates.RUNNING)
        supervisord.set_procattr('process2', 'state', ProcessStates.RUNNING)
        interface = self._makeOne(supervisord)
        callback = interface.stopAllProcesses(wait=False)
        from supervisor.http import NOT_DONE_YET
        from supervisor.xmlrpc import Faults

        # create callbacks in killall()
        self.assertEqual(callback(), NOT_DONE_YET)

        # get a result
        result = callback()

        self.assertEqual(len(result), 2)
        self.assertEqual(result[0]['name'], 'process1')
        self.assertEqual(result[0]['group'], 'foo')
        self.assertEqual(result[0]['status'],  Faults.SUCCESS)
        self.assertEqual(result[0]['description'], 'OK')

        self.assertEqual(result[1]['name'], 'process2')
        self.assertEqual(result[1]['group'], 'foo')
        self.assertEqual(result[1]['status'],  Faults.SUCCESS)
        self.assertEqual(result[1]['description'], 'OK')

    def test_getAllConfigInfo(self):
        options = DummyOptions()
        supervisord = DummySupervisor(options, 'foo')

        pconfig1 = DummyPConfig(options, 'process1', __file__)
        pconfig2 = DummyPConfig(options, 'process2', __file__)
        gconfig = DummyPGroupConfig(options, 'group1', pconfigs=[pconfig1, pconfig2])
        supervisord.process_groups = {'group1': DummyProcessGroup(gconfig)}
        supervisord.options.process_group_configs = [gconfig]

        interface = self._makeOne(supervisord)
        configs = interface.getAllConfigInfo()
        self.assertEqual(configs, [{ 'group': 'group1',
                                     'name': 'process1',
                                     'inuse': True,
                                     'autostart': True,
                                     'process_prio': 999,
                                     'group_prio': 999 },
                                   { 'group': 'group1',
                                     'name': 'process2',
                                     'inuse': True,
                                     'autostart': True,
                                     'process_prio': 999,
                                     'group_prio': 999 }])

    def test__interpretProcessInfo(self):
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        start = _NOW -100
        stop  = _NOW -1
        from supervisor.process import ProcessStates
        running = {'name':'running',
                   'pid':1,
                   'state':ProcessStates.RUNNING,
                   'start':start,
                   'stop':stop,
                   'now':_NOW}
        
        description = interface._interpretProcessInfo(running)
        self.assertEqual(description, 'pid 1, uptime 0:01:40')

        fatal = {'name':'fatal',
                 'pid':2,
                 'state':ProcessStates.FATAL,
                 'start':start,
                 'stop':stop,
                 'now':_NOW,
                 'spawnerr':'Hosed'}
                 
        description = interface._interpretProcessInfo(fatal)
        self.assertEqual(description, 'Hosed')

        fatal2 = {'name':'fatal',
                  'pid':2,
                  'state':ProcessStates.FATAL,
                  'start':start,
                  'stop':stop,
                  'now':_NOW,
                  'spawnerr':'',}
                 
        description = interface._interpretProcessInfo(fatal2)
        self.assertEqual(description, 'unknown error (try "tail fatal")')
        
        stopped = {'name':'stopped',
                   'pid':3,
                   'state':ProcessStates.STOPPED,
                   'start':start,
                   'stop':stop,
                   'now':_NOW,
                   'spawnerr':'',}

        description = interface._interpretProcessInfo(stopped)
        from datetime import datetime
        stoptime = datetime(*time.localtime(stop)[:7])
        self.assertEqual(description, stoptime.strftime(_TIMEFORMAT))
        
        stopped2 = {'name':'stopped',
                   'pid':3,
                   'state':ProcessStates.STOPPED,
                   'start':0,
                   'stop':stop,
                   'now':_NOW,
                   'spawnerr':'',}

        description = interface._interpretProcessInfo(stopped2)
        self.assertEqual(description, 'Not started')
                   

    def test_getProcessInfo(self):
        from supervisor.process import ProcessStates

        options = DummyOptions()
        config = DummyPConfig(options, 'foo', '/bin/foo',
                              stdout_logfile='/tmp/fleeb.bar')
        process = DummyProcess(config)
        process.pid = 111
        process.laststart = 10
        process.laststop = 11
        pgroup_config = DummyPGroupConfig(options, name='foo')
        pgroup = DummyProcessGroup(pgroup_config)
        pgroup.processes = {'foo':process}
        supervisord = DummySupervisor(process_groups={'foo':pgroup})
        interface = self._makeOne(supervisord)
        data = interface.getProcessInfo('foo')

        self.assertEqual(interface.update_text, 'getProcessInfo')
        self.assertEqual(data['logfile'], '/tmp/fleeb.bar')
        self.assertEqual(data['stdout_logfile'], '/tmp/fleeb.bar')
        self.assertEqual(data['stderr_logfile'], '')
        self.assertEqual(data['name'], 'foo')
        self.assertEqual(data['pid'], 111)
        self.assertEqual(data['start'], 10)
        self.assertEqual(data['stop'], 11)
        self.assertEqual(data['state'], ProcessStates.RUNNING)
        self.assertEqual(data['statename'], 'RUNNING')
        self.assertEqual(data['exitstatus'], 0)
        self.assertEqual(data['spawnerr'], '')
        self.failUnless(data['description'].startswith('pid 111'))

    def test_getProcessInfo_logfile_NONE(self):
        options = DummyOptions()
        config = DummyPConfig(options, 'foo', '/bin/foo',
                              stdout_logfile=None)
        process = DummyProcess(config)
        process.pid = 111
        process.laststart = 10
        process.laststop = 11
        pgroup_config = DummyPGroupConfig(options, name='foo')
        pgroup = DummyProcessGroup(pgroup_config)
        pgroup.processes = {'foo':process}
        supervisord = DummySupervisor(process_groups={'foo':pgroup})
        interface = self._makeOne(supervisord)
        data = interface.getProcessInfo('foo')

        self.assertEqual(data['logfile'], '')
        self.assertEqual(data['stdout_logfile'], '')

    def test_getAllProcessInfo(self):
        from supervisor.process import ProcessStates
        options = DummyOptions()

        p1config = DummyPConfig(options, 'process1', '/bin/process1',
                                priority=1,
                                stdout_logfile='/tmp/process1.log')

        p2config = DummyPConfig(options, 'process2', '/bin/process2',
                                priority=2,
                                stdout_logfile='/tmp/process2.log')

        supervisord = PopulatedDummySupervisor(options, 'gname', p1config,
                                               p2config)
        supervisord.set_procattr('process1', 'pid', 111)
        supervisord.set_procattr('process1', 'laststart', 10)
        supervisord.set_procattr('process1', 'laststop', 11)
        supervisord.set_procattr('process1', 'state', ProcessStates.RUNNING)

        supervisord.set_procattr('process2', 'pid', 0)
        supervisord.set_procattr('process2', 'laststart', 20)
        supervisord.set_procattr('process2', 'laststop', 11)
        supervisord.set_procattr('process2', 'state', ProcessStates.STOPPED)

        interface = self._makeOne(supervisord)

        info = interface.getAllProcessInfo()

        self.assertEqual(interface.update_text, 'getProcessInfo')
        self.assertEqual(len(info), 2)

        p1info = info[0]
        self.assertEqual(p1info['logfile'], '/tmp/process1.log')
        self.assertEqual(p1info['stdout_logfile'], '/tmp/process1.log')
        self.assertEqual(p1info['stderr_logfile'], '')
        self.assertEqual(p1info['name'], 'process1')
        self.assertEqual(p1info['pid'], 111)
        self.assertEqual(p1info['start'], 10)
        self.assertEqual(p1info['stop'], 11)
        self.assertEqual(p1info['state'], ProcessStates.RUNNING)
        self.assertEqual(p1info['statename'], 'RUNNING')
        self.assertEqual(p1info['exitstatus'], 0)
        self.assertEqual(p1info['spawnerr'], '')
        self.assertEqual(p1info['group'], 'gname')
        self.failUnless(p1info['description'].startswith('pid 111'))

        p2info = info[1]
        process2 = supervisord.process_groups['gname'].processes['process2']
        self.assertEqual(p2info['logfile'], '/tmp/process2.log')
        self.assertEqual(p2info['stdout_logfile'], '/tmp/process2.log')
        self.assertEqual(p1info['stderr_logfile'], '')
        self.assertEqual(p2info['name'], 'process2')
        self.assertEqual(p2info['pid'], 0)
        self.assertEqual(p2info['start'], process2.laststart)
        self.assertEqual(p2info['stop'], 11)
        self.assertEqual(p2info['state'], ProcessStates.STOPPED)
        self.assertEqual(p2info['statename'], 'STOPPED')
        self.assertEqual(p2info['exitstatus'], 0)
        self.assertEqual(p2info['spawnerr'], '')
        self.assertEqual(p1info['group'], 'gname')
        
        from datetime import datetime
        starttime = datetime(*time.localtime(process2.laststart)[:7])
        self.assertEqual(p2info['description'], 
                            starttime.strftime(_TIMEFORMAT))

    def test_readProcessStdoutLog_unreadable(self):
        from supervisor import xmlrpc
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'process1', '/bin/process1', priority=1,
                               stdout_logfile='/tmp/process1.log')
        supervisord = PopulatedDummySupervisor(options, 'process1', pconfig)
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.NO_FILE,
                             interface.readProcessStdoutLog,
                             'process1', offset=0, length=1)

    def test_readProcessStdoutLog_badargs(self):
        from supervisor import xmlrpc
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'process1', '/bin/process1', priority=1,
                              stdout_logfile='/tmp/process1.log')
        supervisord = PopulatedDummySupervisor(options, 'process1', pconfig)
        interface = self._makeOne(supervisord)
        import os
        process = supervisord.process_groups['process1'].processes['process1']
        logfile = process.config.stdout_logfile

        try:
            f = open(logfile, 'w+')
            f.write('x' * 2048)
            f.close()
            self._assertRPCError(xmlrpc.Faults.BAD_ARGUMENTS,
                                 interface.readProcessStdoutLog,
                                 'process1', offset=-1, length=1)
            self._assertRPCError(xmlrpc.Faults.BAD_ARGUMENTS,
                                 interface.readProcessStdoutLog, 'process1',
                                 offset=-1, length=-1)
        finally:
            os.remove(logfile)

    def test_readProcessStdoutLog(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',
                              stdout_logfile='/tmp/fooooooo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        process = supervisord.process_groups['foo'].processes['foo']
        logfile = process.config.stdout_logfile
        import os
        try:
            f = open(logfile, 'w+')
            f.write('x' * 2048)
            f.write('y' * 2048)
            f.close()
            data = interface.readProcessStdoutLog('foo', offset=0, length=0)
            self.assertEqual(interface.update_text, 'readProcessStdoutLog')
            self.assertEqual(data, ('x' * 2048) + ('y' * 2048))
            data = interface.readProcessStdoutLog('foo', offset=2048, length=0)
            self.assertEqual(data, 'y' * 2048)
            data = interface.readProcessStdoutLog('foo', offset=0, length=2048)
            self.assertEqual(data, 'x' * 2048)
            data = interface.readProcessStdoutLog('foo', offset=-4, length=0)
            self.assertEqual(data, 'y' * 4)
        finally:
            os.remove(logfile)

    def test_readProcessLogAliasedTo_readProcessStdoutLog(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        self.assertEqual(interface.readProcessLog,
                         interface.readProcessStdoutLog)

    def test_readProcessStderrLog_unreadable(self):
        from supervisor import xmlrpc
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'process1', '/bin/process1', priority=1,
                               stderr_logfile='/tmp/process1.log')
        supervisord = PopulatedDummySupervisor(options, 'process1', pconfig)
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.NO_FILE,
                             interface.readProcessStderrLog,
                             'process1', offset=0, length=1)

    def test_readProcessStdoutLog_badargs(self):
        from supervisor import xmlrpc
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'process1', '/bin/process1', priority=1,
                              stderr_logfile='/tmp/process1.log')
        supervisord = PopulatedDummySupervisor(options, 'process1', pconfig)
        interface = self._makeOne(supervisord)
        import os
        process = supervisord.process_groups['process1'].processes['process1']
        logfile = process.config.stderr_logfile

        try:
            f = open(logfile, 'w+')
            f.write('x' * 2048)
            f.close()
            self._assertRPCError(xmlrpc.Faults.BAD_ARGUMENTS,
                                 interface.readProcessStderrLog,
                                 'process1', offset=-1, length=1)
            self._assertRPCError(xmlrpc.Faults.BAD_ARGUMENTS,
                                 interface.readProcessStderrLog, 'process1',
                                 offset=-1, length=-1)
        finally:
            os.remove(logfile)

    def test_readProcessStderrLog(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',
                              stderr_logfile='/tmp/fooooooo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        process = supervisord.process_groups['foo'].processes['foo']
        logfile = process.config.stderr_logfile
        import os
        try:
            f = open(logfile, 'w+')
            f.write('x' * 2048)
            f.write('y' * 2048)
            f.close()
            data = interface.readProcessStderrLog('foo', offset=0, length=0)
            self.assertEqual(interface.update_text, 'readProcessStderrLog')
            self.assertEqual(data, ('x' * 2048) + ('y' * 2048))
            data = interface.readProcessStderrLog('foo', offset=2048, length=0)
            self.assertEqual(data, 'y' * 2048)
            data = interface.readProcessStderrLog('foo', offset=0, length=2048)
            self.assertEqual(data, 'x' * 2048)
            data = interface.readProcessStderrLog('foo', offset=-4, length=0)
            self.assertEqual(data, 'y' * 4)
        finally:
            os.remove(logfile)

    def test_tailProcessStdoutLog_bad_name(self):
        from supervisor import xmlrpc
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.BAD_NAME, 
                             interface.tailProcessStdoutLog, 'BAD_NAME', 0, 10)

    def test_tailProcessStdoutLog_all(self):
        # test entire log is returned when offset==0 and logsize < length
        from string import letters
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',
                               stdout_logfile='/tmp/fooooooo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        process = supervisord.process_groups['foo'].processes['foo']
        logfile = process.config.stdout_logfile
        import os
        try:
            f = open(logfile, 'w+')
            f.write(letters)
            f.close()
            
            data, offset, overflow = interface.tailProcessStdoutLog('foo', 
                                                        offset=0, 
                                                        length=len(letters))
            self.assertEqual(interface.update_text, 'tailProcessStdoutLog')
            self.assertEqual(overflow, False)
            self.assertEqual(offset, len(letters))
            self.assertEqual(data, letters)
        finally:
            os.remove(logfile)

    def test_tailProcessStdoutLog_none(self):
        # test nothing is returned when offset <= logsize
        from string import letters
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',
                               stdout_logfile='/tmp/fooooooo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        process = supervisord.process_groups['foo'].processes['foo']
        logfile = process.config.stdout_logfile
        import os
        try:
            f = open(logfile, 'w+')
            f.write(letters)
            f.close()

            # offset==logsize
            data, offset, overflow = interface.tailProcessStdoutLog('foo', 
                                                        offset=len(letters), 
                                                        length=100)
            self.assertEqual(interface.update_text, 'tailProcessStdoutLog')
            self.assertEqual(overflow, False)
            self.assertEqual(offset, len(letters))
            self.assertEqual(data, '')

            # offset > logsize
            data, offset, overflow = interface.tailProcessStdoutLog('foo', 
                                                        offset=len(letters)+5, 
                                                        length=100)
            self.assertEqual(interface.update_text, 'tailProcessStdoutLog')
            self.assertEqual(overflow, False)
            self.assertEqual(offset, len(letters))
            self.assertEqual(data, '')
        finally:
            os.remove(logfile)

    def test_tailProcessStdoutLog_overflow(self):
        # test buffer overflow occurs when logsize > offset+length
        from string import letters
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',
                              stdout_logfile='/tmp/fooooooo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        process = supervisord.process_groups['foo'].processes['foo']
        logfile = process.config.stdout_logfile
        import os
        try:
            f = open(logfile, 'w+')
            f.write(letters)
            f.close()

            data, offset, overflow = interface.tailProcessStdoutLog('foo', 
                                                        offset=0, length=5)
            self.assertEqual(interface.update_text, 'tailProcessStdoutLog')
            self.assertEqual(overflow, True)
            self.assertEqual(offset, len(letters))
            self.assertEqual(data, letters[-5:])
        finally:
            os.remove(logfile)
    
    def test_tailProcessStdoutLog_unreadable(self):
        # test nothing is returned if the log doesn't exist yet
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',
                               stdout_logfile='/tmp/fooooooo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        process = supervisord.process_groups['foo'].processes['foo']
        logfile = process.config.stdout_logfile
                
        data, offset, overflow = interface.tailProcessStdoutLog('foo', 
                                                    offset=0, length=100)
        self.assertEqual(interface.update_text, 'tailProcessStdoutLog')
        self.assertEqual(overflow, False)
        self.assertEqual(offset, 0)
        self.assertEqual(data, '')

    def test_tailProcessLogAliasedTo_tailProcessStdoutLog(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        self.assertEqual(interface.tailProcessLog,
                         interface.tailProcessStdoutLog)

    def test_tailProcessStderrLog_bad_name(self):
        from supervisor import xmlrpc
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.BAD_NAME, 
                             interface.tailProcessStderrLog, 'BAD_NAME', 0, 10)

    def test_tailProcessStderrLog_all(self):
        # test entire log is returned when offset==0 and logsize < length
        from string import letters
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',
                               stderr_logfile='/tmp/fooooooo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        process = supervisord.process_groups['foo'].processes['foo']
        logfile = process.config.stderr_logfile
        import os
        try:
            f = open(logfile, 'w+')
            f.write(letters)
            f.close()
            
            data, offset, overflow = interface.tailProcessStderrLog('foo', 
                                                        offset=0, 
                                                        length=len(letters))
            self.assertEqual(interface.update_text, 'tailProcessStderrLog')
            self.assertEqual(overflow, False)
            self.assertEqual(offset, len(letters))
            self.assertEqual(data, letters)
        finally:
            os.remove(logfile)

    def test_tailProcessStderrLog_none(self):
        # test nothing is returned when offset <= logsize
        from string import letters
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',
                               stderr_logfile='/tmp/fooooooo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        process = supervisord.process_groups['foo'].processes['foo']
        logfile = process.config.stderr_logfile
        import os
        try:
            f = open(logfile, 'w+')
            f.write(letters)
            f.close()

            # offset==logsize
            data, offset, overflow = interface.tailProcessStderrLog('foo', 
                                                        offset=len(letters), 
                                                        length=100)
            self.assertEqual(interface.update_text, 'tailProcessStderrLog')
            self.assertEqual(overflow, False)
            self.assertEqual(offset, len(letters))
            self.assertEqual(data, '')

            # offset > logsize
            data, offset, overflow = interface.tailProcessStderrLog('foo', 
                                                        offset=len(letters)+5, 
                                                        length=100)
            self.assertEqual(interface.update_text, 'tailProcessStderrLog')
            self.assertEqual(overflow, False)
            self.assertEqual(offset, len(letters))
            self.assertEqual(data, '')
        finally:
            os.remove(logfile)

    def test_tailProcessStderrLog_overflow(self):
        # test buffer overflow occurs when logsize > offset+length
        from string import letters
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',
                              stderr_logfile='/tmp/fooooooo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        process = supervisord.process_groups['foo'].processes['foo']
        logfile = process.config.stderr_logfile
        import os
        try:
            f = open(logfile, 'w+')
            f.write(letters)
            f.close()

            data, offset, overflow = interface.tailProcessStderrLog('foo', 
                                                        offset=0, length=5)
            self.assertEqual(interface.update_text, 'tailProcessStderrLog')
            self.assertEqual(overflow, True)
            self.assertEqual(offset, len(letters))
            self.assertEqual(data, letters[-5:])
        finally:
            os.remove(logfile)
    
    def test_tailProcessStderrLog_unreadable(self):
        # test nothing is returned if the log doesn't exist yet
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',
                               stderr_logfile='/tmp/fooooooo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        process = supervisord.process_groups['foo'].processes['foo']
        logfile = process.config.stderr_logfile
                
        data, offset, overflow = interface.tailProcessStderrLog('foo', 
                                                    offset=0, length=100)
        self.assertEqual(interface.update_text, 'tailProcessStderrLog')
        self.assertEqual(overflow, False)
        self.assertEqual(offset, 0)
        self.assertEqual(data, '')

    def test_clearProcessLogs_bad_name(self):
        from supervisor import xmlrpc
        supervisord = DummySupervisor()
        interface = self._makeOne(supervisord)
        self._assertRPCError(xmlrpc.Faults.BAD_NAME,
                             interface.clearProcessLogs,
                             'spew')

    def test_clearProcessLogs(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', 'foo')
        process = DummyProcess(pconfig)
        pgroup = DummyProcessGroup(None)
        pgroup.processes = {'foo': process}
        supervisord = DummySupervisor(process_groups={'foo':pgroup})
        interface = self._makeOne(supervisord)
        interface.clearProcessLogs('foo')
        self.assertEqual(process.logsremoved, True)

    def test_clearProcessLogs_failed(self):
        from supervisor import xmlrpc
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', 'foo')
        process = DummyProcess(pconfig)
        pgroup = DummyProcessGroup(None)
        pgroup.processes = {'foo': process}
        process.error_at_clear = True
        processes = {'foo': process}
        supervisord = DummySupervisor(process_groups={'foo':pgroup})
        interface = self._makeOne(supervisord)
        self.assertRaises(xmlrpc.RPCError, interface.clearProcessLogs, 'foo')
        
    def test_clearProcessLogAliasedTo_clearProcessLogs(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', '/bin/foo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig)
        interface = self._makeOne(supervisord)
        self.assertEqual(interface.clearProcessLog,
                         interface.clearProcessLogs)

    def test_clearAllProcessLogs(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'foo')
        pconfig2 = DummyPConfig(options, 'process2', 'bar')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1,
                                               pconfig2)
        interface = self._makeOne(supervisord)
        callback = interface.clearAllProcessLogs()
        callback()
        results = callback()
        from supervisor import xmlrpc
        self.assertEqual(results[0],
                         {'name':'process1',
                          'group':'foo',
                          'status':xmlrpc.Faults.SUCCESS,
                          'description':'OK'})
        self.assertEqual(results[1],
                         {'name':'process2',
                          'group':'foo',
                          'status':xmlrpc.Faults.SUCCESS,
                          'description':'OK'})
        process1 = supervisord.process_groups['foo'].processes['process1']
        self.assertEqual(process1.logsremoved, True)
        process2 = supervisord.process_groups['foo'].processes['process2']
        self.assertEqual(process2.logsremoved, True)

    def test_clearAllProcessLogs_onefails(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'foo')
        pconfig2 = DummyPConfig(options, 'process2', 'bar')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1,
                                               pconfig2)
        supervisord.set_procattr('process1', 'error_at_clear', True)
        interface = self._makeOne(supervisord)
        callback = interface.clearAllProcessLogs()
        callback()
        results = callback()
        process1 = supervisord.process_groups['foo'].processes['process1']
        self.assertEqual(process1.logsremoved, False)
        process2 = supervisord.process_groups['foo'].processes['process2']
        self.assertEqual(process2.logsremoved, True)
        self.assertEqual(len(results), 2)
        from supervisor import xmlrpc
        self.assertEqual(results[0],
                         {'name':'process1',
                          'group':'foo',
                          'status':xmlrpc.Faults.FAILED,
                          'description':'FAILED: foo:process1'})
        self.assertEqual(results[1],
                         {'name':'process2',
                          'group':'foo',
                          'status':xmlrpc.Faults.SUCCESS,
                          'description':'OK'})

    def test_sendProcessStdin_raises_incorrect_params_when_not_chars(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'foo')
        supervisord = PopulatedDummySupervisor(options, 'foo', pconfig1)
        interface   = self._makeOne(supervisord)
        thing_not_chars = 42
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.INCORRECT_PARAMETERS,
                             interface.sendProcessStdin,
                             'process1', thing_not_chars)
    
    def test_sendProcessStdin_raises_bad_name_when_no_process(self):
        options = DummyOptions()
        supervisord = PopulatedDummySupervisor(options, 'foo')
        interface = self._makeOne(supervisord)
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.BAD_NAME,
                             interface.sendProcessStdin,
                             'nonexistant_process_name', 'chars for stdin')

    def test_sendProcessStdin_raises_not_running_when_not_process_pid(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'foo')
        supervisord = PopulatedDummySupervisor(options, 'process1', pconfig1)
        supervisord.set_procattr('process1', 'pid', 0)
        interface = self._makeOne(supervisord)
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.NOT_RUNNING,
                            interface.sendProcessStdin,
                            'process1', 'chars for stdin')

    def test_sendProcessStdin_raises_not_running_when_killing(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'foo')
        supervisord = PopulatedDummySupervisor(options, 'process1', pconfig1)
        supervisord.set_procattr('process1', 'pid', 42)
        supervisord.set_procattr('process1', 'killing',True)
        interface   = self._makeOne(supervisord)
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.NOT_RUNNING,
                             interface.sendProcessStdin,
                             'process1', 'chars for stdin')
        
    def test_sendProcessStdin_raises_no_file_when_write_raises_epipe(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'foo')
        supervisord = PopulatedDummySupervisor(options, 'process1', pconfig1)
        supervisord.set_procattr('process1', 'pid', 42)
        supervisord.set_procattr('process1', 'killing', False)
        supervisord.set_procattr('process1', 'write_error', errno.EPIPE)
        interface   = self._makeOne(supervisord)
        from supervisor import xmlrpc
        self._assertRPCError(xmlrpc.Faults.NO_FILE,
                             interface.sendProcessStdin,
                             'process1', 'chars for stdin')

    def test_sendProcessStdin_writes_chars_and_returns_true(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'foo')
        supervisord = PopulatedDummySupervisor(options, 'process1', pconfig1)
        supervisord.set_procattr('process1', 'pid', 42)
        interface   = self._makeOne(supervisord)
        chars = 'chars for stdin'
        self.assertTrue(interface.sendProcessStdin('process1', chars))
        self.assertEqual('sendProcessStdin', interface.update_text)
        process1 = supervisord.process_groups['process1'].processes['process1']
        self.assertEqual(process1.stdin_buffer, chars)

    def test_sendProcessStdin_unicode_encoded_to_utf8(self):
        options = DummyOptions()
        pconfig1 = DummyPConfig(options, 'process1', 'foo')
        supervisord = PopulatedDummySupervisor(options, 'process1', pconfig1)
        supervisord.set_procattr('process1', 'pid', 42)
        interface   = self._makeOne(supervisord)
        interface.sendProcessStdin('process1', u'fi\xed')
        process1 = supervisord.process_groups['process1'].processes['process1']
        self.assertEqual(process1.stdin_buffer, 'fi\xc3\xad')

    def test_sendRemoteCommEvent_notifies_subscribers(self):
        options = DummyOptions()
        supervisord = DummySupervisor(options)
        interface = self._makeOne(supervisord)

        from supervisor import events
        L = []
        def callback(event):
            L.append(event)
        
        try:
            events.callbacks[:] = [(events.RemoteCommunicationEvent, callback)]
            result = interface.sendRemoteCommEvent('foo', 'bar')
        finally:
            events.callbacks[:] = []
            events.clear()

        self.assertTrue(result)
        self.assertEqual(len(L), 1)
        event = L[0]                                     
        self.assertEqual(event.type, 'foo')
        self.assertEqual(event.data, 'bar')

    def test_sendRemoteCommEvent_unicode_encoded_to_utf8(self):
        options = DummyOptions()
        supervisord = DummySupervisor(options)
        interface = self._makeOne(supervisord)

        from supervisor import events
        L = []
        def callback(event):
            L.append(event)
        
        try:
            events.callbacks[:] = [(events.RemoteCommunicationEvent, callback)]
            result = interface.sendRemoteCommEvent(u'fi\xed once', u'fi\xed twice')
        finally:
            events.callbacks[:] = []
            events.clear()

        self.assertTrue(result)
        self.assertEqual(len(L), 1)
        event = L[0]                                     
        self.assertEqual(event.type, 'fi\xc3\xad once')
        self.assertEqual(event.data, 'fi\xc3\xad twice')
        

class SystemNamespaceXMLRPCInterfaceTests(TestBase):
    def _getTargetClass(self):
        from supervisor import xmlrpc
        return xmlrpc.SystemNamespaceRPCInterface

    def _makeOne(self):
        from supervisor import rpcinterface
        supervisord = DummySupervisor()
        supervisor = rpcinterface.SupervisorNamespaceRPCInterface(supervisord)
        return self._getTargetClass()(
            [('supervisor', supervisor),
             ]
            )

    def test_ctor(self):
        interface = self._makeOne()
        self.failUnless(interface.namespaces['supervisor'])
        self.failUnless(interface.namespaces['system'])

    def test_listMethods(self):
        interface = self._makeOne()
        methods = interface.listMethods()
        methods.sort()
        keys = interface._listMethods().keys()
        keys.sort()
        self.assertEqual(methods, keys)

    def test_methodSignature(self):
        from supervisor import xmlrpc
        interface = self._makeOne()
        self._assertRPCError(xmlrpc.Faults.SIGNATURE_UNSUPPORTED,
                             interface.methodSignature,
                             ['foo.bar'])
        result = interface.methodSignature('system.methodSignature')
        self.assertEqual(result, ['array', 'string'])

    def test_allMethodDocs(self):
        from supervisor import xmlrpc
        # belt-and-suspenders test for docstring-as-typing parsing correctness
        # and documentation validity vs. implementation
        _RPCTYPES = ['int', 'double', 'string', 'boolean', 'dateTime.iso8601',
                     'base64', 'binary', 'array', 'struct']
        interface = self._makeOne()
        methods = interface._listMethods()
        for k in methods.keys():
            # if a method doesn't have a @return value, an RPCError is raised.
            # Detect that here.
            try:
                interface.methodSignature(k)
            except xmlrpc.RPCError:
                raise AssertionError, ('methodSignature for %s raises '
                                       'RPCError (missing @return doc?)' % k)

            # we want to test that the number of arguments implemented in
            # the function is the same as the number of arguments implied by
            # the doc @params, and that they show up in the same order.
            ns_name, method_name = k.split('.', 1)
            namespace = interface.namespaces[ns_name]
            meth = getattr(namespace, method_name)
            code = meth.func_code
            argnames = code.co_varnames[1:code.co_argcount]
            parsed = xmlrpc.gettags(str(meth.__doc__))

            plines = []
            ptypes = []
            pnames = []
            ptexts = []

            rlines = []
            rtypes = []
            rnames = []
            rtexts = []

            for thing in parsed:
                if thing[1] == 'param': # tag name
                    plines.append(thing[0]) # doc line number
                    ptypes.append(thing[2]) # data type
                    pnames.append(thing[3]) # function name
                    ptexts.append(thing[4])  # description
                elif thing[1] == 'return': # tag name
                    rlines.append(thing[0]) # doc line number
                    rtypes.append(thing[2]) # data type
                    rnames.append(thing[3]) # function name
                    rtexts.append(thing[4])  # description
                elif thing[1] is not None:
                    raise AssertionError(
                        'unknown tag type %s for %s, parsed %s' % (thing[1],
                                                                   k,
                                                                   parsed))
            # param tokens

            if len(argnames) != len(pnames):
                raise AssertionError, ('Incorrect documentation '
                                       '(%s args, %s doc params) in %s'
                                       % (len(argnames), len(pnames), k))
            for docline in plines:
                self.failUnless(type(docline) == int, (docline,
                                                       type(docline),
                                                       k,
                                                       parsed))
            for doctype in ptypes:
                self.failUnless(doctype in _RPCTYPES, doctype)
            for x in range(len(pnames)):
                if pnames[x] != argnames[x]:
                    msg = 'Name wrong: (%s vs. %s in %s)\n%s' % (pnames[x],
                                                                 argnames[x],
                                                                 k,
                                                                 parsed)
                    raise AssertionError, msg
            for doctext in ptexts:
                self.failUnless(type(doctext) == type(''), doctext)

            # result tokens
            
            if len(rlines) > 1:
                raise AssertionError(
                    'Duplicate @return values in docs for %s' % k)
            for docline in rlines:
                self.failUnless(type(docline) == int, (docline,
                                                       type(docline), k))
            for doctype in rtypes:
                self.failUnless(doctype in _RPCTYPES, doctype)
            for docname in rnames:
                self.failUnless(type(docname) == type(''), (docname,
                                                            type(docname),
                                                            k))
            for doctext in rtexts:
                self.failUnless(type(doctext) == type(''), (doctext,
                                                            type(doctext), k))

    def test_multicall_simplevals(self):
        interface = self._makeOne()
        callback = interface.multicall([
            {'methodName':'system.methodHelp', 'params':['system.methodHelp']},
            {'methodName':'system.listMethods', 'params':[]},
            ])
        from supervisor import http
        result = http.NOT_DONE_YET
        while result is http.NOT_DONE_YET:
            result = callback()
        self.assertEqual(result[0], interface.methodHelp('system.methodHelp'))
        self.assertEqual(result[1], interface.listMethods())

    def test_multicall_recursion_guard(self):
        from supervisor import xmlrpc
        interface = self._makeOne()
        callback = interface.multicall([
            {'methodName': 'system.multicall', 'params': []},        
        ])

        from supervisor import http
        result = http.NOT_DONE_YET
        while result is http.NOT_DONE_YET:
            result = callback()
        
        code = xmlrpc.Faults.INCORRECT_PARAMETERS
        desc = xmlrpc.getFaultDescription(code)
        recursion_fault = {'faultCode': code, 'faultString': desc}

        self.assertEqual(result, [recursion_fault])
        
    def test_multicall_nested_callback(self):
        interface = self._makeOne()
        callback = interface.multicall([
            {'methodName':'supervisor.stopAllProcesses'}])
        from supervisor import http
        result = http.NOT_DONE_YET
        while result is http.NOT_DONE_YET:
            result = callback()
        self.assertEqual(result[0], [])

    def test_methodHelp(self):
        from supervisor import xmlrpc
        interface = self._makeOne()
        self._assertRPCError(xmlrpc.Faults.SIGNATURE_UNSUPPORTED,
                             interface.methodHelp,
                             ['foo.bar'])
        help = interface.methodHelp('system.methodHelp')
        self.assertEqual(help, interface.methodHelp.__doc__)

class DummyRPCInterface:
    def hello(self):
        return 'Hello!'

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')


########NEW FILE########
__FILENAME__ = test_socket_manager
"""Test suite for supervisor.socket_manager"""

import sys
import os
import unittest
import socket
import tempfile

from supervisor.tests.base import DummySocketConfig
from supervisor.datatypes import UnixStreamSocketConfig
from supervisor.datatypes import InetStreamSocketConfig

class TestObject:
    
    def __init__(self):
        self.value = 5
    
    def getValue(self):
        return self.value
        
    def setValue(self, val):
        self.value = val

class ProxyTest(unittest.TestCase):
    
    def setUp(self):
        self.on_deleteCalled = False
    
    def _getTargetClass(self):
        from supervisor.socket_manager import Proxy
        return Proxy

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)
    
    def setOnDeleteCalled(self):
        self.on_deleteCalled = True
    
    def test_proxy_getattr(self):
        proxy = self._makeOne(TestObject())
        self.assertEquals(5, proxy.getValue())
        
    def test_on_delete(self):
        proxy = self._makeOne(TestObject(), on_delete=self.setOnDeleteCalled)
        self.assertEquals(5, proxy.getValue())
        proxy = None
        self.assertTrue(self.on_deleteCalled)
        
class ReferenceCounterTest(unittest.TestCase):

    def setUp(self):
        self.running = False

    def start(self):
        self.running = True
        
    def stop(self):
        self.running = False

    def _getTargetClass(self):
        from supervisor.socket_manager import ReferenceCounter
        return ReferenceCounter

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_incr_and_decr(self):
        ctr = self._makeOne(on_zero=self.stop,on_non_zero=self.start)
        self.assertFalse(self.running)
        ctr.increment()
        self.assertTrue(self.running)
        self.assertEquals(1, ctr.get_count())
        ctr.increment()
        self.assertTrue(self.running)
        self.assertEquals(2, ctr.get_count())
        ctr.decrement()
        self.assertTrue(self.running)
        self.assertEquals(1, ctr.get_count())
        ctr.decrement()
        self.assertFalse(self.running)
        self.assertEquals(0, ctr.get_count())
    
    def test_decr_at_zero_raises_error(self):
        ctr = self._makeOne(on_zero=self.stop,on_non_zero=self.start)
        self.assertRaises(Exception, ctr.decrement)
        
class SocketManagerTest(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.socket_manager import SocketManager
        return SocketManager

    def _makeOne(self, *args, **kw):
        return self._getTargetClass()(*args, **kw)

    def test_get_config(self):
        conf = DummySocketConfig(2)
        sock_manager = self._makeOne(conf)
        self.assertEqual(conf, sock_manager.config())

    def test_tcp_w_hostname(self):
        conf = InetStreamSocketConfig('localhost', 51041)
        sock_manager = self._makeOne(conf)
        self.assertEqual(sock_manager.socket_config, conf)
        sock = sock_manager.get_socket()
        self.assertEqual(sock.getsockname(), ('127.0.0.1', 51041))

    def test_tcp_w_ip(self):
        conf = InetStreamSocketConfig('127.0.0.1', 51041)
        sock_manager = self._makeOne(conf)
        self.assertEqual(sock_manager.socket_config, conf)
        sock = sock_manager.get_socket()
        self.assertEqual(sock.getsockname(), ('127.0.0.1', 51041))

    def test_unix(self):
        (tf_fd, tf_name) = tempfile.mkstemp();
        conf = UnixStreamSocketConfig(tf_name)
        sock_manager = self._makeOne(conf)
        self.assertEqual(sock_manager.socket_config, conf)
        sock = sock_manager.get_socket()
        self.assertEqual(sock.getsockname(), tf_name)
        sock = None
        os.close(tf_fd)
        
    def test_socket_lifecycle(self):
        conf = DummySocketConfig(2)
        sock_manager = self._makeOne(conf)
        #Assert that sockets are created on demand
        self.assertFalse(sock_manager.is_prepared())
        #Get two socket references
        sock = sock_manager.get_socket()
        self.assertTrue(sock_manager.is_prepared()) #socket created on demand
        sock_id = id(sock._get())
        sock2 = sock_manager.get_socket()
        sock2_id = id(sock2._get())
        #Assert that they are not the same proxy object
        self.assertNotEqual(sock, sock2)
        #Assert that they are the same underlying socket
        self.assertEqual(sock_id, sock2_id)
        #Socket not actually closed yet b/c ref ct is 2
        self.assertTrue(sock_manager.is_prepared())
        self.assertFalse(sock_manager.socket.close_called)
        sock = None
        #Socket not actually closed yet b/c ref ct is 1
        self.assertTrue(sock_manager.is_prepared())
        self.assertFalse(sock_manager.socket.close_called)
        sock2 = None
        #Socket closed
        self.assertFalse(sock_manager.is_prepared())
        self.assertTrue(sock_manager.socket.close_called)
        
        #Get a new socket reference
        sock3 = sock_manager.get_socket()
        self.assertTrue(sock_manager.is_prepared())
        sock3_id = id(sock3._get())
        #Assert that it is not the same socket
        self.assertNotEqual(sock_id, sock3_id)
        #Drop ref ct to zero
        del sock3
        #Now assert that socket is closed
        self.assertFalse(sock_manager.is_prepared())
        self.assertTrue(sock_manager.socket.close_called)

    def test_prepare_socket(self):
        conf = DummySocketConfig(1)
        sock_manager = self._makeOne(conf)
        sock = sock_manager.get_socket()
        self.assertTrue(sock_manager.is_prepared())
        self.assertFalse(sock.bind_called)
        self.assertTrue(sock.listen_called)
        self.assertEqual(sock.listen_backlog, socket.SOMAXCONN)
        self.assertFalse(sock.close_called)
    
    def test_tcp_socket_already_taken(self):
        conf = InetStreamSocketConfig('127.0.0.1', 51041)
        sock_manager = self._makeOne(conf)
        sock = sock_manager.get_socket()
        sock_manager2 = self._makeOne(conf)
        self.assertRaises(socket.error, sock_manager2.get_socket)
        sock = None
        
    def test_unix_bad_sock(self):
        conf = UnixStreamSocketConfig('/notthere/foo.sock')
        sock_manager = self._makeOne(conf)
        self.assertRaises(socket.error, sock_manager.get_socket)        

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = test_states
"""Test suite for supervisor.states"""

import sys
import unittest
from StringIO import StringIO
from supervisor import states

class TopLevelProcessStateTests(unittest.TestCase):
    def test_module_has_process_states(self):
        self.assertTrue(hasattr(states, 'ProcessStates'))
    
    def test_stopped_states_do_not_overlap_with_running_states(self):
        for state in states.STOPPED_STATES:
            self.assertFalse(state in states.RUNNING_STATES)

    def test_running_states_do_not_overlap_with_stopped_states(self):
        for state in states.RUNNING_STATES:
            self.assertFalse(state in states.STOPPED_STATES)

    def test_getProcessStateDescription_returns_string_when_found(self):
        state = states.ProcessStates.STARTING
        self.assertEqual(states.getProcessStateDescription(state),
            'STARTING')

    def test_getProcessStateDescription_returns_None_when_not_found(self):
        self.assertEqual(states.getProcessStateDescription(3.14159),
            None)

class TopLevelSupervisorStateTests(unittest.TestCase):
    def test_module_has_supervisor_states(self):
        self.assertTrue(hasattr(states, 'SupervisorStates'))

    def test_getSupervisorStateDescription_returns_string_when_found(self):
        state = states.SupervisorStates.RUNNING
        self.assertEqual(states.getSupervisorStateDescription(state),
            'RUNNING')

    def test_getSupervisorStateDescription_returns_None_when_not_found(self):
        self.assertEqual(states.getSupervisorStateDescription(3.14159),
            None)

class TopLevelEventListenerStateTests(unittest.TestCase):
    def test_module_has_eventlistener_states(self):
        self.assertTrue(hasattr(states, 'EventListenerStates'))

    def test_getEventListenerStateDescription_returns_string_when_found(self):
        state = states.EventListenerStates.ACKNOWLEDGED
        self.assertEqual(states.getEventListenerStateDescription(state),
            'ACKNOWLEDGED')

    def test_getEventListenerStateDescription_returns_None_when_not_found(self):
        self.assertEqual(states.getEventListenerStateDescription(3.14159),
            None)
    

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = test_supervisorctl
import sys
import unittest
from StringIO import StringIO

from supervisor.tests.base import DummyRPCServer

class ControllerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.supervisorctl import Controller
        return Controller

    def _makeOne(self, options):
        return self._getTargetClass()(options)

    def test_ctor(self):
        options = DummyClientOptions()
        controller = self._makeOne(options)
        self.assertEqual(controller.prompt, options.prompt + '> ')

    def test__upcheck(self):
        options = DummyClientOptions()
        controller = self._makeOne(options)
        result = controller.upcheck()
        self.assertEqual(result, True)

    def test__upcheck_wrong_server_version(self):
        options = DummyClientOptions()
        options._server.supervisor.getVersion = lambda *x: '1.0'
        controller = self._makeOne(options)
        controller.stdout = StringIO()
        result = controller.upcheck()
        self.assertEqual(result, False)
        value = controller.stdout.getvalue()
        self.assertEqual(value, 'Sorry, this version of supervisorctl expects '
        'to talk to a server with API version 3.0, but the remote version is '
        '1.0.\n')

    def test__upcheck_unknown_method(self):
        options = DummyClientOptions()
        from xmlrpclib import Fault
        from supervisor.xmlrpc import Faults
        def getVersion():
            raise Fault(Faults.UNKNOWN_METHOD, 'duh')
        options._server.supervisor.getVersion = getVersion
        controller = self._makeOne(options)
        controller.stdout = StringIO()
        result = controller.upcheck()
        self.assertEqual(result, False)
        value = controller.stdout.getvalue()
        self.assertEqual(value, 'Sorry, supervisord responded but did not '
        'recognize the supervisor namespace commands that supervisorctl '
        'uses to control it.  Please check that the '
        '[rpcinterface:supervisor] section is enabled in the '
        'configuration file (see sample.conf).\n')

    def test__upcheck_catches_socket_error_ECONNREFUSED(self):
        options = DummyClientOptions()
        import socket
        import errno
        def raise_fault(*arg, **kw):     
            raise socket.error(errno.ECONNREFUSED, 'nobody home')
        options._server.supervisor.getVersion = raise_fault

        controller = self._makeOne(options)
        controller.stdout = StringIO()

        result = controller.upcheck()
        self.assertEqual(result, False)

        output = controller.stdout.getvalue() 
        self.assertTrue('refused connection' in output)

    def test__upcheck_catches_socket_error_ENOENT(self):
        options = DummyClientOptions()
        import socket
        import errno
        def raise_fault(*arg, **kw):     
            raise socket.error(errno.ENOENT, 'nobody home')
        options._server.supervisor.getVersion = raise_fault

        controller = self._makeOne(options)
        controller.stdout = StringIO()

        result = controller.upcheck()
        self.assertEqual(result, False)

        output = controller.stdout.getvalue() 
        self.assertTrue('no such file' in output)

    def test_onecmd(self):
        options = DummyClientOptions()
        controller = self._makeOne(options)
        controller.stdout = StringIO()
        plugin = DummyPlugin()
        controller.options.plugins = (plugin,)
        result = controller.onecmd('help')
        self.assertEqual(result, None)
        self.assertEqual(plugin.helped, True)

    def test_onecmd_multi_colonseparated(self):
        options = DummyClientOptions()
        controller = self._makeOne(options)
        controller.stdout = StringIO()
        plugin = DummyPlugin()
        controller.options.plugins = (plugin,)
        result = controller.onecmd('help; help')
        self.assertEqual(result, None)
        self.assertEqual(controller.cmdqueue, [' help'])
        self.assertEqual(plugin.helped, True)

    def test_completionmatches(self):
        options = DummyClientOptions()
        controller = self._makeOne(options)
        controller.stdout=StringIO()
        plugin = DummyPlugin()
        controller.options.plugin=(plugin,)
        for i in ['add','remove']:
            result = controller.completionmatches('',i+' ',1)
            self.assertEqual(result,['foo ','bar ','baz '])
        result = controller.completionmatches('','fg baz:')
        self.assertEqual(result,['baz_01 '])

    def test_nohelp(self):
        options = DummyClientOptions()
        controller = self._makeOne(options)
        self.assertEqual(controller.nohelp, '*** No help on %s')
        
    def test_do_help(self):
        options = DummyClientOptions()
        controller = self._makeOne(options)
        controller.stdout = StringIO()
        results = controller.do_help(None)
        helpval = controller.stdout.getvalue()
        self.assertEqual(results, None)
        self.assertEqual(helpval, 'foo helped')

    def test_get_supervisor_returns_serverproxy_supervisor_namespace(self):
        options = DummyClientOptions()
        controller = self._makeOne(options)

        proxy = controller.get_supervisor()
        expected = options.getServerProxy().supervisor
        self.assertEqual(proxy, expected)

    def test_get_server_proxy_with_no_args_returns_serverproxy(self):
        options = DummyClientOptions()
        controller = self._makeOne(options)

        proxy = controller.get_server_proxy()
        expected = options.getServerProxy()
        self.assertEqual(proxy, expected)

    def test_get_server_proxy_with_namespace_returns_that_namespace(self):
        options = DummyClientOptions()
        controller = self._makeOne(options)

        proxy = controller.get_server_proxy('system')
        expected = options.getServerProxy().system
        self.assertEqual(proxy, expected)


class TestControllerPluginBase(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.supervisorctl import ControllerPluginBase
        return ControllerPluginBase

    def _makeOne(self, *arg, **kw):
        klass = self._getTargetClass()
        options = DummyClientOptions()
        ctl = DummyController(options)
        plugin = klass(ctl, *arg, **kw)
        return plugin

    def test_do_help_noarg(self):
        plugin = self._makeOne()
        results = plugin.do_help(None)
        self.assertEqual(plugin.ctl.stdout.getvalue(), '\n')
        self.assertEqual(len(plugin.ctl.topics_printed), 1)
        topics = plugin.ctl.topics_printed[0]
        self.assertEqual(topics[0], 'unnamed commands (type help <topic>):')
        self.assertEqual(topics[1], [])
        self.assertEqual(topics[2], 15) 
        self.assertEqual(topics[3], 80)
        self.assertEqual(results, None)

    def test_do_help_witharg(self):
        plugin = self._makeOne()
        results = plugin.do_help('foo')
        self.assertEqual(plugin.ctl.stdout.getvalue(), 'no help on foo\n')
        self.assertEqual(len(plugin.ctl.topics_printed), 0)
        
class TestDefaultControllerPlugin(unittest.TestCase):

    def _getTargetClass(self):
        from supervisor.supervisorctl import DefaultControllerPlugin
        return DefaultControllerPlugin

    def _makeOne(self, *arg, **kw):
        klass = self._getTargetClass()
        options = DummyClientOptions()
        ctl = DummyController(options)
        plugin = klass(ctl, *arg, **kw)
        return plugin

    def test_tail_toofewargs(self):
        plugin = self._makeOne()
        result = plugin.do_tail('')
        self.assertEqual(result, None)
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(lines[0], 'Error: too few arguments')

    def test_tail_toomanyargs(self):
        plugin = self._makeOne()
        result = plugin.do_tail('one two three four')
        self.assertEqual(result, None)
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(lines[0], 'Error: too many arguments')

    def test_tail_f_noprocname(self):
        plugin = self._makeOne()
        result = plugin.do_tail('-f')
        self.assertEqual(result, None)
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(lines[0], 'Error: tail requires process name')

    def test_tail_defaults(self):
        plugin = self._makeOne()
        result = plugin.do_tail('foo')
        self.assertEqual(result, None)
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(len(lines), 12)
        self.assertEqual(lines[0], 'output line')

    def test_tail_no_file(self):
        plugin = self._makeOne()
        result = plugin.do_tail('NO_FILE')
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(len(lines), 2)
        self.assertEqual(lines[0], 'NO_FILE: ERROR (no log file)')

    def test_tail_failed(self):
        plugin = self._makeOne()
        result = plugin.do_tail('FAILED')
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(len(lines), 2)
        self.assertEqual(lines[0], 'FAILED: ERROR (unknown error reading log)')

    def test_tail_bad_name(self):
        plugin = self._makeOne()
        result = plugin.do_tail('BAD_NAME')
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(len(lines), 2)
        self.assertEqual(lines[0], 'BAD_NAME: ERROR (no such process name)')

    def test_tail_bytesmodifier(self):
        plugin = self._makeOne()
        result = plugin.do_tail('-10 foo')
        self.assertEqual(result, None)
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(len(lines), 3)
        self.assertEqual(lines[0], 'tput line')

    def test_tail_explicit_channel_stdout_nomodifier(self):
        plugin = self._makeOne()
        result = plugin.do_tail('foo stdout')
        self.assertEqual(result, None)
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(len(lines), 12)
        self.assertEqual(lines[0], 'output line')

    def test_tail_explicit_channel_stderr_nomodifier(self):
        plugin = self._makeOne()
        result = plugin.do_tail('foo stderr')
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(len(lines), 12)
        self.assertEqual(lines[0], 'output line')

    def test_tail_explicit_channel_unrecognized(self):
        plugin = self._makeOne()
        result = plugin.do_tail('foo fudge')
        self.assertEqual(result, None)
        value = plugin.ctl.stdout.getvalue().strip()
        self.assertEqual(value, "Error: bad channel 'fudge'")

    def test_status_oneprocess(self):
        plugin = self._makeOne()
        result = plugin.do_status('foo')
        self.assertEqual(result, None)
        value = plugin.ctl.stdout.getvalue().strip()
        self.assertEqual(value.split(None, 2),
                         ['foo', 'RUNNING', 'foo description'])
                         

    def test_status_allprocesses(self):
        plugin = self._makeOne()
        result = plugin.do_status('')
        self.assertEqual(result, None)
        value = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(value[0].split(None, 2),
                         ['foo', 'RUNNING', 'foo description'])
        self.assertEqual(value[1].split(None, 2),
                         ['bar', 'FATAL', 'bar description'])
        self.assertEqual(value[2].split(None, 2),
                         ['baz:baz_01', 'STOPPED', 'baz description'])

    def test_start_fail(self):
        plugin = self._makeOne()
        result = plugin.do_start('')
        self.assertEqual(result, None)
        expected = "Error: start requires a process name"
        self.assertEqual(plugin.ctl.stdout.getvalue().split('\n')[0], expected)

    def test_start_badname(self):
        plugin = self._makeOne()
        result = plugin.do_start('BAD_NAME')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'BAD_NAME: ERROR (no such process)\n')

    def test_start_no_file(self):
        plugin = self._makeOne()
        result = plugin.do_start('NO_FILE')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'NO_FILE: ERROR (no such file)\n')

    def test_start_not_executable(self):
        plugin = self._makeOne()
        result = plugin.do_start('NOT_EXECUTABLE')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'NOT_EXECUTABLE: ERROR (file is not executable)\n')

    def test_start_alreadystarted(self):
        plugin = self._makeOne()
        result = plugin.do_start('ALREADY_STARTED')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'ALREADY_STARTED: ERROR (already started)\n')

    def test_start_spawnerror(self):
        plugin = self._makeOne()
        result = plugin.do_start('SPAWN_ERROR')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'SPAWN_ERROR: ERROR (spawn error)\n')

    def test_start_one_success(self):
        plugin = self._makeOne()
        result = plugin.do_start('foo')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(), 'foo: started\n')

    def test_start_many(self):
        plugin = self._makeOne()
        result = plugin.do_start('foo bar')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'foo: started\nbar: started\n')

    def test_start_group(self):
        plugin = self._makeOne()
        result = plugin.do_start('foo:')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'foo_00: started\nfoo_01: started\n')

    def test_start_all(self):
        plugin = self._makeOne()
        result = plugin.do_start('all')
        self.assertEqual(result, None)

        self.assertEqual(plugin.ctl.stdout.getvalue(),
                'foo: started\nfoo2: started\nfailed: ERROR (spawn error)\n')


    def test_stop_fail(self):
        plugin = self._makeOne()
        result = plugin.do_stop('')
        self.assertEqual(result, None)
        expected = "Error: stop requires a process name"
        self.assertEqual(plugin.ctl.stdout.getvalue().split('\n')[0], expected)

    def test_stop_badname(self):
        plugin = self._makeOne()
        result = plugin.do_stop('BAD_NAME')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'BAD_NAME: ERROR (no such process)\n')

    def test_stop_notrunning(self):
        plugin = self._makeOne()
        result = plugin.do_stop('NOT_RUNNING')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'NOT_RUNNING: ERROR (not running)\n')

    def test_stop_failed(self):
        plugin = self._makeOne()
        result = plugin.do_stop('FAILED')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(), 'FAILED\n')

    def test_stop_one_success(self):
        plugin = self._makeOne()
        result = plugin.do_stop('foo')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(), 'foo: stopped\n')

    def test_stop_many(self):
        plugin = self._makeOne()
        result = plugin.do_stop('foo bar')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'foo: stopped\nbar: stopped\n')

    def test_stop_group(self):
        plugin = self._makeOne()
        result = plugin.do_stop('foo:')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'foo_00: stopped\nfoo_01: stopped\n')

    def test_stop_all(self):
        plugin = self._makeOne()
        result = plugin.do_stop('all')
        self.assertEqual(result, None)

        self.assertEqual(plugin.ctl.stdout.getvalue(),
         'foo: stopped\nfoo2: stopped\nfailed: ERROR (no such process)\n')

    def test_restart_fail(self):
        plugin = self._makeOne()
        result = plugin.do_restart('')
        self.assertEqual(result, None)

        self.assertEqual(plugin.ctl.stdout.getvalue().split('\n')[0],
         'Error: restart requires a process name')

    def test_restart_one(self):
        plugin = self._makeOne()
        result = plugin.do_restart('foo')
        self.assertEqual(result, None)

        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'foo: stopped\nfoo: started\n')

    def test_restart_all(self):
        plugin = self._makeOne()
        result = plugin.do_restart('all')
        self.assertEqual(result, None)

        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         ('foo: stopped\nfoo2: stopped\n'
                          'failed: ERROR (no such process)\n'
                          'foo: started\nfoo2: started\n'
                          'failed: ERROR (spawn error)\n'))

    def test_clear_fail(self):
        plugin = self._makeOne()
        result = plugin.do_clear('')
        self.assertEqual(result, None)
        expected = "Error: clear requires a process name"
        self.assertEqual(plugin.ctl.stdout.getvalue().split('\n')[0], expected)

    def test_clear_badname(self):
        plugin = self._makeOne()
        result = plugin.do_clear('BAD_NAME')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'BAD_NAME: ERROR (no such process)\n')

    def test_clear_one_success(self):
        plugin = self._makeOne()
        result = plugin.do_clear('foo')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(), 'foo: cleared\n')

    def test_clear_many(self):
        plugin = self._makeOne()
        result = plugin.do_clear('foo bar')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'foo: cleared\nbar: cleared\n')

    def test_clear_all(self):
        plugin = self._makeOne()
        result = plugin.do_clear('all')
        self.assertEqual(result, None)

        self.assertEqual(plugin.ctl.stdout.getvalue(),
         'foo: cleared\nfoo2: cleared\nfailed: ERROR (failed)\n')

    def test_open_fail(self):
        plugin = self._makeOne()
        result = plugin.do_open('badname')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'ERROR: url must be http:// or unix://\n')

    def test_open_succeed(self):
        plugin = self._makeOne()
        result = plugin.do_open('http://localhost:9002')
        self.assertEqual(result, None)
        value = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(value[0].split(None, 2),
                         ['foo', 'RUNNING', 'foo description'])
        self.assertEqual(value[1].split(None, 2),
                         ['bar', 'FATAL', 'bar description'])
        self.assertEqual(value[2].split(None, 2),
                         ['baz:baz_01', 'STOPPED', 'baz description'])

    def test_version(self):
        plugin = self._makeOne()
        plugin.do_version(None)
        self.assertEqual(plugin.ctl.stdout.getvalue(), '3000\n')

    def test_reload_fail(self):
        plugin = self._makeOne()
        options = plugin.ctl.options
        options._server.supervisor._restartable = False
        result = plugin.do_reload('')
        self.assertEqual(result, None)
        self.assertEqual(options._server.supervisor._restarted, False)
        
    def test_reload(self):
        plugin = self._makeOne()
        options = plugin.ctl.options
        result = plugin.do_reload('')
        self.assertEqual(result, None)
        self.assertEqual(options._server.supervisor._restarted, True)

    def test_shutdown(self):
        plugin = self._makeOne()
        options = plugin.ctl.options
        result = plugin.do_shutdown('')
        self.assertEqual(result, None)
        self.assertEqual(options._server.supervisor._shutdown, True)
        
    def test_shutdown_catches_xmlrpc_fault_shutdown_state(self):
        plugin = self._makeOne()
        from supervisor import xmlrpc
        import xmlrpclib
        
        def raise_fault(*arg, **kw):     
            raise xmlrpclib.Fault(xmlrpc.Faults.SHUTDOWN_STATE, 'bye')
        plugin.ctl.options._server.supervisor.shutdown = raise_fault

        result = plugin.do_shutdown('')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(), 
                         'ERROR: already shutting down\n')

    def test_shutdown_reraises_other_xmlrpc_faults(self):
        plugin = self._makeOne()
        from supervisor import xmlrpc
        import xmlrpclib
        
        def raise_fault(*arg, **kw):     
            raise xmlrpclib.Fault(xmlrpc.Faults.CANT_REREAD, 'ouch')
        plugin.ctl.options._server.supervisor.shutdown = raise_fault

        self.assertRaises(xmlrpclib.Fault, 
                          plugin.do_shutdown, '')

    def test_shutdown_catches_socket_error_ECONNREFUSED(self):
        plugin = self._makeOne()
        import socket
        import errno
        
        def raise_fault(*arg, **kw):     
            raise socket.error(errno.ECONNREFUSED, 'nobody home')
        plugin.ctl.options._server.supervisor.shutdown = raise_fault

        result = plugin.do_shutdown('')
        self.assertEqual(result, None)

        output = plugin.ctl.stdout.getvalue()
        self.assertTrue('refused connection (already shut down?)' in output)

    def test_shutdown_catches_socket_error_ENOENT(self):
        plugin = self._makeOne()
        import socket
        import errno
        
        def raise_fault(*arg, **kw):     
            raise socket.error(errno.ENOENT, 'no file')
        plugin.ctl.options._server.supervisor.shutdown = raise_fault

        result = plugin.do_shutdown('')
        self.assertEqual(result, None)
        
        output = plugin.ctl.stdout.getvalue() 
        self.assertTrue('no such file (already shut down?)' in output)        

    def test_shutdown_reraises_other_socket_errors(self):
        plugin = self._makeOne()
        import socket
        import errno

        def raise_fault(*arg, **kw):     
            raise socket.error(errno.EPERM, 'denied')
        plugin.ctl.options._server.supervisor.shutdown = raise_fault

        self.assertRaises(socket.error, 
                          plugin.do_shutdown, '')

    def test__formatChanges(self):
        plugin = self._makeOne()
        # Don't explode, plz
        plugin._formatChanges([['added'], ['changed'], ['removed']])
        plugin._formatChanges([[], [], []])

    def test_reread(self):
        plugin = self._makeOne()
        calls = []
        plugin._formatChanges = lambda x: calls.append(x)
        result = plugin.do_reread(None)
        self.assertEqual(result, None)
        self.assertEqual(calls[0], [['added'], ['changed'], ['removed']])

    def test_reread_Fault(self):
        plugin = self._makeOne()
        from supervisor import xmlrpc
        import xmlrpclib
        def raise_fault(*arg, **kw):
            raise xmlrpclib.Fault(xmlrpc.Faults.CANT_REREAD, 'cant')
        plugin.ctl.options._server.supervisor.reloadConfig = raise_fault
        plugin.do_reread(None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'ERROR: cant\n')

    def test__formatConfigInfo(self):
        info = { 'group': 'group1',
                 'name': 'process1',
                 'inuse': True,
                 'autostart': True,
                 'process_prio': 999,
                 'group_prio': 999 }
        plugin = self._makeOne()
        result = plugin._formatConfigInfo(info)
        self.assertTrue('in use' in result)
        info = { 'group': 'group1',
                 'name': 'process1',
                 'inuse': False,
                 'autostart': False,
                 'process_prio': 999,
                 'group_prio': 999 }
        result = plugin._formatConfigInfo(info)
        self.assertTrue('avail' in result)

    def test_avail(self):
        calls = []
        plugin = self._makeOne()

        class FakeSupervisor(object):
            def getAllConfigInfo(self):
                return [{ 'group': 'group1', 'name': 'process1',
                          'inuse': False, 'autostart': False,
                          'process_prio': 999, 'group_prio': 999 }]

        plugin.ctl.get_supervisor = lambda : FakeSupervisor()
        plugin.ctl.output = calls.append
        result = plugin.do_avail('')
        self.assertEqual(result, None)

    def test_add(self):
        plugin = self._makeOne()
        result = plugin.do_add('foo')
        self.assertEqual(result, None)
        supervisor = plugin.ctl.options._server.supervisor
        self.assertEqual(supervisor.processes, ['foo'])

    def test_add_already_added(self):
        plugin = self._makeOne()
        result = plugin.do_add('ALREADY_ADDED')
        self.assertEqual(result, None)
        supervisor = plugin.ctl.options._server.supervisor
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'ERROR: process group already active\n')

    def test_add_bad_name(self):
        plugin = self._makeOne()
        result = plugin.do_add('BAD_NAME')
        self.assertEqual(result, None)
        supervisor = plugin.ctl.options._server.supervisor
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'ERROR: no such process/group: BAD_NAME\n')

    def test_remove(self):
        plugin = self._makeOne()
        supervisor = plugin.ctl.options._server.supervisor
        supervisor.processes = ['foo']
        result = plugin.do_remove('foo')
        self.assertEqual(result, None)
        self.assertEqual(supervisor.processes, [])

    def test_remove_bad_name(self):
        plugin = self._makeOne()
        supervisor = plugin.ctl.options._server.supervisor
        supervisor.processes = ['foo']
        result = plugin.do_remove('BAD_NAME')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'ERROR: no such process/group: BAD_NAME\n')

    def test_remove_still_running(self):
        plugin = self._makeOne()
        supervisor = plugin.ctl.options._server.supervisor
        supervisor.processes = ['foo']
        result = plugin.do_remove('STILL_RUNNING')
        self.assertEqual(result, None)
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'ERROR: process/group still running: STILL_RUNNING\n')

    def test_update_not_on_shutdown(self):
        plugin = self._makeOne()
        supervisor = plugin.ctl.options._server.supervisor
        def reloadConfig():
            from supervisor import xmlrpc
            import xmlrpclib
            raise xmlrpclib.Fault(xmlrpc.Faults.SHUTDOWN_STATE, 'blah')
        supervisor.reloadConfig = reloadConfig
        supervisor.processes = ['removed']
        plugin.do_update('')
        self.assertEqual(supervisor.processes, ['removed'])

    def test_update_added_procs(self):
        plugin = self._makeOne()
        supervisor = plugin.ctl.options._server.supervisor

        calls = []
        def reloadConfig():
            return [[['new_proc'], [], []]]
        supervisor.reloadConfig = reloadConfig

        plugin.do_update('')
        self.assertEqual(supervisor.processes, ['new_proc'])

    def test_update_changed_procs(self):
        from supervisor import xmlrpc
        import xmlrpclib

        plugin = self._makeOne()
        supervisor = plugin.ctl.options._server.supervisor

        calls = []
        def reloadConfig():
            return [[[], ['changed_group'], []]]
        supervisor.reloadConfig = reloadConfig
        supervisor.startProcess = lambda x: calls.append(('start', x))

        supervisor.addProcessGroup('changed_group') # fake existence
        results = [{'name':        'changed_process',
                    'group':       'changed_group',
                    'status':      xmlrpc.Faults.SUCCESS,
                    'description': 'blah'}]
        def stopProcessGroup(name):
            calls.append(('stop', name))
            return results
        supervisor.stopProcessGroup = stopProcessGroup

        plugin.do_update('')
        self.assertEqual(calls, [('stop', 'changed_group')])

        supervisor.addProcessGroup('changed_group') # fake existence
        calls[:] = []
        results[:] = [{'name':        'changed_process1',
                       'group':       'changed_group',
                       'status':      xmlrpc.Faults.NOT_RUNNING,
                       'description': 'blah'},
                      {'name':        'changed_process2',
                       'group':       'changed_group',
                       'status':      xmlrpc.Faults.FAILED,
                       'description': 'blah'}]

        plugin.do_update('')
        self.assertEqual(calls, [('stop', 'changed_group')])

        supervisor.addProcessGroup('changed_group') # fake existence
        calls[:] = []
        results[:] = [{'name':        'changed_process1',
                       'group':       'changed_group',
                       'status':      xmlrpc.Faults.FAILED,
                       'description': 'blah'},
                      {'name':        'changed_process2',
                       'group':       'changed_group',
                       'status':      xmlrpc.Faults.SUCCESS,
                       'description': 'blah'}]

        plugin.do_update('')
        self.assertEqual(calls, [('stop', 'changed_group')])

    def test_update_removed_procs(self):
        from supervisor import xmlrpc

        plugin = self._makeOne()
        supervisor = plugin.ctl.options._server.supervisor

        def reloadConfig():
            return [[[], [], ['removed_group']]]
        supervisor.reloadConfig = reloadConfig

        results = [{'name':        'removed_process',
                    'group':       'removed_group',
                    'status':      xmlrpc.Faults.SUCCESS,
                    'description': 'blah'}]
        supervisor.processes = ['removed_group']

        def stopProcessGroup(name):
            return results
        supervisor.stopProcessGroup = stopProcessGroup

        plugin.do_update('')
        self.assertEqual(supervisor.processes, [])

        results[:] = [{'name':        'removed_process',
                       'group':       'removed_group',
                       'status':      xmlrpc.Faults.NOT_RUNNING,
                       'description': 'blah'}]
        supervisor.processes = ['removed_group']

        plugin.do_update('')
        self.assertEqual(supervisor.processes, [])

        results[:] = [{'name':        'removed_process',
                       'group':       'removed_group',
                       'status':      xmlrpc.Faults.FAILED,
                       'description': 'blah'}]
        supervisor.processes = ['removed_group']

        plugin.do_update('')
        self.assertEqual(supervisor.processes, ['removed_group'])

    def test_pid_supervisord(self):
        plugin = self._makeOne()
        result = plugin.do_pid('')
        options = plugin.ctl.options
        self.assertEqual(result, None)
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(len(lines), 2)
        self.assertEqual(lines[0], str(options._server.supervisor.getPID()))

    def test_pid_allprocesses(self):
        plugin = self._makeOne()
        result = plugin.do_pid('all')
        self.assertEqual(result, None)
        value = plugin.ctl.stdout.getvalue().strip()
        self.assertEqual(value.split(), ['11', '12', '13'])

    def test_pid_badname(self):
        plugin = self._makeOne()
        result = plugin.do_pid('BAD_NAME')
        self.assertEqual(result, None)
        value = plugin.ctl.stdout.getvalue().strip()
        self.assertEqual(value, 'No such process BAD_NAME')

    def test_pid_oneprocess(self):
        plugin = self._makeOne()
        result = plugin.do_pid('foo')
        self.assertEqual(plugin.ctl.stdout.getvalue().strip(), '11')

    def test_maintail_toomanyargs(self):
        plugin = self._makeOne()
        result = plugin.do_maintail('foo bar')
        val = plugin.ctl.stdout.getvalue()
        self.failUnless(val.startswith('Error: too many'), val)

    def test_maintail_minus_string_fails(self):
        plugin = self._makeOne()
        result = plugin.do_maintail('-wrong')
        val = plugin.ctl.stdout.getvalue()
        self.failUnless(val.startswith('Error: bad argument -wrong'), val)

    def test_maintail_wrong(self):
        plugin = self._makeOne()
        result = plugin.do_maintail('wrong')
        val = plugin.ctl.stdout.getvalue()
        self.failUnless(val.startswith('Error: bad argument wrong'), val)

    def test_maintail_dashf(self):
        plugin = self._makeOne()
        plugin.listener = DummyListener()
        result = plugin.do_maintail('-f')
        errors = plugin.listener.errors
        self.assertEqual(len(errors), 1)
        error = errors[0]
        self.assertEqual(plugin.listener.closed,
                         'http://localhost:65532/mainlogtail')
        self.assertEqual(error[0],
                         'http://localhost:65532/mainlogtail')
        for msg in ('Cannot connect', 'socket.error'):
            self.assertTrue(msg in error[1])

    def test_maintail_nobytes(self):
        plugin = self._makeOne()
        result = plugin.do_maintail('')
        self.assertEqual(plugin.ctl.stdout.getvalue(), 'mainlogdata\n')

    def test_maintail_dashbytes(self):
        plugin = self._makeOne()
        result = plugin.do_maintail('-100')
        self.assertEqual(plugin.ctl.stdout.getvalue(), 'mainlogdata\n')

    def test_maintail_readlog_error_nofile(self):
        plugin = self._makeOne()
        supervisor_rpc = plugin.ctl.get_supervisor()
        from supervisor import xmlrpc
        supervisor_rpc._readlog_error = xmlrpc.Faults.NO_FILE
        result = plugin.do_maintail('-100')
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'supervisord: ERROR (no log file)\n')

    def test_maintail_readlog_error_failed(self):
        plugin = self._makeOne()
        supervisor_rpc = plugin.ctl.get_supervisor()
        from supervisor import xmlrpc
        supervisor_rpc._readlog_error = xmlrpc.Faults.FAILED
        result = plugin.do_maintail('-100')
        self.assertEqual(plugin.ctl.stdout.getvalue(),
                         'supervisord: ERROR (unknown error reading log)\n')

    def test_fg_too_few_args(self):
        plugin = self._makeOne()
        result = plugin.do_fg('')
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(result, None)
        self.assertEqual(lines[0], 'Error: no process name supplied')

    def test_fg_too_many_args(self):
        plugin = self._makeOne()
        result = plugin.do_fg('foo bar')
        line = plugin.ctl.stdout.getvalue()
        self.assertEqual(result, None)
        self.assertEqual(line, 'Error: too many process names supplied\n')

    def test_fg_badprocname(self):
        plugin = self._makeOne()
        result = plugin.do_fg('BAD_NAME')
        line = plugin.ctl.stdout.getvalue()
        self.assertEqual(result, None)
        self.assertEqual(line, 'Error: bad process name supplied\n')

    def test_fg_procnotrunning(self):
        plugin = self._makeOne()
        result = plugin.do_fg('bar')
        line = plugin.ctl.stdout.getvalue()
        self.assertEqual(result, None)
        self.assertEqual(line, 'Error: process not running\n')
        result = plugin.do_fg('baz_01')
        lines = plugin.ctl.stdout.getvalue().split('\n')
        self.assertEqual(result, None)
        self.assertEqual(lines[-2], 'Error: process not running')

class DummyListener:
    def __init__(self):
        self.errors = []
    def error(self, url, msg):
        self.errors.append((url, msg))
    def close(self, url):
        self.closed = url

class DummyPluginFactory:
    def __init__(self, ctl, **kw):
        self.ctl = ctl

    def do_help(self, arg):
        self.ctl.stdout.write('foo helped')

class DummyClientOptions:
    def __init__(self):
        self.prompt = 'supervisor'
        self.serverurl = 'http://localhost:65532'
        self.username = 'chrism'
        self.password = '123'
        self.history_file = None
        self.plugins = ()
        self._server = DummyRPCServer()
        self.interactive = False
        self.plugin_factories = [('dummy', DummyPluginFactory, {})]

    def getServerProxy(self):
        return self._server

class DummyController:
    nohelp = 'no help on %s'
    def __init__(self, options):
        self.options = options
        self.topics_printed = []
        self.stdout = StringIO()
        
    def upcheck(self):
        return True

    def get_supervisor(self):
        return self.get_server_proxy('supervisor')

    def get_server_proxy(self, namespace=None):
        proxy = self.options.getServerProxy()
        if namespace is None:
            return proxy
        else:
            return getattr(proxy, namespace)

    def output(self, data):
        self.stdout.write(data + '\n')

    def print_topics(self, doc_headers, cmds_doc, rows, cols):
        self.topics_printed.append((doc_headers, cmds_doc, rows, cols))

class DummyPlugin:
    def __init__(self, controller=None):
        self.ctl = controller
        
    def do_help(self, arg):
        self.helped = True

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')


########NEW FILE########
__FILENAME__ = test_supervisord
import unittest
import time
import signal
import sys
import os
import tempfile
import shutil

from supervisor.tests.base import DummyOptions
from supervisor.tests.base import DummyPConfig
from supervisor.tests.base import DummyPGroupConfig
from supervisor.tests.base import DummyProcess
from supervisor.tests.base import DummyProcessGroup
from supervisor.tests.base import DummyDispatcher

class EntryPointTests(unittest.TestCase):
    def test_main_noprofile(self):
        from supervisor.supervisord import main
        conf = os.path.join(
            os.path.abspath(os.path.dirname(__file__)), 'fixtures',
            'donothing.conf')
        import StringIO
        new_stdout = StringIO.StringIO()
        old_stdout = sys.stdout
        try:
            tempdir = tempfile.mkdtemp()
            log = os.path.join(tempdir, 'log')
            pid = os.path.join(tempdir, 'pid')
            sys.stdout = new_stdout
            main(args=['-c', conf, '-l', log, '-j', pid, '-n'],
                 test=True)
        finally:
            sys.stdout = old_stdout
            shutil.rmtree(tempdir)
        output = new_stdout.getvalue()
        self.failUnless(output.find('supervisord started') != 1, output)

    if sys.version_info[:2] >= (2, 4):
        def test_main_profile(self):
            from supervisor.supervisord import main
            conf = os.path.join(
                os.path.abspath(os.path.dirname(__file__)), 'fixtures',
                'donothing.conf')
            import StringIO
            new_stdout = StringIO.StringIO()
            old_stdout = sys.stdout
            try:
                tempdir = tempfile.mkdtemp()
                log = os.path.join(tempdir, 'log')
                pid = os.path.join(tempdir, 'pid')
                sys.stdout = new_stdout
                main(args=['-c', conf, '-l', log, '-j', pid, '-n',
                           '--profile_options=cumulative,calls'], test=True)
            finally:
                sys.stdout = old_stdout
                shutil.rmtree(tempdir)
            output = new_stdout.getvalue()
            self.failUnless(output.find('cumulative time, call count') != -1,
                            output)

class SupervisordTests(unittest.TestCase):
    def tearDown(self):
        from supervisor.events import clear
        clear()
        
    def _getTargetClass(self):
        from supervisor.supervisord import Supervisor
        return Supervisor

    def _makeOne(self, options):
        return self._getTargetClass()(options)

    def test_main_first(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', 'foo', '/bin/foo')
        gconfigs = [DummyPGroupConfig(options,'foo', pconfigs=[pconfig])]
        options.process_group_configs = gconfigs
        options.test = True
        options.first = True
        supervisord = self._makeOne(options)
        supervisord.main()
        self.assertEqual(options.environment_processed, True)
        self.assertEqual(options.fds_cleaned_up, False)
        self.assertEqual(options.rlimits_set, True)
        self.assertEqual(options.make_logger_messages,
                         (['setuid_called'], [], ['rlimits_set']))
        self.assertEqual(options.autochildlogdir_cleared, True)
        self.assertEqual(len(supervisord.process_groups), 1)
        self.assertEqual(supervisord.process_groups['foo'].config.options,
                         options)
        self.assertEqual(options.environment_processed, True)
        self.assertEqual(options.httpservers_opened, True)
        self.assertEqual(options.signals_set, True)
        self.assertEqual(options.daemonized, True)
        self.assertEqual(options.pidfile_written, True)
        self.assertEqual(options.cleaned_up, True)

    def test_main_notfirst(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', 'foo', '/bin/foo')
        gconfigs = [DummyPGroupConfig(options,'foo', pconfigs=[pconfig])]
        options.process_group_configs = gconfigs
        options.test = True
        options.first = False
        supervisord = self._makeOne(options)
        supervisord.main()
        self.assertEqual(options.environment_processed, True)
        self.assertEqual(options.fds_cleaned_up, True)
        self.failIf(hasattr(options, 'rlimits_set'))
        self.assertEqual(options.make_logger_messages,
                         (['setuid_called'], [], []))
        self.assertEqual(options.autochildlogdir_cleared, True)
        self.assertEqual(len(supervisord.process_groups), 1)
        self.assertEqual(supervisord.process_groups['foo'].config.options,
                         options)
        self.assertEqual(options.environment_processed, True)
        self.assertEqual(options.httpservers_opened, True)
        self.assertEqual(options.signals_set, True)
        self.assertEqual(options.daemonized, False)
        self.assertEqual(options.pidfile_written, True)
        self.assertEqual(options.cleaned_up, True)

    def test_reap(self):
        options = DummyOptions()
        options.waitpid_return = 1, 1
        pconfig = DummyPConfig(options, 'process', 'process', '/bin/process1')
        process = DummyProcess(pconfig)
        process.drained = False
        process.killing = 1
        process.laststop = None
        process.waitstatus = None, None
        options.pidhistory = {1:process}
        supervisord = self._makeOne(options)
        
        supervisord.reap(once=True)
        self.assertEqual(process.finished, (1,1))

    def test_handle_sigterm(self):
        options = DummyOptions()
        options._signal = signal.SIGTERM
        supervisord = self._makeOne(options)
        supervisord.handle_signal()
        self.assertEqual(supervisord.options.mood, -1)
        self.assertEqual(options.logger.data[0],
                         'received SIGTERM indicating exit request')

    def test_handle_sigint(self):
        options = DummyOptions()
        options._signal = signal.SIGINT
        supervisord = self._makeOne(options)
        supervisord.handle_signal()
        self.assertEqual(supervisord.options.mood, -1)
        self.assertEqual(options.logger.data[0],
                         'received SIGINT indicating exit request')

    def test_handle_sigquit(self):
        options = DummyOptions()
        options._signal = signal.SIGQUIT
        supervisord = self._makeOne(options)
        supervisord.handle_signal()
        self.assertEqual(supervisord.options.mood, -1)
        self.assertEqual(options.logger.data[0],
                         'received SIGQUIT indicating exit request')

    def test_handle_sighup(self):
        options = DummyOptions()
        options._signal = signal.SIGHUP
        supervisord = self._makeOne(options)
        supervisord.handle_signal()
        self.assertEqual(supervisord.options.mood, 0)
        self.assertEqual(options.logger.data[0],
                         'received SIGHUP indicating restart request')

    def test_handle_sigusr2(self):
        options = DummyOptions()
        options._signal = signal.SIGUSR2
        pconfig1 = DummyPConfig(options, 'process1', 'process1','/bin/process1')
        from supervisor.process import ProcessStates
        process1 = DummyProcess(pconfig1, state=ProcessStates.STOPPING)
        process1.delay = time.time() - 1
        supervisord = self._makeOne(options)
        pconfigs = [DummyPConfig(options, 'foo', 'foo', '/bin/foo')]
        options.process_group_configs = DummyPGroupConfig(
            options, 'foo',
            pconfigs=pconfigs)
        supervisord.handle_signal()
        self.assertEqual(supervisord.options.mood, 1)
        self.assertEqual(options.logs_reopened, True)
        self.assertEqual(options.logger.data[0],
                         'received SIGUSR2 indicating log reopen request')

    def test_handle_unknown_signal(self):
        options = DummyOptions()
        options._signal = signal.SIGUSR1
        supervisord = self._makeOne(options)
        supervisord.handle_signal()
        self.assertEqual(supervisord.options.mood, 1)
        self.assertEqual(options.logger.data[0],
                         'received SIGUSR1 indicating nothing')

    def test_diff_add_remove(self):
        options = DummyOptions()
        supervisord = self._makeOne(options)

        pconfig = DummyPConfig(options, 'process1', 'process1')
        group1 = DummyPGroupConfig(options, 'group1', pconfigs=[pconfig])

        pconfig = DummyPConfig(options, 'process2', 'process2')
        group2 = DummyPGroupConfig(options, 'group2', pconfigs=[pconfig])

        new = [group1, group2]

        added, changed, removed = supervisord.diff_to_active()
        self.assertEqual(added, [])
        self.assertEqual(changed, [])
        self.assertEqual(removed, [])

        added, changed, removed = supervisord.diff_to_active(new)
        self.assertEqual(added, new)
        self.assertEqual(changed, [])
        self.assertEqual(removed, [])

        supervisord.options.process_group_configs = new
        added, changed, removed = supervisord.diff_to_active()
        self.assertEqual(added, new)

        supervisord.add_process_group(group1)
        supervisord.add_process_group(group2)

        pconfig = DummyPConfig(options, 'process3', 'process3')
        new_group1 = DummyPGroupConfig(options, pconfigs=[pconfig])

        pconfig = DummyPConfig(options, 'process4', 'process4')
        new_group2 = DummyPGroupConfig(options, pconfigs=[pconfig])

        new = [group2, new_group1, new_group2]

        added, changed, removed = supervisord.diff_to_active(new)
        self.assertEqual(added, [new_group1, new_group2])
        self.assertEqual(changed, [])
        self.assertEqual(removed, [group1])

    def test_diff_changed(self):
        from supervisor.options import ProcessConfig, ProcessGroupConfig

        options = DummyOptions()
        supervisord = self._makeOne(options)

        def make_pconfig(name, command, **params):
            result = {
                'name': name, 'command': command,
                'directory': None, 'umask': None, 'priority': 999, 'autostart': True,
                'autorestart': True, 'startsecs': 10, 'startretries': 999,
                'uid': None, 'stdout_logfile': None, 'stdout_capture_maxbytes': 0,
                'stdout_events_enabled': False,
                'stdout_logfile_backups': 0, 'stdout_logfile_maxbytes': 0,
                'stderr_logfile': None, 'stderr_capture_maxbytes': 0,
                'stderr_events_enabled': False,
                'stderr_logfile_backups': 0, 'stderr_logfile_maxbytes': 0,
                'redirect_stderr': False,
                'stopsignal': None, 'stopwaitsecs': 10,
                'stopasgroup': False,
                'killasgroup': False,
                'exitcodes': (0,2), 'environment': None, 'serverurl': None }
            result.update(params)
            return ProcessConfig(options, **result)

        def make_gconfig(name, pconfigs):
            return ProcessGroupConfig(options, name, 25, pconfigs)

        pconfig = make_pconfig('process1', 'process1', uid='new')
        group1 = make_gconfig('group1', [pconfig])

        pconfig = make_pconfig('process2', 'process2')
        group2 = make_gconfig('group2', [pconfig])
        new = [group1, group2]

        pconfig = make_pconfig('process1', 'process1', uid='old')
        group3 = make_gconfig('group1', [pconfig])

        pconfig = make_pconfig('process2', 'process2')
        group4 = make_gconfig('group2', [pconfig])
        supervisord.add_process_group(group3)
        supervisord.add_process_group(group4)

        added, changed, removed = supervisord.diff_to_active(new)

        self.assertEqual([added, removed], [[], []])
        self.assertEqual(changed, [group1])

        options = DummyOptions()
        supervisord = self._makeOne(options)

        pconfig1 = make_pconfig('process1', 'process1')
        pconfig2 = make_pconfig('process2', 'process2')
        group1 = make_gconfig('group1', [pconfig1, pconfig2])
        new = [group1]

        supervisord.add_process_group(make_gconfig('group1', [pconfig1]))

        added, changed, removed = supervisord.diff_to_active(new)
        self.assertEqual([added, removed], [[], []])
        self.assertEqual(changed, [group1])

    def test_add_process_group(self):
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', 'foo', '/bin/foo')
        gconfig = DummyPGroupConfig(options,'foo', pconfigs=[pconfig])
        options.process_group_configs = [gconfig]
        supervisord = self._makeOne(options)

        self.assertEqual(supervisord.process_groups, {})

        result = supervisord.add_process_group(gconfig)
        self.assertEqual(supervisord.process_groups.keys(), ['foo'])
        self.assertTrue(result)

        group = supervisord.process_groups['foo']
        result = supervisord.add_process_group(gconfig)
        self.assertEqual(group, supervisord.process_groups['foo'])
        self.assertTrue(not result)

    def test_remove_process_group(self):
        from supervisor.states import ProcessStates
        options = DummyOptions()
        pconfig = DummyPConfig(options, 'foo', 'foo', '/bin/foo')
        gconfig = DummyPGroupConfig(options, 'foo', pconfigs=[pconfig])
        supervisord = self._makeOne(options)

        self.assertRaises(KeyError, supervisord.remove_process_group, 'asdf')

        supervisord.add_process_group(gconfig)
        result = supervisord.remove_process_group('foo')
        self.assertEqual(supervisord.process_groups, {})
        self.assertTrue(result)

        supervisord.add_process_group(gconfig)
        supervisord.process_groups['foo'].unstopped_processes = [DummyProcess(None)]
        result = supervisord.remove_process_group('foo')
        self.assertEqual(supervisord.process_groups.keys(), ['foo'])
        self.assertTrue(not result)

    def test_runforever_emits_generic_startup_event(self):
        from supervisor import events
        L = []
        def callback(event):
            L.append(1)
        events.subscribe(events.SupervisorStateChangeEvent, callback)
        options = DummyOptions()
        supervisord = self._makeOne(options)
        options.test = True
        supervisord.runforever()
        self.assertEqual(L, [1])

    def test_runforever_emits_generic_specific_event(self):
        from supervisor import events
        L = []
        def callback(event):
            L.append(2)
        events.subscribe(events.SupervisorRunningEvent, callback)
        options = DummyOptions()
        options.test = True
        supervisord = self._makeOne(options)
        supervisord.runforever()
        self.assertEqual(L, [2])

    def test_runforever_calls_tick(self):
        options = DummyOptions()
        options.test = True
        supervisord = self._makeOne(options)
        self.assertEqual(len(supervisord.ticks), 0)
        supervisord.runforever()
        self.assertEqual(len(supervisord.ticks), 3)

    def test_runforever_select_eintr(self):
        options = DummyOptions()
        import errno
        options.select_error = errno.EINTR
        supervisord = self._makeOne(options)
        options.test = True
        supervisord.runforever()
        self.assertEqual(options.logger.data[0], 'EINTR encountered in select')

    def test_runforever_select_uncaught_exception(self):
        options = DummyOptions()
        import errno
        options.select_error = errno.EBADF
        supervisord = self._makeOne(options)
        import select
        options.test = True
        self.assertRaises(select.error, supervisord.runforever)

    def test_runforever_select_dispatchers(self):
        options = DummyOptions()
        supervisord = self._makeOne(options)
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',)
        process = DummyProcess(pconfig)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig])
        pgroup = DummyProcessGroup(gconfig)
        readable = DummyDispatcher(readable=True)
        writable = DummyDispatcher(writable=True)
        error = DummyDispatcher(writable=True, error=OSError)
        pgroup.dispatchers = {6:readable, 7:writable, 8:error}
        supervisord.process_groups = {'foo': pgroup}
        options.select_result = [6], [7, 8], []
        options.test = True
        supervisord.runforever()
        self.assertEqual(pgroup.transitioned, True)
        self.assertEqual(readable.read_event_handled, True)
        self.assertEqual(writable.write_event_handled, True)
        self.assertEqual(error.error_handled, True)

    def test_runforever_select_dispatcher_exitnow(self):
        options = DummyOptions()
        supervisord = self._makeOne(options)
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',)
        process = DummyProcess(pconfig)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig])
        pgroup = DummyProcessGroup(gconfig)
        from supervisor.medusa import asyncore_25 as asyncore
        exitnow = DummyDispatcher(readable=True, error=asyncore.ExitNow)
        pgroup.dispatchers = {6:exitnow}
        supervisord.process_groups = {'foo': pgroup}
        options.select_result = [6], [], []
        options.test = True
        self.assertRaises(asyncore.ExitNow, supervisord.runforever)

    def test_runforever_stopping_emits_events(self):
        options = DummyOptions()
        supervisord = self._makeOne(options)
        gconfig = DummyPGroupConfig(options)
        pgroup = DummyProcessGroup(gconfig)
        supervisord.process_groups = {'foo': pgroup}
        supervisord.options.mood = -1
        L = []
        def callback(event):
            L.append(event)
        from supervisor import events
        events.subscribe(events.SupervisorStateChangeEvent, callback)
        from supervisor.medusa import asyncore_25 as asyncore
        options.test = True
        self.assertRaises(asyncore.ExitNow, supervisord.runforever)
        self.assertTrue(pgroup.all_stopped)
        self.assertTrue(isinstance(L[0], events.SupervisorRunningEvent))
        self.assertTrue(isinstance(L[0], events.SupervisorStateChangeEvent))
        self.assertTrue(isinstance(L[1], events.SupervisorStoppingEvent))
        self.assertTrue(isinstance(L[1], events.SupervisorStateChangeEvent))
        
    def test_exit(self):
        options = DummyOptions()
        supervisord = self._makeOne(options)
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',)
        process = DummyProcess(pconfig)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig])
        pgroup = DummyProcessGroup(gconfig)
        L = []
        def callback():
            L.append(1)
        supervisord.process_groups = {'foo': pgroup}
        supervisord.options.mood = 0
        supervisord.options.test = True
        from supervisor.medusa import asyncore_25 as asyncore
        self.assertRaises(asyncore.ExitNow, supervisord.runforever)
        self.assertEqual(pgroup.all_stopped, True)

    def test_exit_delayed(self):
        options = DummyOptions()
        supervisord = self._makeOne(options)
        pconfig = DummyPConfig(options, 'foo', '/bin/foo',)
        process = DummyProcess(pconfig)
        gconfig = DummyPGroupConfig(options, pconfigs=[pconfig])
        pgroup = DummyProcessGroup(gconfig)
        pgroup.unstopped_processes = [process]
        L = []
        def callback():
            L.append(1)
        supervisord.process_groups = {'foo': pgroup}
        supervisord.options.mood = 0
        supervisord.options.test = True
        supervisord.runforever()
        self.assertNotEqual(supervisord.lastshutdownreport, 0)

    def test_getSupervisorStateDescription(self):
        from supervisor.states import getSupervisorStateDescription
        from supervisor.states import SupervisorStates
        result = getSupervisorStateDescription(SupervisorStates.RUNNING)
        self.assertEqual(result, 'RUNNING')

    def test_tick(self):
        from supervisor import events
        L = []
        def callback(event):
            L.append(event)
        events.subscribe(events.TickEvent, callback)
        options = DummyOptions()
        supervisord = self._makeOne(options)

        supervisord.tick(now=0)
        self.assertEqual(supervisord.ticks[5], 0)
        self.assertEqual(supervisord.ticks[60], 0)
        self.assertEqual(supervisord.ticks[3600], 0)
        self.assertEqual(len(L), 0)

        supervisord.tick(now=6)
        self.assertEqual(supervisord.ticks[5], 5)
        self.assertEqual(supervisord.ticks[60], 0)
        self.assertEqual(supervisord.ticks[3600], 0)
        self.assertEqual(len(L), 1)
        self.assertEqual(L[-1].__class__, events.Tick5Event)
        
        supervisord.tick(now=61)
        self.assertEqual(supervisord.ticks[5], 60)
        self.assertEqual(supervisord.ticks[60], 60)
        self.assertEqual(supervisord.ticks[3600], 0)
        self.assertEqual(len(L), 3)
        self.assertEqual(L[-1].__class__, events.Tick60Event)
        
        supervisord.tick(now=3601)
        self.assertEqual(supervisord.ticks[5], 3600)
        self.assertEqual(supervisord.ticks[60], 3600)
        self.assertEqual(supervisord.ticks[3600], 3600)
        self.assertEqual(len(L), 6)
        self.assertEqual(L[-1].__class__, events.Tick3600Event)

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')


########NEW FILE########
__FILENAME__ = test_web
import sys
import unittest

from supervisor.tests.base import DummySupervisor
from supervisor.tests.base import DummyRequest

class DeferredWebProducerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.web import DeferredWebProducer
        return DeferredWebProducer

    def _makeOne(self, request, callback):
        producer = self._getTargetClass()(request, callback)
        return producer

    def test_ctor(self):
        request = DummyRequest('/index.html', [], '', '')
        callback = lambda *x: None
        callback.delay = 1
        producer = self._makeOne(request, callback)
        self.assertEqual(producer.callback, callback)
        self.assertEqual(producer.request, request)
        self.assertEqual(producer.finished, False)
        self.assertEqual(producer.delay, 1)

    def test_more_not_done_yet(self):
        request = DummyRequest('/index.html', [], '', '')
        from supervisor.http import NOT_DONE_YET
        callback = lambda *x: NOT_DONE_YET
        callback.delay = 1
        producer = self._makeOne(request, callback)
        self.assertEqual(producer.more(), NOT_DONE_YET)

    def test_more_exception_caught(self):
        request = DummyRequest('/index.html', [], '', '')
        def callback(*arg):
            raise ValueError('foo')
        callback.delay = 1
        producer = self._makeOne(request, callback)
        self.assertEqual(producer.more(), None)
        logdata = request.channel.server.logger.logged
        self.assertEqual(len(logdata), 1)
        logged = logdata[0]
        self.assertEqual(logged[0], 'Web interface error')
        self.assertTrue(logged[1].startswith('Traceback'), logged[1])
        self.assertEqual(producer.finished, True)
        self.assertEqual(request._error, 500)

    def test_sendresponse_redirect(self):
        request = DummyRequest('/index.html', [], '', '')
        callback = lambda *arg: None
        callback.delay = 1
        producer = self._makeOne(request, callback)
        response = {'headers': {'Location':'abc'}}
        result = producer.sendresponse(response)
        self.assertEqual(result, None)
        self.assertEqual(request._error, 301)
        self.assertEqual(request.headers['Content-Type'], 'text/plain')
        self.assertEqual(request.headers['Content-Length'], 0)

    def test_sendresponse_withbody_and_content_type(self):
        request = DummyRequest('/index.html', [], '', '')
        callback = lambda *arg: None
        callback.delay = 1
        producer = self._makeOne(request, callback)
        response = {'body': 'abc', 'headers':{'Content-Type':'text/html'}}
        result = producer.sendresponse(response)
        self.assertEqual(result, None)
        self.assertEqual(request.headers['Content-Type'], 'text/html')
        self.assertEqual(request.headers['Content-Length'], 3)
        self.assertEqual(request.producers[0], 'abc')

class UIHandlerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.web import supervisor_ui_handler
        return supervisor_ui_handler

    def _makeOne(self):
        supervisord = DummySupervisor()
        handler = self._getTargetClass()(supervisord)
        return handler

    def test_handle_request_no_view_method(self):
        request = DummyRequest('/foo.css', [], '', '', {'PATH_INFO':'/foo.css'})
        handler = self._makeOne()
        data = handler.handle_request(request)
        self.assertEqual(data, None)
        
    def test_handle_request_default(self):
        request = DummyRequest('/index.html', [], '', '',
                               {'PATH_INFO':'/index.html'})
        handler = self._makeOne()
        data = handler.handle_request(request)
        self.assertEqual(data, None)
        self.assertEqual(request.channel.producer.request, request)
        from supervisor.web import StatusView
        self.assertEqual(request.channel.producer.callback.__class__,StatusView)

    def test_handle_request_index_html(self):
        request = DummyRequest('/index.html', [], '', '',
                               {'PATH_INFO':'/index.html'})
        handler = self._makeOne()
        data = handler.handle_request(request)
        from supervisor.web import StatusView
        view = request.channel.producer.callback
        self.assertEqual(view.__class__, StatusView)
        self.assertEqual(view.context.template, 'ui/status.html')

    def test_handle_request_tail_html(self):
        request = DummyRequest('/tail.html', [], '', '',
                               {'PATH_INFO':'/tail.html'})
        handler = self._makeOne()
        data = handler.handle_request(request)
        from supervisor.web import TailView
        view = request.channel.producer.callback
        self.assertEqual(view.__class__, TailView)
        self.assertEqual(view.context.template, 'ui/tail.html')

    def test_handle_request_ok_html(self):
        request = DummyRequest('/tail.html', [], '', '',
                               {'PATH_INFO':'/ok.html'})
        handler = self._makeOne()
        data = handler.handle_request(request)
        from supervisor.web import OKView
        view = request.channel.producer.callback
        self.assertEqual(view.__class__, OKView)
        self.assertEqual(view.context.template, None)


class StatusViewTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.web import StatusView
        return StatusView

    def _makeOne(self, context):
        klass = self._getTargetClass()
        return klass(context)

    def test_make_callback_noaction(self):
        context = DummyContext()
        context.supervisord = DummySupervisor()
        context.template = 'ui/status.html'
        context.form = {}
        view = self._makeOne(context)
        self.assertRaises(ValueError, view.make_callback, 'process', None)

    def test_render_noaction(self):
        context = DummyContext()
        context.supervisord = DummySupervisor()
        context.template = 'ui/status.html'
        context.request = DummyRequest('/foo', [], '', '')
        context.form = {}
        context.response = {}
        view = self._makeOne(context)
        data = view.render()
        self.assertTrue(data.startswith('<!DOCTYPE html PUBLIC'), data)

    def test_render_refresh(self):
        context = DummyContext()
        context.supervisord = DummySupervisor()
        context.template = 'ui/status.html'
        context.response = {}
        context.form = {'action':'refresh'}
        view = self._makeOne(context)
        data = view.render()
        from supervisor.http import NOT_DONE_YET
        self.assertTrue(data is NOT_DONE_YET, data)

class DummyContext:
    pass

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')

########NEW FILE########
__FILENAME__ = test_xmlrpc
import sys
import unittest

from supervisor.tests.base import DummySupervisor
from supervisor.tests.base import DummyRequest
from supervisor.tests.base import DummySupervisorRPCNamespace

class XMLRPCMarshallingTests(unittest.TestCase):
    def test_xmlrpc_marshal(self):
        import xmlrpclib
        from supervisor import xmlrpc
        data = xmlrpc.xmlrpc_marshal(1)
        self.assertEqual(data, xmlrpclib.dumps((1,), methodresponse=True))
        fault = xmlrpclib.Fault(1, 'foo')
        data = xmlrpc.xmlrpc_marshal(fault)
        self.assertEqual(data, xmlrpclib.dumps(fault))

class XMLRPCHandlerTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.xmlrpc import supervisor_xmlrpc_handler
        return supervisor_xmlrpc_handler
    
    def _makeOne(self, supervisord, subinterfaces):
        return self._getTargetClass()(supervisord, subinterfaces)

    def test_ctor(self):
        supervisor = DummySupervisor()
        subinterfaces = [('supervisor', DummySupervisorRPCNamespace())]
        handler = self._makeOne(supervisor, subinterfaces)
        self.assertEqual(handler.supervisord, supervisor)
        from supervisor.xmlrpc import RootRPCInterface
        self.assertEqual(handler.rpcinterface.__class__, RootRPCInterface)

    def test_match(self):
        class DummyRequest:
            def __init__(self, uri):
                self.uri = uri
        supervisor = DummySupervisor()
        subinterfaces = [('supervisor', DummySupervisorRPCNamespace())]
        handler = self._makeOne(supervisor, subinterfaces)
        self.assertEqual(handler.match(DummyRequest('/RPC2')), True)
        self.assertEqual(handler.match(DummyRequest('/nope')), False)

    def test_continue_request_nosuchmethod(self):
        supervisor = DummySupervisor()
        subinterfaces = [('supervisor', DummySupervisorRPCNamespace())]
        handler = self._makeOne(supervisor, subinterfaces)
        import xmlrpclib
        data = xmlrpclib.dumps(('a', 'b'), 'supervisor.noSuchMethod')
        request = DummyRequest('/what/ever', None, None, None)
        handler.continue_request(data, request)
        logdata = supervisor.options.logger.data
        from supervisor.xmlrpc import loads
        if loads:
            expected = 2
        else:
            expected = 3
        self.assertEqual(len(logdata), expected)
        self.assertEqual(logdata[-2],
                         u'XML-RPC method called: supervisor.noSuchMethod()')
        self.assertEqual(logdata[-1],
           (u'XML-RPC method supervisor.noSuchMethod() returned fault: '
            '[1] UNKNOWN_METHOD'))
        self.assertEqual(len(request.producers), 1)
        xml_response = request.producers[0]
        self.assertRaises(xmlrpclib.Fault, xmlrpclib.loads, xml_response)

    def test_continue_request_methodsuccess(self):
        supervisor = DummySupervisor()
        subinterfaces = [('supervisor', DummySupervisorRPCNamespace())]
        handler = self._makeOne(supervisor, subinterfaces)
        import xmlrpclib
        data = xmlrpclib.dumps((), 'supervisor.getAPIVersion')
        request = DummyRequest('/what/ever', None, None, None)
        handler.continue_request(data, request)
        logdata = supervisor.options.logger.data
        from supervisor.xmlrpc import loads
        if loads:
            expected = 2
        else:
            expected = 3
        self.assertEqual(len(logdata), expected)
        self.assertEqual(logdata[-2],
               u'XML-RPC method called: supervisor.getAPIVersion()')
        self.assertEqual(logdata[-1],
            u'XML-RPC method supervisor.getAPIVersion() returned successfully')
        self.assertEqual(len(request.producers), 1)
        xml_response = request.producers[0]
        response = xmlrpclib.loads(xml_response)
        from supervisor.rpcinterface import API_VERSION
        self.assertEqual(response[0][0], API_VERSION)
        self.assertEqual(request._done, True)
        self.assertEqual(request.headers['Content-Type'], 'text/xml')
        self.assertEqual(request.headers['Content-Length'], len(xml_response))

    def test_continue_request_no_params_in_request(self):
        supervisor = DummySupervisor()
        subinterfaces = [('supervisor', DummySupervisorRPCNamespace())]
        handler = self._makeOne(supervisor, subinterfaces)
        data = '<?xml version="1.0" encoding="UTF-8"?>' \
               '<methodCall>' \
               '<methodName>supervisor.getAPIVersion</methodName>' \
               '</methodCall>'
        request = DummyRequest('/what/ever', None, None, None)
        handler.continue_request(data, request)
        logdata = supervisor.options.logger.data
        from supervisor.xmlrpc import loads
        if loads:
            expected = 2
        else:
            expected = 3
        self.assertEqual(len(logdata), expected)
        self.assertEqual(logdata[-2],
               u'XML-RPC method called: supervisor.getAPIVersion()')
        self.assertEqual(logdata[-1],
            u'XML-RPC method supervisor.getAPIVersion() returned successfully')
        self.assertEqual(len(request.producers), 1)
        xml_response = request.producers[0]
        import xmlrpclib
        response = xmlrpclib.loads(xml_response)
        from supervisor.rpcinterface import API_VERSION
        self.assertEqual(response[0][0], API_VERSION)
        self.assertEqual(request._done, True)
        self.assertEqual(request.headers['Content-Type'], 'text/xml')
        self.assertEqual(request.headers['Content-Length'], len(xml_response))

    def test_continue_request_400_if_method_name_is_empty(self):
        supervisor = DummySupervisor()
        subinterfaces = [('supervisor', DummySupervisorRPCNamespace())]
        handler = self._makeOne(supervisor, subinterfaces)
        data = '<?xml version="1.0" encoding="UTF-8"?>' \
               '<methodCall><methodName></methodName></methodCall>'
        request = DummyRequest('/what/ever', None, None, None)
        handler.continue_request(data, request)
        logdata = supervisor.options.logger.data
        from supervisor.xmlrpc import loads
        if loads:
            expected = 1
        else:
            expected = 2
        self.assertEqual(len(logdata), expected)
        self.assertEqual(logdata[-1],
               u'XML-RPC request received with no method name')
        self.assertEqual(len(request.producers), 0)
        self.assertEqual(request._error, 400)

    def test_continue_request_500(self):
        supervisor = DummySupervisor()
        subinterfaces = [('supervisor', DummySupervisorRPCNamespace())]
        handler = self._makeOne(supervisor, subinterfaces)
        import xmlrpclib
        data = xmlrpclib.dumps((), 'supervisor.raiseError')
        request = DummyRequest('/what/ever', None, None, None)
        handler.continue_request(data, request)
        logdata = supervisor.options.logger.data
        from supervisor.xmlrpc import loads
        if loads:
            expected = 2
        else:
            expected = 3
        self.assertEqual(len(logdata), expected)
        self.assertEqual(logdata[-2],
               u'XML-RPC method called: supervisor.raiseError()')
        self.failUnless(logdata[-1].startswith('Traceback'))
        self.failUnless(logdata[-1].endswith('ValueError: error\n'))
        self.assertEqual(len(request.producers), 0)
        self.assertEqual(request._error, 500)

class TraverseTests(unittest.TestCase):
    def test_underscore(self):
        from supervisor import xmlrpc
        self.assertRaises(xmlrpc.RPCError, xmlrpc.traverse, None, '_', None)

    def test_notfound(self):
        from supervisor import xmlrpc
        self.assertRaises(xmlrpc.RPCError, xmlrpc.traverse, None, 'foo', None)

    def test_badparams(self):
        from supervisor import xmlrpc
        self.assertRaises(xmlrpc.RPCError, xmlrpc.traverse, self,
                          'test_badparams', (1, 2, 3))

    def test_success(self):
        from supervisor import xmlrpc
        L = []
        class Dummy:
            def foo(self, a):
                L.append(a)
        dummy = Dummy()
        xmlrpc.traverse(dummy, 'foo', [1])
        self.assertEqual(L, [1])

class SupervisorTransportTests(unittest.TestCase):
    def _getTargetClass(self):
        from supervisor.xmlrpc import SupervisorTransport
        return SupervisorTransport

    def _makeOne(self, *arg, **kw):
        return self._getTargetClass()(*arg, **kw)

    def test_ctor_unix(self):
        from supervisor import xmlrpc
        transport = self._makeOne('user', 'pass', 'unix:///foo/bar')
        conn = transport._get_connection()
        self.failUnless(isinstance(conn, xmlrpc.UnixStreamHTTPConnection))
        self.assertEqual(conn.host, 'localhost')
        self.assertEqual(conn.socketfile, '/foo/bar')

    def test__get_connection_http_9001(self):
        import httplib
        transport = self._makeOne('user', 'pass', 'http://127.0.0.1:9001/')
        conn = transport._get_connection()
        self.failUnless(isinstance(conn, httplib.HTTPConnection))
        self.assertEqual(conn.host, '127.0.0.1')
        self.assertEqual(conn.port, 9001)

    def test__get_connection_http_80(self):
        import httplib
        transport = self._makeOne('user', 'pass', 'http://127.0.0.1/')
        conn = transport._get_connection()
        self.failUnless(isinstance(conn, httplib.HTTPConnection))
        self.assertEqual(conn.host, '127.0.0.1')
        self.assertEqual(conn.port, 80)

    def test_request_non_200_response(self):
        import xmlrpclib
        transport = self._makeOne('user', 'pass', 'http://127.0.0.1/')
        dummy_conn = DummyConnection(400, '')
        def getconn():
            return dummy_conn
        transport._get_connection = getconn
        self.assertRaises(xmlrpclib.ProtocolError,
                          transport.request, 'localhost', '/', '')
        self.assertEqual(transport.connection, None)
        self.assertEqual(dummy_conn.closed, True)

    def test_request_400_response(self):
        import xmlrpclib
        transport = self._makeOne('user', 'pass', 'http://127.0.0.1/')
        dummy_conn = DummyConnection(400, '')
        def getconn():
            return dummy_conn
        transport._get_connection = getconn
        self.assertRaises(xmlrpclib.ProtocolError,
                          transport.request, 'localhost', '/', '')
        self.assertEqual(transport.connection, None)
        self.assertEqual(dummy_conn.closed, True)
        self.assertEqual(dummy_conn.requestargs[0], 'POST')
        self.assertEqual(dummy_conn.requestargs[1], '/')
        self.assertEqual(dummy_conn.requestargs[2], '')
        self.assertEqual(dummy_conn.requestargs[3]['Content-Length'], '0')
        self.assertEqual(dummy_conn.requestargs[3]['Content-Type'], 'text/xml')
        self.assertEqual(dummy_conn.requestargs[3]['Authorization'],
                         'Basic dXNlcjpwYXNz')
        self.assertEqual(dummy_conn.requestargs[3]['Accept'], 'text/xml')

    def test_request_200_response(self):
        transport = self._makeOne('user', 'pass', 'http://127.0.0.1/')
        response = """<?xml version="1.0"?>
        <methodResponse>
        <params>
        <param>
        <value><string>South Dakota</string></value>
        </param>
        </params>
        </methodResponse>"""
        dummy_conn = DummyConnection(200, response)
        def getconn():
            return dummy_conn
        transport._get_connection = getconn
        result = transport.request('localhost', '/', '')
        self.assertEqual(transport.connection, dummy_conn)
        self.assertEqual(dummy_conn.closed, False)
        self.assertEqual(dummy_conn.requestargs[0], 'POST')
        self.assertEqual(dummy_conn.requestargs[1], '/')
        self.assertEqual(dummy_conn.requestargs[2], '')
        self.assertEqual(dummy_conn.requestargs[3]['Content-Length'], '0')
        self.assertEqual(dummy_conn.requestargs[3]['Content-Type'], 'text/xml')
        self.assertEqual(dummy_conn.requestargs[3]['Authorization'],
                         'Basic dXNlcjpwYXNz')
        self.assertEqual(dummy_conn.requestargs[3]['Accept'], 'text/xml')
        self.assertEqual(result, ('South Dakota',))

    def test_works_with_py25(self):
        instance = self._makeOne('username', 'password', 'http://127.0.0.1')
        # the test is just to insure that this method can be called; failure
        # would be an AttributeError for _use_datetime under Python 2.5
        parser, unmarshaller = instance.getparser() # this uses _use_datetime

class IterparseLoadsTests(unittest.TestCase):
    def test_iterparse_loads_methodcall(self):
        s = """<?xml version="1.0"?>
        <methodCall>
        <methodName>examples.getStateName</methodName>
        <params>
        <param>
        <value><i4>41</i4></value>
        </param>
        <param>
        <value><int>14</int></value>
        </param>
        <param>
        <value><boolean>1</boolean></value>
        </param>
        <param>
        <value><string>hello world</string></value>
        </param>
        <param>
        <value><double>-12.214</double></value>
        </param>
        <param>
        <value><dateTime.iso8601>19980717T14:08:55</dateTime.iso8601></value>
        </param>
        <param>
        <value><base64>eW91IGNhbid0IHJlYWQgdGhpcyE=</base64></value>
        </param>
        <param>
        <struct>
          <member><name>k</name><value><i4>5</i4></value></member>
        </struct>
        </param>
        <param>
        <array><data><value><i4>12</i4></value></data></array>
        </param>
        <param>
        <struct>
        <member>
          <name>k</name>
          <value><array><data><value><i4>1</i4></value></data></array></value>
        </member>
        </struct>
        </param>
        </params>
        </methodCall>
        """
        from supervisor.xmlrpc import loads
        if loads is None:
            return # no cElementTree
        result = loads(s)
        params, method = result
        import datetime
        self.assertEqual(method, 'examples.getStateName')
        self.assertEqual(params[0], 41)
        self.assertEqual(params[1], 14)
        self.assertEqual(params[2], True)
        self.assertEqual(params[3], 'hello world')
        self.assertEqual(params[4], -12.214)
        self.assertEqual(params[5], datetime.datetime(1998, 7, 17, 14, 8, 55))
        self.assertEqual(params[6], "you can't read this!")
        self.assertEqual(params[7], {'k': 5})
        self.assertEqual(params[8], [12])
        self.assertEqual(params[9], {'k': [1]})

class DummyResponse:
    def __init__(self, status=200, body='', reason='reason'):
        self.status = status
        self.body = body
        self.reason = reason

    def read(self):
        return self.body

class DummyConnection:
    closed = False
    def __init__(self, status=200, body='', reason='reason'):
        self.response = DummyResponse(status, body, reason)

    def getresponse(self):
        return self.response
        
    def request(self, *arg, **kw):
        self.requestargs = arg
        self.requestkw = kw

    def close(self):
        self.closed = True

def test_suite():
    return unittest.findTestCases(sys.modules[__name__])

if __name__ == '__main__':
    unittest.main(defaultTest='test_suite')


########NEW FILE########
__FILENAME__ = trackrefs
##############################################################################
#
# Copyright (c) 2007 Zope Corporation and Contributors.
# All Rights Reserved.
#
# This software is subject to the provisions of the Zope Public License,
# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.
# THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL EXPRESS OR IMPLIED
# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS
# FOR A PARTICULAR PURPOSE
#
##############################################################################

""" Code from the zope.testing module to help track down memory leaks

TrackRefs works only in a python compiled with the --with-pydebug flag.
An example of how to use TrackRefs in a function is below.

glen = 0
rc = 0

def doit():
    newglen = gc.collect()
    global glen
    if newglen > glen:
        print
        print "-------------------------------------"
        print "more garbage", newglen - glen
        glen = newglen
        print "-------------------------------------"
        print
    if refs:
        newrc = sys.gettotalrefcount()
        global rc
        if newrc > rc:
            refs.update()
            refs.detailed_refcounts(newrc, rc)
        rc = newrc
"""

import sys
import gc
import types

class TrackRefs(object):
    """Object to track reference counts across test runs."""

    def __init__(self):
        self.type2count = {}
        self.type2all = {}
        self.delta = None
        self.n = 0
        self.update()
        self.delta = None

    def update(self):
        gc.collect()
        obs = sys.getobjects(0)
        type2count = {}
        type2all = {}
        n = 0
        for o in obs:
            if type(o) is str and o == '<dummy key>':
                # avoid dictionary madness
                continue

            all = sys.getrefcount(o) - 3
            n += all

            t = type(o)
            if t is types.InstanceType:
                t = o.__class__

            if t in type2count:
                type2count[t] += 1
                type2all[t] += all
            else:
                type2count[t] = 1
                type2all[t] = all


        ct = [(
               type_or_class_title(t),
               type2count[t] - self.type2count.get(t, 0),
               type2all[t] - self.type2all.get(t, 0),
               )
              for t in type2count.iterkeys()]
        ct += [(
                type_or_class_title(t),
                - self.type2count[t],
                - self.type2all[t],
                )
               for t in self.type2count.iterkeys()
               if t not in type2count]
        ct.sort()
        self.delta = ct
        self.type2count = type2count
        self.type2all = type2all
        self.n = n


    def output(self):
        printed = False
        s1 = s2 = 0
        for t, delta1, delta2 in self.delta:
            if delta1 or delta2:
                if not printed:
                    print (
                        '    Leak details, changes in instances and refcounts'
                        ' by type/class:')
                    print "    %-55s %6s %6s" % ('type/class', 'insts', 'refs')
                    print "    %-55s %6s %6s" % ('-' * 55, '-----', '----')
                    printed = True
                print "    %-55s %6d %6d" % (t, delta1, delta2)
                s1 += delta1
                s2 += delta2

        if printed:
            print "    %-55s %6s %6s" % ('-' * 55, '-----', '----')
            print "    %-55s %6s %6s" % ('total', s1, s2)


        self.delta = None

    def detailed_refcounts(self, rc, prev):
        """Report a change in reference counts, with extra detail."""
        print ("  sum detail refcount=%-8d"
               " sys refcount=%-8d"
               " change=%-6d"
               % (self.n, rc, rc - prev))
        self.output()

def type_or_class_title(t):
    module = getattr(t, '__module__', '__builtin__')
    if module == '__builtin__':
        return t.__name__
    return "%s.%s" % (module, t.__name__)


########NEW FILE########
__FILENAME__ = web
# This file was modified by Xiaomi.com on 2013-6-27.

import os
import re
import cgi
import time
import traceback
import urllib
import datetime
import StringIO

from supervisor.medusa import producers
from supervisor.medusa.http_server import http_date
from supervisor.medusa.http_server import get_header
from supervisor.medusa.xmlrpc_handler import collector

import meld3

from supervisor.process import ProcessStates
from supervisor.http import NOT_DONE_YET

from supervisor.options import make_namespec
from supervisor.options import split_namespec

from supervisor.xmlrpc import SystemNamespaceRPCInterface
from supervisor.xmlrpc import RootRPCInterface
from supervisor.xmlrpc import Faults
from supervisor.xmlrpc import RPCError

from supervisor.rpcinterface import SupervisorNamespaceRPCInterface

class DeferredWebProducer:
    """ A medusa producer that implements a deferred callback; requires
    a subclass of asynchat.async_chat that handles NOT_DONE_YET sentinel """
    CONNECTION = re.compile ('Connection: (.*)', re.IGNORECASE)

    def __init__(self, request, callback):
        self.callback = callback
        self.request = request
        self.finished = False
        self.delay = float(callback.delay)

    def more(self):
        if self.finished:
            return ''
        try:
            response = self.callback()
            if response is NOT_DONE_YET:
                return NOT_DONE_YET
                
            self.finished = True
            return self.sendresponse(response)

        except:
            io = StringIO.StringIO()
            traceback.print_exc(file=io)
            # this should go to the main supervisor log file
            self.request.channel.server.logger.log('Web interface error',
                                                  io.getvalue())
            self.finished = True
            self.request.error(500)

    def sendresponse(self, response):

        headers = response.get('headers', {})
        for header in headers:
            self.request[header] = headers[header]

        if not self.request.has_key('Content-Type'):
            self.request['Content-Type'] = 'text/plain'

        if headers.get('Location'):
            self.request['Content-Length'] = 0
            self.request.error(301)
            return

        body = response.get('body', '')
        self.request['Content-Length'] = len(body)
            
        self.request.push(body)

        connection = get_header(self.CONNECTION, self.request.header)

        close_it = 0
        wrap_in_chunking = 0

        if self.request.version == '1.0':
            if connection == 'keep-alive':
                if not self.request.has_key('Content-Length'):
                    close_it = 1
                else:
                    self.request['Connection'] = 'Keep-Alive'
            else:
                close_it = 1
        elif self.request.version == '1.1':
            if connection == 'close':
                close_it = 1
            elif not self.request.has_key ('Content-Length'):
                if self.request.has_key ('Transfer-Encoding'):
                    if not self.request['Transfer-Encoding'] == 'chunked':
                        close_it = 1
                elif self.request.use_chunked:
                    self.request['Transfer-Encoding'] = 'chunked'
                    wrap_in_chunking = 1
                else:
                    close_it = 1
        elif self.request.version is None:
            close_it = 1

        outgoing_header = producers.simple_producer (
            self.request.build_reply_header())

        if close_it:
            self.request['Connection'] = 'close'

        if wrap_in_chunking:
            outgoing_producer = producers.chunked_producer (
                    producers.composite_producer (self.request.outgoing)
                    )
            # prepend the header
            outgoing_producer = producers.composite_producer(
                [outgoing_header, outgoing_producer]
                )
        else:
            # prepend the header
            self.request.outgoing.insert(0, outgoing_header)
            outgoing_producer = producers.composite_producer (
                self.request.outgoing)

        # apply a few final transformations to the output
        self.request.channel.push_with_producer (
                # globbing gives us large packets
                producers.globbing_producer (
                        # hooking lets us log the number of bytes sent
                        producers.hooked_producer (
                                outgoing_producer,
                                self.request.log
                                )
                        )
                )

        self.request.channel.current_request = None

        if close_it:
            self.request.channel.close_when_done()

class ViewContext:
    def __init__(self, **kw):
        self.__dict__.update(kw)

class MeldView:

    content_type = 'text/html'
    delay = .5

    def __init__(self, context):
        self.context = context
        template = self.context.template
        if not os.path.isabs(template):
            here = os.path.abspath(os.path.dirname(__file__))
            template = os.path.join(here, template)
        self.root = meld3.parse_xml(template)
        self.callback = None

    def __call__(self):
        body = self.render()
        if body is NOT_DONE_YET:
            return NOT_DONE_YET

        response = self.context.response
        headers = response['headers']
        headers['Content-Type'] = self.content_type
        headers['Pragma'] = 'no-cache'
        headers['Cache-Control'] = 'no-cache'
        headers['Expires'] = http_date.build_http_date(0)
        response['body'] = body
        return response

    def clone(self):
        return self.root.clone()

class TailView(MeldView):
    def render(self):
        supervisord = self.context.supervisord
        form = self.context.form

        if not 'processname' in form:
            tail = 'No process name found'
            processname = None
        else:
            processname = form['processname']

            if not processname:
                tail = 'No process name found'
            else:
                rpcinterface = SupervisorNamespaceRPCInterface(supervisord)
                try:
                    tail = rpcinterface.readProcessLog(processname, -1024, 0)
                except RPCError, e:
                    if e.code == Faults.NO_FILE:
                        tail = 'No file for %s' % processname
                    else:
                        raise

        root = self.clone()

        title = root.findmeld('title')
        title.content('Supervisor tail of process %s' % processname)
        tailbody = root.findmeld('tailbody')
        tailbody.content(tail)

        refresh_anchor = root.findmeld('refresh_anchor')
        if processname is not None:
            refresh_anchor.attributes(href='tail.html?processname=%s' %
                                      urllib.quote(processname))
        else:
            refresh_anchor.deparent()

        return root.write_xhtmlstring()

class StatusView(MeldView):
    def actions_for_process(self, process):
        state = process.get_state()
        processname = urllib.quote(make_namespec(process.group.config.name,
                                                 process.config.name))
        start = {
        'name':'Start',
        'href':'index.html?processname=%s&amp;action=start' % processname,
        'target':None,
        }
        restart = {
        'name':'Restart',
        'href':'index.html?processname=%s&amp;action=restart' % processname,
        'target':None,
        }
        stop = {
        'name':'Stop',
        'href':'index.html?processname=%s&amp;action=stop' % processname,
        'target':None,
        }
        clearlog = {
        'name':'Clear Log',
        'href':'index.html?processname=%s&amp;action=clearlog' % processname,
        'target':None,
        }
        tailf = {
        'name':'Tail -f',
        'href':'logtail/%s' % processname,
        'target':'_blank'
        }
        if state == ProcessStates.RUNNING:
            actions = [restart, stop, clearlog, tailf]
        elif state in (ProcessStates.STOPPED, ProcessStates.EXITED,
                       ProcessStates.FATAL):
            actions = [start, None, clearlog, tailf]
        else:
            actions = [None, None, clearlog, tailf]
        return actions

    def css_class_for_state(self, state):
        if state == ProcessStates.RUNNING:
            return 'statusrunning'
        elif state in (ProcessStates.FATAL, ProcessStates.BACKOFF):
            return 'statuserror'
        else:
            return 'statusnominal'

    def make_callback(self, namespec, action):
        message = None
        supervisord = self.context.supervisord

        # the rpc interface code is already written to deal properly in a
        # deferred world, so just use it
        main =   ('supervisor', SupervisorNamespaceRPCInterface(supervisord))
        system = ('system', SystemNamespaceRPCInterface([main]))

        rpcinterface = RootRPCInterface([main, system])

        if action:

            if action == 'refresh':
                def donothing():
                    message = 'Page refreshed at %s' % time.ctime()
                    return message
                donothing.delay = 0.05
                return donothing

            elif action == 'stopall':
                callback = rpcinterface.supervisor.stopAllProcesses()
                def stopall():
                    if callback() is NOT_DONE_YET:
                        return NOT_DONE_YET
                    else:
                        return 'All stopped at %s' % time.ctime()
                stopall.delay = 0.05
                return stopall

            elif action == 'restartall':
                callback = rpcinterface.system.multicall(
                    [ {'methodName':'supervisor.stopAllProcesses'},
                      {'methodName':'supervisor.startAllProcesses'} ] )
                def restartall():
                    result = callback()
                    if result is NOT_DONE_YET:
                        return NOT_DONE_YET
                    return 'All restarted at %s' % time.ctime()
                restartall.delay = 0.05
                return restartall

            elif namespec:
                def wrong():
                    return 'No such process named %s' % namespec
                wrong.delay = 0.05
                group_name, process_name = split_namespec(namespec)
                group = supervisord.process_groups.get(group_name)
                if group is None:
                    return wrong
                process = group.processes.get(process_name)
                if process is None:
                    return wrong

                elif action == 'stop':
                    callback = rpcinterface.supervisor.stopProcess(namespec)
                    def stopprocess():
                        result = callback()
                        if result is NOT_DONE_YET:
                            return NOT_DONE_YET
                        return 'Process %s stopped' % namespec
                    stopprocess.delay = 0.05
                    return stopprocess

                elif action == 'restart':
                    callback = rpcinterface.system.multicall(
                        [ {'methodName':'supervisor.stopProcess',
                           'params': [namespec]},
                          {'methodName':'supervisor.startProcess',
                           'params': [namespec]},
                          ]
                        )
                    def restartprocess():
                        result = callback()
                        if result is NOT_DONE_YET:
                            return NOT_DONE_YET
                        return 'Process %s restarted' % namespec
                    restartprocess.delay = 0.05
                    return restartprocess

                elif action == 'start':
                    try:
                        callback = rpcinterface.supervisor.startProcess(
                            namespec)
                    except RPCError, e:
                        if e.code == Faults.SPAWN_ERROR:
                            def spawnerr():
                                return 'Process %s spawn error' % namespec
                            spawnerr.delay = 0.05
                            return spawnerr
                    def startprocess():
                        if callback() is NOT_DONE_YET:
                            return NOT_DONE_YET
                        return 'Process %s started' % namespec
                    startprocess.delay = 0.05
                    return startprocess
                
                elif action == 'clearlog':
                    callback = rpcinterface.supervisor.clearProcessLog(
                        namespec)
                    def clearlog():
                        return 'Log for %s cleared' % namespec
                    clearlog.delay = 0.05
                    return clearlog

        raise ValueError(action)
    
    def render(self):
        form = self.context.form
        response = self.context.response
        processname = form.get('processname')
        action = form.get('action')
        message = form.get('message')

        if action:
            if not self.callback:
                self.callback = self.make_callback(processname, action)
                return NOT_DONE_YET

            else:
                message =  self.callback()
                if message is NOT_DONE_YET:
                    return NOT_DONE_YET
                if message is not None:
                    server_url = form['SERVER_URL']
                    location = server_url + '?message=%s' % urllib.quote(
                        message)
                    response['headers']['Location'] = location

        supervisord = self.context.supervisord
        rpcinterface = RootRPCInterface(
            [('supervisor',
              SupervisorNamespaceRPCInterface(supervisord))]
            )

        processnames = []
        groups = supervisord.process_groups.values()
        for group in groups:
            gprocnames = group.processes.keys()
            for gprocname in gprocnames:
                processnames.append((group.config.name, gprocname))

        processnames.sort()

        data = []
        for groupname, processname in processnames:
            process = supervisord.process_groups[groupname].processes[processname]
            actions = self.actions_for_process(process)
            sent_name = make_namespec(groupname, processname)
            info = rpcinterface.supervisor.getProcessInfo(sent_name)
            data.append({
                'status':info['statename'],
                'name':processname,
                'group':groupname,
                'actions':actions,
                'state':info['state'],
                'description':info['description'],
                'http_url':process.config.http_url,
                })
        
        root = self.clone()

        if message is not None:
            statusarea = root.findmeld('statusmessage')
            statusarea.attrib['class'] = 'status_msg'
            statusarea.content(message)

        if data:
            iterator = root.findmeld('tr').repeat(data)
            shaded_tr = False

            for tr_element, item in iterator:
                status_text = tr_element.findmeld('status_text')
                status_text.content(item['status'].lower())
                status_text.attrib['class'] = self.css_class_for_state(
                    item['state'])

                info_text = tr_element.findmeld('info_text')
                info_text.content(item['description'])

                anchor = tr_element.findmeld('name_anchor')
                processname = make_namespec(item['group'], item['name'])
                if item['http_url']:
                    anchor.attributes(href='%s' % item['http_url'])
                else:
                    anchor.attributes(href='')
                anchor.content(processname)

                actions = item['actions']
                actionitem_td = tr_element.findmeld('actionitem_td')
                
                for li_element, actionitem in actionitem_td.repeat(actions):
                    anchor = li_element.findmeld('actionitem_anchor')
                    if actionitem is None:
                        anchor.attrib['class'] = 'hidden'
                    else:
                        anchor.attributes(href=actionitem['href'], 
                                          name=actionitem['name'])
                        anchor.content(actionitem['name'])
                        if actionitem['target']:
                            anchor.attributes(target=actionitem['target'])
                if shaded_tr: 
                    tr_element.attrib['class'] = 'shade'
                shaded_tr = not shaded_tr
        else:
            table = root.findmeld('statustable')
            table.replace('No programs to manage')

        copyright_year = str(datetime.date.today().year)
        root.findmeld('copyright_date').content(copyright_year)

        return root.write_xhtmlstring()

class OKView:
    delay = 0
    def __init__(self, context):
        self.context = context
        
    def __call__(self):
        return {'body':'OK'}

VIEWS = {
    'index.html': {
          'template':'ui/status.html',
          'view':StatusView
          },
    'tail.html': {
           'template':'ui/tail.html',
           'view':TailView,
           },
    'ok.html': {
           'template':None,
           'view':OKView,
           },
    }


class supervisor_ui_handler:
    IDENT = 'Supervisor Web UI HTTP Request Handler'

    def __init__(self, supervisord):
        self.supervisord = supervisord

    def match(self, request):
        if request.command not in ('POST', 'GET'):
            return False

        path, params, query, fragment = request.split_uri()

        while path.startswith('/'):
            path = path[1:]

        if not path:
            path = 'index.html'
            
        for viewname in VIEWS.keys():
            if viewname == path:
                return True

    def handle_request(self, request):
        if request.command == 'POST':
            request.collector = collector(self, request)
        else:
            self.continue_request('', request)

    def continue_request (self, data, request):
        form = {}
        cgi_env = request.cgi_environment()
        form.update(cgi_env)
        if not form.has_key('QUERY_STRING'):
            form['QUERY_STRING'] = ''

        query = form['QUERY_STRING']

        # we only handle x-www-form-urlencoded values from POSTs
        form_urlencoded = cgi.parse_qsl(data)
        query_data = cgi.parse_qs(query)

        for k, v in query_data.items():
            # ignore dupes
            form[k] = v[0]

        for k, v in form_urlencoded:
            # ignore dupes
            form[k] = v

        form['SERVER_URL'] = request.get_server_url()

        path = form['PATH_INFO']
        # strip off all leading slashes
        while path and path[0] == '/':
            path = path[1:]
        if not path:
            path = 'index.html'

        viewinfo = VIEWS.get(path)
        if viewinfo is None:
            # this should never happen if our match method works
            return

        response = {}
        response['headers'] = {}

        viewclass = viewinfo['view']
        viewtemplate = viewinfo['template']
        context = ViewContext(template=viewtemplate,
                              request = request,
                              form = form,
                              response = response,
                              supervisord=self.supervisord)
        view = viewclass(context)
        pushproducer = request.channel.push_with_producer
        pushproducer(DeferredWebProducer(request, view))


########NEW FILE########
__FILENAME__ = xmlrpc
import types
import socket
import xmlrpclib
import httplib
import urllib
import re
from cStringIO import StringIO
import traceback
import sys

from supervisor.medusa.http_server import get_header
from supervisor.medusa.xmlrpc_handler import xmlrpc_handler
from supervisor.medusa import producers

from supervisor.http import NOT_DONE_YET

class Faults:
    UNKNOWN_METHOD = 1
    INCORRECT_PARAMETERS = 2
    BAD_ARGUMENTS = 3
    SIGNATURE_UNSUPPORTED = 4
    SHUTDOWN_STATE = 6
    BAD_NAME = 10
    NO_FILE = 20
    NOT_EXECUTABLE = 21
    FAILED = 30
    ABNORMAL_TERMINATION = 40
    SPAWN_ERROR = 50
    ALREADY_STARTED = 60
    NOT_RUNNING = 70
    SUCCESS = 80
    ALREADY_ADDED = 90
    STILL_RUNNING = 91
    CANT_REREAD = 92

def getFaultDescription(code):
    for faultname in Faults.__dict__:
        if getattr(Faults, faultname) == code:
            return faultname
    return 'UNKNOWN'

class RPCError(Exception):
    def __init__(self, code, extra=None):
        self.code = code
        self.text = getFaultDescription(code)
        if extra is not None:
            self.text = '%s: %s' % (self.text, extra)

class DeferredXMLRPCResponse:
    """ A medusa producer that implements a deferred callback; requires
    a subclass of asynchat.async_chat that handles NOT_DONE_YET sentinel """
    CONNECTION = re.compile ('Connection: (.*)', re.IGNORECASE)

    def __init__(self, request, callback):
        self.callback = callback
        self.request = request
        self.finished = False
        self.delay = float(callback.delay)

    def more(self):
        if self.finished:
            return ''
        try:
            try:
                value = self.callback()
                if value is NOT_DONE_YET:
                    return NOT_DONE_YET
            except RPCError, err:
                value = xmlrpclib.Fault(err.code, err.text)
                
            body = xmlrpc_marshal(value)

            self.finished = True

            return self.getresponse(body)

        except:
            # report unexpected exception back to server
            traceback.print_exc()
            self.finished = True
            self.request.error(500)

    def getresponse(self, body):
        self.request['Content-Type'] = 'text/xml'
        self.request['Content-Length'] = len(body)
        self.request.push(body)
        connection = get_header(self.CONNECTION, self.request.header)

        close_it = 0
        wrap_in_chunking = 0

        if self.request.version == '1.0':
            if connection == 'keep-alive':
                if not self.request.has_key ('Content-Length'):
                    close_it = 1
                else:
                    self.request['Connection'] = 'Keep-Alive'
            else:
                close_it = 1
        elif self.request.version == '1.1':
            if connection == 'close':
                close_it = 1
            elif not self.request.has_key ('Content-Length'):
                if self.request.has_key ('Transfer-Encoding'):
                    if not self.request['Transfer-Encoding'] == 'chunked':
                        close_it = 1
                elif self.request.use_chunked:
                    self.request['Transfer-Encoding'] = 'chunked'
                    wrap_in_chunking = 1
                else:
                    close_it = 1
        elif self.request.version is None:
            close_it = 1

        outgoing_header = producers.simple_producer (
            self.request.build_reply_header())

        if close_it:
            self.request['Connection'] = 'close'

        if wrap_in_chunking:
            outgoing_producer = producers.chunked_producer (
                    producers.composite_producer (self.request.outgoing)
                    )
            # prepend the header
            outgoing_producer = producers.composite_producer(
                [outgoing_header, outgoing_producer]
                )
        else:
            # prepend the header
            self.request.outgoing.insert(0, outgoing_header)
            outgoing_producer = producers.composite_producer (
                self.request.outgoing)

        # apply a few final transformations to the output
        self.request.channel.push_with_producer (
                # globbing gives us large packets
                producers.globbing_producer (
                        # hooking lets us log the number of bytes sent
                        producers.hooked_producer (
                                outgoing_producer,
                                self.request.log
                                )
                        )
                )

        self.request.channel.current_request = None

        if close_it:
            self.request.channel.close_when_done()

def xmlrpc_marshal(value):
    ismethodresponse = not isinstance(value, xmlrpclib.Fault)
    if ismethodresponse:
        if not isinstance(value, tuple):
            value = (value,)
        body = xmlrpclib.dumps(value,  methodresponse=ismethodresponse)
    else:
        body = xmlrpclib.dumps(value)
    return body

class SystemNamespaceRPCInterface:
    def __init__(self, namespaces):
        self.namespaces = {}
        for name, inst in namespaces:
            self.namespaces[name] = inst
        self.namespaces['system'] = self

    def _listMethods(self):
        methods = {}
        for ns_name in self.namespaces:
            namespace = self.namespaces[ns_name]
            for method_name in namespace.__class__.__dict__:
                # introspect; any methods that don't start with underscore
                # are published
                func = getattr(namespace, method_name)
                meth = getattr(func, 'im_func', None)
                if meth is not None:
                    if not method_name.startswith('_'):
                        sig = '%s.%s' % (ns_name, method_name)
                        methods[sig] = str(func.__doc__)
        return methods

    def listMethods(self):
        """ Return an array listing the available method names

        @return array result  An array of method names available (strings).
        """
        methods = self._listMethods()
        keys = methods.keys()
        keys.sort()
        return keys

    def methodHelp(self, name):
        """ Return a string showing the method's documentation

        @param string name   The name of the method.
        @return string result The documentation for the method name.
        """
        methods = self._listMethods()
        for methodname in methods.keys():
            if methodname == name:
                return methods[methodname]
        raise RPCError(Faults.SIGNATURE_UNSUPPORTED)
    
    def methodSignature(self, name):
        """ Return an array describing the method signature in the
        form [rtype, ptype, ptype...] where rtype is the return data type
        of the method, and ptypes are the parameter data types that the
        method accepts in method argument order.

        @param string name  The name of the method.
        @return array result  The result.
        """
        methods = self._listMethods()
        L = []
        for method in methods:
            if method == name:
                rtype = None
                ptypes = []
                parsed = gettags(methods[method])
                for thing in parsed:
                    if thing[1] == 'return': # tag name
                        rtype = thing[2] # datatype
                    elif thing[1] == 'param': # tag name
                        ptypes.append(thing[2]) # datatype
                if rtype is None:
                    raise RPCError(Faults.SIGNATURE_UNSUPPORTED)
                return [rtype] + ptypes
        raise RPCError(Faults.SIGNATURE_UNSUPPORTED)

    def multicall(self, calls):
        """Process an array of calls, and return an array of
        results. Calls should be structs of the form {'methodName':
        string, 'params': array}. Each result will either be a
        single-item array containg the result value, or a struct of
        the form {'faultCode': int, 'faultString': string}. This is
        useful when you need to make lots of small calls without lots
        of round trips.

        @param array calls  An array of call requests
        @return array result  An array of results
        """
        producers = []

        for call in calls:
            try:
                name = call['methodName']
                params = call.get('params', [])
                if name == 'system.multicall':
                    # Recursive system.multicall forbidden
                    raise RPCError(Faults.INCORRECT_PARAMETERS)
                root = AttrDict(self.namespaces)
                value = traverse(root, name, params)
            except RPCError, inst:
                value = {'faultCode': inst.code,
                         'faultString': inst.text}
            except:
                errmsg = "%s:%s" % (sys.exc_type, sys.exc_value)
                value = {'faultCode': 1, 'faultString': errmsg}
            producers.append(value)

        results = []

        def multiproduce():
            """ Run through all the producers in order """
            if not producers:
                return []

            callback = producers.pop(0)

            if isinstance(callback, types.FunctionType):
                try:
                    value = callback()
                except RPCError, inst:
                    value = {'faultCode':inst.code, 'faultString':inst.text}

                if value is NOT_DONE_YET:
                    # push it back in the front of the queue because we
                    # need to finish the calls in requested order
                    producers.insert(0, callback)
                    return NOT_DONE_YET
            else:
                value = callback

            results.append(value)

            if producers:
                # only finish when all producers are finished
                return NOT_DONE_YET

            return results

        multiproduce.delay = .05
        return multiproduce

class AttrDict(dict):
    # hack to make a dict's getattr equivalent to its getitem
    def __getattr__(self, name):
        return self[name]

class RootRPCInterface:
    def __init__(self, subinterfaces):
        for name, rpcinterface in subinterfaces:
            setattr(self, name, rpcinterface)

class supervisor_xmlrpc_handler(xmlrpc_handler):
    path = '/RPC2'
    IDENT = 'Supervisor XML-RPC Handler'
    def __init__(self, supervisord, subinterfaces):
        self.rpcinterface = RootRPCInterface(subinterfaces)
        self.supervisord = supervisord
        if loads:
            self.loads = loads
        else:
            self.supervisord.options.logger.warn(
                'cElementTree not installed, using slower XML parser for '
                'XML-RPC'
                )
            self.loads = xmlrpclib.loads

    def match(self, request):
        return request.uri.startswith(self.path)
        
    def continue_request (self, data, request):
        logger = self.supervisord.options.logger
        
        try:

            params, method = self.loads(data)

            # no <methodName> in the request or name is an empty string
            if not method:
                logger.trace('XML-RPC request received with no method name')
                request.error(400)
                return
            
            # we allow xml-rpc clients that do not send empty <params>
            # when there are no parameters for the method call
            if params is None:
                params = ()

            try:
                logger.trace('XML-RPC method called: %s()' % method)
                value = self.call(method, params)
                # application-specific: instead of we never want to
                # marshal None (even though we could by saying allow_none=True
                # in dumps within xmlrpc_marshall), this is meant as
                # a debugging fixture, see issue 223.
                assert value is not None, (
                    'return value from method %r with params %r is None' %
                    (method, params)
                    )
                logger.trace('XML-RPC method %s() returned successfully' %
                             method)
            except RPCError, err:
                # turn RPCError reported by method into a Fault instance
                value = xmlrpclib.Fault(err.code, err.text)
                logger.trace('XML-RPC method %s() returned fault: [%d] %s' % (
                    method,
                    err.code, err.text))

            if isinstance(value, types.FunctionType):
                # returning a function from an RPC method implies that
                # this needs to be a deferred response (it needs to block).
                pushproducer = request.channel.push_with_producer
                pushproducer(DeferredXMLRPCResponse(request, value))

            else:
                # if we get anything but a function, it implies that this
                # response doesn't need to be deferred, we can service it
                # right away.
                body = xmlrpc_marshal(value)
                request['Content-Type'] = 'text/xml'
                request['Content-Length'] = len(body)
                request.push(body)
                request.done()

        except:
            io = StringIO()
            traceback.print_exc(file=io)
            val = io.getvalue()
            logger.critical(val)
            # internal error, report as HTTP server error
            request.error(500)

    def call(self, method, params):
        return traverse(self.rpcinterface, method, params)

def traverse(ob, method, params):
    path = method.split('.')
    for name in path:
        if name.startswith('_'):
            # security (don't allow things that start with an underscore to
            # be called remotely)
            raise RPCError(Faults.UNKNOWN_METHOD)
        ob = getattr(ob, name, None)
        if ob is None:
            raise RPCError(Faults.UNKNOWN_METHOD)

    try:
        return ob(*params)
    except TypeError:
        raise RPCError(Faults.INCORRECT_PARAMETERS)

class SupervisorTransport(xmlrpclib.Transport):
    """
    Provides a Transport for xmlrpclib that uses
    httplib.HTTPConnection in order to support persistent
    connections.  Also support basic auth and UNIX domain socket
    servers.
    """
    connection = None

    _use_datetime = 0 # python 2.5 fwd compatibility
    def __init__(self, username=None, password=None, serverurl=None):
        self.username = username
        self.password = password
        self.verbose = False
        self.serverurl = serverurl
        if serverurl.startswith('http://'):
            type, uri = urllib.splittype(serverurl)
            host, path = urllib.splithost(uri)
            host, port = urllib.splitport(host)
            if port is None:
                port = 80
            else:
                port = int(port)
            def get_connection(host=host, port=port):
                return httplib.HTTPConnection(host, port)
            self._get_connection = get_connection
        elif serverurl.startswith('unix://'):
            def get_connection(serverurl=serverurl):
                # we use 'localhost' here because domain names must be
                # < 64 chars (or we'd use the serverurl filename)
                conn = UnixStreamHTTPConnection('localhost')
                conn.socketfile = serverurl[7:]
                return conn
            self._get_connection = get_connection
        else:
            raise ValueError('Unknown protocol for serverurl %s' % serverurl)

    def request(self, host, handler, request_body, verbose=0):
        if not self.connection:
            self.connection = self._get_connection()
            self.headers = {
                "User-Agent" : self.user_agent,
                "Content-Type" : "text/xml",
                "Accept": "text/xml"
                }
            
            # basic auth
            if self.username is not None and self.password is not None:
                unencoded = "%s:%s" % (self.username, self.password)
                encoded = unencoded.encode('base64')
                encoded = encoded.replace('\012', '')
                self.headers["Authorization"] = "Basic %s" % encoded
                
        self.headers["Content-Length"] = str(len(request_body))

        self.connection.request('POST', handler, request_body, self.headers)

        r = self.connection.getresponse()

        if r.status != 200:
            self.connection.close()
            self.connection = None
            raise xmlrpclib.ProtocolError(host + handler,
                                          r.status,
                                          r.reason,
                                          '' )
        data = r.read()
        p, u = self.getparser()
        p.feed(data)
        p.close()
        return u.close()    

class UnixStreamHTTPConnection(httplib.HTTPConnection):
    def connect(self):
        self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        # we abuse the host parameter as the socketname
        self.sock.connect(self.socketfile)

def gettags(comment):
    """ Parse documentation strings into JavaDoc-like tokens """

    tags = []

    tag = None
    datatype = None
    name = None
    tag_lineno = lineno = 0
    tag_text = []

    for line in comment.split('\n'):
        line = line.strip()
        if line.startswith("@"):
            tags.append((tag_lineno, tag, datatype, name, '\n'.join(tag_text)))
            parts = line.split(None, 3)
            if len(parts) == 1:
                datatype = ''
                name = ''
                tag_text = []
            elif len(parts) == 2:
                datatype = parts[1]
                name = ''
                tag_text = []
            elif len(parts) == 3:
                datatype = parts[1]
                name = parts[2]
                tag_text = []
            elif len(parts) == 4:
                datatype = parts[1]
                name = parts[2]
                tag_text = [parts[3].lstrip()]
            tag = parts[0][1:]
            tag_lineno = lineno
        else:
            if line:
                tag_text.append(line)
        lineno = lineno + 1

    tags.append((tag_lineno, tag, datatype, name, '\n'.join(tag_text)))

    return tags


try:
    # Python 2.6 contains a version of cElementTree inside it.
    from xml.etree.ElementTree import iterparse
except ImportError:
    try:
        # Failing that, try cElementTree instead.
        from cElementTree import iterparse
    except ImportError:
        iterparse = None


if iterparse is not None:
    import datetime, time
    from base64 import decodestring

    def make_datetime(text):
        return datetime.datetime(
            *time.strptime(text, "%Y%m%dT%H:%M:%S")[:6]
        )

    unmarshallers = {
        "int": lambda x: int(x.text),
        "i4": lambda x: int(x.text),
        "boolean": lambda x: x.text == "1",
        "string": lambda x: x.text or "",
        "double": lambda x: float(x.text),
        "dateTime.iso8601": lambda x: make_datetime(x.text),
        "array": lambda x: [v.text for v in x],
        "data": lambda x: x[0].text,
        "struct": lambda x: dict([(k.text or "", v.text) for k, v in x]),
        "base64": lambda x: decodestring(x.text or ""),
        "value": lambda x: x[0].text,
        "param": lambda x: x[0].text,
    }

    def loads(data):
        params = method = None
        for action, elem in iterparse(StringIO(data)):
            unmarshal = unmarshallers.get(elem.tag)
            if unmarshal:
                data = unmarshal(elem)
                elem.clear()
                elem.text = data
            elif elem.tag == "methodName":
                method = elem.text
            elif elem.tag == "params":
                params = tuple([v.text for v in elem])
        return params, method
else:
    loads = None

########NEW FILE########
__FILENAME__ = supervisorctl
#!/usr/bin/env python
#

from supervisor import supervisorctl

supervisorctl.main()

########NEW FILE########
__FILENAME__ = supervisord

from supervisor import supervisord

supervisord.main()

########NEW FILE########
__FILENAME__ = backup
import argparse
import os
import subprocess
import time

def parse_command_line():
  parser = argparse.ArgumentParser(
      formatter_class=argparse.ArgumentDefaultsHelpFormatter,
      description='A utility used to backup tank data')

  parser.add_argument('--hadoop_home', default=os.getcwd(),
      help='The local hadoop home directory')

  parser.add_argument('--cluster', default='lgprc-xiaomi',
      help='The hadoop cluster name')

  parser.add_argument('--backup_root', default='/user/h_tank',
      help='The backup root directory')

  parser.add_argument('--tank_home', default=os.getcwd(),
      help='The tank home directory')

  args = parser.parse_args()
  return args

def backup_sqlite(args):
  cmd = ['%s/bin/hdfs' % args.hadoop_home, 'dfs', '-mkdir',
    '-p', '%s/sqlite/' % args.backup_root]
  print cmd
  subprocess.check_call(cmd)

  cmd = ['%s/bin/hdfs' % args.hadoop_home, 'dfs', '-copyFromLocal',
    '%s/sqlite/tank.db' % args.tank_home,
    '%s/sqlite/tank.db.%d' % (args.backup_root, int(time.time()))]
  print cmd
  subprocess.check_call(cmd)

def backup_data(args):
  for dir in os.listdir('%s/data' % args.tank_home):
    if dir.startswith('.'):
      continue

    cmd = ['%s/bin/hdfs' % args.hadoop_home, 'dfs', '-mkdir',
      '-p', '%s/data/%s' % (args.backup_root, dir)]
    print cmd
    subprocess.check_call(cmd)

    tag_file = '%s/data/%s/tags' % (args.tank_home, dir)
    fp = open(tag_file, 'a+')
    print tag_file
    backed_dirs = [d.strip() for d in fp.readlines()]
    total_dirs = [d for d in os.listdir(
        '%s/data/%s' % (args.tank_home, dir)) if not d.startswith('.')]
    diff_dirs = list(set(total_dirs) - set(backed_dirs) - set(['tags']))

    for d in diff_dirs:
      # only backup package whose modification time is older than 30min
      mod_time = os.path.getmtime('%s/data/%s/%s' % (
            args.tank_home, dir, d))
      if time.time() - mod_time < 1800:
        continue

      cmd = ['%s/bin/hdfs' % args.hadoop_home, 'dfs', '-copyFromLocal',
        '%s/data/%s/%s' % (args.tank_home, dir, d),
        '%s/data/%s/' % (args.backup_root, dir)]
      print cmd
      subprocess.check_call(cmd)
      fp.write('%s\n' % d)

def main():
  args = parse_command_line()
  backup_sqlite(args)
  backup_data(args)

if __name__ == '__main__':
  main()

########NEW FILE########
__FILENAME__ = manage
import os
import sys

if __name__ == "__main__":
    os.environ.setdefault("DJANGO_SETTINGS_MODULE", "tank.settings")

    from django.core.management import execute_from_command_line

    execute_from_command_line(sys.argv)

########NEW FILE########
__FILENAME__ = forms
__author__ = 'wuzesheng'

from django import forms

class UploadFileForm(forms.Form):
  file = forms.FileField()
########NEW FILE########
__FILENAME__ = models
from django import forms
from django.db import models
from tank.settings import MEDIA_URL

# Create your models here.

def get_upload_path(instance, filename):
  return '%s/%s-%s/%s' % (instance.artifact, instance.revision,
      instance.timestamp, instance.name)

class Package(models.Model):
  artifact = models.CharField(max_length=128)
  name = models.CharField(max_length=256)
  revision = models.CharField(max_length=64)
  timestamp = models.CharField(max_length=128)
  checksum = models.CharField(max_length=128)
  file = models.FileField(upload_to=get_upload_path)

  def __unicode__(self):
    return u'%s %s %s %s %s' % (self.artifact, self.revision,
        self.timestamp, self.name, self.checksum)

  def __str__(self):
    field_json_str = "{" \
      "'artifact': '%s'," \
      "'package_name': '%s'," \
      "'revision': '%s'," \
      "'timestamp': '%s'," \
      "'checksum': '%s'" \
    "}" % (
        self.artifact, self.name,
        self.revision, self.timestamp,
        self.checksum)
    return field_json_str

  def download_link(self):
    return '%s/%s/%s-%s/%s' % (MEDIA_URL.rstrip('/'), self.artifact,
        self.revision, self.timestamp, self.name)

########NEW FILE########
__FILENAME__ = tests
"""
This file demonstrates writing tests using the unittest module. These will pass
when you run "manage.py test".

Replace this with more appropriate tests for your application.
"""

from django.test import TestCase


class SimpleTest(TestCase):
    def test_basic_addition(self):
        """
        Tests that 1 + 1 always equals 2.
        """
        self.assertEqual(1 + 1, 2)

########NEW FILE########
__FILENAME__ = views
# Create your views here.
import hashlib
import os
import time

from django.http import HttpResponse
from django.http import HttpResponseRedirect
from django.shortcuts import render_to_response
from django.views.decorators.csrf import csrf_exempt

from package_server.forms import UploadFileForm
from package_server.models import Package
from tank.settings import STATIC_URL

ITEM_LIMITS = 20

@csrf_exempt
def upload_package(request):
  if request.method == 'POST':
    form = UploadFileForm(request.POST, request.FILES)
    if form.is_valid():
      artifact = request.POST.get('artifact')
      revision_no = request.POST.get('revision')
      file_obj = request.FILES.get('file')

      error_message = str()
      if not artifact:
        error_message = 'Artifact should not be empty'
      elif not revision_no:
        error_message = 'Revison should not be empty'

      if error_message:
        return render_to_response('upload.html', {
          'error_message': error_message,
          'STATIC_URL': STATIC_URL.rstrip('/'),
        })
      else:
        package_name = os.path.basename(file_obj.name)
        checksum = generate_checksum(file_obj)
        time = generate_timestamp()
        file_obj.seek(0)
        package = Package(artifact=artifact, name=package_name,
            revision=revision_no, timestamp=time,
            checksum=checksum, file=file_obj)
        package.save()
        return render_to_response('upload.html', {
          'upload_success': True,
          'package': package,
          'STATIC_URL': STATIC_URL.rstrip('/'),
        })
  else:
    form = UploadFileForm()

  return render_to_response('upload.html', {
      'form': form,
      'STATIC_URL': STATIC_URL.rstrip('/'),
  })

def list_packages(request, page_no = 1):
  package_list = Package.objects.order_by('id').reverse()
  has_package = (len(package_list) > 0)
  return render_to_response('package_list.html', {
      'package_list': package_list,
      'has_package': has_package,
      'STATIC_URL': STATIC_URL.rstrip('/'),
  })

def check_package(request):
  artifact = request.GET.get('artifact')
  checksum = request.GET.get('checksum')

  package = get_package(artifact, checksum)
  if package:
    return HttpResponse(str(package))
  else:
    return HttpResponse('Package Not Found')

def get_latest_package_info(request):
  artifact = request.GET.get('artifact')
  package_name = request.GET.get('package_name')
  package = get_latest_package(artifact, package_name)
  if package:
    return HttpResponse(str(package))
  else:
    return HttpResponse('Package Not Found')

def generate_checksum(fp):
  sha1 = hashlib.sha1()
  while True:
    buffer = fp.read(4096)
    if not buffer: break
    sha1.update(buffer)
  return sha1.hexdigest()

def generate_timestamp():
  return time.strftime('%Y%m%d-%H%M%S')

def get_latest_package(artifact, package_name):
  if package_name:
    package_list = Package.objects.filter(
      artifact=artifact, name=package_name,
    ).order_by('id').reverse()
  else:
    package_list = Package.objects.filter(
      artifact=artifact,
    ).order_by('id').reverse()

  if len(package_list) > 0:
    return package_list[0]
  else:
    return None

def get_package(artifact, checksum):
  package_list = Package.objects.filter(
      artifact=artifact,
      checksum=checksum,
  )

  if len(package_list) > 0:
    return package_list[0]
  else:
    return None


########NEW FILE########
__FILENAME__ = settings
# Django settings for tank project.

DEBUG = True
TEMPLATE_DEBUG = DEBUG

ADMINS = (
    # ('Your Name', 'your_email@example.com'),
)

MANAGERS = ADMINS

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3', # Add 'postgresql_psycopg2', 'mysql', 'sqlite3' or 'oracle'.
        'NAME': 'sqlite/tank.db', # Or path to database file if using sqlite3.
        'USER': '',                      # Not used with sqlite3.
        'PASSWORD': '',                  # Not used with sqlite3.
        'HOST': '',                      # Set to empty string for localhost. Not used with sqlite3.
        'PORT': '',                      # Set to empty string for default. Not used with sqlite3.
    }
}

# Local time zone for this installation. Choices can be found here:
# http://en.wikipedia.org/wiki/List_of_tz_zones_by_name
# although not all choices may be available on all operating systems.
# In a Windows environment this must be set to your system time zone.
TIME_ZONE = 'Asia/Shanghai'

# Language code for this installation. All choices can be found here:
# http://www.i18nguy.com/unicode/language-identifiers.html
LANGUAGE_CODE = 'en-us'

SITE_ID = 1

# If you set this to False, Django will make some optimizations so as not
# to load the internationalization machinery.
USE_I18N = True

# If you set this to False, Django will not format dates, numbers and
# calendars according to the current locale.
USE_L10N = True

# If you set this to False, Django will not use timezone-aware datetimes.
USE_TZ = True

# Absolute filesystem path to the directory that will hold user-uploaded files.
# Example: "/home/media/media.lawrence.com/media/"
MEDIA_ROOT = 'data'

# URL that handles the media served from MEDIA_ROOT. Make sure to use a
# trailing slash.
# Examples: "http://media.lawrence.com/media/", "http://example.com/media/"
MEDIA_URL = '/packages/'

# Absolute path to the directory static files should be collected to.
# Don't put anything in this directory yourself; store your static files
# in apps' "static/" subdirectories and in STATICFILES_DIRS.
# Example: "/home/media/media.lawrence.com/static/"
STATIC_ROOT = 'static'

# URL prefix for static files.
# Example: "http://media.lawrence.com/static/"
STATIC_URL = '/static/'

# Additional locations of static files
STATICFILES_DIRS = (
    # Put strings here, like "/home/html/static" or "C:/www/django/static".
    # Always use forward slashes, even on Windows.
    # Don't forget to use absolute paths, not relative paths.
)

# List of finder classes that know how to find static files in
# various locations.
STATICFILES_FINDERS = (
    'django.contrib.staticfiles.finders.FileSystemFinder',
    'django.contrib.staticfiles.finders.AppDirectoriesFinder',
#    'django.contrib.staticfiles.finders.DefaultStorageFinder',
)

# Make this unique, and don't share it with anybody.
SECRET_KEY = 'gw@$79cw)@iuvyq6x9@#vtyad62nwq!10ba_48xo&amp;t_q6^*-h4'

# List of callables that know how to import templates from various sources.
TEMPLATE_LOADERS = (
    'django.template.loaders.filesystem.Loader',
    'django.template.loaders.app_directories.Loader',
#     'django.template.loaders.eggs.Loader',
)

MIDDLEWARE_CLASSES = (
    'django.middleware.common.CommonMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    # Uncomment the next line for simple clickjacking protection:
    # 'django.middleware.clickjacking.XFrameOptionsMiddleware',
)

ROOT_URLCONF = 'tank.urls'

# Python dotted path to the WSGI application used by Django's runserver.
WSGI_APPLICATION = 'tank.wsgi.application'

TEMPLATE_DIRS = ('templates',)

INSTALLED_APPS = (
    # 'django.contrib.auth',
    # 'django.contrib.contenttypes',
    # 'django.contrib.sessions',
    # 'django.contrib.sites',
    # 'django.contrib.messages',
    # 'django.contrib.staticfiles',
    # Uncomment the next line to enable the admin:
    # 'django.contrib.admin',
    # Uncomment the next line to enable admin documentation:
    # 'django.contrib.admindocs',
    'package_server',
)

# A sample logging configuration. The only tangible logging
# performed by this configuration is to send an email to
# the site admins on every HTTP 500 error when DEBUG=False.
# See http://docs.djangoproject.com/en/dev/topics/logging for
# more details on how to customize your logging configuration.
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'filters': {
        'require_debug_false': {
            '()': 'django.utils.log.RequireDebugFalse'
        }
    },
    'handlers': {
        'mail_admins': {
            'level': 'ERROR',
            'filters': ['require_debug_false'],
            'class': 'django.utils.log.AdminEmailHandler'
        }
    },
    'loggers': {
        'django.request': {
            'handlers': ['mail_admins'],
            'level': 'ERROR',
            'propagate': True,
        },
    }
}

########NEW FILE########
__FILENAME__ = urls
from django.conf.urls import patterns, include, url
from django.conf.urls.static import static
from django.conf import settings
from package_server.views import check_package
from package_server.views import get_latest_package_info
from package_server.views import list_packages
from package_server.views import upload_package

# Uncomment the next two lines to enable the admin:
from django.contrib import admin
admin.autodiscover()

urlpatterns = patterns('',
    # Examples:
    # url(r'^$', 'tank.views.home', name='home'),
    # url(r'^tank/', include('tank.foo.urls')),

    # Uncomment the admin/doc line below to enable admin documentation:
    # url(r'^admin/doc/', include('django.contrib.admindocs.urls')),

    # Uncomment the next line to enable the admin:
    # url(r'^admin/', include(admin.site.urls)),
    url(r'^$' , list_packages),
    url(r'^package_list/(\d*)$', list_packages),
    url(r'^upload_package/$', upload_package),
    url(r'^check_package/$', check_package),
    url(r'^get_latest_package_info/$', get_latest_package_info),
) + (static(settings.MEDIA_URL, document_root=settings.MEDIA_ROOT)
  + static(settings.STATIC_URL, document_root=settings.STATIC_ROOT))

########NEW FILE########
__FILENAME__ = wsgi
"""
WSGI config for tank project.

This module contains the WSGI application used by Django's development server
and any production WSGI deployments. It should expose a module-level variable
named ``application``. Django's ``runserver`` and ``runfcgi`` commands discover
this application via the ``WSGI_APPLICATION`` setting.

Usually you will have the standard Django WSGI application here, but it also
might make sense to replace the whole Django WSGI application with a custom one
that later delegates to the Django one. For example, you could introduce WSGI
middleware here, or combine a Django application with an application of another
framework.

"""
import os

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "tank.settings")

# This application object is used by any WSGI server configured to use this
# file. This includes Django's development server, if the WSGI_APPLICATION
# setting points here.
from django.core.wsgi import get_wsgi_application
application = get_wsgi_application()

# Apply WSGI middleware here.
# from helloworld.wsgi import HelloWorldApplication
# application = HelloWorldApplication(application)

########NEW FILE########
