__FILENAME__ = action_runner
#!/usr/bin/env python
"""
Write pid and stdout/stderr to a standard location before execing a command.
"""
import contextlib
import logging
import os
import subprocess
import sys
import yaml


log = logging.getLogger("tron.action_runner")


STATUS_FILE = 'status'


opener = open


class StatusFile(object):
    """Manage a status file."""
    def __init__(self, filename):
        self.filename = filename

    def write(self, command, proc):
        with opener(self.filename, 'w') as fh:
            yaml.dump(self.get_content(command, proc), fh)

    def get_content(self, command, proc):
        return {
            'command':      command,
            'pid':          proc.pid,
            'return_code':  proc.returncode
        }

    @contextlib.contextmanager
    def wrap(self, command, proc):
        self.write(command, proc)
        try:
            yield
        finally:
            self.write(command, proc)


class NoFile(object):

    @classmethod
    @contextlib.contextmanager
    def wrap(self, _command, _proc):
        yield


def get_status_file(output_path):
    if not os.path.isdir(output_path):
        try:
            os.makedirs(output_path)
        except OSError:
            log.warn("Output path %s does not exist", output_path)
            return NoFile
    return StatusFile(os.path.join(output_path, STATUS_FILE))


def register(output_path, command, proc):
    status_file = get_status_file(output_path)
    with status_file.wrap(command, proc):
        proc.wait()
    sys.exit(proc.returncode)


def parse_args(args):
    if len(args) != 3:
        raise SystemExit("Requires both output_path and command.")
    return args[1:]


def run_command(command):
    return subprocess.Popen(
        command, shell=True, stdout=sys.stdout, stderr=sys.stderr)


if __name__ == "__main__":
    logging.basicConfig()
    output_path, command = parse_args(sys.argv)
    proc = run_command(command)
    register(output_path, command, proc)

########NEW FILE########
__FILENAME__ = action_status
#!/usr/bin/env python
"""
Read values from a status file created by action_runner.py
"""
import functools
import logging
import signal
import sys
import os
import yaml


log = logging.getLogger('tron.action_status')


STATUS_FILE = 'status'


def print_field(field, status_file):
    sys.stdout.write(str(status_file[field]))


def print_status_file(status_file):
    yaml.dump(status_file, sys.stdout)


def send_signal(signal_num, status_file):
    pid = status_file['pid']
    try:
        os.kill(pid, signal_num)
    except OSError, e:
        msg = "Failed to signal %s with %s: %s"
        raise SystemExit(msg % (pid, signal_num, e))


commands = {
    'print':        print_status_file,
    'pid':          functools.partial(print_field, 'pid'),
    'return_code':  functools.partial(print_field, 'return_code'),
    'terminate':    functools.partial(send_signal, signal.SIGTERM),
    'kill':         functools.partial(send_signal, signal.SIGKILL),
}


def get_status_file(path):
    with open(path, 'r') as fh:
        return yaml.load(fh)


def parse_args(args):
    if len(args) != 3:
        raise SystemExit("Field and path are required")

    if args[2] not in commands:
        raise SystemExit("Unknown command %s" % args[2])

    return args[1:]


def run_command(command, status_file):
    commands[command](status_file)


if __name__ == "__main__":
    logging.basicConfig()
    path, command = parse_args(sys.argv)
    status_file = get_status_file(os.path.join(path, STATUS_FILE))
    run_command(command, status_file)
########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Tron documentation build configuration file, created by
# sphinx-quickstart on Mon Nov  7 18:05:54 2011.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

sys.path.insert(0, os.path.abspath('..'))

import tron

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.coverage']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Tron'
copyright = u'2011, Yelp, Inc.'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '.'.join(str(s) for s in tron.__version_info__[:2])
# The full version, including alpha/beta/rc tags.
release = tron.__version__

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'nature'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Trondoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Tron.tex', u'Tron Documentation',
   u'Yelp, Inc.', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('man_tronview', 'tronview', u'tronview documentation',
     [u'Yelp, Inc.'], 1),
    ('man_tronfig', 'tronfig', u'tronfig documentation',
     [u'Yelp, Inc.'], 1),
    ('man_tronctl', 'tronctl', u'control Tron jobs and services',
     [u'Yelp, Inc.'], 1),
    ('man_trond', 'trond', u'trond documentation',
     [u'Yelp, Inc.'], 8),
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'Tron', u'Tron Documentation',
   u'Yelp, Inc.', 'Tron', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = actioncommand_test
import mock
from testify import TestCase, assert_equal, setup
from testify.assertions import assert_not_equal
from tests.testingutils import autospec_method
from tron import actioncommand

from tron.actioncommand import ActionCommand
from tron.config import schema
from tron.serialize import filehandler


class ActionCommandTestCase(TestCase):

    @setup
    def setup_command(self):
        self.serializer = mock.create_autospec(filehandler.FileHandleManager)
        self.serializer.open.return_value = filehandler.NullFileHandle
        self.ac = ActionCommand("action.1.do", "do", self.serializer)

    def test_init(self):
        assert_equal(self.ac.state, ActionCommand.PENDING)

    def test_init_no_serializer(self):
        ac = ActionCommand("action.1.do", "do")
        ac.write_stdout("something")
        ac.write_stderr("else")
        assert_equal(ac.stdout, filehandler.NullFileHandle)
        ac.done()

    def test_started(self):
        assert self.ac.started()
        assert self.ac.start_time is not None
        assert_equal(self.ac.state, ActionCommand.RUNNING)

    def test_started_already_started(self):
        self.ac.started()
        assert not self.ac.started()

    def test_exited(self):
        self.ac.started()
        assert self.ac.exited(123)
        assert_equal(self.ac.exit_status, 123)
        assert self.ac.end_time is not None

    def test_exited_from_pending(self):
        assert self.ac.exited(123)
        assert_equal(self.ac.state, ActionCommand.FAILSTART)

    def test_exited_bad_state(self):
        self.ac.started()
        self.ac.exited(123)
        assert not self.ac.exited(1)

    def test_write_stderr_no_fh(self):
        message = "this is the message"
        # Test without a stderr
        self.ac.write_stderr(message)

    def test_write_stderr(self):
        message = "this is the message"
        serializer = mock.create_autospec(filehandler.FileHandleManager)
        fh = serializer.open.return_value = mock.create_autospec(
            filehandler.FileHandleWrapper)
        ac = ActionCommand("action.1.do", "do", serializer)

        ac.write_stderr(message)
        fh.write.assert_called_with(message)

    def test_done(self):
        self.ac.started()
        self.ac.exited(123)
        assert self.ac.done()

    def test_done_bad_state(self):
        assert not self.ac.done()

    def test_handle_errback(self):
        message = "something went wrong"
        self.ac.handle_errback(message)
        assert_equal(self.ac.state, ActionCommand.FAILSTART)
        assert self.ac.end_time

    def test_is_failed(self):
        self.ac.exit_status = 0
        assert not self.ac.is_failed

    def test_is_failed_true(self):
        self.ac.exit_status = 255
        assert self.ac.is_failed

    def test_is_complete(self):
        assert not self.ac.is_complete

    def test_is_complete_true(self):
        self.ac.machine.state = self.ac.COMPLETE
        assert self.ac.is_complete, self.ac.machine.state

    def test_is_done(self):
        self.ac.machine.state = self.ac.FAILSTART
        assert self.ac.is_done, self.ac.machine.state
        self.ac.machine.state = self.ac.COMPLETE
        assert self.ac.is_done, self.ac.machine.state


class CreateActionCommandFactoryFromConfigTestCase(TestCase):

    def test_create_default_action_command_no_config(self):
        config = ()
        factory = actioncommand.create_action_runner_factory_from_config(config)
        assert_equal(factory, actioncommand.NoActionRunnerFactory)

    def test_create_default_action_command(self):
        config = schema.ConfigActionRunner('none', None, None)
        factory = actioncommand.create_action_runner_factory_from_config(config)
        assert_equal(factory, actioncommand.NoActionRunnerFactory)

    def test_create_action_command_with_simple_runner(self):
        status_path, exec_path = '/tmp/what', '/remote/bin'
        config = schema.ConfigActionRunner('subprocess', status_path, exec_path)
        factory = actioncommand.create_action_runner_factory_from_config(config)
        assert_equal(factory.status_path, status_path)
        assert_equal(factory.exec_path, exec_path)


class SubprocessActionRunnerFactoryTestCase(TestCase):

    @setup
    def setup_factory(self):
        self.status_path = 'status_path'
        self.exec_path = 'exec_path'
        self.factory = actioncommand.SubprocessActionRunnerFactory(
            self.status_path, self.exec_path)

    def test_from_config(self):
        config = mock.Mock()
        runner_factory = actioncommand.SubprocessActionRunnerFactory.from_config(
            config)
        assert_equal(runner_factory.status_path, config.remote_status_path)
        assert_equal(runner_factory.exec_path, config.remote_exec_path)

    def test_create(self):
        serializer = mock.create_autospec(actioncommand.StringBufferStore)
        id, command = 'id', 'do a thing'
        autospec_method(self.factory.build_command)
        action_command = self.factory.create(id, command, serializer)
        assert_equal(action_command.id, id)
        assert_equal(action_command.command, self.factory.build_command.return_value)
        assert_equal(action_command.stdout, serializer.open.return_value)
        assert_equal(action_command.stderr, serializer.open.return_value)

    def test_build_command(self):
        id, command, exec_name = 'id', 'do a thing', 'exec_name'
        actual = self.factory.build_command(id, command, exec_name)
        expected = '%s/%s "%s/%s" "%s"' % (
            self.exec_path, exec_name, self.status_path, id, command)
        assert_equal(actual, expected)

    def test_build_stop_action_command(self):
        id, command = 'id', 'do a thing'
        autospec_method(self.factory.build_command)
        action_command = self.factory.build_stop_action_command(id, command)
        assert_equal(action_command.id,
            '%s.%s' % (id, self.factory.build_command.return_value))
        assert_equal(action_command.command,
            self.factory.build_command.return_value)

    def test__eq__true(self):
        first = actioncommand.SubprocessActionRunnerFactory('a', 'b')
        second = actioncommand.SubprocessActionRunnerFactory('a', 'b')
        assert_equal(first, second)

    def test__eq__false(self):
        first = actioncommand.SubprocessActionRunnerFactory('a', 'b')
        second = actioncommand.SubprocessActionRunnerFactory('a', 'c')
        assert_not_equal(first, second)
        assert_not_equal(first, None)
        assert_not_equal(first, actioncommand.NoActionRunnerFactory)

########NEW FILE########
__FILENAME__ = adapter_test
import shutil
import tempfile
import mock
from testify import TestCase, assert_equal, run, setup, teardown
from tests import mocks
from tests.assertions import assert_length
from tron import node, scheduler
from tron.api import adapter
from tron.api.adapter import ReprAdapter, RunAdapter, ActionRunAdapter
from tron.api.adapter import JobRunAdapter, ServiceAdapter
from tron.core import actionrun, job


class MockAdapter(ReprAdapter):

    field_names = ['one', 'two']
    translated_field_names = ['three', 'four']

    def get_three(self):
        return 3

    def get_four(self):
        return 4


class ReprAdapterTestCase(TestCase):

    @setup
    def setup_adapter(self):
        self.original           = mock.Mock(one=1, two=2)
        self.adapter            = MockAdapter(self.original)

    def test__init__(self):
        assert_equal(self.adapter._obj, self.original)
        assert_equal(self.adapter.fields, MockAdapter.field_names)

    def test_get_translation_mapping(self):
        expected = {
            'three': self.adapter.get_three,
            'four':  self.adapter.get_four
        }
        assert_equal(self.adapter.translators, expected)

    def test_get_repr(self):
        expected = dict(one=1, two=2, three=3, four=4)
        assert_equal(self.adapter.get_repr(), expected)


class SampleClassStub(object):

    def __init__(self):
        self.true_flag = True
        self.false_flag = False

    @adapter.toggle_flag('true_flag')
    def expects_true(self):
        return "This is true"

    @adapter.toggle_flag('false_flag')
    def expects_false(self):
        return "This is false"


class ToggleFlagTestCase(TestCase):

    @setup
    def setup_stub(self):
        self.stub = SampleClassStub()

    def test_toggle_flag_true(self):
        assert_equal(self.stub.expects_true(), "This is true")

    def test_toggle_flag_false(self):
        assert not self.stub.expects_false()


class RunAdapterTestCase(TestCase):

    @setup
    def setup_adapter(self):
        self.original           = mock.Mock()
        self.adapter            = RunAdapter(self.original)

    def test_get_state(self):
        assert_equal(self.adapter.get_state(), self.original.state.name)

    @mock.patch('tron.api.adapter.NodeAdapter', autospec=True)
    def test_get_node(self, mock_node_adapter):
        assert_equal(self.adapter.get_node(),
            mock_node_adapter.return_value.get_repr.return_value)
        mock_node_adapter.assert_called_with(self.original.node)

    def test_get_duration(self):
        self.original.start_time = None
        assert_equal(self.adapter.get_duration(), '')


class ActionRunAdapterTestCase(TestCase):

    @setup
    def setup_adapter(self):
        self.temp_dir = tempfile.mkdtemp()
        self.action_run = mock.MagicMock()
        self.job_run = mock.MagicMock()
        self.adapter = ActionRunAdapter(self.action_run, self.job_run, 4)

    @teardown
    def teardown_adapter(self):
        shutil.rmtree(self.temp_dir)

    def test__init__(self):
        assert_equal(self.adapter.max_lines, 4)
        assert_equal(self.adapter.job_run, self.job_run)
        assert_equal(self.adapter._obj, self.action_run)

    def test_get_repr(self):
        result = self.adapter.get_repr()
        assert_equal(result['command'], self.action_run.rendered_command)


class ActionRunGraphAdapterTestCase(TestCase):

    @setup
    def setup_adapter(self):
        self.action_runs = mock.create_autospec(
            actionrun.ActionRunCollection,
            action_graph=mock.MagicMock())
        self.adapter = adapter.ActionRunGraphAdapter(self.action_runs)
        self.action_run = mock.MagicMock()
        self.action_runs.__iter__.return_value = [self.action_run]

    def test_get_repr(self):
        result = self.adapter.get_repr()
        assert_equal(len(result), 1)
        assert_equal(self.action_run.id, result[0]['id'])


class JobRunAdapterTestCase(TestCase):

    @setup
    def setup_adapter(self):
        action_runs = mock.MagicMock()
        action_runs.__iter__.return_value = iter([mock.Mock(), mock.Mock()])
        self.job_run = mock.Mock(
                action_runs=action_runs, action_graph=mocks.MockActionGraph())
        self.adapter = JobRunAdapter(self.job_run, include_action_runs=True)

    def test__init__(self):
        assert self.adapter.include_action_runs

    def test_get_runs(self):
        with mock.patch('tron.api.adapter.ActionRunAdapter'):
            assert_length(self.adapter.get_runs(), 2)

    def test_get_runs_without_action_runs(self):
        self.adapter.include_action_runs = False
        assert_equal(self.adapter.get_runs(), None)


class ServiceAdapterTestCase(TestCase):

    @setup
    def setup_adapter(self):
        self.service = mock.MagicMock()
        self.adapter = ServiceAdapter(self.service)

    @mock.patch('tron.api.adapter.NodePoolAdapter', autospec=True)
    def test_repr(self, mock_node_pool_adapter):
        result = self.adapter.get_repr()
        assert_equal(result['name'], self.service.name)
        assert_equal(result['node_pool'],
            mock_node_pool_adapter.return_value.get_repr.return_value)


class NodeAdapterTestCase(TestCase):

    @setup
    def setup_adapter(self):
        self.node = mock.create_autospec(node.Node)
        self.adapter = adapter.NodeAdapter(self.node)

    def test_repr(self):
        result = self.adapter.get_repr()
        assert_equal(result['hostname'], self.node.hostname)
        assert_equal(result['username'], self.node.username)


class NodePoolAdapterTestCase(TestCase):

    @setup
    def setup_adapter(self):
        self.pool = mock.create_autospec(node.NodePool)
        self.adapter = adapter.NodePoolAdapter(self.pool)

    @mock.patch('tron.api.adapter.adapt_many', autospec=True)
    def test_repr(self, mock_many):
        result = self.adapter.get_repr()
        assert_equal(result['name'], self.pool.get_name.return_value)
        mock_many.assert_called_with(adapter.NodeAdapter,
            self.pool.get_nodes.return_value)


class JobIndexAdapterTestCase(TestCase):

    @setup
    def setup_adapter(self):
        self.job = mock.create_autospec(job.Job)
        self.adapter = adapter.JobIndexAdapter(self.job)

    def test_repr(self):
        result = self.adapter.get_repr()
        self.job.get_runs.assert_called_with()
        runs = self.job.get_runs.return_value
        runs.get_newest.assert_called_with()
        expected = {
            'name': self.job.get_name.return_value,
            'actions': [],
        }
        assert_equal(result, expected)

    def test_get_actions(self):
        action_run = mock.Mock()
        job_run = self.job.get_runs.return_value.get_newest.return_value
        job_run.action_runs.__iter__.return_value = [action_run]
        result = self.adapter.get_actions()
        expected = {
            'name': action_run.action_name,
            'command': action_run.bare_command
        }
        assert_equal(result, [expected])

    def test_get_actions_no_runs(self):
        self.job.get_runs.return_value.get_newest.return_value = None
        result = self.adapter.get_actions()
        assert_equal(result, [])

class SchedulerAdapterTestCase(TestCase):

    @setup
    def setup_adapter(self):
        self.scheduler = mock.create_autospec(scheduler.GeneralScheduler)
        self.adapter = adapter.SchedulerAdapter(self.scheduler)

    @mock.patch('tron.api.adapter.scheduler.get_jitter_str', autospec=True)
    def test_repr(self, mock_get_jitter):
        result = self.adapter.get_repr()
        expected = {
            'type': self.scheduler.get_name.return_value,
            'value': self.scheduler.get_value.return_value,
            'jitter': mock_get_jitter.return_value
        }
        assert_equal(result, expected)
        mock_get_jitter.assert_called_with(self.scheduler.get_jitter())

if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = controller_test
import mock

from testify import setup, TestCase, run, assert_equal
from testify.assertions import assert_in
from tests.testingutils import autospec_method
from tron import mcp
from tron.api import controller

from tron.api.controller import JobCollectionController, ConfigController
from tron.config import ConfigError, manager, config_parse
from tron.core import job, service, jobrun, actionrun


class JobCollectionControllerTestCase(TestCase):

    @setup
    def setup_controller(self):
        self.collection     = mock.create_autospec(job.JobCollection,
                                enable=mock.Mock(), disable=mock.Mock())
        self.controller     = JobCollectionController(self.collection)

    def test_handle_command_enabled(self):
        self.controller.handle_command('enableall')
        self.collection.enable.assert_called_with()

    def test_handle_command_disable(self):
        self.controller.handle_command('disableall')
        self.collection.disable.assert_called_with()


class ActionRunControllerTestCase(TestCase):

    @setup
    def setup_controller(self):
        self.action_run = mock.create_autospec(actionrun.ActionRun,
            cancel=mock.Mock())
        self.job_run = mock.create_autospec(jobrun.JobRun)
        self.job_run.is_scheduled = False
        self.controller = controller.ActionRunController(
            self.action_run, self.job_run)

    def test_handle_command_start_failed(self):
        self.job_run.is_scheduled = True
        result = self.controller.handle_command('start')
        assert not self.action_run.start.mock_calls
        assert_in("can not be started", result)

    def test_handle_command_mapped_command(self):
        result = self.controller.handle_command('cancel')
        self.action_run.cancel.assert_called_with()
        assert_in("now in state", result)

    def test_handle_command_mapped_command_failed(self):
        self.action_run.cancel.return_value = False
        result = self.controller.handle_command('cancel')
        self.action_run.cancel.assert_called_with()
        assert_in("Failed to cancel", result)

    def test_handle_termination_not_implemented(self):
        self.action_run.stop.side_effect = NotImplementedError
        result = self.controller.handle_termination('stop')
        assert_in("Failed to stop", result)

    def test_handle_termination_success(self):
        result = self.controller.handle_termination('kill')
        assert_in("Attempting to kill", result)



class JobRunControllerTestCase(TestCase):

    @setup
    def setup_controller(self):
        self.job_run = mock.create_autospec(jobrun.JobRun,
            run_time=mock.Mock(), cancel=mock.Mock())
        self.job_scheduler = mock.create_autospec(job.JobScheduler)
        self.controller = controller.JobRunController(
            self.job_run, self.job_scheduler)

    def test_handle_command_restart(self):
        self.controller.handle_command('restart')
        self.job_scheduler.manual_start.assert_called_with(self.job_run.run_time)

    def test_handle_mapped_command(self):
        result = self.controller.handle_command('start')
        self.job_run.start.assert_called_with()
        assert_in('now in state', result)

    def test_handle_mapped_command_failure(self):
        self.job_run.cancel.return_value = False
        result = self.controller.handle_command('cancel')
        self.job_run.cancel.assert_called_with()
        assert_in('Failed to cancel', result)


class JobControllerTestCase(TestCase):

    @setup
    def setup_controller(self):
        self.job_scheduler = mock.create_autospec(job.JobScheduler)
        self.controller = controller.JobController(self.job_scheduler)

    def test_handle_command_enable(self):
        self.controller.handle_command('enable')
        self.job_scheduler.enable.assert_called_with()

    def test_handle_command_disable(self):
        self.controller.handle_command('disable')
        self.job_scheduler.disable.assert_called_with()

    def test_handle_command_start(self):
        run_time = mock.Mock()
        self.controller.handle_command('start', run_time)
        self.job_scheduler.manual_start.assert_called_with(run_time=run_time)


class ServiceInstanceControllerTestCase(TestCase):

    @setup
    def setup_controller(self):
        self.instance = mock.create_autospec(service.ServiceInstance)
        self.controller = controller.ServiceInstanceController(self.instance)

    def test_handle_command_stop(self):
        response = self.controller.handle_command('stop')
        self.instance.stop.assert_called_with()
        assert_in("stopping", response)

    def test_handle_command_stop_failure(self):
        self.instance.stop.return_value = False
        response = self.controller.handle_command('stop')
        self.instance.stop.assert_called_with()
        assert_in("Failed to stop", response)

    def test_handle_command_start(self):
        response = self.controller.handle_command('start')
        self.instance.start.assert_called_with()
        assert_in("starting", response)

    def test_handle_command_start_failure(self):
        self.instance.start.return_value = False
        response = self.controller.handle_command('start')
        self.instance.start.assert_called_with()
        assert_in("Failed to start", response)


class ServiceControllerTestCase(TestCase):

    @setup
    def setup_controller(self):
        self.service = mock.create_autospec(service.Service)
        self.controller = controller.ServiceController(self.service)

    def test_handle_command_stop(self):
        self.controller.handle_command('stop')
        self.service.disable.assert_called_with()

    def test_handle_command_start(self):
        self.controller.handle_command('start')
        self.service.enable.assert_called_with()


class ConfigControllerTestCase(TestCase):

    @setup
    def setup_controller(self):
        self.mcp = mock.create_autospec(mcp.MasterControlProgram)
        self.manager = mock.create_autospec(manager.ConfigManager)
        self.mcp.get_config_manager.return_value = self.manager
        self.controller = ConfigController(self.mcp)

    def test_render_template(self):
        config_content = "asdf asdf"
        container = self.manager.load.return_value = mock.create_autospec(
            config_parse.ConfigContainer)
        container.get_node_names.return_value = ['one', 'two', 'three']
        container.get_master.return_value.command_context = {'zing': 'stars'}
        content = self.controller.render_template(config_content)
        assert_in('# one\n# three\n# two\n', content)
        assert_in('# %-30s: %s' % ('zing', 'stars'), content)
        assert_in(config_content, content)

    def test_strip_header_master(self):
        name, content = 'MASTER', mock.Mock()
        assert_equal(self.controller.strip_header(name, content), content)

    def test_strip_header_named(self):
        expected = "\nthing"
        name, content = 'something', self.controller.TEMPLATE + expected
        assert_equal(self.controller.strip_header(name, content), expected)

    def test_strip_header_named_missing(self):
        name, content = 'something', 'whatever content'
        assert_equal(self.controller.strip_header(name, content), content)

    def test_get_config_content_new(self):
        self.manager.__contains__.return_value = False
        content = self.controller._get_config_content('name')
        assert_equal(content, self.controller.DEFAULT_NAMED_CONFIG)
        assert not self.manager.read_raw_config.call_count

    def test_get_config_content_old(self):
        self.manager.__contains__.return_value = True
        name = 'the_name'
        content = self.controller._get_config_content(name)
        assert_equal(content, self.manager.read_raw_config.return_value)
        self.manager.read_raw_config.assert_called_with(name)

    def test_read_config_master(self):
        self.manager.__contains__.return_value = True
        name = 'MASTER'
        resp = self.controller.read_config(name)
        self.manager.read_raw_config.assert_called_with(name)
        self.manager.get_hash.assert_called_with(name)
        assert_equal(resp['config'], self.manager.read_raw_config.return_value)
        assert_equal(resp['hash'], self.manager.get_hash.return_value)

    def test_read_config_named(self):
        name = 'some_name'
        autospec_method(self.controller._get_config_content)
        autospec_method(self.controller.render_template)
        resp = self.controller.read_config(name)
        self.controller._get_config_content.assert_called_with(name)
        self.controller.render_template.assert_called_with(
            self.controller._get_config_content.return_value)
        assert_equal(resp['config'], self.controller.render_template.return_value)
        assert_equal(resp['hash'], self.manager.get_hash.return_value)

    def test_read_config_no_header(self):
        name = 'some_name'
        autospec_method(self.controller._get_config_content)
        autospec_method(self.controller.render_template)
        resp = self.controller.read_config(name, add_header=False)
        assert not self.controller.render_template.called
        assert_equal(resp['config'], self.controller._get_config_content.return_value)

    def test_update_config(self):
        autospec_method(self.controller.strip_header)
        name, content, config_hash = None, mock.Mock(), mock.Mock()
        self.manager.get_hash.return_value = config_hash
        assert not self.controller.update_config(name, content, config_hash)
        striped_content = self.controller.strip_header.return_value
        self.manager.write_config.assert_called_with(name, striped_content)
        self.mcp.reconfigure.assert_called_with()
        self.controller.strip_header.assert_called_with(name, content)
        self.manager.get_hash.assert_called_with(name)

    def test_update_config_failure(self):
        autospec_method(self.controller.strip_header)
        striped_content = self.controller.strip_header.return_value
        name, content, config_hash = None, mock.Mock(), mock.Mock()
        self.manager.get_hash.return_value = config_hash
        self.manager.write_config.side_effect = ConfigError("It broke")
        error = self.controller.update_config(name, striped_content, config_hash)
        assert_equal(error, "It broke")
        self.manager.write_config.assert_called_with(name, striped_content)
        assert not self.mcp.reconfigure.call_count

    def test_update_config_hash_mismatch(self):
        name, content, config_hash = None, mock.Mock(), mock.Mock()
        error = self.controller.update_config(name, content, config_hash)
        assert_equal(error, "Configuration has changed. Please try again.")

    def test_get_namespaces(self):
        result = self.controller.get_namespaces()
        self.manager.get_namespaces.assert_called_with()
        assert_equal(result, self.manager.get_namespaces.return_value)

if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = requestargs_test
import datetime
from testify import TestCase, run, assert_equal, setup
from tests.testingutils import Turtle

from tron.api.requestargs import get_integer, get_string, get_bool, get_datetime


class RequestArgsTestCase(TestCase):

    @setup
    def setup_args(self):
        self.args = {
            'number':   ['123'],
            'string':   ['astring'],
            'boolean':  ['1'],
            'datetime': ['2012-03-14 15:09:26']
        }
        self.datetime = datetime.datetime(2012, 3, 14, 15, 9, 26)
        self.request = Turtle(args=self.args)

    def _add_arg(self, name, value):
        if name not in self.args:
            self.args[name] = []
        self.args[name].append(value)

    def test_get_integer_valid_int(self):
        self._add_arg('number', '5')
        assert_equal(get_integer(self.request, 'number'), 123)

    def test_get_integer_invalid_int(self):
        self._add_arg('nan', 'beez')
        assert not get_integer(self.request, 'nan')

    def test_get_integer_missing(self):
        assert not get_integer(self.request, 'missing')

    def test_get_string(self):
        self._add_arg('string', 'bogus')
        assert_equal(get_string(self.request, 'string'), 'astring')

    def test_get_string_missing(self):
        assert not get_string(self.request, 'missing')

    def test_get_bool(self):
        assert get_bool(self.request, 'boolean')

    def test_get_bool_false(self):
        self._add_arg('false', '0')
        assert not get_bool(self.request, 'false')

    def test_get_bool_missing(self):
        assert not get_bool(self.request, 'missing')

    def test_get_datetime_valid(self):
        assert_equal(get_datetime(self.request, 'datetime'), self.datetime)

    def test_get_datetime_invalid(self):
        self._add_arg('nope', '2012-333-4')
        assert not get_datetime(self.request, 'nope')

    def test_get_datetime_missing(self):
        assert not get_datetime(self.request, 'missing')


if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = resource_test
"""
Test cases for the web services interface to tron
"""
import mock
import twisted.web.resource
import twisted.web.http
import twisted.web.server

from testify import TestCase, class_setup, assert_equal, run, setup
from testify import teardown
from testify import setup_teardown
from tests import mocks
from twisted.web import http
from tests.assertions import assert_call
from tron import event, node
from tron import mcp
from tron.api import resource as www, controller
from tests.testingutils import Turtle, autospec_method
from tron.core import service, serviceinstance, job, jobrun


REQUEST = twisted.web.server.Request(mock.Mock(), None)
REQUEST.childLink = lambda val : "/jobs/%s" % val


def build_request(**kwargs):
    args = dict((k, [v]) for k, v in kwargs.iteritems())
    return mock.create_autospec(twisted.web.server.Request, args=args)


class WWWTestCase(TestCase):
    """Patch www.response to not json encode."""

    @setup_teardown
    def mock_respond(self):
        with mock.patch('tron.api.resource.respond', autospec=True) as self.respond:
            self.respond.side_effect = lambda _req, output, code=None: output
            yield

    @setup
    def setup_request(self):
        self.request = build_request()


class HandleCommandTestCase(TestCase):

    @setup_teardown
    def mock_respond(self):
        with mock.patch('tron.api.resource.respond', autospec=True) as self.respond:
            yield

    def test_handle_command_unknown(self):
        command = 'the command'
        request = build_request(command=command)
        mock_controller, obj = mock.Mock(), mock.Mock()
        error = controller.UnknownCommandError("No")
        mock_controller.handle_command.side_effect = error
        response = www.handle_command(request, mock_controller, obj)
        mock_controller.handle_command.assert_called_with(command)
        assert_equal(response, self.respond.return_value)
        self.respond.assert_called_with(request, {'error': str(error)},
            code=http.NOT_IMPLEMENTED)

    def test_handle_command(self):
        command = 'the command'
        request = build_request(command=command)
        mock_controller, obj = mock.Mock(), mock.Mock()
        response = www.handle_command(request, mock_controller, obj)
        mock_controller.handle_command.assert_called_with(command)
        assert_equal(response, self.respond.return_value)
        self.respond.assert_called_with(request,
                {'result': mock_controller.handle_command.return_value})


class ActionRunResourceTestCase(WWWTestCase):

    @setup
    def setup_resource(self):
        self.job_run = mock.MagicMock()
        self.action_run = mock.MagicMock(output_path=['one'])
        self.resource = www.ActionRunResource(self.action_run, self.job_run)

    def test_render_GET(self):
        request = build_request(num_lines="12")
        response = self.resource.render_GET(request)
        assert_equal(response['id'], self.action_run.id)


class JobrunResourceTestCase(WWWTestCase):

    @setup
    def setup_resource(self):
        self.job_run = mock.MagicMock()
        self.job_scheduler = mock.Mock()
        self.resource = www.JobRunResource(self.job_run, self.job_scheduler)

    def test_render_GET(self):
        response = self.resource.render_GET(self.request)
        assert_equal(response['id'], self.job_run.id)


class ApiRootResourceTestCase(WWWTestCase):

    @setup
    def build_resource(self):
        self.mcp = mock.create_autospec(mcp.MasterControlProgram)
        self.resource = www.ApiRootResource(self.mcp)

    def test__init__(self):
        expected_children = ['jobs', 'services', 'config', 'status', 'events', '']
        assert_equal(set(expected_children), set(self.resource.children))

    def test_render_GET(self):
        expected_keys = [ 'jobs', 'services', 'namespaces', ]
        response = self.resource.render_GET(build_request())
        assert_equal(set(response.keys()), set(expected_keys))
        self.mcp.get_job_collection().get_jobs.assert_called_with()
        self.mcp.get_service_collection.return_value.get_names.assert_called_with()


class RootResourceTestCase(WWWTestCase):

    @setup
    def build_resource(self):
        self.web_path = '/bogus/path'
        self.mcp = mock.create_autospec(mcp.MasterControlProgram)
        self.resource = www.RootResource(self.mcp, self.web_path)

    def test_render_GET(self):
        request = build_request()
        response = self.resource.render_GET(request)
        assert_equal(response, 1)
        assert_equal(request.redirect.call_count, 1)
        request.finish.assert_called_with()

    def test_get_children(self):
        assert_equal(set(self.resource.children), set(['api', 'web', '']))


class ActionRunHistoryResourceTestCase(WWWTestCase):

    @setup
    def setup_resource(self):
        self.action_runs = [mock.MagicMock(), mock.MagicMock()]
        self.resource = www.ActionRunHistoryResource(self.action_runs)

    def test_render_GET(self):
        response = self.resource.render_GET(self.request)
        assert_equal(len(response), len(self.action_runs))


class JobCollectionResourceTestCase(WWWTestCase):

    @class_setup
    def build_resource(self):
        self.job = mock.Mock(
            repr_data=lambda: {'name': 'testname'},
            name="testname",
            last_success=None,
            runs=mock.Mock(),
            scheduler_str="testsched",
            node_pool=mocks.MockNodePool()
        )
        self.job_collection = mock.create_autospec(job.JobCollection)
        self.resource = www.JobCollectionResource(self.job_collection)

    def test_render_GET(self):
        self.resource.get_data = Turtle()
        result = self.resource.render_GET(REQUEST)
        assert_call(self.resource.get_data, 0, False, False)
        assert 'jobs' in result

    def test_getChild(self):
        child = self.resource.getChild("testname", mock.Mock())
        assert isinstance(child, www.JobResource)
        self.job_collection.get_by_name.assert_called_with("testname")

    def test_getChild_missing_job(self):
        self.job_collection.get_by_name.return_value = None
        child = self.resource.getChild("bar", mock.Mock())
        assert isinstance(child, twisted.web.resource.NoResource)


class JobResourceTestCase(WWWTestCase):

    @setup
    def setup_resource(self):
        self.job_scheduler = mock.create_autospec(job.JobScheduler)
        self.job_runs = mock.create_autospec(jobrun.JobRunCollection)
        self.job = mock.create_autospec(job.Job,
            runs=self.job_runs,
            all_nodes=False,
            allow_overlap=True,
            queueing=True,
            action_graph=mock.MagicMock(),
            scheduler=mock.Mock(),
            node_pool=mock.create_autospec(node.NodePool),
            max_runtime=mock.Mock())
        self.job_scheduler.get_job.return_value = self.job
        self.job_scheduler.get_job_runs.return_value = self.job_runs
        self.resource = www.JobResource(self.job_scheduler)

    def test_render_GET(self):
        result = self.resource.render_GET(self.request)
        assert_equal(result['name'], self.job_scheduler.get_job().get_name())

    def test_get_run_from_identifier_HEAD(self):
        job_run = self.resource.get_run_from_identifier('HEAD')
        self.job_scheduler.get_job_runs.assert_called_with()
        assert_equal(job_run, self.job_runs.get_newest.return_value)

    def test_get_run_from_identifier_number(self):
        job_run = self.resource.get_run_from_identifier('3')
        self.job_scheduler.get_job_runs.assert_called_with()
        assert_equal(job_run, self.job_runs.get_run_by_num.return_value)
        self.job_runs.get_run_by_num.assert_called_with(3)

    def test_get_run_from_identifier_state_name(self):
        job_run = self.resource.get_run_from_identifier('SUCC')
        assert_equal(job_run, self.job_runs.get_run_by_state_short_name.return_value)
        self.job_runs.get_run_by_state_short_name.assert_called_with('SUCC')

    def test_get_run_from_identifier_negative_index(self):
        job_run = self.resource.get_run_from_identifier('-2')
        assert_equal(job_run, self.job_runs.get_run_by_index.return_value)
        self.job_runs.get_run_by_index.assert_called_with(-2)

    def test_getChild(self):
        autospec_method(self.resource.get_run_from_identifier)
        identifier = 'identifier'
        resource = self.resource.getChild(identifier, None)
        assert_equal(resource.job_run,
            self.resource.get_run_from_identifier.return_value)

    def test_getChild_action_run_history(self):
        autospec_method(self.resource.get_run_from_identifier, return_value=None)
        action_name = 'action_name'
        action_runs = [mock.Mock(), mock.Mock()]
        self.job.action_graph.names = [action_name]
        self.job.runs.get_action_runs.return_value = action_runs
        resource = self.resource.getChild(action_name, None)
        assert_equal(resource.__class__, www.ActionRunHistoryResource)
        assert_equal(resource.action_runs, action_runs)


class ServiceResourceTestCase(WWWTestCase):

    @setup
    def setup_resource(self):
        instances = mock.create_autospec(
            serviceinstance.ServiceInstanceCollection,
            node_pool=mock.create_autospec(node.NodePool))
        self.service = mock.create_autospec(service.Service,
            instances=instances,
            enabled=True,
            config=mock.Mock())
        self.resource = www.ServiceResource(self.service)
        self.resource.controller = mock.create_autospec(
            controller.ServiceController)

    def test_getChild(self):
        number = '3'
        resource = self.resource.getChild(number, None)
        assert isinstance(resource, www.ServiceInstanceResource)
        self.service.instances.get_by_number.assert_called_with(3)

    def test_render_GET(self):
        response = self.resource.render_GET(build_request())
        assert_equal(response['name'], self.service.name)

    def test_render_POST(self):
        response = self.resource.render_POST(build_request())
        assert_equal(response['result'],
            self.resource.controller.handle_command.return_value)


class ServiceCollectionResourceTestCase(TestCase):

    @setup
    def build_resource(self):
        self.mcp = mock.create_autospec(mcp.MasterControlProgram)
        self.resource = www.ServiceCollectionResource(self.mcp)
        self.resource.collection = mock.create_autospec(service.ServiceCollection)

    def test_getChild(self):
        child = self.resource.collection.get_by_name.return_value = mock.Mock()
        child_resource = self.resource.getChild('name', None)
        assert isinstance(child_resource, www.ServiceResource)
        assert_equal(child_resource.service, child)

    def test_getChild_missing(self):
        self.resource.collection.get_by_name.return_value = None
        child_resource = self.resource.getChild('name', None)
        assert isinstance(child_resource, twisted.web.resource.NoResource)

    def test_render_GET(self):
        service_count = 3
        services = [mock.MagicMock() for _ in xrange(service_count)]
        self.resource.collection.__iter__.return_value = services
        with mock.patch('tron.api.resource.respond', autospec=True) as respond:
            response = self.resource.render_GET(build_request())
            assert_equal(response, respond.return_value)
            assert_equal(len(respond.call_args[0][1]['services']), service_count)


class EventResourceTestCase(WWWTestCase):

    @setup
    def setup_resource(self):
        self.name       = 'the_name'
        self.resource   = www.EventResource(self.name)

    @teardown
    def teardown_resource(self):
        event.EventManager.reset()

    def test_render_GET(self):
        recorder = event.get_recorder(self.name)
        ok_message, critical_message ='ok message', 'critical message'
        recorder.ok(ok_message)
        recorder.critical(critical_message)
        response = self.resource.render_GET(self.request())
        names = [e['name'] for e in response['data']]
        assert_equal(names, [critical_message, ok_message])


class ConfigResourceTestCase(TestCase):

    @setup_teardown
    def setup_resource(self):
        self.mcp = mock.create_autospec(mcp.MasterControlProgram)
        self.resource = www.ConfigResource(self.mcp)
        self.controller = self.resource.controller = mock.create_autospec(
            controller.ConfigController)
        with mock.patch('tron.api.resource.respond', autospec=True) as self.respond:
            yield

    def test_render_GET(self):
        name = 'the_nane'
        request = build_request(name=name, no_header='1')
        self.resource.render_GET(request)
        self.controller.read_config.assert_called_with(name, add_header=False)
        self.respond.assert_called_with(request,
                self.resource.controller.read_config.return_value)

    def test_render_POST(self):
        name, config, hash = 'the_name', mock.Mock(), mock.Mock()
        request = build_request(name=name, config=config, hash=hash)
        self.resource.render_POST(request)
        self.controller.update_config.assert_called_with(name, config, hash)
        response_content = {
            'status': 'Active',
            'error': self.controller.update_config.return_value}
        self.respond.assert_called_with(request, response_content)


if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = assertions
"""
 Assertions for testify.
"""
from testify.assertions import assert_not_reached, assert_in


def assert_raises(expected_exception_class, callable_obj, *args, **kwargs):
    """Returns the exception if the callable raises expected_exception_class"""
    try:
        callable_obj(*args, **kwargs)
    except expected_exception_class, e:
        # we got the expected exception
        return e
    assert_not_reached("No exception was raised (expected %s)" %
                       expected_exception_class)


def assert_length(sequence, expected, msg=None):
    """Assert that a sequence or iterable has an expected length."""
    msg = msg or "%(sequence)s has length %(length)s expected %(expected)s"
    length = len(list(sequence))
    assert length == expected, msg % locals()


def assert_call(turtle, call_idx, *args, **kwargs):
    """Assert that a function was called on turtle with the correct args."""
    actual = turtle.calls[call_idx] if turtle.calls else None
    msg = "Call %s expected %s, was %s" % (call_idx, (args, kwargs), actual)
    assert actual == (args, kwargs), msg


def assert_mock_calls(expected, mock_calls):
    """Assert that all expected calls are in the list of mock_calls."""
    for expected_call in expected:
        assert_in(expected_call, mock_calls)
########NEW FILE########
__FILENAME__ = action_runner_test
import contextlib
import tempfile

import mock
from testify import assert_equal, setup, TestCase, setup_teardown


import action_runner
from tests.testingutils import autospec_method

class StatusFileTestCase(TestCase):

    @setup
    def setup_status_file(self):
        self.filename = tempfile.NamedTemporaryFile().name
        self.status_file = action_runner.StatusFile(self.filename)

    @mock.patch('action_runner.opener', autospec=True)
    @mock.patch('action_runner.yaml', autospec=True)
    def test_write(self, mock_yaml, mock_open):
        command, proc = 'do this', mock.Mock()
        autospec_method(self.status_file.get_content)
        self.status_file.write(command, proc)
        self.status_file.get_content.assert_called_with(command, proc)
        mock_yaml.dump.assert_called_with(
            self.status_file.get_content.return_value,
            mock_open.return_value.__enter__.return_value)

    def test_get_content(self):
        command, proc = 'do this', mock.Mock()
        content = self.status_file.get_content(command, proc)
        expected = dict(command=command, pid=proc.pid, return_code=proc.returncode)
        assert_equal(content, expected)


class RegisterTestCase(TestCase):
    mock_isdir = mock_status_file = None
    mock_makedirs = None

    @setup_teardown
    def patch_sys(self):
        with contextlib.nested(
            mock.patch('action_runner.os.path.isdir', autospec=True),
            mock.patch('action_runner.os.makedirs', autospec=True),
            mock.patch('action_runner.StatusFile', autospec=True),
            ) as (self.mock_isdir,
                  self.mock_makedirs,
                  self.mock_status_file):
            self.output_path = '/bogus/path/does/not/exist'
            self.command = 'command'
            self.proc = mock.Mock()
            yield

    def test_get_status_file_dir_does_not_exist_created(self):
        self.mock_isdir.return_value = False
        status_file = action_runner.get_status_file(self.output_path)
        assert_equal(status_file, self.mock_status_file.return_value)
        self.mock_status_file.assert_called_with(
            self.output_path + '/' + action_runner.STATUS_FILE)

    def test_get_status_file_dir_does_not_exist_create_failed(self):
        self.mock_isdir.return_value = False
        self.mock_makedirs.side_effect = OSError
        status_file = action_runner.get_status_file(self.output_path)
        assert_equal(status_file, action_runner.NoFile)

    @mock.patch('action_runner.sys.exit', autospec=True)
    def test_register(self, mock_sys_exit):
        action_runner.register(self.output_path, self.command, self.proc)
        self.mock_status_file.assert_called_with(
            self.output_path + '/' + action_runner.STATUS_FILE)
        self.mock_status_file.return_value.wrap.assert_called_with(
            self.command, self.proc)
        self.proc.wait.assert_called_with()
        mock_sys_exit.assert_called_with(self.proc.returncode)
########NEW FILE########
__FILENAME__ = action_status_test
import mock
from testify import TestCase

import action_status


class ActionStatusTestCase(TestCase):

    @mock.patch('action_status.os.kill', autospec=True)
    def test_send_signal(self, mock_kill):
        status_file = {'pid': 123}
        signal_num = 7
        action_status.send_signal(signal_num, status_file)
        mock_kill.assert_called_with(status_file['pid'], signal_num)

    @mock.patch.dict(action_status.commands)
    def test_run_command(self):
        command, func, status_file = 'print', mock.Mock(), 'status_file'
        action_status.commands['print'] = func
        action_status.run_command(command, status_file)
        func.assert_called_with(status_file)
########NEW FILE########
__FILENAME__ = client_test
import urllib2

import mock
from testify import TestCase, setup, assert_equal, run, setup_teardown
from testify.assertions import assert_in
from tests.assertions import assert_raises
from tests.testingutils import autospec_method

from tron.commands import client
from tron.commands.client import get_object_type_from_identifier, TronObjectType


def build_file_mock(content):
    return mock.Mock(read=mock.Mock(return_value=content))


class RequestTestCase(TestCase):

    @setup
    def setup_options(self):
        self.url = 'http://localhost:8089/services/'

    @setup_teardown
    def patch_urllib(self):
        patcher = mock.patch('tron.commands.client.urllib2.urlopen',
                             autospec=True)
        with patcher as self.mock_urlopen:
            yield

    def test_build_url_request_no_data(self):
        request = client.build_url_request(self.url, None)
        assert request.has_header('User-agent')
        assert_equal(request.get_method(), 'GET')
        assert_equal(request.get_full_url(), self.url)

    def test_build_url_request_with_data(self):
        data = {'param': 'is_set', 'other': 1}
        request = client.build_url_request(self.url, data)
        assert request.has_header('User-agent')
        assert_equal(request.get_method(), 'POST')
        assert_equal(request.get_full_url(), self.url)
        assert_in('param=is_set', request.get_data())
        assert_in('other=1', request.get_data())

    def test_load_response_content_success(self):
        content = 'not:valid:json'
        http_response = build_file_mock(content)
        response = client.load_response_content(http_response)
        assert_equal(response.error, client.DECODE_ERROR)
        assert_in('No JSON object', response.msg)
        assert_equal(response.content, content)

    def test_request_http_error(self):
        self.mock_urlopen.side_effect = urllib2.HTTPError(
            self.url, 500, 'broke', {}, build_file_mock('oops'))
        response = client.request(self.url)
        expected = client.Response(500, 'broke', 'oops')
        assert_equal(response, expected)

    def test_request_url_error(self):
        self.mock_urlopen.side_effect = urllib2.URLError('broke')
        response = client.request(self.url)
        expected = client.Response(client.URL_ERROR, 'broke', None)
        assert_equal(response, expected)

    def test_request_success(self):
        self.mock_urlopen.return_value = build_file_mock('{"ok": "ok"}')
        response = client.request(self.url)
        expected = client.Response(None, None, {'ok': 'ok'})
        assert_equal(response, expected)


class ClientRequestTestCase(TestCase):

    @setup
    def setup_client(self):
        self.url = 'http://localhost:8089/'
        self.client = client.Client(self.url)

    @setup_teardown
    def patch_request(self):
        with mock.patch('tron.commands.client.request') as self.mock_request:
            yield

    def test_request_error(self):
        exception = assert_raises(client.RequestError,
            self.client.request, '/jobs')
        assert_in(self.url, str(exception))

    def test_request_success(self):
        ok_response = {'ok': 'ok'}
        client.request.return_value = client.Response(None, None, ok_response)
        response = self.client.request('/jobs')
        assert_equal(response, ok_response)


class ClientTestCase(TestCase):

    @setup
    def setup_client(self):
        self.url = 'http://localhost:8089/'
        self.client = client.Client(self.url)
        autospec_method(self.client.request)

    def test_config_post(self):
        name, data, hash = 'name', 'stuff', 'hash'
        self.client.config(name, config_data=data, config_hash=hash)
        expected_data =  {'config': data, 'name': name, 'hash': hash}
        self.client.request.assert_called_with('/api/config', expected_data)

    def test_config_get_default(self):
        self.client.config('config_name')
        self.client.request.assert_called_with(
            '/api/config?name=config_name&no_header=0')

    def test_http_get(self):
        self.client.http_get('/api/jobs', {'include': 1})
        self.client.request.assert_called_with('/api/jobs?include=1')

    def test_action_runs(self):
        self.client.action_runs('/api/jobs/name/0/act', num_lines=40)
        self.client.request.assert_called_with(
            '/api/jobs/name/0/act?include_stdout=1&num_lines=40&include_stderr=1')

    def test_job_runs(self):
        self.client.job_runs('/api/jobs/name/0')
        self.client.request.assert_called_with(
            '/api/jobs/name/0?include_action_runs=1&include_action_graph=0')

    def test_job(self):
        self.client.job('/api/jobs/name', count=20)
        self.client.request.assert_called_with(
            '/api/jobs/name?include_action_runs=0&num_runs=20')

    def test_jobs(self):
        self.client.jobs()
        self.client.request.assert_called_with(
            '/api/jobs?include_job_runs=0&include_action_runs=0')


class GetUrlTestCase(TestCase):

    def test_get_job_url_for_action_run(self):
        url = client.get_job_url('MASTER.name.1.act')
        assert_equal(url, '/api/jobs/MASTER.name/1/act')

    def test_get_job_url_for_job(self):
        url = client.get_job_url('MASTER.name')
        assert_equal(url, '/api/jobs/MASTER.name')

    def test_get_service_url(self):
        url = client.get_service_url('MASTER.name.2')
        assert_equal(url, '/api/services/MASTER.name/2')


class GetContentFromIdentifierTestCase(TestCase):

    @setup
    def setup_client(self):
        self.options = mock.Mock()
        self.index = {
            'namespaces': ['OTHER', 'MASTER'],
            'jobs': {
                'MASTER.namea': '',
                'MASTER.nameb': '',
                'OTHER.nameg':  '',
            },
            'services': ['MASTER.foo']
        }

    def test_get_url_from_identifier_job_no_namespace(self):
        identifier = get_object_type_from_identifier(self.index, 'namea')
        assert_equal(identifier.url, '/api/jobs/MASTER.namea')
        assert_equal(identifier.type, TronObjectType.job)

    def test_get_url_from_identifier_service_no_namespace(self):
        identifier = get_object_type_from_identifier(self.index, 'foo')
        assert_equal(identifier.url, '/api/services/MASTER.foo')
        assert_equal(identifier.type, TronObjectType.service)

    def test_get_url_from_identifier_job(self):
        identifier = get_object_type_from_identifier(self.index, 'MASTER.namea')
        assert_equal(identifier.url, '/api/jobs/MASTER.namea')
        assert_equal(identifier.type, TronObjectType.job)

    def test_get_url_from_identifier_service(self):
        identifier = get_object_type_from_identifier(self.index, 'MASTER.foo')
        assert_equal(identifier.url, '/api/services/MASTER.foo')
        assert_equal(identifier.type, TronObjectType.service)

    def test_get_url_from_identifier_service_instance(self):
        identifier = get_object_type_from_identifier(self.index, 'MASTER.foo.1')
        assert_equal(identifier.url, '/api/services/MASTER.foo/1')
        assert_equal(identifier.type, TronObjectType.service_instance)

    def test_get_url_from_identifier_job_run(self):
        identifier = get_object_type_from_identifier(self.index, 'MASTER.nameb.7')
        assert_equal(identifier.url, '/api/jobs/MASTER.nameb/7')
        assert_equal(identifier.type, TronObjectType.job_run)

    def test_get_url_from_identifier_action_run(self):
        identifier = get_object_type_from_identifier(self.index, 'MASTER.nameb.7.run')
        assert_equal(identifier.url, '/api/jobs/MASTER.nameb/7/run')
        assert_equal(identifier.type, TronObjectType.action_run)

    def test_get_url_from_identifier_job_no_namespace_not_master(self):
        identifier = get_object_type_from_identifier(self.index, 'nameg')
        assert_equal(identifier.url, '/api/jobs/OTHER.nameg')
        assert_equal(identifier.type, TronObjectType.job)

    def test_get_url_from_identifier_no_match(self):
        exc = assert_raises(ValueError,
            get_object_type_from_identifier, self.index, 'MASTER.namec')
        assert_in('namec', str(exc))

if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = cmd_utils_test
import contextlib
import mock
from testify import TestCase, assert_equal, setup_teardown
import tron
from tron.commands import cmd_utils


class GetConfigTestCase(TestCase):

    @setup_teardown
    def patch_environment(self):
        with contextlib.nested(
            mock.patch('tron.commands.cmd_utils.opener', autospec=True),
            mock.patch('tron.commands.cmd_utils.yaml', autospec=True),
        ) as (self.mock_opener, self.mock_yaml):
            yield

    def test_read_config_missing(self):
        self.mock_opener.side_effect = IOError
        assert_equal(cmd_utils.read_config(), {})

    def test_read_config(self):
        assert_equal(cmd_utils.read_config(), self.mock_yaml.load.return_value)

    @mock.patch('tron.commands.cmd_utils.os.access', autospec=True)
    def test_get_client_config(self, mock_access):
        mock_access.return_value = False
        config = cmd_utils.get_client_config()
        assert_equal(mock_access.call_count, 2)
        assert_equal(config, {})


class BuildOptionParserTestCase(TestCase):

    def test_build_option_parser(self):
        """Assert that we don't set default options so that we can load
        the defaults from the config.
        """
        parser_class = mock.Mock()
        usage = 'Something'
        parser = cmd_utils.build_option_parser(usage, parser_class=parser_class)
        assert_equal(parser, parser_class.return_value)
        parser_class.assert_called_with(
            usage, version="%%prog %s" % tron.__version__)
        assert_equal(parser.add_option.call_count, 3)

        options = [call[1] for call in parser.add_option.mock_calls]
        expected = [('-v', '--verbose'), ('--server',), ('-s', '--save')]
        assert_equal(options, expected)

        defaults = [
            call[2].get('default') for call in parser.add_option.mock_calls]
        assert_equal(defaults, [None, None, None])

########NEW FILE########
__FILENAME__ = display_test
import mock
from testify import TestCase, run, setup, assert_equal
from testify import setup_teardown
from tron.commands import display

from tron.commands.display import DisplayServices, DisplayJobRuns
from tron.commands.display import DisplayActionRuns, DisplayJobs
from tron.core import actionrun, service


class DisplayServicesTestCase(TestCase):

    @setup
    def setup_data(self):
        self.data = [
            dict(name="My Service",      state="stopped", live_count="4", enabled=True),
            dict(name="Another Service", state="running", live_count="2", enabled=False),
            dict(name="Yet another",     state="running", live_count="1", enabled=True)
        ]
        self.display = DisplayServices()

    def test_format(self):
        out = self.display.format(self.data)
        lines = out.split('\n')
        assert_equal(len(lines), 6)
        assert lines[3].startswith('Another')

    def test_format_no_data(self):
        out = self.display.format([])
        lines = out.split("\n")
        assert_equal(len(lines), 4)
        assert_equal(lines[2], 'No Services')


class DisplayJobRunsTestCase(TestCase):

    @setup
    def setup_data(self):
        self.data = [
            dict(
                id='something.23', state='FAIL', node=mock.MagicMock(),
                run_num=23,
                run_time='2012-01-20 23:11:23',
                start_time='2012-01-20 23:11:23',
                end_time='2012-02-21 23:10:10',
                duration='2 days',
                manual=False,
            ),
            dict(
                id='something.55', state='QUE', node=mock.MagicMock(),
                run_num=55,
                run_time='2012-01-20 23:11:23',
                start_time='2012-01-20 23:11:23',
                end_time='',
                duration='',
                manual=False,
            )
        ]

        self.action_run = dict(
            id='something.23.other',
            name='other',
            state='FAIL',
            node=mock.MagicMock(),
            command='echo 123',
            raw_command='echo 123',
            run_time='2012-01-20 23:11:23',
            start_time='2012-01-20 23:11:23',
            end_time='2012-02-21 23:10:10',
            duration='2 days',
            stdout=[],
            stderr=[]
        )


    def test_format(self):
        out = DisplayJobRuns().format(self.data)
        lines = out.split('\n')
        assert_equal(len(lines), 7)


class DisplayJobsTestCase(TestCase):

    @setup
    def setup_data(self):
        self.data = [
            dict(name='important_things', status='running',
                scheduler=mock.MagicMock(), last_success='unknown'),
            dict(name='other_thing', status='success',
                scheduler=mock.MagicMock(), last_success='2012-01-23 10:23:23',
                action_names=['other', 'first'],
                node_pool=['blam']),
        ]
        self.run_data = [
            dict(
                id='something.23', state='FAIL', node='machine4',
                run_time='2012-01-20 23:11:23',
                start_time='2012-01-20 23:11:23',
                end_time='2012-02-21 23:10:10',
                duration='2 days',
                runs=[dict(
                    id='something.23.other',
                    name='other',
                    state='FAIL',
                    node=mock.MagicMock(),
                    command='echo 123',
                    raw_command='echo 123',
                    run_time='2012-01-20 23:11:23',
                    start_time='2012-01-20 23:11:23',
                    end_time='2012-02-21 23:10:10',
                    duration='2 days',
                    stdout=[],
                    stderr=[]
                )]
            ),
            dict(
                id='something.55', state='QUE', node='machine3',
                run_time='2012-01-20 23:11:23',
                start_time='2012-01-20 23:11:23',
                end_time='',
                duration='',
                runs=[]
            )
        ]

    def do_format(self):
        out = DisplayJobs().format(self.data)
        lines = out.split('\n')
        return lines

    def test_format(self):
        lines = self.do_format()
        assert_equal(len(lines), 5)


class DisplayActionsTestCase(TestCase):

    @setup
    def setup_data(self):
        self.data = {
            'id': 'something.23',
            'state': 'UNKWN',
            'node': {'hostname': 'something', 'username': 'a'},
            'run_time': 'sometime',
            'start_time': 'sometime',
            'end_time': 'sometime',
            'manual': False,
            'runs': [
                dict(
                    id='something.23.run_other_thing',
                    state='UNKWN',
                    start_time='2012-01-23 10:10:10.123456',
                    end_time='',
                    duration='',
                    run_time='sometime',
                ),
                dict(
                    id='something.1.run_foo',
                    state='FAIL',
                    start_time='2012-01-23 10:10:10.123456',
                    end_time='2012-01-23 10:40:10.123456',
                    duration='1234.123456',
                    run_time='sometime',
                ),
                dict(
                    id='something.23.run_other_thing',
                    state='QUE',
                    start_time='2012-01-23 10:10:10.123456',
                    end_time='',
                    duration='',
                    run_time='sometime',
                ),
            ]
        }
        self.details = {
            'id':               'something.1.foo',
            'state':            'FAIL',
            'node':             'localhost',
            'stdout':           ['Blah', 'blah', 'blah'],
            'stderr':           ['Crash', 'and', 'burn'],
            'command':          '/bin/bash ./runme.sh now',
            'raw_command':      'bash runme.sh now',
            'requirements':     ['.run_first_job'],
        }

    def format_lines(self):
        out = DisplayActionRuns().format(self.data)
        return out.split('\n')

    def test_format(self):
        lines = self.format_lines()
        assert_equal(len(lines), 13)


class AddColorForStateTestCase(TestCase):

    @setup_teardown
    def enable_color(self):
        with display.Color.enable():
            yield

    def test_add_red(self):
        text = display.add_color_for_state(actionrun.ActionRun.STATE_FAILED.name)
        assert text.startswith(display.Color.colors['red']), text

    def test_add_green(self):
        text = display.add_color_for_state(actionrun.ActionRun.STATE_RUNNING.name)
        assert text.startswith(display.Color.colors['green']), text

    def test_add_blue(self):
        text = display.add_color_for_state(service.ServiceState.DISABLED)
        assert text.startswith(display.Color.colors['blue']), text


class DisplayNodeTestCase(TestCase):

    node_source = {
        'name': 'name',
        'hostname': 'hostname',
        'username': 'username'}

    def test_display_node(self):
        result = display.display_node(self.node_source)
        assert_equal(result, 'username@hostname')

    def test_display_node_pool(self):
        source = {'name': 'name', 'nodes': [self.node_source]}
        result = display.display_node_pool(source)
        assert_equal(result, 'name (1 node(s))')

class DisplaySchedulerTestCase(TestCase):

    def test_display_scheduler_no_jitter(self):
        source = {'value': '5 minutes', 'type': 'interval', 'jitter': ''}
        result = display.display_scheduler(source)
        assert_equal(result, 'interval 5 minutes')

    def test_display_scheduler_with_jitter(self):
        source = {
            'value': '5 minutes',
            'type': 'interval',
            'jitter': ' (+/- 2 min)'}
        result = display.display_scheduler(source)
        assert_equal(result, 'interval 5 minutes%s' % (source['jitter']))



if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = command_context_test
import datetime
import mock
from testify import TestCase, setup, assert_raises, assert_equal, run

from tron import command_context, scheduler, node
from tron.core import job, jobrun, actionrun, serviceinstance
from tron.core.jobrun import JobRunCollection


class EmptyContextTestCase(TestCase):
    @setup
    def build_context(self):
        self.context = command_context.CommandContext(None)

    def test__getitem__(self):
        assert_raises(KeyError, self.context.__getitem__, 'foo')

    def test_get(self):
        assert not self.context.get('foo')


class BuildFilledContextTestCase(TestCase):

    def test_build_filled_context_no_objects(self):
        output = command_context.build_filled_context()
        assert not output.base
        assert not output.next

    def test_build_filled_context_single(self):
        output = command_context.build_filled_context(command_context.JobContext)
        assert isinstance(output.base, command_context.JobContext)
        assert not output.next

    def test_build_filled_context_chain(self):
        objs = [command_context.JobContext, command_context.JobRunContext]
        output = command_context.build_filled_context(*objs)
        assert isinstance(output.base, objs[1])
        assert isinstance(output.next.base, objs[0])
        assert not output.next.next


class SimpleContextTestCaseBase(TestCase):
    __test__ = False

    def test_hit(self):
        assert_equal(self.context['foo'], 'bar')

    def test_miss(self):
        assert_raises(KeyError, self.context.__getitem__, 'your_mom')

    def test_get_hit(self):
        assert_equal(self.context.get('foo'), 'bar')

    def test_get_miss(self):
        assert not self.context.get('unknown')


class SimpleDictContextTestCase(SimpleContextTestCaseBase):
    @setup
    def build_context(self):
        self.context = command_context.CommandContext(dict(foo='bar'))


class SimpleObjectContextTestCase(SimpleContextTestCaseBase):

    @setup
    def build_context(self):
        class Obj(object):
            foo = 'bar'
        self.context = command_context.CommandContext(Obj)


class ChainedDictContextTestCase(SimpleContextTestCaseBase):

    @setup
    def build_context(self):
        self.next_context = command_context.CommandContext(
                dict(foo='bar', next_foo='next_bar'))
        self.context = command_context.CommandContext(dict(), self.next_context)

    def test_chain_get(self):
        assert_equal(self.context['next_foo'], 'next_bar')


class ChainedDictOverrideContextTestCase(SimpleContextTestCaseBase):
    @setup
    def build_context(self):
        self.next_context = command_context.CommandContext(
                dict(foo='your mom', next_foo='next_bar'))
        self.context = command_context.CommandContext(
                dict(foo='bar'), self.next_context)

    def test_chain_get(self):
        assert_equal(self.context['next_foo'], 'next_bar')


class ChainedObjectOverrideContextTestCase(SimpleContextTestCaseBase):
    @setup
    def build_context(self):
        class MyObject(object):
            pass
        obj = MyObject()
        obj.foo = 'bar'

        self.next_context = command_context.CommandContext(
                dict(foo='your mom', next_foo='next_bar'))
        self.context = command_context.CommandContext(obj, self.next_context)

    def test_chain_get(self):
        assert_equal(self.context['next_foo'], 'next_bar')


class JobContextTestCase(TestCase):

    @setup
    def setup_job(self):
        self.last_success = mock.Mock(run_time=datetime.datetime(2012, 3, 14))
        mock_scheduler = mock.create_autospec(scheduler.ConstantScheduler)
        run_collection = mock.create_autospec(JobRunCollection,
                        last_success=self.last_success)
        self.job = job.Job("jobname", mock_scheduler, run_collection=run_collection)
        self.context = command_context.JobContext(self.job)

    def test_name(self):
        assert_equal(self.context.name, self.job.name)

    def test__getitem__last_success(self):
        item = self.context["last_success:day-1"]
        expected = (self.last_success.run_time - datetime.timedelta(days=1)).day
        assert_equal(item, str(expected))

        item = self.context["last_success:shortdate"]
        assert_equal(item, "2012-03-14")

    def test__getitem__last_success_bad_date_spec(self):
        name = "last_success:beers-3"
        assert_raises(KeyError, lambda: self.context[name])

    def test__getitem__last_success_bad_date_name(self):
        name = "first_success:shortdate-1"
        assert_raises(KeyError, lambda: self.context[name])

    def test__getitem__last_success_no_date_spec(self):
        name = "last_success"
        assert_raises(KeyError, lambda: self.context[name])

    def test__getitem__missing(self):
        assert_raises(KeyError, lambda: self.context['bogus'])


class JobRunContextTestCase(TestCase):

    @setup
    def setup_context(self):
        self.jobrun = mock.create_autospec(jobrun.JobRun, run_time='sometime')
        self.context = command_context.JobRunContext(self.jobrun)

    def test_cleanup_job_status(self):
        self.jobrun.action_runs.is_failed = False
        self.jobrun.action_runs.is_complete_without_cleanup = True
        assert_equal(self.context.cleanup_job_status, 'SUCCESS')

    def test_cleanup_job_status_failure(self):
        self.jobrun.action_runs.is_failed = True
        assert_equal(self.context.cleanup_job_status, 'FAILURE')

    def test_runid(self):
        assert_equal(self.context.runid, self.jobrun.id)

    @mock.patch('tron.command_context.timeutils.DateArithmetic', autospec=True)
    def test__getitem__(self, mock_date_math):
        name = 'date_name'
        time_value = self.context[name]
        mock_date_math.parse.assert_called_with(name, self.jobrun.run_time)
        assert_equal(time_value, mock_date_math.parse.return_value)


class ActionRunContextTestCase(TestCase):

    @setup
    def build_context(self):
        mock_node = mock.create_autospec(node.Node, hostname='something')
        self.action_run = mock.create_autospec(actionrun.ActionRun,
            action_name='something', node=mock_node)
        self.context = command_context.ActionRunContext(self.action_run)

    def test_actionname(self):
        assert_equal(self.context.actionname, self.action_run.action_name)

    def test_node_hostname(self):
        assert_equal(self.context.node, self.action_run.node.hostname)


class ServiceInstanceContextTestCase(TestCase):

    @setup
    def build_context(self):
        self.service_instance = mock.create_autospec(
            serviceinstance.ServiceInstance,
            instance_number=123,
            node=mock.Mock(hostname='something'),
            config=mock.Mock(name='name', pid_file=mock.MagicMock()))
        self.context = command_context.ServiceInstanceContext(self.service_instance)

    def test_instance_number(self):
        assert_equal(self.context.instance_number, self.service_instance.instance_number)

    def test_node(self):
        assert_equal(self.context.node, self.service_instance.node.hostname)

    def test_name(self):
        assert_equal(self.context.name, self.service_instance.config.name)

    def test_pid_file(self):
        self.service_instance.parent_context = {'one': 'thing'}
        self.service_instance.config.pid_file = '%(one)s %(instance_number)s'
        assert_equal(self.context.pid_file, 'thing 123')


class FillerTestCase(TestCase):

    @setup
    def setup_filler(self):
        self.filler = command_context.Filler()

    def test_filler_with_service_instance_pid_file(self):
        context = command_context.ServiceInstanceContext(self.filler)
        assert_equal(context.pid_file, self.filler)

    def test_filler_with_job__getitem__(self):
        context = command_context.JobContext(self.filler)
        todays_date = datetime.date.today().strftime("%Y-%m-%d")
        assert_equal(context['last_success:shortdate'], todays_date)

    def test_filler_with_job_run__getitem__(self):
        context = command_context.JobRunContext(self.filler)
        todays_date = datetime.date.today().strftime("%Y-%m-%d")
        assert_equal(context['shortdate'], todays_date)


if __name__ == '__main__':
    run()
########NEW FILE########
__FILENAME__ = config_parse_test
import datetime
import os
import shutil
import stat
import tempfile
from textwrap import dedent
import textwrap

import mock
import pytz

from testify import assert_equal, assert_in
from testify import run, setup, teardown, TestCase
import yaml
from tron.config import config_parse, schema, manager, config_utils, schedule_parse
from tron.config.config_parse import valid_config, valid_cleanup_action_name
from tron.config.config_parse import valid_output_stream_dir
from tron.config.config_parse import valid_node_pool
from tron.config.config_parse import validate_fragment
from tron.config.config_utils import NullConfigContext
from tron.config.config_parse import build_format_string_validator
from tron.config.config_parse import CLEANUP_ACTION_NAME
from tron.config.config_parse import valid_job
from tron.config import ConfigError
from tron.config.schedule_parse import ConfigConstantScheduler
from tron.config.schedule_parse import ConfigIntervalScheduler
from tron.config.schema import MASTER_NAMESPACE
from tests.assertions import assert_raises
from tron.utils.dicts import FrozenDict


BASE_CONFIG = """
ssh_options:
    agent: false
    identities:
        - tests/test_id_rsa

nodes:
    - name: node0
      hostname: 'node0'
    - name: node1
      hostname: 'node1'

node_pools:
    - name: NodePool
      nodes: [node0, node1]
"""


def valid_config_from_yaml(config_content):
    return valid_config(manager.from_string(config_content))


class ConfigTestCase(TestCase):
    BASE_CONFIG = """
output_stream_dir: "/tmp"

time_zone: "EST"

ssh_options:
    agent: false
    identities:
        - tests/test_id_rsa

nodes:
    -   name: node0
        hostname: 'node0'
    -   name: node1
        hostname: 'node1'
node_pools:
    -   name: nodePool
        nodes: [node0, node1]
    """

    config = BASE_CONFIG + """

command_context:
    batch_dir: /tron/batch/test/foo
    python: /usr/bin/python

jobs:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        actions:
            -
                name: "action0_0"
                command: "test_command0.0"
        cleanup_action:
            command: "test_command0.1"

    -
        name: "test_job1"
        node: node0
        schedule: "daily 00:30:00 MWF"
        allow_overlap: True
        actions:
            -
                name: "action1_0"
                command: "test_command1.0"
            -
                name: "action1_1"
                command: "test_command1.1"
                requires: [action1_0]

    -
        name: "test_job2"
        node: node1
        schedule: "daily 16:30:00"
        actions:
            -
                name: "action2_0"
                command: "test_command2.0"

    -
        name: "test_job3"
        node: node1
        schedule: "constant"
        actions:
            -
                name: "action3_0"
                command: "test_command3.0"
            -
                name: "action3_1"
                command: "test_command3.1"
            -
                name: "action3_2"
                node: node0
                command: "test_command3.2"
                requires: [action3_0, action3_1]

    -
        name: "test_job4"
        node: nodePool
        all_nodes: True
        schedule: "daily"
        enabled: False
        actions:
            -
                name: "action4_0"
                command: "test_command4.0"

services:
    -
        name: "service0"
        node: nodePool
        command: "service_command0"
        count: 2
        pid_file: "/var/run/%(name)s-%(instance_number)s.pid"
        monitor_interval: 20
"""

    def test_attributes(self):
        expected = schema.TronConfig(
            action_runner=FrozenDict(),
            output_stream_dir='/tmp',
            command_context=FrozenDict({
                'python': '/usr/bin/python',
                'batch_dir': '/tron/batch/test/foo'
            }),
            ssh_options=schema.ConfigSSHOptions(
                agent=False,
                identities=('tests/test_id_rsa',),
                known_hosts_file=None,
                connect_timeout=30,
                idle_connection_timeout=3600,
                jitter_min_load=4,
                jitter_max_delay=20,
                jitter_load_factor=1,
            ),
            notification_options=None,
            time_zone=pytz.timezone("EST"),
            state_persistence=config_parse.DEFAULT_STATE_PERSISTENCE,
            nodes=FrozenDict({
                'node0': schema.ConfigNode(name='node0',
                    username=os.environ['USER'], hostname='node0', port=22),
                'node1': schema.ConfigNode(name='node1',
                    username=os.environ['USER'], hostname='node1', port=22)
            }),
            node_pools=FrozenDict({
                'nodePool': schema.ConfigNodePool(nodes=('node0', 'node1'),
                                                name='nodePool')
            }),
            jobs=FrozenDict({
                'MASTER.test_job0': schema.ConfigJob(
                    name='MASTER.test_job0',
                    namespace='MASTER',
                    node='node0',
                    schedule=ConfigIntervalScheduler(
                        timedelta=datetime.timedelta(0, 20), jitter=None),
                    actions=FrozenDict({
                        'action0_0': schema.ConfigAction(
                            name='action0_0',
                            command='test_command0.0',
                            requires=(),
                            node=None)
                    }),
                    queueing=True,
                    run_limit=50,
                    all_nodes=False,
                    cleanup_action=schema.ConfigCleanupAction(
                        name='cleanup',
                        command='test_command0.1',
                        node=None),
                    enabled=True,
                    max_runtime=None,
                    allow_overlap=False),
                'MASTER.test_job1': schema.ConfigJob(
                    name='MASTER.test_job1',
                    namespace='MASTER',
                    node='node0',
                    enabled=True,
                    schedule=schedule_parse.ConfigDailyScheduler(
                        days=set([1, 3, 5]),
                        hour=0, minute=30, second=0,
                        original="00:30:00 MWF",
                        jitter=None,
                    ),
                    actions=FrozenDict({
                        'action1_1': schema.ConfigAction(
                            name='action1_1',
                            command='test_command1.1',
                            requires=('action1_0',),
                            node=None),
                        'action1_0': schema.ConfigAction(
                            name='action1_0',
                            command='test_command1.0',
                            requires=(),
                            node=None)
                    }),
                    queueing=True,
                    run_limit=50,
                    all_nodes=False,
                    cleanup_action=None,
                    max_runtime=None,
                    allow_overlap=True),
                'MASTER.test_job2': schema.ConfigJob(
                    name='MASTER.test_job2',
                    namespace='MASTER',
                    node='node1',
                    enabled=True,
                    schedule=schedule_parse.ConfigDailyScheduler(
                        days=set(),
                        hour=16, minute=30, second=0,
                        original="16:30:00 ",
                        jitter=None,
                    ),
                    actions=FrozenDict({
                        'action2_0': schema.ConfigAction(
                            name='action2_0',
                            command='test_command2.0',
                            requires=(),
                            node=None)
                    }),
                    queueing=True,
                    run_limit=50,
                    all_nodes=False,
                    cleanup_action=None,
                    max_runtime=None,
                    allow_overlap=False),
                'MASTER.test_job3': schema.ConfigJob(
                    name='MASTER.test_job3',
                    namespace='MASTER',
                    node='node1',
                    schedule=ConfigConstantScheduler(),
                    enabled=True,
                    actions=FrozenDict({
                        'action3_1': schema.ConfigAction(
                            name='action3_1',
                            command='test_command3.1',
                            requires=(),
                            node=None),
                        'action3_0': schema.ConfigAction(
                            name='action3_0',
                            command='test_command3.0',
                            requires=(),
                            node=None),
                        'action3_2': schema.ConfigAction(
                            name='action3_2',
                            command='test_command3.2',
                            requires=('action3_0', 'action3_1'),
                            node='node0')
                    }),
                    queueing=True,
                    run_limit=50,
                    all_nodes=False,
                    cleanup_action=None,
                    max_runtime=None,
                    allow_overlap=False),
                'MASTER.test_job4': schema.ConfigJob(
                    name='MASTER.test_job4',
                    namespace='MASTER',
                    node='nodePool',
                    schedule=schedule_parse.ConfigDailyScheduler(
                        days=set(),
                        hour=0, minute=0, second=0,
                        original='00:00:00 ',
                        jitter=None,
                    ),
                    actions=FrozenDict({
                        'action4_0': schema.ConfigAction(
                            name='action4_0',
                            command='test_command4.0',
                            requires=(),
                            node=None)}),
                    queueing=True,
                    run_limit=50,
                    all_nodes=True,
                    cleanup_action=None,
                    enabled=False,
                    max_runtime=None,
                    allow_overlap=False)
                }),
                services=FrozenDict({
                    'MASTER.service0': schema.ConfigService(
                        name='MASTER.service0',
                        namespace='MASTER',
                        node='nodePool',
                        pid_file='/var/run/%(name)s-%(instance_number)s.pid',
                        command='service_command0',
                        monitor_interval=20,
                        monitor_retries=5,
                        restart_delay=None,
                        count=2)
                }
            )
        )

        test_config = valid_config_from_yaml(self.config)
        assert_equal(test_config.command_context, expected.command_context)
        assert_equal(test_config.ssh_options, expected.ssh_options)
        assert_equal(test_config.notification_options, expected.notification_options)
        assert_equal(test_config.time_zone, expected.time_zone)
        assert_equal(test_config.nodes, expected.nodes)
        assert_equal(test_config.node_pools, expected.node_pools)
        assert_equal(test_config.jobs['MASTER.test_job0'], expected.jobs['MASTER.test_job0'])
        assert_equal(test_config.jobs['MASTER.test_job1'], expected.jobs['MASTER.test_job1'])
        assert_equal(test_config.jobs['MASTER.test_job2'], expected.jobs['MASTER.test_job2'])
        assert_equal(test_config.jobs['MASTER.test_job3'], expected.jobs['MASTER.test_job3'])
        assert_equal(test_config.jobs['MASTER.test_job4'], expected.jobs['MASTER.test_job4'])
        assert_equal(test_config.jobs, expected.jobs)
        assert_equal(test_config.services, expected.services)
        assert_equal(test_config, expected)
        assert_equal(test_config.jobs['MASTER.test_job4'].enabled, False)

    def test_empty_node_test(self):
        valid_config_from_yaml("""nodes:""")


class NamedConfigTestCase(TestCase):
    config = """
jobs:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        actions:
            -
                name: "action0_0"
                command: "test_command0.0"
        cleanup_action:
            command: "test_command0.1"

    -
        name: "test_job1"
        node: node0
        schedule: "daily 00:30:00 MWF"
        allow_overlap: True
        actions:
            -
                name: "action1_0"
                command: "test_command1.0 %(some_var)s"
            -
                name: "action1_1"
                command: "test_command1.1"
                requires: [action1_0]

    -
        name: "test_job2"
        node: node1
        schedule: "daily 16:30:00"
        actions:
            -
                name: "action2_0"
                command: "test_command2.0"

    -
        name: "test_job3"
        node: node1
        schedule: "constant"
        actions:
            -
                name: "action3_0"
                command: "test_command3.0"
            -
                name: "action3_1"
                command: "test_command3.1"
            -
                name: "action3_2"
                node: node0
                command: "test_command3.2"
                requires: [action3_0, action3_1]

    -
        name: "test_job4"
        node: NodePool
        all_nodes: True
        schedule: "daily"
        enabled: False
        actions:
            -
                name: "action4_0"
                command: "test_command4.0"

services:
    -
        name: "service0"
        node: NodePool
        command: "service_command0"
        count: 2
        pid_file: "/var/run/%(name)s-%(instance_number)s.pid"
        monitor_interval: 20
"""

    def test_attributes(self):
        expected = schema.NamedTronConfig(
            jobs=FrozenDict({
                'test_job0': schema.ConfigJob(
                    name='test_job0',
                    namespace='test_namespace',
                    node='node0',
                    schedule=ConfigIntervalScheduler(
                        timedelta=datetime.timedelta(0, 20),
                        jitter=None,
                    ),
                    actions=FrozenDict({
                        'action0_0': schema.ConfigAction(
                            name='action0_0',
                            command='test_command0.0',
                            requires=(),
                            node=None)
                    }),
                    queueing=True,
                    run_limit=50,
                    all_nodes=False,
                    cleanup_action=schema.ConfigCleanupAction(
                        name='cleanup',
                        command='test_command0.1',
                        node=None),
                    enabled=True,
                    max_runtime=None,
                    allow_overlap=False),
                'test_job1': schema.ConfigJob(
                    name='test_job1',
                    namespace='test_namespace',
                    node='node0',
                    enabled=True,
                    schedule=schedule_parse.ConfigDailyScheduler(
                        days=set([1, 3, 5]),
                        hour=0,
                        minute=30,
                        second=0,
                        original="00:30:00 MWF",
                        jitter=None,
                    ),
                    actions=FrozenDict({
                        'action1_1': schema.ConfigAction(
                            name='action1_1',
                            command='test_command1.1',
                            requires=('action1_0',),
                            node=None),
                        'action1_0': schema.ConfigAction(
                            name='action1_0',
                            command='test_command1.0 %(some_var)s',
                            requires=(),
                            node=None)
                    }),
                    queueing=True,
                    run_limit=50,
                    all_nodes=False,
                    cleanup_action=None,
                    max_runtime=None,
                    allow_overlap=True),
                'test_job2': schema.ConfigJob(
                    name='test_job2',
                    namespace='test_namespace',
                    node='node1',
                    enabled=True,
                    schedule=schedule_parse.ConfigDailyScheduler(
                        days=set(),
                        hour=16,
                        minute=30,
                        second=0,
                        original="16:30:00 ",
                        jitter=None,
                    ),
                    actions=FrozenDict({
                        'action2_0': schema.ConfigAction(
                            name='action2_0',
                            command='test_command2.0',
                            requires=(),
                            node=None)
                    }),
                    queueing=True,
                    run_limit=50,
                    all_nodes=False,
                    cleanup_action=None,
                    max_runtime=None,
                    allow_overlap=False),
                'test_job3': schema.ConfigJob(
                    name='test_job3',
                    namespace='test_namespace',
                    node='node1',
                    schedule=ConfigConstantScheduler(),
                    enabled=True,
                    actions=FrozenDict({
                        'action3_1': schema.ConfigAction(
                            name='action3_1',
                            command='test_command3.1',
                            requires=(),
                            node=None),
                        'action3_0': schema.ConfigAction(
                            name='action3_0',
                            command='test_command3.0',
                            requires=(),
                            node=None),
                        'action3_2': schema.ConfigAction(
                            name='action3_2',
                            command='test_command3.2',
                            requires=('action3_0', 'action3_1'),
                            node='node0')
                    }),
                    queueing=True,
                    run_limit=50,
                    all_nodes=False,
                    cleanup_action=None,
                    max_runtime=None,
                    allow_overlap=False),
                'test_job4': schema.ConfigJob(
                    name='test_job4',
                    namespace='test_namespace',
                    node='NodePool',
                    schedule=schedule_parse.ConfigDailyScheduler(
                        days=set(),
                        hour=0, minute=0, second=0,
                        original="00:00:00 ",
                        jitter=None,
                    ),
                    actions=FrozenDict({
                        'action4_0': schema.ConfigAction(
                            name='action4_0',
                            command='test_command4.0',
                            requires=(),
                            node=None)}),
                    queueing=True,
                    run_limit=50,
                    all_nodes=True,
                    cleanup_action=None,
                    enabled=False,
                    max_runtime=None,
                    allow_overlap=False)
                }),
                services=FrozenDict({
                    'service0': schema.ConfigService(
                        namespace='test_namespace',
                        name='service0',
                        node='NodePool',
                        pid_file='/var/run/%(name)s-%(instance_number)s.pid',
                        command='service_command0',
                        monitor_interval=20,
                        monitor_retries=5,
                        restart_delay=None,
                        count=2)
                }
            )
        )

        test_config = validate_fragment('test_namespace', yaml.load(self.config))
        assert_equal(test_config.jobs['test_job0'], expected.jobs['test_job0'])
        assert_equal(test_config.jobs['test_job1'], expected.jobs['test_job1'])
        assert_equal(test_config.jobs['test_job2'], expected.jobs['test_job2'])
        assert_equal(test_config.jobs['test_job3'], expected.jobs['test_job3'])
        assert_equal(test_config.jobs['test_job4'], expected.jobs['test_job4'])
        assert_equal(test_config.jobs, expected.jobs)
        assert_equal(test_config.services, expected.services)
        assert_equal(test_config, expected)
        assert_equal(test_config.jobs['test_job4'].enabled, False)


class JobConfigTestCase(TestCase):

    def test_no_actions(self):
        test_config = BASE_CONFIG + """
jobs:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        """
        expected_message = "Job test_job0 is missing options: actions"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_in(expected_message, str(exception))

    def test_empty_actions(self):
        test_config = BASE_CONFIG + """
jobs:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        actions:
        """
        expected_message = "Value at config.jobs.Job.test_job0.actions"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_in(expected_message, str(exception))

    def test_dupe_names(self):
        test_config = BASE_CONFIG + """
jobs:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        actions:
            -
                name: "action0_0"
                command: "test_command0.0"
            -
                name: "action0_0"
                command: "test_command0.0"

        """
        expected = "Duplicate name action0_0 at config.jobs.Job.test_job0.actions"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_in(expected, str(exception))

    def test_bad_requires(self):
        test_config = BASE_CONFIG + """
jobs:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        actions:
            -
                name: "action0_0"
                command: "test_command0.0"
            -
                name: "action0_1"
                command: "test_command0.1"

    -
        name: "test_job1"
        node: node0
        schedule: "interval 20s"
        actions:
            -
                name: "action1_0"
                command: "test_command1.0"
                requires: [action0_0]

        """
        expected_message = ('jobs.MASTER.test_job1.action1_0 has a dependency '
                '"action0_0" that is not in the same job!')
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_in(expected_message, str(exception))


    def test_circular_dependency(self):
        test_config = BASE_CONFIG + """
jobs:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        actions:
            -
                name: "action0_0"
                command: "test_command0.0"
                requires: [action0_1]
            -
                name: "action0_1"
                command: "test_command0.1"
                requires: [action0_0]
        """
        expect = "Circular dependency in job.MASTER.test_job0: action0_0 -> action0_1"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_in(expect, exception)

    def test_config_cleanup_name_collision(self):
        test_config = BASE_CONFIG + """
jobs:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        actions:
            -
                name: "%s"
                command: "test_command0.0"

        """ % CLEANUP_ACTION_NAME
        expected_message = "config.jobs.Job.test_job0.actions.Action.cleanup.name"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_in(expected_message, str(exception))

    def test_config_cleanup_action_name(self):
        test_config = BASE_CONFIG + """
jobs:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        actions:
            -
                name: "action0_0"
                command: "test_command0.0"
        cleanup_action:
            name: "gerald"
            command: "test_command0.1"
        """
        expected_msg = "Cleanup actions cannot have custom names"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_in(expected_msg, str(exception))

    def test_config_cleanup_requires(self):
        test_config = BASE_CONFIG + """
jobs:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        actions:
            -
                name: "action0_0"
                command: "test_command0.0"
        cleanup_action:
            command: "test_command0.1"
            requires: [action0_0]
        """
        expected_msg = "Unknown keys in CleanupAction : requires"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_equal(expected_msg, str(exception))

    def test_job_in_services(self):
        test_config = BASE_CONFIG + """
services:
    -
        name: "test_job0"
        node: node0
        schedule: "interval 20s"
        actions:
            -
                name: "action0_0"
                command: "test_command0.0"
        cleanup_action:
            command: "test_command0.1"
"""
        expected_msg = "Service test_job0 is missing options:"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_in(expected_msg, str(exception))

    def test_overlap_job_service_names(self):
        tron_config = dict(
            nodes=['localhost'],
            jobs=[
                dict(
                    name="sameName",
                    node="localhost",
                    schedule="interval 20s",
                    actions=[dict(name="someAction", command="something")]
                )
            ],
            services=[
                dict(
                    name="sameName",
                    node="localhost",
                    pid_file="file",
                    command="something",
                    monitor_interval=20
                )
            ]
        )
        expected_message = "Job and Service names must be unique MASTER.sameName"
        exception = assert_raises(ConfigError, valid_config, tron_config)
        assert_in(expected_message, str(exception))

    def test_validate_job_no_actions(self):
        job_config = dict(
            name="job_name",
            node="localhost",
            schedule="constant",
            actions=[]
        )
        config_context = config_utils.ConfigContext('config', ['localhost'], None, None)
        expected_msg = "Required non-empty list at config.Job.job_name.actions"
        exception = assert_raises(ConfigError, valid_job, job_config, config_context)
        assert_in(expected_msg, str(exception))


class NodeConfigTestCase(TestCase):

    def test_validate_node_pool(self):
        config_node_pool = valid_node_pool(
            dict(name="theName", nodes=["node1", "node2"]))
        assert_equal(config_node_pool.name, "theName")
        assert_equal(len(config_node_pool.nodes), 2)

    def test_overlap_node_and_node_pools(self):
        tron_config = dict(
            nodes=[
                dict(name="sameName", hostname="localhost")
            ],
            node_pools=[
                dict(name="sameName", nodes=["sameNode"])
            ]
        )
        expected_msg = "Node and NodePool names must be unique sameName"
        exception = assert_raises(ConfigError, valid_config, tron_config)
        assert_in(expected_msg, str(exception))

    def test_invalid_node_name(self):
        test_config = BASE_CONFIG + dedent("""
            jobs:
                -
                    name: "test_job0"
                    node: "some_unknown_node"
                    schedule: "interval 20s"
                    actions:
                        -
                            name: "action0_0"
                            command: "test_command0.0"
            """)
        expected_msg = "Unknown node name some_unknown_node at config.jobs.Job.test_job0.node"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_equal(expected_msg, str(exception))

    def test_invalid_nested_node_pools(self):
        test_config = dedent("""
            nodes:
                - name: node0
                  hostname: node0

            node_pools:
                - name: pool0
                  nodes: [node1]
                - name: pool1
                  nodes: [node0, pool0]
            jobs:
                - name: somejob
                  node: pool1
                  schedule: "interval 30s"
                  actions:
                    - name: first
                      command: "echo 1"
        """)
        expected_msg = "NodePool pool1 contains other NodePools: pool0"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_in(expected_msg, str(exception))

    def test_invalid_node_pool_config(self):
        test_config = dedent("""
            nodes:
                - name: node0
                  hostname: node0

            node_pools:
                - name: pool0
                  hostname: node1
                - name: pool1
                  nodes: [node0, pool0]
            jobs:
                - name: somejob
                  node: pool1
                  schedule: "interval 30s"
                  actions:
                    - name: first
                      command: "echo 1"
        """)
        expected_msg = "NodePool pool0 is missing options"
        exception = assert_raises(ConfigError, valid_config_from_yaml, test_config)
        assert_in(expected_msg, str(exception))

    def test_invalid_named_update(self):
        test_config = """bozray:"""
        test_config = yaml.load(test_config)
        expected_message = "Unknown keys in NamedConfigFragment : bozray"
        exception = assert_raises(ConfigError, validate_fragment, 'foo', test_config)
        assert_in(expected_message, str(exception))


class ValidateJobsAndServicesTestCase(TestCase):

    def test_valid_jobs_and_services_success(self):
        test_config = BASE_CONFIG + textwrap.dedent("""
            jobs:
                -
                    name: "test_job0"
                    node: node0
                    schedule: "interval 20s"
                    actions:
                        -
                            name: "action0_0"
                            command: "test_command0.0"
                    cleanup_action:
                        command: "test_command0.1"
            services:
                -
                    name: "test_service0"
                    node: node0
                    command: "service_command0"
                    count: 2
                    pid_file: "/var/run/%(name)s-%(instance_number)s.pid"
                    monitor_interval: 20
                    """)
        expected_jobs = {'MASTER.test_job0':
            schema.ConfigJob(name='MASTER.test_job0',
                namespace='MASTER',
                node='node0',
                schedule=ConfigIntervalScheduler(
                    timedelta=datetime.timedelta(0, 20), jitter=None),
                actions=FrozenDict({'action0_0':
                      schema.ConfigAction(name='action0_0',
                                   command='test_command0.0',
                                   requires=(),
                                   node=None)}),
                queueing=True,
                run_limit=50,
                all_nodes=False,
                cleanup_action=schema.ConfigCleanupAction(command='test_command0.1',
                     name='cleanup',
                     node=None),
                enabled=True,
                allow_overlap=False,
                max_runtime=None)
            }

        expected_services = {'MASTER.test_service0':
            schema.ConfigService(name='MASTER.test_service0',
                          namespace='MASTER',
                          node='node0',
                          pid_file='/var/run/%(name)s-%(instance_number)s.pid',
                          command='service_command0',
                          monitor_interval=20,
                          monitor_retries=5,
                          restart_delay=None,
                          count=2)
            }

        config = manager.from_string(test_config)
        context = config_utils.ConfigContext('config', ['node0'], None, MASTER_NAMESPACE)
        config_parse.validate_jobs_and_services(config, context)
        assert_equal(expected_jobs, config['jobs'])
        assert_equal(expected_services, config['services'])


class ValidCleanupActionNameTestCase(TestCase):

    def test_valid_cleanup_action_name_pass(self):
        name = valid_cleanup_action_name(CLEANUP_ACTION_NAME, None)
        assert_equal(CLEANUP_ACTION_NAME, name)

    def test_valid_cleanup_action_name_fail(self):
        assert_raises(ConfigError,
            valid_cleanup_action_name, 'other', NullConfigContext)


class ValidOutputStreamDirTestCase(TestCase):

    @setup
    def setup_dir(self):
        self.dir = tempfile.mkdtemp()

    @teardown
    def teardown_dir(self):
        shutil.rmtree(self.dir)

    def test_valid_dir(self):
        path = valid_output_stream_dir(self.dir, NullConfigContext)
        assert_equal(self.dir, path)

    def test_missing_dir(self):
        exception = assert_raises(ConfigError,
            valid_output_stream_dir, 'bogus-dir', NullConfigContext)
        assert_in("is not a directory", str(exception))

    def test_no_ro_dir(self):
        os.chmod(self.dir, stat.S_IRUSR)
        exception = assert_raises(ConfigError,
            valid_output_stream_dir, self.dir, NullConfigContext)
        assert_in("is not writable", str(exception))

    def test_missing_with_partial_context(self):
        dir = '/bogus/path/does/not/exist'
        context = config_utils.PartialConfigContext('path', 'MASTER')
        path = config_parse.valid_output_stream_dir(dir, context)
        assert_equal(path, dir)


class BuildFormatStringValidatorTestCase(TestCase):

    @setup
    def setup_keys(self):
        self.context = dict.fromkeys(['one', 'seven', 'stars'])
        self.validator = build_format_string_validator(self.context)

    def test_validator_passes(self):
        template = "The %(one)s thing I %(seven)s is %(stars)s"
        assert self.validator(template, NullConfigContext)

    def test_validator_error(self):
        template = "The %(one)s thing I %(seven)s is %(unknown)s"
        exception = assert_raises(ConfigError,
            self.validator, template, NullConfigContext)
        assert_in("Unknown context variable 'unknown'", str(exception))

    def test_validator_passes_with_context(self):
        template = "The %(one)s thing I %(seven)s is %(mars)s"
        context = config_utils.ConfigContext(None, None, {'mars': 'ok'}, None)
        assert self.validator(template, context)


class ValidateConfigMappingTestCase(TestCase):

    config = BASE_CONFIG + textwrap.dedent(
        """
        command_context:
            some_var: "The string"
        """)

    def test_validate_config_mapping_missing_master(self):
        config_mapping = {'other': mock.Mock()}
        seq = config_parse.validate_config_mapping(config_mapping)
        exception = assert_raises(ConfigError, list, seq)
        assert_in('requires a MASTER namespace', str(exception))

    def test_validate_config_mapping(self):
        master_config = manager.from_string(self.config)
        other_config = manager.from_string(NamedConfigTestCase.config)
        config_mapping = {'other': other_config, MASTER_NAMESPACE: master_config}
        result = list(config_parse.validate_config_mapping(config_mapping))
        assert_equal(len(result), 2)
        assert_equal(result[0][0], MASTER_NAMESPACE)
        assert_equal(result[1][0], 'other')


class ConfigContainerTestCase(TestCase):

    config = BASE_CONFIG + textwrap.dedent(
        """
        command_context:
            some_var: "The string"
        """)

    @setup
    def setup_container(self):
        other_config = yaml.load(NamedConfigTestCase.config)
        self.config_mapping = {
            MASTER_NAMESPACE: valid_config(yaml.load(self.config)),
            'other': validate_fragment('other', other_config)}
        self.container = config_parse.ConfigContainer(self.config_mapping)

    def test_create(self):
        config_mapping = {
            MASTER_NAMESPACE: yaml.load(self.config),
            'other': yaml.load(NamedConfigTestCase.config)}

        container = config_parse.ConfigContainer.create(config_mapping)
        assert_equal(set(container.configs.keys()), set(['MASTER', 'other']))

    def test_create_missing_master(self):
        config_mapping = {'other': mock.Mock()}
        assert_raises(ConfigError,
            config_parse.ConfigContainer.create, config_mapping)

    def test_get_job_and_service_names(self):
        job_names, service_names = self.container.get_job_and_service_names()
        expected = ['test_job1', 'test_job0', 'test_job3', 'test_job2', 'test_job4']
        assert_equal(job_names, expected)
        assert_equal(service_names, ['service0'])

    def test_get_jobs(self):
        expected = ['test_job1', 'test_job0', 'test_job3', 'test_job2', 'test_job4']
        assert_equal(expected, self.container.get_jobs().keys())

    def test_get_services(self):
        assert_equal(self.container.get_services().keys(), ['service0'])

    def test_get_node_names(self):
        node_names = self.container.get_node_names()
        expected = set(['node0', 'node1', 'NodePool'])
        assert_equal(node_names, expected)


class ValidateServiceTestCase(TestCase):

    def test_cast_restart_interval_deprecation(self):
        config = {'restart_interval': 50.0}
        context = config_utils.NullConfigContext
        casted_config = config_parse.ValidateService().cast(config, context)
        expected = {'restart_delay': 50.0, 'namespace': schema.MASTER_NAMESPACE}
        assert_equal(casted_config, expected)


class ValidateSSHOptionsTestCase(TestCase):

    @setup
    def setup_context(self):
        self.context = config_utils.NullConfigContext
        self.config = {'agent': True, 'identities': []}

    @mock.patch.dict('tron.config.config_parse.os.environ')
    def test_post_validation_failed(self):
        if 'SSH_AUTH_SOCK' in os.environ:
            del os.environ['SSH_AUTH_SOCK']
        assert_raises(ConfigError, config_parse.valid_ssh_options.validate,
            self.config, self.context)

    @mock.patch.dict('tron.config.config_parse.os.environ')
    def test_post_validation_success(self):
        os.environ['SSH_AUTH_SOCK'] = 'something'
        config = config_parse.valid_ssh_options.validate(self.config, self.context)
        assert_equal(config.agent, True)


class ValidateIdentityFileTestCase(TestCase):

    @setup
    def setup_context(self):
        self.context = config_utils.NullConfigContext
        self.private_file = tempfile.NamedTemporaryFile()

    def test_valid_identity_file_missing_private_key(self):
        exception = assert_raises(ConfigError,
            config_parse.valid_identity_file,'/file/not/exist', self.context)
        assert_in("Private key file", str(exception))

    def test_valid_identity_files_missing_public_key(self):
        filename = self.private_file.name
        exception = assert_raises(ConfigError,
            config_parse.valid_identity_file, filename, self.context)
        assert_in("Public key file", str(exception))

    def test_valid_identity_files_valid(self):
        filename = self.private_file.name
        fh_private = open(filename + '.pub', 'w')
        try:
            config = config_parse.valid_identity_file(filename, self.context)
        finally:
            fh_private.close()
            os.unlink(fh_private.name)
        assert_equal(config, filename)

    def test_valid_identity_files_missing_with_partial_context(self):
        path = '/bogus/file/does/not/exist'
        context = config_utils.PartialConfigContext('path', 'MASTER')
        file_path = config_parse.valid_identity_file(path, context)
        assert_equal(path, file_path)


class ValidKnownHostsFileTestCase(TestCase):

    @setup
    def setup_context(self):
        self.context = config_utils.NullConfigContext
        self.known_hosts_file = tempfile.NamedTemporaryFile()

    def test_valid_known_hosts_file_exists(self):
        filename = config_parse.valid_known_hosts_file(
            self.known_hosts_file.name, self.context)
        assert_equal(filename, self.known_hosts_file.name)

    def test_valid_known_hosts_file_missing(self):
        exception = assert_raises(ConfigError,
            config_parse.valid_known_hosts_file, '/bogus/path', self.context)
        assert_in('Known hosts file /bogus/path', str(exception))

    def test_valid_known_hosts_file_missing_partial_context(self):
        context = config_utils.PartialConfigContext
        expected = '/bogus/does/not/exist'
        filename = config_parse.valid_known_hosts_file(
            expected, context)
        assert_equal(filename, expected)


if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = config_utils_test
import datetime
import mock
from testify import TestCase, run, assert_equal, setup
from testify.assertions import assert_in
from tests.assertions import assert_raises
from tron.config import config_utils, ConfigError, schema
from tron.config.config_utils import build_list_of_type_validator, ConfigContext
from tron.config.config_utils import valid_identifier


class UniqueNameDictTestCase(TestCase):

    @setup
    def setup_dict(self):
        self.msg = "The key %s was there."
        self.dict = config_utils.UniqueNameDict(self.msg)

    def test_set_item_no_conflict(self):
        self.dict['a'] = 'something'
        assert_in('a', self.dict)

    def test_set_item_conflict(self):
        self.dict['a'] = 'something'
        assert_raises(ConfigError, self.dict.__setitem__, 'a', 'next_thing')


class ValidatorIdentifierTestCase(TestCase):

    def test_valid_identifier_too_long(self):
        assert_raises(ConfigError, valid_identifier, 'a' * 256, mock.Mock())

    def test_valid_identifier(self):
        name = 'avalidname'
        assert_equal(name, valid_identifier(name, mock.Mock()))

    def test_valid_identifier_invalid_character(self):
        for name in ['invalid space', '*name', '1numberstarted', 123, '']:
            assert_raises(ConfigError, valid_identifier, name, mock.Mock())


class BuildListOfTypeValidatorTestCase(TestCase):

    @setup
    def setup_validator(self):
        self.item_validator = mock.Mock()
        self.validator = build_list_of_type_validator(self.item_validator)

    def test_validator_passes(self):
        items, context = ['one', 'two'], mock.create_autospec(ConfigContext)
        self.validator(items, context)
        expected = [mock.call(item, context) for item in items]
        assert_equal(self.item_validator.mock_calls, expected)

    def test_validator_fails(self):
        self.item_validator.side_effect = ConfigError
        items, context = ['one', 'two'], mock.create_autospec(ConfigContext)
        assert_raises(ConfigError, self.validator, items, context)


class BuildEnumValidatorTestCase(TestCase):

    @setup
    def setup_enum_validator(self):
        self.enum = dict(a=1, b=2)
        self.validator = config_utils.build_enum_validator(self.enum)
        self.context = config_utils.NullConfigContext

    def test_validate(self):
        assert_equal(self.validator('a', self.context), 'a')
        assert_equal(self.validator('b', self.context), 'b')

    def test_invalid(self):
        exception = assert_raises(ConfigError, self.validator, 'c', self.context)
        assert_in('Value at  is not in %s: ' % str(set(self.enum)), str(exception))


class ValidTimeTestCase(TestCase):

    @setup
    def setup_config(self):
        self.context = config_utils.NullConfigContext

    def test_valid_time(self):
        time_spec = config_utils.valid_time("14:32", self.context)
        assert_equal(time_spec.hour, 14)
        assert_equal(time_spec.minute, 32)
        assert_equal(time_spec.second, 0)

    def test_valid_time_with_seconds(self):
        time_spec = config_utils.valid_time("14:32:12", self.context)
        assert_equal(time_spec.hour, 14)
        assert_equal(time_spec.minute, 32)
        assert_equal(time_spec.second, 12)

    def test_valid_time_invalid(self):
        assert_raises(ConfigError, config_utils.valid_time,
            "14:32:12:34", self.context)
        assert_raises(ConfigError, config_utils.valid_time, None, self.context)


class ValidTimeDeltaTestCase(TestCase):

    @setup
    def setup_config(self):
        self.context = config_utils.NullConfigContext

    def test_valid_time_delta_invalid(self):
        exception = assert_raises(ConfigError,
            config_utils.valid_time_delta,'no time', self.context)
        assert_in('not a valid time delta: no time', str(exception))

    def test_valid_time_delta_valid_seconds(self):
        for jitter in [' 82s ', '82 s', '82 sec', '82seconds  ']:
            delta = datetime.timedelta(seconds=82)
            assert_equal(delta, config_utils.valid_time_delta(jitter, self.context))

    def test_valid_time_delta_valid_minutes(self):
        for jitter in ['10m', '10 m', '10   min', '  10minutes']:
            delta = datetime.timedelta(seconds=600)
            assert_equal(delta, config_utils.valid_time_delta(jitter, self.context))

    def test_valid_time_delta_invalid_unit(self):
        for jitter in ['1 year', '3 mo', '3 months']:
            assert_raises(ConfigError,
                config_utils.valid_time_delta, jitter, self.context)


class ConfigContextTestCase(TestCase):

    def test_build_config_context(self):
        path, nodes, namespace = 'path', set([1,2,3]), 'namespace'
        command_context = mock.MagicMock()
        parent_context = config_utils.ConfigContext(
            path, nodes, command_context, namespace)

        child = parent_context.build_child_context('child')
        assert_equal(child.path, '%s.child' % path)
        assert_equal(child.nodes, nodes)
        assert_equal(child.namespace, namespace)
        assert_equal(child.command_context, command_context)
        assert not child.partial


StubConfigObject = schema.config_object_factory(
    'StubConfigObject',
    ['req1', 'req2'],
    ['opt1', 'opt2']
)

class StubValidator(config_utils.Validator):
    config_class = StubConfigObject

class ValidatorTestCase(TestCase):

    @setup
    def setup_validator(self):
        self.validator = StubValidator()

    def test_validate_with_none(self):
        expected_msg = "A StubObject is required"
        exception = assert_raises(ConfigError,
            self.validator.validate, None, config_utils.NullConfigContext)
        assert_in(expected_msg, str(exception))

    def test_validate_optional_with_none(self):
        self.validator.optional = True
        config = self.validator.validate(None, config_utils.NullConfigContext)
        assert_equal(config, None)

if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = manager_test
import os
import shutil
import tempfile
import mock
from testify import TestCase, assert_equal, run, setup, teardown
import yaml
from tests.assertions import assert_raises
from tests.testingutils import autospec_method
from tron.config import manager, ConfigError, schema


class FromStringTestCase(TestCase):

    def test_from_string_valid(self):
        content = "{'one': 'thing', 'another': 'thing'}\n"
        actual = manager.from_string(content)
        expected = {'one': 'thing', 'another': 'thing'}
        assert_equal(actual, expected)

    def test_from_string_invalid(self):
        content = "{} asdf"
        assert_raises(ConfigError, manager.from_string, content)

class ReadWriteTestCase(TestCase):

    @setup
    def setup_tempfile(self):
        self.filename = tempfile.NamedTemporaryFile().name

    @teardown
    def teardown_tempfile(self):
        os.unlink(self.filename)

    def test_read_write(self):
        content = {'one': 'stars', 'two': 'beers'}
        manager.write(self.filename, content)
        actual = manager.read(self.filename)
        assert_equal(content, actual)

    def test_read_raw_write_raw(self):
        content = "Some string"
        manager.write_raw(self.filename, content)
        actual = manager.read_raw(self.filename)
        assert_equal(content, actual)

class ManifestFileTestCase(TestCase):

    @setup
    def setup_manifest(self):
        self.temp_dir = tempfile.mkdtemp()
        self.manifest = manager.ManifestFile(self.temp_dir)
        self.manifest.create()

    @teardown
    def teardown_dir(self):
        shutil.rmtree(self.temp_dir)

    @mock.patch('tron.config.manager.os.path')
    @mock.patch('tron.config.manager.write', autospec=True)
    def test_create_exists(self, mock_write, mock_os):
        mock_os.isfile.return_value = True
        self.manifest.create()
        assert not mock_write.call_count

    def test_create(self):
        assert_equal(manager.read(self.manifest.filename), {})

    def test_add(self):
        self.manifest.add('zing', 'zing.yaml')
        expected = {'zing': 'zing.yaml'}
        assert_equal(manager.read(self.manifest.filename), expected)

    def test_get_file_mapping(self):
        file_mapping = {
            'one': 'a.yaml',
            'two': 'b.yaml',
        }
        manager.write(self.manifest.filename, file_mapping)
        assert_equal(self.manifest.get_file_mapping(), file_mapping)


class ConfigManagerTestCase(TestCase):

    content = {'one': 'stars', 'two': 'other'}
    raw_content = "{'one': 'stars', 'two': 'other'}\n"

    @setup
    def setup_config_manager(self):
        self.temp_dir = tempfile.mkdtemp()
        self.manager = manager.ConfigManager(self.temp_dir)
        self.manifest = mock.create_autospec(manager.ManifestFile)
        self.manager.manifest = self.manifest

    @teardown
    def teardown_dir(self):
        shutil.rmtree(self.temp_dir)

    def test_build_file_path(self):
        path = self.manager.build_file_path('what')
        assert_equal(path, os.path.join(self.temp_dir, 'what.yaml'))

    def test_build_file_path_with_invalid_chars(self):
        path = self.manager.build_file_path('/etc/passwd')
        assert_equal(path, os.path.join(self.temp_dir, '_etc_passwd.yaml'))
        path = self.manager.build_file_path('../../etc/passwd')
        assert_equal(path, os.path.join(self.temp_dir, '______etc_passwd.yaml'))

    def test_read_raw_config(self):
        name = 'name'
        path = os.path.join(self.temp_dir, name)
        manager.write(path, self.content)
        self.manifest.get_file_name.return_value = path
        config = self.manager.read_raw_config(name)
        assert_equal(config, yaml.dump(self.content))

    def test_write_config(self):
        name = 'filename'
        path = self.manager.build_file_path(name)
        self.manifest.get_file_name.return_value = path
        autospec_method(self.manager.validate_with_fragment)
        self.manager.write_config(name, self.raw_content)
        assert_equal(manager.read(path), self.content)
        self.manifest.get_file_name.assert_called_with(name)
        assert not self.manifest.add.call_count
        self.manager.validate_with_fragment.assert_called_with(name, self.content)

    def test_write_config_new_name(self):
        name = 'filename2'
        path = self.manager.build_file_path(name)
        self.manifest.get_file_name.return_value = None
        autospec_method(self.manager.validate_with_fragment)
        self.manager.write_config(name, self.raw_content)
        assert_equal(manager.read(path), self.content)
        self.manifest.get_file_name.assert_called_with(name)
        self.manifest.add.assert_called_with(name, path)

    @mock.patch('tron.config.manager.config_parse.ConfigContainer', autospec=True)
    def test_validate_with_fragment(self, mock_config_container):
        name = 'the_name'
        name_mapping = {'something': 'content', name: 'old_content'}
        autospec_method(self.manager.get_config_name_mapping)
        self.manager.get_config_name_mapping.return_value = name_mapping
        self.manager.validate_with_fragment(name, self.content)
        expected_mapping = dict(name_mapping)
        expected_mapping[name] = self.content
        mock_config_container.create.assert_called_with(expected_mapping)

    @mock.patch('tron.config.manager.read')
    @mock.patch('tron.config.manager.config_parse.ConfigContainer', autospec=True)
    def test_load(self, mock_config_container, mock_read):
        content_items = self.content.items()
        self.manifest.get_file_mapping().iteritems.return_value = content_items
        container = self.manager.load()
        self.manifest.get_file_mapping.assert_called_with()
        assert_equal(container, mock_config_container.create.return_value)

        expected = dict((name, call.return_value)
            for ((name, _), call) in zip(content_items, mock_read.mock_calls))
        mock_config_container.create.assert_called_with(expected)

    def test_get_hash_default(self):
        self.manifest.__contains__.return_value = False
        hash_digest = self.manager.get_hash('name')
        assert_equal(hash_digest, self.manager.DEFAULT_HASH)

    def test_get_hash(self):
        content = "OkOkOk"
        autospec_method(self.manager.read_raw_config, return_value=content)
        self.manifest.__contains__.return_value = True
        hash_digest = self.manager.get_hash('name')
        assert_equal(hash_digest, manager.hash_digest(content))


class CreateNewConfigTestCase(TestCase):

    @mock.patch('tron.config.manager.os.makedirs', autospec=True)
    @mock.patch('tron.config.manager.ManifestFile', autospec=True)
    @mock.patch('tron.config.manager.write_raw', autospec=True)
    def test_create_new_config(self, mock_write, mock_manifest, mock_makedirs):
        path, master_content = '/bogus/path/', mock.Mock()
        filename = '/bogus/path/MASTER.yaml'
        manifest = mock_manifest.return_value
        manifest.get_file_name.return_value = None

        manager.create_new_config(path, master_content)
        mock_makedirs.assert_called_with(path)
        mock_write.assert_called_with(filename, master_content)
        manifest.create.assert_called_with()
        manifest.add.assert_called_with(schema.MASTER_NAMESPACE, filename)


if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = schedule_parse_test
# -*- coding: utf-8 -*-
import datetime
import mock
from testify import TestCase, run, assert_equal, assert_raises

from tron.config import schedule_parse, ConfigError, config_utils


class PadSequenceTestCase(TestCase):

    def test_pad_sequence_short(self):
        expected = [0, 1, 2, 3, None, None]
        assert_equal(schedule_parse.pad_sequence(range(4), 6), expected)

    def test_pad_sequence_long(self):
        expected = [0, 1, 2, 3]
        assert_equal(schedule_parse.pad_sequence(range(6), 4), expected)

    def test_pad_sequence_exact(self):
        expected = [0, 1, 2, 3]
        assert_equal(schedule_parse.pad_sequence(range(4), 4), expected)

    def test_pad_sequence_empty(self):
        expected = ["a", "a"]
        assert_equal(schedule_parse.pad_sequence([], 2, "a"), expected)

    def test_pad_negative_size(self):
        assert_equal(schedule_parse.pad_sequence([], -2, "a"), [])


class ScheduleConfigFromStringTestCase(TestCase):

    @mock.patch('tron.config.schedule_parse.parse_groc_expression', autospec=True)
    def test_groc_config(self, mock_parse_groc):
        schedule = 'every Mon,Wed at 12:00'
        context = config_utils.NullConfigContext
        config = schedule_parse.schedule_config_from_string(schedule, context)
        assert_equal(config, mock_parse_groc.return_value)
        generic_config = schedule_parse.ConfigGenericSchedule(
            'groc daily', schedule, None)
        mock_parse_groc.assert_called_with(generic_config, context)

    def test_constant_config(self):
        schedule = 'constant'
        context = config_utils.NullConfigContext
        config = schedule_parse.schedule_config_from_string(schedule, context)
        assert_equal(config, schedule_parse.ConfigConstantScheduler())


class ValidSchedulerTestCase(TestCase):

    @mock.patch('tron.config.schedule_parse.schedulers', autospec=True)
    def assert_validation(self, schedule, expected, mock_schedulers):
        context = config_utils.NullConfigContext
        config = schedule_parse.valid_schedule(schedule, context)
        mock_schedulers.__getitem__.assert_called_with('cron')
        func = mock_schedulers.__getitem__.return_value
        assert_equal(config, func.return_value)
        func.assert_called_with(expected, context)

    def test_cron_from_dict(self):
        schedule = {'type': 'cron', 'value': '* * * * *'}
        config = schedule_parse.ConfigGenericSchedule(
            'cron', schedule['value'], datetime.timedelta())
        self.assert_validation(schedule, config)

    def test_cron_from_dict_with_jitter(self):
        schedule = {'type': 'cron', 'value': '* * * * *', 'jitter': '5 min'}
        config = schedule_parse.ConfigGenericSchedule(
            'cron', schedule['value'], datetime.timedelta(minutes=5))
        self.assert_validation(schedule, config)


class ValidCronSchedulerTestCase(TestCase):
    _suites = ['integration']

    def validate(self, line):
        config = schedule_parse.ConfigGenericSchedule('cron', line, None)
        context = config_utils.NullConfigContext
        return schedule_parse.valid_cron_scheduler(config, context)

    def test_valid_config(self):
        config = self.validate('5 0 L * *')
        assert_equal(config.minutes, [5])
        assert_equal(config.months, None)
        assert_equal(config.monthdays, ['LAST'])

    def test_invalid_config(self):
        assert_raises(ConfigError, self.validate, '* * *')


class ValidDailySchedulerTestCase(TestCase):

    def validate(self, config):
        context = config_utils.NullConfigContext
        config = schedule_parse.ConfigGenericSchedule('daily', config, None)
        return schedule_parse.valid_daily_scheduler(config, context)

    def assert_parse(self, config, expected):
        config = self.validate(config)
        expected = schedule_parse.ConfigDailyScheduler(*expected, jitter=None)
        assert_equal(config, expected)

    def test_valid_daily_scheduler_start_time(self):
        expected = ('14:32 ', 14, 32, 0, set())
        self.assert_parse('14:32', expected)

    def test_valid_daily_scheduler_just_days(self):
        expected = ("00:00:00 MWS", 0, 0, 0, set([1, 3, 6]))
        self.assert_parse("00:00:00 MWS", expected)

    def test_valid_daily_scheduler_time_and_day(self):
        expected = ("17:02:44 SU", 17, 2, 44, set([0, 6]))
        self.assert_parse("17:02:44 SU", expected)

    def test_valid_daily_scheduler_invalid_start_time(self):
        assert_raises(ConfigError, self.validate, "5 MWF")
        assert_raises(ConfigError, self.validate, "05:30:45:45 MWF")
        assert_raises(ConfigError, self.validate, "25:30:45 MWF")

    def test_valid_daily_scheduler_invalid_days(self):
        assert_raises(ConfigError, self.validate, "SUG")
        assert_raises(ConfigError, self.validate, "3")


class ValidIntervalSchedulerTestCase(TestCase):

    def validate(self, config_value):
        context = config_utils.NullConfigContext
        config = schedule_parse.ConfigGenericSchedule(
            'interval', config_value, None)
        return schedule_parse.valid_interval_scheduler(config, context)

    def test_valid_interval_scheduler_shortcut(self):
        config = self.validate("hourly")
        expected = datetime.timedelta(hours=1)
        assert_equal(config.timedelta, expected)

    def test_valid_interval_scheduler_minutes(self):
        config = self.validate("5 minutes")
        expected = datetime.timedelta(minutes=5)
        assert_equal(config.timedelta, expected)

    def test_valid_interval_scheduler_hours(self):
        for spec in ['6h', '6 hours', '6 h', '6 hrs', '6 hour', u'6 hours', u'6hour']:
            config = self.validate(spec)
            expected = datetime.timedelta(hours=6)
            assert_equal(config.timedelta, expected)

    def test_valid_interval_scheduler_invalid_tokens(self):
        assert_raises(ConfigError, self.validate, "6 hours 22 minutes")

    def test_valid_interval_scheduler_unknown_unit(self):
        assert_raises(ConfigError, self.validate, "22 beats")

    def test_valid_interval_scheduler_bogus(self):
        assert_raises(ConfigError, self.validate, "asdasd.asd")

    def test_valid_interval_scheduler_underscore(self):
        assert_raises(ConfigError, self.validate, u"6_minute")

    def test_valid_interval_scheduler_unicode(self):
        assert_raises(ConfigError, self.validate, u"6 ຖminute")

    def test_valid_interval_scheduler_alias(self):
        config = self.validate("  hourly  ")
        expected = datetime.timedelta(hours=1)
        assert_equal(config.timedelta, expected)


if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = actiongraph_test
from testify import setup, run, TestCase, assert_equal, turtle, assert_raises

from tron.core import actiongraph

class ActionGraphTestCase(TestCase):

    @setup
    def setup_graph(self):
        self.action_names = [
            'base_one', 'base_two', 'dep_one', 'dep_one_one', 'dep_multi']
        am = self.action_map = dict(
            (name, turtle.Turtle(name=name)) for name in self.action_names)

        am['dep_multi'].required_actions   = [am['dep_one_one'], am['base_two']]
        am['dep_one_one'].required_actions = [am['dep_one']]
        am['dep_one'].required_actions     = [am['base_one']]

        self.graph = [am['base_one'], am['base_two']]
        self.action_graph = actiongraph.ActionGraph(self.graph, am)

    def test_from_config(self):
        config = dict(
            (name, turtle.Turtle(name=name, node='first', requires=[]))
            for name in self.action_names)
        config['dep_multi'].requires    = ['dep_one_one', 'base_two']
        config['dep_one_one'].requires  = ['dep_one']
        config['dep_one'].requires      = ['base_one']

        built_graph = actiongraph.ActionGraph.from_config(config)
        am = built_graph.action_map

        graph_base_names = set(a.name for a in built_graph.graph)
        assert_equal(graph_base_names, set(a.name for a in self.graph))
        assert_equal(graph_base_names, set(['base_one', 'base_two']))
        assert_equal(
            set(am['dep_multi'].required_actions),
            set([am['dep_one_one'], am['base_two']])
        )

        assert_equal(set(am.keys()), set(self.action_names))
        assert_equal(am['base_one'].dependent_actions, [am['dep_one']])
        assert_equal(am['dep_one'].dependent_actions, [am['dep_one_one']])

    def test_actions_for_names(self):
        actions = list(
                self.action_graph.actions_for_names(['base_one', 'dep_multi']))
        expected_actions = [
                self.action_map['base_one'], self.action_map['dep_multi']]
        assert_equal(actions, expected_actions)

    def test__getitem__(self):
        assert_equal(self.action_graph['base_one'], self.action_map['base_one'])

    def test__getitem__miss(self):
        assert_raises(KeyError, lambda: self.action_graph['unknown'])

    def test__eq__(self):
        other_graph = turtle.Turtle(
                graph=self.graph, action_map=self.action_map)
        assert_equal(self.action_graph, other_graph)

        other_graph.graph = None
        assert not self.action_graph == other_graph

    def test__ne__(self):
        other_graph = turtle.Turtle
        assert self.graph != other_graph

if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = actionrun_test
import datetime
import shutil
import tempfile
import mock

from testify import run, setup, TestCase, assert_equal, turtle, teardown
from testify.assertions import assert_raises, assert_in
from tests import testingutils
from tests.assertions import assert_length
from tests.testingutils import Turtle, autospec_method

from tron import node, actioncommand
from tron.core import jobrun, actiongraph
from tron.core.actionrun import ActionCommand, ActionRun
from tron.core.actionrun import ActionRunCollection, ActionRunFactory
from tron.serialize import filehandler


class ActionRunFactoryTestCase(TestCase):

    @setup
    def setup_action_runs(self):
        self.run_time = datetime.datetime(2012, 3, 14, 15, 9 ,26)
        actions = [Turtle(name='act1'), Turtle(name='act2')]
        self.action_graph = actiongraph.ActionGraph(
                actions, dict((a.name, a) for a in actions))

        mock_node = mock.create_autospec(node.Node)
        self.job_run = jobrun.JobRun('jobname', 7, self.run_time, mock_node,
                action_graph=self.action_graph)

        self.action_state_data = {
            'job_run_id':       'job_run_id',
            'action_name':      'act1',
            'state':            'succeeded',
            'run_time':         'the_run_time',
            'start_time':       None,
            'end_time':         None,
            'command':          'do action1',
            'node_name':        'anode'
        }
        self.action_runner = mock.create_autospec(actioncommand.SubprocessActionRunnerFactory)

    def test_build_action_run_collection(self):
        collection = ActionRunFactory.build_action_run_collection(
            self.job_run, self.action_runner)
        assert_equal(collection.action_graph, self.action_graph)
        assert_in('act1', collection.run_map)
        assert_in('act2', collection.run_map)
        assert_length(collection.run_map, 2)
        assert_equal(collection.run_map['act1'].action_name, 'act1')

    def test_action_run_collection_from_state(self):
        state_data = [self.action_state_data]
        cleanup_action_state_data = {
            'job_run_id':       'job_run_id',
            'action_name':      'cleanup',
            'state':            'succeeded',
            'run_time':         self.run_time,
            'start_time':       None,
            'end_time':         None,
            'command':          'do cleanup',
            'node_name':        'anode'
        }
        collection = ActionRunFactory.action_run_collection_from_state(
            self.job_run, state_data, cleanup_action_state_data)

        assert_equal(collection.action_graph, self.action_graph)
        assert_length(collection.run_map, 2)
        assert_equal(collection.run_map['act1'].action_name, 'act1')
        assert_equal(collection.run_map['cleanup'].action_name, 'cleanup')

    def test_build_run_for_action(self):
        action = Turtle(
            name='theaction', node_pool=None, is_cleanup=False, command="doit")
        action_run = ActionRunFactory.build_run_for_action(
            self.job_run, action, self.action_runner)

        assert_equal(action_run.job_run_id, self.job_run.id)
        assert_equal(action_run.node, self.job_run.node)
        assert_equal(action_run.action_name, action.name)
        assert not action_run.is_cleanup
        assert_equal(action_run.command, action.command)

    def test_build_run_for_action_with_node(self):
        action = Turtle(name='theaction', is_cleanup=True, command="doit")
        action_run = ActionRunFactory.build_run_for_action(
            self.job_run, action, self.action_runner)

        assert_equal(action_run.job_run_id, self.job_run.id)
        assert_equal(action_run.node, action.node_pool.next.returns[0])
        assert action_run.is_cleanup
        assert_equal(action_run.action_name, action.name)
        assert_equal(action_run.command, action.command)

    def test_action_run_from_state(self):
        state_data = self.action_state_data
        action_run = ActionRunFactory.action_run_from_state(
                self.job_run, state_data)

        assert_equal(action_run.job_run_id, state_data['job_run_id'])
        assert not action_run.is_cleanup


class ActionRunTestCase(TestCase):

    @setup
    def setup_action_run(self):
        self.output_path = filehandler.OutputPath(tempfile.mkdtemp())
        self.action_runner = mock.create_autospec(
            actioncommand.NoActionRunnerFactory)
        self.command = "do command %(actionname)s"
        self.rendered_command = "do command action_name"
        self.action_run = ActionRun(
                "id",
                "action_name",
                mock.create_autospec(node.Node),
                self.command,
                output_path=self.output_path,
                action_runner=self.action_runner)

    @teardown
    def teardown_action_run(self):
        shutil.rmtree(self.output_path.base, ignore_errors=True)

    def test_init_state(self):
        assert_equal(self.action_run.state, ActionRun.STATE_SCHEDULED)

    def test_start(self):
        self.action_run.machine.transition('ready')
        assert self.action_run.start()
        assert self.action_run.is_starting
        assert self.action_run.start_time

    def test_start_bad_state(self):
        self.action_run.fail()
        assert not self.action_run.start()

    def test_start_invalid_command(self):
        self.action_run.bare_command = "%(notfound)s"
        self.action_run.machine.transition('ready')
        assert not self.action_run.start()
        assert self.action_run.is_failed
        assert_equal(self.action_run.exit_status, -1)

    def test_start_node_error(self):
        def raise_error(c):
            raise node.Error("The error")
        self.action_run.node = turtle.Turtle(submit_command=raise_error)
        self.action_run.machine.transition('ready')
        assert not self.action_run.start()
        assert_equal(self.action_run.exit_status, -2)
        assert self.action_run.is_failed

    @mock.patch('tron.core.actionrun.filehandler', autospec=True)
    def test_build_action_command(self, mock_filehandler):
        autospec_method(self.action_run.watch)
        serializer = mock_filehandler.OutputStreamSerializer.return_value
        action_command = self.action_run.build_action_command()
        assert_equal(action_command, self.action_run.action_command)
        assert_equal(action_command, self.action_runner.create.return_value)
        self.action_runner.create.assert_called_with(
            self.action_run.id, self.action_run.command, serializer)
        mock_filehandler.OutputStreamSerializer.assert_called_with(
            self.action_run.output_path)
        self.action_run.watch.assert_called_with(action_command)

    def test_handler_running(self):
        self.action_run.build_action_command()
        self.action_run.machine.transition('start')
        assert self.action_run.handler(
                self.action_run.action_command, ActionCommand.RUNNING)
        assert self.action_run.is_running

    def test_handler_failstart(self):
        self.action_run.build_action_command()
        assert self.action_run.handler(
                self.action_run.action_command, ActionCommand.FAILSTART)
        assert self.action_run.is_failed

    def test_handler_exiting_fail(self):
        self.action_run.build_action_command()
        self.action_run.action_command.exit_status = -1
        self.action_run.machine.transition('start')
        assert self.action_run.handler(
            self.action_run.action_command, ActionCommand.EXITING)
        assert self.action_run.is_failed
        assert_equal(self.action_run.exit_status, -1)

    def test_handler_exiting_success(self):
        self.action_run.build_action_command()
        self.action_run.action_command.exit_status = 0
        self.action_run.machine.transition('start')
        self.action_run.machine.transition('started')
        assert self.action_run.handler(
            self.action_run.action_command, ActionCommand.EXITING)
        assert self.action_run.is_succeeded
        assert_equal(self.action_run.exit_status, 0)

    def test_handler_exiting_failunknown(self):
        self.action_run.action_command = mock.create_autospec(
            actioncommand.ActionCommand, exit_status=None)
        self.action_run.machine.transition('start')
        self.action_run.machine.transition('started')
        assert self.action_run.handler(
            self.action_run.action_command, ActionCommand.EXITING)
        assert self.action_run.is_unknown
        assert_equal(self.action_run.exit_status, None)

    def test_handler_unhandled(self):
        self.action_run.build_action_command()
        assert self.action_run.handler(
            self.action_run.action_command, ActionCommand.PENDING) is None
        assert self.action_run.is_scheduled

    def test_success(self):
        assert self.action_run.ready()
        self.action_run.machine.transition('start')
        self.action_run.machine.transition('started')

        assert self.action_run.is_running
        assert self.action_run.success()
        assert not self.action_run.is_running
        assert self.action_run.is_done
        assert self.action_run.end_time
        assert_equal(self.action_run.exit_status, 0)

    def test_success_bad_state(self):
        self.action_run.cancel()
        assert not self.action_run.success()

    def test_failure(self):
        self.action_run.fail(1)
        assert not self.action_run.is_running
        assert self.action_run.is_done
        assert self.action_run.end_time
        assert_equal(self.action_run.exit_status, 1)

    def test_failure_bad_state(self):
        self.action_run.fail(444)
        assert not self.action_run.fail(123)
        assert_equal(self.action_run.exit_status, 444)

    def test_skip(self):
        assert not self.action_run.is_running
        self.action_run.ready()
        assert self.action_run.start()

        assert self.action_run.fail(-1)
        assert self.action_run.skip()
        assert self.action_run.is_skipped

    def test_skip_bad_state(self):
        assert not self.action_run.skip()

    def test_state_data(self):
        state_data = self.action_run.state_data
        assert_equal(state_data['command'], self.action_run.bare_command)
        assert not self.action_run.rendered_command
        assert not state_data['rendered_command']

    def test_state_data_after_rendered(self):
        command = self.action_run.command
        state_data = self.action_run.state_data
        assert_equal(state_data['command'], command)
        assert_equal(state_data['rendered_command'], command)

    def test_render_command(self):
        self.action_run.context = {'stars': 'bright'}
        self.action_run.bare_command = "%(stars)s"
        assert_equal(self.action_run.render_command(), 'bright')

    def test_command_not_yet_rendered(self):
        assert_equal(self.action_run.command, self.rendered_command)

    def test_command_already_rendered(self):
        assert self.action_run.command
        self.action_run.bare_command = "new command"
        assert_equal(self.action_run.command, self.rendered_command)

    def test_command_failed_render(self):
        self.action_run.bare_command = "%(this_is_missing)s"
        assert_equal(self.action_run.command, ActionRun.FAILED_RENDER)

    def test_is_complete(self):
        self.action_run.machine.state = ActionRun.STATE_SUCCEEDED
        assert self.action_run.is_complete
        self.action_run.machine.state = ActionRun.STATE_SKIPPED
        assert self.action_run.is_complete
        self.action_run.machine.state = ActionRun.STATE_RUNNING
        assert not self.action_run.is_complete

    def test_is_broken(self):
        self.action_run.machine.state = ActionRun.STATE_UNKNOWN
        assert self.action_run.is_broken
        self.action_run.machine.state = ActionRun.STATE_FAILED
        assert self.action_run.is_broken
        self.action_run.machine.state = ActionRun.STATE_QUEUED
        assert not self.action_run.is_broken

    def test__getattr__(self):
        assert not self.action_run.is_succeeded
        assert not self.action_run.is_failed
        assert not self.action_run.is_queued
        assert self.action_run.is_scheduled
        assert self.action_run.cancel()
        assert self.action_run.is_cancelled

    def test__getattr__missing_attribute(self):
        assert_raises(AttributeError,
            self.action_run.__getattr__, 'is_not_a_real_state')


class ActionRunStateRestoreTestCase(testingutils.MockTimeTestCase):

    now = datetime.datetime(2012, 3, 14, 15, 19)

    @setup
    def setup_action_run(self):
        self.parent_context = {}
        self.output_path = ['one', 'two']
        self.state_data = {
            'job_run_id':       'theid',
            'action_name':      'theaction',
            'node_name':        'anode',
            'command':          'do things',
            'start_time':       'start_time',
            'end_time':         'end_time',
            'state':            'succeeded'
        }
        self.run_node = Turtle()

    def test_from_state(self):
        state_data = self.state_data
        action_run = ActionRun.from_state(state_data, self.parent_context,
                list(self.output_path), self.run_node)

        for key, value in self.state_data.iteritems():
            if key in ['state', 'node_name']:
                continue
            assert_equal(getattr(action_run, key), value)

        assert action_run.is_succeeded
        assert not action_run.is_cleanup
        assert_equal(action_run.output_path[:2], self.output_path)

    def test_from_state_running(self):
        self.state_data['state'] = 'running'
        action_run = ActionRun.from_state(self.state_data,
                self.parent_context, self.output_path, self.run_node)
        assert action_run.is_unknown
        assert_equal(action_run.exit_status, 0)
        assert_equal(action_run.end_time, self.now)

    def test_from_state_queued(self):
        self.state_data['state'] = 'queued'
        action_run = ActionRun.from_state(self.state_data, self.parent_context,
                self.output_path, self.run_node)
        assert action_run.is_queued

    def test_from_state_no_node_name(self):
        del self.state_data['node_name']
        action_run = ActionRun.from_state(self.state_data,
                self.parent_context, self.output_path, self.run_node)
        assert_equal(action_run.node, self.run_node)

    @mock.patch('tron.core.actionrun.node.NodePoolRepository')
    def test_from_state_with_node_exists(self, mock_store):
        ActionRun.from_state(self.state_data,
                self.parent_context, self.output_path, self.run_node)
        mock_store.get_instance().get_node.assert_called_with(
            self.state_data['node_name'], self.run_node)

    def test_from_state_before_rendered_command(self):
        self.state_data['command'] = 'do things %(actionname)s'
        self.state_data['rendered_command'] = None
        action_run = ActionRun.from_state(self.state_data,
                self.parent_context, self.output_path, self.run_node)
        assert_equal(action_run.bare_command, self.state_data['command'])
        assert not action_run.rendered_command

    def test_from_state_old_state(self):
        self.state_data['command'] = 'do things %(actionname)s'
        action_run = ActionRun.from_state(self.state_data,
                self.parent_context, self.output_path, self.run_node)
        assert_equal(action_run.bare_command, self.state_data['command'])
        assert not action_run.rendered_command

    def test_from_state_after_rendered_command(self):
        self.state_data['command'] = 'do things theaction'
        self.state_data['rendered_command'] = self.state_data['command']
        action_run = ActionRun.from_state(self.state_data,
                self.parent_context, self.output_path, self.run_node)
        assert_equal(action_run.bare_command, self.state_data['command'])
        assert_equal(action_run.rendered_command, self.state_data['command'])


class ActionRunCollectionTestCase(TestCase):

    def _build_run(self, name):
        mock_node = mock.create_autospec(node.Node)
        return ActionRun("id", name, mock_node, self.command,
            output_path=self.output_path)

    @setup
    def setup_runs(self):
        action_names = ['action_name', 'second_name', 'cleanup']

        action_graph = [
            mock.Mock(name=name, required_actions=[])
            for name in action_names
        ]
        self.action_graph = actiongraph.ActionGraph(
            action_graph, dict((a.name, a) for a in action_graph))
        self.output_path = filehandler.OutputPath(tempfile.mkdtemp())
        self.command = "do command"
        self.action_runs = [self._build_run(name) for name in action_names]
        self.run_map = dict((a.action_name, a) for a in self.action_runs)
        self.run_map['cleanup'].is_cleanup = True
        self.collection = ActionRunCollection(self.action_graph, self.run_map)

    @teardown
    def teardown_action_run(self):
        shutil.rmtree(self.output_path.base, ignore_errors=True)

    def test__init__(self):
        assert_equal(self.collection.action_graph, self.action_graph)
        assert_equal(self.collection.run_map, self.run_map)
        assert self.collection.proxy_action_runs_with_cleanup

    def test_action_runs_for_actions(self):
        actions = [Turtle(name='action_name')]
        action_runs = self.collection.action_runs_for_actions(actions)
        assert_equal(list(action_runs), self.action_runs[:1])

    def test_get_action_runs_with_cleanup(self):
        runs = self.collection.get_action_runs_with_cleanup()
        assert_equal(set(runs), set(self.action_runs))

    def test_get_action_runs(self):
        runs = self.collection.get_action_runs()
        assert_equal(set(runs), set(self.action_runs[:2]))

    def test_cleanup_action_run(self):
        assert_equal(self.action_runs[2], self.collection.cleanup_action_run)

    def test_state_data(self):
        state_data = self.collection.state_data
        assert_length(state_data, len(self.action_runs[:2]))

    def test_cleanup_action_state_data(self):
        state_data = self.collection.cleanup_action_state_data
        assert_equal(state_data['action_name'], 'cleanup')

    def test_cleanup_action_state_data_no_cleanup_action(self):
        del self.collection.run_map['cleanup']
        assert not self.collection.cleanup_action_state_data

    def test_get_startable_action_runs(self):
        action_runs = self.collection.get_startable_action_runs()
        assert_equal(set(action_runs), set(self.action_runs[:2]))

    def test_get_startable_action_runs_none(self):
        self.collection.run_map.clear()
        action_runs = self.collection.get_startable_action_runs()
        assert_equal(set(action_runs), set())

    def test_has_startable_action_runs(self):
        assert self.collection.has_startable_action_runs

    def test_has_startable_action_runs_false(self):
        self.collection.run_map.clear()
        assert not self.collection.has_startable_action_runs

    def test_is_complete_false(self):
        assert not self.collection.is_complete

    def test_is_complete_true(self):
        for action_run in self.collection.action_runs_with_cleanup:
            action_run.machine.state = ActionRun.STATE_SKIPPED
        assert self.collection.is_complete

    def test_is_done_false(self):
        assert not self.collection.is_done

    def test_is_done_false_because_of_running(self):
        action_run = self.collection.run_map['action_name']
        action_run.machine.state = ActionRun.STATE_RUNNING
        assert not self.collection.is_done

    def test_is_done_true_because_blocked(self):
        self.run_map['action_name'].machine.state = ActionRun.STATE_FAILED
        self.run_map['second_name'].machine.state = ActionRun.STATE_QUEUED
        autospec_method(self.collection._is_run_blocked)
        blocked_second_action_run = lambda ar: ar == self.run_map['second_name']
        self.collection._is_run_blocked.side_effect = blocked_second_action_run
        assert self.collection.is_done
        assert self.collection.is_failed

    def test_is_done_true(self):
        for action_run in self.collection.action_runs_with_cleanup:
            action_run.machine.state = ActionRun.STATE_FAILED
        assert self.collection.is_done

    def test_is_failed_false_not_done(self):
        self.run_map['action_name'].machine.state = ActionRun.STATE_FAILED
        assert not self.collection.is_failed

    def test_is_failed_false_no_failed(self):
        for action_run in self.collection.action_runs_with_cleanup:
            action_run.machine.state = ActionRun.STATE_SUCCEEDED
        assert not self.collection.is_failed

    def test_is_failed_true(self):
        for action_run in self.collection.action_runs_with_cleanup:
            action_run.machine.state = ActionRun.STATE_FAILED
        assert self.collection.is_failed

    def test__getattr__(self):
        assert self.collection.is_scheduled
        assert not self.collection.is_cancelled
        assert not self.collection.is_running
        assert self.collection.ready()

    def test__str__(self):
        self.collection._is_run_blocked = lambda r: r.action_name != 'cleanup'
        expected = [
            "ActionRunCollection",
            "second_name(scheduled:blocked)",
            "action_name(scheduled:blocked)",
            "cleanup(scheduled)"
        ]
        for expectation in expected:
            assert_in(expectation, str(self.collection))

    def test_end_time(self):
        max_end_time = datetime.datetime(2013, 6, 15)
        self.run_map['action_name'].machine.state = ActionRun.STATE_FAILED
        self.run_map['action_name'].end_time = datetime.datetime(2013, 5, 12)
        self.run_map['second_name'].machine.state = ActionRun.STATE_SUCCEEDED
        self.run_map['second_name'].end_time = max_end_time
        assert_equal(self.collection.end_time, max_end_time)

    def test_end_time_not_done(self):
        self.run_map['action_name'].end_time = datetime.datetime(2013, 5, 12)
        self.run_map['action_name'].machine.state = ActionRun.STATE_FAILED
        self.run_map['second_name'].end_time = None
        self.run_map['second_name'].machine.state = ActionRun.STATE_RUNNING
        assert_equal(self.collection.end_time, None)

    def test_end_time_not_started(self):
        assert_equal(self.collection.end_time, None)


class ActionRunCollectionIsRunBlockedTestCase(TestCase):

    def _build_run(self, name):
        mock_node = mock.create_autospec(node.Node)
        return ActionRun("id", name, mock_node, self.command,
            output_path=self.output_path)

    @setup
    def setup_collection(self):
        action_names = ['action_name', 'second_name', 'cleanup']

        action_graph = [
            Turtle(name=name, required_actions=[])
            for name in action_names
        ]
        self.second_act = second_act = action_graph.pop(1)
        second_act.required_actions.append(action_graph[0])
        action_map = dict((a.name, a) for a in action_graph)
        action_map['second_name'] = second_act
        self.action_graph = actiongraph.ActionGraph(action_graph, action_map)

        self.output_path = filehandler.OutputPath(tempfile.mkdtemp())
        self.command = "do command"
        self.action_runs = [self._build_run(name) for name in action_names]
        self.run_map = dict((a.action_name, a) for a in self.action_runs)
        self.run_map['cleanup'].is_cleanup = True
        self.collection = ActionRunCollection(self.action_graph, self.run_map)

    @teardown
    def teardown_action_run(self):
        shutil.rmtree(self.output_path.base, ignore_errors=True)

    def test_is_run_blocked_no_required_actions(self):
        assert not self.collection._is_run_blocked(self.run_map['action_name'])

    def test_is_run_blocked_completed_run(self):
        self.run_map['second_name'].machine.state = ActionRun.STATE_FAILED
        assert not self.collection._is_run_blocked(self.run_map['second_name'])

        self.run_map['second_name'].machine.state = ActionRun.STATE_RUNNING
        assert not self.collection._is_run_blocked(self.run_map['second_name'])

    def test_is_run_blocked_required_actions_completed(self):
        self.run_map['action_name'].machine.state = ActionRun.STATE_SKIPPED
        assert not self.collection._is_run_blocked(self.run_map['second_name'])

    def test_is_run_blocked_required_actions_blocked(self):
        third_act = Turtle(name='third_act', required_actions=[self.second_act])
        self.action_graph.action_map['third_act'] = third_act
        self.run_map['third_act'] = self._build_run('third_act')

        self.run_map['action_name'].machine.state = ActionRun.STATE_FAILED
        assert self.collection._is_run_blocked(self.run_map['third_act'])

    def test_is_run_blocked_required_actions_scheduled(self):
        self.run_map['action_name'].machine.state = ActionRun.STATE_SCHEDULED
        assert self.collection._is_run_blocked(self.run_map['second_name'])

    def test_is_run_blocked_required_actions_starting(self):
        self.run_map['action_name'].machine.state = ActionRun.STATE_STARTING
        assert self.collection._is_run_blocked(self.run_map['second_name'])

    def test_is_run_blocked_required_actions_queued(self):
        self.run_map['action_name'].machine.state = ActionRun.STATE_QUEUED
        assert self.collection._is_run_blocked(self.run_map['second_name'])

    def test_is_run_blocked_required_actions_failed(self):
        self.run_map['action_name'].machine.state = ActionRun.STATE_FAILED
        assert self.collection._is_run_blocked(self.run_map['second_name'])


if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = action_test
import mock
from testify import setup, run
from testify import TestCase, assert_equal
from tron import node

from tron.core import action

class TestAction(TestCase):

    @setup
    def setup_action(self):
        self.node_pool = mock.create_autospec(node.NodePool)
        self.action = action.Action("my_action", "doit", self.node_pool)

    def test_from_config(self):
        config = mock.Mock(
            name="ted",
            command="do something",
            node="first")
        new_action = action.Action.from_config(config)
        assert_equal(new_action.name, config.name)
        assert_equal(new_action.command, config.command)
        assert_equal(new_action.node_pool, None)
        assert_equal(new_action.required_actions, [])

    def test__eq__(self):
        new_action = action.Action(
            self.action.name, self.action.command, self.node_pool)
        assert_equal(new_action, self.action)


if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = jobrun_test
import datetime
import mock
import pytz
from testify import TestCase, setup, assert_equal
from testify.assertions import assert_in
from tests.assertions import assert_length, assert_raises, assert_call
from tron import node, event, actioncommand
from tron.core import jobrun, actionrun, actiongraph, job
from tests.testingutils import Turtle, autospec_method
from tron.serialize import filehandler


def build_mock_job():
    action_graph = mock.create_autospec(actiongraph.ActionGraph)
    runner = mock.create_autospec(actioncommand.SubprocessActionRunnerFactory)
    return mock.create_autospec(job.Job,
        action_graph=action_graph,
        output_path=mock.Mock(),
        context=mock.Mock(),
        action_runner=runner)


class JobRunTestCase(TestCase):

    now = datetime.datetime(2012, 3, 14, 15, 9, 20)

    @setup
    def setup_jobrun(self):
        self.job = build_mock_job()
        self.action_graph = self.job.action_graph
        self.run_time = datetime.datetime(2012, 3, 14, 15, 9 ,26)
        mock_node = mock.create_autospec(node.Node)
        self.job_run = jobrun.JobRun('jobname', 7, self.run_time, mock_node,
                action_runs=Turtle(
                    action_runs_with_cleanup=[],
                    get_startable_action_runs=lambda: []))
        autospec_method(self.job_run.watch)
        autospec_method(self.job_run.notify)
        self.job_run.event = mock.create_autospec(event.EventRecorder)
        self.action_run = mock.create_autospec(actionrun.ActionRun, is_skipped=False)

    def test__init__(self):
        assert_equal(self.job_run.job_name, 'jobname')
        assert_equal(self.job_run.run_time, self.run_time)
        assert str(self.job_run.output_path).endswith(self.job_run.id)

    def test_for_job(self):
        run_num = 6
        mock_node = mock.create_autospec(node.Node)
        run = jobrun.JobRun.for_job(
                self.job, run_num, self.run_time, mock_node, False)

        assert_equal(run.action_runs.action_graph, self.action_graph)
        assert_equal(run.job_name, self.job.get_name.return_value)
        assert_equal(run.run_num, run_num)
        assert_equal(run.node, mock_node)
        assert not run.manual

    def test_for_job_manual(self):
        run_num = 6
        mock_node = mock.create_autospec(node.Node)
        run = jobrun.JobRun.for_job(
                self.job, run_num, self.run_time, mock_node, True)
        assert_equal(run.action_runs.action_graph, self.action_graph)
        assert run.manual

    def test_state_data(self):
        state_data = self.job_run.state_data
        assert_equal(state_data['run_num'], 7)
        assert not state_data['manual']
        assert_equal(state_data['run_time'], self.run_time)

    def test_set_action_runs(self):
        self.job_run._action_runs = None
        count = 2
        action_runs = [
            mock.create_autospec(actionrun.ActionRun) for _ in xrange(count)]
        run_collection = mock.create_autospec(actionrun.ActionRunCollection,
            action_runs_with_cleanup=action_runs)
        self.job_run._set_action_runs(run_collection)
        assert_equal(self.job_run.watch.call_count, count)

        expected = [mock.call(run) for run in action_runs]
        assert_equal(self.job_run.watch.mock_calls, expected)
        assert_equal(self.job_run.action_runs, run_collection)
        assert self.job_run.action_runs_proxy

    def test_set_action_runs_none(self):
        self.job_run._action_runs = None
        run_collection = mock.create_autospec(actionrun.ActionRunCollection)
        self.job_run._set_action_runs(run_collection)
        assert not self.job_run.watch.mock_calls
        assert_equal(self.job_run.action_runs, run_collection)

    def test_set_action_runs_duplicate(self):
        run_collection = mock.create_autospec(actionrun.ActionRunCollection)
        assert_raises(ValueError,
            self.job_run._set_action_runs, run_collection)

    @mock.patch('tron.core.jobrun.timeutils.current_time', autospec=True)
    def test_seconds_until_run_time(self, mock_current_time):
        mock_current_time.return_value = self.now
        seconds = self.job_run.seconds_until_run_time()
        assert_equal(seconds, 6)

    @mock.patch('tron.core.jobrun.timeutils.current_time', autospec=True)
    def test_seconds_until_run_time_with_tz(self, mock_current_time):
        mock_current_time.return_value = self.now
        self.job_run.run_time = self.run_time.replace(tzinfo=pytz.utc)
        seconds = self.job_run.seconds_until_run_time()
        assert_equal(seconds, 6)

    def test_start(self):
        autospec_method(self.job_run._do_start)
        assert self.job_run.start()
        self.job_run.event.info.assert_called_with('start')
        self.job_run._do_start.assert_called_with()

    def test_start_failed(self):
        autospec_method(self.job_run._do_start, return_value=False)
        assert not self.job_run.start()
        self.job_run.event.info.assert_called_with('start')
        assert not self.job_run.event.ok.mock_calls

    def test_start_no_startable_action_runs(self):
        autospec_method(self.job_run._do_start)
        self.job_run.action_runs.has_startable_action_runs = False

        assert not self.job_run.start()
        self.job_run.event.info.assert_called_with('start')
        assert not self.job_run.event.ok.mock_calls

    def test_do_start(self):
        startable_runs = [
            mock.create_autospec(actionrun.ActionRun) for _ in xrange(3)]
        self.job_run.action_runs.get_startable_action_runs = lambda: startable_runs

        assert self.job_run._do_start()
        self.job_run.action_runs.ready.assert_called_with()
        for startable_run in startable_runs:
            startable_run.start.assert_called_with()

        assert_equal(self.job_run.event.ok.call_count, 1)
        self.job_run.event.ok.assert_called_with('started')

    def test_do_start_all_failed(self):
        autospec_method(self.job_run._start_action_runs, return_value=[None])
        assert not self.job_run._do_start()
        assert not self.job_run.event.ok.mock_calls

    def test_do_start_some_failed(self):
        returns = [True, None]
        autospec_method(self.job_run._start_action_runs, return_value=returns)

        assert self.job_run._do_start()
        assert_equal(self.job_run.event.ok.call_count, 1)
        self.job_run.event.ok.assert_called_with('started')

    def test_do_start_no_runs(self):
        assert not self.job_run._do_start()

    def test_start_action_runs(self):
        startable_runs = [
            mock.create_autospec(actionrun.ActionRun) for _ in xrange(3)]
        self.job_run.action_runs.get_startable_action_runs = lambda: startable_runs

        started_runs = self.job_run._start_action_runs()
        assert_equal(started_runs, startable_runs)

    def test_start_action_runs_failed(self):
        startable_runs = [
            mock.create_autospec(actionrun.ActionRun) for _ in xrange(3)]
        startable_runs[0].start.return_value = False
        self.job_run.action_runs.get_startable_action_runs = lambda: startable_runs

        started_runs = self.job_run._start_action_runs()
        assert_equal(started_runs, startable_runs[1:])

    def test_start_action_runs_all_failed(self):
        startable_runs = [
            mock.create_autospec(actionrun.ActionRun) for _ in xrange(2)]
        for startable_run in startable_runs:
            startable_run.start.return_value = False
        self.job_run.action_runs.get_startable_action_runs = lambda: startable_runs

        started_runs = self.job_run._start_action_runs()
        assert_equal(started_runs, [])

    def test_handler_not_end_state_event(self):
        autospec_method(self.job_run.finalize)
        autospec_method(self.job_run._start_action_runs)
        self.action_run.is_done = False
        self.job_run.handler(self.action_run, mock.Mock())
        assert not self.job_run.finalize.mock_calls
        assert not self.job_run._start_action_runs.mock_calls

    def test_handler_with_startable(self):
        startable_run = mock.create_autospec(actionrun.ActionRun)
        self.job_run.action_runs.get_startable_action_runs = lambda: [startable_run]
        autospec_method(self.job_run.finalize)
        self.action_run.is_broken = False

        self.job_run.handler(self.action_run, mock.Mock())
        self.job_run.notify.assert_called_with(self.job_run.NOTIFY_STATE_CHANGED)
        startable_run.start.assert_called_with()
        assert not self.job_run.finalize.mock_calls

    def test_handler_is_active(self):
        self.job_run.action_runs.is_active = True
        autospec_method(self.job_run._start_action_runs, return_value=[])
        autospec_method(self.job_run.finalize)
        self.job_run.handler(self.action_run, mock.Mock())
        assert not self.job_run.finalize.mock_calls

    def test_handler_finished_without_cleanup(self):
        self.job_run.action_runs.is_active = False
        self.job_run.action_runs.is_scheduled = False
        self.job_run.action_runs.cleanup_action_run = None
        autospec_method(self.job_run.finalize)
        self.job_run.handler(self.action_run, mock.Mock())
        self.job_run.finalize.assert_called_with()

    def test_handler_finished_with_cleanup_done(self):
        self.job_run.action_runs.is_active = False
        self.job_run.action_runs.is_scheduled = False
        self.job_run.action_runs.cleanup_action_run = mock.Mock(is_done=True)
        autospec_method(self.job_run.finalize)
        self.job_run.handler(self.action_run, mock.Mock())
        self.job_run.finalize.assert_called_with()

    def test_handler_finished_with_cleanup(self):
        self.job_run.action_runs.is_active = False
        self.job_run.action_runs.is_scheduled = False
        self.job_run.action_runs.cleanup_action_run = mock.Mock(is_done=False)
        autospec_method(self.job_run.finalize)
        self.job_run.handler(self.action_run, mock.Mock())
        assert not self.job_run.finalize.mock_calls
        self.job_run.action_runs.cleanup_action_run.start.assert_called_with()

    def test_handler_action_run_cancelled(self):
        self.action_run.is_broken = True
        autospec_method(self.job_run._start_action_runs)
        self.job_run.handler(self.action_run, mock.Mock())
        assert not self.job_run._start_action_runs.mock_calls

    def test_handler_action_run_skipped(self):
        self.action_run.is_broken = False
        self.action_run.is_skipped = True
        self.job_run.action_runs.is_scheduled = True
        autospec_method(self.job_run._start_action_runs)
        self.job_run.handler(self.action_run, mock.Mock())
        assert not self.job_run._start_action_runs.mock_calls

    def test_state(self):
        assert_equal(self.job_run.state, actionrun.ActionRun.STATE_SUCCEEDED)

    def test_state_with_no_action_runs(self):
        self.job_run._action_runs = None
        assert_equal(self.job_run.state, actionrun.ActionRun.STATE_UNKNOWN)

    def test_finalize(self):
        self.job_run.action_runs.is_failed = False
        self.job_run.finalize()
        self.job_run.event.ok.assert_called_with('succeeded')
        self.job_run.notify.assert_called_with(self.job_run.NOTIFY_DONE)

    def test_finalize_failure(self):
        self.job_run.finalize()
        self.job_run.event.critical.assert_called_with('failed')
        self.job_run.notify.assert_called_with(self.job_run.NOTIFY_DONE)

    def test_cleanup(self):
        autospec_method(self.job_run.clear_observers)
        self.job_run.output_path = mock.create_autospec(filehandler.OutputPath)
        self.job_run.cleanup()

        self.job_run.clear_observers.assert_called_with()
        self.job_run.output_path.delete.assert_called_with()
        assert not self.job_run.node
        assert not self.job_run.action_graph
        assert not self.job_run.action_runs

    def test__getattr__(self):
        assert self.job_run.cancel
        assert self.job_run.is_queued
        assert self.job_run.is_succeeded

    def test__getattr__miss(self):
        assert_raises(AttributeError, lambda: self.job_run.bogus)


class JobRunFromStateTestCase(TestCase):

    @setup
    def setup_jobrun(self):
        self.action_graph = mock.create_autospec(actiongraph.ActionGraph)
        self.run_time = datetime.datetime(2012, 3, 14, 15, 9 ,26)
        self.path = ['base', 'path']
        self.output_path = mock.create_autospec(filehandler.OutputPath)
        self.node_pool = mock.create_autospec(node.NodePool)
        self.action_run_state_data = [{
            'job_run_id':       'thejobname.22',
            'action_name':      'blingaction',
            'state':            'succeeded',
            'run_time':         'sometime',
            'start_time':       'sometime',
            'end_time':         'sometime',
            'command':          'doit',
            'node_name':        'thenode'
        }]
        self.state_data = {
            'job_name':         'thejobname',
            'run_num':          22,
            'run_time':         self.run_time,
            'node_name':        'thebox',
            'end_time':         'the_end',
            'start_time':       'start_time',
            'runs':             self.action_run_state_data,
            'cleanup_run':      None,
            'manual':           True
        }
        self.context = mock.Mock()

    def test_from_state(self):
        run = jobrun.JobRun.from_state(self.state_data, self.action_graph,
            self.output_path, self.context, self.node_pool)
        assert_length(run.action_runs.run_map, 1)
        assert_equal(run.job_name, self.state_data['job_name'])
        assert_equal(run.run_time, self.run_time)
        assert run.manual
        assert_equal(run.output_path, self.output_path)
        assert run.context.next
        assert run.action_graph

    def test_from_state_node_no_longer_exists(self):
        run = jobrun.JobRun.from_state(self.state_data, self.action_graph,
            self.output_path, self.context, self.node_pool)
        assert_length(run.action_runs.run_map, 1)
        assert_equal(run.job_name, 'thejobname')
        assert_equal(run.run_time, self.run_time)
        assert_equal(run.node, self.node_pool)


class MockJobRun(Turtle):

    manual = False

    node = 'anode'

    @property
    def is_scheduled(self):
        return self.state == actionrun.ActionRun.STATE_SCHEDULED

    @property
    def is_queued(self):
        return self.state == actionrun.ActionRun.STATE_QUEUED

    @property
    def is_running(self):
        return self.state == actionrun.ActionRun.STATE_RUNNING

    @property
    def is_starting(self):
        return self.state == actionrun.ActionRun.STATE_STARTING

    def __repr__(self):
        return str(self.__dict__)


class JobRunCollectionTestCase(TestCase):

    def _mock_run(self, **kwargs):
        return MockJobRun(**kwargs)

    @setup
    def setup_runs(self):
        self.run_collection = jobrun.JobRunCollection(5)
        self.job_runs = [
            self._mock_run(state=actionrun.ActionRun.STATE_QUEUED, run_num=4),
            self._mock_run(state=actionrun.ActionRun.STATE_RUNNING, run_num=3)
        ] + [
            self._mock_run(state=actionrun.ActionRun.STATE_SUCCEEDED, run_num=i)
            for i in xrange(2,0,-1)
        ]
        self.run_collection.runs.extend(self.job_runs)
        self.mock_node = mock.create_autospec(node.Node)

    def test__init__(self):
        assert_equal(self.run_collection.run_limit, 5)

    def test_from_config(self):
        job_config = mock.Mock(run_limit=20)
        runs = jobrun.JobRunCollection.from_config(job_config)
        assert_equal(runs.run_limit, 20)

    def test_restore_state(self):
        run_collection = jobrun.JobRunCollection(20)
        state_data = [
            dict(
                run_num=i,
                job_name="thename",
                run_time="sometime",
                start_time="start_time",
                end_time="sometime",
                cleanup_run=None,
                runs=[]
            ) for i in xrange(3,-1,-1)
        ]
        action_graph = mock.create_autospec(actiongraph.ActionGraph)
        output_path = mock.create_autospec(filehandler.OutputPath)
        context = mock.Mock()
        node_pool = mock.create_autospec(node.NodePool)

        restored_runs = run_collection.restore_state(
                state_data, action_graph, output_path, context, node_pool)
        assert_equal(run_collection.runs[0].run_num, 3)
        assert_equal(run_collection.runs[3].run_num, 0)
        assert_length(restored_runs, 4)

    def test_restore_state_with_runs(self):
        assert_raises(ValueError,
                self.run_collection.restore_state, None, None, None, None, None)

    def test_build_new_run(self):
        autospec_method(self.run_collection.remove_old_runs)
        run_time = datetime.datetime(2012, 3, 14, 15, 9, 26)
        mock_job = build_mock_job()
        job_run = self.run_collection.build_new_run(
            mock_job, run_time, self.mock_node)
        assert_in(job_run, self.run_collection.runs)
        self.run_collection.remove_old_runs.assert_called_with()
        assert_equal(job_run.run_num, 5)
        assert_equal(job_run.job_name, mock_job.get_name.return_value)

    def test_build_new_run_manual(self):
        autospec_method(self.run_collection.remove_old_runs)
        run_time = datetime.datetime(2012, 3, 14, 15, 9, 26)
        mock_job = build_mock_job()
        job_run = self.run_collection.build_new_run(
            mock_job, run_time, self.mock_node, True)
        assert_in(job_run, self.run_collection.runs)
        self.run_collection.remove_old_runs.assert_called_with()
        assert_equal(job_run.run_num, 5)
        assert job_run.manual

    def test_cancel_pending(self):
        pending_runs = [mock.Mock() for _ in xrange(2)]
        autospec_method(self.run_collection.get_pending,
            return_value=pending_runs)
        self.run_collection.cancel_pending()
        for pending_run in pending_runs:
            pending_run.cancel.assert_called_with()

    def test_cancel_pending_no_pending(self):
        autospec_method(self.run_collection.get_pending, return_value=[])
        self.run_collection.cancel_pending()

    def test_remove_pending(self):
        self.run_collection.remove_pending()
        assert_length(self.run_collection.runs, 3)
        assert_equal(self.run_collection.runs[0], self.job_runs[1])
        assert_call(self.job_runs[0].cleanup, 0)

    def test_get_run_by_state(self):
        state = actionrun.ActionRun.STATE_SUCCEEDED
        run = self.run_collection.get_run_by_state(state)
        assert_equal(run, self.job_runs[2])

    def test_get_run_by_state_no_match(self):
        state = actionrun.ActionRun.STATE_UNKNOWN
        run = self.run_collection.get_run_by_state(state)
        assert_equal(run, None)

    def test_get_run_by_num(self):
        run = self.run_collection.get_run_by_num(1)
        assert_equal(run.run_num, 1)

    def test_get_run_by_num_no_match(self):
        run = self.run_collection.get_run_by_num(7)
        assert_equal(run, None)

    def test_get_run_by_index(self):
        run = self.run_collection.get_run_by_index(-1)
        assert_equal(run, self.job_runs[0])
        run = self.run_collection.get_run_by_index(-2)
        assert_equal(run, self.job_runs[1])
        run = self.run_collection.get_run_by_index(0)
        assert_equal(run, self.job_runs[-1])
        run = self.run_collection.get_run_by_index(1)
        assert_equal(run, self.job_runs[-2])

    def test_get_run_by_index_invalid_index(self):
        run = self.run_collection.get_run_by_index(-5)
        assert_equal(run, None)
        run = self.run_collection.get_run_by_index(4)
        assert_equal(run, None)

    def test_get_run_by_state_short_name(self):
        run = self.run_collection.get_run_by_state_short_name("RUNN")
        assert_equal(run, self.job_runs[1])

    def test_get_run_by_state_short_name_no_match(self):
        run = self.run_collection.get_run_by_state_short_name("FAIL")
        assert_equal(run, None)

    def test_get_newest(self):
        run = self.run_collection.get_newest()
        assert_equal(run, self.job_runs[0])

    def test_get_newest_exclude_manual(self):
        run = self._mock_run(
                state=actionrun.ActionRun.STATE_RUNNING, run_num=5, manual=True)
        self.job_runs.insert(0, run)
        newest_run = self.run_collection.get_newest(include_manual=False)
        assert_equal(newest_run, self.job_runs[1])

    def test_get_newest_no_runs(self):
        run_collection = jobrun.JobRunCollection(5)
        assert_equal(run_collection.get_newest(), None)

    def test_pending(self):
        run_num = self.run_collection.next_run_num()
        scheduled_run = self._mock_run(
                run_num=run_num,
                state=actionrun.ActionRun.STATE_SCHEDULED)
        self.run_collection.runs.appendleft(scheduled_run)
        pending = list(self.run_collection.get_pending())
        assert_length(pending, 2)
        assert_equal(pending, [scheduled_run, self.job_runs[0]])

    def test_get_active(self):
        starting_run = self._mock_run(
            run_num=self.run_collection.next_run_num(),
            state=actionrun.ActionRun.STATE_STARTING)
        self.run_collection.runs.appendleft(starting_run)
        active = list(self.run_collection.get_active())
        assert_length(active, 2)
        assert_equal(active, [starting_run, self.job_runs[1]])

    def test_get_active_with_node(self):
        starting_run = self._mock_run(
            run_num=self.run_collection.next_run_num(),
            state=actionrun.ActionRun.STATE_STARTING)
        starting_run.node = 'differentnode'
        self.run_collection.runs.appendleft(starting_run)
        active = list(self.run_collection.get_active('anode'))
        assert_length(active, 1)
        assert_equal(active, [self.job_runs[1]])

    def test_get_active_none(self):
        active = list(self.run_collection.get_active('bogus'))
        assert_length(active, 0)

    def test_get_first_queued(self):
        run_num = self.run_collection.next_run_num()
        second_queued = self._mock_run(
            run_num=run_num, state=actionrun.ActionRun.STATE_QUEUED)
        self.run_collection.runs.appendleft(second_queued)

        first_queued = self.run_collection.get_first_queued()
        assert_equal(first_queued, self.job_runs[0])

    def test_get_first_queued_no_match(self):
        self.job_runs[0].state = actionrun.ActionRun.STATE_CANCELLED
        first_queued = self.run_collection.get_first_queued()
        assert not first_queued

    def test_get_next_to_finish(self):
        next_run = self.run_collection.get_next_to_finish()
        assert_equal(next_run, self.job_runs[1])

    def test_get_next_to_finish_by_node(self):
        self.job_runs[1].node = "seven"
        scheduled_run = self._mock_run(
            run_num=self.run_collection.next_run_num(),
            state=actionrun.ActionRun.STATE_SCHEDULED,
            node="nine")
        self.run_collection.runs.appendleft(scheduled_run)

        next_run = self.run_collection.get_next_to_finish(node="seven")
        assert_equal(next_run, self.job_runs[1])

    def test_get_next_to_finish_none(self):
        next_run = self.run_collection.get_next_to_finish(node="seven")
        assert_equal(next_run, None)

        self.job_runs[1].state = None
        next_run = self.run_collection.get_next_to_finish()
        assert_equal(next_run, None)

    def test_get_next_run_num(self):
        assert_equal(self.run_collection.next_run_num(), 5)

    def test_get_next_run_num_first(self):
        run_collection = jobrun.JobRunCollection(5)
        assert_equal(run_collection.next_run_num(), 0)

    def test_remove_old_runs(self):
        self.run_collection.run_limit = 1
        self.run_collection.remove_old_runs()

        assert_length(self.run_collection.runs, 1)
        assert_call(self.job_runs[-1].cleanup, 0)
        for job_run in self.run_collection.runs:
            assert_length(job_run.cancel.calls, 0)

    def test_remove_old_runs_none(self):
        self.run_collection.remove_old_runs()
        for job_run in self.job_runs:
            assert_length(job_run.cancel.calls, 0)

    def test_remove_old_runs_no_runs(self):
        run_collection = jobrun.JobRunCollection(4)
        run_collection.remove_old_runs()

    def test_state_data(self):
        assert_length(self.run_collection.state_data, len(self.job_runs))

    def test_last_success(self):
        assert_equal(self.run_collection.last_success, self.job_runs[2])

    def test__str__(self):
        expected = "JobRunCollection[4(queued), 3(running), 2(succeeded), 1(succeeded)]"
        assert_equal(str(self.run_collection), expected)

    def test_get_action_runs(self):
        action_name = 'action_name'
        self.run_collection.runs = job_runs = [mock.Mock(), mock.Mock()]
        runs = self.run_collection.get_action_runs(action_name)
        expected = [job_run.get_action_run.return_value for job_run in job_runs]
        assert_equal(runs, expected)
        for job_run in job_runs:
            job_run.get_action_run.assert_called_with(action_name)

########NEW FILE########
__FILENAME__ = job_test
import datetime
import mock

from testify import setup, teardown, TestCase, run, assert_equal
from testify import setup_teardown
from testify.assertions import assert_not_equal
from tests import mocks
from tests.assertions import assert_length, assert_call, assert_mock_calls
from tests.testingutils import Turtle, autospec_method
from tests import testingutils
from tron import node, event, actioncommand
from tron.core import job, jobrun
from tron.core.actionrun import ActionRun


class JobTestCase(TestCase):

    @setup_teardown
    def setup_job(self):
        action_graph = mock.Mock(names=lambda: ['one', 'two'])
        scheduler = mock.Mock()
        run_collection = Turtle()
        self.nodes = mock.create_autospec(node.NodePool)
        self.action_runner = mock.create_autospec(
            actioncommand.SubprocessActionRunnerFactory)

        patcher = mock.patch('tron.core.job.node.NodePoolRepository')
        with patcher as self.mock_node_repo:
            self.job = job.Job("jobname", scheduler,
                    run_collection=run_collection, action_graph=action_graph,
                    node_pool=self.nodes)
            autospec_method(self.job.notify)
            autospec_method(self.job.watch)
            self.job.event = mock.create_autospec(event.EventRecorder)
            yield

    def test__init__(self):
        assert str(self.job.output_path).endswith(self.job.name)

    @mock.patch('tron.core.job.event', autospec=True)
    def test_from_config(self, _mock_event):
        action = mock.Mock(name='first', command='doit', node=None, requires=[])
        job_config = mock.Mock(
            name='ajob',
            node='thenodepool',
            all_nodes=False,
            queueing=True,
            enabled=True,
            run_limit=20,
            actions={action.name: action},
            cleanup_action=None)
        scheduler = 'scheduler_token'
        parent_context = 'parent_context_token'
        output_path = ["base_path"]
        new_job = job.Job.from_config(
            job_config, scheduler, parent_context, output_path, self.action_runner)

        assert_equal(new_job.scheduler, scheduler)
        assert_equal(new_job.context.next, parent_context)
        self.mock_node_repo.get_instance().get_by_name.assert_called_with(
            job_config.node)
        assert_equal(new_job.enabled, True)
        assert new_job.action_graph

    def test_update_from_job(self):
        action_runner = mock.Mock()
        other_job = job.Job('otherjob', 'scheduler', action_runner=action_runner)
        self.job.update_from_job(other_job)
        assert_equal(self.job.name, 'otherjob')
        assert_equal(self.job.scheduler, 'scheduler')
        assert_equal(self.job, other_job)
        self.job.event.ok.assert_called_with('reconfigured')

    def test_status_disabled(self):
        self.job.enabled = False
        assert_equal(self.job.status, self.job.STATUS_DISABLED)

    def test_status_enabled(self):
        def state_in(state):
            return state in [ActionRun.STATE_SCHEDULED, ActionRun.STATE_QUEUED]

        self.job.runs.get_run_by_state = state_in
        assert_equal(self.job.status, self.job.STATUS_ENABLED)

    def test_status_running(self):
        self.job.runs.get_run_by_state = lambda s: Turtle()
        assert_equal(self.job.status, self.job.STATUS_RUNNING)

    def test_status_unknown(self):
        self.job.runs.get_run_by_state = lambda s: None
        assert_equal(self.job.status, self.job.STATUS_UNKNOWN)

    def test_state_data(self):
        state_data = self.job.state_data
        assert_equal(state_data['runs'], self.job.runs.state_data)
        assert state_data['enabled']

    def test_restore_state(self):
        run_data = ['one', 'two']
        job_runs = [Turtle(), Turtle()]
        self.job.runs.restore_state = lambda r, a, o, c, n: job_runs
        state_data = {'enabled': False, 'runs': run_data}

        self.job.restore_state(state_data)

        assert not self.job.enabled
        calls = [mock.call(job_runs[i]) for i in xrange(len(job_runs))]
        self.job.watch.assert_has_calls(calls)
        self.job.event.ok.assert_called_with('restored')

    def test_build_new_runs(self):
        run_time = datetime.datetime(2012, 3, 14, 15, 9, 26)
        runs = list(self.job.build_new_runs(run_time))

        self.job.node_pool.next.assert_called_with()
        node = self.job.node_pool.next.return_value
        assert_call(self.job.runs.build_new_run,
                0, self.job, run_time, node, manual=False)
        assert_length(runs, 1)
        self.job.watch.assert_called_with(runs[0])

    def test_build_new_runs_all_nodes(self):
        self.job.all_nodes = True
        run_time = datetime.datetime(2012, 3, 14, 15, 9, 26)
        node_count = 2
        self.job.node_pool.nodes = [mock.Mock()] * node_count
        runs = list(self.job.build_new_runs(run_time))

        assert_length(runs, node_count)
        for i in xrange(len(runs)):
            node = self.job.node_pool.nodes[i]
            assert_call(self.job.runs.build_new_run,
                    i, self.job, run_time, node, manual=False)

        self.job.watch.assert_has_calls([mock.call(run) for run in runs])

    def test_build_new_runs_manual(self):
        run_time = datetime.datetime(2012, 3, 14, 15, 9, 26)
        runs = list(self.job.build_new_runs(run_time, manual=True))

        self.job.node_pool.next.assert_called_with()
        node = self.job.node_pool.next.return_value
        assert_length(runs, 1)
        assert_call(self.job.runs.build_new_run,
                0, self.job, run_time, node, manual=True)
        self.job.watch.assert_called_with(runs[0])

    def test_handler(self):
        self.job.handler(None, jobrun.JobRun.NOTIFY_STATE_CHANGED)
        self.job.notify.assert_called_with(self.job.NOTIFY_STATE_CHANGE)

        self.job.handler(None, jobrun.JobRun.NOTIFY_DONE)
        self.job.notify.assert_called_with(self.job.NOTIFY_RUN_DONE)

    def test__eq__(self):
        other_job = job.Job("jobname", 'scheduler')
        assert not self.job == other_job
        other_job.update_from_job(self.job)
        assert_equal(self.job, other_job)

    def test__ne__(self):
        other_job = job.Job("jobname", 'scheduler')
        assert self.job != other_job
        other_job.update_from_job(self.job)
        assert not self.job != other_job

    def test__eq__true(self):
        action_runner = mock.Mock()
        first = job.Job("jobname", 'scheduler', action_runner=action_runner)
        second = job.Job("jobname", 'scheduler', action_runner=action_runner)
        assert_equal(first, second)

    def test__eq__false(self):
        first = job.Job("jobname", 'scheduler', action_runner=mock.Mock())
        second = job.Job("jobname", 'scheduler', action_runner=mock.Mock())
        assert_not_equal(first, second)


class JobSchedulerTestCase(TestCase):

    @setup
    def setup_job(self):
        self.scheduler = Turtle()
        run_collection = Turtle()
        node_pool = Turtle()
        self.job = job.Job(
                "jobname",
                self.scheduler,
                run_collection=run_collection,
                node_pool=node_pool,
        )
        self.job_scheduler = job.JobScheduler(self.job)

    def test_restore_job_state(self):
        run_collection = mocks.MockJobRunCollection(get_scheduled=lambda: ['a'])
        self.job_scheduler.job = Turtle(runs=run_collection)
        self.job_scheduler._set_callback = Turtle()
        state_data = 'state_data_token'
        self.job_scheduler.restore_state(state_data)
        assert_call(self.job_scheduler.job.restore_state, 0, state_data)
        assert_length(self.job_scheduler._set_callback.calls, 1)
        assert_call(self.job_scheduler._set_callback, 0, 'a')

    def test_disable(self):
        self.job_scheduler.disable()
        assert not self.job.enabled
        assert_length(self.job.runs.cancel_pending.calls, 1)

    def test_schedule_reconfigured(self):
        autospec_method(self.job_scheduler.create_and_schedule_runs)
        self.job_scheduler.schedule_reconfigured()
        assert_length(self.job.runs.remove_pending.calls, 1)
        self.job_scheduler.create_and_schedule_runs.assert_called_with(
            ignore_last_run_time=True)

    def test_run_job(self):
        self.job_scheduler.schedule = Turtle()
        self.scheduler.schedule_on_complete = False
        self.job.runs.get_active = lambda n: []
        job_run = Turtle(is_cancelled=False)
        self.job_scheduler.run_job(job_run)
        assert_length(job_run.start.calls, 1)
        assert_length(self.job_scheduler.schedule.calls, 1)

    def test_run_job_shutdown_requested(self):
        self.job_scheduler.shutdown_requested = True
        self.job_scheduler.schedule = Turtle()
        job_run = Turtle()
        self.job_scheduler.run_job(job_run)
        assert_length(self.job_scheduler.schedule.calls, 0)
        assert_length(job_run.start.calls, 0)
        assert_length(job_run.cancel.calls, 0)

    def test_run_job_job_disabled(self):
        self.job_scheduler.schedule = Turtle()
        job_run = Turtle()
        self.job.enabled = False
        self.job_scheduler.run_job(job_run)
        assert_length(self.job_scheduler.schedule.calls, 0)
        assert_length(job_run.start.calls, 0)
        assert_length(job_run.cancel.calls, 1)

    def test_run_job_cancelled(self):
        self.job_scheduler.schedule = Turtle()
        job_run = Turtle(is_scheduled=False)
        self.job_scheduler.run_job(job_run)
        assert_length(job_run.start.calls, 0)
        assert_length(self.job_scheduler.schedule.calls, 1)

    def test_run_job_already_running_queuing(self):
        self.job_scheduler.schedule = Turtle()
        self.job.runs.get_active = lambda s: [Turtle()]
        job_run = Turtle(is_cancelled=False)
        self.job_scheduler.run_job(job_run)
        assert_length(job_run.start.calls, 0)
        assert_length(job_run.queue.calls, 1)
        assert_length(self.job_scheduler.schedule.calls, 0)

    def test_run_job_already_running_cancel(self):
        self.job_scheduler.schedule = Turtle()
        self.job.runs.get_active = lambda s: [Turtle()]
        self.job.queueing = False
        job_run = Turtle(is_cancelled=False)
        self.job_scheduler.run_job(job_run)
        assert_length(job_run.start.calls, 0)
        assert_length(job_run.cancel.calls, 1)
        assert_length(self.job_scheduler.schedule.calls, 1)

    def test_run_job_already_running_allow_overlap(self):
        self.job_scheduler.schedule = mock.Mock()
        self.job.runs.get_active = lambda s: [mock.Mock()]
        self.job.allow_overlap = True
        job_run = Turtle(is_cancelled=False)
        self.job_scheduler.run_job(job_run)
        job_run.start.assert_called_with()

    def test_run_job_has_starting_queueing(self):
        self.job_scheduler.schedule = Turtle()
        self.job.runs.get_active = lambda s: [Turtle()]
        job_run = Turtle(is_cancelled=False)
        self.job_scheduler.run_job(job_run)
        assert_length(job_run.start.calls, 0)
        assert_length(job_run.queue.calls, 1)
        assert_length(self.job_scheduler.schedule.calls, 0)

    def test_run_job_schedule_on_complete(self):
        self.job_scheduler.schedule = Turtle()
        self.scheduler.schedule_on_complete = True
        self.job.runs.get_active = lambda s: []
        job_run = Turtle(is_cancelled=False)
        self.job_scheduler.run_job(job_run)
        assert_length(job_run.start.calls, 1)
        assert_length(self.job_scheduler.schedule.calls, 0)

class JobSchedulerGetRunsToScheduleTestCase(TestCase):

    @setup
    def setup_job(self):
        self.scheduler = mock.Mock()
        run_collection = mock.Mock(has_pending=False)
        node_pool = mock.Mock()
        self.job = job.Job(
            "jobname",
            self.scheduler,
            run_collection=run_collection,
            node_pool=node_pool,
        )
        self.job_scheduler = job.JobScheduler(self.job)
        self.job.runs.get_pending.return_value = False
        self.scheduler.queue_overlapping = True

    def test_get_runs_to_schedule_no_queue_with_pending(self):
        self.scheduler.queue_overlapping = False
        self.job.runs.has_pending = True
        job_runs = self.job_scheduler.get_runs_to_schedule(False)
        assert_length(job_runs, 0)

    def test_get_runs_to_schedule_queue_with_pending(self):
        job_runs = list(self.job_scheduler.get_runs_to_schedule(False))

        self.job.runs.get_newest.assert_called_with(include_manual=False)
        self.job.scheduler.next_run_time.assert_called_once_with(
                self.job.runs.get_newest.return_value.run_time)
        assert_length(job_runs, 1)
        # This should return a JobRun which has the job attached as an observer
        job_runs[0].attach.assert_any_call(True, self.job)

    def test_get_runs_to_schedule_no_pending(self):
        job_runs = list(self.job_scheduler.get_runs_to_schedule(False))

        self.job.runs.get_newest.assert_called_with(include_manual=False)
        self.job.scheduler.next_run_time.assert_called_once_with(
            self.job.runs.get_newest.return_value.run_time)
        assert_length(job_runs, 1)
        # This should return a JobRun which has the job attached as an observer
        job_runs[0].attach.assert_any_call(True, self.job)

    def test_get_runs_to_schedule_no_last_run(self):
        self.job.runs.get_newest.return_value = None

        job_runs = list(self.job_scheduler.get_runs_to_schedule(False))
        self.job.scheduler.next_run_time.assert_called_once_with(None)
        assert_length(job_runs, 1)
        # This should return a JobRun which has the job attached as an observer
        job_runs[0].attach.assert_any_call(True, self.job)

    def test_get_runs_to_schedule_ignore_last(self):
        job_runs = list(self.job_scheduler.get_runs_to_schedule(True))
        self.job.scheduler.next_run_time.assert_called_once_with(None)
        assert_length(job_runs, 1)
        self.scheduler.next_run_time.assert_called_once_with(None)


class JobSchedulerManualStartTestCase(testingutils.MockTimeTestCase):

    now = datetime.datetime.now()

    @setup
    def setup_job(self):
        self.scheduler = mock.Mock()
        run_collection = mock.Mock()
        node_pool = mock.Mock()
        self.job = job.Job(
            "jobname",
            self.scheduler,
            run_collection=run_collection,
            node_pool=node_pool,
        )
        self.job_scheduler = job.JobScheduler(self.job)
        self.manual_run = mock.Mock()
        self.job.build_new_runs = mock.Mock(return_value=[self.manual_run])

    def test_manual_start(self):
        manual_runs = self.job_scheduler.manual_start()

        self.job.build_new_runs.assert_called_with(self.now, manual=True)
        assert_length(manual_runs, 1)
        self.manual_run.start.assert_called_once_with()

    def test_manual_start_with_run_time(self):
        run_time = datetime.datetime(2012, 3, 14, 15, 9, 26)
        manual_runs = self.job_scheduler.manual_start(run_time)

        self.job.build_new_runs.assert_called_with(run_time, manual=True)
        assert_length(manual_runs, 1)
        self.manual_run.start.assert_called_once_with()


class JobSchedulerScheduleTestCase(TestCase):

    @setup
    def setup_job(self):
        self.scheduler = mock.Mock()
        run_collection = mock.Mock(has_pending=False)
        node_pool = mock.Mock()
        self.job = job.Job(
            "jobname",
            self.scheduler,
            run_collection=run_collection,
            node_pool=node_pool,
        )
        self.job_scheduler = job.JobScheduler(self.job)

    @setup_teardown
    def mock_eventloop(self):
        patcher = mock.patch('tron.core.job.eventloop', autospec=True)
        with patcher as self.eventloop:
            yield

    @teardown
    def teardown_job(self):
        event.EventManager.reset()

    def test_enable(self):
        self.job.enabled = False
        self.job_scheduler.enable()
        assert self.job.enabled
        assert_length(self.eventloop.call_later.mock_calls, 1)

    def test_enable_noop(self):
        self.job.enalbed = True
        self.job_scheduler.enable()
        assert self.job.enabled
        assert_length(self.eventloop.call_later.mock_calls, 0)

    def test_schedule(self):
        self.job_scheduler.schedule()
        assert_length(self.eventloop.call_later.mock_calls, 1)

        # Args passed to callLater
        call_args = self.eventloop.call_later.mock_calls[0][1]
        assert_equal(call_args[1], self.job_scheduler.run_job)
        secs = call_args[0]
        run = call_args[2]

        run.seconds_until_run_time.assert_called_with()
        # Assert that we use the seconds we get from the run to schedule
        assert_equal(run.seconds_until_run_time.return_value, secs)

    def test_schedule_disabled_job(self):
        self.job.enabled = False
        self.job_scheduler.schedule()
        assert_length(self.eventloop.call_later.mock_calls, 0)

    def test_handle_job_events_no_schedule_on_complete(self):
        self.job_scheduler.run_job = mock.Mock()
        self.job.scheduler.schedule_on_complete = False
        queued_job_run = mock.Mock()
        self.job.runs.get_first_queued = lambda: queued_job_run
        self.job_scheduler.handle_job_events(self.job, job.Job.NOTIFY_RUN_DONE)
        self.eventloop.call_later.assert_any_call(0,
            self.job_scheduler.run_job, queued_job_run, run_queued=True)

    def test_handle_job_events_schedule_on_complete(self):
        self.job_scheduler.schedule = mock.Mock()
        self.job.scheduler.schedule_on_complete = True
        self.job_scheduler.handle_job_events(self.job, job.Job.NOTIFY_RUN_DONE)
        self.job_scheduler.schedule.assert_called_with()

    def test_handler_unknown_event(self):
        self.job.runs.get_runs_by_state = mock.Mock()
        self.job_scheduler.handler(self.job, 'some_other_event')
        self.job.runs.get_runs_by_state.assert_not_called()

    def test_handler_no_queued(self):
        self.job_scheduler.run_job = mock.Mock()
        def get_queued(state):
            if state == ActionRun.STATE_QUEUED:
                return []
        self.job.runs.get_runs_by_state = get_queued
        self.job_scheduler.handler(self.job, job.Job.NOTIFY_RUN_DONE)
        self.job_scheduler.run_job.assert_not_called()

    def test_run_queue_schedule(self):
        with mock.patch.object(self.job_scheduler, 'schedule') as mock_schedule:
            self.job_scheduler.run_job = mock.Mock()
            self.job.scheduler.schedule_on_complete = False
            queued_job_run = mock.Mock()
            self.job.runs.get_first_queued = lambda: queued_job_run
            self.job_scheduler.run_queue_schedule()
            self.eventloop.call_later.assert_called_once_with(0,
                self.job_scheduler.run_job, queued_job_run, run_queued=True) 
            mock_schedule.assert_called_once_with()


class JobSchedulerFactoryTestCase(TestCase):

    @setup
    def setup_factory(self):
        self.context = mock.Mock()
        self.output_stream_dir = mock.Mock()
        self.time_zone = mock.Mock()
        self.action_runner = mock.create_autospec(
            actioncommand.SubprocessActionRunnerFactory)
        self.factory = job.JobSchedulerFactory(
            self.context, self.output_stream_dir, self.time_zone, self.action_runner)

    def test_build(self):
        config = mock.Mock()
        with mock.patch('tron.core.job.Job', autospec=True) as mock_job:
            job_scheduler = self.factory.build(config)
            args, _ = mock_job.from_config.call_args
            job_config, scheduler, context, output_path, action_runner = args
            assert_equal(job_config, config)
            assert_equal(job_scheduler.get_job(), mock_job.from_config.return_value)
            assert_equal(context, self.context)
            assert_equal(output_path.base, self.output_stream_dir)
            assert_equal(action_runner, self.action_runner)


class JobCollectionTestCase(TestCase):

    @setup
    def setup_collection(self):
        self.collection = job.JobCollection()

    def test_load_from_config(self):
        autospec_method(self.collection.jobs.filter_by_name)
        autospec_method(self.collection.add)
        factory = mock.create_autospec(job.JobSchedulerFactory)
        job_configs = {'a': mock.Mock(), 'b': mock.Mock()}
        result = self.collection.load_from_config(job_configs, factory, True)
        result = list(result)
        self.collection.jobs.filter_by_name.assert_called_with(job_configs)
        expected_calls = [mock.call(v) for v in job_configs.itervalues()]
        assert_mock_calls(expected_calls, factory.build.mock_calls)
        assert_length(self.collection.add.mock_calls, len(job_configs) * 2)
        assert_length(result, len(job_configs))
        job_schedulers = [call[1][0] for call in self.collection.add.mock_calls[::2]]
        for job_scheduler in job_schedulers:
            job_scheduler.schedule.assert_called_with()
            job_scheduler.get_job.assert_called_with()

    def test_update(self):
        mock_scheduler = mock.create_autospec(job.JobScheduler)
        existing_scheduler = mock.create_autospec(job.JobScheduler)
        autospec_method(self.collection.get_by_name, return_value=existing_scheduler)
        assert self.collection.update(mock_scheduler)
        self.collection.get_by_name.assert_called_with(mock_scheduler.get_name())
        existing_scheduler.get_job().update_from_job.assert_called_with(
            mock_scheduler.get_job.return_value)
        existing_scheduler.schedule_reconfigured.assert_called_with()


if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = serviceinstance_test
import mock
from testify import setup, assert_equal, TestCase, run, setup_teardown
from testify.assertions import assert_in, assert_not_equal
from tests.assertions import assert_length
from tests.testingutils import autospec_method

from tron import node, eventloop, command_context, actioncommand
from tron.actioncommand import ActionCommand
from tron.core import serviceinstance
from tron.utils import state


class BuildActionTestCase(TestCase):

    @setup
    def setup_task(self):
        self.command = 'command'
        self.id = 'the_id'
        self.name = 'the_name'
        self.serializer = mock.create_autospec(actioncommand.StringBufferStore)
        self.task = mock.Mock(command=self.command,
            id=self.id, task_name=self.name, buffer_store=self.serializer)

    @setup_teardown
    def setup_mock(self):
        patcher = mock.patch('tron.core.serviceinstance.ActionCommand', autospec=True)
        with patcher as self.mock_action_command:
            yield

    def test_build_action(self):
        action = serviceinstance.build_action(self.task)
        self.mock_action_command.assert_called_with(
            '%s.%s' % (self.id, self.name), self.command, serializer=self.serializer)
        assert_equal(action, self.mock_action_command.return_value)
        self.task.watch.assert_called_with(action)


class RunActionTestCase(TestCase):

    @setup
    def setup_task(self):
        self.node = mock.create_autospec(node.Node)
        self.failed = 'NOTIFY_FAILED'
        self.task = mock.Mock(node=self.node, NOTIFY_FAILED=self.failed,
            task_name='mock_task')
        self.action = mock.create_autospec(actioncommand.ActionCommand)

    def test_run_action(self):
        assert serviceinstance.run_action(self.task, self.action)
        self.node.submit_command.assert_called_with(self.action)

    def test_run_action_failed(self):
        error = self.task.node.submit_command.side_effect = node.Error("Oops")
        assert not serviceinstance.run_action(self.task, self.action)
        self.task.notify.assert_called_with(self.failed)
        self.task.buffer_store.open.return_value.write.assert_called_with(
            "Node run failure for mock_task: %s" % str(error))


class ServiceInstanceMonitorTaskTestCase(TestCase):

    @setup_teardown
    def setup_task(self):
        self.interval = 20
        self.filename = "/tmp/filename"
        mock_node = mock.create_autospec(node.Node)
        self.task = serviceinstance.ServiceInstanceMonitorTask(
            "id", mock_node, self.interval, self.filename)
        autospec_method(self.task.notify)
        autospec_method(self.task.watch)
        self.task.hang_check_callback = mock.create_autospec(eventloop.UniqueCallback)
        self.task.callback = mock.create_autospec(eventloop.UniqueCallback)
        self.mock_eventloop = None
        with mock.patch('tron.core.serviceinstance.eventloop') as self.mock_eventloop:
            yield

    def test_queue(self):
        self.task.queue()
        self.task.callback.start.assert_called_with()

    def test_queue_no_interval(self):
        self.task.interval = 0
        self.task.queue()
        assert_equal(self.mock_eventloop.call_later.call_count, 0)

    def test_run(self):
        self.task.run()

        self.task.notify.assert_called_with(self.task.NOTIFY_START)
        self.task.node.submit_command.assert_called_with(self.task.action)
        self.task.hang_check_callback.start.assert_called_with()
        assert_equal(self.task.action.command, self.task.command)

    def test_run_failed(self):
        with mock.patch('tron.core.serviceinstance.run_action') as mock_run:
            mock_run.return_value = False
            self.task.run()
            assert_equal(self.mock_eventloop.call_later.call_count, 0)

    def test_run_action_exists(self):
        self.task.action = mock.create_autospec(ActionCommand, is_done=False)
        with mock.patch('tron.core.serviceinstance.log', autospec=True) as mock_log:
            self.task.run()
            assert_equal(mock_log.warn.call_count, 1)

    def test_handle_action_event_failstart(self):
        autospec_method(self.task.queue)
        self.task.handle_action_event(self.task.action, ActionCommand.FAILSTART)
        self.task.notify.assert_called_with(self.task.NOTIFY_FAILED)
        self.task.queue.assert_called_with()
        self.task.hang_check_callback.cancel.assert_called_with()

    def test_handle_action_event_exit(self):
        autospec_method(self.task._handle_action_exit)
        self.task.handle_action_event(self.task.action, ActionCommand.EXITING)
        self.task._handle_action_exit.assert_called_with()
        self.task.hang_check_callback.cancel.assert_called_with()

    def test_handle_action_running(self):
        autospec_method(self.task.queue)
        self.task.handle_action_event(self.task.action, ActionCommand.RUNNING)
        assert not self.task.hang_check_callback.cancel.mock_calls
        assert not self.task.queue.mock_calls

    def test_handle_action_mismatching_action(self):
        autospec_method(self.task._handle_action_exit)
        action = mock.create_autospec(actioncommand.ActionCommand)
        self.task.handle_action_event(action, ActionCommand.EXITING)
        assert not self.task._handle_action_exit.mock_calls

    def test_handle_action_exit_up(self):
        self.task.action = mock.create_autospec(ActionCommand)
        self.task.action.is_failed = False
        autospec_method(self.task.queue)
        self.task._handle_action_exit()
        self.task.notify.assert_called_with(self.task.NOTIFY_UP)
        self.task.queue.assert_called_with()

    def test_handle_action_exit_down(self):
        self.task.action = mock.create_autospec(ActionCommand)
        autospec_method(self.task.queue)
        self.task._handle_action_exit()
        self.task.notify.assert_called_with(self.task.NOTIFY_FAILED)
        assert_equal(self.task.queue.call_count, 0)

    def test_fail(self):
        self.task.action = mock.create_autospec(actioncommand.ActionCommand)
        original_action = self.task.action
        self.task.fail()
        self.task.notify.assert_called_with(self.task.NOTIFY_FAILED)
        self.task.node.stop.assert_called_with(original_action)
        assert_equal(self.task.action, actioncommand.CompletedActionCommand)
        assert_equal(original_action.write_stderr.call_count, 1)


class ServiceInstanceStopTaskTestCase(TestCase):

    @setup
    def setup_task(self):
        self.node = mock.create_autospec(node.Node)
        self.pid_filename = '/tmp/filename'
        self.task = serviceinstance.ServiceInstanceStopTask(
            'id', self.node, self.pid_filename)
        autospec_method(self.task.watch)
        autospec_method(self.task.notify)

    def test_kill_task(self):
        assert self.task.stop()

    def test_handle_action_event_complete(self):
        action = mock.create_autospec(ActionCommand)
        event = ActionCommand.COMPLETE
        self.task.handle_action_event(action, event)
        self.task.notify.assert_called_with(self.task.NOTIFY_SUCCESS)

    def test_handle_action_event_failstart(self):
        action = mock.create_autospec(ActionCommand)
        event = ActionCommand.FAILSTART
        self.task.handle_action_event(action, event)
        self.task.notify.assert_called_with(self.task.NOTIFY_FAILED)

    def test_handle_complete_failed(self):
        action = mock.create_autospec(ActionCommand, is_failed=True)
        with mock.patch('tron.core.serviceinstance.log', autospec=True) as mock_log:
            self.task._handle_complete(action)
            assert_equal(mock_log.error.call_count, 1)

        self.task.notify.assert_called_with(self.task.NOTIFY_SUCCESS)

    def test_handle_complete(self):
        action = mock.create_autospec(ActionCommand, is_failed=False)
        self.task._handle_complete(action)
        self.task.notify.assert_called_with(self.task.NOTIFY_SUCCESS)


class ServiceInstanceStartTaskTestCase(TestCase):

    @setup
    def setup_task(self):
        self.node = mock.create_autospec(node.Node)
        self.task = serviceinstance.ServiceInstanceStartTask('id', self.node)
        autospec_method(self.task.notify)
        autospec_method(self.task.watch)

    def test_start(self):
        command = 'the command'
        patcher = mock.patch('tron.core.serviceinstance.ActionCommand', autospec=True)
        with patcher as mock_ac:
            self.task.start(command)
            self.task.watch.assert_called_with(mock_ac.return_value)
            self.node.submit_command.assert_called_with(mock_ac.return_value)
            mock_ac.assert_called_with("%s.start" % self.task.id, command,
                serializer=self.task.buffer_store)

    def test_start_failed(self):
        command = 'the command'
        self.node.submit_command.side_effect = node.Error
        self.task.start(command)
        self.task.notify.assert_called_with(self.task.NOTIFY_FAILED)

    def test_handle_action_event_exit(self):
        action = mock.create_autospec(ActionCommand)
        event = ActionCommand.EXITING
        self.task.handle_action_event(action, event)
        self.task.notify(self.task.NOTIFY_STARTED)

    def test_handle_action_event_failstart(self):
        action = mock.create_autospec(ActionCommand)
        event = ActionCommand.FAILSTART
        patcher = mock.patch('tron.core.serviceinstance.log', autospec=True)
        with patcher as mock_log:
            self.task.handle_action_event(action, event)
            assert_equal(mock_log.warn.call_count, 1)

    def test_handle_action_exit_fail(self):
        action = mock.create_autospec(ActionCommand, is_failed=True)
        self.task._handle_action_exit(action)
        self.task.notify.assert_called_with(self.task.NOTIFY_FAILED)

    def test_handle_action_exit_success(self):
        action = mock.create_autospec(ActionCommand, is_failed=False)
        self.task._handle_action_exit(action)
        self.task.notify.assert_called_with(self.task.NOTIFY_STARTED)


class ServiceInstanceTestCase(TestCase):

    @setup
    def setup_instance(self):
        self.config = mock.MagicMock()
        self.node = mock.create_autospec(node.Node, hostname='hostname')
        self.number = 5
        self.context = mock.create_autospec(command_context.CommandContext)
        self.instance = serviceinstance.ServiceInstance(
            self.config, self.node, self.number, self.context)
        self.instance.machine = mock.create_autospec(state.StateMachine, state=None)
        self.instance.start_task = mock.create_autospec(
            serviceinstance.ServiceInstanceStartTask)
        self.instance.stop_task = mock.create_autospec(
            serviceinstance.ServiceInstanceStopTask)
        self.instance.monitor_task = mock.create_autospec(
            serviceinstance.ServiceInstanceMonitorTask)
        self.instance.watch = mock.create_autospec(self.instance.watch)

    def test_create_tasks(self):
        self.instance.create_tasks()
        assert_equal(self.instance.watch.mock_calls, [
            mock.call(self.instance.monitor_task),
            mock.call(self.instance.start_task),
            mock.call(self.instance.stop_task),
        ])

    def test_start_invalid_state(self):
        self.instance.machine.transition.return_value = False
        self.instance.start()
        assert_equal(self.instance.start_task.start.call_count, 0)

    def test_start(self):
        self.instance.start()
        self.instance.start_task.start.assert_called_with(self.instance.command)

    def test_stop_invalid_state(self):
        self.instance.machine.check.return_value = False
        self.instance.stop()
        assert not self.instance.machine.transition.call_count

    def test_stop(self):
        self.instance.stop()
        self.instance.stop_task.stop.assert_called_with()
        self.instance.machine.transition.assert_called_with('stop')
        self.instance.monitor_task.cancel.assert_called_with()

    def test_handler_transition_map(self):
        obs = mock.Mock()
        event = serviceinstance.ServiceInstanceMonitorTask.NOTIFY_START
        self.instance.handler(obs, event)
        self.instance.machine.transition.assert_called_with("monitor")

    def test_handler_notify_started(self):
        obs = mock.Mock()
        event = serviceinstance.ServiceInstanceStartTask.NOTIFY_STARTED
        autospec_method(self.instance._handle_start_task_complete)
        self.instance.handler(obs, event)
        self.instance._handle_start_task_complete.assert_called_with()

    def test_handler_notify_success(self):
        obs = mock.Mock()
        event = serviceinstance.ServiceInstanceStopTask.NOTIFY_SUCCESS
        self.instance.handler(obs, event)
        self.instance.machine.transition.assert_called_with('down')

    def test_handle_start_task_complete(self):
        self.instance.machine = mock.Mock(
            state=serviceinstance.ServiceInstance.STATE_STARTING)
        self.instance._handle_start_task_complete()
        self.instance.monitor_task.queue.assert_called_with()

    def test_handle_start_task_complete_from_unknown(self):
        self.instance._handle_start_task_complete()
        self.instance.stop_task.stop.assert_called_with()

    def test_state_data(self):
        expected = {
            'instance_number': self.number,
            'node': self.node.hostname
        }
        assert_equal(self.instance.state_data, expected)

    def test_handler_many_monitor_failure(self):
        self.instance.failures = [1]*6
        self.instance.config.monitor_retries = 5
        self.instance.handler(self.instance.monitor_task,  serviceinstance.ServiceInstanceMonitorTask.NOTIFY_FAILED)
        self.instance.monitor_task.cancel.assert_called_with()
        self.instance.machine.transition.assert_called_with('stop')

class NodeSelectorTestCase(TestCase):

    @setup
    def setup_mocks(self):
        self.node_pool = mock.create_autospec(node.NodePool)

    def test_node_selector_no_hostname(self):
        selected_node = serviceinstance.node_selector(self.node_pool)
        assert_equal(selected_node, self.node_pool.next_round_robin())

    def test_node_selector_hostname_not_in_pool(self):
        hostname = 'hostname'
        self.node_pool.get_by_hostname.return_value = None
        selected_node = serviceinstance.node_selector(self.node_pool, hostname)
        assert_equal(selected_node, self.node_pool.next_round_robin.return_value)

    def test_node_selector_hostname_found(self):
        hostname = 'hostname'
        selected_node = serviceinstance.node_selector(self.node_pool, hostname)
        assert_equal(selected_node, self.node_pool.get_by_hostname.return_value)


def create_mock_instance(**kwargs):
    return mock.create_autospec(serviceinstance.ServiceInstance, **kwargs)

class ServiceInstanceCollectionTestCase(TestCase):

    @setup
    def setup_collection(self):
        self.node_pool      = mock.create_autospec(node.NodePool)
        self.config         = mock.Mock()
        self.context        = mock.Mock()
        self.collection     = serviceinstance.ServiceInstanceCollection(
            self.config, self.node_pool, self.context)

    def test__init__(self):
        assert_equal(self.collection.config.count, self.config.count)
        assert_equal(self.collection.config, self.config)
        assert_equal(self.collection.instances,
            self.collection.instances_proxy.obj_list_getter())

    def test_clear_failed(self):
        def build(state):
            inst = create_mock_instance()
            inst.get_state.return_value = state
            return inst
        instances = [
            build(serviceinstance.ServiceInstance.STATE_FAILED),
            build(serviceinstance.ServiceInstance.STATE_UP)]
        self.collection.instances.extend(instances)
        self.collection.clear_failed()
        assert_equal(self.collection.instances, instances[1:])

    def test_clear_failed_none(self):
        instances = [create_mock_instance(state=serviceinstance.ServiceInstance.STATE_UP)]
        self.collection.instances.extend(instances)
        self.collection.clear_failed()
        assert_equal(self.collection.instances, instances)

    def test_create_missing(self):
        self.collection.config.count = 5
        autospec_method(self.collection._build_instance)
        created = self.collection.create_missing()
        assert_length(created, 5)
        assert_equal(set(created), set(self.collection.instances))

    def test_create_missing_none(self):
        self.collection.config.count = 2
        self.collection.instances = [create_mock_instance(instance_number=i) for i in range(2)]
        created = self.collection.create_missing()
        assert_length(created, 0)

    def test_build_instance(self):
        patcher = mock.patch('tron.core.serviceinstance.ServiceInstance', autospec=True)
        mock_node = mock.create_autospec(node.Node)
        number = 7
        with patcher as mock_service_instance_class:
            instance = self.collection._build_instance(mock_node, number)
            factory = mock_service_instance_class.create
            assert_equal(instance, factory.return_value)
            factory.assert_called_with(
                    self.config, mock_node, number, self.collection.context)

    def test_restore_state(self):
        count = 3
        state_data = [
            dict(instance_number=i*3, node='node') for i in xrange(count)]
        autospec_method(self.collection._build_instance)
        created = self.collection.restore_state(state_data)
        assert_length(created, count)
        assert_equal(set(created), set(self.collection.instances))
        expected = [
            mock.call(self.node_pool.get_by_hostname.return_value,
                d['instance_number'])
            for d in state_data]
        for expected_call in expected:
            assert_in(expected_call, self.collection._build_instance.mock_calls)

    def test_build_and_sort(self):
        autospec_method(self.collection.sort)
        count = 4
        builder, seq = mock.Mock(), range(count)
        instances = self.collection._build_and_sort(builder, seq)
        self.collection.sort.assert_called_with()
        assert_equal(builder.mock_calls, [mock.call(i) for i in seq])
        assert_length(instances, count)
        assert_equal(instances, self.collection.instances)

    def test_next_instance_number(self):
        self.collection.config.count = 6
        self.collection.instances = [create_mock_instance(instance_number=i) for i in range(5)]
        assert_equal(self.collection.next_instance_number(), 5)

    def test_next_instance_number_in_middle(self):
        self.collection.config.count = 6
        self.collection.instances = [
            create_mock_instance(instance_number=i) for i in range(6) if i != 3]
        assert_equal(self.collection.next_instance_number(), 3)

    def test_missing(self):
        self.collection.config.count = 5
        assert_equal(self.collection.missing, 5)

        self.collection.instances = range(5)
        assert_equal(self.collection.missing, 0)

    def test_all_true(self):
        state = serviceinstance.ServiceInstance.STATE_UP
        self.collection.config.count = count = 4
        def build():
            inst = create_mock_instance()
            inst.get_state.return_value = state
            return inst
        self.collection.instances = [build() for _ in xrange(count)]
        assert self.collection.all(state)

    def test_all_empty(self):
        assert not self.collection.all(serviceinstance.ServiceInstance.STATE_UP)

    def test_all_false(self):
        state = serviceinstance.ServiceInstance.STATE_UP
        def build():
            inst = create_mock_instance()
            inst.get_state.return_value = state
            return inst
        self.collection.instances = [build() for _ in xrange(3)]
        self.collection.instances.append(create_mock_instance())
        assert not self.collection.all(state)

    def test__eq__(self):
        other = serviceinstance.ServiceInstanceCollection(
            self.config, self.node_pool, self.context)
        assert_equal(self.collection, other)

    def test__ne__(self):
        other = serviceinstance.ServiceInstanceCollection(
            mock.Mock(), self.node_pool, self.context)
        assert_not_equal(self.collection, other)
        other = serviceinstance.ServiceInstanceCollection(
            self.config, mock.Mock(), self.context)
        assert_not_equal(self.collection, other)

    def test_get_by_number(self):
        self.collection.instances = instances = [
                    create_mock_instance(instance_number=i) for i in range(5)]
        instance = self.collection.get_by_number(3)
        assert_equal(instance, instances[3])


if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = service_test
import mock
from testify import setup, assert_equal, TestCase, run
from testify.assertions import assert_not_equal
from tests.assertions import assert_mock_calls

from tests.testingutils import autospec_method
from tron.core import service, serviceinstance
from tron import node, command_context, event, eventloop
from tron.core.serviceinstance import ServiceInstance

class ServiceStateTestCase(TestCase):

    @setup
    def setup_service(self):
        self.instances = mock.create_autospec(
            serviceinstance.ServiceInstanceCollection)
        self.service = mock.Mock(enabled=True, instances=self.instances)

    def test_state_disabled(self):
        self.service.enabled = False
        state = service.ServiceState.from_service(self.service)
        assert_equal(state, service.ServiceState.DISABLED)

    def test_state_up(self):
        self.service.enabled = True
        state = service.ServiceState.from_service(self.service)
        assert_equal(state, service.ServiceState.UP)
        self.instances.is_up.assert_called_with()

    def test_state_degraded(self):
        self.service.enabled = True
        self.instances.all.return_value = False
        self.instances.is_starting.return_value = False
        self.instances.is_up.return_value = False
        state = service.ServiceState.from_service(self.service)
        assert_equal(state, service.ServiceState.DEGRADED)


class ServiceTestCase(TestCase):

    @setup
    def setup_service(self):
        self.config = mock.MagicMock()
        self.instances = mock.create_autospec(
            serviceinstance.ServiceInstanceCollection,
            stop=mock.Mock(), start=mock.Mock(), state_data=mock.Mock(),
            restore=mock.Mock())
        self.service = service.Service(self.config, self.instances)
        autospec_method(self.service.watch)
        self.service.repair_callback = mock.create_autospec(
            eventloop.UniqueCallback)

    @mock.patch('tron.core.service.node')
    def test_from_config(self, mock_node):
        node_store = mock.create_autospec(node.NodePoolRepository)
        mock_node.NodePoolRepository.get_instance.return_value = node_store
        node_store.get_by_name.return_value = mock.create_autospec(node.Node)
        context = mock.create_autospec(command_context.CommandContext)

        service_inst = service.Service.from_config(self.config, context)
        collection = service_inst.instances
        assert_equal(service_inst.config, self.config)
        assert_equal(collection.node_pool, node_store.get_by_name.return_value)
        assert_equal(collection.context, context)

    def test_enable(self):
        autospec_method(self.service.repair)
        self.service.enable()
        assert self.service.enabled
        self.service.repair.assert_called_with()

    def test_disable(self):
        self.service.disable()
        assert not self.service.enabled
        self.instances.stop.assert_called_with()
        self.service.repair_callback.cancel.assert_called_with()

    def test_repair(self):
        autospec_method(self.service.notify)
        count = 3
        created_instances = [
            mock.create_autospec(ServiceInstance) for _ in xrange(count)]
        self.instances.create_missing.return_value = created_instances
        self.service.repair()
        self.instances.clear_failed.assert_called_with()
        assert_equal(self.service.watch.mock_calls,
            [mock.call(inst.get_observable(), True) for inst in created_instances])
        self.instances.restore.assert_called_with()
        self.instances.start.assert_called_with()
        self.service.notify.assert_called_with(self.service.NOTIFY_STATE_CHANGE)

    def test_handle_instance_state_change_down(self):
        autospec_method(self.service.notify)
        instance_event = serviceinstance.ServiceInstance.STATE_DOWN
        self.service._handle_instance_state_change(mock.Mock(), instance_event)
        self.service.notify.assert_called_with(self.service.NOTIFY_STATE_CHANGE)
        self.service.instances.clear_down.assert_called_with()

    def test_handle_instance_state_change_failed(self):
        autospec_method(self.service.notify)
        autospec_method(self.service.record_events)
        instance_event = serviceinstance.ServiceInstance.STATE_FAILED
        self.service._handle_instance_state_change(mock.Mock(), instance_event)
        assert not self.service.notify.mock_calls
        self.service.record_events.assert_called_with()

    def test_handle_instance_state_change_starting(self):
        autospec_method(self.service.notify)
        autospec_method(self.service.record_events)
        instance_event = serviceinstance.ServiceInstance.STATE_STARTING
        self.service._handle_instance_state_change(mock.Mock(), instance_event)
        assert not self.service.notify.mock_calls
        assert not self.service.record_events.mock_calls

    def test_record_events_failure(self):
        autospec_method(self.service.get_state)
        state = self.service.get_state.return_value  = service.ServiceState.FAILED
        self.service.event_recorder = mock.create_autospec(event.EventRecorder)
        self.service.record_events()
        self.service.event_recorder.critical.assert_called_with(state)

    def test_record_events_up(self):
        autospec_method(self.service.get_state)
        state = self.service.get_state.return_value  = service.ServiceState.UP
        self.service.event_recorder = mock.create_autospec(event.EventRecorder)
        self.service.record_events()
        self.service.event_recorder.ok.assert_called_with(state)

    def test_state_data(self):
        expected = dict(enabled=False, instances=self.instances.state_data)
        assert_equal(self.service.state_data, expected)

    def test__eq__not_equal(self):
        assert_not_equal(self.service, None)
        assert_not_equal(self.service, mock.Mock())
        other = service.Service(self.config, mock.Mock())
        assert_not_equal(self.service, other)

    def test__eq__(self):
        other = service.Service(self.config, self.instances)
        assert_equal(self.service, other)

    def test_restore_state(self):
        autospec_method(self.service.watch_instances)
        autospec_method(self.service.enable)
        state_data = {'enabled': True, 'instances': []}
        self.service.restore_state(state_data)
        self.service.watch_instances.assert_called_with(
            self.instances.restore_state.return_value)
        self.service.enable.assert_called_with()


class ServiceCollectionTestCase(TestCase):

    @setup
    def setup_collection(self):
        self.collection = service.ServiceCollection()
        self.service_list = [
            mock.create_autospec(service.Service) for _ in xrange(3)]

    def _add_service(self):
        self.collection.services.update(
            (serv.name, serv) for serv in self.service_list)

    @mock.patch('tron.core.service.Service', autospec=True)
    def test_load_from_config(self, mock_service):
        autospec_method(self.collection.get_names)
        autospec_method(self.collection.add)
        service_configs = {'a': mock.Mock(), 'b': mock.Mock()}
        context = mock.create_autospec(command_context.CommandContext)
        result = list(self.collection.load_from_config(service_configs, context))
        expected = [mock.call(config, context)
                    for config in service_configs.itervalues()]
        assert_mock_calls(expected, mock_service.from_config.mock_calls)
        expected = [mock.call(s) for s in result]
        assert_mock_calls(expected, self.collection.add.mock_calls)

    def test_add(self):
        self.collection.services = mock.MagicMock()
        service = mock.Mock()
        result = self.collection.add(service)
        self.collection.services.replace.assert_called_with(service)
        assert_equal(result, self.collection.services.replace.return_value)

    def test_restore_state(self):
        state_count = 2
        state_data = dict(
            (serv.name, serv) for serv in self.service_list[:state_count])
        self._add_service()
        self.collection.restore_state(state_data)
        for name in state_data:
            service = self.collection.services[name]
            service.restore_state.assert_called_with(state_data[name])


if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = crash_reporter_test
import logging
from testify import TestCase, run, setup, assert_equal
from testify.utils import turtle
from tests.assertions import assert_length, assert_call

from tron import crash_reporter

class TestError(Exception):
    pass

class SimpleDeferredTestCase(TestCase):

    @setup
    def setup_crash_reporter(self):
        self.emailer = turtle.Turtle()
        self.mcp = turtle.Turtle()
        self.reporter = crash_reporter.CrashReporter(self.emailer)
        self.event_dict = {'isError': False, 'message': ['']}

    def test__init__(self):
        assert_equal(self.reporter.emailer, self.emailer)
        assert self.reporter.event_recorder

    def test_get_level(self):
        event_dict = {'logLevel': 'WHAT'}
        assert_equal(self.reporter._get_level(event_dict), 'WHAT')

    def test_get_level_error(self):
        event_dict = {'isError': True}
        assert_equal(self.reporter._get_level(event_dict), logging.ERROR)

    def test_get_level_default(self):
        assert_equal(self.reporter._get_level(self.event_dict), logging.INFO)

    def test_emit_no_text(self):
        self.reporter.emit(self.event_dict)
        assert_length(self.emailer.send.calls, 0)

    def test_emit_unhandled(self):
        self.event_dict['message'] = ["Unhandled error in Deferred:"]
        self.event_dict['isError'] = True
        self.reporter.emit(self.event_dict)
        assert_length(self.emailer.send.calls, 0)

    def test_emit_ignored_level(self):
        self.event_dict['message'] = "Some message."
        self.reporter.emit(self.event_dict)
        assert_length(self.emailer.send.calls, 0)

    def test_emit_crash(self):
        self.event_dict['message'] = ["Ooops"]
        self.event_dict['isError'] = True
        self.reporter.emit(self.event_dict)
        assert_call(self.emailer.send, 0, "Ooops")


if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = eventloop_test
import mock
from testify import TestCase
from testify import setup
from testify.assertions import assert_equal

from tron import eventloop


class UniqueCallTestCase(TestCase):

    @setup
    def setup_monitor(self):
        self.func = mock.Mock()
        self.callback = eventloop.UniqueCallback(5, self.func)
        self.callback.delayed_call = mock.Mock()

    def test__init__(self):
        assert_equal(self.callback.delay, 5)
        assert_equal(self.callback.func, self.func)

    def test_start_no_restart_interval(self):
        self.callback.delay = None
        with mock.patch('tron.eventloop.call_later', autospec=True) as mock_call_later:
            self.callback.start()
            assert not mock_call_later.call_count

    def test_start(self):
        self.callback.delayed_call.active.return_value = False
        with mock.patch('tron.eventloop.call_later', autospec=True) as mock_call_later:
            self.callback.start()
            mock_call_later.assert_called_with(
                self.callback.delay, self.callback.func)
            assert_equal(self.callback.delayed_call,
                mock_call_later.return_value)

    def test_start_already_actice(self):
        self.callback.delayed_call.active.return_value = True
        with mock.patch('tron.eventloop.call_later', autospec=True) as mock_call_later:
            self.callback.start()
            assert not mock_call_later.call_count
########NEW FILE########
__FILENAME__ = event_test
from testify import setup, TestCase, assert_equal, teardown, assert_raises
from tests.assertions import assert_length

from tron import event

class EventStoreTestCase(TestCase):

    @setup
    def build_store(self):
        self.limits = {
            event.LEVEL_INFO:       2,
            event.LEVEL_CRITICAL:   3
        }
        self.store = event.EventStore(self.limits)

    def _build_event(self, level, name):
        return event.Event('entity', level, name)

    @setup
    def add_data(self):
        for i in xrange(1,5):
            self.store.append(self._build_event(event.LEVEL_INFO, "test%s" % i))

        for i in xrange(5,10):
            e = self._build_event(event.LEVEL_CRITICAL, "test%s" % i)
            self.store.append(e)

        self.store.append(self._build_event(event.LEVEL_OK, "alpha"))

    def test_build_deque(self):
        deq = self.store._build_deque('stars')
        deq.extend(range(12))
        assert_equal(len(deq), event.EventStore.DEFAULT_LIMIT)

    def test_append(self):
        assert_equal(len(self.store.events), 3)
        for level, limit in self.limits.iteritems():
            assert_equal(len(self.store.events[level]), limit)

    def test_get_events(self):
        values = set(e.name for e in self.store.get_events())
        expected = set(['test3', 'test4', 'test7', 'test8', 'test9', 'alpha'])
        assert_equal(values, expected)

    def test_get_events_with_min_level(self):
        values = set(e.name for e in self.store.get_events(event.LEVEL_OK))
        expected = set(['test7', 'test8', 'test9', 'alpha'])
        assert_equal(values, expected)


class EventRecorderTestCase(TestCase):

    @setup
    def build_recorders(self):
        self.entity_name = 'the_name'
        self.recorder    = event.EventRecorder(self.entity_name)

    def test_get_child(self):
        child_rec = self.recorder.get_child('start')
        assert_equal(self.recorder.get_child('start'), child_rec)
        assert_equal(child_rec.name, 'the_name.start')

    def test_get_child_missing(self):
        child_rec = self.recorder.get_child('next')
        assert_equal(child_rec.name, 'the_name.next')
        assert_equal(self.recorder.children['next'], child_rec)

    def test_remove_child(self):
        self.recorder.get_child('next')
        self.recorder.remove_child('next')
        assert 'next' not in self.recorder.children

    def test_remove_child_missing(self):
        self.recorder.remove_child('bogus')
        assert 'bogus' not in self.recorder.children

    def test_record(self):
        self.recorder._record(event.LEVEL_CRITICAL, 'this thing')
        recorded_event = self.recorder.events.events[event.LEVEL_CRITICAL][0]
        assert_equal(recorded_event.level, event.LEVEL_CRITICAL)
        assert_equal(recorded_event.name, 'this thing')
        assert_equal(recorded_event.entity, self.entity_name)

    def test_list_with_children(self):
        self.recorder.ok('one')
        self.recorder.notice('two')
        child_rec = self.recorder.get_child('stars')
        child_rec.critical('three')
        child_rec.ok('four')
        self.recorder.info('five')

        events = self.recorder.list()
        expected = reversed(['one', 'two', 'three', 'four', 'five'])
        assert_equal([e.name for e in events], list(expected))

    def test_list_without_children(self):
        self.recorder.ok('one')
        self.recorder.notice('two')
        child_rec = self.recorder.get_child('stars')
        child_rec.critical('three')
        child_rec.ok('four')
        self.recorder.info('five')

        events = self.recorder.list(child_events=False)
        expected = ['five', 'two', 'one']
        assert_equal([e.name for e in events], expected)

    def test_list_no_events(self):
        assert_length(self.recorder.list(), 0)
        assert_length(self.recorder.list(child_events=False), 0)


class EventManagerTestCase(TestCase):

    @setup
    def setup_manager(self):
        self.manager = event.EventManager.get_instance()
        self.root = self.manager.root_recorder

    @teardown
    def teardown_manager(self):
        self.manager.reset()

    def test_get_instance(self):
        assert_equal(self.manager, event.EventManager.get_instance())
        assert_raises(ValueError, event.EventManager)

    def test_get_root(self):
        recorder = self.manager.get('')
        assert_equal(recorder, self.root)

    def test_get_nested(self):
        name = 'one.two'
        recorder = self.manager.get(name)
        assert_equal(self.manager.get(name), recorder)
        assert_equal(recorder.name, name)

    def test_get_missing(self):
        name = 'name.second.third'
        recorder = self.manager.get(name)
        assert_equal(
            self.root.children['name'].children['second'].children['third'],
            recorder)
        assert_equal(recorder.name, name)

    def test_remove(self):
        self.manager.get('one')
        self.manager.remove('one')
        assert not self.manager.root_recorder.children.get('one')

    def test_remove_nested(self):
        self.manager.get('one.two')
        self.manager.remove('one.two')
        assert not self.root.children['one'].children.get('two')
        assert self.root.children['one']

    def test_remove_missing_nested(self):
        self.manager.get('one')
        self.manager.remove('one.two')
        assert not self.root.children['one'].children.get('two')
        assert self.root.children['one']

    def test_remove_missing(self):
        self.manager.remove('bogus')
        assert not self.manager.root_recorder.children.get('bogus')

########NEW FILE########
__FILENAME__ = mcp_reconfigure_test
"""Tests for reconfiguring mcp."""
import tempfile

from testify import TestCase, run, setup, assert_equal, teardown, suite

from tests.assertions import assert_length
from tron import mcp, event
from tron.config import config_parse, schema
from tron.serialize import filehandler


class MCPReconfigureTestCase(TestCase):

    pre_config = dict(
        ssh_options=dict(
            agent=True,
            identities=['tests/test_id_rsa'],
        ),
        nodes=[
            dict(name='node0', hostname='batch0'),
            dict(name='node1', hostname='batch1'),
        ],
        node_pools=[dict(name='nodePool', nodes=['node0', 'node1'])],
        command_context={
            'thischanges': 'froma'
        },
        jobs=[
            dict(
                name='test_unchanged',
                node='node0',
                schedule='daily',
                actions=[dict(name='action_unchanged',
                              command='command_unchanged') ]
            ),
            dict(
                name='test_remove',
                node='node1',
                schedule=dict(interval='20s'),
                actions=[dict(name='action_remove',
                              command='command_remove')],
                cleanup_action=dict(name='cleanup', command='doit')
            ),
            dict(
                name='test_change',
                node='nodePool',
                schedule=dict(interval='20s'),
                actions=[
                    dict(name='action_change',
                         command='command_change'),
                    dict(name='action_remove2',
                         command='command_remove2',
                         requires=['action_change']),
                ],
            ),
            dict(
                name='test_daily_change',
                node='node0',
                schedule='daily',
                        actions=[dict(name='action_daily_change',
                                      command='command')],
            ),
            dict(
                name='test_action_added',
                node='node0',
                schedule=dict(interval='10s'),
                actions=[
                    dict(name='action_first', command='command_do_it')
                ]
            )
        ])

    post_config = dict(
        ssh_options=dict(
            agent=True,
            identities=['tests/test_id_rsa'],
        ),
        nodes=[
            dict(name='node0', hostname='batch0'),
            dict(name='node1', hostname='batch1'),
        ],
        node_pools=[dict(name='nodePool', nodes=['node0', 'node1'])],
        command_context={
            'a_variable': 'is_constant',
            'thischanges': 'tob'
        },
        jobs=[
            dict(
                name='test_unchanged',
                node='node0',
                schedule='daily',
                actions=[dict(name='action_unchanged',
                              command='command_unchanged') ]
            ),
            dict(
                name='test_change',
                node='nodePool',
                schedule='daily',
                actions=[
                    dict(name='action_change',
                         command='command_changed'),
                ],
            ),
            dict(
                name='test_daily_change',
                node='node0',
                schedule='daily',
                        actions=[dict(name='action_daily_change',
                                      command='command_changed')],
            ),
            dict(
                name='test_new',
                node='nodePool',
                schedule=dict(interval='20s'),
                actions=[dict(name='action_new',
                              command='command_new')]
            ),
            dict(
                name='test_action_added',
                node='node0',
                schedule=dict(interval='10s'),
                actions=[
                    dict(name='action_first', command='command_do_it'),
                    dict(name='action_second', command='command_ok'),
                ]
            )
        ])

    def _get_config(self, idx, output_dir):
        config = dict(self.post_config if idx else self.pre_config)
        config['output_stream_dir'] = output_dir
        return config

    @setup
    def setup_mcp(self):
        self.test_dir = tempfile.mkdtemp()
        self.mcp = mcp.MasterControlProgram(self.test_dir, 'config')
        config = {schema.MASTER_NAMESPACE: self._get_config(0, self.test_dir)}
        container = config_parse.ConfigContainer.create(config)
        self.mcp.apply_config(container)

    @teardown
    def teardown_mcp(self):
        event.EventManager.reset()
        filehandler.OutputPath(self.test_dir).delete()
        filehandler.FileHandleManager.reset()

    def reconfigure(self):
        config = {schema.MASTER_NAMESPACE: self._get_config(1, self.test_dir)}
        container = config_parse.ConfigContainer.create(config)
        self.mcp.apply_config(container, reconfigure=True)

    @suite('integration')
    def test_job_list(self):
        count = len(self.pre_config['jobs'])
        assert_equal(len(self.mcp.jobs.get_names()), count)
        self.reconfigure()
        assert_equal(len(self.mcp.jobs.get_names()), count)

    @suite('integration')
    def test_job_unchanged(self):
        assert 'MASTER.test_unchanged' in self.mcp.jobs
        job_sched = self.mcp.jobs.get_by_name('MASTER.test_unchanged')
        orig_job = job_sched.job
        run0 = job_sched.get_runs_to_schedule(False).next()
        run0.start()
        run1 = job_sched.get_runs_to_schedule(False).next()

        assert_equal(job_sched.job.name, "MASTER.test_unchanged")
        action_map = job_sched.job.action_graph.action_map
        assert_equal(len(action_map), 1)
        assert_equal(action_map['action_unchanged'].name, 'action_unchanged')
        assert_equal(str(job_sched.job.scheduler), "daily 00:00:00 ")

        self.reconfigure()
        assert job_sched is self.mcp.jobs.get_by_name('MASTER.test_unchanged')
        assert job_sched.job is orig_job

        assert_equal(len(job_sched.job.runs.runs), 2)
        assert_equal(job_sched.job.runs.runs[1], run0)
        assert_equal(job_sched.job.runs.runs[0], run1)
        assert run1.is_scheduled
        assert_equal(job_sched.job.context['a_variable'], 'is_constant')
        assert_equal(job_sched.job.context['thischanges'], 'tob')

    @suite('integration')
    def test_job_unchanged_disabled(self):
        job_sched = self.mcp.jobs.get_by_name('MASTER.test_unchanged')
        orig_job = job_sched.job
        job_sched.get_runs_to_schedule(False).next()
        job_sched.disable()

        self.reconfigure()
        assert job_sched is self.mcp.jobs.get_by_name('MASTER.test_unchanged')
        assert job_sched.job is orig_job
        assert not job_sched.job.enabled

    @suite('integration')
    def test_job_removed(self):
        assert 'MASTER.test_remove' in self.mcp.jobs
        job_sched = self.mcp.jobs.get_by_name('MASTER.test_remove')
        run0 = job_sched.get_runs_to_schedule(False).next()
        run0.start()
        run1 = job_sched.get_runs_to_schedule(False).next()

        assert_equal(job_sched.job.name, "MASTER.test_remove")
        action_map = job_sched.job.action_graph.action_map
        assert_equal(len(action_map), 2)
        assert_equal(action_map['action_remove'].name, 'action_remove')

        self.reconfigure()
        assert not 'test_remove' in self.mcp.jobs
        assert not job_sched.job.enabled
        assert not run1.is_scheduled

    @suite('integration')
    def test_job_changed(self):
        assert 'MASTER.test_change' in self.mcp.jobs
        job_sched = self.mcp.jobs.get_by_name('MASTER.test_change')
        run0 = job_sched.get_runs_to_schedule(False).next()
        run0.start()
        job_sched.get_runs_to_schedule(False).next()
        assert_equal(len(job_sched.job.runs.runs), 2)

        assert_equal(job_sched.job.name, "MASTER.test_change")
        action_map = job_sched.job.action_graph.action_map
        assert_equal(len(action_map), 2)

        self.reconfigure()
        new_job_sched = self.mcp.jobs.get_by_name('MASTER.test_change')
        assert new_job_sched is job_sched
        assert new_job_sched.job is job_sched.job

        assert_equal(new_job_sched.job.name, "MASTER.test_change")
        action_map = job_sched.job.action_graph.action_map
        assert_equal(len(action_map), 1)

        assert_equal(len(new_job_sched.job.runs.runs), 2)
        assert new_job_sched.job.runs.runs[1].is_starting
        assert new_job_sched.job.runs.runs[0].is_scheduled
        assert_equal(job_sched.job.context['a_variable'], 'is_constant')
        assert new_job_sched.job.context.base.job is new_job_sched.job

    @suite('integration')
    def test_job_changed_disabled(self):
        job_sched = self.mcp.jobs.get_by_name('MASTER.test_change')
        job_sched.disable()
        assert not job_sched.job.enabled

        self.reconfigure()
        new_job_sched = self.mcp.jobs.get_by_name('MASTER.test_change')
        assert not new_job_sched.job.enabled

    @suite('integration')
    def test_job_new(self):
        assert not 'test_new' in self.mcp.jobs
        self.reconfigure()

        assert 'MASTER.test_new' in self.mcp.jobs
        job_sched = self.mcp.jobs.get_by_name('MASTER.test_new')

        assert_equal(job_sched.job.name, "MASTER.test_new")
        action_map = job_sched.job.action_graph.action_map
        assert_equal(len(action_map), 1)
        assert_equal(action_map['action_new'].name, 'action_new')
        assert_equal(action_map['action_new'].command, 'command_new')
        assert_equal(len(job_sched.job.runs.runs), 1)
        assert job_sched.job.runs.runs[0].is_scheduled

    @suite('integration')
    def test_daily_reschedule(self):
        job_sched = self.mcp.jobs.get_by_name('MASTER.test_daily_change')

        job_sched.get_runs_to_schedule(False).next()

        assert_equal(len(job_sched.job.runs.runs), 1)
        run = job_sched.job.runs.runs[0]
        assert run.is_scheduled

        self.reconfigure()
        assert run.is_cancelled

        assert_equal(len(job_sched.job.runs.runs), 1)
        new_run = job_sched.job.runs.runs[0]
        assert new_run is not run
        assert new_run.is_scheduled
        assert_equal(run.run_time, new_run.run_time)

    @suite('integration')
    def test_action_added(self):
        self.reconfigure()
        job_sched = self.mcp.jobs.get_by_name('MASTER.test_action_added')
        assert_length(job_sched.job.action_graph.action_map, 2)

if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = mcp_test
import shutil
import tempfile

import mock
from testify import TestCase, setup, teardown
from testify import  assert_equal, run
from tests.testingutils import autospec_method

from tron import mcp, event
from tron.core import service, job
from tron.serialize.runstate import statemanager
from tron.config import config_parse, manager


class MasterControlProgramTestCase(TestCase):

    TEST_CONFIG = 'tests/data/test_config.yaml'

    @setup
    def setup_mcp(self):
        self.working_dir    = tempfile.mkdtemp()
        self.config_path    = tempfile.mkdtemp()
        self.mcp            = mcp.MasterControlProgram(
                                self.working_dir, self.config_path)
        self.mcp.state_watcher = mock.create_autospec(
                                statemanager.StateChangeWatcher)

    @teardown
    def teardown_mcp(self):
        event.EventManager.reset()
        shutil.rmtree(self.config_path)
        shutil.rmtree(self.working_dir)

    def test_reconfigure(self):
        autospec_method(self.mcp._load_config)
        self.mcp.state_watcher = mock.MagicMock()
        self.mcp.reconfigure()
        self.mcp._load_config.assert_called_with(reconfigure=True)

    def test_load_config(self):
        autospec_method(self.mcp.apply_config)
        self.mcp.config = mock.create_autospec(manager.ConfigManager)
        self.mcp._load_config()
        self.mcp.state_watcher.disabled.assert_called_with()
        self.mcp.apply_config.assert_called_with(
            self.mcp.config.load.return_value, reconfigure=False)

    def test_graceful_shutdown(self):
        self.mcp.graceful_shutdown()
        for job_sched in self.mcp.get_job_collection():
            assert job_sched.shutdown_requested

    @mock.patch('tron.mcp.node.NodePoolRepository', autospec=True)
    def test_apply_config(self, mock_repo):
        config_container = mock.create_autospec(config_parse.ConfigContainer)
        master_config = config_container.get_master.return_value
        autospec_method(self.mcp.apply_collection_config)
        autospec_method(self.mcp.apply_notification_options)
        autospec_method(self.mcp.build_job_scheduler_factory)
        self.mcp.apply_config(config_container)
        self.mcp.state_watcher.update_from_config.assert_called_with(
            master_config.state_persistence)
        assert_equal(self.mcp.context.base, master_config.command_context)
        assert_equal(len(self.mcp.apply_collection_config.mock_calls), 2)
        self.mcp.apply_notification_options.assert_called_with(
            master_config.notification_options)
        mock_repo.update_from_config.assert_called_with(master_config.nodes, 
            master_config.node_pools, master_config.ssh_options)
        self.mcp.build_job_scheduler_factory(master_config)

    def test_update_state_watcher_config_changed(self):
        self.mcp.state_watcher.update_from_config.return_value = True
        self.mcp.jobs = mock.create_autospec(job.JobCollection)
        self.mcp.jobs.__iter__.return_values = {'a': mock.Mock(), 'b': mock.Mock()}
        self.mcp.services = mock.create_autospec(service.ServiceCollection)
        self.mcp.services.__iter__.return_value = {'c': mock.Mock(), 'd': mock.Mock()}
        state_config = mock.Mock()
        self.mcp.update_state_watcher_config(state_config)
        self.mcp.state_watcher.update_from_config.assert_called_with(state_config)
        assert_equal(
            self.mcp.state_watcher.save_job.mock_calls,
            [mock.call(j.job) for j in self.mcp.jobs])
        assert_equal(
            self.mcp.state_watcher.save_service.mock_calls,
            [mock.call(s) for s in self.mcp.services])

    def test_update_state_watcher_config_no_change(self):
        self.mcp.state_watcher.update_from_config.return_value = False
        self.mcp.jobs = {'a': mock.Mock(), 'b': mock.Mock()}
        state_config = mock.Mock()
        self.mcp.update_state_watcher_config(state_config)
        assert not self.mcp.state_watcher.save_job.mock_calls


class MasterControlProgramRestoreStateTestCase(TestCase):

    @setup
    def setup_mcp(self):
        self.working_dir        = tempfile.mkdtemp()
        self.config_path        = tempfile.mkdtemp()
        self.mcp                = mcp.MasterControlProgram(
                                    self.working_dir, self.config_path)
        self.mcp.jobs           = mock.create_autospec(job.JobCollection)
        self.mcp.services       = mock.create_autospec(service.ServiceCollection)
        self.mcp.state_watcher  = mock.create_autospec(statemanager.StateChangeWatcher)

    @teardown
    def teardown_mcp(self):
        event.EventManager.reset()
        shutil.rmtree(self.working_dir)
        shutil.rmtree(self.config_path)

    def test_restore_state(self):
        service_state_data = {'3': 'things', '4': 'things'}
        job_state_data = {'1': 'things', '2': 'things'}
        self.mcp.state_watcher.restore.return_value = job_state_data, service_state_data
        self.mcp.restore_state()
        self.mcp.jobs.restore_state.assert_called_with(job_state_data)
        self.mcp.services.restore_state.assert_called_with(service_state_data)


if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = mocks
import atexit
import datetime
from exceptions import KeyError
import itertools
import shutil
import tempfile
from tests.testingutils import Turtle


class MockAction(Turtle):

    def __init__(self, *args, **kwargs):
        kwargs.setdefault('name', 'action_name')
        kwargs.setdefault('required_actions', [])
        kwargs.setdefault('dependent_actions', [])
        super(MockAction, self).__init__(*args, **kwargs)


class MockActionGraph(Turtle):

    def __init__(self, *args, **kwargs):
        action = MockAction()
        kwargs.setdefault('graph', [action])
        kwargs.setdefault('action_map', {action.name: action})
        super(MockActionGraph, self).__init__(*args, **kwargs)

    def __getitem__(self, item):
        action = MockAction(name=item)
        self.action_map.setdefault(item, action)
        return self.action_map[item]

    def get_required_actions(self, name):
        return []


class MockActionRun(Turtle):

    def __init__(self, *args, **kwargs):
        kwargs.setdefault('output_path', [tempfile.mkdtemp()])
        kwargs.setdefault('start_time', datetime.datetime.now())
        kwargs.setdefault('end_time', datetime.datetime.now())
        atexit.register(lambda: shutil.rmtree(kwargs['output_path'][0]))
        super(MockActionRun, self).__init__(*args, **kwargs)


class MockActionRunCollection(Turtle):

    def __init__(self, *args, **kwargs):
        kwargs.setdefault('action_graph', MockActionGraph())
        kwargs.setdefault('run_map', {})
        super(MockActionRunCollection, self).__init__(*args, **kwargs)

    def __getitem__(self, item):
        action_run = MockActionRun(name=item)
        self.run_map.setdefault(item, action_run)
        return self.run_map[item]


class MockJobRun(Turtle):

    def __init__(self, *args, **kwargs):
        kwargs.setdefault('output_path', [tempfile.mkdtemp()])
        kwargs.setdefault('action_graph', MockActionGraph())
        action_runs = MockActionRunCollection(action_graph=kwargs['action_graph'])
        kwargs.setdefault('action_runs', action_runs)
        atexit.register(lambda: shutil.rmtree(kwargs['output_path'][0]))
        super(MockJobRun, self).__init__(*args, **kwargs)


class MockNode(Turtle):

    def __init__(self, hostname=None):
        super(MockNode, self).__init__()
        self.name = self.hostname = hostname

    def run(self, runnable):
        runnable.started()
        return type(self)()


class MockNodePool(object):
    _node = None

    def __init__(self, *node_names):
        self.nodes = []
        self._ndx_cycle = None
        for hostname in node_names:
            self.nodes.append(MockNode(hostname=hostname))

        if self.nodes:
            self._ndx_cycle = itertools.cycle(range(0, len(self.nodes)))

    def __getitem__(self, value):
        for node in self.nodes:
            if node.hostname == value:
                return node
        else:
            raise KeyError

    def next(self):
        if not self.nodes:
            self.nodes.append(MockNode())

        if self._ndx_cycle:
            return self.nodes[self._ndx_cycle.next()]
        else:
            return self.nodes[0]

    next_round_robin = next


class MockJobRunCollection(Turtle):

    def __iter__(self):
        return iter(self.runs)
########NEW FILE########
__FILENAME__ = mock_daemon
"""
 A mock daemon for testing service handling.
"""
import daemon
import sys
import time
from tron.trondaemon import PIDFile

def do_main_program():
    while True:
        print "ok"
        time.sleep(2)


if __name__ == "__main__":
    filename = sys.argv[1] if len(sys.argv) > 1 else None
    pidfile = pidfile=PIDFile(filename or '/tmp/mock_daemon.pid')
    with daemon.DaemonContext(
            pidfile=pidfile,
            files_preserve=[pidfile.lock.file]):
        do_main_program()

########NEW FILE########
__FILENAME__ = node_test
import mock
from testify import setup, TestCase, assert_equal, run
from testify import assert_in, assert_raises
from testify.assertions import assert_not_in, assert_not_equal
from testify import teardown, setup_teardown
from tests.testingutils import autospec_method

from tron import node, ssh, actioncommand
from tron.config import schema
from tron.core import actionrun
from tron.serialize import filehandler


def create_mock_node(name=None):
    mock_node = mock.create_autospec(node.Node)
    if name:
        mock_node.get_name.return_value = name
    return mock_node


def create_mock_pool():
    return mock.create_autospec(node.NodePool)


class NodePoolRepositoryTestCase(TestCase):

    @setup
    def setup_store(self):
        self.node = create_mock_node()
        self.repo = node.NodePoolRepository.get_instance()
        self.repo.add_node(self.node)

    @teardown
    def teardown_store(self):
        self.repo.clear()

    def test_single_instance(self):
        assert_raises(ValueError, node.NodePoolRepository)
        assert self.repo is node.NodePoolRepository.get_instance()

    def test_get_by_name(self):
        node_pool = self.repo.get_by_name(self.node.get_name())
        assert_equal(self.node, node_pool.next())

    def test_get_by_name_miss(self):
        assert_equal(None, self.repo.get_by_name('bogus'))

    def test_clear(self):
        self.repo.clear()
        assert_not_in(self.node, self.repo.nodes)
        assert_not_in(self.node, self.repo.pools)

    def test_update_from_config(self):
        mock_nodes = {'a': create_mock_node('a'), 'b': create_mock_node('b')}
        self.repo.nodes.update(mock_nodes)
        node_config = {'a': mock.Mock(), 'b': mock.Mock()}
        node_pool_config = {'c': mock.Mock(nodes=['a', 'b'])}
        ssh_options = mock.Mock(identities=[], known_hosts_file=None)
        node.NodePoolRepository.update_from_config(
            node_config, node_pool_config, ssh_options)
        node_names = [node_config['a'].name, node_config['b'].name]
        assert_equal(set(self.repo.pools), set(node_names + [node_pool_config['c'].name]))
        assert_equal(set(self.repo.nodes), set(node_names + mock_nodes.keys()))

    def test_nodes_by_name(self):
        mock_nodes = {'a': mock.Mock(), 'b': mock.Mock()}
        self.repo.nodes.update(mock_nodes)
        nodes = self.repo._get_nodes_by_name(['a', 'b'])
        assert_equal(nodes, mock_nodes.values())

    def test_get_node(self):
        returned_node = self.repo.get_node(self.node.get_name())
        assert_equal(returned_node, self.node)


class KnownHostTestCase(TestCase):

    @setup
    def setup_known_hosts(self):
        self.known_hosts = node.KnownHosts(None)
        self.entry = mock.Mock()
        self.known_hosts._entries.append(self.entry)

    def test_get_public_key(self):
        hostname = 'hostname'
        pub_key = self.known_hosts.get_public_key(hostname)
        self.entry.matchesHost.assert_called_with(hostname)
        assert_equal(pub_key, self.entry.publicKey)

    def test_get_public_key_not_found(self):
        self.entry.matchesHost.return_value = False
        assert not self.known_hosts.get_public_key('hostname')


class DetermineJitterTestCase(TestCase):

    @setup
    def setup_node_settings(self):
        self.settings = mock.Mock(
                jitter_load_factor=1, jitter_min_load=4, jitter_max_delay=20)

    @setup_teardown
    def patch_random(self):
        with mock.patch('tron.node.random', autospec=True) as mock_random:
            mock_random.random.return_value = 1
            yield

    def test_jitter_under_min_load(self):
        assert_equal(node.determine_jitter(3, self.settings), 0)
        assert_equal(node.determine_jitter(4, self.settings), 0)

    def test_jitter_with_load_factor(self):
        self.settings.jitter_load_factor = 2
        assert_equal(node.determine_jitter(3, self.settings), 2.0)
        assert_equal(node.determine_jitter(2, self.settings), 0)

    def test_jitter_with_max_delay(self):
        self.settings.jitter_max_delay = 15
        assert_equal(node.determine_jitter(20, self.settings), 15.0)
        assert_equal(node.determine_jitter(100, self.settings), 15.0)


def build_node(
        hostname='localhost', username='theuser', name='thename', pub_key=None):
    config = mock.Mock(hostname=hostname, username=username, name=name)
    ssh_opts = mock.create_autospec(ssh.SSHAuthOptions)
    node_settings = mock.create_autospec(schema.ConfigSSHOptions)
    return node.Node(config, ssh_opts, pub_key, node_settings)


class NodeTestCase(TestCase):

    class TestConnection(object):
        def openChannel(self, chan):
            self.chan = chan

    @setup
    def setup_node(self):
        self.node = build_node()

    def test_output_logging(self):
        test_node = build_node()
        serializer = mock.create_autospec(filehandler.FileHandleManager)
        action_cmd = actionrun.ActionCommand("test", "false", serializer)

        test_node.connection = self.TestConnection()
        test_node.run_states = {action_cmd.id: mock.Mock(state=0)}
        test_node.run_states[action_cmd.id].state = node.RUN_STATE_CONNECTING

        test_node._open_channel(action_cmd)
        assert test_node.connection.chan is not None
        test_node.connection.chan.dataReceived("test")
        serializer.open.return_value.write.assert_called_with('test')

    def test_from_config(self):
        ssh_options = self.node.conch_options
        node_config = mock.Mock(hostname='localhost', username='theuser', name='thename')
        ssh_options.__getitem__.return_value = 'something'
        public_key = mock.Mock()
        node_settings = mock.Mock()
        new_node = node.Node.from_config(
                node_config, ssh_options, public_key, node_settings)
        assert_equal(new_node.name, node_config.name)
        assert_equal(new_node.hostname, node_config.hostname)
        assert_equal(new_node.username, node_config.username)
        assert_equal(new_node.pub_key, public_key)
        assert_equal(new_node.node_settings, node_settings)

    def test__eq__true(self):
        other_node = build_node()
        other_node.conch_options = self.node.conch_options
        other_node.node_settings = self.node.node_settings
        other_node.config = self.node.config
        assert_equal(other_node, self.node)

    def test__eq__false_config_changed(self):
        other_node = build_node(username='different')
        assert_not_equal(other_node, self.node)

    def test__eq__false_pub_key_changed(self):
        other_node = build_node(pub_key='something')
        assert_not_equal(other_node, self.node)

    def test__eq__false_ssh_options_changed(self):
        other_node = build_node()
        other_node.conch_options = mock.create_autospec(ssh.SSHAuthOptions)
        assert_not_equal(other_node, self.node)

    def test_stop_not_tracked(self):
        action_command = mock.create_autospec(actioncommand.ActionCommand,
            id=mock.Mock())
        self.node.stop(action_command)

    def test_stop(self):
        autospec_method(self.node._fail_run)
        action_command = mock.create_autospec(actioncommand.ActionCommand,
            id=mock.Mock())
        self.node.run_states[action_command.id] = mock.Mock()
        self.node.stop(action_command)
        assert_equal(self.node._fail_run.call_count, 1)


class NodePoolTestCase(TestCase):

    @setup
    def setup_nodes(self):
        self.nodes = [build_node(name='node%s' % i) for i in xrange(5)]
        self.node_pool = node.NodePool(self.nodes, 'thename')

    def test_from_config(self):
        name = 'the pool name'
        nodes = [create_mock_node(), create_mock_node()]
        config = mock.Mock(name=name)
        new_pool = node.NodePool.from_config(config, nodes)
        assert_equal(new_pool.name, config.name)
        assert_equal(new_pool.nodes, nodes)

    def test__init__(self):
        new_node = node.NodePool(self.nodes, 'thename')
        assert_equal(new_node.name, 'thename')

    def test__eq__(self):
        other_pool = node.NodePool(self.nodes, 'othername')
        assert_equal(self.node_pool, other_pool)

    def test_next(self):
        # Call next many times
        for _ in xrange(len(self.nodes) * 2 + 1):
            assert_in(self.node_pool.next(), self.nodes)

    def test_next_round_robin(self):
        node_order = [
            self.node_pool.next_round_robin()
            for _ in xrange(len(self.nodes) * 2)
        ]
        assert_equal(node_order, self.nodes + self.nodes)


if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = sandbox
import logging
import os
import shutil
import signal
import socket
from subprocess import Popen, PIPE, CalledProcessError
import sys
import tempfile
import time
import contextlib
import functools
import mock

from testify import TestCase, setup, teardown
from testify.assertions import assert_not_equal

from tron.commands import client
from tron.config import manager, schema


# Used for getting the locations of the executable
test_dir, _ = os.path.split(__file__)
repo_root, _   = os.path.split(test_dir)

log = logging.getLogger(__name__)


def wait_on_sandbox(func, delay=0.1, max_wait=5.0):
    """Poll for func() to return True. Sleeps `delay` seconds between polls
    up to a max of `max_wait` seconds.
    """
    start_time = time.time()
    while time.time() - start_time < max_wait:
        time.sleep(delay)
        if func():
            return
    raise TronSandboxException("Failed %s" % func.__name__)


def wait_on_state(client_func, url, state, field='state'):
    """Use client_func(url) to wait until the resource changes to state."""
    def wait_func():
        return client_func(url)[field] == state
    wait_func.__name__ = '%s wait on %s' % (url, state)
    wait_on_sandbox(wait_func)


def wait_on_proc_terminate(pid):
    def wait_on_terminate():
        try:
            os.kill(pid, 0)
        except:
            return True
    wait_on_terminate.__name__ = "Wait on %s to terminate" % pid
    wait_on_sandbox(wait_on_terminate)


def build_waiter_func(client_func, url):
    return functools.partial(wait_on_state, client_func, url)


def handle_output(cmd, (stdout, stderr), returncode):
    """Log process output before it is parsed. Raise exception if exit code
    is nonzero.
    """
    cmd = ' '.join(cmd)
    if stdout:
        log.warn("%s STDOUT: %s", cmd, stdout)
    if stderr:
        log.warn("%s STDERR: %s", cmd, stderr)
    if returncode:
        raise CalledProcessError(returncode, cmd)


def find_unused_port():
    """Return a port number that is not in use."""
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    with contextlib.closing(sock) as sock:
        sock.bind(('localhost', 0))
        _, port = sock.getsockname()
    return port


class TronSandboxException(Exception):
    pass


class SandboxTestCase(TestCase):

    _suites = ['sandbox']

    sandbox = None

    @setup
    def make_sandbox(self):
        verify_environment()
        self.sandbox = TronSandbox()
        self.client = self.sandbox.client

    @teardown
    def delete_sandbox(self):
        if self.sandbox:
            self.sandbox.delete()
            self.sandbox = None

    def start_with_config(self, config):
        self.sandbox.save_config(config)
        self.sandbox.trond()

    def restart_trond(self):
        old_pid = self.sandbox.get_trond_pid()
        self.sandbox.shutdown_trond()
        wait_on_proc_terminate(self.sandbox.get_trond_pid())

        self.sandbox.trond()
        assert_not_equal(old_pid, self.sandbox.get_trond_pid())


class ClientProxy(object):
    """Wrap calls to client and raise a TronSandboxException on connection
    failures.
    """

    def __init__(self, client, log_filename):
        self.client         = client
        self.log_filename   = log_filename

    def log_contents(self):
        """Return the contents of the log file."""
        with open(self.log_filename, 'r') as f:
            return f.read()

    def wrap(self, func, *args, **kwargs):
        with mock.patch('tron.commands.client.log'):
            try:
                return func(*args, **kwargs)
            except (client.RequestError, ValueError), e:
                # ValueError for JSONDecode errors
                log_contents = self.log_contents()
                if log_contents:
                    log.warn("%r, Log:\n%s" % (e, log_contents))
                return False

    def __getattr__(self, name):
        attr = getattr(self.client, name)
        if not callable(attr):
            return attr

        return functools.partial(self.wrap, attr)


def verify_environment():
    for env_var in ['SSH_AUTH_SOCK', 'PYTHONPATH']:
        if not os.environ.get(env_var):
            raise TronSandboxException("Missing $%s in test environment." % env_var)


class TronSandbox(object):
    """A sandbox for running trond and tron commands in subprocesses."""

    def __init__(self):
        """Set up a temp directory and store paths to relevant binaries"""
        self.tmp_dir        = tempfile.mkdtemp(prefix='tron-')
        cmd_path_func       = functools.partial(os.path.join, repo_root, 'bin')
        cmds                = 'tronctl', 'trond', 'tronfig', 'tronview'
        self.commands       = dict((cmd, cmd_path_func(cmd)) for cmd in cmds)
        self.log_file       = self.abs_path('tron.log')
        self.log_conf       = self.abs_path('logging.conf')
        self.pid_file       = self.abs_path('tron.pid')
        self.config_path    = self.abs_path('configs/')
        self.port           = find_unused_port()
        self.host           = 'localhost'
        self.api_uri        = 'http://%s:%s' % (self.host, self.port)
        cclient             = client.Client(self.api_uri)
        self.client         = ClientProxy(cclient, self.log_file)
        self.setup_logging_conf()

    def abs_path(self, filename):
        """Return the absolute path for a file in the sandbox."""
        return os.path.join(self.tmp_dir, filename)

    def setup_logging_conf(self):
        config_template = os.path.join(repo_root, 'tests/data/logging.conf')
        with open(config_template, 'r') as fh:
            config = fh.read()

        with open(self.log_conf, 'w') as fh:
            fh.write(config.format(self.log_file))

    def delete(self):
        """Delete the temp directory and shutdown trond."""
        self.shutdown_trond(sig_num=signal.SIGKILL)
        shutil.rmtree(self.tmp_dir)

    def save_config(self, config_text):
        """Save the initial tron configuration."""
        manager.create_new_config(self.config_path, config_text)

    def run_command(self, command_name, args=None, stdin_lines=None):
        """Run the command by name and return (stdout, stderr)."""
        args        = args or []
        command     = [sys.executable, self.commands[command_name]] + args
        stdin       = PIPE if stdin_lines else None
        proc        = Popen(command, stdout=PIPE, stderr=PIPE, stdin=stdin)
        streams     = proc.communicate(stdin_lines)
        try:
            handle_output(command, streams, proc.returncode)
        except CalledProcessError:
            log.warn(self.client.log_contents())
            raise
        return streams

    def tronctl(self, *args):
        args = list(args) if args else []
        return self.run_command('tronctl', args + ['--server', self.api_uri])

    def tronview(self, *args):
        args = list(args) if args else []
        args += ['--nocolor', '--server', self.api_uri]
        return self.run_command('tronview', args)

    def trond(self, *args):
        args = list(args) if args else []
        args += ['--working-dir=%s'     % self.tmp_dir,
                   '--pid-file=%s'      % self.pid_file,
                   '--port=%d'          % self.port,
                   '--host=%s'          % self.host,
                   '--config-path=%s'   % self.config_path,
                   '--log-conf=%s'      % self.log_conf]

        self.run_command('trond', args)
        wait_on_sandbox(lambda: bool(self.client.home()))

    def tronfig(self,
                config_content=None,
                name=schema.MASTER_NAMESPACE,
                no_header=False):
        args = ['--server', self.api_uri, name]
        if no_header:
            args += ['--no-header']
        args += ['-'] if config_content else ['-p']
        return self.run_command('tronfig', args, stdin_lines=config_content)

    def get_trond_pid(self):
        if not os.path.exists(self.pid_file):
            return None
        with open(self.pid_file, 'r') as f:
            return int(f.read())

    def shutdown_trond(self, sig_num=signal.SIGTERM):
        trond_pid = self.get_trond_pid()
        if trond_pid:
            os.kill(trond_pid, sig_num)

########NEW FILE########
__FILENAME__ = scheduler_test
import calendar
import datetime
import mock
import pytz

from testify import setup, run, assert_equal, TestCase
from testify import assert_gte, assert_lte, assert_gt, assert_lt
from testify import setup_teardown
from tests import testingutils

from tron import scheduler
from tron.config import schedule_parse, config_utils
from tron.config.config_utils import NullConfigContext
from tron.config.schedule_parse import parse_groc_expression
from tron.utils import timeutils


class SchedulerFromConfigTestCase(TestCase):

    def test_cron_scheduler(self):
        line = "cron */5 * * 7,8 *"
        config_context = mock.Mock(path='test')
        config = schedule_parse.valid_schedule(line, config_context)
        sched = scheduler.scheduler_from_config(config, mock.Mock())
        start_time = datetime.datetime(2012, 3, 14, 15, 9, 26)
        next_time = sched.next_run_time(start_time)
        assert_equal(next_time, datetime.datetime(2012, 7, 1, 0))
        assert_equal(str(sched), "cron */5 * * 7,8 *")

    def test_daily_scheduler(self):
        config_context = config_utils.NullConfigContext
        line = "daily 17:32 MWF"
        config = schedule_parse.valid_schedule(line, config_context)
        sched = scheduler.scheduler_from_config(config, mock.Mock())
        assert_equal(sched.time_spec.hours, [17])
        assert_equal(sched.time_spec.minutes, [32])
        start_time = datetime.datetime(2012, 3, 14, 15, 9, 26)
        for day in [14, 16, 19]:
            next_time = sched.next_run_time(start_time)
            assert_equal(next_time, datetime.datetime(2012, 3, day, 17, 32))
            start_time = next_time

        assert_equal(str(sched), "daily 17:32 MWF")


class ConstantSchedulerTest(testingutils.MockTimeTestCase):

    now = datetime.datetime(2012, 3, 14)

    @setup
    def build_scheduler(self):
        self.scheduler = scheduler.ConstantScheduler()

    def test_next_run_time(self):
        scheduled_time = self.scheduler.next_run_time(None)
        assert_equal(scheduled_time, self.now)

    def test__str__(self):
        assert_equal(str(self.scheduler), 'constant')


class GeneralSchedulerTestCase(testingutils.MockTimeTestCase):

    now = datetime.datetime.now().replace(hour=15, minute=0)

    def expected_time(self, date):
        return datetime.datetime.combine(date, datetime.time(14, 30))

    @setup
    def build_scheduler(self):
        self.scheduler = scheduler.GeneralScheduler(timestr='14:30')
        one_day = datetime.timedelta(days=1)
        self.today = self.now.date()
        self.yesterday = self.now - one_day
        self.tomorrow = self.now + one_day

    def test_next_run_time(self):
        next_run = self.scheduler.next_run_time(timeutils.current_time())
        assert_equal(self.expected_time(self.tomorrow), next_run)

        next_run = self.scheduler.next_run_time(self.yesterday)
        assert_equal(self.expected_time(self.today), next_run)

    @mock.patch('tron.scheduler.get_jitter', autospec=True)
    def test_next_run_time_with_jitter(self, mock_jitter):
        mock_jitter.return_value = delta = datetime.timedelta(seconds=-300)
        self.scheduler.jitter = datetime.timedelta(seconds=400)
        expected = self.expected_time(self.tomorrow) + delta
        next_run_time = self.scheduler.next_run_time(None)
        assert_equal(next_run_time, expected)

    def test__str__(self):
        assert_equal(str(self.scheduler), "daily ")

    def test__str__with_jitter(self):
        self.scheduler.jitter = datetime.timedelta(seconds=300)
        assert_equal(str(self.scheduler), "daily  (+/- 0:05:00)")


class GeneralSchedulerTimeTestBase(testingutils.MockTimeTestCase):

    now = datetime.datetime(2012, 3, 14, 15, 9, 26)

    @setup
    def build_scheduler(self):
        self.scheduler = scheduler.GeneralScheduler(timestr='14:30')


class GeneralSchedulerTodayTest(GeneralSchedulerTimeTestBase):

    now = datetime.datetime.now().replace(hour=12, minute=0)

    def test(self):
        # If we schedule a job for later today, it should run today
        run_time = self.scheduler.next_run_time(self.now)
        next_run_date = run_time.date()

        assert_equal(next_run_date, self.now.date())
        earlier_time = datetime.datetime(
            self.now.year, self.now.month, self.now.day, hour=13)
        assert_lte(earlier_time, run_time)


class GeneralSchedulerTomorrowTest(GeneralSchedulerTimeTestBase):

    now = datetime.datetime.now().replace(hour=15, minute=0)

    def test(self):
        # If we schedule a job for later today, it should run today
        run_time = self.scheduler.next_run_time(self.now)
        next_run_date = run_time.date()
        tomorrow = self.now.date() + datetime.timedelta(days=1)

        assert_equal(next_run_date, tomorrow)
        earlier_time = datetime.datetime(year=tomorrow.year, month=tomorrow.month,
            day=tomorrow.day, hour=13)
        assert_lte(earlier_time, run_time)


class GeneralSchedulerLongJobRunTest(GeneralSchedulerTimeTestBase):

    now = datetime.datetime.now().replace(hour=12, minute=0)

    def test_long_jobs_dont_wedge_scheduler(self):
        # Advance days twice as fast as they are scheduled, demonstrating
        # that the scheduler will put things in the past if that's where
        # they belong, and run them as fast as possible

        last_run = self.scheduler.next_run_time(None)
        for i in range(10):
            next_run = self.scheduler.next_run_time(last_run)
            assert_equal(next_run, last_run + datetime.timedelta(days=1))

            self.now += datetime.timedelta(days=2)
            last_run = next_run


class GeneralSchedulerDSTTest(testingutils.MockTimeTestCase):

    now = datetime.datetime(2011, 11, 6, 1, 10, 0)

    def hours_until_time(self, run_time, sch):
        tz = sch.time_zone
        now = timeutils.current_time()
        now = tz.localize(now) if tz else now
        seconds = timeutils.delta_total_seconds(run_time - now)
        return round(max(0, seconds) / 60 / 60, 1)

    def hours_diff_at_datetime(self, sch, *args, **kwargs):
        """Return the number of hours until the next *two* runs of a job with
        the given scheduler
        """
        self.now = datetime.datetime(*args, **kwargs)
        next_run = sch.next_run_time(self.now)
        t1 = self.hours_until_time(next_run, sch)
        next_run = sch.next_run_time(next_run.replace(tzinfo=None))
        t2 = self.hours_until_time(next_run, sch)
        return t1, t2

    def _assert_range(self, x, lower, upper):
        assert_gt(x, lower)
        assert_lt(x, upper)

    def test_fall_back(self):
        """This test checks the behavior of the scheduler at the daylight
        savings time 'fall back' point, when the system time zone changes
        from (e.g.) PDT to PST.
        """
        sch = scheduler.GeneralScheduler(time_zone=pytz.timezone('US/Pacific'))

        # Exact crossover time:
        # datetime.datetime(2011, 11, 6, 9, 0, 0, tzinfo=pytz.utc)
        # This test will use times on either side of it.

        # From the PDT vantage point, the run time is 24.2 hours away:
        s1a, s1b = self.hours_diff_at_datetime(sch, 2011, 11, 6, 0, 50, 0)

        # From the PST vantage point, the run time is 22.8 hours away:
        # (this is measured from the point in absolute time 20 minutes after
        # the other measurement)
        s2a, s2b = self.hours_diff_at_datetime(sch, 2011, 11, 6, 1, 10, 0)

        self._assert_range(s1b - s1a, 23.99, 24.11)
        self._assert_range(s2b - s2a, 23.99, 24.11)
        self._assert_range(s1a - s2a, 1.39, 1.41)

    def test_correct_time(self):
        sch = scheduler.GeneralScheduler(time_zone=pytz.timezone('US/Pacific'))
        next_run_time = sch.next_run_time(self.now)
        assert_equal(next_run_time.hour, 0)

    def test_spring_forward(self):
        """This test checks the behavior of the scheduler at the daylight
        savings time 'spring forward' point, when the system time zone changes
        from (e.g.) PST to PDT.
        """
        sch = scheduler.GeneralScheduler(time_zone=pytz.timezone('US/Pacific'))

        # Exact crossover time:
        # datetime.datetime(2011, 3, 13, 2, 0, 0, tzinfo=pytz.utc)
        # This test will use times on either side of it.

        # From the PST vantage point, the run time is 20.2 hours away:
        s1a, s1b = self.hours_diff_at_datetime(sch, 2011, 3, 13, 2, 50, 0)

        # From the PDT vantage point, the run time is 20.8 hours away:
        # (this is measured from the point in absolute time 20 minutes after
        # the other measurement)
        s2a, s2b = self.hours_diff_at_datetime(sch, 2011, 3, 13, 3, 10, 0)

        self._assert_range(s1b - s1a, 23.99, 24.11)
        self._assert_range(s2b - s2a, 23.99, 24.11)
        self._assert_range(s1a - s2a, -0.61, -0.59)


def parse_groc(config):
    config = schedule_parse.ConfigGenericSchedule('groc daily', config, None)
    return parse_groc_expression(config, NullConfigContext)


def scheduler_from_config(config):
    return scheduler.scheduler_from_config(parse_groc(config), None)


class ComplexParserTest(testingutils.MockTimeTestCase):

    now = datetime.datetime(2011, 6, 1)

    def test_parse_all(self):
        config_string = '1st,2nd,3rd,4th monday,Tue of march,apr,September at 00:00'
        cfg = parse_groc(config_string)
        assert_equal(cfg.ordinals, set((1, 2, 3, 4)))
        assert_equal(cfg.monthdays, None)
        assert_equal(cfg.weekdays, set((1, 2)))
        assert_equal(cfg.months, set((3, 4, 9)))
        assert_equal(cfg.timestr, '00:00')
        assert_equal(scheduler_from_config(config_string),
                     scheduler_from_config(config_string))

    def test_parse_no_weekday(self):
        cfg = parse_groc('1st,2nd,3rd,10th day of march,apr,September at 00:00')
        assert_equal(cfg.ordinals, None)
        assert_equal(cfg.monthdays, set((1,2,3,10)))
        assert_equal(cfg.weekdays, None)
        assert_equal(cfg.months, set((3, 4, 9)))
        assert_equal(cfg.timestr, '00:00')

    def test_parse_no_month(self):
        cfg = parse_groc('1st,2nd,3rd,10th day at 00:00')
        assert_equal(cfg.ordinals, None)
        assert_equal(cfg.monthdays, set((1,2,3,10)))
        assert_equal(cfg.weekdays, None)
        assert_equal(cfg.months, None)
        assert_equal(cfg.timestr, '00:00')

    def test_parse_monthly(self):
        for test_str in ('1st day', '1st day of month'):
            cfg = parse_groc(test_str)
            assert_equal(cfg.ordinals, None)
            assert_equal(cfg.monthdays, set([1]))
            assert_equal(cfg.weekdays, None)
            assert_equal(cfg.months, None)
            assert_equal(cfg.timestr, '00:00')

    def test_wildcards(self):
        cfg = parse_groc('every day')
        assert_equal(cfg.ordinals, None)
        assert_equal(cfg.monthdays, None)
        assert_equal(cfg.weekdays, None)
        assert_equal(cfg.months, None)
        assert_equal(cfg.timestr, '00:00')

    def test_daily(self):
        sch = scheduler_from_config('every day')
        next_run_date = sch.next_run_time(None)

        assert_gte(next_run_date, self.now)
        assert_equal(next_run_date.month, 6)
        assert_equal(next_run_date.day, 2)
        assert_equal(next_run_date.hour, 0)

    def test_daily_with_time(self):
        sch = scheduler_from_config('every day at 02:00')
        next_run_date = sch.next_run_time(None)

        assert_gte(next_run_date, self.now)
        assert_equal(next_run_date.year, self.now.year)
        assert_equal(next_run_date.month, 6)
        assert_equal(next_run_date.day, 1)
        assert_equal(next_run_date.hour, 2)
        assert_equal(next_run_date.minute, 0)

    def test_weekly(self):
        sch = scheduler_from_config('every monday at 01:00')
        next_run_date = sch.next_run_time(None)

        assert_gte(next_run_date, self.now)
        assert_equal(calendar.weekday(next_run_date.year,
                                      next_run_date.month,
                                      next_run_date.day), 0)

    def test_weekly_in_month(self):
        sch = scheduler_from_config('every monday of january at 00:01')
        next_run_date = sch.next_run_time(None)

        assert_gte(next_run_date, self.now)
        assert_equal(next_run_date.year, self.now.year+1)
        assert_equal(next_run_date.month, 1)
        assert_equal(next_run_date.hour, 0)
        assert_equal(next_run_date.minute, 1)
        assert_equal(calendar.weekday(next_run_date.year,
                                      next_run_date.month,
                                      next_run_date.day), 0)

    def test_monthly(self):
        sch = scheduler_from_config('1st day')
        next_run_date = sch.next_run_time(None)

        assert_gt(next_run_date, self.now)
        assert_equal(next_run_date.month, 7)


class IntervalSchedulerTestCase(TestCase):

    now = datetime.datetime(2012, 3, 14)

    @setup_teardown
    def patch_time(self):
        with mock.patch('tron.scheduler.timeutils.current_time') as self.mock_now:
            self.mock_now.return_value = self.now
            yield

    @setup
    def build_scheduler(self):
        self.seconds = 7
        self.interval = datetime.timedelta(seconds=self.seconds)
        self.scheduler = scheduler.IntervalScheduler(self.interval, None)

    def test_next_run_time_no_jitter(self):
        prev_run_time = datetime.datetime(2011, 5, 21)
        run_time = self.scheduler.next_run_time(prev_run_time)
        assert_equal(prev_run_time + self.interval, run_time)

    def test_next_run_time_no_last_run_time_no_jitter(self):
        run_time = self.scheduler.next_run_time(None)
        assert_equal(self.now + self.interval, run_time)

    @mock.patch('tron.scheduler.random', autospec=True)
    def test_next_run_time_with_jitter(self, mock_random):
        jitter = datetime.timedelta(seconds=234)
        mock_random.randint.return_value = random_jitter = -200
        random_delta = datetime.timedelta(seconds=random_jitter)
        prev_run_time = datetime.datetime(2011, 5, 21)
        interval_sched = scheduler.IntervalScheduler(self.interval, jitter)
        run_time = interval_sched.next_run_time(prev_run_time)
        assert_equal(run_time, prev_run_time + random_delta + self.interval)

    def test__str__(self):
        assert_equal(str(self.scheduler), "interval %s" % self.interval)

    def test__str__with_jitter(self):
        self.scheduler.jitter = datetime.timedelta(seconds=300)
        assert_equal(str(self.scheduler), "interval 0:00:07 (+/- 0:05:00)")


if __name__ == '__main__':
    run()

########NEW FILE########
__FILENAME__ = filehandler_test
import os
import shutil
import time
from tempfile import NamedTemporaryFile, mkdtemp

from testify import TestCase, run, assert_equal, assert_not_in, assert_in
from testify import assert_not_equal, turtle
from testify import setup, teardown, suite

from tron.serialize.filehandler import FileHandleManager, OutputStreamSerializer
from tron.serialize.filehandler import OutputPath, NullFileHandle

class FileHandleWrapperTestCase(TestCase):

    @setup
    def setup_fh_wrapper(self):
        self.file = NamedTemporaryFile('r')
        self.manager = FileHandleManager.get_instance()
        self.fh_wrapper = self.manager.open(self.file.name)

    @teardown
    def teardown_fh_wrapper(self):
        self.fh_wrapper.close()
        FileHandleManager.reset()

    def test_init(self):
        assert_equal(self.fh_wrapper._fh, NullFileHandle)

    def test_close(self):
        # Test close without a write, no exception is good
        self.fh_wrapper.close()
        # Test close again, after already closed
        self.fh_wrapper.close()

    def test_close_with_write(self):
        # Test close with a write
        self.fh_wrapper.write("some things")
        self.fh_wrapper.close()
        assert_equal(self.fh_wrapper._fh, NullFileHandle)
        assert_equal(self.fh_wrapper.manager, self.manager)
        # This is somewhat coupled
        assert_not_in(self.fh_wrapper, self.manager.cache)

    def test_write(self):
        # Test write without a previous open
        before_time = time.time()
        self.fh_wrapper.write("some things")
        after_time = time.time()

        assert self.fh_wrapper._fh
        assert_equal(self.fh_wrapper._fh.closed, False)
        assert before_time <= self.fh_wrapper.last_accessed <= after_time

        # Test write after previous open
        before_time = time.time()
        self.fh_wrapper.write("\nmore things")
        after_time = time.time()
        assert before_time <= self.fh_wrapper.last_accessed <= after_time
        self.fh_wrapper.close()
        with open(self.file.name) as fh:
            assert_equal(fh.read(), "some things\nmore things")

    def test_close_many(self):
        self.fh_wrapper.write("some things")
        self.fh_wrapper.close()
        self.fh_wrapper.close()

    def test_context_manager(self):
        with self.fh_wrapper as fh:
            fh.write("123")
        assert fh._fh.closed
        with open(self.file.name) as fh:
            assert_equal(fh.read(), "123")


class FileHandleManagerTestCase(TestCase):

    @setup
    def setup_fh_manager(self):
        FileHandleManager.reset()
        self.file1 = NamedTemporaryFile('r')
        self.file2 = NamedTemporaryFile('r')
        FileHandleManager.set_max_idle_time(2)
        self.manager = FileHandleManager.get_instance()

    @teardown
    def teardown_fh_manager(self):
        FileHandleManager.reset()

    def test_get_instance(self):
        assert_equal(self.manager, FileHandleManager.get_instance())
        # Repeat for good measure
        assert_equal(self.manager, FileHandleManager.get_instance())

    def test_set_max_idle_time(self):
        max_idle_time = 300
        FileHandleManager.set_max_idle_time(max_idle_time)
        assert_equal(max_idle_time, self.manager.max_idle_time)

    def test_open(self):
        # Not yet in cache
        fh_wrapper = self.manager.open(self.file1.name)
        assert_in(fh_wrapper.name, self.manager.cache)

        # Should now be in cache
        fh_wrapper2 = self.manager.open(self.file1.name)

        # Same wrapper
        assert_equal(fh_wrapper, fh_wrapper2)

        # Different wrapper
        assert_not_equal(fh_wrapper, self.manager.open(self.file2.name))

    def test_cleanup_none(self):
        # Nothing to remove
        fh_wrapper = self.manager.open(self.file1.name)
        self.manager.cleanup()
        assert_in(fh_wrapper.name, self.manager.cache)

    def test_cleanup_single(self):
        fh_wrapper = self.manager.open(self.file1.name)
        fh_wrapper.last_accessed = 123456
        time_func = lambda: 123458.1
        self.manager.cleanup(time_func)
        assert_not_in(fh_wrapper.name, self.manager.cache)
        assert_equal(len(self.manager.cache), 0)

    def test_cleanup_many(self):
        fh_wrappers = [
            self.manager.open(self.file1.name),
            self.manager.open(self.file2.name),
            self.manager.open(NamedTemporaryFile('r').name),
            self.manager.open(NamedTemporaryFile('r').name),
            self.manager.open(NamedTemporaryFile('r').name),
        ]
        for i, fh_wrapper in enumerate(fh_wrappers):
            fh_wrapper.last_accessed = 123456 + i

        time_func = lambda: 123460.1
        self.manager.cleanup(time_func)
        assert_equal(len(self.manager.cache), 2)

        for fh_wrapper in fh_wrappers[:3]:
            assert_not_in(fh_wrapper.name, self.manager.cache)

        for fh_wrapper in fh_wrappers[3:]:
            assert_in(fh_wrapper.name, self.manager.cache)

    def test_cleanup_opened(self):
        fh_wrapper = self.manager.open(self.file1.name)
        fh_wrapper.write("Some things")

        fh_wrapper.last_accessed = 123456
        time_func = lambda: 123458.1
        self.manager.cleanup(time_func)
        assert_not_in(fh_wrapper.name, self.manager.cache)
        assert_equal(len(self.manager.cache), 0)

    def test_cleanup_natural(self):
        FileHandleManager.set_max_idle_time(1)
        fh_wrapper1 = self.manager.open(self.file1.name)
        fh_wrapper2 = self.manager.open(self.file2.name)
        fh_wrapper1.write("Some things")

        time.sleep(1.5)
        fh_wrapper2.write("Other things.")

        assert_not_in(fh_wrapper1.name, self.manager.cache)
        assert_in(fh_wrapper2.name, self.manager.cache)

        # Now that 1 is closed, try writing again
        fh_wrapper1.write("Some things")
        assert_in(fh_wrapper1.name, self.manager.cache)
        assert not fh_wrapper1._fh.closed

    def test_remove(self):
        # In cache
        fh_wrapper = self.manager.open(self.file1.name)
        assert_in(fh_wrapper.name, self.manager.cache)
        self.manager.remove(fh_wrapper)
        assert_not_in(fh_wrapper.name, self.manager.cache)

        # Not in cache
        self.manager.remove(fh_wrapper)
        assert_not_in(fh_wrapper.name, self.manager.cache)

    def test_update(self):
        fh_wrapper1 = self.manager.open(self.file1.name)
        fh_wrapper2 = self.manager.open(self.file2.name)
        assert_equal(self.manager.cache.keys(), [fh_wrapper1.name, fh_wrapper2.name])

        self.manager.update(fh_wrapper1)
        assert_equal(self.manager.cache.keys(), [fh_wrapper2.name, fh_wrapper1.name])


class OutputStreamSerializerTestCase(TestCase):

    @setup
    def setup_serializer(self):
        self.test_dir = mkdtemp()
        self.serial = OutputStreamSerializer([self.test_dir])
        self.filename = "STARS"
        self.content = "123\n456\n789"
        self.expected = self.content.split('\n')

    @teardown
    def teardown_test_dir(self):
        shutil.rmtree(self.test_dir)

    def _write_contents(self):
        with open(self.serial.full_path(self.filename), 'w') as f:
            f.write(self.content)

    def test_open(self):
        with self.serial.open(self.filename) as fh:
            fh.write(self.content)

        with open(self.serial.full_path(self.filename)) as f:
            assert_equal(f.read(), self.content)

    @suite('integration')
    def test_init_with_output_path(self):
        path = OutputPath(self.test_dir, 'one', 'two', 'three')
        stream = OutputStreamSerializer(path)
        assert_equal(stream.base_path, str(path))

    def test_tail(self):
        self._write_contents()
        assert_equal(self.serial.tail(self.filename), self.expected)

    def test_tail_num_lines(self):
        self._write_contents()
        assert_equal(self.serial.tail(self.filename, 1), self.expected[-1:])

    def test_tail_file_does_not_exist(self):
        file_dne = 'bogusfile123'
        assert_equal(self.serial.tail(file_dne), [])


class OutputPathTestCase(TestCase):

    @setup
    def setup_path(self):
        self.path = OutputPath('one', 'two', 'three')

    def test__init__(self):
        assert_equal(self.path.base, 'one')
        assert_equal(self.path.parts, ['two', 'three'])

        path = OutputPath('base')
        assert_equal(path.base, 'base')
        assert_equal(path.parts, [])

    def test__iter__(self):
        assert_equal(list(self.path), ['one', 'two', 'three'])

    def test__str__(self):
        # Breaks in windows probably,
        assert_equal('one/two/three', str(self.path))

    def test_append(self):
        self.path.append('four')
        assert_equal(self.path.parts, ['two', 'three', 'four'])

    def test_clone(self):
        new_path = self.path.clone()
        assert_equal(str(new_path), str(self.path))

        self.path.append('alpha')
        assert_equal(str(new_path), 'one/two/three')

        new_path.append('beta')
        assert_equal(str(self.path), 'one/two/three/alpha')

    def test_clone_with_parts(self):
        new_path = self.path.clone('seven', 'ten')
        assert_equal(list(new_path), ['one/two/three', 'seven', 'ten'])

    def test_delete(self):
        tmp_dir = mkdtemp()
        path = OutputPath(tmp_dir)
        path.delete()
        assert not os.path.exists(tmp_dir)

    def test__eq__(self):
        other = turtle.Turtle(base='one', parts=['two', 'three'])
        assert_equal(self.path, other)

    def test__ne__(self):
        other = turtle.Turtle(base='one/two', parts=['three'])
        assert_not_equal(self.path, other)

if __name__ == "__main__":
    run()

########NEW FILE########
__FILENAME__ = mongostore_test
from testify import TestCase, run, setup, assert_equal, teardown
from tests import testingutils
from tron.serialize import runstate
mongostore = None # pyflakes


class MongoStateStoreTestCase(TestCase):
    _suites = ['mongodb']

    store = None

    @setup
    def setup_store(self):
        # Defer import
        from tron.serialize.runstate import mongostore
        global mongostore
        self.db_name = 'test_base'
        self.store = mongostore.MongoStateStore(self.db_name, None)

    @teardown
    def teardown_store(self):
        if self.store:
            # Clear out records
            self.store.connection.drop_database(self.db_name)
            self.store.cleanup()

    def _create_doc(self, key, doc):
        import pymongo
        db = pymongo.Connection()[self.db_name]
        doc['_id'] = key.key
        db[key.collection].save(doc)
        db.connection.disconnect()

    def test__init__(self):
        assert_equal(self.store.db_name, self.db_name)

    def test_connect(self):
        assert self.store.connection
        assert_equal(self.store.connection.host, 'localhost')
        assert_equal(self.store.db.name, self.db_name)

    def test_parse_connection_details(self):
        details = "hostname=mongoserver&port=55555"
        params = self.store._parse_connection_details(details)
        assert_equal(params, {'hostname': 'mongoserver', 'port': '55555'})

    def test_parse_connection_details_with_user_creds(self):
        details = "hostname=mongoserver&port=55555&username=ted&password=sam"
        params = self.store._parse_connection_details(details)
        expected = {
            'hostname': 'mongoserver',
            'port':     '55555',
            'username': 'ted',
            'password': 'sam'}
        assert_equal(params, expected)

    def test_parse_connection_details_none(self):
        params = self.store._parse_connection_details(None)
        assert_equal(params, {})

    def test_parse_connection_details_empty(self):
        params = self.store._parse_connection_details("")
        assert_equal(params, {})

    def test_build_key(self):
        key = self.store.build_key(runstate.JOB_STATE, 'stars')
        assert_equal(key.collection, self.store.JOB_COLLECTION)
        assert_equal(key.key, 'stars')

    def test_save(self):
        import pymongo
        doc0, doc1 = {'a':"Hey there"}, {'a': "Howsit"}
        key_value_pairs = [
            (mongostore.MongoStateKey(self.store.JOB_COLLECTION, "1"), doc0),
            (mongostore.MongoStateKey(self.store.SERVICE_COLLECTION, "2"), doc1)
        ]
        self.store.save(key_value_pairs)
        self.store.cleanup()

        @testingutils.retry()
        def verify():
            db = pymongo.Connection()[self.db_name]
            assert_equal(db[self.store.JOB_COLLECTION].find()[0], doc0)
            assert_equal(db[self.store.SERVICE_COLLECTION].find()[0], doc1)
        verify()

    def test_restore(self):
        keys = [
            mongostore.MongoStateKey(runstate.JOB_STATE, "1"),
            mongostore.MongoStateKey(runstate.SERVICE_STATE, "2")
        ]
        docs = [
            {'ahh': 'first doc'},
            {'bzz': 'second doc'}
        ]
        for i in xrange(2):
            self._create_doc(keys[i], docs[i])

        @testingutils.retry()
        def verify():
            restored_data = self.store.restore(keys)
            assert_equal(restored_data[keys[0]], docs[0])
            assert_equal(restored_data[keys[1]], docs[1])
        verify()

    def test_restore_not_found(self):
        keys = [mongostore.MongoStateKey(runstate.JOB_STATE, "1")]
        restored_data = self.store.restore(keys)
        assert_equal(restored_data, {})

    def test_restore_partial(self):
        keys = [
            mongostore.MongoStateKey(runstate.JOB_STATE, "1"),
            mongostore.MongoStateKey(runstate.SERVICE_STATE, "2")
        ]
        docs = [{'ahh': 'first doc'}]
        self._create_doc(keys[0], docs[0])

        @testingutils.retry()
        def verify():
            restored_data = self.store.restore(keys)
            assert_equal(restored_data[keys[0]], docs[0])
        verify()

    def test_cleanup(self):
        self.store.cleanup()
        assert not self.store.connection.host
        assert not self.store.connection.port


if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = shelvestore_test
import os
import shelve
import tempfile
from testify import TestCase, run, setup, assert_equal
from testify import teardown
from tron.serialize.runstate.shelvestore import ShelveStateStore, ShelveKey


class ShelveStateStoreTestCase(TestCase):

    @setup
    def setup_store(self):
        self.filename = os.path.join(tempfile.gettempdir(), 'state')
        self.store = ShelveStateStore(self.filename)

    @teardown
    def teardown_store(self):
        os.unlink(self.filename)

    def test__init__(self):
        assert_equal(self.filename, self.store.filename)

    def test_save(self):
        key_value_pairs = [
            (ShelveKey("one", "two"), {'this': 'data'}),
            (ShelveKey("three", "four"), {'this': 'data2'})
        ]
        self.store.save(key_value_pairs)
        self.store.cleanup()

        stored_data = shelve.open(self.filename)
        for key, value in key_value_pairs:
            assert_equal(stored_data[key.key], value)

    def test_restore(self):
        keys = [ShelveKey("thing", i) for i in xrange(5)]
        value = {'this': 'data'}
        store = shelve.open(self.filename)
        for key in keys:
            store[key.key] = value
        store.close()

        retrieved_data = self.store.restore(keys)
        for key in keys:
            assert_equal(retrieved_data[key], value)


if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = sqlalchemystore_test
from testify import TestCase, run, setup, assert_equal, teardown
from tests.assertions import assert_length
from tron.serialize import runstate
sqlalchemystore = None # pyflakes


class SQLAlchmeyStateStoreTestCase(TestCase):
    _suites = ['sqlalchemy']

    @setup
    def setup_store(self):
        from tron.serialize.runstate import sqlalchemystore
        global sqlalchemystore
        assert sqlalchemystore # pyflakes
        details = 'sqlite:///:memory:'
        self.store = sqlalchemystore.SQLAlchemyStateStore('name', details)
        self.store.create_tables()

    @teardown
    def teardown_store(self):
        self.store.cleanup()

    def test_create_engine(self):
        assert_equal(self.store.engine.url.database, ':memory:')

    def test_create_tables(self):
        assert self.store.job_table.name
        assert self.store.service_table.name
        assert self.store.metadata_table.name

    def test_build_key(self):
        key = self.store.build_key(runstate.SERVICE_STATE, 'blah')
        assert_equal(key.table, self.store.service_table)
        assert_equal(key.id, 'blah')

    def test_save(self):
        key = sqlalchemystore.SQLStateKey(self.store.job_table, 'stars')
        doc = {'docs': 'blocks'}
        items = [(key, doc)]
        self.store.save(items)

        rows = self.store.engine.execute(self.store.job_table.select())
        assert_equal(rows.fetchone(), ('stars', "{docs: blocks}\n"))

    def test_restore_missing(self):
        key = sqlalchemystore.SQLStateKey(self.store.job_table, 'stars')
        docs = self.store.restore([key])
        assert_equal(docs, {})

    def test_restore_many(self):
        keys = [
            sqlalchemystore.SQLStateKey(self.store.job_table, 'stars'),
            sqlalchemystore.SQLStateKey(self.store.service_table, 'foo')
        ]
        items = [
            {'docs': 'builder', 'a': 'b'},
            {'docks': 'helper', 'c': 'd'}
        ]
        self.store.save(zip(keys, items))

        docs = self.store.restore(keys)
        assert_equal(docs[keys[0]], items[0])
        assert_equal(docs[keys[1]], items[1])

    def test_restore_partial(self):
        keys = [
            sqlalchemystore.SQLStateKey(self.store.job_table, 'stars'),
            sqlalchemystore.SQLStateKey(self.store.service_table, 'foo')
        ]
        item = {'docs': 'builder', 'a': 'b'}
        self.store.save([(keys[0], item)])

        docs = self.store.restore(keys)
        assert_length(docs, 1)
        assert_equal(docs[keys[0]], item)


if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = statemanager_test
import os
import mock
from testify import TestCase, assert_equal, setup, run

from tests.assertions import assert_raises
from tests.testingutils import autospec_method
from tron.config import schema
from tron.serialize import runstate
from tron.serialize.runstate.shelvestore import ShelveStateStore
from tron.serialize.runstate.statemanager import PersistentStateManager, StateChangeWatcher
from tron.serialize.runstate.statemanager import StateSaveBuffer
from tron.serialize.runstate.statemanager import StateMetadata
from tron.serialize.runstate.statemanager import PersistenceStoreError
from tron.serialize.runstate.statemanager import VersionMismatchError
from tron.serialize.runstate.statemanager import PersistenceManagerFactory


class PersistenceManagerFactoryTestCase(TestCase):

    def test_from_config_shelve(self):
        thefilename = 'thefilename'
        config = schema.ConfigState(
            store_type='shelve', name=thefilename, buffer_size=0,
            connection_details=None)
        manager = PersistenceManagerFactory.from_config(config)
        store = manager._impl
        assert_equal(store.filename, config.name)
        assert isinstance(store, ShelveStateStore)
        os.unlink(thefilename)


class StateMetadataTestCase(TestCase):

    def test_validate_metadata(self):
        metadata = {'version': (0, 5, 2)}
        StateMetadata.validate_metadata(metadata)

    def test_validate_metadata_no_state_data(self):
        metadata = None
        StateMetadata.validate_metadata(metadata)

    def test_validate_metadata_mismatch(self):
        metadata = {'version': (200, 1, 1)}
        assert_raises(
                VersionMismatchError, StateMetadata.validate_metadata, metadata)


class StateSaveBufferTestCase(TestCase):

    @setup
    def setup_buffer(self):
        self.buffer_size = 5
        self.buffer = StateSaveBuffer(self.buffer_size)

    def test_save(self):
        assert self.buffer.save(1, 2)
        assert not self.buffer.save(1, 3)
        assert not self.buffer.save(1, 4)
        assert not self.buffer.save(1, 5)
        assert not self.buffer.save(1, 6)
        assert self.buffer.save(1, 7)
        assert_equal(self.buffer.buffer[1], 7)

    def test__iter__(self):
        self.buffer.save(1, 2)
        self.buffer.save(2, 3)
        items = list(self.buffer)
        assert not self.buffer.buffer
        assert_equal(items, [(1,2), (2,3)])


class PersistentStateManagerTestCase(TestCase):

    @setup
    def setup_manager(self):
        self.store = mock.Mock()
        self.store.build_key.side_effect = lambda t, i: '%s%s' % (t, i)
        self.buffer = StateSaveBuffer(1)
        self.manager = PersistentStateManager(self.store, self.buffer)

    def test__init__(self):
        assert_equal(self.manager._impl, self.store)

    def test_keys_for_items(self):
        names = ['namea', 'nameb']
        key_to_item_map = self.manager._keys_for_items('type', names)

        keys = ['type%s' % name for name in names]
        assert_equal(key_to_item_map, dict(zip(keys, names)))

    def test_restore_dicts(self):
        names = ['namea', 'nameb']
        autospec_method(self.manager._keys_for_items)
        self.manager._keys_for_items.return_value = dict(enumerate(names))
        self.store.restore.return_value = {
            0: {'state': 'data'}, 1: {'state': '2data'}}
        state_data = self.manager._restore_dicts('type', names)
        expected = {
            names[0]: {'state': 'data'},
            names[1]: {'state': '2data'}}
        assert_equal(expected, state_data)

    def test_save(self):
        name, state_data = 'name', mock.Mock()
        self.manager.save(runstate.JOB_STATE, name, state_data)
        key = '%s%s' % (runstate.JOB_STATE, name)
        self.store.save.assert_called_with([(key, state_data)])

    def test_save_failed(self):
        self.store.save.side_effect = PersistenceStoreError("blah")
        assert_raises(PersistenceStoreError, self.manager.save, None, None, None)

    def test_save_while_disabled(self):
        with self.manager.disabled():
            self.manager.save("something", 'name', mock.Mock())
        assert not self.store.save.mock_calls

    def test_cleanup(self):
        self.manager.cleanup()
        self.store.cleanup.assert_called_with()

    def test_disabled(self):
        with self.manager.disabled():
            assert not self.manager.enabled
        assert self.manager.enabled

    def test_disabled_with_exception(self):
        def testfunc():
            with self.manager.disabled():
                raise ValueError()
        assert_raises(ValueError, testfunc)
        assert self.manager.enabled

    def test_disabled_nested(self):
        self.manager.enabled = False
        with self.manager.disabled():
            pass
        assert not self.manager.enabled


class StateChangeWatcherTestCase(TestCase):

    @setup
    def setup_watcher(self):
        self.watcher = StateChangeWatcher()
        self.state_manager = mock.create_autospec(PersistentStateManager)
        self.watcher.state_manager = self.state_manager

    def test_update_from_config_no_change(self):
        self.watcher.config = state_config = mock.Mock()
        assert not self.watcher.update_from_config(state_config)
        autospec_method(self.watcher.shutdown)
        assert_equal(self.watcher.state_manager, self.state_manager)
        assert not self.watcher.shutdown.mock_calls

    @mock.patch('tron.serialize.runstate.statemanager.PersistenceManagerFactory',
    autospec=True)
    def test_update_from_config_changed(self, mock_factory):
        state_config = mock.Mock()
        autospec_method(self.watcher.shutdown)
        assert self.watcher.update_from_config(state_config)
        assert_equal(self.watcher.config, state_config)
        self.watcher.shutdown.assert_called_with()
        assert_equal(self.watcher.state_manager,
            mock_factory.from_config.return_value)
        mock_factory.from_config.assert_called_with(state_config)

    def test_save_job(self):
        mock_job = mock.Mock()
        self.watcher.save_job(mock_job)
        self.watcher.state_manager.save.assert_called_with(
            runstate.JOB_STATE, mock_job.name, mock_job.state_data)

    def test_save_service(self):
        mock_service = mock.Mock()
        self.watcher.save_service(mock_service)
        self.watcher.state_manager.save.assert_called_with(
            runstate.SERVICE_STATE, mock_service.name, mock_service.state_data)

    def test_save_metadata(self):
        patcher = mock.patch('tron.serialize.runstate.statemanager.StateMetadata')
        with patcher as mock_state_metadata:
            self.watcher.save_metadata()
            meta_data = mock_state_metadata.return_value
            self.watcher.state_manager.save.assert_called_with(
                runstate.MCP_STATE, meta_data.name, meta_data.state_data)

    def test_shutdown(self):
        self.watcher.shutdown()
        assert not self.watcher.state_manager.enabled
        self.watcher.state_manager.cleanup.assert_called_with()

    def test_disabled(self):
        context = self.watcher.disabled()
        assert_equal(self.watcher.state_manager.disabled.return_value, context)

    def test_restore(self):
        jobs, services = mock.Mock(), mock.Mock()
        self.watcher.restore(jobs, services)
        self.watcher.state_manager.restore.assert_called_with(jobs, services)



if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = yamlstore_test
import os
import tempfile

from testify import TestCase, run, setup, assert_equal, teardown
import yaml
from tron.serialize.runstate import yamlstore

class YamlStateStoreTestCase(TestCase):

    @setup
    def setup_store(self):
        self.filename = os.path.join(tempfile.gettempdir(), 'yaml_state')
        self.store = yamlstore.YamlStateStore(self.filename)
        self.test_data = {
            'one': {'a': 1},
            'two': {'b': 2},
            'three': {'c': 3}
        }

    @teardown
    def teardown_store(self):
        try:
            os.unlink(self.filename)
        except OSError:
            pass

    def test_restore(self):
        with open(self.filename, 'w') as fh:
            yaml.dump(self.test_data, fh)

        keys = [yamlstore.YamlKey('one', 'a'), yamlstore.YamlKey('three', 'c')]
        state_data = self.store.restore(keys)
        assert_equal(self.store.buffer, self.test_data)

        expected = {keys[0]: 1, keys[1]: 3}
        assert_equal(expected, state_data)

    def test_restore_missing_type_key(self):
        with open(self.filename, 'w') as fh:
            yaml.dump(self.test_data, fh)

        keys = [yamlstore.YamlKey('seven', 'a')]
        state_data = self.store.restore(keys)
        assert_equal(self.store.buffer, self.test_data)
        assert_equal({}, state_data)

    def test_restore_file_missing(self):
        state_data = self.store.restore(['some', 'keys'])
        assert_equal(state_data, {})

    def test_save(self):
        expected = {'one': {'five': 'dataz'}, 'two': {'seven': 'stars'}}

        key_value_pairs = [
            (yamlstore.YamlKey('one', 'five'), 'barz')
        ]
        # Save first
        self.store.save(key_value_pairs)

        # Save second
        key_value_pairs = [
            (yamlstore.YamlKey('two', 'seven'), 'stars'),
            (yamlstore.YamlKey('one', 'five'), 'dataz')
        ]
        self.store.save(key_value_pairs)

        assert_equal(self.store.buffer, expected)
        with open(self.filename, 'r') as fh:
            actual = yaml.load(fh)
        assert_equal(actual, expected)






if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = ssh_test
import mock
from testify import TestCase, assert_equal, setup
from testify.assertions import assert_not_equal
from twisted.python import failure

from tests.testingutils import autospec_method
from tron import ssh


class ClientTransportTestCase(TestCase):

    @setup
    def setup_transport(self):
        self.username = 'username'
        self.options = mock.Mock()
        self.expected_pub_key = mock.Mock()
        self.transport = ssh.ClientTransport(self.username, self.options, self.expected_pub_key)

    def test_verifyHostKey_missing_pub_key(self):
        self.transport.expected_pub_key = None
        result = self.transport.verifyHostKey(mock.Mock(), mock.Mock())
        assert_equal(result.result, 1)

    @mock.patch('tron.ssh.keys', autospec=True)
    def test_verifyHostKey_matching_pub_key(self, mock_keys):
        mock_keys.Key.fromString.return_value = self.expected_pub_key
        public_key = mock.Mock()
        result = self.transport.verifyHostKey(public_key, mock.Mock())
        assert_equal(result.result, 2)
        mock_keys.Key.fromString.assert_called_with(public_key)

    @mock.patch('tron.ssh.keys', autospec=True)
    def test_verifyHostKey_mismatch_pub_key(self, _):
        public_key = mock.Mock()
        result = self.transport.verifyHostKey(public_key, mock.Mock())
        assert isinstance(result.result, failure.Failure)

    def test_connnectionSecure(self):
        self.transport.connection_defer = mock.Mock()
        autospec_method(self.transport.requestService)
        self.transport.connectionSecure()
        conn = self.transport.connection_defer.mock_calls[0][1][0]
        assert isinstance(conn, ssh.ClientConnection)
        auth_service  = self.transport.requestService.mock_calls[0][1][0]
        assert isinstance(auth_service, ssh.NoPasswordAuthClient)


class SSHAuthOptionsTestCase(TestCase):

    def test_from_config_none(self):
        ssh_conf = mock.Mock(agent=False, identities=[])
        ssh_options = ssh.SSHAuthOptions.from_config(ssh_conf)
        assert_equal(ssh_options['noagent'], True)
        assert_equal(ssh_options.identitys, [])

    def test_from_config_both(self):
        identities = ['one', 'two']
        ssh_conf = mock.Mock(agent=True, identities=identities)
        ssh_options = ssh.SSHAuthOptions.from_config(ssh_conf)
        assert_equal(ssh_options['noagent'], False)
        assert_equal(ssh_options.identitys, identities)

    def test__eq__true(self):
        config = mock.Mock(agent=True, identities=['one', 'two'])
        assert_equal(
            ssh.SSHAuthOptions.from_config(config),
            ssh.SSHAuthOptions.from_config(config))

    def test__eq__false(self):
        config = mock.Mock(agent=True, identities=['one', 'two'])
        second_config = mock.Mock(agent=True, identities=['two'])
        assert_not_equal(
            ssh.SSHAuthOptions.from_config(config),
            ssh.SSHAuthOptions.from_config(second_config))
########NEW FILE########
__FILENAME__ = testingutils
import logging
import functools
import mock

from testify import  TestCase, setup
from testify import class_setup, class_teardown
from testify import teardown
import time
from tron.utils import timeutils


log = logging.getLogger(__name__)

# TODO: remove when replaced with tron.eventloop
class MockReactorTestCase(TestCase):
    """Patch the reactor to a MockReactor."""

    # Override this in subclasses
    module_to_mock = None

    @class_setup
    def class_setup_patched_reactor(self):
        msg = "%s must set a module_to_mock field" % self.__class__
        assert self.module_to_mock, msg
        self.old_reactor = getattr(self.module_to_mock, 'reactor')

    @class_teardown
    def teardown_patched_reactor(self):
        setattr(self.module_to_mock, 'reactor', self.old_reactor)

    @setup
    def setup_mock_reactor(self):
        self.reactor = Turtle()
        setattr(self.module_to_mock, 'reactor', self.reactor)


# TODO: remove
class MockTimeTestCase(TestCase):

    now = None

    @setup
    def setup_current_time(self):
        assert self.now, "%s must set a now field" % self.__class__
        self.old_current_time  = timeutils.current_time
        timeutils.current_time = lambda: self.now

    @teardown
    def teardown_current_time(self):
        timeutils.current_time = self.old_current_time
        # Reset 'now' back to what was set on the class because some test may
        # have changed it
        self.now = self.__class__.now


def retry(max_tries=3, delay=0.1, exceptions=(KeyError, IndexError)):
    """A function decorator for re-trying an operation. Useful for MongoDB
    which is only eventually consistent.
    """
    def wrapper(f):
        @functools.wraps(f)
        def wrap(*args, **kwargs):
            for _ in xrange(max_tries):
                try:
                    return f(*args, **kwargs)
                except exceptions:
                    time.sleep(delay)
            raise
        return wrap
    return wrapper


# TODO: remove when replaced with mock
class Turtle(object):
    """A more complete Mock implementation."""
    def __init__(self, *args, **kwargs):
        self.__dict__.update(kwargs)
        self.calls = []
        self.returns = []

    def __getattr__(self, name):
        self.__dict__[name] = type(self)()
        return self.__dict__[name]

    def __call__(self, *args, **kwargs):
        self.calls.append((args, kwargs))
        new_turtle = type(self)()
        self.returns.append(new_turtle)
        return new_turtle


def autospec_method(method, *args, **kwargs):
    """create an autospec for an instance method."""
    mocked_method = mock.create_autospec(method, *args, **kwargs)
    setattr(method.im_self, method.__name__, mocked_method)

########NEW FILE########
__FILENAME__ = trondaemon_test
import os
import tempfile
import lockfile
import mock
from testify import TestCase, assert_equal, run, setup, teardown
from testify.assertions import assert_in
from tests.assertions import assert_raises
from tron.trondaemon import PIDFile


class PIDFileTestCase(TestCase):

    @setup
    def setup_pidfile(self):
        self.filename = os.path.join(tempfile.gettempdir(), 'test.pid')
        self.pidfile = PIDFile(self.filename)

    @teardown
    def teardown_pidfile(self):
        self.pidfile.__exit__(None, None, None)

    def test__init__(self):
        # Called from setup already
        assert self.pidfile.lock.is_locked()
        assert_equal(self.filename, self.pidfile.filename)

    def test_check_if_pidfile_exists_file_locked(self):
        assert_raises(lockfile.AlreadyLocked, PIDFile, self.filename)

    def test_check_if_pidfile_exists_file_exists(self):
        self.pidfile.__exit__(None, None, None)
        with open(self.filename, 'w') as fh:
            fh.write('123\n')

        with mock.patch.object(PIDFile, 'is_process_running') as mock_method:
            mock_method.return_value = True
            exception = assert_raises(SystemExit, PIDFile, self.filename)
            assert_in('Daemon running as 123', str(exception))

    def test_is_process_running(self):
        assert self.pidfile.is_process_running(os.getpid())

    def test_is_process_running_not_running(self):
        assert not self.pidfile.is_process_running(None)
        # Hope this isn't in use
        assert not self.pidfile.is_process_running(99999)

    def test__enter__(self):
        self.pidfile.__enter__()
        with open(self.filename, 'r') as fh:
            assert_equal(fh.read(), '%s\n' % os.getpid())

    def test__exit__(self):
        self.pidfile.__exit__(None, None, None)
        assert not self.pidfile.lock.is_locked()
        assert not os.path.exists(self.filename)


if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = trond_test
import datetime
import os
from subprocess import CalledProcessError
from textwrap import dedent
import textwrap

from testify import assert_equal
from testify import assert_gt
from testify.assertions import assert_in, assert_raises_such_that
from tests import sandbox
from tron.core import service, actionrun


BASIC_CONFIG = """
ssh_options:
    agent: true

nodes:
  - name: local
    hostname: 'localhost'

state_persistence:
    name: "state_data.shelve"
    store_type: shelve

"""

SINGLE_ECHO_CONFIG = BASIC_CONFIG + """
jobs:
  - name: "echo_job"
    node: local
    schedule: "interval 1 hour"
    actions:
      - name: "echo_action"
        command: "echo 'Echo!'" """

DOUBLE_ECHO_CONFIG = SINGLE_ECHO_CONFIG + """
      - name: "another_echo_action"
        command: "echo 'Today is %(shortdate)s, which is the same
                    as %(year)s-%(month)s-%(day)s' && false" """

ALT_NAMESPACED_ECHO_CONFIG = """
jobs:
  - name: "echo_job"
    node: local
    schedule: "interval 1 hour"
    actions:
      - name: "echo_action"
        command: "echo 'Echo!'" """

TOUCH_CLEANUP_FMT = """
    cleanup_action:
      command: "echo 'at last'"
"""


def summarize_events(events):
    return [(event['entity'], event['name']) for event in events]


class TrondEndToEndTestCase(sandbox.SandboxTestCase):

    def test_end_to_end_basic(self):
        self.start_with_config(SINGLE_ECHO_CONFIG)
        client = self.sandbox.client

        assert_equal(self.client.config('MASTER')['config'], SINGLE_ECHO_CONFIG)

        # reconfigure and confirm results
        second_config = DOUBLE_ECHO_CONFIG + TOUCH_CLEANUP_FMT
        self.sandbox.tronfig(second_config)
        events = summarize_events(client.events())
        assert_in(('', 'restoring'), events)
        assert_in(('MASTER.echo_job.0', 'created'), events)
        assert_equal(client.config('MASTER')['config'], second_config)

        # reconfigure, by uploading a third configuration
        self.sandbox.tronfig(ALT_NAMESPACED_ECHO_CONFIG, name='ohce')
        self.sandbox.client.home()

        # run the job and check its output
        echo_job_name = 'MASTER.echo_job'
        job_url = client.get_url(echo_job_name)
        action_url = client.get_url('MASTER.echo_job.1.echo_action')

        self.sandbox.tronctl('start', echo_job_name)

        def wait_on_cleanup():
            return (len(client.job(job_url)['runs']) >= 2 and
                    client.action_runs(action_url)['state'] ==
                    actionrun.ActionRun.STATE_SUCCEEDED.name)
        sandbox.wait_on_sandbox(wait_on_cleanup)

        echo_action_run = client.action_runs(action_url)
        another_action_url = client.get_url('MASTER.echo_job.1.another_echo_action')
        other_act_run = client.action_runs(another_action_url)
        assert_equal(echo_action_run['state'],
            actionrun.ActionRun.STATE_SUCCEEDED.name)
        assert_equal(echo_action_run['stdout'], ['Echo!'])
        assert_equal(other_act_run['state'],
            actionrun.ActionRun.STATE_FAILED.name)

        now = datetime.datetime.now()
        stdout = now.strftime('Today is %Y-%m-%d, which is the same as %Y-%m-%d')
        assert_equal(other_act_run['stdout'], [stdout])

        job_runs_url = client.get_url('%s.1' % echo_job_name)
        assert_equal(client.job_runs(job_runs_url)['state'],
            actionrun.ActionRun.STATE_FAILED.name)

    def test_node_reconfig(self):
        job_service_config = dedent("""
            jobs:
                - name: a_job
                  node: local
                  schedule: "interval 1s"
                  actions:
                    - name: first_action
                      command: "echo something"

            services:
                - name: a_service
                  node: local
                  pid_file: /tmp/does_not_exist
                  command: "echo service start"
                  monitor_interval: 1
        """)
        second_config = dedent("""
            ssh_options:
                agent: true

            nodes:
              - name: local
                hostname: '127.0.0.1'

            state_persistence:
                name: "state_data.shelve"
                store_type: shelve

        """) + job_service_config
        self.start_with_config(BASIC_CONFIG + job_service_config)

        service_name = 'MASTER.a_service'
        service_url = self.client.get_url(service_name)
        self.sandbox.tronctl('start', service_name)
        sandbox.wait_on_state(self.client.service, service_url,
            service.ServiceState.FAILED)

        job_url = self.client.get_url('MASTER.a_job.0')
        sandbox.wait_on_state(self.client.job_runs, job_url,
            actionrun.ActionRun.STATE_SUCCEEDED.name)

        self.sandbox.tronfig(second_config)

        sandbox.wait_on_state(self.client.service, service_url,
            service.ServiceState.DISABLED)

        job_url = self.client.get_url('MASTER.a_job')
        def wait_on_next_run():
            last_run = self.client.job(job_url)['runs'][0]
            return last_run['node']['hostname'] == '127.0.0.1'

        sandbox.wait_on_sandbox(wait_on_next_run)


class TronCommandsTestCase(sandbox.SandboxTestCase):

    def test_tronview(self):
        self.start_with_config(SINGLE_ECHO_CONFIG)
        expected = """\nServices:\nNo Services\n\n\nJobs:
            Name       State       Scheduler           Last Success
            MASTER.echo_job   enabled     interval 1:00:00    None
            """

        def remove_line_space(s):
            return [l.replace(' ', '') for l in s.split('\n')]

        actual = self.sandbox.tronview()[0]
        assert_equal(remove_line_space(actual), remove_line_space(expected))

    def test_tronctl_with_job(self):
        self.start_with_config(SINGLE_ECHO_CONFIG + TOUCH_CLEANUP_FMT)
        job_name = 'MASTER.echo_job'
        job_url = self.client.get_url(job_name)
        self.sandbox.tronctl('start', job_name)

        cleanup_url = self.client.get_url('MASTER.echo_job.1.cleanup')
        sandbox.wait_on_state(self.client.action_runs, cleanup_url,
            actionrun.ActionRun.STATE_SUCCEEDED.name)

        action_run_url = self.client.get_url('MASTER.echo_job.1.echo_action')
        assert_equal(self.client.action_runs(action_run_url)['state'],
            actionrun.ActionRun.STATE_SUCCEEDED.name)

        job_run_url = self.client.get_url('MASTER.echo_job.1')
        assert_equal(self.client.job_runs(job_run_url)['state'],
            actionrun.ActionRun.STATE_SUCCEEDED.name)

        assert_equal(self.client.job(job_url)['status'], 'enabled')
        self.sandbox.tronctl('disable', job_name)
        sandbox.wait_on_state(self.client.job, job_url, 'disabled', 'status')

    def test_tronfig(self):
        self.start_with_config(SINGLE_ECHO_CONFIG)
        stdout, stderr = self.sandbox.tronfig()
        assert_equal(stdout.rstrip(), SINGLE_ECHO_CONFIG.rstrip())

    def test_tronfig_failure(self):
        self.start_with_config(SINGLE_ECHO_CONFIG)
        bad_config = 'this is not valid: yaml: is it?'
        def test_return_code(exc):
            assert_equal(exc.returncode, 1)
        assert_raises_such_that(CalledProcessError, test_return_code,
            self.sandbox.tronfig, bad_config)

    def test_tronfig_no_header(self):
        self.start_with_config(SINGLE_ECHO_CONFIG)
        namespace = 'second'
        self.sandbox.tronfig(ALT_NAMESPACED_ECHO_CONFIG, name=namespace)
        stdout, stderr = self.sandbox.tronfig(name=namespace, no_header=True)
        assert_equal(stdout.rstrip(), ALT_NAMESPACED_ECHO_CONFIG.rstrip())


class JobEndToEndTestCase(sandbox.SandboxTestCase):

    def test_cleanup_on_failure(self):
        config = BASIC_CONFIG + dedent("""
        jobs:
          - name: "failjob"
            node: local
            schedule: "constant"
            actions:
              - name: "failaction"
                command: "failplz"
        """) + TOUCH_CLEANUP_FMT
        self.start_with_config(config)

        action_run_url = self.client.get_url('MASTER.failjob.0.failaction')
        sandbox.wait_on_state(self.client.action_runs, action_run_url,
            actionrun.ActionRun.STATE_FAILED.name)

        action_run_url = self.client.get_url('MASTER.failjob.1.cleanup')
        sandbox.wait_on_state(self.client.action_runs, action_run_url,
            actionrun.ActionRun.STATE_SUCCEEDED.name)
        job_runs = self.client.job(self.client.get_url('MASTER.failjob'))['runs']
        assert_gt(len(job_runs), 1)

    def test_skip_failed_actions(self):
        config = BASIC_CONFIG + dedent("""
        jobs:
          - name: "multi_step_job"
            node: local
            schedule: "constant"
            actions:
              - name: "broken"
                command: "failingcommand"
              - name: "works"
                command: "echo ok"
                requires: [broken]
        """)
        self.start_with_config(config)
        action_run_url = self.client.get_url('MASTER.multi_step_job.0.broken')
        waiter = sandbox.build_waiter_func(self.client.action_runs, action_run_url)

        waiter(actionrun.ActionRun.STATE_FAILED.name)
        self.sandbox.tronctl('skip', 'MASTER.multi_step_job.0.broken')
        waiter(actionrun.ActionRun.STATE_SKIPPED.name)

        action_run_url = self.client.get_url('MASTER.multi_step_job.0.works')
        sandbox.wait_on_state(self.client.action_runs, action_run_url,
            actionrun.ActionRun.STATE_SUCCEEDED.name)

        job_run_url = self.client.get_url('MASTER.multi_step_job.0')
        sandbox.wait_on_state(self.client.job_runs, job_run_url,
            actionrun.ActionRun.STATE_SUCCEEDED.name)

    def test_failure_on_multi_step_job_doesnt_wedge_tron(self):
        config = BASIC_CONFIG + dedent("""
            jobs:
                -   name: "random_failure_job"
                    node: local
                    queueing: true
                    schedule: "constant"
                    actions:
                        -   name: "fa"
                            command: "sleep 0.1; failplz"
                        -   name: "sa"
                            command: "echo 'you will never see this'"
                            requires: [fa]
        """)
        self.start_with_config(config)
        job_url = self.client.get_url('MASTER.random_failure_job')

        def wait_on_random_failure_job():
            return len(self.client.job(job_url)['runs']) >= 4
        sandbox.wait_on_sandbox(wait_on_random_failure_job)

        job_runs = self.client.job(job_url)['runs']
        expected = [actionrun.ActionRun.STATE_FAILED.name for _ in range(3)]
        assert_equal([run['state'] for run in job_runs[-3:]], expected)

    def test_cancel_schedules_a_new_run(self):
        config = BASIC_CONFIG + dedent("""
            jobs:
                -   name: "a_job"
                    node: local
                    schedule: "daily 05:00:00"
                    actions:
                        -   name: "first_action"
                            command: "echo OK"
        """)
        self.start_with_config(config)
        job_name = 'MASTER.a_job'
        job_url = self.client.get_url(job_name)

        self.sandbox.tronctl('cancel', '%s.0' % job_name)
        def wait_on_cancel():
            return len(self.client.job(job_url)['runs']) == 2
        sandbox.wait_on_sandbox(wait_on_cancel)

        run_states = [run['state'] for run in self.client.job(job_url)['runs']]
        expected = [
            actionrun.ActionRun.STATE_SCHEDULED.name,
            actionrun.ActionRun.STATE_CANCELLED.name]
        assert_equal(run_states, expected)

    def test_job_queueing_false_with_overlap(self):
        """Test that a job that has queueing false properly cancels an
        overlapping job run.
        """
        config = BASIC_CONFIG + dedent("""
            jobs:
                -   name: "cancel_overlap"
                    schedule: "interval 1s"
                    queueing: False
                    node: local
                    actions:
                        -   name: "do_something"
                            command: "sleep 3s"
                        -   name: "do_other"
                            command: "sleep 3s"
                    cleanup_action:
                        command: "echo done"
        """)
        self.start_with_config(config)
        job_url = self.client.get_url('MASTER.cancel_overlap')
        job_run_url = self.client.get_url('MASTER.cancel_overlap.1')

        def wait_on_job_schedule():
            return len(self.client.job(job_url)['runs']) == 2
        sandbox.wait_on_sandbox(wait_on_job_schedule)

        sandbox.wait_on_state(self.client.job, job_run_url,
            actionrun.ActionRun.STATE_CANCELLED.name)

        action_run_states = [action_run['state'] for action_run in
                             self.client.job_runs(job_run_url)['runs']]
        expected = [actionrun.ActionRun.STATE_CANCELLED.name
                    for _ in xrange(len(action_run_states))]
        assert_equal(action_run_states, expected)

    def test_trond_restart_job_with_run_history(self):
        config = BASIC_CONFIG + textwrap.dedent("""
           jobs:
              - name: fast_job
                node: local
                schedule: constant
                actions:
                  - name: single_act
                    command: "sleep 20 && echo good"
        """)
        self.start_with_config(config)

        action_run_url = self.client.get_url('MASTER.fast_job.0.single_act')
        sandbox.wait_on_state(self.client.action_runs, action_run_url,
            actionrun.ActionRun.STATE_RUNNING.name)

        self.restart_trond()

        assert_equal(self.client.job_runs(action_run_url)['state'],
            actionrun.ActionRun.STATE_UNKNOWN.name)

        next_run_url = self.client.get_url('MASTER.fast_job.-1.single_act')
        sandbox.wait_on_state(self.client.action_runs, next_run_url,
            actionrun.ActionRun.STATE_RUNNING.name)

    def test_trond_restart_job_running_with_dependencies(self):
        config = BASIC_CONFIG + textwrap.dedent("""
            jobs:
                - name: complex_job
                  node: local
                  schedule: interval 10min
                  actions:
                    - name: first_act
                      command: sleep 20 && echo "I'm waiting"
                    - name: following_act
                      command: echo "thing"
                      requires: ['first_act']
                    - name: last_act
                      command: echo foo
                      requires: ['following_act']
        """)
        self.start_with_config(config)
        job_name = 'MASTER.complex_job'
        self.sandbox.tronctl('start', job_name)

        action_run_url = self.client.get_url('MASTER.complex_job.1.first_act')
        sandbox.wait_on_state(self.client.action_runs, action_run_url,
            actionrun.ActionRun.STATE_RUNNING.name)

        self.restart_trond()

        assert_equal(self.client.job_runs(action_run_url)['state'],
            actionrun.ActionRun.STATE_UNKNOWN.name)

        for followup_action_run in ('following_act', 'last_act'):
            url = self.client.get_url('%s.1.%s' % (job_name, followup_action_run))
            assert_equal(self.client.action_runs(url)['state'],
                actionrun.ActionRun.STATE_QUEUED.name)


class ServiceEndToEndTestCase(sandbox.SandboxTestCase):

    def test_service_reconfigure(self):
        config_template = BASIC_CONFIG + dedent("""
            services:
                -   name: "a_service"
                    node: local
                    pid_file: "{wd}/%(name)s-%(instance_number)s.pid"
                    command: "{command}"
                    monitor_interval: {monitor_interval}
                    restart_delay: 2
        """)

        command = ("cd {path} && PYTHONPATH=. python "
                   "{path}/tests/mock_daemon.py %(pid_file)s")
        command = command.format(path=os.path.abspath('.'))
        config = config_template.format(
            command=command, monitor_interval=1, wd=self.sandbox.tmp_dir)

        self.start_with_config(config)
        service_name = 'MASTER.a_service'
        service_url = self.client.get_url(service_name)

        self.sandbox.tronctl('start', service_name)
        waiter = sandbox.build_waiter_func(self.client.service, service_url)
        waiter(service.ServiceState.UP)

        new_config = config_template.format(
            command=command, monitor_interval=2, wd=self.sandbox.tmp_dir)
        self.sandbox.tronfig(new_config)

        waiter(service.ServiceState.DISABLED)
        self.sandbox.tronctl('start', service_name)
        waiter(service.ServiceState.UP)
        self.sandbox.tronctl('stop', service_name)
        waiter(service.ServiceState.DISABLED)

    def test_service_failed_restart(self):
        config = BASIC_CONFIG + dedent("""
            services:
                -   name: service_restart
                    node: local
                    pid_file: "/tmp/file_dne"
                    command: "sleep 1; cat /bogus/file/DNE"
                    monitor_interval: 1
                    restart_delay: 2
        """)
        self.start_with_config(config)
        service_name = 'MASTER.service_restart'
        service_url = self.client.get_url(service_name)
        self.sandbox.tronctl('start', service_name)

        waiter = sandbox.build_waiter_func(self.client.service, service_url)
        waiter(service.ServiceState.FAILED)
        service_content = self.client.service(service_url)
        expected = 'cat: /bogus/file/DNE: No such file or directory'
        assert_in(service_content['instances'][0]['failures'][0], expected)
        waiter(service.ServiceState.STARTING)

########NEW FILE########
__FILENAME__ = collections_test
import mock
from testify import TestCase, setup
from testify.assertions import assert_in, assert_raises, assert_not_in
from testify.assertions import assert_equal
from tests.assertions import assert_mock_calls
from tests.testingutils import autospec_method
from tron.utils import collections


class MappingCollectionsTestCase(TestCase):

    @setup
    def setup_collection(self):
        self.name = 'some_name'
        self.collection = collections.MappingCollection(self.name)

    def test_filter_by_name(self):
        autospec_method(self.collection.remove)
        self.collection.update(dict.fromkeys(['c', 'd', 'e']))
        self.collection.filter_by_name(['a', 'c'])
        expected = [mock.call(name) for name in ['d', 'e']]
        assert_mock_calls(expected, self.collection.remove.mock_calls)

    def test_remove_missing(self):
        assert_raises(ValueError, self.collection.remove, 'name')

    def test_remove(self):
        name = 'the_name'
        self.collection[name] = item = mock.Mock()
        self.collection.remove(name)
        assert_not_in(name, self.collection)
        item.disable.assert_called_with()

    def test_restore_state(self):
        state_data = {'a': mock.Mock(), 'b': mock.Mock()}
        self.collection.update({'a': mock.Mock(), 'b': mock.Mock()})
        self.collection.restore_state(state_data)
        for key in state_data:
            self.collection[key].restore_state.assert_called_with(state_data[key])

    def test_contains_item_false(self):
        mock_item, mock_func = mock.Mock(), mock.Mock()
        assert not self.collection.contains_item(mock_item, mock_func)
        assert not mock_func.mock_calls

    def test_contains_item_not_equal(self):
        mock_item, mock_func = mock.Mock(), mock.Mock()
        self.collection[mock_item.get_name()] = 'other item'
        result = self.collection.contains_item(mock_item, mock_func)
        assert_equal(result, mock_func.return_value)
        mock_func.assert_called_with(mock_item)

    def test_contains_item_true(self):
        mock_item, mock_func = mock.Mock(), mock.Mock()
        self.collection[mock_item.get_name()] = mock_item
        assert self.collection.contains_item(mock_item, mock_func)

    def test_add_contains(self):
        autospec_method(self.collection.contains_item)
        item, update_func = mock.Mock(), mock.Mock()
        assert not self.collection.add(item, update_func)
        assert_not_in(item.get_name(), self.collection)

    def test_add_new(self):
        autospec_method(self.collection.contains_item, return_value=False)
        item, update_func = mock.Mock(), mock.Mock()
        assert self.collection.add(item, update_func)
        assert_in(item.get_name(), self.collection)

    def test_replace(self):
        autospec_method(self.collection.add)
        item = mock.Mock()
        self.collection.replace(item)
        self.collection.add.assert_called_with(item, self.collection.remove_item)


class EnumTestCase(TestCase):

    @setup
    def setup_enum(self):
        self.values = ['one', 'two', 'three']
        self.enum = collections.Enum.create(*self.values)

    def test_create(self):
        assert_equal(self.enum.values, set(self.values))

    def test__contains__(self):
        assert_in('one', self.enum)
        assert_in('two', self.enum)
        assert_in('three', self.enum)
        assert_not_in('four', self.enum)
        assert_not_in('zero', self.enum)

    def test__getattr__(self):
        assert_equal(self.enum.one, 'one')
        assert_equal(self.enum.two, 'two')

    def test__getattr__miss(self):
        assert_raises(AttributeError, lambda: self.enum.seven)

    def test__iter__(self):
        assert_equal(set(self.enum), set(self.values))
########NEW FILE########
__FILENAME__ = crontab_test
from testify import TestCase, run, assert_equal, assert_raises, setup

from tron.utils import crontab


class ConvertPredefinedTestCase(TestCase):

    def test_convert_predefined_valid(self):
        expected = crontab.PREDEFINED_SCHEDULE['@hourly']
        assert_equal(crontab.convert_predefined('@hourly'), expected)

    def test_convert_predefined_invalid(self):
        assert_raises(ValueError, crontab.convert_predefined, '@bogus')

    def test_convert_predefined_none(self):
        line = 'something else'
        assert_equal(crontab.convert_predefined(line), line)


class ParseCrontabTestCase(TestCase):

    def test_parse_asterisk(self):
        line = '* * * * *'
        actual = crontab.parse_crontab(line)
        assert_equal(actual['minutes'], None)
        assert_equal(actual['hours'], None)
        assert_equal(actual['months'], None)


class MinuteFieldParserTestCase(TestCase):
    @setup
    def setup_parser(self):
        self.parser = crontab.MinuteFieldParser()

    def test_validate_bounds(self):
        assert_equal(self.parser.validate_bounds(0), 0)
        assert_equal(self.parser.validate_bounds(59), 59)
        assert_raises(ValueError, self.parser.validate_bounds, 60)

    def test_get_values_asterisk(self):
        assert_equal(self.parser.get_values("*"), range(0, 60))

    def test_get_values_min_only(self):
        assert_equal(self.parser.get_values("4"), [4])
        assert_equal(self.parser.get_values("33"), [33])

    def test_get_values_with_step(self):
        assert_equal(self.parser.get_values("*/10"), [0,10,20,30,40,50])

    def test_get_values_with_step_and_range(self):
        assert_equal(self.parser.get_values("10-30/10"), [10,20,30])

    def test_get_values_with_step_and_overflow_range(self):
        assert_equal(self.parser.get_values("30-0/10"), [30,40,50,0])

    def test_parse_with_groups(self):
        assert_equal(self.parser.parse("5,1,7,8,5"), [1,5,7,8])

    def test_parse_with_groups_and_ranges(self):
        expected = [0,1,11,13,15,17,19,20,21,40]
        assert_equal(self.parser.parse("1,11-22/2,*/20"), expected)


class MonthFieldParserTestCase(TestCase):

    @setup
    def setup_parser(self):
        self.parser = crontab.MonthFieldParser()

    def test_parse(self):
        expected = [1, 2, 3, 7, 12]
        assert_equal(self.parser.parse("DEC, Jan-Feb, jul, MaR"), expected)


class WeekdayFieldParserTestCase(TestCase):

    @setup
    def setup_parser(self):
        self.parser = crontab.WeekdayFieldParser()

    def test_parser(self):
        expected = [0,3,5,6]
        assert_equal(self.parser.parse("Sun, 3, FRI, SaT-Sun"), expected)


class MonthdayFieldParserTestCase(TestCase):

    @setup
    def setup_parser(self):
        self.parser = crontab.MonthdayFieldParser()

    def test_parse_last(self):
        expected = [5, 6, 'LAST']
        assert_equal(self.parser.parse("5, 6, L"), expected)

if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = dicts_test
from testify import TestCase
from testify import assert_equal
from tron.utils import dicts


class InvertDictListTestCase(TestCase):

    def test_invert_dict_list(self):
        expected = {
            'a': 1,
            'b': 1,
            'c': 1,
            'd': 2,
            'e': 3,
            'f': 3,
        }
        original = {
            1: ['a', 'b', 'c'],
            2: ['d'],
            3: ['e', 'f']
        }
        assert_equal(dicts.invert_dict_list(original), expected)
########NEW FILE########
__FILENAME__ = iteration_test
from testify import TestCase, assert_equal, setup, run
from tests.assertions import assert_raises
from tron.utils.iteration import min_filter, max_filter, list_all

class FilterFuncTestCase(TestCase):

    __test__ = False

    @setup
    def setup_seq(self):
        self.test_func = None

    def test_filter_empty_seq(self):
        assert_equal(self.test_func([]), None)

    def test_filter_all_nones(self):
        assert_equal(self.test_func([None, None, None]), None)

    def test_filter_none(self):
        assert_equal(self.test_func(None), None)

    def test_filter_single_item(self):
        assert_equal(self.test_func([1]), 1)

    def test_filter_single_item_with_nones(self):
        assert_equal(self.test_func([None, 4, None, None]), 4)


class FilteredMinTestCase(FilterFuncTestCase):

    @setup
    def setup_func(self):
        self.test_func = min_filter

    def test_min_filter(self):
        seq = [None, 2, None, 7, None, 9, 10, 12, 1]
        assert_equal(min_filter(seq), 1)


class FilteredMaxTestCase(FilterFuncTestCase):

    @setup
    def setup_func(self):
        self.test_func = max_filter

    def test_max_filter(self):
        seq = [None, 2, None, 7, None, 9, 10, 12, 1]
        assert_equal(max_filter(seq), 12)


class ListAllTestCase(TestCase):

    def test_all_true(self):
        assert list_all(range(1,5))

    def test_all_false(self):
        assert not list_all(0 for _ in xrange(7))

    def test_full_iteration(self):
        seq = iter([1, 0, 3, 0, 5])
        assert not list_all(seq)
        assert_raises(StopIteration, seq.next)

if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = observer_test
from testify import run, setup, assert_equal, TestCase, turtle
from tests.assertions import assert_length
from tron.utils.observer import Observable, Observer


class ObservableTestCase(TestCase):

    @setup
    def setup_observer(self):
        self.obs = Observable()

    def test_attach(self):
        func = lambda: 1
        self.obs.attach('a', func)
        assert_equal(len(self.obs._observers), 1)
        assert_equal(self.obs._observers['a'], [func])

    def test_listen_seq(self):
        func = lambda: 1
        self.obs.attach(['a', 'b'], func)
        assert_equal(len(self.obs._observers), 2)
        assert_equal(self.obs._observers['a'], [func])
        assert_equal(self.obs._observers['b'], [func])

    def test_notify(self):
        handler = turtle.Turtle()
        self.obs.attach(['a', 'b'], handler)
        self.obs.notify('a')
        assert_equal(len(handler.handler.calls), 1)
        self.obs.notify('b')
        assert_equal(len(handler.handler.calls), 2)


class ObserverClearTestCase(TestCase):

    @setup
    def setup_observer(self):
        self.obs = Observable()
        func = lambda: 1
        self.obs.attach('a', func)
        self.obs.attach('b', func)
        self.obs.attach(True, func)
        self.obs.attach(['a', 'b'], func)

    def test_clear_listeners_all(self):
        self.obs.clear_observers()
        assert_equal(len(self.obs._observers), 0)

    def test_clear_listeners_some(self):
        self.obs.clear_observers('a')
        assert_equal(len(self.obs._observers), 2)
        assert_equal(set(self.obs._observers.keys()), set([True, 'b']))

    def test_remove_observer_none(self):
        observer = lambda: 2
        self.obs.remove_observer(observer)
        assert_equal(set(self.obs._observers.keys()), set([True, 'a', 'b']))
        assert_length(self.obs._observers['a'], 2)
        assert_length(self.obs._observers['b'], 2)
        assert_length(self.obs._observers[True], 1)

    def test_remove_observer(self):
        observer = lambda: 2
        self.obs.attach('a', observer)
        self.obs.attach('c', observer)
        self.obs.remove_observer(observer)
        assert_length(self.obs._observers['a'], 2)
        assert_length(self.obs._observers['b'], 2)
        assert_length(self.obs._observers[True], 1)
        assert_length(self.obs._observers['c'], 0)


class MockObserver(Observer):

    def __init__(self, obs, event):
        self.obs = obs
        self.event = event
        self.watch(obs, event)
        self.has_watched = 0

    def handler(self, obs, event):
        assert_equal(obs, self.obs)
        assert_equal(event, self.event)
        self.has_watched += 1


class ObserverTestCase(TestCase):

    @setup
    def setup_observer(self):
        self.obs = Observable()

    def test_watch(self):
        event = "FIVE"
        handler = MockObserver(self.obs, event)

        self.obs.notify(event)
        assert_equal(handler.has_watched, 1)
        self.obs.notify("other event")
        assert_equal(handler.has_watched, 1)
        self.obs.notify(event)
        assert_equal(handler.has_watched, 2)





if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = proxy_test
from testify import run, TestCase, assert_equal, assert_raises, assert_in, setup

from tron.utils.proxy import CollectionProxy, AttributeProxy


class DummyTarget(object):

    def __init__(self, v):
        self.v = v

    def foo(self):
        return self.v

    @property
    def not_foo(self):
        return not self.v

    def equals(self, b, sometimes=False):
        if sometimes:
            return 'sometimes'
        return self.v == b


class DummyObject(object):

    def __init__(self, proxy):
        self.proxy = proxy

    def __getattr__(self, item):
        return self.proxy.perform(item)


class CollectionProxyTestCase(TestCase):

    @setup
    def setup_proxy(self):
        self.target_list = [DummyTarget(1), DummyTarget(2), DummyTarget(0)]
        self.proxy = CollectionProxy(lambda: self.target_list, [
            ('foo', any, True),
            ('not_foo', all, False),
            ('equals', lambda a: list(a), True)
        ])
        self.dummy = DummyObject(self.proxy)

    def test_add(self):
        self.proxy.add('foo', any, True)
        assert_equal(self.proxy._defs['foo'], (any, True))

    def test_perform(self):
        assert self.dummy.foo()
        assert not self.dummy.not_foo

    def test_perform_not_defined(self):
        assert_raises(AttributeError, self.dummy.proxy.perform, 'bar')

    def test_perform_with_params(self):
        assert_equal(self.proxy.perform('equals')(2), [False, True, False])
        sometimes = ['sometimes'] * 3
        assert_equal(self.proxy.perform('equals')(3, sometimes=True), sometimes)


class AttributeProxyTestCase(TestCase):

    @setup
    def setup_proxy(self):
        self.target = DummyTarget(1)
        self.proxy = AttributeProxy(self.target, ['foo', 'not_foo'])
        self.dummy = DummyObject(self.proxy)

    def test_add(self):
        self.proxy.add('bar')
        assert_in('bar', self.proxy._attributes)

    def test_perform(self):
        assert_equal(self.dummy.foo(), 1)
        assert_equal(self.dummy.not_foo, False)

    def test_perform_not_defined(self):
        assert_raises(AttributeError, self.dummy.proxy.perform, 'zzz')


if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = state_test
from testify import TestCase, setup, assert_equal, assert_raises
from testify.utils import turtle

from tron.utils import state
from tron.utils.state import NamedEventState

class StateMachineSimpleTestCase(TestCase):

    @setup
    def build_machine(self):
        self.state_green = NamedEventState('green')
        self.state_red = NamedEventState('red', true=self.state_green)

        self.machine = state.StateMachine(self.state_red)

    def test_transition_many(self):
        # Stay the same
        assert not self.machine.transition(False)
        assert_equal(self.machine.state, self.state_red)

        # Traffic has arrived
        self.machine.transition('true')
        assert_equal(self.machine.state, self.state_green)

        # Still traffic
        self.machine.transition('true')
        assert_equal(self.machine.state, self.state_green)

    def test_check(self):
        assert not self.machine.check(False)
        assert_equal(self.machine.check('true'), self.state_green)
        assert_equal(self.machine.state, self.state_red)

    def test_transition(self):
        handler = turtle.Turtle()
        self.machine.attach(True, handler)
        self.machine.transition('true')
        assert_equal(handler.handler.calls,
            [((self.machine, self.state_green),{})])

    def test_notify_delegate(self):
        delegate = turtle.Turtle()
        handler = turtle.Turtle()
        self.machine = state.StateMachine(self.state_red, delegate=delegate)
        self.machine.attach(True, handler)
        self.machine.transition('true')
        assert_equal(handler.handler.calls,
            [((delegate, self.state_green),{})])


class StateMachineMultiOptionTestCase(TestCase):
    @setup
    def build_machine(self):
        # Generalized rules of a conversation
        # If they are talking, we should listen
        # If they are listening, we should talk
        # If they are ignoring us we should get angry

        self.state_ignoring = NamedEventState('ignoring')
        self.state_talking = NamedEventState('talking')
        self.state_angry = NamedEventState('angry')

        self.state_listening = NamedEventState('listening',
            listening=self.state_talking
        )

        self.state_talking.update({
            'ignoring': self.state_angry,
            'talking': self.state_listening,
        })

        self.machine = state.StateMachine(self.state_listening)

    def test_transition_many(self):
        # Talking, we should listen
        self.machine.transition("talking")
        assert_equal(self.machine.state, self.state_listening)

        # Now be polite
        self.machine.transition("listening")
        assert_equal(self.machine.state, self.state_talking)

        self.machine.transition("listening")
        assert_equal(self.machine.state, self.state_talking)

        # But they are tired of us...
        self.machine.transition("ignoring")
        assert_equal(self.machine.state, self.state_angry)

    def test_transition_set(self):
        expected = set(['listening', 'talking', 'ignoring'])
        assert_equal(set(self.machine.transitions), expected)


class TraverseCircularTestCase(TestCase):
    @setup
    def build_machine(self):
        # Going around and around in circles
        self.state_telling_false = NamedEventState('telling_false')

        self.state_telling_truth = NamedEventState('telling_truth',
            true=self.state_telling_false)
        self.state_telling_false.update({'true': self.state_telling_truth})

        self.machine = state.StateMachine(self.state_telling_truth)

    def test_transition(self):
        assert_raises(state.CircularTransitionError, self.machine.transition, 'true')


class NamedEventByNameTestCase(TestCase):

    @setup
    def create_state_graph(self):
        self.start = STATE_A = state.NamedEventState("a")
        STATE_B = state.NamedEventState("b")
        self.end = STATE_C = state.NamedEventState("c", next=STATE_A)
        STATE_A['next'] = STATE_B
        STATE_B['next'] = STATE_C

    def test_match(self):
        assert_equal(state.named_event_by_name(self.start, "c"), self.end)

    def test_miss(self):
        assert_raises(ValueError, state.named_event_by_name, self.start, 'x')

########NEW FILE########
__FILENAME__ = timeutils_test
import datetime
from testify import TestCase, assert_equal, setup
from tests import testingutils

from tron.utils import timeutils
from tron.utils.timeutils import duration, macro_timedelta, DateArithmetic

class TimeDeltaTestCase(TestCase):

    @setup
    def make_dates(self):
        self.start_nonleap = datetime.datetime(year=2011, month=1, day=1)
        self.end_nonleap = datetime.datetime(year=2011, month=12, day=31)
        self.begin_feb_nonleap = datetime.datetime(year=2011, month=2, day=1)
        self.start_leap = datetime.datetime(year=2012, month=1, day=1)
        self.end_leap = datetime.datetime(year=2012, month=12, day=31)
        self.begin_feb_leap = datetime.datetime(year=2012, month=2, day=1)

    def check_delta(self, start, target, years=0, months=0, days=0):
        assert_equal(start + macro_timedelta(start, years=years, months=months, days=days),
                     target)

    def test_days(self):
        self.check_delta(self.start_nonleap,
                         datetime.datetime(year=2011, month=1, day=11),
                         days=10)
        self.check_delta(self.end_nonleap,
                         datetime.datetime(year=2012, month=1, day=10),
                         days=10)
        self.check_delta(self.start_leap,
                         datetime.datetime(year=2012, month=1, day=11),
                         days=10)
        self.check_delta(self.end_leap,
                         datetime.datetime(year=2013, month=1, day=10),
                         days=10)
        self.check_delta(self.begin_feb_nonleap,
                         datetime.datetime(year=2011, month=3, day=1),
                         days=28)
        self.check_delta(self.begin_feb_leap,
                         datetime.datetime(year=2012, month=3, day=1),
                         days=29)

    def test_months(self):
        self.check_delta(self.start_nonleap,
                         datetime.datetime(year=2011, month=11, day=1),
                         months=10)
        self.check_delta(self.end_nonleap,
                         datetime.datetime(year=2012, month=10, day=31),
                         months=10)
        self.check_delta(self.start_leap,
                         datetime.datetime(year=2012, month=11, day=1),
                         months=10)
        self.check_delta(self.end_leap,
                         datetime.datetime(year=2013, month=10, day=31),
                         months=10)
        self.check_delta(self.begin_feb_nonleap,
                         datetime.datetime(year=2011, month=12, day=1),
                         months=10)
        self.check_delta(self.begin_feb_leap,
                         datetime.datetime(year=2012, month=12, day=1),
                         months=10)

    def test_years(self):
        self.check_delta(self.start_nonleap,
                         datetime.datetime(year=2015, month=1, day=1),
                         years=4)
        self.check_delta(self.end_nonleap,
                         datetime.datetime(year=2015, month=12, day=31),
                         years=4)
        self.check_delta(self.start_leap,
                         datetime.datetime(year=2016, month=1, day=1),
                         years=4)
        self.check_delta(self.end_leap,
                         datetime.datetime(year=2016, month=12, day=31),
                         years=4)
        self.check_delta(self.begin_feb_nonleap,
                         datetime.datetime(year=2015, month=2, day=1),
                         years=4)
        self.check_delta(self.begin_feb_leap,
                         datetime.datetime(year=2016, month=2, day=1),
                         years=4)


class DurationTestCase(TestCase):

    @setup
    def setup_times(self):
        self.earliest = datetime.datetime(2012, 2, 1, 3, 0, 0)
        self.latest =   datetime.datetime(2012, 2, 1, 3, 20, 0)

    def test_duration(self):
        assert_equal(
            duration(self.earliest, self.latest),
            datetime.timedelta(0, 60*20)
        )

    def test_duration_no_end(self):
        delta = duration(self.earliest)
        assert delta.days >= 40

    def test_duration_no_start(self):
        assert_equal(duration(None), None)

class DeltaTotalSecondsTestCase(TestCase):

    def test(self):
        expected = 86702.004002999995
        delta = datetime.timedelta(*range(1,6))
        delta_seconds = timeutils.delta_total_seconds(delta)
        assert_equal(delta_seconds, expected)


class DateArithmeticTestCase(testingutils.MockTimeTestCase):

    # Set a date with days less then 28, otherwise some tests will fail
    # when run on days > 28.
    now = datetime.datetime(2012, 3, 20)

    def _cmp_date(self, item, dt):
        assert_equal(DateArithmetic.parse(item), dt.strftime("%Y-%m-%d"))

    def _cmp_day(self, item, dt):
        assert_equal(DateArithmetic.parse(item), dt.strftime("%d"))

    def _cmp_month(self, item, dt):
        assert_equal(DateArithmetic.parse(item), dt.strftime("%m"))

    def _cmp_year(self, item, dt):
        assert_equal(DateArithmetic.parse(item), dt.strftime("%Y"))

    def test_shortdate(self):
        self._cmp_date('shortdate', self.now)

    def test_shortdate_plus(self):
        for i in xrange(50):
            dt = self.now + datetime.timedelta(days=i)
            self._cmp_date('shortdate+%s' % i, dt)

    def test_shortdate_minus(self):
        for i in xrange(50):
            dt = self.now - datetime.timedelta(days=i)
            self._cmp_date('shortdate-%s' % i, dt)

    def test_day(self):
        self._cmp_day('day', self.now)

    def test_day_minus(self):
        for i in xrange(50):
            dt = self.now - datetime.timedelta(days=i)
            self._cmp_day('day-%s' % i, dt)

    def test_day_plus(self):
        for i in xrange(50):
            dt = self.now + datetime.timedelta(days=i)
            self._cmp_day('day+%s' % i, dt)

    def test_month(self):
        self._cmp_month('month', self.now)

    def test_month_plus(self):
        for i in xrange(50):
            dt = self.now + timeutils.macro_timedelta(self.now, months=i)
            self._cmp_month('month+%s' % i, dt)

    def test_month_minus(self):
        for i in xrange(50):
            dt = self.now - timeutils.macro_timedelta(self.now, months=i)
            self._cmp_month('month-%s' % i, dt)

    def test_year(self):
        self._cmp_year('year', self.now)

    def test_year_plus(self):
        for i in xrange(50):
            dt = self.now + timeutils.macro_timedelta(self.now, years=i)
            self._cmp_year('year+%s' % i, dt)

    def test_year_minus(self):
        for i in xrange(50):
            dt = self.now - timeutils.macro_timedelta(self.now, years=i)
            self._cmp_year('year-%s' % i, dt)

    def test_unixtime(self):
        timestamp = int(timeutils.to_timestamp(self.now))
        assert_equal(DateArithmetic.parse('unixtime'), timestamp)

    def test_unixtime_plus(self):
        timestamp = int(timeutils.to_timestamp(self.now)) + 100
        assert_equal(DateArithmetic.parse('unixtime+100'), timestamp)

    def test_unixtime_minus(self):
        timestamp = int(timeutils.to_timestamp(self.now)) - 99
        assert_equal(DateArithmetic.parse('unixtime-99'), timestamp)

    def test_daynumber(self):
        daynum = self.now.toordinal()
        assert_equal(DateArithmetic.parse('daynumber'), daynum)

    def test_daynumber_plus(self):
        daynum = self.now.toordinal() + 1
        assert_equal(DateArithmetic.parse('daynumber+1'), daynum)

    def test_daynumber_minus(self):
        daynum = self.now.toordinal() - 1
        assert_equal(DateArithmetic.parse('daynumber-1'), daynum)

    def test_bad_date_format(self):
        assert DateArithmetic.parse('~~') is None
########NEW FILE########
__FILENAME__ = tool_utils_test
import os
import tempfile
from testify import TestCase, run, assert_equal, setup, class_teardown, teardown
from tests.assertions import assert_raises
from tron.utils import tool_utils

class WorkingDirTestCase(TestCase):

    @setup
    def setup_cwd(self):
        self.cwd = os.getcwd()
        self.temp_dir = tempfile.mkdtemp()
        self.second_dir = tempfile.mkdtemp()

    @teardown
    def cleanup(self):
        os.rmdir(self.temp_dir)
        os.rmdir(self.second_dir)

    @class_teardown
    def check_for_test_pollution(self):
        assert_equal(self.cwd, os.getcwd())

    def test_working_dir(self):
        with tool_utils.working_dir(self.temp_dir):
            assert_equal(os.getcwd(), self.temp_dir)
        assert_equal(os.getcwd(), self.cwd)

    def test_working_dir_with_exception(self):
        def with_exc():
            with tool_utils.working_dir(self.temp_dir):
                assert_equal(os.getcwd(), self.temp_dir)
                raise Exception("oops")

        assert_raises(Exception, with_exc)
        assert_equal(os.getcwd(), self.cwd)

    def test_working_dir_nested(self):
        with tool_utils.working_dir(self.temp_dir):
            with tool_utils.working_dir(self.second_dir):
                assert_equal(os.getcwd(), self.second_dir)
            assert_equal(os.getcwd(), self.temp_dir)
        assert_equal(os.getcwd(), self.cwd)


if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = trontimespec_test
import datetime

from testify import run, assert_equal, TestCase
from tron.utils import trontimespec


class GetTimeTestCase(TestCase):

    def test_get_time(self):
        assert_equal(datetime.time(4, 15), trontimespec.get_time("4:15"))
        assert_equal(datetime.time(22, 59), trontimespec.get_time("22:59"))


    def test_get_time_invalid_time(self):
        assert not trontimespec.get_time("25:00")
        assert not trontimespec.get_time("22:61")


class TimeSpecificationTestCase(TestCase):

    def _cmp(self, start_time, expected):
        start_time = datetime.datetime(*start_time)
        expected = datetime.datetime(*expected)
        assert_equal(self.time_spec.get_match(start_time), expected)

    def test_get_match_months(self):
        self.time_spec = trontimespec.TimeSpecification(months=[1,5])
        self._cmp((2012, 3, 14), (2012, 5, 1))
        self._cmp((2012, 5, 22), (2012, 5, 23))
        self._cmp((2012, 12, 22), (2013, 1, 1))

    def test_get_match_monthdays(self):
        self.time_spec = trontimespec.TimeSpecification(monthdays=[10, 3, 3, 10])
        self._cmp((2012, 3, 14), (2012, 4, 3))
        self._cmp((2012, 3, 1), (2012, 3, 3))

    def test_get_match_weekdays(self):
        self.time_spec = trontimespec.TimeSpecification(weekdays=[2,3])
        self._cmp((2012, 3, 14), (2012, 3, 20))
        self._cmp((2012, 3, 20),(2012, 3, 21))

    def test_next_month_generator(self):
        time_spec = trontimespec.TimeSpecification(months=[2,5])
        gen = time_spec.next_month(datetime.datetime(2012, 3, 14))
        expected = [(5, 2012), (2, 2013), (5, 2013), (2, 2014)]
        assert_equal([gen.next() for _ in xrange(4)], expected)

    def test_next_day_monthdays(self):
        time_spec = trontimespec.TimeSpecification(monthdays=[5,10,15])
        gen = time_spec.next_day(14, 2012, 3)
        assert_equal(list(gen), [15])

        gen = time_spec.next_day(1, 2012, 3)
        assert_equal(list(gen), [5, 10, 15])

    def test_next_day_monthdays_with_last(self):
        time_spec = trontimespec.TimeSpecification(monthdays=[5,'LAST'])
        gen = time_spec.next_day(14, 2012, 3)
        assert_equal(list(gen), [31])

    def test_next_day_weekdays(self):
        time_spec = trontimespec.TimeSpecification(weekdays=[1,5])
        gen = time_spec.next_day(14, 2012, 3)
        assert_equal(list(gen), [16, 19, 23, 26, 30])

        gen = time_spec.next_day(1, 2012, 3)
        assert_equal(list(gen), [2, 5, 9, 12, 16, 19, 23, 26, 30])

    def test_next_day_weekdays_with_ordinals(self):
        time_spec = trontimespec.TimeSpecification(weekdays=[1,5], ordinals=[1,3])
        gen = time_spec.next_day(14, 2012, 3)
        assert_equal(list(gen), [16, 19])

        gen = time_spec.next_day(1, 2012, 3)
        assert_equal(list(gen), [2, 5, 16, 19])

    def test_next_time_timestr(self):
        time_spec = trontimespec.TimeSpecification(timestr="13:13")
        start_date = datetime.datetime(2012, 3, 14, 0, 15)
        time = time_spec.next_time(start_date, True)
        assert_equal(time, datetime.time(13, 13))

        start_date = datetime.datetime(2012, 3, 14, 13, 13)
        assert time_spec.next_time(start_date, True) is None
        time = time_spec.next_time(start_date, False)
        assert_equal(time, datetime.time(13, 13))

    def test_next_time_hours(self):
        time_spec = trontimespec.TimeSpecification(hours=[4,10])
        start_date = datetime.datetime(2012, 3, 14, 0, 15)
        time = time_spec.next_time(start_date, True)
        assert_equal(time, datetime.time(4, 0))

        start_date = datetime.datetime(2012, 3, 14, 13, 13)
        assert time_spec.next_time(start_date, True) is None
        time = time_spec.next_time(start_date, False)
        assert_equal(time, datetime.time(4, 0))

    def test_next_time_minutes(self):
        time_spec = trontimespec.TimeSpecification(minutes=[30, 20, 30], seconds=[0])
        start_date = datetime.datetime(2012, 3, 14, 0, 25)
        time = time_spec.next_time(start_date, True)
        assert_equal(time, datetime.time(0, 30))

        start_date = datetime.datetime(2012, 3, 14, 23, 30)
        assert time_spec.next_time(start_date, True) is None
        time = time_spec.next_time(start_date, False)
        assert_equal(time, datetime.time(0, 20))

    def test_next_time_hours_and_minutes_and_seconds(self):
        time_spec = trontimespec.TimeSpecification(
                    minutes=[20,30], hours=[1,5], seconds=[4,5])
        start_date = datetime.datetime(2012, 3, 14, 1, 25)
        time = time_spec.next_time(start_date, True)
        assert_equal(time, datetime.time(1, 30, 4))

        start_date = datetime.datetime(2012, 3, 14, 5, 30, 6)
        assert time_spec.next_time(start_date, True) is None
        time = time_spec.next_time(start_date, False)
        assert_equal(time, datetime.time(1, 20, 4))


if __name__ == "__main__":
    run()
########NEW FILE########
__FILENAME__ = action_dag_diagram
"""
 Create a graphviz diagram from a Tron Job configuration.

 Usage:
    python tools/action_dag_diagram.py -c <config> -n <job_name>

 This will create a file named <job_name>.dot
 You can create a diagram using:
    dot -Tpng -o <job_name>.png <job_name>.dot
"""
import optparse
from tron.config import manager, schema

def parse_args():
    parser = optparse.OptionParser()
    parser.add_option('-c', '--config', help="Tron configuration path.")
    parser.add_option('-n', '--name',
            help="Job name to graph. Also used as output filename.")
    parser.add_option('--namespace', default=schema.MASTER_NAMESPACE,
        help="Configuration namespace which contains the job.")
    opts, _ = parser.parse_args()

    if not opts.config:
        parser.error("A config filename is required.")
    if not opts.name:
        parser.error("A Job name is required.")
    return opts


def build_diagram(job_config):
    edges, nodes    = [], []

    for action in job_config.actions.itervalues():
        shape = 'invhouse' if not action.requires else 'rect'
        nodes.append("node [shape = %s]; %s" % (shape, action.name))
        for required_action in action.requires:
            edges.append("%s -> %s" % (required_action, action.name))

    return "digraph g{%s\n%s}" % ('\n'.join(nodes), '\n'.join(edges))

def get_job(config_container, namespace, job_name):
    if namespace not in config_container:
        raise ValueError("Unknown namespace: %s" % namespace)

    config = config_container[opts.namespace]
    if job_name not in config.jobs:
        raise ValueError("Could not find Job %s" % job_name)

    return config.jobs[job_name]


if __name__ == '__main__':
    opts = parse_args()

    config_manager  = manager.ConfigManager(opts.config)
    container       = config_manager.load()
    job_config      = get_job(container, opts.namespace, opts.name)
    graph           = build_diagram(job_config)

    with open('%s.dot' % opts.name, 'w') as fh:
        fh.write(graph)

########NEW FILE########
__FILENAME__ = inspect_serialized_state
"""Read a state file or db and create a report which summarizes it's contents.

Displays:
State configuration
Count of jobs
Count of services

Table of Jobs with start date of last run
Table of Services with state and instance count

"""
import optparse
from tron.config import manager
from tron.serialize.runstate import statemanager
from tron.utils import tool_utils


def parse_options():
    parser = optparse.OptionParser()
    parser.add_option("-c", "--config-path", help="Path to the configuration.")
    parser.add_option("-w", "--working-dir", default=".",
        help="Working directory to resolve relative paths.")
    opts, _ = parser.parse_args()

    if not opts.config_path:
        parser.error("A --config-path is required.")
    return opts


def get_container(config_path):
    config_manager  = manager.ConfigManager(config_path)
    return config_manager.load()


def get_state(container):
    config          = container.get_master().state_persistence
    state_manager   = statemanager.PersistenceManagerFactory.from_config(config)
    names           = container.get_job_and_service_names()
    return state_manager.restore(*names)


def format_date(date_string):
    return date_string.strftime("%Y-%m-%d %H:%M:%S") if date_string else None


def format_jobs(job_states):
    format = "%-30s %-8s %-5s %s\n"
    header = format % ("Name", "Enabled", "Runs", "Last Update")

    def max_run(item):
        start_time = filter(None, (run['start_time'] for run in item))
        return max(start_time) if start_time else None

    def build(name, job):
        start_times = (max_run(job_run['runs']) for job_run in job['runs'])
        start_times = filter(None, start_times)
        last_run = format_date(max(start_times)) if start_times else None
        return format % (name, job['enabled'], len(job['runs']), last_run)
    seq = sorted(build(*item) for item in job_states.iteritems())
    return header + "".join(seq)


def format_service(service_states):
    format = "%-30s %-8s %s\n"
    header = format % ("Name", "Enabled", "Instances")
    def build(name, service):
        return format % (name, service.get('enabled'), len(service['instances']))
    seq = sorted(build(*item) for item in service_states.iteritems())
    return header + "".join(seq)


def display_report(state_config, job_states, service_states):
    print "State Config: %s" % str(state_config)
    print "Total Jobs: %s" % len(job_states)
    print "Total Services: %s" % len(service_states)

    print "\n%s" % format_jobs(job_states)
    print "\n%s" % format_service(service_states)


def main(config_path, working_dir):
    container = get_container(config_path)
    config    = container.get_master().state_persistence
    with tool_utils.working_dir(working_dir):
        display_report(config, *get_state(container))


if __name__ == "__main__":
    opts = parse_options()
    main(opts.config_path, opts.working_dir)
########NEW FILE########
__FILENAME__ = migrate_config_0.2_to_0.3
"""
 Convert a 0.2.x Tron configuration file to the 0.3 format.

 Removes YAML anchors, references, and tags.
 Display warnings for NodePools under the nodes section.
 Display warnings for action requires sections that are not lists.

"""
import optparse
import re
import sys
import yaml

YAML_TAG_RE = re.compile(r'!\w+\b')

class Loader(yaml.Loader):
    """A YAML loader that does not clear its anchor mapping."""

    def compose_document(self):
        self.get_event()
        node = self.compose_node(None, None)
        self.get_event()
        return node


def strip_tags(source):
    """Remove YAML tags."""
    return YAML_TAG_RE.sub('', source)


def name_from_doc(doc):
    """Find the string idenfitier for a doc."""
    if 'name' in doc:
        return doc['name']

    # Special case for node without a name, their name defaults to their hostname
    if set(doc.keys()) == set(['hostname']):
        return doc['hostname']

    if set(doc.keys()) == set(['nodes']):
        raise ValueError("Please create a name for NodePool %s" % doc)

    raise ValueError("Could not find a name for %s" % doc)


def warn_node_pools(content):
    doc = yaml.safe_load(content)

    node_pools = [
        node_doc for node_doc in doc['nodes'] if 'nodes' in node_doc]

    if not node_pools:
        return

    print >>sys.stderr, ("\n\nNode Pools should be moved into a node_pools section." +
        " The following node pools were found:\n" +
        "\n".join(str(n) for n in node_pools))


def warn_requires_list(content):
    action_names = []
    doc = yaml.safe_load(content)

    for job in doc['jobs']:
        for action in job['actions']:
            if 'requires' not in action:
                continue

            if isinstance(action['requires'], list):
                continue

            action_names.append("%s.%s" % (job['name'], action['name']))

    if not action_names:
        return

    print >>sys.stderr, ("\n\nAction requires should be a list." +
        " The following actions have requires that are not lists:\n" +
        "\n".join(action_names))


def create_loader(content):
    """Create a loader, and have it create the document from content."""
    loader = Loader(content)
    loader.get_single_node()
    return loader


def build_anchor_mapping(content):
    """Return a map of anchors to the new name to use."""
    loader = create_loader(content)

    return dict(
        (anchor_name, name_from_doc(loader.construct_document(yaml_node)))
        for anchor_name, yaml_node in loader.anchors.iteritems()
    )


def update_references(content):
    anchor_mapping = build_anchor_mapping(content)

    key_length_func = lambda (k, v): len(k)
    anchors_by_length = sorted(
            anchor_mapping.iteritems(), key=key_length_func, reverse=True)
    for anchor_name, string_name in anchors_by_length:
        # Remove the anchors
        content = re.sub(r'\s*&%s ?' % anchor_name, '', content)
        # Update the reference to use the string identifier
        content = re.sub(r'\*%s\b' % anchor_name, '"%s"' % string_name, content)

    return content


def convert(source, dest):
    with open(source, 'r') as fh:
        content = fh.read()

    try:
        content = strip_tags(content)
        content = update_references(content)
        warn_node_pools(content)
        warn_requires_list(content)
    except yaml.scanner.ScannerError, e:
        print "Bad content: %s\n%s" % (e, content)

    with open(dest, 'w') as fh:
        fh.write(content)


if __name__ == "__main__":
    opt_parser = optparse.OptionParser()
    opt_parser.add_option('-s', dest="source", help="Source config filename.")
    opt_parser.add_option('-d', dest="dest", help="Destination filename.")
    opts, args = opt_parser.parse_args()
    if not opts.source or not opts.dest:
        print >>sys.stderr, "Source and destination filenames required."
        sys.exit(1)

    convert(opts.source, opts.dest)

########NEW FILE########
__FILENAME__ = migrate_config_0.5.1_to_0.5.2
"""Migrate a single configuration file (tron 0.5.1) to the new 0.5.2
multi-file format.

Usage:

python tools/migration/migrate_config_0.5.1_to_0.5.2.py \
    --source old_config_filename \
    --dest new_config_dir
"""
import optparse
import os

from tron.config import manager


def parse_options():
    parser = optparse.OptionParser()
    parser.add_option('-s', '--source', help="Path to old configuration file.")
    parser.add_option('-d', '--dest', help="Path to new configuration directory.")
    opts, _ = parser.parse_args()

    if not opts.source:
        parser.error("--source is required")
    if not opts.dest:
        parser.error("--dest is required")
    return opts


def main(source, dest):
    dest = os.path.abspath(dest)
    if not os.path.isfile(source):
        raise SystemExit("Error: Source (%s) is not a file" % source)
    if os.path.exists(dest):
        raise SystemExit("Error: Destination path (%s) already exists" % dest)
    old_config = manager.read_raw(source)
    manager.create_new_config(dest, old_config)


if __name__ == "__main__":
    opts = parse_options()
    main(opts.source, opts.dest)
########NEW FILE########
__FILENAME__ = migrate_state
"""
 Migrate a state file/database from one StateStore implementation to another. It
 may also be used to add namespace names to jobs/services when upgrading
 from pre-0.5.2 to version 0.5.2.

 Usage:
    python tools/migration/migrate_state.py \
        -s <old_config_dir> -d <new_config_dir> [ --namespace ]

 old_config.yaml and new_config.yaml should be configuration files with valid
 state_persistence sections. The state_persistence section configures the
 StateStore.

 Pre 0.5 state files can be read by the YamlStateStore. See the configuration
 documentation for more details on how to create state_persistence sections.
"""
import optparse
from tron.config import manager, schema
from tron.serialize import runstate
from tron.serialize.runstate.statemanager import PersistenceManagerFactory
from tron.utils import tool_utils


def parse_options():
    parser = optparse.OptionParser()
    parser.add_option('-s', '--source',
        help="The source configuration path which contains a state_persistence "
             "section configured for the state file/database.")
    parser.add_option('-d', '--dest',
        help="The destination configuration path which contains a "
             "state_persistence section configured for the state file/database.")
    parser.add_option('--source-working-dir',
        help="The working directory for source dir to resolve relative paths.")
    parser.add_option('--dest-working-dir',
        help="The working directory for dest dir to resolve relative paths.")
    parser.add_option('--namespace', action='store_true',
        help="Move jobs/services which are missing a namespace to the MASTER")

    opts, args = parser.parse_args()

    if not opts.source:
        parser.error("--source is required")
    if not opts.dest:
        parser.error("--dest is required.")

    return opts, args


def get_state_manager_from_config(config_path, working_dir):
    """Return a state manager from the configuration.
    """
    config_manager = manager.ConfigManager(config_path)
    config_container = config_manager.load()
    state_config = config_container.get_master().state_persistence
    with tool_utils.working_dir(working_dir):
        return PersistenceManagerFactory.from_config(state_config)


def get_current_config(config_path):
    config_manager = manager.ConfigManager(config_path)
    return config_manager.load()


def add_namespaces(state_data):
    return dict(('%s.%s' % (schema.MASTER_NAMESPACE, name), data)
                for (name, data) in state_data.iteritems())

def strip_namespace(names):
    return [name.split('.', 1)[1] for name in names]


def convert_state(opts):
    source_manager  = get_state_manager_from_config(opts.source, opts.source_working_dir)
    dest_manager    = get_state_manager_from_config(opts.dest, opts.dest_working_dir)
    container       = get_current_config(opts.source)

    msg = "Migrating state from %s to %s"
    print msg % (source_manager._impl, dest_manager._impl)

    job_names, service_names = container.get_job_and_service_names()
    if opts.namespace:
        job_names       = strip_namespace(job_names)
        service_names   = strip_namespace(service_names)

    job_states, service_states = source_manager.restore(
        job_names, service_names, skip_validation=True)
    source_manager.cleanup()

    if opts.namespace:
        job_states      = add_namespaces(job_states)
        service_states  = add_namespaces(service_states)

    for name, job in job_states.iteritems():
        dest_manager.save(runstate.JOB_STATE, name, job)
    print "Migrated %s jobs." % len(job_states)

    for name, service in service_states.iteritems():
        dest_manager.save(runstate.SERVICE_STATE, name, service)
    print "Migrated %s services." % len(service_states)

    dest_manager.cleanup()


if __name__ == "__main__":
    opts, _args = parse_options()
    convert_state(opts)
########NEW FILE########
__FILENAME__ = state_diagram
"""
 Create a graphviz state diagram from a tron.util.state.NamedEventState.

 Running this script will create two graphviz files:
    action.dot
    service.dot

"""
from tron.core.actionrun import ActionRun
from tron.core.service import ServiceInstance

def traverse_graph(starting_state, func=lambda f, a, t: None, seen_states=None):
    """Traverse the graph depth-first without cycling."""
    seen_states = seen_states or []
    seen_states.append(starting_state)

    for action, state in starting_state.iteritems():
        func(starting_state, action, state)
        if state in seen_states:
            continue
        traverse_graph(state, func, seen_states)

    return seen_states


def build_diagram(states, starting_state):
    """Build the diagram."""

    def build_node(state):
        return '%s[label="state: %s"];' % (
            state.name,
            state.name
        )

    def build_edges(starting_state):
        edges = []

        def collection_edges(from_state, act, to_state):
            edges.append((from_state.name, act, to_state.name))
        traverse_graph(starting_state, collection_edges)

        for edge in edges:
            yield '%s -> %s[label="%s"];' % (edge[0], edge[2], edge[1])

    return "digraph g{%s\n%s}" % (
        '\n'.join(build_node(state) for state in states),
        '\n'.join(build_edges(starting_state))
    )


def dot_from_starting_state(starting_state):
    state_data = traverse_graph(starting_state)
    return build_diagram(state_data, starting_state)

machines = {
    'action':           ActionRun.STATE_SCHEDULED,
    'service_instance': ServiceInstance.STATE_DOWN,
}

if __name__ == "__main__":
    for name, starting_state in machines.iteritems():
        with open("%s.dot" % name, 'w') as f:
            f.write(dot_from_starting_state(starting_state))

########NEW FILE########
__FILENAME__ = actioncommand
import logging
import os
from tron.config import schema
from tron.serialize import filehandler

from tron.utils import state, timeutils

log = logging.getLogger(__name__)


class ActionState(state.NamedEventState):
    pass


class CompletedActionCommand(object):
    """This is a null object for ActionCommand."""
    is_complete = True
    is_done = True
    is_failed = False

    @staticmethod
    def write_stderr(_):
        pass


class ActionCommand(object):
    """An ActionCommand encapsulates a runnable task that is passed to a node
    for execution.

    A Node calls:
      started   (when the command starts)
      exited    (when the command exits)
      write_<channel> (when output is received)
      done      (when the command is finished)
    """

    COMPLETE    = ActionState('complete')
    FAILSTART   = ActionState('failstart')
    EXITING     = ActionState('exiting', close=COMPLETE)
    RUNNING     = ActionState('running', exit=EXITING)
    PENDING     = ActionState('pending', start=RUNNING, exit=FAILSTART)

    STDOUT      = '.stdout'
    STDERR      = '.stderr'

    def __init__(self, id, command, serializer=None):
        self.id             = id
        self.command        = command
        self.machine        = state.StateMachine(self.PENDING, delegate=self)
        self.exit_status    = None
        self.start_time     = None
        self.end_time       = None
        self.stdout         = filehandler.NullFileHandle
        self.stderr         = filehandler.NullFileHandle
        if serializer:
            self.stdout         = serializer.open(self.STDOUT)
            self.stderr         = serializer.open(self.STDERR)

    @property
    def state(self):
        return self.machine.state

    @property
    def attach(self):
        return self.machine.attach

    def started(self):
        if not self.machine.check('start'):
            return False
        self.start_time = timeutils.current_timestamp()
        return self.machine.transition('start')

    def exited(self, exit_status):
        if not self.machine.check('exit'):
            return False
        self.end_time    = timeutils.current_timestamp()
        self.exit_status = exit_status
        return self.machine.transition('exit')

    def write_stderr(self, value):
        self.stderr.write(value)

    def write_stdout(self, value):
        self.stdout.write(value)

    def done(self):
        if not self.machine.check('close'):
            return False
        self.stdout.close()
        self.stderr.close()
        return self.machine.transition('close')

    def handle_errback(self, result):
        """Handle an unexpected error while being run.  This will likely be
        an interval error. Cleanup the state of this AcctionCommand and log
        something useful for debugging.
        """
        log.error("Unknown failure for ActionCommand run %s: %s\n%s",
                self.id, self.command, str(result))
        self.exited(result)
        self.done()

    @property
    def is_failed(self):
        return self.exit_status != 0

    @property
    def is_complete(self):
        """Complete implies done and success."""
        return self.machine.state == self.COMPLETE

    @property
    def is_done(self):
        """Done implies no more work will be done, but might not be success."""
        return self.machine.state in (self.COMPLETE, self.FAILSTART)

    def __repr__(self):
        return "ActionCommand %s %s: %s" % (self.id, self.command, self.state)


class StringBuffer(object):
    """An object which stores strings."""

    def __init__(self):
        self.buffer = []

    def write(self, msg):
        self.buffer.append(msg)

    def get_value(self):
        return ''.join(self.buffer).rstrip()

    def close(self):
        pass


class StringBufferStore(object):
    """A serializer object which can be passed to ActionCommand as a
    serializer, but stores streams in memory.
    """
    def __init__(self):
        self.buffers = {}

    def open(self, name):
        return self.buffers.setdefault(name, StringBuffer())

    def get_stream(self, name):
        return self.buffers[name].get_value()

    def clear(self):
        self.buffers.clear()


class NoActionRunnerFactory(object):
    """Action runner factory that does not wrap the action run command."""

    @classmethod
    def create(cls, id, command, serializer):
        return ActionCommand(id, command, serializer)

    @classmethod
    def build_stop_action_command(cls, _id, _command):
        """It is not possible to stop action commands without a runner."""
        raise NotImplementedError("An action_runner is required to stop.")


class SubprocessActionRunnerFactory(object):
    """Run actions by wrapping them in `action_runner.py`."""

    runner_exec_name =  "action_runner.py"
    status_exec_name =  "action_status.py"

    def __init__(self, status_path, exec_path):
        self.status_path = status_path
        self.exec_path = exec_path

    @classmethod
    def from_config(cls, config):
        return cls(config.remote_status_path, config.remote_exec_path)

    def create(self, id, command, serializer):
        command = self.build_command(id, command, self.runner_exec_name)
        return ActionCommand(id, command, serializer)

    def build_command(self, id, command, exec_name):
        status_path = os.path.join(self.status_path, id)
        runner_path = os.path.join(self.exec_path, exec_name)
        return '''%s "%s" "%s"''' % (runner_path, status_path, command)

    def build_stop_action_command(self, id, command):
        command = self.build_command(id, command, self.status_exec_name)
        run_id = '%s.%s' % (id, command)
        return ActionCommand(run_id, command, StringBufferStore())

    def __eq__(self, other):
        return (self.__class__ == other.__class__ and
            self.status_path == other.status_path and
            self.exec_path == other.exec_path)

    def __ne__(self, other):
        return not self == other


def create_action_runner_factory_from_config(config):
    """A factory-factory method which returns a callable that can be used to
    create ActionCommand objects. The factory definition should match the
    constructor for ActionCommand.
    """
    if not config:
        return NoActionRunnerFactory

    if config.runner_type not in schema.ActionRunnerTypes:
        raise ValueError("Unknown runner type: %s", config.runner_type)

    if config.runner_type == schema.ActionRunnerTypes.none:
        return NoActionRunnerFactory

    if config.runner_type == schema.ActionRunnerTypes.subprocess:
        return SubprocessActionRunnerFactory.from_config(config)

########NEW FILE########
__FILENAME__ = adapter
"""
 Classes which create external representations of core objects. This allows
 the core objects to remain decoupled from the API and clients. These classes
 act as an adapter between the data format api clients expect, and the internal
 data of an object.
"""
import functools
import urllib
from tron import actioncommand
from tron import scheduler
from tron.serialize import filehandler
from tron.utils import timeutils


class ReprAdapter(object):
    """Creates a dictionary from the given object for a set of rules."""

    field_names = []
    translated_field_names = []

    def __init__(self, internal_obj):
        self._obj               = internal_obj
        self.fields             = self._get_field_names()
        self.translators        = self._get_translation_mapping()

    def _get_field_names(self):
        return self.field_names

    def _get_translation_mapping(self):
        return dict(
            (field_name, getattr(self, 'get_%s' % field_name))
            for field_name in self.translated_field_names)

    def get_repr(self):
        repr_data = dict(
                (field, getattr(self._obj, field)) for field in self.fields)
        translated = dict(
                (field, func()) for field, func in self.translators.iteritems())
        repr_data.update(translated)
        return repr_data


def adapt_many(adapter_class, seq, *args, **kwargs):
    return [adapter_class(item, *args, **kwargs).get_repr() for item in seq]


def toggle_flag(flag_name):
    """Create a decorator which checks if flag_name is true before running
    the wrapped function. If False returns None.
    """

    def wrap(f):
        @functools.wraps(f)
        def wrapper(self, *args, **kwargs):
            if getattr(self, flag_name):
                return f(self, *args, **kwargs)
            return None
        return wrapper
    return wrap


class RunAdapter(ReprAdapter):
    """Base class for JobRun and ActionRun adapters."""

    def get_state(self):
        return self._obj.state.name

    def get_node(self):
        return NodeAdapter(self._obj.node).get_repr()

    def get_duration(self):
        duration = timeutils.duration(self._obj.start_time, self._obj.end_time)
        return str(duration or '')


class ActionRunAdapter(RunAdapter):
    """Adapt a JobRun and an Action name to an external representation of an
    ActionRun.
    """

    field_names = [
            'id',
            'start_time',
            'end_time',
            'exit_status',
            'action_name'
    ]

    translated_field_names = [
            'state',
            'node',
            'command',
            'raw_command',
            'requirements',
            'stdout',
            'stderr',
            'duration',
            'job_name',
            'run_num',
    ]

    def __init__(self, action_run, job_run=None,
                 max_lines=10, include_stdout=False, include_stderr=False):
        super(ActionRunAdapter, self).__init__(action_run)
        self.job_run            = job_run
        self.max_lines          = max_lines or None
        self.include_stdout     = include_stdout
        self.include_stderr     = include_stderr

    def get_raw_command(self):
        return self._obj.bare_command

    def get_command(self):
        return self._obj.rendered_command

    @toggle_flag('job_run')
    def get_requirements(self):
        action_name = self._obj.action_name
        required = self.job_run.action_graph.get_required_actions(action_name)
        return [act.name for act in required]

    def _get_serializer(self):
        return filehandler.OutputStreamSerializer(self._obj.output_path)

    @toggle_flag('include_stdout')
    def get_stdout(self):
        filename = actioncommand.ActionCommand.STDOUT
        return self._get_serializer().tail(filename, self.max_lines)

    @toggle_flag('include_stderr')
    def get_stderr(self):
        filename = actioncommand.ActionCommand.STDERR
        return self._get_serializer().tail(filename, self.max_lines)

    def get_job_name(self):
        return self._obj.job_run_id.rsplit('.', 1)[-2]

    def get_run_num(self):
        return self._obj.job_run_id.split('.')[-1]


class ActionGraphAdapter(object):

    def __init__(self, action_graph):
        self.action_graph = action_graph

    def get_repr(self):
        def build(action):
            return {
                'name':         action.name,
                'command':      action.command,
                'dependent':    [dep.name for dep in action.dependent_actions],
            }

        return [build(action) for action in self.action_graph.get_actions()]

class ActionRunGraphAdapter(object):

    def __init__(self, action_run_collection):
        self.action_runs = action_run_collection

    def get_repr(self):
        def build(action_run):
            deps = self.action_runs.action_graph.get_dependent_actions(
                action_run.action_name)
            return {
                'id':           action_run.id,
                'name':         action_run.action_name,
                'command':      action_run.rendered_command,
                'raw_command':  action_run.bare_command,
                'state':        action_run.state.name,
                'start_time':   action_run.start_time,
                'end_time':     action_run.end_time,
                'dependent':    [dep.name for dep in deps],
            }

        return [build(action_run) for action_run in self.action_runs]


class JobRunAdapter(RunAdapter):

    field_names = [
       'id',
        'run_num',
        'run_time',
        'start_time',
        'end_time',
        'manual',
        'job_name',
    ]
    translated_field_names = [
        'state',
        'node',
        'duration',
        'url',
        'runs',
        'action_graph',
    ]

    def __init__(self, job_run,
            include_action_runs=False,
            include_action_graph=False):
        super(JobRunAdapter, self).__init__(job_run)
        self.include_action_runs = include_action_runs
        self.include_action_graph = include_action_graph

    def get_url(self):
        return '/jobs/%s/%s' % (self._obj.job_name, self._obj.run_num)

    @toggle_flag('include_action_runs')
    def get_runs(self):
        return adapt_many(ActionRunAdapter, self._obj.action_runs, self._obj)

    @toggle_flag('include_action_graph')
    def get_action_graph(self):
        return ActionRunGraphAdapter(self._obj.action_runs).get_repr()

class JobAdapter(ReprAdapter):

    field_names = ['status', 'all_nodes', 'allow_overlap', 'queueing']
    translated_field_names = [
        'name',
        'scheduler',
        'action_names',
        'node_pool',
        'last_success',
        'next_run',
        'url',
        'runs',
        'max_runtime',
        'action_graph',
    ]

    def __init__(self, job,
             include_job_runs=False,
             include_action_runs=False,
             include_action_graph=True,
             num_runs=None):
        super(JobAdapter, self).__init__(job)
        self.include_job_runs     = include_job_runs
        self.include_action_runs  = include_action_runs
        self.include_action_graph = include_action_graph
        self.num_runs             = num_runs

    def get_name(self):
        return self._obj.get_name()

    def get_scheduler(self):
        return SchedulerAdapter(self._obj.scheduler).get_repr()

    def get_action_names(self):
        return self._obj.action_graph.names

    def get_node_pool(self):
        return NodePoolAdapter(self._obj.node_pool).get_repr()

    def get_last_success(self):
        last_success = self._obj.runs.last_success
        return last_success.end_time if last_success else None

    def get_next_run(self):
        next_run = self._obj.runs.next_run
        return next_run.run_time if next_run else None

    def get_url(self):
        return '/jobs/%s' % urllib.quote(self._obj.get_name())

    @toggle_flag('include_job_runs')
    def get_runs(self):
        runs = adapt_many(JobRunAdapter, self._obj.runs, self.include_action_runs)
        return runs[:self.num_runs or None]

    def get_max_runtime(self):
        return str(self._obj.max_runtime)

    @toggle_flag('include_action_graph')
    def get_action_graph(self):
        return ActionGraphAdapter(self._obj.action_graph).get_repr()


class JobIndexAdapter(ReprAdapter):

    translated_field_names = ['name', 'actions']

    def get_name(self):
        return self._obj.get_name()

    def get_actions(self):
        def adapt_run(run):
            return {'name': run.action_name, 'command': run.bare_command}

        job_run = self._obj.get_runs().get_newest()
        if not job_run:
            return []
        return [adapt_run(action_run) for action_run in job_run.action_runs]


class SchedulerAdapter(ReprAdapter):

    translated_field_names = ['value', 'type', 'jitter']

    def get_value(self):
        return self._obj.get_value()

    def get_type(self):
        return self._obj.get_name()

    def get_jitter(self):
        return scheduler.get_jitter_str(self._obj.get_jitter())


class ServiceAdapter(ReprAdapter):

    field_names = ['name', 'enabled']
    translated_field_names = [
        'count',
        'url',
        'state',
        'command',
        'pid_filename',
        'instances',
        'node_pool',
        'live_count',
        'monitor_interval',
        'restart_delay',
        'events']

    def __init__(self, service, include_events=False):
        super(ServiceAdapter, self).__init__(service)
        self.include_events = include_events

    def get_url(self):
        return "/services/%s" % urllib.quote(self._obj.get_name())

    def get_count(self):
        return self._obj.config.count

    def get_state(self):
        return self._obj.get_state()

    def get_command(self):
        return self._obj.config.command

    def get_pid_filename(self):
        return self._obj.config.pid_file

    def get_instances(self):
        return adapt_many(ServiceInstanceAdapter, self._obj.instances)

    def get_node_pool(self):
        return NodePoolAdapter(self._obj.instances.node_pool).get_repr()

    def get_live_count(self):
        return len(self._obj.instances)

    def get_monitor_interval(self):
        return self._obj.config.monitor_interval

    def get_restart_delay(self):
        return self._obj.config.restart_delay

    @toggle_flag('include_events')
    def get_events(self):
        events = adapt_many(EventAdapter, self._obj.event_recorder.list())
        return events[:self.include_events]


class ServiceInstanceAdapter(ReprAdapter):

    field_names = ['id', 'failures']
    translated_field_names = ['state', 'node']

    def get_state(self):
        return str(self._obj.get_state())

    def get_node(self):
        return NodeAdapter(self._obj.node).get_repr()


class EventAdapter(ReprAdapter):

    field_names = ['name', 'entity', 'time']
    translated_field_names = ['level']

    def get_level(self):
        return self._obj.level.label


class NodeAdapter(ReprAdapter):
    field_names = ['name', 'hostname', 'username', 'port']


class NodePoolAdapter(ReprAdapter):
    translated_field_names = ['name', 'nodes']

    def get_name(self):
        return self._obj.get_name()

    def get_nodes(self):
        return adapt_many(NodeAdapter, self._obj.get_nodes())

########NEW FILE########
__FILENAME__ = controller
"""
Web Controllers for the API.
"""
import logging
import pkg_resources
import tron
from tron.config import schema


log = logging.getLogger(__name__)


class UnknownCommandError(Exception):
    """Exception raised when a controller received an unknown command."""


class JobCollectionController(object):

    def __init__(self, job_collection):
        self.job_collection = job_collection

    def handle_command(self, command):
        if command == 'disableall':
            self.job_collection.disable()
            return "Disabled all jobs."

        if command == 'enableall':
            self.job_collection.enable()
            return "Enabled all jobs."

        raise UnknownCommandError("Unknown command %s" % command)


class ActionRunController(object):

    mapped_commands = set(
        ('start', 'success', 'cancel', 'fail', 'skip', 'stop', 'kill'))

    def __init__(self, action_run, job_run):
        self.action_run = action_run
        self.job_run    = job_run

    def handle_command(self, command):
        if command not in self.mapped_commands:
            raise UnknownCommandError("Unknown command %s" % command)

        if command == 'start' and self.job_run.is_scheduled:
            return ("Action run can not be started if it's job run is still "
                    "scheduled.")

        if command in ('stop', 'kill'):
            return self.handle_termination(command)

        if getattr(self.action_run, command)():
            msg = "%s now in state %s"
            return msg % (self.action_run, self.action_run.state)

        msg = "Failed to %s on %s. State is %s."
        return msg % (command, self.action_run, self.action_run.state)

    def handle_termination(self, command):
        try:
            getattr(self.action_run, command)()
            msg = "Attempting to %s %s"
            return msg % (command, self.action_run)
        except NotImplementedError, e:
            msg = "Failed to %s: %s"
            return msg % (command, e)



class JobRunController(object):

    mapped_commands = set(('start', 'success', 'cancel', 'fail', 'stop'))

    def __init__(self, job_run, job_scheduler):
        self.job_run       = job_run
        self.job_scheduler = job_scheduler

    def handle_command(self, command):
        if command == 'restart':
            runs = self.job_scheduler.manual_start(self.job_run.run_time)
            return "Created %s" % ",".join(str(run) for run in runs)

        if command in self.mapped_commands:
            if getattr(self.job_run, command)():
                return "%s now in state %s" % (self.job_run, self.job_run.state)

            msg = "Failed to %s, %s in state %s"
            return msg % (command, self.job_run, self.job_run.state)

        raise UnknownCommandError("Unknown command %s" % command)


class JobController(object):

    def __init__(self, job_scheduler):
        self.job_scheduler = job_scheduler

    def handle_command(self, command, run_time=None):
        if command == 'enable':
            self.job_scheduler.enable()
            return "%s is enabled" % self.job_scheduler.get_job()

        elif command == 'disable':
            self.job_scheduler.disable()
            return "%s is disabled" % self.job_scheduler.get_job()

        elif command == 'start':
            runs = self.job_scheduler.manual_start(run_time=run_time)
            return "Created %s" % ",".join(str(run) for run in runs)

        raise UnknownCommandError("Unknown command %s" % command)


class ServiceInstanceController(object):

    def __init__(self, service_instance):
        self.service_instance = service_instance

    def handle_command(self, command):
        error_msg = "Failed to %s from state %s."
        if command == 'stop':
            if self.service_instance.stop():
                return "%s stopping." % self.service_instance
            return error_msg % (command, self.service_instance.get_state())

        if command == 'start':
            if self.service_instance.start():
                return "%s starting." % self.service_instance
            return error_msg % (command, self.service_instance.get_state())

        raise UnknownCommandError("Unknown command %s" % command)


class ServiceController(object):

    def __init__(self, service):
        self.service = service

    def handle_command(self, command):
        if command == 'stop':
            self.service.disable()
            return "%s stopping." % self.service

        if command == 'start':
            self.service.enable()
            return "%s starting." % self.service

        if command == 'kill':
            self.service.disable(force=True)
            return "Killing %s." % self.service

        raise UnknownCommandError("Unknown command %s" % command)


def format_seq(seq):
    return "\n# ".join(sorted(seq))

def format_mapping(mapping):
    seq = ("%-30s: %s" % (k, v) for k, v in sorted(mapping.iteritems()))
    return format_seq(seq)


class ConfigController(object):
    """Control config. Return config contents and accept updated configuration
    from the API.
    """

    TEMPLATE_FILE = 'named_config_template.yaml'

    TEMPLATE = pkg_resources.resource_string(tron.__name__, TEMPLATE_FILE)

    HEADER_END = TEMPLATE.split('\n')[-2] + '\n'

    DEFAULT_NAMED_CONFIG =  "\njobs:\n\nservices:\n"

    def __init__(self, mcp):
        self.mcp = mcp
        self.config_manager = mcp.get_config_manager()

    def render_template(self, config_content):
        container = self.config_manager.load()
        command_context = container.get_master().command_context or {}
        context = {
            'node_names': format_seq(container.get_node_names()),
            'command_context': format_mapping(command_context)}
        return self.TEMPLATE % context + config_content

    def strip_header(self, name, content):
        if name == schema.MASTER_NAMESPACE:
            return content

        header_end_index = content.find(self.HEADER_END)
        if header_end_index > -1:
            return content[header_end_index + len(self.HEADER_END):]
        return content

    def _get_config_content(self, name):
        if name not in self.config_manager:
            return self.DEFAULT_NAMED_CONFIG
        return self.config_manager.read_raw_config(name)

    def read_config(self, name, add_header=True):
        config_content = self._get_config_content(name)
        config_hash = self.config_manager.get_hash(name)

        if name != schema.MASTER_NAMESPACE and add_header:
            config_content = self.render_template(config_content)
        return dict(config=config_content, hash=config_hash)

    def update_config(self, name, content, config_hash):
        """Update a configuration fragment and reload the MCP."""
        if self.config_manager.get_hash(name) != config_hash:
            return "Configuration has changed. Please try again."
        content = self.strip_header(name, content)
        try:
            self.config_manager.write_config(name, content)
            self.mcp.reconfigure()
        except Exception, e:
            log.error("Configuration update failed: %s" % e)
            return str(e)

    def get_namespaces(self):
        return self.config_manager.get_namespaces()

########NEW FILE########
__FILENAME__ = requestargs
"""Functions for returning validated values from a twisted.web.Request object.
"""
import datetime


DATE_FORMAT = "%Y-%m-%d %H:%M:%S"


def get_integer(request, key):
    """Returns the first value in the request args for the given key, if that
    value is an integer. Otherwise returns None.
    """
    if not request.args or key not in request.args:
        return None

    value = request.args[key][0]
    if not value.isdigit():
        return None
    return int(value)


def get_string(request, key):
    """Returns the first value in the request args for a given key."""
    if not request.args or key not in request.args:
        return None
    return request.args[key][0]


def get_bool(request, key):
    """Returns True if the key exists and is truthy in the request args."""
    return bool(get_integer(request, key))


def get_datetime(request, key):
    """Returns the first value in the request args for a given key. Casts to
    a datetime. Returns None if the value cannot be converted to datetime.
    """
    if not request.args or key not in request.args:
        return False
    try:
        return datetime.datetime.strptime(request.args[key][0], DATE_FORMAT)
    except ValueError:
        return None
########NEW FILE########
__FILENAME__ = resource
"""
Web Services Interface used by command-line clients and web frontend to
view current state, event history and send commands to trond.
"""

import datetime
import logging

try:
    import simplejson as json
    _silence_pyflakes = [json]
except ImportError:
    import json

from twisted.web import http, resource, static, server

from tron import event
from tron.api import adapter, controller
from tron.api import requestargs


log = logging.getLogger(__name__)


class JSONEncoder(json.JSONEncoder):
    """Custom JSON for certain objects"""

    def default(self, o):
        if isinstance(o, datetime.datetime):
            return o.strftime("%Y-%m-%d %H:%M:%S")

        if isinstance(o, datetime.date):
            return o.isoformat()

        return super(JSONEncoder, self).default(o)


def respond(request, response_dict, code=http.OK, headers=None):
    """Helper to generate a json response"""
    request.setResponseCode(code)
    request.setHeader('content-type', 'text/json')
    for key, val in (headers or {}).iteritems():
        request.setHeader(key, val)
    return json.dumps(response_dict, cls=JSONEncoder) if response_dict else ""


def handle_command(request, api_controller, obj, **kwargs):
    """Handle a request to perform a command."""
    command = requestargs.get_string(request, 'command')
    log.info("Handling '%s' request on %s", command, obj)
    try:
        response = api_controller.handle_command(command, **kwargs)
        return respond(request, {'result': response})
    except controller.UnknownCommandError, e:
        log.warning("Unknown command %s for %s", command, obj)
        return respond(request, {'error': str(e)}, code=http.NOT_IMPLEMENTED)


def resource_from_collection(collection, name, child_resource):
    """Return a child resource from a collection by name.  If no item is found,
    return NoResource.
    """
    item = collection.get_by_name(name)
    if item is None:
        return resource.NoResource("Cannot find child %s" % name)
    return child_resource(item)


class ActionRunResource(resource.Resource):

    isLeaf = True

    def __init__(self, action_run, job_run):
        resource.Resource.__init__(self)
        self.action_run = action_run
        self.job_run    = job_run
        self.controller = controller.ActionRunController(action_run, job_run)

    def render_GET(self, request):
        run_adapter = adapter.ActionRunAdapter(
            self.action_run,
            self.job_run,
            requestargs.get_integer(request, 'num_lines'),
            include_stdout=requestargs.get_bool(request, 'include_stdout'),
            include_stderr=requestargs.get_bool(request, 'include_stderr'))
        return respond(request, run_adapter.get_repr())

    def render_POST(self, request):
        return handle_command(request, self.controller, self.action_run)


class JobRunResource(resource.Resource):

    def __init__(self, job_run, job_scheduler):
        resource.Resource.__init__(self)
        self.job_run       = job_run
        self.job_scheduler = job_scheduler
        self.controller    = controller.JobRunController(job_run, job_scheduler)

    def getChild(self, action_name, _):
        if not action_name:
            return self
        if action_name == '_events':
            return EventResource(self.job_run.id)
        if action_name in self.job_run.action_runs:
            action_run = self.job_run.action_runs[action_name]
            return ActionRunResource(action_run, self.job_run)

        msg = "Cannot find action %s for %s"
        return resource.NoResource(msg % (action_name, self.job_run))

    def render_GET(self, request):
        include_runs = requestargs.get_bool(request, 'include_action_runs')
        include_graph = requestargs.get_bool(request, 'include_action_graph')
        run_adapter = adapter.JobRunAdapter(self.job_run,
            include_action_runs=include_runs,
            include_action_graph=include_graph)
        return respond(request, run_adapter.get_repr())

    def render_POST(self, request):
        return handle_command(request, self.controller, self.job_run)


def is_negative_int(string):
    return string.startswith('-') and string[1:].isdigit()


class JobResource(resource.Resource):

    def __init__(self, job_scheduler):
        resource.Resource.__init__(self)
        self.job_scheduler = job_scheduler
        self.controller    = controller.JobController(job_scheduler)

    def get_run_from_identifier(self, run_id):
        job_runs = self.job_scheduler.get_job_runs()
        if run_id.upper() == 'HEAD':
            return job_runs.get_newest()
        if run_id.isdigit():
            return job_runs.get_run_by_num(int(run_id))
        if is_negative_int(run_id):
            return job_runs.get_run_by_index(int(run_id))
        return job_runs.get_run_by_state_short_name(run_id)

    def getChild(self, run_id, _):
        if not run_id:
            return self
        if run_id == '_events':
            return EventResource(self.job_scheduler.get_name())

        run = self.get_run_from_identifier(run_id)
        if run:
            return JobRunResource(run, self.job_scheduler)

        job = self.job_scheduler.get_job()
        if run_id in job.action_graph.names:
            action_runs = job.runs.get_action_runs(run_id)
            return ActionRunHistoryResource(action_runs)
        msg = "Cannot find job run %s for %s"
        return resource.NoResource(msg % (run_id, job))

    def render_GET(self, request):
        include_action_runs = requestargs.get_bool(request, 'include_action_runs')
        include_graph = requestargs.get_bool(request, 'include_action_graph')
        num_runs = requestargs.get_integer(request, 'num_runs')
        job_adapter = adapter.JobAdapter(
                self.job_scheduler.get_job(),
                include_job_runs=True,
                include_action_runs=include_action_runs,
                include_action_graph=include_graph,
                num_runs=num_runs)
        return respond(request, job_adapter.get_repr())

    def render_POST(self, request):
        run_time = requestargs.get_datetime(request, 'run_time')
        return handle_command(
            request,
            self.controller,
            self.job_scheduler,
            run_time=run_time)


class ActionRunHistoryResource(resource.Resource):

    isLeaf = True

    def __init__(self, action_runs):
        resource.Resource.__init__(self)
        self.action_runs = action_runs

    def render_GET(self, request):
        return respond(request,
            adapter.adapt_many(adapter.ActionRunAdapter, self.action_runs))


class JobCollectionResource(resource.Resource):

    def __init__(self, job_collection):
        self.job_collection = job_collection
        self.controller     = controller.JobCollectionController(job_collection)
        resource.Resource.__init__(self)

    def getChild(self, name, request):
        if not name:
            return self
        return resource_from_collection(self.job_collection, name, JobResource)

    def get_data(self, include_job_run=False, include_action_runs=False):
        return adapter.adapt_many(adapter.JobAdapter,
            self.job_collection.get_jobs(),
            include_job_run,
            include_action_runs,
            num_runs=5)

    def get_job_index(self):
        jobs = adapter.adapt_many(
            adapter.JobIndexAdapter, self.job_collection.get_jobs())
        return dict((job['name'], job['actions']) for job in jobs)

    def render_GET(self, request):
        include_job_runs = requestargs.get_bool(request, 'include_job_runs')
        include_action_runs = requestargs.get_bool(request, 'include_action_runs')
        output = dict(jobs=self.get_data(include_job_runs, include_action_runs))
        return respond(request, output)

    def render_POST(self, request):
        return handle_command(request, self.controller, self.job_collection)


class ServiceInstanceResource(resource.Resource):

    isLeaf = True

    def __init__(self, service_instance):
        resource.Resource.__init__(self)
        self.service_instance = service_instance
        self.controller = controller.ServiceInstanceController(service_instance)

    def render_POST(self, request):
        return handle_command(request, self.controller, self.service_instance)


class ServiceResource(resource.Resource):
    """A resource that describes a particular service"""
    def __init__(self, service):
        resource.Resource.__init__(self)
        self.service    = service
        self.controller = controller.ServiceController(self.service)

    def getChild(self, name, _):
        if not name:
            return self
        if name == '_events':
            return EventResource(str(self.service))

        number = int(name) if name.isdigit() else None
        instance = self.service.instances.get_by_number(number)
        if instance:
            return ServiceInstanceResource(instance)

        return resource.NoResource("Cannot find service instance: %s" % name)

    def render_GET(self, request):
        include_events = requestargs.get_integer(request, 'include_events')
        response = adapter.ServiceAdapter(self.service,
            include_events=include_events).get_repr()
        return respond(request, response)

    def render_POST(self, request):
        return handle_command(request, self.controller, self.service)


class ServiceCollectionResource(resource.Resource):
    """Resource for ServiceCollection."""

    def __init__(self, service_collection):
        self.collection = service_collection
        resource.Resource.__init__(self)

    def getChild(self, name, _):
        if not name:
            return self
        return resource_from_collection(self.collection, name, ServiceResource)

    def get_data(self):
        return adapter.adapt_many(adapter.ServiceAdapter, self.collection)

    def get_service_index(self):
        return self.collection.get_names()

    def render_GET(self, request):
        return respond(request, dict(services=self.get_data()))


class ConfigResource(resource.Resource):
    """Resource for configuration changes"""

    isLeaf = True

    def __init__(self, master_control):
        self.controller = controller.ConfigController(master_control)
        resource.Resource.__init__(self)

    def get_config_index(self):
        return self.controller.get_namespaces()

    def render_GET(self, request):
        config_name = requestargs.get_string(request, 'name')
        no_header = requestargs.get_bool(request, 'no_header')
        if not config_name:
            return respond(request, {'error': "'name' for config is required."})
        response = self.controller.read_config(
                config_name, add_header=not no_header)
        return respond(request, response)

    def render_POST(self, request):
        config_content = requestargs.get_string(request, 'config')
        name = requestargs.get_string(request, 'name')
        config_hash = requestargs.get_string(request, 'hash')
        log.info("Handling reconfigure request: %s, %s" % (name, config_hash))
        if not name:
            return respond(request, {'error': "'name' for config is required."})

        response = {'status': "Active"}
        error = self.controller.update_config(name, config_content, config_hash)
        if error:
            response['error'] = error
        return respond(request, response)


class StatusResource(resource.Resource):

    isLeaf = True

    def __init__(self, master_control):
        self._master_control = master_control
        resource.Resource.__init__(self)

    def render_GET(self, request):
        return respond(request, {'status': "I'm alive."})


class EventResource(resource.Resource):

    isLeaf = True

    def __init__(self, entity_name):
        resource.Resource.__init__(self)
        self.entity_name = entity_name

    def render_GET(self, request):
        recorder      = event.get_recorder(self.entity_name)
        response_data = adapter.adapt_many(adapter.EventAdapter, recorder.list())
        return respond(request, dict(data=response_data))


class ApiRootResource(resource.Resource):

    def __init__(self, mcp):
        self._master_control = mcp
        resource.Resource.__init__(self)

        # Setup children
        self.putChild('jobs',
            JobCollectionResource(mcp.get_job_collection()))
        self.putChild('services',
            ServiceCollectionResource(mcp.get_service_collection()))
        self.putChild('config',   ConfigResource(mcp))
        self.putChild('status',   StatusResource(mcp))
        self.putChild('events',   EventResource(''))
        self.putChild('', self)

    def render_GET(self, request):
        """Return an index of urls for resources."""
        response = {
            'jobs':             self.children['jobs'].get_job_index(),
            'services':         self.children['services'].get_service_index(),
            'namespaces':       self.children['config'].get_config_index()
        }
        return respond(request, response)


class RootResource(resource.Resource):

    def __init__(self, mcp, web_path):
        resource.Resource.__init__(self)
        self.web_path = web_path
        self.mcp = mcp
        self.putChild('api', ApiRootResource(self.mcp))
        self.putChild('web', static.File(web_path))
        self.putChild('', self)

    def render_GET(self, request):
        request.redirect(request.prePathURL() + 'web')
        request.finish()
        return server.NOT_DONE_YET

    def __str__(self):
        return "%s(%s, %s)" % (type(self).__name__, self.mcp, self.web_path)

class LogAdapter(object):

    def __init__(self, logger):
        self.logger = logger

    def write(self, line):
        self.logger.info(line.rstrip('\n'))

    def close(self):
        pass


class TronSite(server.Site):
    """Subclass of a twisted Site to customize logging."""

    access_log = logging.getLogger('tron.api.www.access')

    @classmethod
    def create(cls, mcp, web_path):
        return cls(RootResource(mcp, web_path))

    def startFactory(self):
        server.Site.startFactory(self)
        self.logFile = LogAdapter(self.access_log)

    def __repr__(self):
        return '%s(%s)' % (self.__class__.__name__, self.resource)

########NEW FILE########
__FILENAME__ = client
"""
A command line http client used by tronview, tronctl, and tronfig
"""
from collections import namedtuple
import logging
import urllib
import urllib2
import urlparse
import itertools
import tron
from tron.config.schema import MASTER_NAMESPACE

try:
    import simplejson
    assert simplejson # Pyflakes
except ImportError:
    import json as simplejson


log = logging.getLogger(__name__)


USER_AGENT   = "Tron Command/%s +http://github.com/Yelp/Tron" % tron.__version__
DECODE_ERROR = "DECODE_ERROR"
URL_ERROR    = 'URL_ERROR'


class RequestError(ValueError):
    """Raised when the request to tron API fails."""


Response = namedtuple('Response', 'error msg content')

default_headers = {
    "User-Agent": USER_AGENT
}


def build_url_request(uri, data, headers=None):
    headers     = headers or default_headers
    enc_data    = urllib.urlencode(data) if data else None
    return urllib2.Request(uri, enc_data, headers)


def load_response_content(http_response):
    content = http_response.read()
    try:
        return Response(None, None, simplejson.loads(content))
    except ValueError, e:
        log.error("Failed to decode response: %s, %s", e, content)
        return Response(DECODE_ERROR, str(e), content)


def build_http_error_response(exc):
    content = exc.read() if hasattr(exc, 'read') else None
    return Response(exc.code, exc.msg, content)


def request(uri, data=None):
    log.info("Request to %s with %s", uri, data)
    request = build_url_request(uri, data)
    try:
        response = urllib2.urlopen(request)
    except urllib2.HTTPError, e:
        log.error("Received error response: %s" % e)
        return build_http_error_response(e)
    except urllib2.URLError, e:
        log.error("Received error response: %s" % e)
        return Response(URL_ERROR, e.reason, None)

    return load_response_content(response)


def build_get_url(url, data=None):
     return '%s?%s' % (url, urllib.urlencode(data)) if data else url


class Client(object):
    """An HTTP client used to issue commands to the Tron API.
    """

    def __init__(self, url_base):
        """Create a new client.
            url_base - A url with a schema, hostname and port
        """
        self.url_base = url_base

    def status(self):
        return self.http_get('/api/status')

    def events(self):
        return self.http_get('/api/events')['data']

    def config(self,
              config_name, config_data=None, config_hash=None, no_header=False):
        """Retrieve or update the configuration."""
        if config_data:
            request_data = dict(
                        config=config_data, name=config_name, hash=config_hash)
            return self.request('/api/config', request_data)
        request_data = dict(name=config_name, no_header=int(no_header))
        return self.http_get('/api/config', request_data)

    def home(self):
        return self.http_get('/api/')

    index = home

    def get_url(self, identifier):
        return get_object_type_from_identifier(self.index(), identifier).url

    def services(self):
        return self.http_get('/api/services').get('services')

    def service(self, service_url):
        return self.http_get(service_url)

    def jobs(self, include_job_runs=False, include_action_runs=False):
        params = {'include_job_runs': int(include_job_runs),
                  'include_action_runs': int(include_action_runs)}
        return self.http_get('/api/jobs', params).get('jobs')

    def job(self, job_url, include_action_runs=False, count=0):
        params = {'include_action_runs': int(include_action_runs),
                  'num_runs': count}
        return self.http_get(job_url, params)

    def job_runs(self, url, include_runs=True, include_graph=False):
        params = {
            'include_action_runs': int(include_runs),
            'include_action_graph': int(include_graph)}
        return self.http_get(url, params)

    def action_runs(self, action_run_url, num_lines=0):
        params = {
            'num_lines':        num_lines,
            'include_stdout':   1,
            'include_stderr':   1}
        return self.http_get(action_run_url, params)

    def object_events(self, item_url):
        return self.http_get('%s/_events' % item_url)['data']

    def http_get(self, url, data=None):
        return self.request(build_get_url(url, data))

    def request(self, url, data=None):
        log.info("Request: %s, %s, %s", self.url_base, url, data)
        uri = urlparse.urljoin(self.url_base, url)
        response = request(uri, data)
        if response.error:
            raise RequestError("%s: %s" % (uri, response))
        return response.content


def build_api_url(resource, identifier_parts):
    return '/api/%s/%s' % (resource, '/'.join(identifier_parts))


def split_identifier(identifier):
    return identifier.rsplit('.', identifier.count('.') - 1)


def get_job_url(identifier):
    return build_api_url('jobs', split_identifier(identifier))


def get_service_url(identifier):
    return build_api_url('services', split_identifier(identifier))


class TronObjectType(object):
    """Constants to identify a Tron object type."""
    job              = 'JOB'
    job_run          = 'JOB_RUN'
    action_run       = 'ACTION_RUN'
    service          = 'SERVICE'
    service_instance = 'SERVICE_INSTANCE'

    url_builders = {
        'jobs':     get_job_url,
        'services': get_service_url
    }

    groups = {
        'jobs':     [job, job_run, action_run],
        'services': [service, service_instance]
    }


TronObjectIdentifier = namedtuple('TronObjectIdentifier', 'type url')

IdentifierParts = namedtuple('IdentifierParts', 'name full_id length')


def first(seq):
    for item in itertools.ifilter(None, seq):
        return item


def get_object_type_from_identifier(url_index, identifier):
    """Given a string identifier, return a TronObjectIdentifier. """
    name_mapping = {
        'jobs':     set(url_index['jobs']),
        'services': set(url_index['services'])
    }
    def get_name_parts(identifier, namespace=None):
        if namespace:
            identifier = '%s.%s' % (namespace, identifier)

        name_elements       = identifier.split('.')
        name                = '.'.join(name_elements[:2])
        length              = len(name_elements) - 2
        return IdentifierParts(name, identifier, length)

    def find_by_type(id_parts, index_name):
        url_type_index = name_mapping[index_name]
        if id_parts.name in url_type_index:
            tron_type = TronObjectType.groups[index_name][id_parts.length]
            url = TronObjectType.url_builders[index_name](id_parts.full_id)
            return TronObjectIdentifier(tron_type, url)

    def find_by_name(name, namespace=None):
        id = get_name_parts(name, namespace)
        return find_by_type(id, 'jobs') or find_by_type(id, 'services')

    namespaces = [None, MASTER_NAMESPACE] + url_index['namespaces']
    id_obj = first(find_by_name(identifier, name) for name in namespaces)
    if id_obj:
        return id_obj

    raise ValueError("Unknown identifier: %s" % identifier)

########NEW FILE########
__FILENAME__ = cmd_utils
"""
Common code for command line utilities (see bin/)
"""
from __future__ import with_statement
import logging
import optparse
import os
import sys

import yaml
import tron


log = logging.getLogger("tron.commands")


class ExitCode(object):
    """Enumeration of exit status codes."""
    success =           0
    fail =              1


GLOBAL_CONFIG_FILE_NAME = os.environ.get('TRON_CONFIG') or "/etc/tron/tron.yaml"
CONFIG_FILE_NAME = os.path.expanduser('~/.tron')

DEFAULT_HOST = 'localhost'
DEFAULT_PORT = 8089

DEFAULT_CONFIG = {
    'server':           "http://%s:%d" % (DEFAULT_HOST, DEFAULT_PORT),
    'display_color':    False,
}


opener = open


def build_option_parser(usage, parser_class=optparse.OptionParser):
    parser = parser_class(usage, version="%%prog %s" % tron.__version__)

    parser.add_option("-v", "--verbose", action="count",
        help="Verbose logging", default=None)
    parser.add_option("--server", default=None,
        help="Url including scheme, host and port, Default: %default")
    parser.add_option("-s", "--save", action="store_true", dest="save_config",
        help="Save options used on this job for next time.")

    return parser


def get_client_config():
    config_file_list = [CONFIG_FILE_NAME, GLOBAL_CONFIG_FILE_NAME]
    for config_file in config_file_list:
        filename = os.path.expanduser(config_file)
        if os.access(filename, os.R_OK):
            config = read_config(filename)
            if config:
                return config

    log.debug("Could not find a config in: %s." % ', '.join(config_file_list))
    return {}


def load_config(options):
    """Attempt to load a user specific configuration or a global config file
    and set any unset options based on values from the config. Finally fallback
    to DEFAULT_CONFIG for those settings.

    Also save back options to the config if options.save_config is True.
    """
    config = get_client_config()

    for opt_name in DEFAULT_CONFIG.keys():
        if not hasattr(options, opt_name):
            continue

        if getattr(options, opt_name) is not None:
            continue

        default_value = DEFAULT_CONFIG[opt_name]
        setattr(options, opt_name, config.get(opt_name, default_value))

    if options.save_config:
        save_config(options)


def read_config(filename=CONFIG_FILE_NAME):
    try:
        with opener(filename, 'r') as config_file:
            return yaml.load(config_file)
    except (IOError, OSError):
        log.info("Failed to read config file: %s" % CONFIG_FILE_NAME)
    return {}


def write_config(config):
    with open(CONFIG_FILE_NAME, "w") as config_file:
        yaml.dump(config, config_file)


def save_config(options):
    config = read_config()
    for opt_name in DEFAULT_CONFIG.keys():
        if not hasattr(options, opt_name):
            continue
        config[opt_name] = getattr(options, opt_name)
    write_config(config)


def setup_logging(options):
    level = logging.INFO if options.verbose else logging.WARNING

    logging.basicConfig(
        level=level,
        format='%(name)s %(levelname)s %(message)s',
        stream=sys.stdout)
########NEW FILE########
__FILENAME__ = display
"""
Format and color output for tron commands.
"""
import contextlib
from functools import partial
from operator import itemgetter
from tron.core import actionrun, job, service


class Color(object):

    enabled = None
    colors = {
        'gray':                 '\033[90m',
        'red':                  '\033[91m',
        'green':                '\033[92m',
        'yellow':               '\033[93m',
        'blue':                 '\033[94m',
        'purple':               '\033[95m',
        'cyan':                 '\033[96m',
        'white':                '\033[99m',
        # h is for highlighted
        'hgray':                '\033[100m',
        'hred':                 '\033[101m',
        'hgreen':               '\033[102m',
        'hyellow':              '\033[103m',
        'hblue':                '\033[104m',
        'hcyan':                '\033[106m',
        'end':                  '\033[0m',
    }

    @classmethod
    @contextlib.contextmanager
    def enable(cls):
        old_val = cls.enabled
        try:
            cls.enabled = True
            yield
        finally:
            cls.enabled = old_val

    @classmethod
    def set(cls, color_name, text):
        if not cls.enabled or not color_name:
            return unicode(text)
        return cls.colors[color_name.lower()] + unicode(text) + cls.colors['end']

    @classmethod
    def toggle(cls, enable):
        cls.enabled = enable


class TableDisplay(object):
    """Base class for displaying columns of data.  This class takes a list
    of dict objects and formats it so that it displays properly in fixed width
    columns.  Overlap is truncated.

    This class provides many hooks for customizing the output, including:
        - sorting of rows
        - building composite values from more then one field
        - custom formatting of a columns values
        - adding additional data after each row
        - coloring of header, columns, or rows

    The default output is:

        Banner
        Header
        Row
        (optional post row)
        Row
        (optional post row)
        ...
        Footer
    """

    columns = None
    fields = None
    widths = None
    colors = None
    title = None
    resize_fields = set()
    reversed = False

    header_color = 'hgray'

    def __init__(self):
        self.out = []

    def banner(self):
        if not self.title:
            return
        title = self.title.capitalize()
        self.out.append("\n%s:" % title)
        if not self.rows():
            self.out.append("No %s\n" % title)

    def header(self):
        row = [label.ljust(self.get_field_width(i))
               for i, label in enumerate(self.columns)]
        self.out.append(Color.set(self.header_color, "".join(row)))

    def footer(self):
        pass

    def color(self, col, field):
        return None

    def sorted_fields(self, values):
        return [values[name] for name in self.fields]

    def format_row(self, fields):
        row = [
            Color.set(self.color(i, value), self.trim_value(i, value))
            for i, value in enumerate(self.sorted_fields(fields))
        ]
        return Color.set(self.row_color(fields), "".join(row))

    def get_field_width(self, field_idx):
        return self.widths[field_idx]

    def trim_value(self, field_idx, value):
        length = self.get_field_width(field_idx)
        value = self.format_value(field_idx, value)
        if len(value) > length:
            return (value[:length - 3] + '...').ljust(length)
        return value.ljust(length)

    def format_value(self, field_idx, value):
        return unicode(value)

    def output(self):
        out = "\n".join(self.out)
        self.out = []
        return out

    def post_row(self, row):
        pass

    def row_color(self, row):
        return None

    def rows(self):
        return sorted(self.data,
            key=itemgetter(self.fields[0]), reverse=self.reversed)

    def store_data(self, data):
        self.data = data

    def update_column_widths(self):
        """Update column widths to fit the data."""
        for field_idx, field in enumerate(self.fields):
            if field in self.resize_fields:
                self.widths[field_idx] = self.calculate_width(field_idx)

    def calculate_width(self, field_idx):
        default_width = self.widths[field_idx]
        column = [
            self.format_value(field_idx, row[self.fields[field_idx]])
            for row in self.data]
        if not column:
            return default_width
        max_value_width = max(len(value) for value in column)
        return max(max_value_width + 1, default_width)

    def format(self, data):
        self.store_data(data)
        self.update_column_widths()
        self.banner()

        if not self.rows():
            return self.output()

        self.header()
        for row in self.rows():
            self.out.append(self.format_row(row))
            self.post_row(row)

        self.footer()
        return self.output()


def add_color_for_state(state):
    if state == actionrun.ActionRun.STATE_FAILED.name:
        return Color.set('red', state)
    if state in set((
        actionrun.ActionRun.STATE_RUNNING.name,
        actionrun.ActionRun.STATE_SUCCEEDED.name,
        job.Job.STATUS_ENABLED,
        service.ServiceState.UP
    )):
        return Color.set('green', state)
    if state in set((job.Job.STATUS_DISABLED, service.ServiceState.DISABLED)):
        return Color.set('blue', state)
    return state


def format_fields(display_obj, content):
    """Format fields with some color."""
    def add_color(field, field_value):
        if field not in display_obj.colors:
            return field_value
        return display_obj.colors[field](field_value)

    def format_field(field):
        value = content[field]
        if value is None:
            return ''
        return field_display_mapping.get(field, lambda f: f)(value)

    def build_field(label, field):
        return "%-20s: %s" % (label, add_color(field, format_field(field)))

    return "\n".join(build_field(*item) for item in display_obj.detail_labels)


def format_service_details(service_content):
    """Format details about a service."""

    def format_instances(service_instances):
        format_str = "    %s : %-30s %s%s"
        def get_failure_messages(failures):
            if not failures:
                return ""
            header = Color.set("red", "\n    stderr: ")
            return header + Color.set("red", "\n".join(failures))

        def format(inst):
            state = add_color_for_state(inst['state'])
            failures = get_failure_messages(inst['failures'])
            node = display_node(inst['node'])
            return format_str % (inst['id'], node, state, failures)
        return [format(instance) for instance in service_instances]

    details     = format_fields(DisplayServices, service_content)
    instances   = format_instances(service_content['instances'])
    return details + '\n\nInstances:\n' + '\n'.join(instances)


def format_job_details(job_content):
    details = format_fields(DisplayJobs, job_content)
    job_runs = DisplayJobRuns().format(job_content['runs'])
    actions = "\n\nList of Actions:\n%s" % '\n'.join(job_content['action_names'])
    return details + actions + "\n" + job_runs


def format_action_run_details(content, stdout=True, stderr=True):
    out = ["Requirements:"] + content['requirements'] + ['']
    if stdout:
        out.append("Stdout:\n%s\n" % '\n'.join(content['stdout']))

    if stderr:
        out.append("Stderr:\n%s\n" % '\n'.join(content['stderr']))

    details = format_fields(DisplayActionRuns, content)
    return details + '\n' + '\n'.join(out)


class DisplayServices(TableDisplay):

    columns = ['Name',  'State',    'Count'      ]
    fields  = ['name',  'state',    'live_count' ]
    widths  = [50,      12,          5           ]
    title   = 'services'
    resize_fields = ['name']

    detail_labels = [
        ('Service',             'name'              ),
        ('Enabled',             'enabled'           ),
        ('State',               'state'             ),
        ('Max instances',       'count'             ),
        ('Command',             'command'           ),
        ('Pid Filename',        'pid_filename'      ),
        ('Node Pool',           'node_pool'         ),
        ('Monitor interval',    'monitor_interval'  ),
        ('Restart delay',       'restart_delay'     ),
    ]

    colors = {
        'name':     partial(Color.set, 'yellow'),
        'state':    add_color_for_state
    }


class DisplayJobRuns(TableDisplay):
    """Format Job runs."""

    columns = ['Run ID',    'State',    'Node', 'Scheduled Time']
    fields  = ['run_num',   'state',    'node', 'run_time']
    widths  = [10,          12,         30,     25]
    title = 'job runs'
    reversed = True

    detail_labels = [
        ('Job Run',             'id'),
        ('State',               'state'),
        ('Node',                'node'),
        ('Scheduled time',      'run_time'),
        ('Start time',          'start_time'),
        ('End time',            'end_time'),
        ('Manual run',          'manual'),
    ]

    colors = {
        'id':        partial(Color.set, 'yellow'),
        'state':     add_color_for_state,
        'manual':    lambda value: Color.set('cyan' if value else None, value),
    }

    def format_value(self, field_idx, value):
        if self.fields[field_idx] == 'run_num':
            value = '.' + str(value)

        if self.fields[field_idx] == 'scheduled_time':
            value = value or '-'

        if self.fields[field_idx] == 'node':
            value = display_node(value)

        return super(DisplayJobRuns, self).format_value(field_idx, value)

    def row_color(self, fields):
        return 'red' if fields['state'] == 'FAIL' else 'white'

    def post_row(self, row):
        start = row['start_time'] or "-"
        end =   row['end_time']   or "-"
        duration = row['duration'][:-7] if row['duration'] else "-"

        row_data = "%sStart: %s  End: %s  (%s)" % (
            ' ' * self.widths[0], start, end, duration)
        self.out.append(Color.set('gray', row_data))


class DisplayJobs(TableDisplay):

    columns = ['Name',  'State',    'Scheduler',    'Last Success']
    fields  = ['name',  'status',   'scheduler',    'last_success']
    widths  = [50,       10,         20,             20           ]
    title = 'jobs'
    resize_fields = ['name']

    detail_labels = [
        ('Job',                 'name'              ),
        ('State',               'status'            ),
        ('Scheduler',           'scheduler'         ),
        ('Max runtime',         'max_runtime'       ),
        ('Node Pool',           'node_pool'         ),
        ('Run on all nodes',    'all_nodes'         ),
        ('Allow overlapping',   'allow_overlap'     ),
        ('Queue overlapping',   'queueing'          ),
    ]

    colors = {
        'name':      partial(Color.set, 'yellow'),
        'status':    add_color_for_state
    }

    def format_value(self, field_idx, value):
        if self.fields[field_idx] == 'scheduler':
            value = display_scheduler(value)

        return super(DisplayJobs, self).format_value(field_idx, value)


class DisplayActionRuns(TableDisplay):

    columns = ['Action', 'State', 'Start Time', 'End Time', 'Duration']
    fields  = ['id',     'state', 'start_time', 'end_time', 'duration']
    widths  = [40,       12,      22,           22,         10     ]
    title = 'actions'
    resize_fields = ['id']

    detail_labels = [
        ('Action Run',          'id'),
        ('State',               'state'),
        ('Node',                'node'),
        ('Command',             'command'),
        ('Bare command',        'raw_command'),
        ('Start time',          'start_time'),
        ('End time',            'end_time'),
        ('Exit status',         'exit_status'),
    ]

    colors = {
        'id':           partial(Color.set, 'yellow'),
        'state':        add_color_for_state,
        'command':      partial(Color.set, 'gray'),
    }

    def banner(self):
        self.out.append(format_fields(DisplayJobRuns, self.job_run))
        super(DisplayActionRuns, self).banner()

    def format_value(self, field_idx, value):
        if self.fields[field_idx] == 'id':
            value = '.' + value.rsplit('.', 1)[-1]
        if self.fields[field_idx] in ('start_time', 'end_time'):
            value = value or "-"
        if self.fields[field_idx] == 'duration':
            # Strip microseconds
            value = value[:-7] if value else "-"

        return super(DisplayActionRuns, self).format_value(field_idx, value)

    def row_color(self, fields):
        return 'red' if fields['state'] == 'FAIL' else 'white'

    def store_data(self, data):
        self.data = data['runs']
        self.job_run = data


class DisplayEvents(TableDisplay):

    columns = ['Time', 'Level', 'Entity', 'Name']
    fields  = ['time', 'level', 'entity', 'name']
    widths  = [22,     12,       35,      20    ]
    title = 'events'
    resize_fields = ['entity']


def display_node(source):
    return '%s@%s' % (source['username'], source['hostname'])


def display_node_pool(source):
    return "%s (%d node(s))" % (source['name'], len(source['nodes']))


def display_scheduler(source):
    return "%s %s%s" % (source['type'], source['value'], source['jitter'])


field_display_mapping = {
    'node':             display_node,
    'node_pool':        display_node_pool,
    'scheduler':        display_scheduler,
}


def view_with_less(content, color=True):
    """Send `content` through less."""
    import subprocess
    cmd = ['less']
    if color:
        cmd.append('-r')

    less_proc = subprocess.Popen(cmd, stdin=subprocess.PIPE)
    less_proc.stdin.write(content)
    less_proc.stdin.close()
    less_proc.wait()

########NEW FILE########
__FILENAME__ = command_context
"""Command Context is how we construct the command line for a command which may
have variables that need to be rendered.
"""
import operator
from tron.utils import timeutils


def build_context(object, parent):
    """Construct a CommandContext for object. object must have a property
    'context_class'.
    """
    return CommandContext(object.context_class(object), parent)


def build_filled_context(*context_objects):
    """Create a CommandContext chain from context_objects, using a Filler
    object to pass to each CommandContext. Can be used to validate a format
    string.
    """
    if not context_objects:
        return CommandContext()

    filler = Filler()
    def build(current, next):
        return CommandContext(next(filler), current)

    return reduce(build, context_objects, None)


class CommandContext(object):
    """A CommandContext object is a wrapper around any object which has values
    to be used to render a command for execution.  It looks up values by name.

    It's lookup order is:
        base[name],
        base.__getattr__(name),
        next[name],
        next.__getattr__(name)
    """

    def __init__(self, base=None, next=None):
        """
          base - Object to look for attributes in
          next - Next place to look for more pieces of context
                 Generally this will be another instance of CommandContext
        """
        self.base = base or {}
        self.next = next or {}

    def get(self, name, default=None):
        try:
            return self.__getitem__(name)
        except KeyError:
            return default

    def __getitem__(self, name):
        getters = [operator.itemgetter(name), operator.attrgetter(name)]
        for target in [self.base, self.next]:
            for getter in getters:
                try:
                    return getter(target)
                except (KeyError, TypeError, AttributeError):
                    pass

        raise KeyError(name)

    def __eq__(self, other):
        return self.base == other.base and self.next == other.next

    def __ne__(self, other):
        return not self == other


class JobContext(object):
    """A class which exposes properties for rendering commands."""

    def __init__(self, job):
        self.job = job

    @property
    def name(self):
        return self.job.name

    def __getitem__(self, item):
        date_name, date_spec = self._get_date_spec_parts(item)
        if not date_spec:
            raise KeyError(item)

        if date_name == 'last_success':
            last_success = self.job.runs.last_success
            last_success = last_success.run_time if last_success else None

            time_value = timeutils.DateArithmetic.parse(date_spec, last_success)
            if time_value:
                return time_value

        raise KeyError(item)

    def _get_date_spec_parts(self, name):
        parts = name.rsplit(':', 1)
        if len(parts) != 2:
            return name, None
        return parts


class JobRunContext(object):

    def __init__(self, job_run):
        self.job_run = job_run

    @property
    def runid(self):
        return self.job_run.id

    @property
    def cleanup_job_status(self):
        """Provide 'SUCCESS' or 'FAILURE' to a cleanup action context based on
        the status of the other steps
        """
        if self.job_run.action_runs.is_failed:
            return 'FAILURE'
        elif self.job_run.action_runs.is_complete_without_cleanup:
            return 'SUCCESS'
        return 'UNKNOWN'

    def __getitem__(self, name):
        """Attempt to parse date arithmetic syntax and apply to run_time."""
        run_time = self.job_run.run_time
        time_value = timeutils.DateArithmetic.parse(name, run_time)
        if time_value:
            return time_value

        raise KeyError(name)


class ActionRunContext(object):
    """Context object that gives us access to data about the action run."""

    def __init__(self, action_run):
        self.action_run = action_run

    @property
    def actionname(self):
        return self.action_run.action_name

    @property
    def node(self):
        return self.action_run.node.hostname


class ServiceInstancePidContext(object):

    def __init__(self, service_instance):
        self.service_instance = service_instance

    @property
    def instance_number(self):
        return self.service_instance.instance_number

    @property
    def node(self):
        return self.service_instance.node.hostname

    @property
    def name(self):
        return self.service_instance.config.name


class ServiceInstanceContext(ServiceInstancePidContext):

    @property
    def pid_file(self):
        context = CommandContext(self, self.service_instance.parent_context)
        return self.service_instance.config.pid_file % context


class Filler(object):
    """Filler object for using CommandContext during config parsing. This class
    is used as a substitute for objects that would be passed to Context objects.
    This allows the Context objects to be used directly for config validation.
    """

    def __getattr__(self, _):
        return self

    def __str__(self):
        return "%(...)s"

    def __mod__(self, _):
        return self

    def __nonzero__(self):
        return False
########NEW FILE########
__FILENAME__ = config_parse
"""
Parse a dictionary structure and return an immutable structure that
contain a validated configuration.
"""
import itertools
import logging
import os

import pytz
from tron import command_context

from tron.config import ConfigError, config_utils, schema
from tron.config.config_utils import ConfigContext, Validator
from tron.config.config_utils import valid_string, valid_bool
from tron.config.config_utils import valid_identifier
from tron.config.config_utils import build_list_of_type_validator
from tron.config.config_utils import valid_name_identifier
from tron.config.config_utils import build_dict_name_validator
from tron.config.config_utils import valid_int, valid_float, valid_dict
from tron.config.config_utils import PartialConfigContext
from tron.config.schedule_parse import valid_schedule
from tron.config.schema import TronConfig, NamedTronConfig, NotificationOptions
from tron.config.schema import CLEANUP_ACTION_NAME
from tron.config.schema import ConfigSSHOptions
from tron.config.schema import ConfigState
from tron.config.schema import ConfigJob, ConfigAction, ConfigCleanupAction
from tron.config.schema import ConfigService
from tron.config.schema import MASTER_NAMESPACE
from tron.utils.dicts import FrozenDict


log = logging.getLogger(__name__)


def build_format_string_validator(context_object):
    """Validate that a string does not contain any unexpected formatting keys.
        valid_keys - a sequence of strings
    """
    def validator(value, config_context):
        if config_context.partial:
            return valid_string(value, config_context)

        context = command_context.CommandContext(
                    context_object, config_context.command_context)

        try:
            value % context
            return value
        except (KeyError, ValueError), e:
            error_msg = "Unknown context variable %s at %s: %s"
            raise ConfigError(error_msg % (e, config_context.path, value))

    return validator


def valid_output_stream_dir(output_dir, config_context):
    """Returns a valid string for the output directory, or raises ConfigError
    if the output_dir is not valid.
    """
    if not output_dir:
        return

    if config_context.partial:
        return output_dir

    valid_string(output_dir, config_context)
    if not os.path.isdir(output_dir):
        msg = "output_stream_dir '%s' is not a directory"
        raise ConfigError(msg % output_dir)

    if not os.access(output_dir, os.W_OK):
        raise ConfigError("output_stream_dir '%s' is not writable" % output_dir)

    return output_dir


def valid_identity_file(file_path, config_context):
    valid_string(file_path, config_context)

    if config_context.partial:
        return file_path

    file_path = os.path.expanduser(file_path)
    if not os.path.exists(file_path):
        raise ConfigError("Private key file %s doesn't exist" % file_path)

    public_key_path = file_path + '.pub'
    if not os.path.exists(public_key_path):
        raise ConfigError("Public key file %s doesn't exist" % public_key_path)
    return file_path


def valid_known_hosts_file(file_path, config_context):
    valid_string(file_path, config_context)

    if config_context.partial:
        return file_path

    file_path = os.path.expanduser(file_path)
    if not os.path.exists(file_path):
        raise ConfigError("Known hosts file %s doesn't exist" % file_path)
    return file_path


def valid_command_context(context, config_context):
    # context can be any dict.
    return FrozenDict(**valid_dict(context or {}, config_context))


def valid_time_zone(tz, config_context):
    if tz is None:
        return None
    valid_string(tz, config_context)
    try:
        return pytz.timezone(tz)
    except pytz.exceptions.UnknownTimeZoneError:
        raise ConfigError('%s is not a valid time zone' % tz)


def valid_node_name(value, config_context):
    valid_identifier(value, config_context)
    if not config_context.partial and value not in config_context.nodes:
        msg = "Unknown node name %s at %s"
        raise ConfigError(msg % (value, config_context.path))
    return value


class ValidateSSHOptions(Validator):
    """Validate SSH options."""
    config_class =                  ConfigSSHOptions
    optional =                      True
    defaults = {
        'agent':                    False,
        'identities':               (),
        'known_hosts_file':         None,
        'connect_timeout':          30,
        'idle_connection_timeout':  3600,
        'jitter_min_load':          4,
        'jitter_max_delay':         20,
        'jitter_load_factor':       1,
    }

    validators = {
        'agent':                    valid_bool,
        'identities':               build_list_of_type_validator(
                                        valid_identity_file, allow_empty=True),
        'known_hosts_file':         valid_known_hosts_file,
        'connect_timeout':          config_utils.valid_int,
        'idle_connection_timeout':  config_utils.valid_int,
        'jitter_min_load':          config_utils.valid_int,
        'jitter_max_delay':         config_utils.valid_int,
        'jitter_load_factor':       config_utils.valid_int,
    }

    def post_validation(self, valid_input, config_context):
        if config_context.partial:
            return

        if valid_input['agent'] and 'SSH_AUTH_SOCK' not in os.environ:
            raise ConfigError("No SSH Agent available ($SSH_AUTH_SOCK)")


valid_ssh_options = ValidateSSHOptions()


class ValidateNotificationOptions(Validator):
    """Validate notification options."""
    config_class =              NotificationOptions
    optional =                  True

valid_notification_options = ValidateNotificationOptions()


class ValidateNode(Validator):
    config_class =              schema.ConfigNode
    validators = {
        'name':                 config_utils.valid_identifier,
        'username':             config_utils.valid_string,
        'hostname':             config_utils.valid_string,
        'port':                 config_utils.valid_int,
    }

    defaults = {
        'port':                 22,
        'username':             os.environ['USER'],
    }

    def do_shortcut(self, node):
        """Nodes can be specified with just a hostname string."""
        if isinstance(node, basestring):
            return schema.ConfigNode(hostname=node, name=node, **self.defaults)

    def set_defaults(self, output_dict, config_context):
        super(ValidateNode, self).set_defaults(output_dict, config_context)
        output_dict.setdefault('name', output_dict['hostname'])

valid_node = ValidateNode()


class ValidateNodePool(Validator):
    config_class =              schema.ConfigNodePool
    validators = {
        'name':                 valid_identifier,
        'nodes':                build_list_of_type_validator(valid_identifier),
    }

    def cast(self, node_pool, _context):
        if isinstance(node_pool, list):
            node_pool = dict(nodes=node_pool)
        return node_pool

    def set_defaults(self, node_pool, _):
        node_pool.setdefault('name', '_'.join(node_pool['nodes']))


valid_node_pool = ValidateNodePool()


def valid_action_name(value, config_context):
    valid_identifier(value, config_context)
    if value == CLEANUP_ACTION_NAME:
        error_msg = "Invalid action name %s at %s"
        raise ConfigError(error_msg % (value, config_context.path))
    return value

action_context = command_context.build_filled_context(
        command_context.JobContext,
        command_context.JobRunContext,
        command_context.ActionRunContext)


class ValidateAction(Validator):
    """Validate an action."""
    config_class =              ConfigAction

    defaults = {
        'node':                 None,
        'requires':             (),
    }
    requires = build_list_of_type_validator(valid_action_name, allow_empty=True)
    validators = {
        'name':                 valid_action_name,
        'command':              build_format_string_validator(action_context),
        'node':                 valid_node_name,
        'requires':             requires,
    }

valid_action = ValidateAction()


def valid_cleanup_action_name(value, config_context):
    if value != CLEANUP_ACTION_NAME:
        msg = "Cleanup actions cannot have custom names %s.%s"
        raise ConfigError(msg % (config_context.path, value))
    return CLEANUP_ACTION_NAME


class ValidateCleanupAction(Validator):
    config_class =              ConfigCleanupAction
    defaults = {
        'node':                 None,
        'name':                 CLEANUP_ACTION_NAME,
    }
    validators = {
        'name':                 valid_cleanup_action_name,
        'command':              build_format_string_validator(action_context),
        'node':                 valid_node_name,
    }

valid_cleanup_action = ValidateCleanupAction()


class ValidateJob(Validator):
    """Validate jobs."""
    config_class =              ConfigJob
    defaults = {
        'run_limit':            50,
        'all_nodes':            False,
        'cleanup_action':       None,
        'enabled':              True,
        'queueing':             True,
        'allow_overlap':        False,
        'max_runtime':          None,
    }

    validators = {
        'name':                 valid_name_identifier,
        'schedule':             valid_schedule,
        'run_limit':            valid_int,
        'all_nodes':            valid_bool,
        'actions':              build_dict_name_validator(valid_action),
        'cleanup_action':       valid_cleanup_action,
        'node':                 valid_node_name,
        'queueing':             valid_bool,
        'enabled':              valid_bool,
        'allow_overlap':        valid_bool,
        'max_runtime':          config_utils.valid_time_delta,
    }

    def cast(self, in_dict, config_context):
        in_dict['namespace'] = config_context.namespace
        return in_dict

    # TODO: extract common code to a util function
    def _validate_dependencies(self, job, actions,
        base_action, current_action=None, stack=None):
        """Check for circular or misspelled dependencies."""
        stack = stack or []
        current_action = current_action or base_action

        stack.append(current_action.name)
        for dep in current_action.requires:
            if dep == base_action.name and len(stack) > 0:
                msg = 'Circular dependency in job.%s: %s'
                raise ConfigError(msg % (job['name'], ' -> '.join(stack)))
            if dep not in actions:
                raise ConfigError(
                    'Action jobs.%s.%s has a dependency "%s"'
                    ' that is not in the same job!' %
                    (job['name'], current_action.name, dep))
            self._validate_dependencies(
                job, actions, base_action, actions[dep], stack)

        stack.pop()

    def post_validation(self, job, config_context):
        """Validate actions for the job."""
        for action in job['actions'].itervalues():
            self._validate_dependencies(job, job['actions'], action)

valid_job = ValidateJob()


class ValidateService(Validator):
    """Validate a services configuration."""
    config_class =              ConfigService

    service_context =           command_context.build_filled_context(
                                    command_context.ServiceInstanceContext)

    service_pid_context =       command_context.build_filled_context(
                                    command_context.ServiceInstancePidContext)

    defaults = {
        'count':                1,
        'monitor_retries':      5,
        'restart_delay':        None
    }

    validators = {
        'name':                 valid_name_identifier,
        'pid_file':             build_format_string_validator(service_pid_context),
        'command':              build_format_string_validator(service_context),
        'monitor_interval':     valid_float,
        'monitor_retries':      valid_int,
        'count':                valid_int,
        'node':                 valid_node_name,
        'restart_delay':        valid_float,
    }

    def cast(self, in_dict, config_context):
        in_dict['namespace'] = config_context.namespace

        # TODO: Deprecated - remove in 0.7
        if 'restart_interval' in in_dict:
            msg = ("restart_interval at %s is deprecated. It has been renamed "
                   "restart_delay and will be removed in 0.7")
            log.warn(msg % config_context.path)
            in_dict['restart_delay'] = in_dict.pop('restart_interval')
        return in_dict

valid_service = ValidateService()


class ValidateActionRunner(Validator):
    config_class =              schema.ConfigActionRunner
    optional =                  True
    defaults = {
        'runner_type':          None,
        'remote_exec_path':     '',
        'remote_status_path':   '/tmp',
    }

    validators = {
        'runner_type':          config_utils.build_enum_validator(
                                    schema.ActionRunnerTypes),
        'remote_status_path':   valid_string,
        'remote_exec_path':     valid_string,
    }


class ValidateStatePersistence(Validator):
    config_class                = schema.ConfigState
    defaults = {
        'buffer_size':          1,
        'connection_details':   None,
    }

    validators = {
        'name':                 valid_string,
        'store_type':           config_utils.build_enum_validator(
                                    schema.StatePersistenceTypes),
        'connection_details':   valid_string,
        'buffer_size':          valid_int,
    }

    def post_validation(self, config, config_context):
        buffer_size = config.get('buffer_size')

        if buffer_size and buffer_size < 1:
            path = config_context.path
            raise ConfigError("%s buffer_size must be >= 1." % path)

valid_state_persistence = ValidateStatePersistence()


def validate_jobs_and_services(config, config_context):
    """Validate jobs and services."""
    valid_jobs      = build_dict_name_validator(valid_job, allow_empty=True)
    valid_services  = build_dict_name_validator(valid_service, allow_empty=True)
    validation      = [('jobs', valid_jobs), ('services', valid_services)]

    for config_name, valid in validation:
        child_context = config_context.build_child_context(config_name)
        config[config_name] = valid(config.get(config_name, []), child_context)

    fmt_string = 'Job and Service names must be unique %s'
    config_utils.unique_names(fmt_string, config['jobs'], config['services'])


DEFAULT_STATE_PERSISTENCE = ConfigState('tron_state', 'shelve', None, 1)
DEFAULT_NODE = ValidateNode().do_shortcut('localhost')


class ValidateConfig(Validator):
    """Given a parsed config file (should be only basic literals and
    containers), return an immutable, fully populated series of namedtuples and
    FrozenDicts with all defaults filled in, all valid values, and no unused
    values. Throws a ConfigError if any part of the input dict is invalid.
    """
    config_class =              TronConfig
    defaults = {
        'action_runner':        {},
        'output_stream_dir':    None,
        'command_context':      {},
        'ssh_options':          ValidateSSHOptions.defaults,
        'notification_options': None,
        'time_zone':            None,
        'state_persistence':    DEFAULT_STATE_PERSISTENCE,
        'nodes':                {'localhost': DEFAULT_NODE},
        'node_pools':           {},
        'jobs':                 (),
        'services':             (),
    }
    node_pools  = build_dict_name_validator(valid_node_pool, allow_empty=True)
    nodes       = build_dict_name_validator(valid_node, allow_empty=True)
    validators = {
        'action_runner':        ValidateActionRunner(),
        'output_stream_dir':    valid_output_stream_dir,
        'command_context':      valid_command_context,
        'ssh_options':          valid_ssh_options,
        'notification_options': valid_notification_options,
        'time_zone':            valid_time_zone,
        'state_persistence':    valid_state_persistence,
        'nodes':                nodes,
        'node_pools':           node_pools,
    }
    optional = False

    def validate_node_pool_nodes(self, config):
        """Validate that each node in a node_pool is in fact a node, and not
        another pool.
        """
        all_node_names = set(config['nodes'])
        for node_pool in config['node_pools'].itervalues():
            invalid_names = set(node_pool.nodes) - all_node_names
            if invalid_names:
                msg = "NodePool %s contains other NodePools: " % node_pool.name
                raise ConfigError(msg + ",".join(invalid_names))

    def post_validation(self, config, _):
        """Validate a non-named config."""
        node_names = config_utils.unique_names(
            'Node and NodePool names must be unique %s',
            config['nodes'], config.get('node_pools', []))

        if config.get('node_pools'):
            self.validate_node_pool_nodes(config)

        config_context = ConfigContext('config', node_names,
            config.get('command_context'), MASTER_NAMESPACE)
        validate_jobs_and_services(config, config_context)


class ValidateNamedConfig(Validator):
    """A shorter validator for named configurations, which allow for
    jobs and services to be defined as configuration fragments that
    are, in turn, reconciled by Tron.
    """
    config_class =              NamedTronConfig
    type_name =                 "NamedConfigFragment"
    defaults = {
        'jobs':                 (),
        'services':             ()
    }

    optional = False

    def post_validation(self, config, config_context):
        validate_jobs_and_services(config, config_context)


valid_config = ValidateConfig()
valid_named_config = ValidateNamedConfig()


def validate_fragment(name, fragment):
    """Validate a fragment with a partial context."""
    config_context = PartialConfigContext(name, name)
    if name == MASTER_NAMESPACE:
        return valid_config(fragment, config_context=config_context)
    return valid_named_config(fragment, config_context=config_context)


def get_nodes_from_master_namespace(master):
    return set(itertools.chain(master.nodes, master.node_pools))


def validate_config_mapping(config_mapping):
    if MASTER_NAMESPACE not in config_mapping:
        msg = "A config mapping requires a %s namespace"
        raise ConfigError(msg % MASTER_NAMESPACE)

    master = valid_config(config_mapping.pop(MASTER_NAMESPACE))
    nodes = get_nodes_from_master_namespace(master)
    yield MASTER_NAMESPACE, master

    for name, content in config_mapping.iteritems():
        context = ConfigContext(name, nodes, master.command_context, name)
        yield name, valid_named_config(content, config_context=context)


class ConfigContainer(object):
    """A container around configuration fragments (and master)."""

    def __init__(self, config_mapping):
        self.configs = config_mapping

    def iteritems(self):
        return self.configs.iteritems()

    @classmethod
    def create(cls, config_mapping):
        return cls(dict(validate_config_mapping(config_mapping)))

    # TODO: DRY with get_jobs(), get_services()
    def get_job_and_service_names(self):
        job_names, service_names = [], []
        for config in self.configs.itervalues():
            job_names.extend(config.jobs)
            service_names.extend(config.services)
        return job_names, service_names

    def get_jobs(self):
        return dict(itertools.chain.from_iterable(
            config.jobs.iteritems() for config in self.configs.itervalues()))

    def get_services(self):
        return dict(itertools.chain.from_iterable(
            config.services.iteritems() for config in self.configs.itervalues()))

    def get_master(self):
        return self.configs[MASTER_NAMESPACE]

    def get_node_names(self):
        return get_nodes_from_master_namespace(self.get_master())

    def __getitem__(self, name):
        return self.configs[name]

    def __contains__(self, name):
        return name in self.configs

########NEW FILE########
__FILENAME__ = config_utils
"""Utilities used for configuration parsing and validation."""
import functools
import itertools
import re
import datetime
from tron.config import ConfigError
from tron.config.schema import MASTER_NAMESPACE
from tron.utils import dicts
from tron.utils.dicts import FrozenDict


MAX_IDENTIFIER_LENGTH       = 255
IDENTIFIER_RE               = re.compile(r'^[A-Za-z_][\w\-]{0,254}$')


class UniqueNameDict(dict):
    """A dict like object that throws a ConfigError if a key exists and
    __setitem__ is called to change the value of that key.

     fmt_string - format string used to create an error message, expects a
                  single format argument of 'key'
    """
    def __init__(self, fmt_string):
        super(dict, self).__init__()
        self.fmt_string = fmt_string

    def __setitem__(self, key, value):
        if key in self:
            raise ConfigError(self.fmt_string % key)
        super(UniqueNameDict, self).__setitem__(key, value)


def unique_names(fmt_string, *seqs):
    """Validate that each object in all sequences has a unique name."""
    name_dict = UniqueNameDict(fmt_string)
    for item in itertools.chain.from_iterable(seqs):
        name_dict[item] = True
    return name_dict


def build_type_validator(validator, error_fmt):
    """Create a validator function using `validator` to validate the value.
        validator - a function which takes a single argument `value`
        error_fmt - a string which accepts two format variables (path, value)

        Returns a function func(value, config_context) where
            value - the value to validate
            config_context - a ConfigContext object
            Returns True if the value is valid
    """
    def f(value, config_context):
        if not validator(value):
            raise ConfigError(error_fmt % (config_context.path, value))
        return value
    return f


def valid_number(type_func, value, config_context):
    path = config_context.path
    try:
        value = type_func(value)
    except TypeError:
        name = type_func.__name__
        raise ConfigError('Value at %s is not an %s: %s' % (path, name, value))

    if value < 0:
        raise ConfigError('%s must be a positive int.' % path)

    return value

valid_int   = functools.partial(valid_number, int)
valid_float = functools.partial(valid_number, float)

valid_identifier = build_type_validator(
    lambda s: isinstance(s, basestring) and IDENTIFIER_RE.match(s),
    'Identifier at %s is not a valid identifier: %s')

valid_list = build_type_validator(
    lambda s: isinstance(s, list), 'Value at %s is not a list: %s')

valid_string  = build_type_validator(
    lambda s: isinstance(s, basestring), 'Value at %s is not a string: %s')

valid_dict = build_type_validator(
    lambda s: isinstance(s, dict), 'Value at %s is not a dictionary: %s')

valid_bool = build_type_validator(
    lambda s: isinstance(s, bool), 'Value at %s is not a boolean: %s')


def build_enum_validator(enum):
    enum = set(enum)
    msg = 'Value at %%s is not in %s: %%s.' % str(enum)
    return build_type_validator(enum.__contains__, msg)


def valid_time(value, config_context):
    valid_string(value, config_context)
    for format in ['%H:%M', '%H:%M:%S']:
        try:
            return datetime.datetime.strptime(value, format)
        except ValueError, exc:
            pass

    msg = 'Value at %s is not a valid time: %s'
    raise ConfigError(msg % (config_context.path, exc))


# Translations from possible configuration units to the argument to
# datetime.timedelta
TIME_INTERVAL_UNITS = dicts.invert_dict_list({
    'days':     ['d', 'day', 'days'],
    'hours':    ['h', 'hr', 'hrs', 'hour', 'hours'],
    'minutes':  ['m', 'min', 'mins', 'minute', 'minutes'],
    'seconds':  ['s', 'sec', 'secs', 'second', 'seconds']
})

TIME_INTERVAL_RE = re.compile(r"^\s*(?P<value>\d+)\s*(?P<units>[a-zA-Z]+)\s*$")

def valid_time_delta(value, config_context):
    error_msg = "Value at %s is not a valid time delta: %s"
    matches = TIME_INTERVAL_RE.match(value)
    if not matches:
        raise ConfigError(error_msg % (config_context.path, value))

    units = matches.group('units')
    if units not in TIME_INTERVAL_UNITS:
        raise ConfigError(error_msg % (config_context.path, value))

    time_spec = {TIME_INTERVAL_UNITS[units]: int(matches.group('value'))}
    return datetime.timedelta(**time_spec)


def valid_name_identifier(value, config_context):
    valid_identifier(value, config_context)
    if config_context.partial:
        return value
    return '%s.%s' % (config_context.namespace, value)


def build_list_of_type_validator(item_validator, allow_empty=False):
    """Build a validator which validates a list contains items which pass
    item_validator.
    """
    def validator(value, config_context):
        if allow_empty and not value:
            return ()
        seq = valid_list(value, config_context)
        if not seq:
            msg = "Required non-empty list at %s"
            raise ConfigError(msg % config_context.path)
        return tuple(item_validator(item, config_context) for item in seq)
    return validator


def build_dict_name_validator(item_validator, allow_empty=False):
    """Build a validator which validates a list, and returns a dict."""
    valid = build_list_of_type_validator(item_validator, allow_empty)
    def validator(value, config_context):
        msg = "Duplicate name %%s at %s" % config_context.path
        name_dict = UniqueNameDict(msg)
        for item in valid(value, config_context):
            name_dict[item.name] = item
        return FrozenDict(**name_dict)
    return validator


class ConfigContext(object):
    """An object to encapsulate the context in a configuration file. Supplied
    to Validators to perform validation which requires knowledge of
    configuration outside of the immediate configuration dictionary.
    """
    partial = False

    def __init__(self, path, nodes, command_context, namespace):
        self.path = path
        self.nodes = set(nodes or [])
        self.command_context = command_context or {}
        self.namespace = namespace

    def build_child_context(self, path):
        """Construct a new ConfigContext based on this one."""
        path = '%s.%s' % (self.path, path)
        args = path, self.nodes, self.command_context, self.namespace
        return ConfigContext(*args)


class PartialConfigContext(object):
    """A context object which has only a partial context. It is missing
    command_context and nodes.  This is likely because it is being used in
    a named configuration fragment that does not have access to those pieces
    of the configuration.
    """
    partial = True

    def __init__(self, path, namespace):
        self.path = path
        self.namespace = namespace

    def build_child_context(self, path):
        path = '%s.%s' % (self.path, path)
        return PartialConfigContext(path, self.namespace)


class NullConfigContext(object):
    path = ''
    nodes = set()
    command_context = {}
    namespace = MASTER_NAMESPACE
    partial = False

    @staticmethod
    def build_child_context(_):
        return NullConfigContext


# TODO: extract code
class Validator(object):
    """Base class for validating a collection and creating a mutable
    collection from the source.
    """
    config_class            = None
    defaults                = {}
    validators              = {}
    optional                = False

    def validate(self, in_dict, config_context):
        if self.optional and in_dict is None:
            return None

        if in_dict is None:
            raise ConfigError("A %s is required." % self.type_name)

        shortcut_value = self.do_shortcut(in_dict)
        if shortcut_value:
            return shortcut_value

        config_context = self.build_context(in_dict, config_context)
        in_dict = self.cast(in_dict, config_context)
        self.validate_required_keys(in_dict)
        self.validate_extra_keys(in_dict)
        return self.build_config(in_dict, config_context)

    def __call__(self, in_dict, config_context=NullConfigContext):
        return self.validate(in_dict, config_context)

    @property
    def type_name(self):
        """Return a string that represents the config_class being validated.
        This name is used for error messages, so we strip off the word
        Config so the name better matches what the user sees in the config.
        """
        return self.config_class.__name__.replace("Config", "")

    @property
    def all_keys(self):
        return self.config_class.required_keys + self.config_class.optional_keys

    def do_shortcut(self, in_dict):
        """Override if your validator can skip most of the validation by
        checking this condition.  If this returns a truthy value, the
        validation will end immediately and return that value.
        """
        pass

    def cast(self, in_dict, _):
        """If your validator accepts input in different formations, override
        this method to cast your input into a common format.
        """
        return in_dict

    def build_context(self, in_dict, config_context):
        path = self.path_name(in_dict.get('name'))
        return config_context.build_child_context(path)

    def validate_required_keys(self, in_dict):
        """Check that all required keys are present."""
        missing_keys = set(self.config_class.required_keys) - set(in_dict)
        if not missing_keys:
            return

        missing_key_str = ', '.join(missing_keys)
        if 'name' in self.all_keys and 'name' in in_dict:
            msg  = "%s %s is missing options: %s"
            name = in_dict['name']
            raise ConfigError(msg % (self.type_name, name, missing_key_str))

        msg = "Nameless %s is missing options: %s"
        raise ConfigError(msg % (self.type_name, missing_key_str))

    def validate_extra_keys(self, in_dict):
        """Check that no unexpected keys are present."""
        extra_keys = set(in_dict) - set(self.all_keys)
        if not extra_keys:
            return

        msg  = "Unknown keys in %s %s: %s"
        name = in_dict.get('name', '')
        raise ConfigError(msg % (self.type_name, name, ', '.join(extra_keys)))

    def set_defaults(self, output_dict, _config_context):
        """Set any default values for any optional values that were not
        specified.
        """
        for key, value in self.defaults.iteritems():
            output_dict.setdefault(key, value)

    def path_name(self, name=None):
        return '%s.%s' % (self.type_name, name) if name else self.type_name

    def post_validation(self, valid_input, config_context):
        """Hook to perform additional validation steps after key validation
        completes.
        """
        pass

    def build_config(self, in_dict, config_context):
        """Construct the configuration by validating the contents, setting
        defaults, and returning an instance of the config_class.
        """
        output_dict = self.validate_contents(in_dict, config_context)
        self.post_validation(output_dict, config_context)
        self.set_defaults(output_dict, config_context)
        return self.config_class(**output_dict)

    def validate_contents(self, input, config_context):
        """Override this to validate each value in the input."""
        valid_input = {}
        for key, value in input.iteritems():
            if key in self.validators:
                child_context = config_context.build_child_context(key)
                valid_input[key] = self.validators[key](value, child_context)
            else:
                valid_input[key] = value
        return valid_input
########NEW FILE########
__FILENAME__ = manager
import hashlib
import logging
import os
import yaml

from tron.config import schema, config_parse, ConfigError


log = logging.getLogger(__name__)

def from_string(content):
    try:
        return yaml.load(content)
    except yaml.error.YAMLError, e:
        raise ConfigError("Invalid config format: %s" % str(e))


def write(path, content):
    with open(path, 'w') as fh:
        yaml.dump(content, fh)


def read(path):
    with open(path, 'r') as fh:
        return from_string(fh)


def write_raw(path, content):
    with open(path, 'w') as fh:
        fh.write(content)


def read_raw(path):
    with open(path, 'r') as fh:
        return fh.read()


def hash_digest(content):
    return hashlib.sha1(content).hexdigest()


class ManifestFile(object):
    """Manage the manifest file, which tracks name to filename."""

    MANIFEST_FILENAME = '_manifest.yaml'

    def __init__(self, path):
        self.filename = os.path.join(path, self.MANIFEST_FILENAME)

    def create(self):
        if os.path.isfile(self.filename):
            msg = "Refusing to create manifest. File %s exists."
            log.info(msg % self.filename)
            return

        write(self.filename, {})

    def add(self, name, filename):
        manifest = read(self.filename)
        manifest[name] = filename
        write(self.filename, manifest)

    def get_file_mapping(self):
        return read(self.filename)

    def get_file_name(self, name):
        return self.get_file_mapping().get(name)

    def __contains__(self, name):
        return name in self.get_file_mapping()


class ConfigManager(object):
    """Read, load and write configuration."""

    DEFAULT_HASH = hash_digest("")

    def __init__(self, config_path):
        self.config_path = config_path
        self.manifest = ManifestFile(config_path)

    def build_file_path(self, name):
        name = name.replace('.', '_').replace(os.path.sep, '_')
        return os.path.join(self.config_path, '%s.yaml' % name)

    def read_raw_config(self, name=schema.MASTER_NAMESPACE):
        """Read the config file without converting to yaml."""
        filename = self.manifest.get_file_name(name)
        return read_raw(filename)

    def write_config(self, name, content):
        self.validate_with_fragment(name, from_string(content))
        filename = self.get_filename_from_manifest(name)
        write_raw(filename, content)

    def get_filename_from_manifest(self, name):
        def create_filename():
            filename = self.build_file_path(name)
            self.manifest.add(name, filename)
            return filename
        return self.manifest.get_file_name(name) or create_filename()

    def validate_with_fragment(self, name, content):
        name_mapping = self.get_config_name_mapping()
        name_mapping[name] = content
        config_parse.ConfigContainer.create(name_mapping)

    def get_config_name_mapping(self):
        seq = self.manifest.get_file_mapping().iteritems()
        return dict((name, read(filename)) for name, filename in seq)

    def load(self):
        """Return the fully constructed configuration."""
        log.info("Loading full config from %s" % self.config_path)
        name_mapping = self.get_config_name_mapping()
        return config_parse.ConfigContainer.create(name_mapping)

    def get_hash(self, name):
        """Return a hash of the configuration contents for name."""
        if name not in self:
            return self.DEFAULT_HASH
        return hash_digest(self.read_raw_config(name))

    def __contains__(self, name):
        return name in self.manifest

    def get_namespaces(self):
        return self.manifest.get_file_mapping().keys()


def create_new_config(path, master_content):
    """Create a new configuration directory with master config."""
    os.makedirs(path)
    manager = ConfigManager(path)
    manager.manifest.create()
    filename = manager.get_filename_from_manifest(schema.MASTER_NAMESPACE)
    write_raw(filename , master_content)
########NEW FILE########
__FILENAME__ = schedule_parse
"""
Parse and validate scheduler configuration and return immutable structures.
"""
import calendar
from collections import namedtuple
import datetime
import re

from tron.config import ConfigError, config_utils, schema
from tron.utils import crontab


ConfigGenericSchedule = schema.config_object_factory(
    'ConfigGenericSchedule',
    ['type', 'value'],
    ['jitter']
)

ConfigGrocScheduler = namedtuple('ConfigGrocScheduler',
    'original ordinals weekdays monthdays months timestr jitter')

ConfigCronScheduler = namedtuple('ConfigCronScheduler',
    'original minutes hours monthdays months weekdays ordinals jitter')

ConfigDailyScheduler    = namedtuple('ConfigDailyScheduler',
    'original hour minute second days jitter')

ConfigConstantScheduler = namedtuple('ConfigConstantScheduler', [])

ConfigIntervalScheduler = namedtuple('ConfigIntervalScheduler',
    'timedelta jitter')


class ScheduleParseError(ConfigError):
    pass


def pad_sequence(seq, size, padding=None):
    """Force a sequence to size. Pad with padding if too short, and ignore
    extra pieces if too long."""
    return (list(seq) + [padding for _ in xrange(size)])[:size]


def schedule_config_from_string(schedule, config_context):
    """Return a scheduler config object from a string."""
    schedule = schedule.strip()
    name, schedule_config = pad_sequence(schedule.split(None, 1), 2, padding='')
    if name not in schedulers:
        config = ConfigGenericSchedule('groc daily', schedule, jitter=None)
        return parse_groc_expression(config, config_context)

    config = ConfigGenericSchedule(name, schedule_config, jitter=None)
    return validate_generic_schedule_config(config, config_context)


def validate_generic_schedule_config(config, config_context):
    return schedulers[config.type](config, config_context)


# TODO: remove in 0.7
def schedule_config_from_legacy_dict(schedule, config_context):
    """Support old style schedules as dicts."""
    if 'interval' in schedule:
        config = ConfigGenericSchedule('interval', schedule['interval'], None)
        return valid_interval_scheduler(config, config_context)

    if 'start_time' in schedule or 'days' in schedule:
        start_time = schedule.get('start_time', '00:00:00')
        days = schedule.get('days', '')
        scheduler_config = '%s %s' % (start_time, days)
        config = ConfigGenericSchedule('daily', scheduler_config, None)
        return valid_daily_scheduler(config, config_context)

    path = config_context.path
    raise ConfigError("Unknown scheduler at %s: %s" % (path, schedule))


def valid_schedule(schedule, config_context):
    if isinstance(schedule, basestring):
        return schedule_config_from_string(schedule, config_context)

    if 'type' not in schedule:
        return schedule_config_from_legacy_dict(schedule, config_context)

    schedule = ScheduleValidator().validate(schedule, config_context)
    return validate_generic_schedule_config(schedule, config_context)


def valid_constant_scheduler(_config, _context):
    """Adapter for validation interface and constant scheduler."""
    return ConfigConstantScheduler()


def valid_daily_scheduler(config, config_context):
    """Daily scheduler, accepts a time of day and an optional list of days."""
    schedule_config = config.value
    time_string, days = pad_sequence(schedule_config.split(), 2)
    time_string = time_string or '00:00:00'
    time_spec   = config_utils.valid_time(time_string, config_context)
    days        = config_utils.valid_string(days or "", config_context)

    def valid_day(day):
        if day not in CONVERT_DAYS_INT:
            raise ConfigError("Unknown day %s at %s" % (day, config_context.path))
        return CONVERT_DAYS_INT[day]

    original = "%s %s" % (time_string, days)
    weekdays = set(valid_day(day) for day in days or ())
    return ConfigDailyScheduler(original,
        time_spec.hour, time_spec.minute, time_spec.second, weekdays,
        jitter=config.jitter)


# Shortcut values for intervals
TIME_INTERVAL_SHORTCUTS = {
    'hourly': datetime.timedelta(hours=1)
}

def valid_interval_scheduler(config,  config_context):
    def build_config(delta):
        return ConfigIntervalScheduler(timedelta=delta, jitter=config.jitter)

    interval_key = config.value.strip()
    if interval_key in TIME_INTERVAL_SHORTCUTS:
        return build_config(TIME_INTERVAL_SHORTCUTS[interval_key])

    return build_config(
        config_utils.valid_time_delta(config.value, config_context))


def normalize_weekdays(seq):
    return seq[6:7] + seq[:6]

def day_canonicalization_map():
    """Build a map of weekday synonym to int index 0-6 inclusive."""
    canon_map = dict()

    # 7-element lists with weekday names in order
    weekday_lists = [
        normalize_weekdays(calendar.day_name),
        normalize_weekdays(calendar.day_abbr),
        ('u', 'm', 't', 'w', 'r', 'f', 's',),
        ('su', 'mo', 'tu', 'we', 'th', 'fr', 'sa',)]
    for day_list in weekday_lists:
        for day_name_synonym, day_index in zip(day_list, range(7)):
            canon_map[day_name_synonym] = day_index
            canon_map[day_name_synonym.lower()] = day_index
            canon_map[day_name_synonym.upper()] = day_index

    return canon_map

# Canonicalize weekday names to integer indices
CONVERT_DAYS_INT = day_canonicalization_map()   # day name/abbrev => {0123456}


def month_canonicalization_map():
    """Build a map of month synonym to int index 0-11 inclusive."""
    canon_map = dict()

    # calendar stores month data with a useless element in front. cut it off.
    monthname_lists = (calendar.month_name[1:], calendar.month_abbr[1:])
    for month_list in monthname_lists:
        for key, value in zip(month_list, range(1, 13)):
            canon_map[key] = value
            canon_map[key.lower()] = value
    return canon_map


# Canonicalize month names to integer indices
# month name/abbrev => {0 <= k <= 11}
CONVERT_MONTHS = month_canonicalization_map()


def build_groc_schedule_parser_re():
    """Build a regular expression that matches this:

        ("every"|ordinal) (day) ["of|in" (monthspec)] (["at"] HH:MM)

    ordinal   - comma-separated list of "1st" and so forth
    days      - comma-separated list of days of the week (for example,
                "mon", "tuesday", with both short and long forms being
                accepted); "every day" is equivalent to
                "every mon,tue,wed,thu,fri,sat,sun"
    monthspec - comma-separated list of month names (for example,
                "jan", "march", "sep"). If omitted, implies every month.
                You can also say "month" to mean every month, as in
                "1,8th,15,22nd of month 09:00".
    HH:MM     - time of day in 24 hour time.

    This is a slightly more permissive version of Google App Engine's schedule
    parser, documented here:
    http://code.google.com/appengine/docs/python/config/cron.html#The_Schedule_Format
    """

    # m|mon|monday|...|day
    DAY_VALUES = '|'.join(CONVERT_DAYS_INT.keys() + ['day'])

    # jan|january|...|month
    MONTH_VALUES = '|'.join(CONVERT_MONTHS.keys() + ['month'])

    DATE_SUFFIXES = 'st|nd|rd|th'

    # every|1st|2nd|3rd (also would accept 3nd, 1rd, 4st)
    MONTH_DAYS_EXPR = '(?P<month_days>every|((\d+(%s),?)+))?' % DATE_SUFFIXES
    DAYS_EXPR = r'((?P<days>((%s),?)+))?' % DAY_VALUES
    MONTHS_EXPR = r'((in|of)\s+(?P<months>((%s),?)+))?' % MONTH_VALUES

    # [at] 00:00
    TIME_EXPR = r'((at\s+)?(?P<time>\d\d:\d\d))?'

    DAILY_SCHEDULE_EXPR = ''.join([
        r'^',
        MONTH_DAYS_EXPR, r'\s*',
        DAYS_EXPR, r'\s*',
        MONTHS_EXPR, r'\s*',
         TIME_EXPR, r'\s*',
        r'$'
    ])
    return re.compile(DAILY_SCHEDULE_EXPR)


# Matches expressions of the form
# ``("every"|ordinal) (days) ["of|in" (monthspec)] (["at"] HH:MM)``.
# See :py:func:`daily_schedule_parser_re` for details.
DAILY_SCHEDULE_RE = build_groc_schedule_parser_re()


def _parse_number(day):
    return int(''.join(c for c in day if c.isdigit()))

def parse_groc_expression(config, config_context):
    """Given an expression of the form in the docstring of
    daily_schedule_parser_re(), return the parsed values in a
    ConfigGrocScheduler
    """
    expression = config.value
    m = DAILY_SCHEDULE_RE.match(expression.lower())
    if not m:
        msg = 'Schedule at %s is not a valid expression: %s'
        raise ScheduleParseError(msg % (config_context.path, expression))

    timestr = m.group('time')
    if timestr is None:
        timestr = '00:00'

    if m.group('days') in (None, 'day'):
        weekdays = None
    else:
        weekdays = set(CONVERT_DAYS_INT[d] for d in m.group('days').split(','))

    monthdays = None
    ordinals = None

    if m.group('month_days') != 'every':
        values = set(_parse_number(n)
                     for n in m.group('month_days').split(','))
        if weekdays is None:
            monthdays = values
        else:
            ordinals = values

    if m.group('months') in (None, 'month'):
        months = None
    else:
        months = set(CONVERT_MONTHS[mo] for mo in m.group('months').split(','))

    return ConfigGrocScheduler(
        original=expression,
        ordinals=ordinals,
        weekdays=weekdays,
        monthdays=monthdays,
        months=months,
        timestr=timestr,
        jitter=config.jitter)


def valid_cron_scheduler(config, config_context):
    """Parse a cron schedule."""
    try:
        crontab_kwargs = crontab.parse_crontab(config.value)
        return ConfigCronScheduler(
            original=config.value, jitter=config.jitter, **crontab_kwargs)
    except ValueError, e:
        msg = "Invalid cron scheduler %s: %s"
        raise ConfigError(msg % (config_context.path, e))


schedulers = {
    'constant':     valid_constant_scheduler,
    'daily':        valid_daily_scheduler,
    'interval':     valid_interval_scheduler,
    'cron':         valid_cron_scheduler,
    'groc daily':   parse_groc_expression,
}


class ScheduleValidator(config_utils.Validator):
    """Validate the structure of a scheduler config."""
    config_class = ConfigGenericSchedule
    defaults = {
        'jitter':       datetime.timedelta()
    }
    validators = {
        'type':         config_utils.build_enum_validator(schedulers.keys()),
        'jitter':       config_utils.valid_time_delta
    }

########NEW FILE########
__FILENAME__ = schema
"""
 Immutable config schema objects.
"""
from collections import namedtuple
from tron.utils.collections import Enum


MASTER_NAMESPACE = "MASTER"

CLEANUP_ACTION_NAME = 'cleanup'

def config_object_factory(name, required=None, optional=None):
    """
    Creates a namedtuple which has two additional attributes:
        required_keys:
            all keys required to be set on this configuration object
        optional keys:
            optional keys for this configuration object

    The tuple is created from required + optional
    """
    required = required or []
    optional = optional or []
    config_class = namedtuple(name, required + optional)
    config_class.required_keys = required
    config_class.optional_keys = optional
    return config_class


TronConfig = config_object_factory(
    'TronConfig',
    optional=[
        'output_stream_dir',   # str
        'action_runner',       # ConfigActionRunner
        'state_persistence',   # ConfigState
        'command_context',     # FrozenDict of str
        'ssh_options',         # ConfigSSHOptions
        'notification_options',# NotificationOptions or None
        'time_zone',           # pytz time zone
        'nodes',               # FrozenDict of ConfigNode
        'node_pools',          # FrozenDict of ConfigNodePool
        'jobs',                # FrozenDict of ConfigJob
        'services',            # FrozenDict of ConfigService
    ])


NamedTronConfig = config_object_factory(
    'NamedTronConfig',
    optional=[
        'jobs',                # FrozenDict of ConfigJob
        'services'             # FrozenDict of ConfigService
    ])


NotificationOptions = config_object_factory(
    'NotificationOptions',
    [
        'smtp_host',            # str
        'notification_addr',    # str
    ])


ConfigActionRunner = config_object_factory('ConfigActionRunner',
    optional=['runner_type', 'remote_status_path', 'remote_exec_path'])


ConfigSSHOptions = config_object_factory(
    'ConfigSSHOptions',
    optional=[
        'agent',
        'identities',
        'known_hosts_file',
        'connect_timeout',
        'idle_connection_timeout',
        'jitter_min_load',
        'jitter_max_delay',
        'jitter_load_factor',
    ])


ConfigNode = config_object_factory('ConfigNode',
    ['hostname'], ['name', 'username', 'port'])


ConfigNodePool = config_object_factory('ConfigNodePool', ['nodes'], ['name'])


ConfigState = config_object_factory(
    'ConfigState',
    [
        'name',
        'store_type',
        ],[
        'connection_details',
        'buffer_size'
    ])


ConfigJob = config_object_factory(
    'ConfigJob',
    [
        'name',                 # str
        'node',                 # str
        'schedule',             # Config*Scheduler
        'actions',              # FrozenDict of ConfigAction
        'namespace',            # str
    ],[
        'queueing',             # bool
        'run_limit',            # int
        'all_nodes',            # bool
        'cleanup_action',       # ConfigAction
        'enabled',              # bool
        'allow_overlap',        # bool
        'max_runtime',          # datetime.Timedelta
    ])


ConfigAction = config_object_factory(
    'ConfigAction',
    [
        'name',                 # str
        'command',              # str
    ],[
        'requires',             # tuple of str
        'node',                 # str
    ])

ConfigCleanupAction = config_object_factory(
    'ConfigCleanupAction',
    [
        'command',              # str
    ],[
        'name',                 # str
        'node',                 # str
    ])


ConfigService = config_object_factory(
    'ConfigService',
    [
        'name',                 # str
        'node',                 # str
        'pid_file',             # str
        'command',              # str
        'monitor_interval',     # float
        'namespace',            # str
    ],[
        'restart_delay',        # float
        'monitor_retries',      # int
        'count',                # int
    ])


StatePersistenceTypes = Enum.create('shelve', 'sql', 'mongo', 'yaml')


ActionRunnerTypes = Enum.create('none', 'subprocess')

########NEW FILE########
__FILENAME__ = action
import logging
from tron import node
from tron.config.schema import CLEANUP_ACTION_NAME

log = logging.getLogger(__name__)


class Action(object):
    """A configurable data object for an Action."""

    def __init__(self, name, command, node_pool, required_actions=None,
                dependent_actions=None):
        self.name               = name
        self.command            = command
        self.node_pool          = node_pool
        self.required_actions   = required_actions or []
        self.dependent_actions  = dependent_actions or []

    @property
    def is_cleanup(self):
        return self.name == CLEANUP_ACTION_NAME

    @classmethod
    def from_config(cls, config):
        """Factory method for creating a new Action."""
        node_repo = node.NodePoolRepository.get_instance()
        return cls(
            name=       config.name,
            command=    config.command,
            node_pool=  node_repo.get_by_name(config.node))

    def __eq__(self, other):
        attributes_match = all(
            getattr(self, attr, None) == getattr(other, attr, None)
            for attr in ['name', 'command', 'node_pool', 'is_cleanup']
        )
        return attributes_match and all(
            self_act == other_act for (self_act, other_act)
            in zip(self.required_actions, other.required_actions)
        )

    def __ne__(self, other):
        return not self == other

########NEW FILE########
__FILENAME__ = actiongraph
import logging
from tron.core import action

log = logging.getLogger(__name__)


class ActionGraph(object):
    """A directed graph of actions and their requirements."""

    def __init__(self, graph, action_map):
        self.graph              = graph
        self.action_map         = action_map

    @classmethod
    def from_config(cls, actions_config, cleanup_action_config=None):
        """Create this graph from a job config."""
        actions = dict((name, action.Action.from_config(conf))
                        for name, conf in actions_config.iteritems())
        if cleanup_action_config:
            cleanup_action = action.Action.from_config(cleanup_action_config)
            actions[cleanup_action.name] = cleanup_action

        return cls(cls._build_dag(actions, actions_config), actions)

    @classmethod
    def _build_dag(cls, actions, actions_config):
        """Return a directed graph from a dict of actions keyed by name."""
        base = []
        for action in actions.itervalues():
            dependencies = cls._get_dependencies(actions_config, action.name)
            if not dependencies:
                base.append(action)
                continue

            for dependency in dependencies:
                dependency_action = actions[dependency]
                action.required_actions.append(dependency_action)
                dependency_action.dependent_actions.append(action)
        return base

    @classmethod
    def _get_dependencies(cls, actions_config, action_name):
        if action_name == action.CLEANUP_ACTION_NAME:
            return []
        return actions_config[action_name].requires

    def actions_for_names(self, names):
        return (self.action_map[name] for name in names)

    def get_required_actions(self, name):
        """Given an Action's name return the Actions required to run
        before that Action.
        """
        if name not in self.action_map:
            return []
        return self.action_map[name].required_actions

    def get_dependent_actions(self, name):
        return self.action_map[name].dependent_actions

    def get_actions(self):
        return self.action_map.itervalues()

    def get_action_map(self):
        return self.action_map

    @property
    def names(self):
        return self.action_map.keys()

    def __getitem__(self, name):
        return self.action_map[name]

    def __eq__(self, other):
        return self.graph == other.graph and self.action_map == other.action_map

    def __ne__(self, other):
        return not self == other

########NEW FILE########
__FILENAME__ = actionrun
"""
 tron.core.actionrun
"""
import logging
import traceback
import itertools
from tron import command_context
from tron.core import action
from tron.serialize import filehandler
from tron import node
from tron.actioncommand import ActionCommand, NoActionRunnerFactory

from tron.utils import state, timeutils, proxy, iteration
from tron.utils.observer import Observer

log = logging.getLogger(__name__)


class ActionRunFactory(object):
    """Construct ActionRuns and ActionRunCollections for a JobRun and
    ActionGraph.
    """

    @classmethod
    def build_action_run_collection(cls, job_run, action_runner):
        """Create an ActionRunGraph from an ActionGraph and JobRun."""
        action_map = job_run.action_graph.get_action_map().iteritems()
        action_run_map = dict(
            (name, cls.build_run_for_action(job_run, action_inst, action_runner))
            for name, action_inst in action_map)
        return ActionRunCollection(job_run.action_graph, action_run_map)

    @classmethod
    def action_run_collection_from_state(cls, job_run, runs_state_data,
                cleanup_action_state_data):
        action_runs = [cls.action_run_from_state(job_run, state_data)
                       for state_data in runs_state_data]
        if cleanup_action_state_data:
            action_runs.append(cls.action_run_from_state(
                job_run, cleanup_action_state_data, cleanup=True))

        action_run_map = dict(
            (action_run.action_name, action_run) for action_run in action_runs)
        return ActionRunCollection(job_run.action_graph, action_run_map)

    @classmethod
    def build_run_for_action(cls, job_run, action, action_runner):
        """Create an ActionRun for a JobRun and Action."""
        run_node = action.node_pool.next() if action.node_pool else job_run.node

        return ActionRun(
            job_run.id,
            action.name,
            run_node,
            action.command,
            parent_context=job_run.context,
            output_path=job_run.output_path.clone(),
            cleanup=action.is_cleanup,
            action_runner=action_runner)

    @classmethod
    def action_run_from_state(cls, job_run, state_data, cleanup=False):
        """Restore an ActionRun for this JobRun from the state data."""
        return ActionRun.from_state(
            state_data,
            job_run.context,
            job_run.output_path.clone(),
            job_run.node,
            cleanup=cleanup)


class ActionRun(Observer):
    """Tracks the state of a single run of an Action.

    ActionRuns observers ActionCommands they create and are observed by a
    parent JobRun.
    """
    STATE_CANCELLED     = state.NamedEventState('cancelled')
    STATE_UNKNOWN       = state.NamedEventState('unknown', short_name='UNKWN')
    STATE_FAILED        = state.NamedEventState('failed')
    STATE_SUCCEEDED     = state.NamedEventState('succeeded')
    STATE_RUNNING       = state.NamedEventState('running')
    STATE_STARTING      = state.NamedEventState('starting', short_chars=5)
    STATE_QUEUED        = state.NamedEventState('queued')
    STATE_SCHEDULED     = state.NamedEventState('scheduled')
    STATE_SKIPPED       = state.NamedEventState('skipped')

    STATE_SCHEDULED['ready']    = STATE_QUEUED
    STATE_SCHEDULED['queue']    = STATE_QUEUED
    STATE_SCHEDULED['cancel']   = STATE_CANCELLED
    STATE_SCHEDULED['start']    = STATE_STARTING

    STATE_QUEUED['cancel']      = STATE_CANCELLED
    STATE_QUEUED['start']       = STATE_STARTING
    STATE_QUEUED['schedule']    = STATE_SCHEDULED

    STATE_STARTING['started']   = STATE_RUNNING
    STATE_STARTING['fail']      = STATE_FAILED

    STATE_RUNNING['fail']       = STATE_FAILED
    STATE_RUNNING['fail_unknown'] = STATE_UNKNOWN
    STATE_RUNNING['success']    = STATE_SUCCEEDED

    STATE_FAILED['skip']        = STATE_SKIPPED
    STATE_CANCELLED ['skip']    = STATE_SKIPPED

    # We can force many states to be success or failure
    for event_state in (STATE_UNKNOWN, STATE_QUEUED, STATE_SCHEDULED):
        event_state['success']  = STATE_SUCCEEDED
        event_state['fail']     = STATE_FAILED

    # The set of states that are considered end states. Technically some of
    # these states can be manually transitioned to other states.
    END_STATES = set(
        (STATE_FAILED,
         STATE_SUCCEEDED,
         STATE_CANCELLED,
         STATE_SKIPPED,
         STATE_UNKNOWN)
    )

    # Failed render command is false to ensure that it will fail when run
    FAILED_RENDER = 'false'

    context_class               = command_context.ActionRunContext

    # TODO: create a class for ActionRunId, JobRunId, Etc
    def __init__(self, job_run_id, name, node, bare_command=None,
            parent_context=None, output_path=None, cleanup=False,
            start_time=None, end_time=None, run_state=STATE_SCHEDULED,
            rendered_command=None, exit_status=None, action_runner=None):
        self.job_run_id         = job_run_id
        self.action_name        = name
        self.node               = node
        self.start_time         = start_time
        self.end_time           = end_time
        self.exit_status        = exit_status
        self.bare_command       = bare_command
        self.rendered_command   = rendered_command
        self.action_runner      = action_runner or NoActionRunnerFactory
        self.machine            = state.StateMachine(
                    self.STATE_SCHEDULED, delegate=self, force_state=run_state)
        self.is_cleanup         = cleanup
        self.output_path        = output_path or filehandler.OutputPath()
        self.output_path.append(self.id)
        self.context = command_context.build_context(self, parent_context)

    @property
    def state(self):
        return self.machine.state

    @property
    def attach(self):
        return self.machine.attach

    @property
    def id(self):
        return "%s.%s" % (self.job_run_id, self.action_name)

    def check_state(self, state):
        """Check if the state machine can be transitioned to state."""
        return self.machine.check(state)

    @classmethod
    def from_state(cls, state_data, parent_context, output_path,
                job_run_node, cleanup=False):
        """Restore the state of this ActionRun from a serialized state."""
        pool_repo = node.NodePoolRepository.get_instance()

        # Support state from older version
        if 'id' in state_data:
            job_run_id, action_name = state_data['id'].rsplit('.', 1)
        else:
            job_run_id = state_data['job_run_id']
            action_name = state_data['action_name']

        job_run_node = pool_repo.get_node(
            state_data.get('node_name'), job_run_node)

        rendered_command = state_data.get('rendered_command')
        run = cls(
            job_run_id,
            action_name,
            job_run_node,
            parent_context=parent_context,
            output_path=output_path,
            rendered_command=rendered_command,
            bare_command=state_data['command'],
            cleanup=cleanup,
            start_time=state_data['start_time'],
            end_time=state_data['end_time'],
            run_state=state.named_event_by_name(
                    cls.STATE_SCHEDULED, state_data['state']),
            exit_status=state_data.get('exit_status')
        )

        # Transition running to fail unknown because exit status was missed
        if run.is_running:
            run._done('fail_unknown')
        if run.is_starting:
            run.fail(None)
        return run

    def start(self):
        """Start this ActionRun."""
        if not self.machine.check('start'):
            return False

        log.info("Starting action run %s", self.id)
        self.start_time = timeutils.current_time()
        self.machine.transition('start')

        if not self.is_valid_command:
            log.error("Command for action run %s is invalid: %r",
                self.id, self.bare_command)
            self.fail(-1)
            return

        action_command = self.build_action_command()
        try:
            self.node.submit_command(action_command)
        except node.Error, e:
            log.warning("Failed to start %s: %r", self.id, e)
            self.fail(-2)
            return

        return True

    def stop(self):
        stop_command = self.action_runner.build_stop_action_command(
            self.id, 'terminate')
        self.node.submit_command(stop_command)

    def kill(self):
        kill_command = self.action_runner.build_stop_action_command(
            self.id, 'kill')
        self.node.submit_command(kill_command)

    def build_action_command(self):
        """Create a new ActionCommand instance to send to the node."""
        serializer = filehandler.OutputStreamSerializer(self.output_path)
        self.action_command = self.action_runner.create(
            self.id, self.command, serializer)
        self.watch(self.action_command)
        return self.action_command

    def handle_action_command_state_change(self, action_command, event):
        """Observe ActionCommand state changes."""
        log.debug("Action command state change: %s", action_command.state)

        if event == ActionCommand.RUNNING:
            return self.machine.transition('started')

        if event == ActionCommand.FAILSTART:
            return self.fail(None)

        if event == ActionCommand.EXITING:
            if action_command.exit_status is None:
                return self.fail_unknown()

            if not action_command.exit_status:
                return self.success()

            return self.fail(action_command.exit_status)
    handler = handle_action_command_state_change

    def _done(self, target, exit_status=0):
        log.info("Action run %s completed with %s and exit status %r",
            self.id, target, exit_status)
        if self.machine.check(target):
            self.exit_status = exit_status
            self.end_time = timeutils.current_time()
            return self.machine.transition(target)

    def fail(self, exit_status=0):
        return self._done('fail', exit_status)

    def success(self):
        return self._done('success')

    def fail_unknown(self):
        """Failed with unknown reason."""
        log.warn("Lost communication with action run %s", self.id)
        return self.machine.transition('fail_unknown')

    @property
    def state_data(self):
        """This data is used to serialize the state of this action run."""
        rendered_command = self.rendered_command
        # Freeze command after it's run
        command = rendered_command if rendered_command else self.bare_command
        return {
            'job_run_id':       self.job_run_id,
            'action_name':      self.action_name,
            'state':            str(self.state),
            'start_time':       self.start_time,
            'end_time':         self.end_time,
            'command':          command,
            'rendered_command': self.rendered_command,
            'node_name':        self.node.get_name() if self.node else None,
            'exit_status':      self.exit_status,
        }

    def render_command(self):
        """Render our configured command using the command context."""
        return self.bare_command % self.context

    @property
    def command(self):
        if self.rendered_command:
            return self.rendered_command

        try:
            self.rendered_command = self.render_command()
        except Exception:
            log.error("Failed generating rendering command\n%s" %
                      traceback.format_exc())

            # Return a command string that will always fail
            self.rendered_command = self.FAILED_RENDER
        return self.rendered_command

    @property
    def is_valid_command(self):
        """Returns True if the bare_command was rendered without any errors.
        This has the side effect of actually rendering the bare_command.
        """
        return self.command != self.FAILED_RENDER

    @property
    def is_done(self):
        return self.state in self.END_STATES

    @property
    def is_complete(self):
        return self.is_succeeded or self.is_skipped

    @property
    def is_broken(self):
        return self.is_failed or self.is_cancelled or self.is_unknown

    @property
    def is_active(self):
        return self.is_starting or self.is_running

    def cleanup(self):
        self.machine.clear_observers()
        self.cancel()

    def __getattr__(self, name):
        """Support convenience properties for checking if this ActionRun is in
        a specific state (Ex: self.is_running would check if self.state is
        STATE_RUNNING) or for transitioning to a new state (ex: ready).
        """
        if name in self.machine.transitions:
            return lambda: self.machine.transition(name)

        state_name = name.replace('is_', 'state_').upper()
        try:
            return self.state == self.__getattribute__(state_name)
        except AttributeError:
            raise AttributeError(name)

    def __str__(self):
        return "ActionRun: %s" % self.id


class ActionRunCollection(object):
    """A collection of ActionRuns used by a JobRun."""

    # An ActionRunCollection is blocked when it has runs running which
    # are required for other blocked runs to start.
    STATE_BLOCKED       = state.NamedEventState('blocked')

    def __init__(self, action_graph, run_map):
        self.action_graph       = action_graph
        self.run_map            = run_map
        # Setup proxies
        self.proxy_action_runs_with_cleanup = proxy.CollectionProxy(
            self.get_action_runs_with_cleanup, [
                proxy.attr_proxy('is_running',      any),
                proxy.attr_proxy('is_starting',     any),
                proxy.attr_proxy('is_scheduled',    any),
                proxy.attr_proxy('is_cancelled',    any),
                proxy.attr_proxy('is_active',       any),
                proxy.attr_proxy('is_queued',       all),
                proxy.attr_proxy('is_complete',     all),
                proxy.func_proxy('queue',           iteration.list_all),
                proxy.func_proxy('cancel',          iteration.list_all),
                proxy.func_proxy('success',         iteration.list_all),
                proxy.func_proxy('fail',            iteration.list_all),
                proxy.func_proxy('ready',           iteration.list_all),
                proxy.func_proxy('cleanup',         iteration.list_all),
                proxy.func_proxy('stop',            iteration.list_all),
                proxy.attr_proxy('start_time',      iteration.min_filter),
            ])

    def action_runs_for_actions(self, actions):
        return (self.run_map[a.name] for a in actions)

    def get_action_runs_with_cleanup(self):
        return self.run_map.itervalues()
    action_runs_with_cleanup = property(get_action_runs_with_cleanup)

    def get_action_runs(self):
        return (run for run in self.run_map.itervalues() if not run.is_cleanup)
    action_runs = property(get_action_runs)

    @property
    def cleanup_action_run(self):
        return self.run_map.get(action.CLEANUP_ACTION_NAME)

    @property
    def state_data(self):
        return [run.state_data for run in self.action_runs]

    @property
    def cleanup_action_state_data(self):
        if self.cleanup_action_run:
            return self.cleanup_action_run.state_data

    def _get_runs_using(self, func, include_cleanup=False):
        """Return an iterator of all the ActionRuns which cause func to return
        True. func should be a callable that takes a single ActionRun and
        returns True or False.
        """
        if include_cleanup:
            action_runs = self.action_runs_with_cleanup
        else:
            action_runs = self.action_runs
        return itertools.ifilter(func, action_runs)

    def get_startable_action_runs(self):
        """Returns any actions that are scheduled or queued that can be run."""
        def startable(action_run):
            return (action_run.check_state('start') and
                    not self._is_run_blocked(action_run))
        return self._get_runs_using(startable)

    @property
    def has_startable_action_runs(self):
        return any(self.get_startable_action_runs())

    def _is_run_blocked(self, action_run):
        """Returns True if the ActionRun is waiting on a required run to
        finish before it can run.
        """
        if action_run.is_done or action_run.is_active:
            return False

        required_actions = self.action_graph.get_required_actions(
                action_run.action_name)
        if not required_actions:
            return False

        required_runs = self.action_runs_for_actions(required_actions)

        def is_required_run_blocking(required_run):
            if required_run.is_complete:
                return False
            return True

        return any(is_required_run_blocking(run) for run in required_runs)

    @property
    def is_done(self):
        """Returns True when there are no running ActionRuns and all
        non-blocked ActionRuns are done.
        """
        if self.is_running:
            return False

        def done_or_blocked(action_run):
            return action_run.is_done or self._is_run_blocked(action_run)
        return all(done_or_blocked(run) for run in self.action_runs)

    @property
    def is_failed(self):
        """Return True if there are failed actions and all ActionRuns are
        done or blocked.
        """
        return self.is_done and any(run.is_failed for run in self.action_runs)

    @property
    def is_complete_without_cleanup(self):
        return all(run.is_complete for run in self.action_runs)

    @property
    def names(self):
        return self.run_map.keys()

    @property
    def end_time(self):
        if not self.is_done:
            return None
        end_times = (run.end_time for run in self.get_action_runs_with_cleanup())
        return iteration.max_filter(end_times)

    def __str__(self):
        def blocked_state(action_run):
            return ":blocked" if self._is_run_blocked(action_run) else ""

        run_states = ', '.join(
            "%s(%s%s)" % (a.action_name, a.state, blocked_state(a))
            for a in self.run_map.itervalues())
        return "%s[%s]" % (self.__class__.__name__, run_states)

    def __getattr__(self, name):
        return self.proxy_action_runs_with_cleanup.perform(name)

    def __getitem__(self, name):
        return self.run_map[name]

    def __contains__(self, name):
        return name in self.run_map

    def __iter__(self):
        return self.run_map.itervalues()

    def get(self, name):
        return self.run_map.get(name)

########NEW FILE########
__FILENAME__ = job
import logging
import itertools

from tron import command_context, event, node, eventloop
from tron.core import jobrun
from tron.core import actiongraph
from tron.core.actionrun import ActionRun
from tron.scheduler import scheduler_from_config
from tron.serialize import filehandler
from tron.utils import timeutils, proxy, iteration, collections
from tron.utils.observer import Observable, Observer

class Error(Exception):
    pass


class ConfigBuildMismatchError(Error):
    pass


class InvalidStartStateError(Error):
    pass


log = logging.getLogger(__name__)


class Job(Observable, Observer):
    """A configurable data object.

    Job uses JobRunCollection to manage its runs, and ActionGraph to manage its
    actions and their dependency graph.
    """

    STATUS_DISABLED         = "disabled"
    STATUS_ENABLED          = "enabled"
    STATUS_UNKNOWN          = "unknown"
    STATUS_RUNNING          = "running"

    NOTIFY_STATE_CHANGE     = 'notify_state_change'
    NOTIFY_RUN_DONE         = 'notify_run_done'

    context_class           = command_context.JobContext

    # These attributes determine equality between two Job objects
    equality_attributes = [
        'name',
        'queueing',
        'scheduler',
        'node_pool',
        'all_nodes',
        'action_graph',
        'output_path',
        'action_runner',
        'max_runtime',
        'allow_overlap',
    ]

    # TODO: use config object
    def __init__(self, name, scheduler, queueing=True, all_nodes=False,
            node_pool=None, enabled=True, action_graph=None,
            run_collection=None, parent_context=None, output_path=None,
            allow_overlap=None, action_runner=None, max_runtime=None):
        super(Job, self).__init__()
        self.name               = name
        self.action_graph       = action_graph
        self.scheduler          = scheduler
        self.runs               = run_collection
        self.queueing           = queueing
        self.all_nodes          = all_nodes
        self.enabled            = enabled
        self.node_pool          = node_pool
        self.allow_overlap      = allow_overlap
        self.action_runner      = action_runner
        self.max_runtime        = max_runtime
        self.output_path        = output_path or filehandler.OutputPath()
        self.output_path.append(name)
        self.event              = event.get_recorder(self.name)
        self.context = command_context.build_context(self, parent_context)
        self.event.ok('created')

    @classmethod
    def from_config(cls,
            job_config, scheduler, parent_context, output_path, action_runner):
        """Factory method to create a new Job instance from configuration."""
        action_graph = actiongraph.ActionGraph.from_config(
                job_config.actions, job_config.cleanup_action)
        runs         = jobrun.JobRunCollection.from_config(job_config)
        node_repo    = node.NodePoolRepository.get_instance()

        return cls(
            name                = job_config.name,
            queueing            = job_config.queueing,
            all_nodes           = job_config.all_nodes,
            node_pool           = node_repo.get_by_name(job_config.node),
            scheduler           = scheduler,
            enabled             = job_config.enabled,
            run_collection      = runs,
            action_graph        = action_graph,
            parent_context      = parent_context,
            output_path         = output_path,
            allow_overlap       = job_config.allow_overlap,
            action_runner       = action_runner,
            max_runtime         = job_config.max_runtime)

    def update_from_job(self, job):
        """Update this Jobs configuration from a new config. This method
        actually takes an already constructed job and copies out its
        configuration data.
        """
        for attr in self.equality_attributes:
            setattr(self, attr, getattr(job, attr))
        self.event.ok('reconfigured')

    @property
    def status(self):
        """Current status."""
        if not self.enabled:
            return self.STATUS_DISABLED
        if self.runs.get_run_by_state(ActionRun.STATE_RUNNING):
            return self.STATUS_RUNNING

        if self.runs.get_run_by_state(ActionRun.STATE_SCHEDULED):
            return self.STATUS_ENABLED

        log.warn("%s in an unknown state: %s" % (self, self.runs))
        return self.STATUS_UNKNOWN

    def get_name(self):
        return self.name

    def get_runs(self):
        return self.runs

    @property
    def state_data(self):
        """This data is used to serialize the state of this job."""
        return {
            'runs':             self.runs.state_data,
            'enabled':          self.enabled
        }

    def restore_state(self, state_data):
        """Apply a previous state to this Job."""
        self.enabled = state_data['enabled']
        job_runs = self.runs.restore_state(
                state_data['runs'],
                self.action_graph,
                self.output_path.clone(),
                self.context,
                self.node_pool)
        for run in job_runs:
            self.watch(run)

        self.event.ok('restored')

    def build_new_runs(self, run_time, manual=False):
        """Uses its JobCollection to build new JobRuns. If all_nodes is set,
        build a run for every node, otherwise just builds a single run on a
        single node.
        """
        pool = self.node_pool
        nodes = pool.nodes if self.all_nodes else [pool.next()]
        for node in nodes:
            run = self.runs.build_new_run(self, run_time, node, manual=manual)
            self.watch(run)
            yield run

    def handle_job_run_state_change(self, _job_run, event):
        """Handle state changes from JobRuns and propagate changes to any
        observers.
        """
        # Propagate state change for serialization
        if event == jobrun.JobRun.NOTIFY_STATE_CHANGED:
            self.notify(self.NOTIFY_STATE_CHANGE)
            return

        # Propagate DONE JobRun notifications to JobScheduler
        if event == jobrun.JobRun.NOTIFY_DONE:
            self.notify(self.NOTIFY_RUN_DONE)
            return
    handler = handle_job_run_state_change

    def __eq__(self, other):
        return all(getattr(other, attr, None) == getattr(self, attr, None)
                   for attr in self.equality_attributes)

    def __ne__(self, other):
        return not self == other

    def __str__(self):
        return "Job:%s" % self.name


class JobScheduler(Observer):
    """A JobScheduler is responsible for scheduling Jobs and running JobRuns
    based on a Jobs configuration. Runs jobs by setting a callback to fire
    x seconds into the future.
    """

    def __init__(self, job):
        self.job                = job
        self.shutdown_requested = False
        self.watch(job)

    def restore_state(self, job_state_data):
        """Restore the job state and schedule any JobRuns."""
        self.job.restore_state(job_state_data)
        scheduled = self.job.runs.get_scheduled()
        for job_run in scheduled:
            self._set_callback(job_run)
        # Ensure we have at least 1 scheduled run
        self.schedule()

    def enable(self):
        """Enable the job and start its scheduling cycle."""
        if self.job.enabled:
            return

        self.job.enabled = True
        self.create_and_schedule_runs(ignore_last_run_time=True)

    def create_and_schedule_runs(self, ignore_last_run_time=False):
        for job_run in self.get_runs_to_schedule(ignore_last_run_time):
            self._set_callback(job_run)

    def disable(self):
        """Disable the job and cancel and pending scheduled jobs."""
        self.job.enabled = False
        self.job.runs.cancel_pending()

    @property
    def is_shutdown(self):
        """Return True if there are no running or starting runs."""
        return not any(self.job.runs.get_active())

    def manual_start(self, run_time=None):
        """Trigger a job run manually (instead of from the scheduler)."""
        run_time = run_time or timeutils.current_time()
        manual_runs = list(self.job.build_new_runs(run_time, manual=True))
        for r in manual_runs:
            r.start()
        return manual_runs

    def schedule_reconfigured(self):
        """Remove the pending run and create new runs with the new JobScheduler.
        """
        self.job.runs.remove_pending()
        self.create_and_schedule_runs(ignore_last_run_time=True)

    def schedule(self):
        """Schedule the next run for this job by setting a callback to fire
        at the appropriate time.
        """
        if not self.job.enabled:
            return
        self.create_and_schedule_runs()

    def _set_callback(self, job_run):
        """Set a callback for JobRun to fire at the appropriate time."""
        log.info("Scheduling next Jobrun for %s", self.job.name)
        seconds = job_run.seconds_until_run_time()
        eventloop.call_later(seconds, self.run_job, job_run)

    # TODO: new class for this method
    def run_job(self, job_run, run_queued=False):
        """Triggered by a callback to actually start the JobRun. Also
        schedules the next JobRun.
        """
        if self.shutdown_requested:
            return

        # If the Job has been disabled after this run was scheduled, then cancel
        # the JobRun and do not schedule another
        if not self.job.enabled:
            log.info("%s cancelled because job has been disabled." % job_run)
            return job_run.cancel()

        # If the JobRun was cancelled we won't run it.  A JobRun may be
        # cancelled if the job was disabled, or manually by a user. It's
        # also possible this job was run (or is running) manually by a user.
        # Alternatively, if run_queued is True, this job_run is already queued.
        if not run_queued and not job_run.is_scheduled:
            log.info("%s in state %s already out of scheduled state." % (
                    job_run, job_run.state))
            return self.schedule()

        node = job_run.node if self.job.all_nodes else None
        # If there is another job run still running, queue or cancel this one
        if not self.job.allow_overlap and any(self.job.runs.get_active(node)):
            self._queue_or_cancel_active(job_run)
            return

        job_run.start()
        self.schedule_termination(job_run)
        if not self.job.scheduler.schedule_on_complete:
            self.schedule()

    def schedule_termination(self, job_run):
        if self.job.max_runtime:
            seconds = timeutils.delta_total_seconds(self.job.max_runtime)
            eventloop.call_later(seconds, job_run.stop)

    def _queue_or_cancel_active(self, job_run):
        if self.job.queueing:
            log.info("%s still running, queueing %s." % (self.job, job_run))
            return job_run.queue()

        log.info("%s still running, cancelling %s." % (self.job, job_run))
        job_run.cancel()
        self.schedule()

    def handle_job_events(self, _observable, event):
        """Handle notifications from observables. If a JobRun has completed
        look for queued JobRuns that may need to start now.
        """
        if event != Job.NOTIFY_RUN_DONE:
            return
        self.run_queue_schedule()

    def run_queue_schedule(self):
        # TODO: this should only start runs on the same node if this is an
        # all_nodes job, but that is currently not possible
        queued_run = self.job.runs.get_first_queued()
        if queued_run:
            eventloop.call_later(0, self.run_job, queued_run, run_queued=True)

        # Attempt to schedule a new run.  This will only schedule a run if the
        # previous run was cancelled from a scheduled state, or if the job
        # scheduler is `schedule_on_complete`.
        self.schedule()

    handler = handle_job_events

    def get_runs_to_schedule(self, ignore_last_run_time):
        """Build and return the runs to schedule."""
        if self.job.runs.has_pending:
            log.info("%s has pending runs, can't schedule more." % self.job)
            return []

        if ignore_last_run_time:
            last_run_time = None
        else:
            last_run = self.job.runs.get_newest(include_manual=False)
            last_run_time = last_run.run_time if last_run else None
        next_run_time = self.job.scheduler.next_run_time(last_run_time)
        return self.job.build_new_runs(next_run_time)

    def request_shutdown(self):
        self.shutdown_requested = True

    def __str__(self):
        return "%s(%s)" % (self.__class__.__name__, self.job)

    def get_name(self):
        return self.job.name

    def get_job(self):
        return self.job

    def get_job_runs(self):
        return self.job.runs

    def __eq__(self, other):
        return bool(other and self.get_job() == other.get_job())

    def __ne__(self, other):
        return not self == other


class JobSchedulerFactory(object):
    """Construct JobScheduler instances from configuration."""

    def __init__(self, context, output_stream_dir, time_zone, action_runner):
        self.context            = context
        self.output_stream_dir  = output_stream_dir
        self.time_zone          = time_zone
        self.action_runner      = action_runner

    def build(self, job_config):
        log.debug("Building new job %s", job_config.name)
        output_path = filehandler.OutputPath(self.output_stream_dir)
        scheduler = scheduler_from_config(job_config.schedule, self.time_zone)
        job = Job.from_config(job_config, scheduler, self.context, output_path, self.action_runner)
        return JobScheduler(job)


class JobCollection(object):
    """A collection of jobs."""

    def __init__(self):
        self.jobs = collections.MappingCollection('jobs')
        self.proxy = proxy.CollectionProxy(self.jobs.itervalues, [
            proxy.func_proxy('request_shutdown',    iteration.list_all),
            proxy.func_proxy('enable',              iteration.list_all),
            proxy.func_proxy('disable',             iteration.list_all),
            proxy.func_proxy('schedule',            iteration.list_all),
            proxy.func_proxy('run_queue_schedule',  iteration.list_all),
            proxy.attr_proxy('is_shutdown',         all)
        ])

    def load_from_config(self, job_configs, factory, reconfigure):
        """Apply a configuration to this collection and return a generator of
        jobs which were added.
        """
        self.jobs.filter_by_name(job_configs)

        def map_to_job_and_schedule(job_schedulers):
            for job_scheduler in job_schedulers:
                if reconfigure:
                    job_scheduler.schedule()
                yield job_scheduler.get_job()

        seq = (factory.build(config) for config in job_configs.itervalues())
        return map_to_job_and_schedule(itertools.ifilter(self.add, seq))

    def add(self, job_scheduler):
        return self.jobs.add(job_scheduler, self.update)

    def update(self, new_job_scheduler):
        log.info("Updating %s", new_job_scheduler)
        job_scheduler = self.get_by_name(new_job_scheduler.get_name())
        job_scheduler.get_job().update_from_job(new_job_scheduler.get_job())
        job_scheduler.schedule_reconfigured()
        return True

    def restore_state(self, job_state_data):
        self.jobs.restore_state(job_state_data)

    def get_by_name(self, name):
        return self.jobs.get(name)

    def get_names(self):
        return self.jobs.keys()

    def get_jobs(self):
        return [sched.get_job() for sched in self]

    def get_job_run_collections(self):
        return [sched.get_job_runs() for sched in self]

    def __iter__(self):
        return self.jobs.itervalues()

    def __getattr__(self, name):
        return self.proxy.perform(name)

    def __contains__(self, name):
        return name in self.jobs

########NEW FILE########
__FILENAME__ = jobrun
"""
 Classes to manage job runs.
"""

from collections import deque
import logging
import itertools
from tron import node, command_context, event
from tron.core.actionrun import ActionRun, ActionRunFactory
from tron.serialize import filehandler
from tron.utils import timeutils, proxy
from tron.utils.observer import Observable, Observer

log = logging.getLogger(__name__)

class Error(Exception):
    pass


class JobRun(Observable, Observer):
    """A JobRun is an execution of a Job.  It has a list of ActionRuns and is
    responsible for starting ActionRuns in the correct order and managing their
    dependencies.
    """

    NOTIFY_DONE           = 'notify_done'
    NOTIFY_STATE_CHANGED  = 'notify_state_changed'

    context_class         = command_context.JobRunContext

    # TODO: use config object
    def __init__(self, job_name, run_num, run_time, node, output_path=None,
                base_context=None, action_runs=None, action_graph=None,
                manual=None):
        super(JobRun, self).__init__()
        self.job_name           = job_name
        self.run_num            = run_num
        self.run_time           = run_time
        self.node               = node
        self.output_path        = output_path or filehandler.OutputPath()
        self.output_path.append(self.id)
        self.action_runs_proxy  = None
        self._action_runs       = None
        self.action_graph       = action_graph
        self.manual             = manual
        self.event              = event.get_recorder(self.id)
        self.event.ok('created')

        if action_runs:
            self.action_runs    = action_runs

        self.context = command_context.build_context(self, base_context)

    @property
    def id(self):
        return '%s.%s' % (self.job_name, self.run_num)

    @classmethod
    def for_job(cls, job, run_num, run_time, node, manual):
        """Create a JobRun for a job."""
        run = cls(job.get_name(), run_num, run_time, node,
                job.output_path.clone(),
                job.context,
                action_graph=job.action_graph,
                manual=manual)

        action_runs     = ActionRunFactory.build_action_run_collection(run, job.action_runner)
        run.action_runs = action_runs
        return run

    @classmethod
    def from_state(cls, state_data, action_graph, output_path, context,
                run_node):
        """Restore a JobRun from a serialized state."""
        pool_repo = node.NodePoolRepository.get_instance()
        run_node  = pool_repo.get_node(state_data.get('node_name'), run_node)
        job_name  = state_data['job_name']

        job_run =  cls(
            job_name,
            state_data['run_num'],
            state_data['run_time'],
            run_node,
            action_graph=action_graph,
            manual=state_data.get('manual', False),
            output_path=output_path,
            base_context=context
        )
        action_runs = ActionRunFactory.action_run_collection_from_state(
                job_run, state_data['runs'], state_data['cleanup_run'])
        job_run.action_runs = action_runs
        return job_run

    @property
    def state_data(self):
        """This data is used to serialize the state of this job run."""
        return {
            'job_name':         self.job_name,
            'run_num':          self.run_num,
            'run_time':         self.run_time,
            'node_name':        self.node.get_name() if self.node else None,
            'runs':             self.action_runs.state_data,
            'cleanup_run':      self.action_runs.cleanup_action_state_data,
            'manual':           self.manual,
        }

    def _get_action_runs(self):
        return self._action_runs

    def _set_action_runs(self, run_collection):
        """Store action runs and register callbacks."""
        if self._action_runs is not None:
            raise ValueError("ActionRunCollection already set on %s" % self)

        self._action_runs = run_collection
        for action_run in run_collection.action_runs_with_cleanup:
            self.watch(action_run)

        self.action_runs_proxy = proxy.AttributeProxy(
            self.action_runs,
            [
                'queue',
                'cancel',
                'success',
                'fail',
                'is_cancelled',
                'is_unknown',
                'is_failed',
                'is_succeeded',
                'is_running',
                'is_starting',
                'is_queued',
                'is_scheduled',
                'is_skipped',
                'is_starting',
                'start_time',
                'end_time'
            ])

    def _del_action_runs(self):
        del self._action_runs

    action_runs = property(_get_action_runs, _set_action_runs, _del_action_runs)

    def seconds_until_run_time(self):
        run_time = self.run_time
        now = timeutils.current_time()
        if run_time.tzinfo:
            now = run_time.tzinfo.localize(now)
        return max(0, timeutils.delta_total_seconds(run_time - now))

    def start(self):
        """Start this JobRun as a scheduled run (not a manual run)."""
        self.event.info('start')
        if self.action_runs.has_startable_action_runs and self._do_start():
            return True

    def _do_start(self):
        log.info("Starting JobRun %s", self.id)

        self.action_runs.ready()
        if any(self._start_action_runs()):
            self.event.ok('started')
            return True

    def stop(self):
        if self.action_runs.is_done:
            return
        self.action_runs.stop()

    def _start_action_runs(self):
        """Start all startable action runs, and return any that were
        successfully started.
        """
        started_actions = []
        for action_run in self.action_runs.get_startable_action_runs():
            if action_run.start():
                started_actions.append(action_run)

        return started_actions

    def handle_action_run_state_change(self, action_run, _):
        """Handle events triggered by JobRuns."""
        # propagate all state changes (from action runs) up to state serializer
        self.notify(self.NOTIFY_STATE_CHANGED)

        if not action_run.is_done:
            return

        if action_run.is_skipped and self.action_runs.is_scheduled:
            return

        if not action_run.is_broken and any(self._start_action_runs()):
            log.info("Action runs started for %s." % self)
            return

        if self.action_runs.is_active or self.action_runs.is_scheduled:
            log.info("%s still has running or scheduled actions." % self)
            return

        # If we can't make any progress, we're done
        cleanup_run = self.action_runs.cleanup_action_run
        if not cleanup_run or cleanup_run.is_done:
            return self.finalize()

        # TODO: remove in (0.6), start() no longer raises an exception
        # When a job is being disabled, or the daemon is being shut down a bunch
        # of ActionRuns will be cancelled/failed. This would cause cleanup
        # action to be triggered more then once. Guard against that.
        if cleanup_run.check_state('start'):
            cleanup_run.start()
    handler = handle_action_run_state_change

    def finalize(self):
        """The last step of a JobRun. Called when the cleanup action
        completes or if the job has no cleanup action, called once all action
        runs have reached a 'done' state.

        Triggers an event to notifies the Job that is is done.
        """
        if self.action_runs.is_failed:
            self.event.critical('failed')
        else:
            self.event.ok('succeeded')

        # Notify Job that this JobRun is complete
        self.notify(self.NOTIFY_DONE)

    def cleanup(self):
        """Cleanup any resources used by this JobRun."""
        self.event.notice('removed')
        event.EventManager.get_instance().remove(str(self))
        self.clear_observers()
        self.action_runs.cleanup()
        self.node = None
        self.action_graph = None
        self._action_runs = None
        self.output_path.delete()

    def get_action_run(self, action_name):
        return self.action_runs.get(action_name)

    @property
    def state(self):
        """The overall state of this job run. Based on the state of its actions.
        """
        if not self.action_runs:
            log.info("%s has no state" % self)
            return ActionRun.STATE_UNKNOWN

        if self.action_runs.is_complete:
            return ActionRun.STATE_SUCCEEDED
        if self.action_runs.is_cancelled:
            return ActionRun.STATE_CANCELLED
        if self.action_runs.is_running:
            return ActionRun.STATE_RUNNING
        if self.action_runs.is_starting:
            return ActionRun.STATE_STARTING
        if self.action_runs.is_failed:
            return ActionRun.STATE_FAILED
        if self.action_runs.is_scheduled:
            return ActionRun.STATE_SCHEDULED
        if self.action_runs.is_queued:
            return ActionRun.STATE_QUEUED

        log.info("%s in an unknown state: %s" % (self, self.action_runs))
        return ActionRun.STATE_UNKNOWN

    def __getattr__(self, name):
        if self.action_runs_proxy:
            return self.action_runs_proxy.perform(name)
        raise AttributeError(name)

    def __str__(self):
        return "JobRun:%s" % self.id


class JobRunCollection(object):
    """A JobRunCollection is a deque of JobRun objects. Responsible for
    ordering and logic related to a group of JobRuns which should all be runs
    for the same Job.

    A JobRunCollection is created in two stages. First it's populated from a
    configuration object, and second its state is loaded from a serialized
    state dict.

    Runs in a JobRunCollection should always remain sorted by their run_num.
    """

    def __init__(self, run_limit):
        self.run_limit = run_limit
        self.runs = deque()

    @classmethod
    def from_config(cls, job_config):
        """Factory method for creating a JobRunCollection from a config."""
        return cls(job_config.run_limit)

    def restore_state(self, state_data, action_graph, output_path, context,
            node_pool):
        """Apply state to all jobs from the state dict."""
        if self.runs:
            msg = "State can not be restored to a collection with runs."
            raise ValueError(msg)

        restored_runs = [
            JobRun.from_state(run_state, action_graph, output_path.clone(),
                context, node_pool.next())
            for run_state in state_data
        ]
        self.runs.extend(restored_runs)
        return restored_runs

    def build_new_run(self, job, run_time, node, manual=False):
        """Create a new run for the job, add it to the runs list,
        and return it.
        """
        run_num = self.next_run_num()
        log.info("Building JobRun %s for %s on %s at %s" %
             (run_num, job, node, run_time))

        run = JobRun.for_job(job, run_num, run_time, node, manual)
        self.runs.appendleft(run)
        self.remove_old_runs()
        return run

    def cancel_pending(self):
        """Find any queued or scheduled runs and cancel them."""
        for pending in self.get_pending():
            pending.cancel()

    def remove_pending(self):
        """Remove pending runs from the run list."""
        for pending in list(self.get_pending()):
            pending.cleanup()
            self.runs.remove(pending)

    def _get_runs_using(self, func, reverse=False):
        """Filter runs using func()."""
        job_runs = self.runs if not reverse else reversed(self.runs)
        return itertools.ifilter(func, job_runs)

    def _get_run_using(self, func, reverse=False):
        """Find the first run (from most recent to least recent), where func()
        returns true.  func() should be a callable which takes a single
        argument (a JobRun), and return True or False.
        """
        try:
            return self._get_runs_using(func, reverse).next()
        except StopIteration:
            return None

    def _filter_by_state(self, state):
        return lambda r: r.state == state

    def get_run_by_state(self, state):
        """Returns the most recent run which matches the state."""
        return self._get_run_using(self._filter_by_state(state))

    def get_run_by_num(self, num):
        """Return a the run with run number which matches num."""
        return self._get_run_using(lambda r: r.run_num == num)

    def get_run_by_index(self, index):
        """Return the job run at index. Jobs are indexed from oldest to newest.
        """
        try:
            return self.runs[index * -1 - 1]
        except IndexError:
            return None

    def get_run_by_state_short_name(self, short_name):
        """Returns the most recent run which matches the state short name."""
        return self._get_run_using(lambda r: r.state.short_name == short_name)

    def get_newest(self, include_manual=True):
        """Returns the most recently created JobRun."""
        func = lambda r: True if include_manual else not r.manual
        return self._get_run_using(func)

    def get_pending(self):
        """Return the job runs that are queued or scheduled."""
        return self._get_runs_using(lambda r: r.is_scheduled or r.is_queued)

    @property
    def has_pending(self):
        return any(self.get_pending())

    def get_active(self, node=None):
        if node:
            func = lambda r: (r.is_running or r.is_starting) and r.node == node
        else:
            func = lambda r: r.is_running or r.is_starting
        return self._get_runs_using(func)

    def get_first_queued(self, node=None):
        state = ActionRun.STATE_QUEUED
        if node:
            queued_func = lambda r: r.state == state and r.node == node
        else:
            queued_func = self._filter_by_state(state)
        return self._get_run_using(queued_func, reverse=True)

    def get_scheduled(self):
        state = ActionRun.STATE_SCHEDULED
        return self._get_runs_using(self._filter_by_state(state))

    def get_next_to_finish(self, node=None):
        """Return the most recent run which is either running or scheduled. If
        node is not None, then only looks for runs on that node.
        """
        def compare(run):
            if node and run.node != node:
                return False
            if run.is_running or run.is_scheduled:
                return run
        return self._get_run_using(compare)

    def next_run_num(self):
        """Return the next run number to use."""
        if not self.runs:
            return 0
        return max(r.run_num for r in self.runs) + 1

    def remove_old_runs(self):
        """Remove old runs to reduce the number of completed runs
        to within RUN_LIMIT.
        """
        while len(self.runs) > self.run_limit:
            run = self.runs.pop()
            run.cleanup()

    def get_action_runs(self, action_name):
        return [job_run.get_action_run(action_name) for job_run in self]

    @property
    def state_data(self):
        """Return the state data to serialize."""
        return [r.state_data for r in self.runs]

    @property
    def last_success(self):
        return self.get_run_by_state(ActionRun.STATE_SUCCEEDED)

    @property
    def next_run(self):
        return self.get_run_by_state(ActionRun.STATE_SCHEDULED)

    def __iter__(self):
        return iter(self.runs)

    def __str__(self):
        return "%s[%s]" % (
                type(self).__name__,
                ', '.join("%s(%s)" % (r.run_num, r.state) for r in self.runs)
        )

########NEW FILE########
__FILENAME__ = service
import logging
import itertools

from tron import event, node, eventloop
from tron.core import serviceinstance
from tron.core.serviceinstance import ServiceInstance
from tron.utils import observer, collections


log = logging.getLogger(__name__)


class ServiceState(object):
    """Determine the state of a Service."""
    DISABLED      = "disabled"
    STARTING      = "starting"
    UP            = "up"
    DEGRADED      = "degraded"
    FAILED        = "failed"
    STOPPING      = "stopping"
    UNKNOWN       = "unknown"

    FAILURE_STATES = set([DEGRADED, FAILED])

    @classmethod
    def from_service(cls, service):
        if not service.enabled:
            return cls.disabled_states(service)

        if service.instances.is_up():
            return cls.UP

        if service.instances.is_starting():
            return cls.STARTING

        if service.instances.all(ServiceInstance.STATE_FAILED):
            return cls.FAILED

        return cls.DEGRADED

    @classmethod
    def disabled_states(cls, service):
        if not len(service.instances):
            return cls.DISABLED

        if service.instances.all(ServiceInstance.STATE_STOPPING):
            return cls.STOPPING

        return cls.UNKNOWN


class Service(observer.Observer, observer.Observable):
    """Manage a collection of service instances."""

    NOTIFY_STATE_CHANGE = 'event_state_changed'

    def __init__(self, config, instance_collection):
        super(Service, self).__init__()
        self.config             = config
        self.instances          = instance_collection
        self.enabled            = False
        args                    = config.restart_delay, self.repair
        self.repair_callback    = eventloop.UniqueCallback(*args)
        self.event_recorder     = event.get_recorder(str(self))

    @classmethod
    def from_config(cls, config, base_context):
        node_repo           = node.NodePoolRepository.get_instance()
        node_pool           = node_repo.get_by_name(config.node)
        args                = config, node_pool, base_context
        instance_collection = serviceinstance.ServiceInstanceCollection(*args)
        return cls(config, instance_collection)

    def get_name(self):
        return self.config.name

    name = property(get_name)

    def get_state(self):
        return ServiceState.from_service(self)

    def enable(self):
        """Enable the service."""
        self.enabled = True
        self.event_recorder.ok('enabled')
        self.repair()

    def disable(self, force=False):
        self.enabled = False
        (self.instances.kill if force else self.instances.stop)()
        self.repair_callback.cancel()
        self.event_recorder.ok('disabled')

    def repair(self):
        """Repair the service by restarting instances."""
        self.instances.clear_failed()
        self.instances.restore()
        self.watch_instances(self.instances.create_missing())
        self.notify(self.NOTIFY_STATE_CHANGE)
        self.event_recorder.ok('repairing')
        self.instances.start()

    def _handle_instance_state_change(self, _instance, event):
        """Handle any changes to the state of this service's instances."""
        if event == serviceinstance.ServiceInstance.STATE_DOWN:
            self.instances.clear_down()
            self.notify(self.NOTIFY_STATE_CHANGE)

        if event in (serviceinstance.ServiceInstance.STATE_FAILED,
                     serviceinstance.ServiceInstance.STATE_UP):
            self.record_events()

        if self.get_state() in ServiceState.FAILURE_STATES:
            log.info("Starting service repair for %s", self)
            self.repair_callback.start()

    handler = _handle_instance_state_change

    def record_events(self):
        """Record an event when the state changes."""
        state = self.get_state()
        if state in ServiceState.FAILURE_STATES:
            return self.event_recorder.critical(state)

        if state == ServiceState.UP:
            return self.event_recorder.ok(state)

    @property
    def state_data(self):
        """Data used to serialize the state of this service."""
        return dict(enabled=self.enabled, instances=self.instances.state_data)

    def __eq__(self, other):
        if other is None or not isinstance(other, Service):
            return False

        return self.config == other.config and self.instances == other.instances

    def __ne__(self, other):
        return not self == other

    def __str__(self):
        return "Service:%s" % self.name

    def watch_instances(self, instances):
        self.watch_all(instance.get_observable() for instance in instances)

    def restore_state(self, state_data):
        instances = self.instances.restore_state(state_data['instances'])
        self.watch_instances(instances)

        (self.enable if state_data.get('enabled') else self.disable)()
        self.event_recorder.info("restored")


class ServiceCollection(object):
    """A collection of services."""

    def __init__(self):
        self.services = collections.MappingCollection('services')

    def load_from_config(self, service_configs, context):
        """Apply a configuration to this collection and return a generator of
        services which were added.
        """
        self.services.filter_by_name(service_configs.keys())

        def build(config):
            log.debug("Building new service %s", config.name)
            return Service.from_config(config, context)

        seq = (build(config) for config in service_configs.itervalues())
        return itertools.ifilter(self.add, seq)

    def add(self, service):
        return self.services.replace(service)

    def restore_state(self, service_state_data):
        self.services.restore_state(service_state_data)

    def get_by_name(self, name):
        return self.services.get(name)

    def get_names(self):
        return self.services.keys()

    def __iter__(self):
        return self.services.itervalues()

########NEW FILE########
__FILENAME__ = serviceinstance
import logging

import operator
import signal

from tron import command_context, actioncommand
from tron import eventloop
from tron import node
from tron.actioncommand import ActionCommand
from tron.utils import observer, proxy, iteration
from tron.utils import state


log = logging.getLogger(__name__)


MIN_HANG_CHECK_SECONDS  = 10
HANG_CHECK_DELAY_RATIO = 0.9


def create_hang_check(delay, func):
    delay = max(delay * HANG_CHECK_DELAY_RATIO, MIN_HANG_CHECK_SECONDS)
    return eventloop.UniqueCallback(delay, func)


def build_action(task, command=None):
    """Create an action for a task which is an Observer, and which has
    properties 'task_name', 'command', and 'buffer_store'.
    """
    name = '%s.%s' % (task.id, task.task_name)
    command = command or task.command
    action = ActionCommand(name, command, serializer=task.buffer_store)
    task.watch(action)
    return action


def run_action(task, action):
    """Run an action for a task which is an Observable and has has properties
    'node' and 'NOTIFY_FAILED'. Returns True on success, and calls
    task.notify(NOTIF_FAILURE) and returns False on failure.
    """
    log.debug("Executing %s on %s for %s" % (action, task.node, task))
    try:
        task.node.submit_command(action)
        return True
    except node.Error, e:
        log.error("Failed to run %s on %s: %s", action, task.node, e)
        stream = task.buffer_store.open(actioncommand.ActionCommand.STDERR)
        stream.write("Node run failure for %s: %s" % (task.task_name, e))
        task.notify(task.NOTIFY_FAILED)


def get_failures_from_task(task):
    return task.buffer_store.get_stream(actioncommand.ActionCommand.STDERR)


class ServiceInstanceMonitorTask(observer.Observable, observer.Observer):
    """ServiceInstance task which monitors the service process and
    notifies observers if the process is up or down. Also monitors itself
    to ensure this check does not hang.

    This task will be a no-op if interval is Falsy.
    """
    NOTIFY_START            = 'monitor_task_notify_start'
    NOTIFY_FAILED           = 'monitor_task_notify_failed'
    NOTIFY_UP               = 'monitor_task_notify_up'
    NOTIFY_DOWN             = 'monitor_task_notify_down'

    command_template        = "cat %s | xargs kill -0"
    task_name               = 'monitor'

    def __init__(self, id, node, interval, pid_filename):
        super(ServiceInstanceMonitorTask, self).__init__()
        self.interval            = interval or 0
        self.node                = node
        self.id                  = id
        self.pid_filename        = pid_filename
        self.action              = actioncommand.CompletedActionCommand
        self.callback            = eventloop.UniqueCallback(
                                    self.interval, self.run)
        self.hang_check_callback = create_hang_check(self.interval, self.fail)
        self.buffer_store        = actioncommand.StringBufferStore()

    def queue(self):
        """Queue this task to run after monitor_interval."""
        log.info("Queueing %s" % self)
        self.callback.start()

    def run(self):
        """Run the monitoring command."""
        if not self.action.is_done:
            log.warn("%s: Monitor action already exists.", self)
            return

        self.notify(self.NOTIFY_START)
        self.action = build_action(self)
        if run_action(self, self.action):
            self.hang_check_callback.start()

    @property
    def command(self):
        return self.command_template % self.pid_filename

    def handle_action_event(self, action, event):
        if action != self.action:
            msg = "Ignoring %s %s, action was cleared due to hang check."
            log.warn(msg % (action, event))
            return

        if event == ActionCommand.EXITING:
            self.hang_check_callback.cancel()
            self._handle_action_exit()
        if event == ActionCommand.FAILSTART:
            self.hang_check_callback.cancel()
            self.notify(self.NOTIFY_FAILED)
            self.queue()

    handler = handle_action_event

    def _handle_action_exit(self):
        log.debug("%s exit, failure: %r", self, self.action.is_failed)
        if self.action.is_failed:
            self.notify(self.NOTIFY_FAILED)
            return

        self.notify(self.NOTIFY_UP)
        self.queue()
        self.buffer_store.clear()

    def cancel(self):
        """Cancel the monitor callback and hang check."""
        self.callback.cancel()
        self.hang_check_callback.cancel()

    def fail(self):
        log.warning("%s is still running %s.", self, self.action)
        self.node.stop(self.action)
        self.action.write_stderr("Monitoring failed")
        self.notify(self.NOTIFY_FAILED)
        self.action = actioncommand.CompletedActionCommand

    def __str__(self):
        return "%s(%s)" % (self.__class__.__name__, self.id)


class ServiceInstanceStopTask(observer.Observable, observer.Observer):
    """ServiceInstance task which kills the service process."""

    NOTIFY_SUCCESS              = 'stop_task_notify_success'
    NOTIFY_FAILED               = 'stop_task_notify_fail'

    command_template            = "cat %s | xargs kill -%s"
    task_name                   = 'stop'

    def __init__(self, id, node, pid_filename):
        super(ServiceInstanceStopTask, self).__init__()
        self.id             = id
        self.node           = node
        self.pid_filename   = pid_filename
        self.buffer_store   = actioncommand.StringBufferStore()
        self.command        = None

    def get_command(self, signal):
        return self.command_template % (self.pid_filename, signal)

    def stop(self):
        self.command = self.get_command(signal.SIGTERM)
        return run_action(self, build_action(self))

    def kill(self):
        self.command = self.get_command(signal.SIGKILL)
        return run_action(self, build_action(self))

    def handle_action_event(self, action, event):
        if event == ActionCommand.COMPLETE:
            return self._handle_complete(action)

        if event == ActionCommand.FAILSTART:
            log.warn("Failed to start kill command for %s", self.id)
            self.notify(self.NOTIFY_FAILED)

    handler = handle_action_event

    def _handle_complete(self, action):
        if action.is_failed:
            log.error("Failed to stop %s", self)

        self.notify(self.NOTIFY_SUCCESS)

    def __str__(self):
        return "%s(%s)" % (self.__class__.__name__, self.id)


class ServiceInstanceStartTask(observer.Observable, observer.Observer):
    """ServiceInstance task which starts the service process."""

    NOTIFY_FAILED           = 'start_task_notify_failed'
    NOTIFY_STARTED          = 'start_task_notify_started'

    task_name               = 'start'

    def __init__(self, id, node):
        super(ServiceInstanceStartTask, self).__init__()
        self.id             = id
        self.node           = node
        self.buffer_store   = actioncommand.StringBufferStore()

    def start(self, command):
        """Start the service command. command is rendered by the caller."""
        return run_action(self,  build_action(self, command=command))

    def handle_action_event(self, action, event):
        """Watch for events from the ActionCommand."""
        if event == ActionCommand.EXITING:
            return self._handle_action_exit(action)
        if event == ActionCommand.FAILSTART:
            log.warn("Failed to start service %s on %s.", self.id, self.node)
            self.notify(self.NOTIFY_FAILED)
    handler = handle_action_event

    def _handle_action_exit(self, action):
        event = self.NOTIFY_FAILED if action.is_failed else self.NOTIFY_STARTED
        self.notify(event)

    def __str__(self):
        return "%s(%s)" % (self.__class__.__name__, self.id)


class ServiceInstanceState(state.NamedEventState):
    """Event state subclass for service instances"""


class ServiceInstance(observer.Observer):
    """An instance of a service."""

    STATE_DOWN          = ServiceInstanceState("down")
    STATE_UP            = ServiceInstanceState("up")
    STATE_FAILED        = ServiceInstanceState("failed",
                            stop=STATE_DOWN,
                            up=STATE_UP)
    STATE_STOPPING      = ServiceInstanceState("stopping",
                            down=STATE_DOWN,
                            stop_fail=STATE_FAILED)
    STATE_MONITORING    = ServiceInstanceState("monitoring",
                            down=STATE_FAILED,
                            stop=STATE_STOPPING,
                            up=STATE_UP)
    STATE_STARTING      = ServiceInstanceState("starting",
                            down=STATE_FAILED,
                            monitor=STATE_MONITORING,
                            stop=STATE_STOPPING)
    STATE_UNKNOWN       = ServiceInstanceState("unknown",
                            monitor=STATE_MONITORING,
                            stop=STATE_DOWN)

    STATE_MONITORING['monitor_fail']    = STATE_UNKNOWN
    STATE_UP['stop']                    = STATE_STOPPING
    STATE_UP['monitor']                 = STATE_MONITORING
    STATE_DOWN['start']                 = STATE_STARTING
    STATE_DOWN['monitor']               = STATE_MONITORING

    context_class               = command_context.ServiceInstanceContext

    def __init__(self, config, node, instance_number, parent_context):
        self.config             = config
        self.node               = node
        self.instance_number    = instance_number
        self.id                 = "%s.%s" % (config.name, self.instance_number)

        start_state             = ServiceInstance.STATE_DOWN
        self.machine            = state.StateMachine(start_state, delegate=self)
        self.parent_context     = parent_context
        self.context = command_context.build_context(self, parent_context)
        self.failures           = []

    def create_tasks(self):
        """Create and watch tasks."""
        interval                = self.config.monitor_interval
        pid_file                = self.pid_filename
        self.monitor_task       = ServiceInstanceMonitorTask(
                                    self.id, self.node, interval, pid_file)
        self.start_task         = ServiceInstanceStartTask(self.id, self.node)
        self.stop_task          = ServiceInstanceStopTask(
                                    self.id, self.node, pid_file)
        self.watch(self.monitor_task)
        self.watch(self.start_task)
        self.watch(self.stop_task)

    @classmethod
    def create(cls, config, node, instance_number, context):
        instance = cls(config, node, instance_number, context)
        instance.create_tasks()
        return instance

    @property
    def pid_filename(self):
        return self.config.pid_file % self.context

    @property
    def command(self):
        return self.config.command % self.context

    def start(self):
        if not self.machine.transition('start'):
            return False
        return self.start_task.start(self.command)

    def stop(self):
        return self.perform_stop_task(self.stop_task.stop)

    def kill(self):
        return self.perform_stop_task(self.stop_task.kill)

    def perform_stop_task(self, method):
        if self.machine.check('stop'):
            method()
            self.monitor_task.cancel()
            return self.machine.transition('stop')

    def restore(self):
        self.monitor_task.run()

    event_to_transition_map = {
        ServiceInstanceMonitorTask.NOTIFY_START:        'monitor',
        ServiceInstanceMonitorTask.NOTIFY_FAILED:       'monitor_fail',
        ServiceInstanceMonitorTask.NOTIFY_DOWN:         'down',
        ServiceInstanceMonitorTask.NOTIFY_UP:           'up',
        ServiceInstanceStopTask.NOTIFY_FAILED:          'stop_fail',
        ServiceInstanceStopTask.NOTIFY_SUCCESS:         'down',
        ServiceInstanceStartTask.NOTIFY_FAILED:         'down',
    }

    def handler(self, task, event):
        """Handle events from ServiceInstance tasks."""
        log.debug("Service instance event %s on task %s" % (event, task))
        if event in self.event_to_transition_map:
            self.machine.transition(self.event_to_transition_map[event])

        if event == ServiceInstanceStartTask.NOTIFY_STARTED:
            self._handle_start_task_complete()

        if event == task.NOTIFY_FAILED:
            self.failures.append(get_failures_from_task(task))

        if event == ServiceInstanceMonitorTask.NOTIFY_FAILED and self.config.monitor_retries and self.config.monitor_retries < len(self.failures):
            log.info("Too many monitor failures(%d) of %s" % (len(self.failures), task))
            self.monitor_task.cancel()
            self.machine.transition('stop')

        if event == ServiceInstanceMonitorTask.NOTIFY_UP:
            self.failures = []

    def _handle_start_task_complete(self):
        if self.machine.state != ServiceInstance.STATE_STARTING:
            self.stop_task.stop()
            return

        log.info("Start for %s complete, starting monitor" % self.id)
        self.monitor_task.queue()

    @property
    def state_data(self):
        return dict(instance_number=self.instance_number,
                    node=self.node.hostname)

    def get_observable(self):
        return self.machine

    def get_state(self):
        return self.machine.state

    def __str__(self):
        return "%s:%s" % (self.__class__.__name__, self.id)


# TODO: shouldn't this check which nodes are not used to properly
# balance across nodes? But doing this makes it less resilient to
# failures of a node
def node_selector(node_pool, hostname=None):
    """Attempt to retrieve the node by hostname.  If that node is not
    available, or hostname is None, then pick one the next node.
    """
    next_node = node_pool.next_round_robin
    if not hostname:
        return next_node()

    return node_pool.get_by_hostname(hostname) or next_node()


class ServiceInstanceCollection(object):
    """A collection of ServiceInstances."""

    def __init__(self, config, node_pool, context):
        self.config             = config
        self.node_pool          = node_pool
        self.instances          = []
        self.context            = context

        self.instances_proxy    = proxy.CollectionProxy(
            lambda: self.instances, [
                proxy.func_proxy('stop',    iteration.list_all),
                proxy.func_proxy('kill',    iteration.list_all),
                proxy.func_proxy('start',   iteration.list_all),
                proxy.func_proxy('restore', iteration.list_all),
                proxy.attr_proxy('state_data', list)
            ])

    def clear_failed(self):
        self._clear(ServiceInstance.STATE_FAILED)

    def clear_down(self):
        self._clear(ServiceInstance.STATE_DOWN)

    def _clear(self, state):
        log.info("clear instances in state %s from %s", state, self)
        self.instances = [i for i in self.instances if i.get_state() != state]

    def sort(self):
        self.instances.sort(key=operator.attrgetter('instance_number'))

    def create_missing(self):
        """Create instances until this collection contains the configured
        number of instances.
        """
        def builder(_):
            node = node_selector(self.node_pool)
            return self._build_instance(node, self.next_instance_number())
        log.info("Creating %s instances for %s" % (self.missing, self))
        return self._build_and_sort(builder, xrange(self.missing))

    def _build_instance(self, node, number):
        return ServiceInstance.create(self.config, node, number, self.context)

    def restore_state(self, state_data):
        assert not self.instances
        def builder(instance_state):
            node = node_selector(self.node_pool, instance_state['node'])
            return self._build_instance(node, instance_state['instance_number'])

        return self._build_and_sort(builder, state_data)

    def _build_and_sort(self, builder, seq):
        def build_and_add(item):
            instance = builder(item)
            log.info("Building and adding %s to %s" % (instance, self))
            self.instances.append(instance)
            return instance
        instances = list(build_and_add(item) for item in seq)
        self.sort()
        return instances

    def next_instance_number(self):
        """Return the next available instance number."""
        instance_nums = set(inst.instance_number for inst in self.instances)
        for num in xrange(self.config.count):
            if num not in instance_nums:
                return num

    def get_by_number(self, instance_number):
        for instance in self.instances:
            if instance.instance_number == instance_number:
                return instance

    @property
    def missing(self):
        return self.config.count - len(self.instances)

    def all(self, state):
        if len(self.instances) != self.config.count:
            return False
        return self._all_states_match([state])

    def is_starting(self):
        states = set([ServiceInstance.STATE_STARTING,
                      ServiceInstance.STATE_MONITORING,
                      ServiceInstance.STATE_UP])
        return self._all_states_match(states)

    def is_up(self):
        states = set([ServiceInstance.STATE_MONITORING,
                     ServiceInstance.STATE_UP])
        return self._all_states_match(states)

    def _all_states_match(self, states):
        return all(inst.get_state() in states for inst in self.instances)

    def __len__(self):
        return len(self.instances)

    def __getattr__(self, item):
        return self.instances_proxy.perform(item)

    def __eq__(self, other):
        return (self.node_pool == other.node_pool and
                self.config == other.config)

    def __ne__(self, other):
        return not self == other

    def __iter__(self):
        return iter(self.instances)

    def __str__(self):
        return "ServiceInstanceCollection:%s" % self.config.name

########NEW FILE########
__FILENAME__ = crash_reporter
import logging
from twisted.python import log
from tron import event


logger = logging.getLogger(__name__)


class CrashReporter(object):
    """Observer for twisted events that can send emails on crashes

    Based on twisted.log.PythonLoggingObserver
    """

    def __init__(self, emailer):
        self.emailer        = emailer
        self.event_recorder = event.get_recorder(str(self))

    def _get_level(self, event_dict):
        """Returns the logging level for an event."""
        if 'logLevel' in event_dict:
            return event_dict['logLevel']
        if event_dict['isError']:
            return logging.ERROR
        return logging.INFO

    def emit(self, event_dict):
        text = log.textFromEventDict(event_dict)
        if text is None:
            return

        if text == "Unhandled error in Deferred:":
            # This annoying error message is just a pre-cursor to an actual
            # error message, so filter it out.
            return None

        if self._get_level(event_dict) < logging.ERROR:
            return

        try:
            self.event_recorder.critical("crash", msg=text)
            self.emailer.send(text)

        except Exception:
            logger.exception("Error sending notification")
            self.event_recorder.critical("email_failure", msg=text)

    def start(self):
        log.addObserver(self.emit)

    def stop(self):
        log.removeObserver(self.emit)

    def __str__(self):
        return 'CrashReporter'

########NEW FILE########
__FILENAME__ = event
from collections import deque
import logging
import operator
import itertools

from tron.utils import timeutils

log = logging.getLogger(__name__)


# Special character used to split an entity name into hierarchy levels
NAME_CHARACTER = '.'


class EventLevel(object):
    """An event level that supports ordering."""
    __slots__ = ('order', 'label')

    def __init__(self, order, label):
        self.order          = order
        self.label          = label

    def __eq__(self, other):
        return self.order == other.order

    def __cmp__(self, other):
        return cmp(self.order, other.order)

    def __hash__(self):
        return hash(self.order)


LEVEL_INFO      = EventLevel(0, "INFO")         # Troubleshooting information
LEVEL_OK        = EventLevel(1, "OK")           # Expected behaviour
LEVEL_NOTICE    = EventLevel(2, "NOTICE")       # Troubling behaviour
LEVEL_CRITICAL  = EventLevel(3, "CRITICAL")     # Major Failure


class EventStore(object):
    """An index of event level to a circular buffer of events. Supports
    retrieving events which with a minimal level.
    """
    DEFAULT_LIMIT   = 10
    NO_LEVEL        = EventLevel(None, None)

    def __init__(self, limits=None):
        self.limits = limits or dict()
        self.events = {}

    def _build_deque(self, category):
        limit = self.limits.get(category, self.DEFAULT_LIMIT)
        return deque(maxlen=limit)

    def append(self, event):
        level = event.level
        if level not in self.events:
            self.events[level] = self._build_deque(level)
        self.events[level].append(event)

    def get_events(self, min_level=None):
        min_level       = min_level or self.NO_LEVEL
        event_iterable  = self.events.iteritems()
        groups          = (e for key, e in event_iterable if key >= min_level)
        return itertools.chain.from_iterable(groups)
    __iter__ = get_events


class Event(object):
    """Data object for storing details of an event."""
    __slots__ = ('entity', 'time', 'level', 'name', 'data')

    def __init__(self, entity, level, name, **data):
        self.entity     = entity
        self.time       = timeutils.current_time()
        self.level      = level
        self.name       = name
        self.data       = data


class EventRecorder(object):
    """A node in a tree which stores EventRecorders, links to children,
    and adds missing children on get_child().
    """
    __slots__ = ('name', 'children', 'events')

    def __init__(self, name):
        self.name           = name
        self.children       = {}
        self.events         = EventStore()

    def get_child(self, child_key):
        if child_key in self.children:
            return self.children[child_key]

        split_char      = NAME_CHARACTER
        name_parts      = [self.name, child_key] if self.name else [child_key]
        child_name      = split_char.join(name_parts)
        child           = EventRecorder(child_name)
        return self.children.setdefault(child_key, child)

    def remove_child(self, child_key):
        if child_key in self.children:
            del self.children[child_key]

    def _record(self, level, name, **data):
        self.events.append(Event(self.name, level, name, **data))

    def list(self, min_level=None, child_events=True):
        if child_events:
            events = self._events_with_child_events(min_level)
        else:
            events = self.events.get_events(min_level)
        return sorted(events, key=operator.attrgetter('time'), reverse=True)

    def _events_with_child_events(self, min_level):
        """Yield all events and all child events which were recorded with a
        level greater than or equal to min_level.
        """
        for event in self.events.get_events(min_level):
            yield event
        for child in self.children.itervalues():
            for event in child._events_with_child_events(min_level):
                yield event

    def info(self, name, **data):
        return self._record(LEVEL_INFO,     name, **data)

    def ok(self, name, **data):
        return self._record(LEVEL_OK,       name, **data)

    def notice(self, name, **data):
        return self._record(LEVEL_NOTICE,   name, **data)

    def critical(self, name, **data):
        return self._record(LEVEL_CRITICAL, name, **data)


def get_recorder(entity_name=''):
    """Return an EventRecorder object which stores events for the entity
    identified by `entity_name`. Returns the root recorder if not name is
    given.
    """
    return EventManager.get_instance().get(entity_name)


class EventManager(object):
    """Create and store EventRecorder objects in a hierarchy based on
    the name of the entity name.
    """

    _instance = None

    def __init__(self):
        if self._instance is not None:
            raise ValueError("Use EventManger.get_instance()")
        self.root_recorder = EventRecorder('')

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def _get_name_parts(self, entity_name):
        return entity_name.split(NAME_CHARACTER) if entity_name else []

    def get(self, entity_name):
        """Search for and return the event recorder in the tree."""
        recorder = self.root_recorder
        for child_key in self._get_name_parts(entity_name):
            recorder = recorder.get_child(child_key)

        return recorder

    @classmethod
    def reset(cls):
        cls.get_instance().recorders = EventRecorder('')

    def remove(self, entity_name):
        """Remove an event recorder."""
        recorder        = self.root_recorder
        name_parts      = self._get_name_parts(entity_name)
        for child_key in name_parts[:-1]:
            recorder = recorder.get_child(child_key)

        recorder.remove_child(name_parts[-1])

########NEW FILE########
__FILENAME__ = eventloop
"""Minimal abstraction oer an event loop."""

from twisted.internet import reactor
from twisted.internet.base import DelayedCall


class Callback(DelayedCall):
    """
        active() - not (cancelled or called)
    """
    pass

class NullCallback(object):

    @staticmethod
    def cancel():
        pass

    @staticmethod
    def active():
        return False


def call_later(interval, *args, **kwargs):
    return reactor.callLater(interval, *args, **kwargs)


class UniqueCallback(object):
    """Wrap a DelayedCall so there can be only one instance of this call
    queued at a time. A Falsy delay causes this object to do nothing.
    """

    def __init__(self, delay, func, *args, **kwargs):
        self.delay          = delay
        self.func           = func
        self.args           = args
        self.kwargs         = kwargs
        self.delayed_call   = NullCallback

    def start(self):
        if not self.delay or self.delayed_call.active():
            return

        self.delayed_call = call_later(
            self.delay, self.func, *self.args, **self.kwargs)

    def cancel(self):
        if self.delayed_call.active():
            self.delayed_call.cancel()
########NEW FILE########
__FILENAME__ = mcp
from __future__ import with_statement
import logging

from tron import command_context, actioncommand
from tron import event
from tron import crash_reporter
from tron import node
from tron.config import manager
from tron.core import service, job
from tron.serialize.runstate import statemanager
from tron.utils import emailer


log = logging.getLogger(__name__)


def apply_master_configuration(mapping, master_config):
    def get_config_value(seq):
        return [getattr(master_config, item) for item in seq]

    for entry in mapping:
        func, args = entry[0], get_config_value(entry[1:])
        func(*args)


class MasterControlProgram(object):
    """Central state object for the Tron daemon."""

    def __init__(self, working_dir, config_path):
        super(MasterControlProgram, self).__init__()
        self.jobs               = job.JobCollection()
        self.services           = service.ServiceCollection()
        self.working_dir        = working_dir
        self.crash_reporter     = None
        self.config             = manager.ConfigManager(config_path)
        self.context            = command_context.CommandContext()
        self.event_recorder     = event.get_recorder()
        self.event_recorder.ok('started')
        self.state_watcher      = statemanager.StateChangeWatcher()

    def shutdown(self):
        self.state_watcher.shutdown()

    def graceful_shutdown(self):
        """Inform JobCollection that a shutdown has been requested."""
        self.jobs.request_shutdown()

    def reconfigure(self):
        """Reconfigure MCP while Tron is already running."""
        self.event_recorder.ok("reconfigured")
        try:
            self._load_config(reconfigure=True)
        except Exception:
            self.event_recorder.critical("reconfigure_failure")
            log.exception("reconfigure failure")
            raise

    def _load_config(self, reconfigure=False):
        """Read config data and apply it."""
        with self.state_watcher.disabled():
            self.apply_config(self.config.load(), reconfigure=reconfigure)

    def initial_setup(self):
        """When the MCP is initialized the config is applied before the state.
        In this case jobs shouldn't be scheduled until the state is applied.
        """
        self._load_config()
        self.restore_state()
        # Any job with existing state would have been scheduled already. Jobs
        # without any state will be scheduled here.
        self.jobs.run_queue_schedule()

    def apply_config(self, config_container, reconfigure=False):
        """Apply a configuration."""
        master_config_directives = [
            (self.update_state_watcher_config,           'state_persistence'),
            (self.set_context_base,                      'command_context'),
            (node.NodePoolRepository.update_from_config, 'nodes',
                                                         'node_pools',
                                                         'ssh_options'),
            (self.apply_notification_options,            'notification_options'),
        ]
        master_config = config_container.get_master()
        apply_master_configuration(master_config_directives, master_config)

        # TODO: unify NOTIFY_STATE_CHANGE and simplify this
        factory = self.build_job_scheduler_factory(master_config)
        self.apply_collection_config(config_container.get_jobs(),
            self.jobs, job.Job.NOTIFY_STATE_CHANGE, factory, reconfigure)

        self.apply_collection_config(config_container.get_services(),
            self.services, service.Service.NOTIFY_STATE_CHANGE, self.context)

    def apply_collection_config(self, config, collection, notify_type, *args):
        items = collection.load_from_config(config, *args)
        self.state_watcher.watch_all(items, notify_type)

    def build_job_scheduler_factory(self, master_config):
        output_stream_dir = master_config.output_stream_dir or self.working_dir
        action_runner = actioncommand.create_action_runner_factory_from_config(
            master_config.action_runner)
        return job.JobSchedulerFactory(
            self.context,
            output_stream_dir,
            master_config.time_zone,
            action_runner)

    def update_state_watcher_config(self, state_config):
        """Update the StateChangeWatcher, and save all state if the state config
        changed.
        """
        if self.state_watcher.update_from_config(state_config):
            for job_scheduler in self.jobs:
                self.state_watcher.save_job(job_scheduler.get_job())
            for service in self.services:
                self.state_watcher.save_service(service)

    def apply_notification_options(self, conf):
        if not conf:
            return

        if self.crash_reporter:
            self.crash_reporter.stop()

        email_sender = emailer.Emailer(conf.smtp_host, conf.notification_addr)
        self.crash_reporter = crash_reporter.CrashReporter(email_sender)
        self.crash_reporter.start()

    def set_context_base(self, command_context):
        self.context.base = command_context

    def get_job_collection(self):
        return self.jobs

    def get_service_collection(self):
        return self.services

    def get_config_manager(self):
        return self.config

    def restore_state(self):
        """Use the state manager to retrieve to persisted state and apply it
        to the configured Jobs and Services.
        """
        self.event_recorder.notice('restoring')
        job_states, service_states = self.state_watcher.restore(
                self.jobs.get_names(), self.services.get_names())

        self.jobs.restore_state(job_states)
        self.services.restore_state(service_states)
        self.state_watcher.save_metadata()

    def __str__(self):
        return "MCP"

########NEW FILE########
__FILENAME__ = node
import logging
import itertools
import random
from twisted.conch.client.knownhosts import KnownHostsFile

from twisted.internet import protocol, defer, reactor
from twisted.python import failure
from twisted.python.filepath import FilePath

from tron import ssh, eventloop
from tron.utils import twistedutils, collections


log = logging.getLogger(__name__)


# We should also only wait a certain amount of time for a new channel to be
# established when we already have an open connection.  This timeout will
# usually get triggered prior to even a TCP timeout, so essentially it's our
# shortcut to discovering the connection died.
RUN_START_TIMEOUT = 20

# Love to run this, but we need to finish connecting to our node first
RUN_STATE_CONNECTING = 0

# We are connected and trying to open a channel to exec the process
RUN_STATE_STARTING = 5

# Process has been exec'ed, just waiting for it to exit
RUN_STATE_RUNNING = 10

# Process has exited
RUN_STATE_COMPLETE = 100


class Error(Exception):
    pass


class ConnectError(Error):
    """There was a problem connecting, run was never started"""
    pass


class ResultError(Error):
    """There was a problem retrieving the result from this run

    We did try to execute the command, but we don't know if it succeeded or
    failed.
    """
    pass


class NodePoolRepository(object):
    """A Singleton to store Node and NodePool objects."""

    _instance = None

    def __init__(self):
        if self._instance is not None:
            raise ValueError("NodePoolRepository is already instantiated.")
        super(NodePoolRepository, self).__init__()
        self.nodes = collections.MappingCollection('nodes')
        self.pools = collections.MappingCollection('pools')

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def filter_by_name(self, node_configs, node_pool_configs):
        self.nodes.filter_by_name(node_configs)
        self.pools.filter_by_name(node_configs.keys() + node_pool_configs.keys())

    @classmethod
    def update_from_config(cls, node_configs, node_pool_configs, ssh_config):
        instance = cls.get_instance()
        ssh_options = ssh.SSHAuthOptions.from_config(ssh_config)
        known_hosts = KnownHosts.from_path(ssh_config.known_hosts_file)
        instance.filter_by_name(node_configs, node_pool_configs)
        instance._update_nodes(node_configs, ssh_options, known_hosts, ssh_config)
        instance._update_node_pools(node_pool_configs)

    def _update_nodes(self, node_configs, ssh_options, known_hosts, ssh_config):
        for config in node_configs.itervalues():
            pub_key = known_hosts.get_public_key(config.hostname)
            node = Node.from_config(config, ssh_options, pub_key, ssh_config)
            self.add_node(node)

    def _update_node_pools(self, node_pool_configs):
        for config in node_pool_configs.itervalues():
            nodes = self._get_nodes_by_name(config.nodes)
            pool  = NodePool.from_config(config, nodes)
            self.pools.replace(pool)

    def add_node(self, node):
        self.nodes.replace(node)
        self.pools.replace(NodePool.from_node(node))

    def get_node(self, node_name, default=None):
        return self.nodes.get(node_name, default)

    def __contains__(self, node):
        return node.get_name() in self.pools

    def get_by_name(self, name, default=None):
        return self.pools.get(name, default)

    def _get_nodes_by_name(self, names):
        return [self.nodes[name] for name in names]

    def clear(self):
        self.nodes.clear()
        self.pools.clear()


class NodePool(object):
    """A pool of Node objects."""
    def __init__(self, nodes, name):
        self.nodes      = nodes
        self.disabled   = False
        self.name       = name or '_'.join(n.get_name() for n in nodes)
        self.iter       = itertools.cycle(self.nodes)

    @classmethod
    def from_config(cls, node_pool_config, nodes):
        return cls(nodes, node_pool_config.name)

    @classmethod
    def from_node(cls, node):
        return cls([node], node.get_name())

    def __eq__(self, other):
        return isinstance(other, NodePool) and self.nodes == other.nodes

    def __ne__(self, other):
        return not self == other

    def get_name(self):
        return self.name

    def get_nodes(self):
        return self.nodes

    def next(self):
        """Return a random node from the pool."""
        return random.choice(self.nodes)

    def next_round_robin(self):
        """Return the next node cycling in a consistent order."""
        return self.iter.next()

    def disable(self):
        """Required for MappingCollection.Item interface."""
        self.disabled = True

    def get_by_hostname(self, hostname):
        for node in self.nodes:
            if node.hostname == hostname:
                return node

    def __str__(self):
        return "NodePool:%s" % self.name


class KnownHosts(KnownHostsFile):
    """Lookup host key for a hostname."""

    @classmethod
    def from_path(cls, file_path):
        if not file_path:
            return cls(None)
        return cls.fromPath(FilePath(file_path))

    def get_public_key(self, hostname):
        for entry in self._entries:
            if entry.matchesHost(hostname):
                return entry.publicKey
        log.warn("Missing host key for: %s", hostname)


class RunState(object):
    def __init__(self, action_run):
        self.run = action_run
        self.state = RUN_STATE_CONNECTING
        self.deferred = defer.Deferred()
        self.channel = None


def determine_jitter(count, node_settings):
    """Return a pseudo-random number of seconds to delay a run."""
    count *= node_settings.jitter_load_factor
    min_count = node_settings.jitter_min_load
    max_jitter = max(0.0, count - min_count)
    max_jitter = min(node_settings.jitter_max_delay, max_jitter)
    return random.random() * float(max_jitter)


class Node(object):
    """A node is tron's interface to communicating with an actual machine.
    """

    def __init__(self, config, ssh_options, pub_key, node_settings):
        self.config = config
        self.node_settings = node_settings

        # SSH Options
        self.conch_options = ssh_options

        # The SSH connection we use to open channels on. If present, means we
        # are connected.
        self.connection = None

        # If present, means we are trying to connect
        self.connection_defer = None

        # Map of run id to instance of RunState
        self.run_states = {}

        self.idle_timer = eventloop.NullCallback
        self.disabled = False
        self.pub_key = pub_key

    @property
    def hostname(self):
        return self.config.hostname

    @property
    def username(self):
        return self.config.username

    @property
    def port(self):
        return self.config.port

    @classmethod
    def from_config(cls, node_config, ssh_options, pub_key, node_settings):
        return cls(node_config, ssh_options, pub_key, node_settings)

    def get_name(self):
        return self.config.name

    name = property(get_name)

    def disable(self):
        """Required for MappingCollection.Item interface."""
        self.disabled = True

    def __eq__(self, other):
        if not isinstance(other, self.__class__):
            return False
        return (self.config == other.config and
                self.conch_options == other.conch_options and
                self.pub_key == other.pub_key and
                self.node_settings == other.node_settings)


    def __ne__(self, other):
        return not self == other

    # TODO: Test
    def submit_command(self, command):
        """Submit an ActionCommand to be run on this node. Optionally provide
        an error callback which will be called on error.
        """
        deferred = self.run(command)
        deferred.addErrback(command.handle_errback)
        return deferred

    def run(self, run):
        """Execute the specified run

        A run consists of a very specific set of interfaces which allow us to
        execute a command on this remote machine and return results.
        """
        log.info("Running %s for %s on %s", run.command, run.id, self.hostname)

        # When this run completes, for good or bad, we'll inform the caller by
        # calling 'succeed' or 'fail' on the run Since the definined interface
        # is on these specific callbacks, we won't bother returning the
        # deferred here. This allows the caller to not really care about
        # twisted specific stuff at all, all it needs to know is that one of
        # those functions will eventually be called back

        if run.id in self.run_states:
            raise Error("Run %s already running !?!", run.id)

        if self.idle_timer.active():
            self.idle_timer.cancel()

        self.run_states[run.id] = RunState(run)

        # TODO: have this return a runner instead of number
        fudge_factor = determine_jitter(len(self.run_states), self.node_settings)
        if fudge_factor == 0.0:
            self._do_run(run)
        else:
            log.info("Delaying execution of %s for %.2f secs", run.id, fudge_factor)
            eventloop.call_later(fudge_factor, self._do_run, run)

        # We return the deferred here, but really we're trying to keep the rest
        # of the world from getting too involved with twisted.
        return self.run_states[run.id].deferred

    def stop(self, command):
        """Stop this command by marking it as failed."""
        exc = failure.Failure(exc_value=ResultError("Run stopped"))
        self._fail_run(command, exc)

    def _do_run(self, run):
        """Finish starting to execute a run

        This step may have been delayed.
        """

        # Now let's see if we need to start this off by establishing a
        # connection or if we are already connected
        if self.connection is None:
            self._connect_then_run(run)
        else:
            self._open_channel(run)

    def _cleanup(self, run):
        # TODO: why set to None before deleting it?
        self.run_states[run.id].channel = None
        del self.run_states[run.id]

        if not self.run_states:
            self.idle_timer = eventloop.call_later(
                self.node_settings.idle_connection_timeout,
                self._connection_idle_timeout)

    def _connection_idle_timeout(self):
        if self.connection:
            log.info("Connection to %s idle for %d secs. Closing.",
                     self.hostname, self.node_settings.idle_connection_timeout)
            self.connection.transport.loseConnection()

    def _fail_run(self, run, result):
        """Indicate the run has failed, and cleanup state"""
        log.debug("Run %s has failed", run.id)
        if run.id not in self.run_states:
            log.warning("Run %s no longer tracked (_fail_run)", run.id)
            return

        # Add a dummy errback handler to prevent Unhandled error messages.
        # Unless somone is explicitly caring about this defer the error will
        # have been reported elsewhere.
        self.run_states[run.id].deferred.addErrback(lambda failure: None)

        cb = self.run_states[run.id].deferred.errback

        self._cleanup(run)

        log.info("Calling fail_run callbacks")
        run.exited(None)
        cb(result)

    def _connect_then_run(self, run):
        # Have we started the connection process ?
        if self.connection_defer is None:
            self.connection_defer = self._connect()

        def call_open_channel(arg):
            self._open_channel(run)
            return arg

        def connect_fail(result):
            log.warning("Cannot run %s, Failed to connect to %s",
                        run, self.hostname)
            self.connection_defer = None
            self._fail_run(run, failure.Failure(
                exc_value=ConnectError("Connection to %s failed" %
                                       self.hostname)))

        self.connection_defer.addCallback(call_open_channel)
        self.connection_defer.addErrback(connect_fail)

    def _service_stopped(self, connection):
        """Called when the SSH service has disconnected fully.

        We should be in a state where we know there are no runs in progress
        because all the SSH channels should have disconnected them.
        """
        assert self.connection is connection
        self.connection = None

        log.info("Service to %s stopped", self.hostname)

        for run_id, run in self.run_states.iteritems():
            if run.state == RUN_STATE_CONNECTING:
                # Now we can trigger a reconnect and re-start any waiting runs.
                self._connect_then_run(run)
            elif run.state == RUN_STATE_RUNNING:
                self._fail_run(run, None)
            elif run.state == RUN_STATE_STARTING:
                if run.channel and run.channel.start_defer is not None:

                    # This means our run IS still waiting to start. There
                    # should be an outstanding timeout sitting on this guy as
                    # well. We'll just short circut it.
                    twistedutils.defer_timeout(run.channel.start_defer, 0)
                else:
                    # Doesn't seem like this should ever happen.
                    log.warning("Run %r caught in starting state, but"
                                " start_defer is over.", run_id)
                    self._fail_run(run, None)
            else:
                # Service ended. The open channels should know how to handle
                # this (and cleanup) themselves, so if there should not be any
                # runs except those waiting to connect
                raise Error("Run %s in state %s when service stopped",
                            run_id, run.state)

    def _connect(self):
        # This is complicated because we have to deal with a few different
        # steps before our connection is really available for us:
        #  1. Transport is created (our client creator does this)
        #  2. Our transport is secure, and we can create our connection
        #  3. The connection service is started, so we can use it

        client_creator = protocol.ClientCreator(reactor,
            ssh.ClientTransport, self.username, self.conch_options, self.pub_key)
        create_defer = client_creator.connectTCP(self.hostname, self.config.port)

        # We're going to create a deferred, returned to the caller, that will
        # be called back when we have an established, secure connection ready
        # for opening channels. The value will be this instance of node.
        connect_defer = defer.Deferred()
        twistedutils.defer_timeout(connect_defer, self.node_settings.connect_timeout)

        def on_service_started(connection):
            # Booyah, time to start doing stuff
            self.connection = connection
            self.connection_defer = None

            connect_defer.callback(self)
            return connection

        def on_connection_secure(connection):
            # We have a connection, but it might not be fully ready....
            connection.service_start_defer = defer.Deferred()
            connection.service_stop_defer = defer.Deferred()

            connection.service_start_defer.addCallback(on_service_started)
            connection.service_stop_defer.addCallback(self._service_stopped)
            return connection

        def on_transport_create(transport):
            transport.connection_defer = defer.Deferred()
            transport.connection_defer.addCallback(on_connection_secure)
            return transport

        def on_transport_fail(fail):
            log.warning("Cannot connect to %s", self.hostname)
            connect_defer.errback(fail)

        create_defer.addCallback(on_transport_create)
        create_defer.addErrback(on_transport_fail)

        return connect_defer

    def _open_channel(self, run):
        assert self.connection
        assert self.run_states[run.id].state < RUN_STATE_RUNNING

        self.run_states[run.id].state = RUN_STATE_STARTING

        chan = ssh.ExecChannel(conn=self.connection)

        chan.addOutputCallback(run.write_stdout)
        chan.addErrorCallback(run.write_stderr)
        chan.addEndCallback(run.done)

        chan.command = run.command
        chan.start_defer = defer.Deferred()
        chan.start_defer.addCallback(self._run_started, run)
        chan.start_defer.addErrback(self._run_start_error, run)

        chan.exit_defer = defer.Deferred()
        chan.exit_defer.addCallback(self._channel_complete, run)
        chan.exit_defer.addErrback(self._channel_complete_unknown, run)

        twistedutils.defer_timeout(chan.start_defer, RUN_START_TIMEOUT)

        self.run_states[run.id].channel = chan
        # TODO: I believe this needs to be checking the health of the connection
        # before trying to open a new channel.  If the connection is gone it
        # needs to re-establish, or if the connection is not responding
        # we shouldn't create this new channel
        self.connection.openChannel(chan)

    def _channel_complete(self, channel, run):
        """Callback once our channel has completed it's operation

        This is how we let our run know that we succeeded or failed.
        """
        log.info("Run %s has completed with %r", run.id, channel.exit_status)
        if run.id not in self.run_states:
            log.warning("Run %s no longer tracked", run.id)
            return

        assert self.run_states[run.id].state < RUN_STATE_COMPLETE

        self.run_states[run.id].state = RUN_STATE_COMPLETE
        cb = self.run_states[run.id].deferred.callback
        self._cleanup(run)

        run.exited(channel.exit_status)
        cb(channel.exit_status)

    def _channel_complete_unknown(self, result, run):
        """Channel has closed on a running process without a proper exit

        We don't actually know if the run succeeded
        """
        log.error("Failure waiting on channel completion: %s", str(result))
        self._fail_run(run, failure.Failure(exc_value=ResultError()))

    def _run_started(self, channel, run):
        """Our run is actually a running process now, update the state"""
        log.info("Run %s started for %s", run.id, self.hostname)
        channel.start_defer = None
        if run.id not in self.run_states:
            log.warning("Run %s no longer tracked (_run_started)", run.id)
            return
        assert self.run_states[run.id].state == RUN_STATE_STARTING
        self.run_states[run.id].state = RUN_STATE_RUNNING

        run.started()

    def _run_start_error(self, result, run):
        """We failed to even run the command due to communication difficulties

        Once all the runs have closed out we can try to reconnect.
        """
        log.error("Error running %s, disconnecting from %s: %s",
                  run.id, self.hostname, str(result))

        # We clear out the deferred that likely called us because there are
        # actually more than one error paths because of user timeouts.
        if run.id in self.run_states:
            self.run_states[run.id].channel.start_defer = None

        self._fail_run(run, failure.Failure(
            exc_value=ConnectError("Connection to %s failed" % self.hostname)))

        # We want to hard hangup on this connection. It could theoretically
        # come back thanks to the magic of TCP, but something is up, best to
        # fail right now then limp along for and unknown amount of time.
        #self.connection.transport.connectionLost(failure.Failure())

    def __str__(self):
        return "Node:%s@%s:%s" % (
            self.username or "<default>", self.hostname, self.config.port)

    def __repr__(self):
        return self.__str__()

########NEW FILE########
__FILENAME__ = scheduler
"""
Tron schedulers

 A scheduler has a simple interface.

 class Scheduler(object):

    schedule_on_complete = <bool>

    def next_run_time(self, last_run_time):
        <returns datetime>


 next_run_time() should return a datetime which is the time the next job run
 will be run.

 schedule_on_complete is a bool that identifies if this scheduler should have
 jobs scheduled with the start_time of the previous run (False), or the
 end time of the previous run (False).
"""
import logging
import random
import datetime

from pytz import AmbiguousTimeError, NonExistentTimeError
from tron.config import schedule_parse

from tron.utils import trontimespec
from tron.utils import timeutils


log = logging.getLogger(__name__)


def scheduler_from_config(config, time_zone):
    """A factory for creating a scheduler from a configuration object."""
    if isinstance(config, schedule_parse.ConfigConstantScheduler):
        return ConstantScheduler()

    if isinstance(config, schedule_parse.ConfigIntervalScheduler):
        return IntervalScheduler(config.timedelta, config.jitter)

    if isinstance(config, schedule_parse.ConfigGrocScheduler):
        return GeneralScheduler(
            time_zone=time_zone,
            timestr=config.timestr or '00:00',
            ordinals=config.ordinals,
            monthdays=config.monthdays,
            months=config.months,
            weekdays=config.weekdays,
            name='groc',
            original=config.original,
            jitter=config.jitter)

    if isinstance(config, schedule_parse.ConfigCronScheduler):
        return GeneralScheduler(
            minutes=config.minutes,
            hours=config.hours,
            monthdays=config.monthdays,
            months=config.months,
            weekdays=config.weekdays,
            ordinals=config.ordinals,
            seconds=[0],
            name='cron',
            original=config.original,
            jitter=config.jitter)

    if isinstance(config, schedule_parse.ConfigDailyScheduler):
        return GeneralScheduler(
            hours=[config.hour],
            minutes=[config.minute],
            seconds=[config.second],
            weekdays=config.days,
            name='daily',
            original=config.original,
            jitter=config.jitter)


class ConstantScheduler(object):
    """The constant scheduler schedules a new job immediately."""
    schedule_on_complete = True

    def next_run_time(self, _):
        return timeutils.current_time()

    def __str__(self):
        return self.get_name()

    def __eq__(self, other):
        return isinstance(other, ConstantScheduler)

    def __ne__(self, other):
        return not self == other

    def get_jitter(self):
        pass

    def get_name(self):
        return 'constant'

    def get_value(self):
        return ''


def get_jitter(time_delta):
    if not time_delta:
        return datetime.timedelta()
    seconds = timeutils.delta_total_seconds(time_delta)
    return datetime.timedelta(seconds=random.randint(-seconds, seconds))


def get_jitter_str(time_delta):
    if not time_delta:
        return ''
    return ' (+/- %s)' % time_delta


class GeneralScheduler(object):
    """Scheduler which uses a TimeSpecification.
    """
    schedule_on_complete = False

    def __init__(self,
            ordinals=None,
            weekdays=None,
            months=None,
            monthdays=None,
            timestr=None,
            minutes=None,
            hours=None,
            seconds=None,
            time_zone=None,
            name=None,
            original=None,
            jitter=None):
        """Parameters:
          timestr     - the time of day to run, as 'HH:MM'
          ordinals    - first, second, third &c, as a set of integers in 1..5 to
                        be used with "1st <weekday>", etc.
          monthdays   - set of integers to be used with "<month> 3rd", etc.
          months      - the months that this should run, as a set of integers in
                        1..12
          weekdays    - the days of the week that this should run, as a set of
                        integers, 0=Sunday, 6=Saturday
          timezone    - the optional timezone as a string for this specification.
                        Defaults to UTC - valid entries are things like
                        Australia/Victoria or PST8PDT.
        """
        self.time_zone      = time_zone
        self.jitter         = jitter
        self.name           = name or 'daily'
        self.original       = original or ''
        self.time_spec      = trontimespec.TimeSpecification(
            ordinals=ordinals,
            weekdays=weekdays,
            months=months,
            monthdays=monthdays,
            timestr=timestr,
            hours=hours,
            minutes=minutes,
            seconds=seconds,
            timezone=time_zone.zone if time_zone else None)

    def next_run_time(self, start_time):
        """Find the next time to run."""
        if not start_time:
            start_time = timeutils.current_time()
        elif self.time_zone:
            try:
                start_time = self.time_zone.localize(start_time, is_dst=None)
            except AmbiguousTimeError:
                # We are in the infamous 1 AM block which happens twice on
                # fall-back. Pretend like it's the first time, every time.
                start_time = self.time_zone.localize(start_time, is_dst=True)
            except NonExistentTimeError:
                # We are in the infamous 2:xx AM block which does not
                # exist. Pretend like it's the later time, every time.
                start_time = self.time_zone.localize(start_time, is_dst=True)

        return self.time_spec.get_match(start_time) + get_jitter(self.jitter)

    def __str__(self):
        return '%s %s%s' % (
            self.name, self.original, get_jitter_str(self.jitter))

    def __eq__(self, other):
        return hasattr(other, 'time_spec') and self.time_spec == other.time_spec

    def __ne__(self, other):
        return not self == other

    def get_jitter(self):
        return self.jitter

    def get_name(self):
        return self.name

    def get_value(self):
        return self.original


class IntervalScheduler(object):
    """The interval scheduler runs a job (to success) based on a configured
    interval.
    """
    schedule_on_complete = False

    def __init__(self, interval, jitter):
        self.interval = interval
        self.jitter = jitter

    def next_run_time(self, last_run_time):
        last_run_time = last_run_time or timeutils.current_time()
        return last_run_time + self.interval + get_jitter(self.jitter)

    def __str__(self):
        return "%s %s%s" % (
            self.get_name(), self.interval, get_jitter_str(self.jitter))

    def __eq__(self, other):
        return (isinstance(other, IntervalScheduler) and
                self.interval == other.interval)

    def __ne__(self, other):
        return not self == other

    def get_jitter(self):
        return self.jitter

    def get_name(self):
        return "interval"

    def get_value(self):
        return str(self.interval)

########NEW FILE########
__FILENAME__ = filehandler
"""
Tools for managing and properly closing file handles.
"""
import logging
import os
import os.path
import shutil
import sys
from subprocess import PIPE, Popen
import time


from tron.utils.dicts import OrderedDict

log = logging.getLogger(__name__)


class NullFileHandle(object):
    """A No-Op object that supports a File interface."""
    closed = True
    @classmethod
    def write(cls, _):
        pass
    @classmethod
    def close(cls):
        pass


class FileHandleWrapper(object):
    """Acts as a proxy to file handles.  Wrap a file handle and stores
    access time and metadata.  These objects should only be created
    by FileHandleManager. Do not instantiate them on their own.
    """
    __slots__ = ['manager', 'name', 'last_accessed', '_fh']

    def __init__(self, manager, name):
        self.manager = manager
        self.name = name
        self.last_accessed = time.time()
        self._fh = NullFileHandle

    def close(self):
        self.close_wrapped()
        self.manager.remove(self)

    def close_wrapped(self):
        """Close only the underlying file handle."""
        self._fh.close()
        self._fh = NullFileHandle

    def write(self, content):
        """Write content to the fh. Re-open if necessary."""
        if self._fh == NullFileHandle:
            try:
                self._fh = open(self.name, 'a')
            except IOError, e:
                log.error("Failed to open %s: %s", self.name, e)
                return

        self.last_accessed = time.time()
        self._fh.write(content)
        self.manager.update(self)

    def __enter__(self):
        return self

    def __exit__(self, _exc_type, _exc_val, _exc_tb):
        self.close()


class FileHandleManager(object):
    """Creates FileHandleWrappers, closes handles when they have
    been inactive for a period of time, and transparently re-open the next
    time they are needed. All files are opened in append mode.

    This class is singleton.  An already configured instance can be
    retrieving by using get_instance() (and will be created if None),
    max_idle_time can be set by calling the classmethod set_max_idle_time()
    """

    _instance = None

    def __init__(self, max_idle_time=60):
        """
            Create a new instance.
            max_idle_time           - max idle time in seconds
        """
        if self.__class__._instance:
            msg = "FileHandleManager is a singleton. Call get_instance()"
            raise ValueError(msg)
        self.max_idle_time = max_idle_time
        self.cache = OrderedDict()
        self.__class__._instance = self

    @classmethod
    def set_max_idle_time(cls, max_idle_time):
        inst = cls.get_instance()
        inst.max_idle_time = max_idle_time

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    @classmethod
    def reset(cls):
        """Empty the cache and reset the instance to it's original state."""
        inst = cls.get_instance()
        for fh_wrapper in inst.cache.values():
            inst.remove(fh_wrapper)

    def open(self, filename):
        """Retrieve a file handle from the cache based on name.  Returns a
        FileHandleWrapper. If the handle is not in the cache, create a new
        instance.
        """
        if filename in self.cache:
            return self.cache[filename]
        fhw = FileHandleWrapper(self, filename)
        self.cache[filename] = fhw
        return fhw

    def cleanup(self, time_func=time.time):
        """Close any file handles that have been idle for longer than
        max_idle_time. time_func is primary used for testing.
        """
        if not self.cache:
            return

        cur_time = time_func()
        for name, fh_wrapper in self.cache.items():
            if cur_time - fh_wrapper.last_accessed > self.max_idle_time:
                fh_wrapper.close()
            else:
                break

    def remove(self, fh_wrapper):
        """Remove the fh_wrapper from the cache and access_order."""
        if fh_wrapper.name in self.cache:
            del self.cache[fh_wrapper.name]

    def update(self, fh_wrapper):
        """Remove and re-add the file handle to the cache so that it's keys
        are still ordered by last access. Calls cleanup() to remove any file
        handles that have been idle for too long.
        """
        self.remove(fh_wrapper)
        self.cache[fh_wrapper.name] = fh_wrapper
        self.cleanup()


class OutputStreamSerializer(object):
    """Manage writing to and reading from files in a directory hierarchy."""

    def __init__(self, base_path):
        self.base_path = os.path.join(*base_path)
        if not os.path.exists(self.base_path):
            os.makedirs(self.base_path)

    def full_path(self, filename):
        return os.path.join(self.base_path, filename)

    # TODO: do not use subprocess
    def tail(self, filename, num_lines=None):
        """Tail a file using `tail`."""
        path = self.full_path(filename)
        if not path or not os.path.exists(path):
            return []
        if not num_lines:
            num_lines = sys.maxint

        try:
            cmd = ('tail', '-n', str(num_lines), path)
            tail_sub = Popen(cmd, stdout=PIPE)
            return list(line.rstrip() for line in tail_sub.stdout)
        except OSError, e:
            log.error("Could not tail %s: %s" % (path, e))
            return []

    def open(self, filename):
        """Return a FileHandleManager for the output path."""
        path = self.full_path(filename)
        return FileHandleManager.get_instance().open(path)


class OutputPath(object):
    """A list like object used to construct a file path for output. The
    file path is constructed by joining the base path with any additional
    path elements.
    """
    __slots__ = ['base', 'parts']

    def __init__(self, base='.', *path_parts):
        self.base = base
        self.parts = list(path_parts or [])

    def append(self, part):
        self.parts.append(part)

    def __iter__(self):
        yield self.base
        for p in self.parts:
            yield p

    def __str__(self):
        return os.path.join(*self)

    def clone(self, *parts):
        """Return a new OutputPath object which has a base of the str value
        of this object.
        """
        return type(self)(str(self), *parts)

    def delete(self):
        """Remove the directory and its contents."""
        try:
            shutil.rmtree(str(self))
        except OSError, e:
            log.warn("Failed to delete %s: %s" % (self, e))

    def __eq__(self, other):
        return self.base == other.base and self.parts == other.parts

    def __ne__(self, other):
        return not self == other

########NEW FILE########
__FILENAME__ = mongostore
"""
 State storage using mongoDB.
 Tested with pymongo 2.2
"""

from collections import namedtuple
import urlparse
import itertools
import operator
from tron.serialize import runstate
pymongo = None # pyflakes


MongoStateKey = namedtuple('MongoStateKey', ['collection', 'key'])


class MongoStateStore(object):

    JOB_COLLECTION              = 'job_state_collection'
    SERVICE_COLLECTION          = 'service_state_collection'
    METADATA_COLLECTION         = 'metadata_collection'

    TYPE_TO_COLLECTION_MAP = {
        runstate.JOB_STATE:     JOB_COLLECTION,
        runstate.SERVICE_STATE: SERVICE_COLLECTION,
        runstate.MCP_STATE:     METADATA_COLLECTION
    }

    def __init__(self, db_name, connection_details):
        import pymongo
        global pymongo
        assert pymongo

        self.db_name        = db_name
        connection_params   = self._parse_connection_details(connection_details)
        self._connect(db_name, connection_params)

    def _connect(self, db_name, params):
        """Connect to MongoDB."""
        hostname            = params.get('hostname')
        port                = params.get('port')
        username            = params.get('username')
        password            = params.get('password')
        self.connection     = pymongo.Connection(hostname, port)
        self.db             = self.connection[db_name]
        if username and password:
            self.db.authenticate(username, password)

    def _parse_connection_details(self, connection_details):
        if not connection_details:
            return {}
        return dict(urlparse.parse_qsl(connection_details))

    def build_key(self, type, iden):
        return MongoStateKey(self.TYPE_TO_COLLECTION_MAP[type], iden)

    def save(self, key_value_pairs):
        for key, state_data in key_value_pairs:
            state_data['_id'] = key.key
            collection = self.db[key.collection]
            collection.save(state_data)

    def restore(self, keys):
        items = [
            (key, self.db[key.collection].find_one(key.key)) for key in keys]
        return dict(itertools.ifilter(operator.itemgetter(1), items))

    def cleanup(self):
        self.connection.disconnect()

    def __str__(self):
        return "MongoStateStore(%s)" % self.db_name

########NEW FILE########
__FILENAME__ = shelvestore
import logging
import shelve
import operator
import itertools

log = logging.getLogger(__name__)

class ShelveKey(object):
    __slots__ = ['type', 'iden']

    def __init__(self, type, iden):
        self.type               = type
        self.iden               = iden

    @property
    def key(self):
        return "%s___%s" % (self.type, self.iden)

    def __str__(self):
        return "%s %s" % (self.type, self.iden)

    def __eq__(self, other):
        return self.type == other.type and self.iden == other.iden

    def __hash__(self):
        return hash(self.key)

class ShelveStateStore(object):
    """Persist state using `shelve`."""

    def __init__(self, filename):
        self.filename = filename
        self.shelve = shelve.open(self.filename)

    def build_key(self, type, iden):
        return ShelveKey(type, iden)

    def save(self, key_value_pairs):
        for key, state_data in key_value_pairs:
            self.shelve[key.key] = state_data
        self.shelve.sync()

    def restore(self, keys):
        items = itertools.izip(keys, (self.shelve.get(key.key) for key in keys))
        return dict(itertools.ifilter(operator.itemgetter(1), items))

    def cleanup(self):
        self.shelve.close()

    def __repr__(self):
        return "ShelveStateStore('%s')" % self.filename
########NEW FILE########
__FILENAME__ = sqlalchemystore
from collections import namedtuple
from contextlib import contextmanager
import itertools
import operator

import yaml
sqlalchemy = None # pyflakes

from tron.serialize import runstate
from tron.config.config_utils import MAX_IDENTIFIER_LENGTH


SQLStateKey = namedtuple('SQLStateKey', ['table', 'id'])


class SQLAlchemyStateStore(object):

    def __init__(self, name, connection_details):
        import sqlalchemy
        global sqlalchemy
        assert sqlalchemy # pyflakes

        self.name               = name
        self._connection        = None
        self.encoder            = yaml.dump
        self.decoder            = yaml.load
        self._create_engine(connection_details)
        self._build_tables()
        self.create_tables()

    def _create_engine(self, connection_details):
        """Connect to the configured database."""
        self.engine = sqlalchemy.create_engine(connection_details)

    def _build_tables(self):
        """Build table objects."""
        from sqlalchemy import Table, Column, String, Text
        self._metadata = sqlalchemy.MetaData()
        self.job_table = Table('job_state_data', self._metadata,
            Column('id', String(MAX_IDENTIFIER_LENGTH), primary_key=True),
            Column('state_data', Text)
        )

        self.service_table = Table('service_state_data', self._metadata,
            Column('id', String(MAX_IDENTIFIER_LENGTH), primary_key=True),
            Column('state_data', Text)
        )

        self.metadata_table = Table('metadata_table', self._metadata,
            Column('id', String(MAX_IDENTIFIER_LENGTH), primary_key=True),
            Column('state_data', Text)
        )

    def create_tables(self):
        """Execute the create table statements."""
        self._metadata.create_all(self.engine)

    @contextmanager
    def connect(self):
        """Yield a connection."""
        # TODO: handle 'mysql has gone away' and similar exceptions
        if not self._connection or self._connection.closed:
            self._connection = self.engine.connect()
        yield self._connection

    def build_key(self, type, iden):
        table = None
        if type == runstate.JOB_STATE:
            table = self.job_table
        if type == runstate.SERVICE_STATE:
            table = self.service_table
        if type == runstate.MCP_STATE:
            table = self.metadata_table
        return SQLStateKey(table, iden)

    def save(self, key_value_pairs):
        with self.connect() as conn:
            for key, state_data in key_value_pairs:
                state_data = self.encoder(state_data)

                # The first state update requires an insert
                if not self._update(conn, key, state_data):
                    self._insert(conn, key, state_data)

    def _update(self, conn, key, data):
        """Attempt to update the state_data."""
        update  = key.table.update()
        where   = key.table.c.id==key.id
        results = conn.execute(update.where(where).values(state_data=data))
        return results.rowcount

    def _insert(self, conn, key, state_data):
        """Attempt to insert the state_data."""
        insert = key.table.insert()
        conn.execute(insert.values(id=key.id, state_data=state_data))

    def restore(self, keys):
        with self.connect() as conn:
            items = [(key, self._select(conn, key)) for key in keys]
            return dict(itertools.ifilter(operator.itemgetter(1), items))

    def _select(self, conn, key):
        cols = [key.table.c.state_data]
        select = sqlalchemy.sql.select(cols, key.table.c.id==key.id)
        result = conn.execute(select).fetchone()
        return self.decoder(result[0]) if result else None

    def cleanup(self):
        if self._connection:
            self._connection.close()

    def __str__(self):
        return "SQLAlchemyStateStore(%s)" % self.name

########NEW FILE########
__FILENAME__ = statemanager
from contextlib import contextmanager
import logging
import time
import itertools
import tron
from tron.config import schema
from tron.core import job, service
from tron.serialize import runstate
from tron.serialize.runstate.mongostore import MongoStateStore
from tron.serialize.runstate.shelvestore import ShelveStateStore
from tron.serialize.runstate.sqlalchemystore import SQLAlchemyStateStore
from tron.serialize.runstate.yamlstore import YamlStateStore
from tron.utils import observer

log = logging.getLogger(__name__)


class VersionMismatchError(ValueError):
    """Raised when the state has a newer version then tron.__version.__."""

class PersistenceStoreError(ValueError):
    """Raised if the store can not be created or fails a read or write."""


class PersistenceManagerFactory(object):
    """Create a PersistentStateManager."""

    @classmethod
    def from_config(cls, persistence_config):
        store_type              = persistence_config.store_type
        name                    = persistence_config.name
        connection_details      = persistence_config.connection_details
        buffer_size             = persistence_config.buffer_size
        store                   = None

        if store_type not in schema.StatePersistenceTypes:
            raise PersistenceStoreError("Unknown store type: %s" % store_type)

        if store_type == schema.StatePersistenceTypes.shelve:
            store = ShelveStateStore(name)

        if store_type == schema.StatePersistenceTypes.sql:
            store = SQLAlchemyStateStore(name, connection_details)

        if store_type == schema.StatePersistenceTypes.mongo:
            store = MongoStateStore(name, connection_details)

        if store_type == schema.StatePersistenceTypes.yaml:
            store = YamlStateStore(name)

        buffer = StateSaveBuffer(buffer_size)
        return PersistentStateManager(store, buffer)


class StateMetadata(object):
    """A data object for saving state metadata. Conforms to the same
    RunState interface as Jobs and Services.
    """
    name                        = 'StateMetadata'
    version                     = tron.__version_info__

    def __init__(self):
        self.state_data         = {
            'version':              self.version,
            'create_time':          time.time(),
        }

    @classmethod
    def validate_metadata(cls, metadata):
        """Raises an exception if the metadata version is newer then
        tron.__version__.
        """
        if not metadata:
            return

        version = metadata['version']
        # Names (and state keys) changed in 0.5.2, requires migration
        # see tools/migration/migrate_state_to_namespace
        if version > cls.version or version < (0, 5, 2):
            msg = "State for version %s, expected %s"
            raise VersionMismatchError(
                msg % (metadata['version'] , cls.version))


class StateSaveBuffer(object):
    """Buffer calls to save, and perform the saves when buffer reaches
    buffer size. This buffer will only store one state_data for each key.
    """

    def __init__(self, buffer_size):
        self.buffer_size        = buffer_size
        self.buffer             = {}
        self.counter            = itertools.cycle(xrange(buffer_size))

    def save(self, key, state_data):
        """Save the state_data indexed by key and return True if the buffer
        is full.
        """
        self.buffer[key] = state_data
        return not self.counter.next()

    def __iter__(self):
        """Return all buffered data and clear the buffer."""
        for key, item in self.buffer.iteritems():
            yield key, item
        self.buffer.clear()


class PersistentStateManager(object):
    """Provides an interface to persist the state of Tron.

    The implementation of persisting and restoring the state from disk is
    handled by a class which supports the StateStore interface:

    class IStateStore(object):

        def build_key(self, type, identifier):
            return <a key>

        def restore(self, keys):
            return <dict of key to states>

        def save(self, key, state_data):
            pass

        def cleanup(self):
            pass

    """

    def __init__(self, persistence_impl, buffer):
        self.enabled            = True
        self._buffer            = buffer
        self._impl              = persistence_impl
        self.metadata_key       = self._impl.build_key(
                                    runstate.MCP_STATE, StateMetadata.name)

    def restore(self, job_names, service_names, skip_validation=False):
        """Return the most recent serialized state."""
        log.debug("Restoring state.")
        if not skip_validation:
            self._restore_metadata()

        return (self._restore_dicts(runstate.JOB_STATE, job_names),
                self._restore_dicts(runstate.SERVICE_STATE, service_names))

    def _restore_metadata(self):
        metadata = self._impl.restore([self.metadata_key])
        StateMetadata.validate_metadata(metadata.get(self.metadata_key))

    def _keys_for_items(self, item_type, names):
        """Returns a dict of item to the key for that item."""
        keys = (self._impl.build_key(item_type, name) for name in names)
        return dict(itertools.izip(keys, names))

    def _restore_dicts(self, item_type, items):
        """Return a dict mapping of the items name to its state data."""
        key_to_item_map  = self._keys_for_items(item_type, items)
        key_to_state_map = self._impl.restore(key_to_item_map.keys())
        return dict((key_to_item_map[key], state_data)
                    for key, state_data in key_to_state_map.iteritems())

    def save(self, type_enum, name, state_data):
        """Persist an items state."""
        key = self._impl.build_key(type_enum, name)
        log.info("Buffering state save for: %s", key)
        if self._buffer.save(key, state_data) and self.enabled:
            self._save_from_buffer()

    def _save_from_buffer(self):
        key_state_pairs = list(self._buffer)
        if not key_state_pairs:
            return

        keys = ','.join(str(key) for key, _ in key_state_pairs)
        log.info("Saving state for %s" % keys)

        with self._timeit():
            try:
                self._impl.save(key_state_pairs)
            except Exception, e:
                msg = "Failed to save state for %s: %s" % (keys, e)
                log.warn(msg)
                raise PersistenceStoreError(msg)

    def cleanup(self):
        self._save_from_buffer()
        self._impl.cleanup()

    @contextmanager
    def _timeit(self):
        """Log the time spent saving the state."""
        start_time = time.time()
        yield
        duration = time.time() - start_time
        log.info("State saved using %s in %0.3fs." % (self._impl, duration))

    @contextmanager
    def disabled(self):
        """Temporarily disable the state manager."""
        self.enabled, prev_enabled = False, self.enabled
        try:
            yield
        finally:
            self.enabled = prev_enabled


class NullStateManager(object):
    enabled = False

    @staticmethod
    def cleanup():
        pass

    @classmethod
    def disabled(cls):
        return cls()

    def __enter__(self):
        return

    def __exit__(self, *args):
        return


class StateChangeWatcher(observer.Observer):
    """Observer of stateful objects."""

    def __init__(self):
        self.state_manager = NullStateManager
        self.config        = None

    def update_from_config(self, state_config):
        if self.config == state_config:
            return False

        self.shutdown()
        self.state_manager = PersistenceManagerFactory.from_config(state_config)
        self.config = state_config
        return True

    def handler(self, observable, _event):
        """Handle a state change in an observable by saving its state."""
        if isinstance(observable, job.Job):
            self.save_job(observable)
        if isinstance(observable, service.Service):
            self.save_service(observable)

    def save_job(self, job):
        self._save_object(runstate.JOB_STATE, job)

    def save_service(self, service):
        self._save_object(runstate.SERVICE_STATE, service)

    def save_metadata(self):
        self._save_object(runstate.MCP_STATE, StateMetadata())

    def _save_object(self, state_type, obj):
        self.state_manager.save(state_type, obj.name, obj.state_data)

    def shutdown(self):
        self.state_manager.enabled = False
        self.state_manager.cleanup()

    def disabled(self):
        return self.state_manager.disabled()

    def restore(self, jobs, services):
        return self.state_manager.restore(jobs, services)

########NEW FILE########
__FILENAME__ = yamlstore
"""Store state in a local YAML file.

WARNING: Using this store is NOT recommended.  It will be far too slow for
anything but the most trivial setups.  It should only be used with a high
buffer size (10+), and a low run_limit (< 10).
"""
from collections import namedtuple
import itertools
import operator
import os
from tron.serialize import runstate

yaml = None # For pyflakes

YamlKey = namedtuple('YamlKey', ['type', 'iden'])

TYPE_MAPPING = {
    runstate.JOB_STATE:     'jobs',
    runstate.SERVICE_STATE: 'services',
    runstate.MCP_STATE:     runstate.MCP_STATE
}

class YamlStateStore(object):

    def __init__(self, filename):
        # Differ import of yaml until class is instantiated
        import yaml
        global yaml
        assert yaml
        self.filename           = filename
        self.buffer             = {}

    def build_key(self, type, iden):
        return YamlKey(TYPE_MAPPING[type], iden)

    def restore(self, keys):
        if not os.path.exists(self.filename):
            return {}

        with open(self.filename, 'r') as fh:
            self.buffer = yaml.load(fh)

        items = (self.buffer.get(key.type, {}).get(key.iden) for key in keys)
        key_item_pairs = itertools.izip(keys, items)
        return dict(itertools.ifilter(operator.itemgetter(1), key_item_pairs))

    def save(self, key_value_pairs):
        for key, state_data in key_value_pairs:
            self.buffer.setdefault(key.type, {})[key.iden] = state_data
        self._write_buffer()

    def _write_buffer(self):
        with open(self.filename, 'w') as fh:
            yaml.dump(self.buffer, fh)

    def cleanup(self):
        pass

    def __repr__(self):
        return "YamlStateStore('%s')" % self.filename

########NEW FILE########
__FILENAME__ = ssh
import struct
import logging

from twisted.internet import defer
from twisted.conch.ssh import channel, common, keys
from twisted.conch.ssh import connection
from twisted.conch.ssh import transport
from twisted.conch.client import default
from twisted.python import failure

log = logging.getLogger('tron.ssh')


class Error(Exception):
    pass


class ChannelClosedEarlyError(Error):
    """Indicates the SSH Channel has closed before we were done handling the
    command"""
    pass


class SSHAuthOptions(object):
    """An options class which can be used by NoPasswordAuthClient. This supports
    the interface provided by: twisted.conch.client.options.ConchOptions.
    """
    def __init__(self, identitys, use_agent):
        self.use_agent = use_agent
        self.identitys = identitys

    @classmethod
    def from_config(cls, ssh_config):
        return cls(ssh_config.identities, ssh_config.agent)

    def __getitem__(self, item):
        if item != 'noagent':
            raise KeyError(item)
        return not self.use_agent

    def __eq__(self, other):
        return other and (
            self.use_agent == other.use_agent and
            self.identitys == other.identitys)

    def __ne__(self, other):
        return not self == other

    def __str__(self):
        context = self.__class__.__name__, self.identitys, self.use_agent
        return "%s(%s, %s)" % context


class NoPasswordAuthClient(default.SSHUserAuthClient):
    """Only support passwordless auth."""
    preferredOrder              = ['publickey']
    auth_password               = None
    auth_keyboard_interactive   = None


class ClientTransport(transport.SSHClientTransport):

    connection_defer = None

    def __init__(self, username, options, expected_pub_key):
        self.username         = username
        self.options          = options
        self.expected_pub_key = expected_pub_key

    def verifyHostKey(self, public_key, fingerprint):
        if not self.expected_pub_key:
            return defer.succeed(1)

        if self.expected_pub_key == keys.Key.fromString(public_key):
            return defer.succeed(2)

        msg = "Public key mismatch got %s expected %s" % (
            fingerprint, self.expected_pub_key.fingerprint())
        log.error(msg)
        return defer.fail(ValueError(msg))

    def connectionSecure(self):
        conn = ClientConnection()
        # TODO: this should be initialized by the ClientConnection constructor
        conn.service_defer = defer.Deferred()
        # TODO: this should be initialized by the constructor
        self.connection_defer.callback(conn)

        auth_service = NoPasswordAuthClient(self.username, self.options, conn)
        self.requestService(auth_service)


class ClientConnection(connection.SSHConnection):

    service_start_defer = None
    service_stop_defer = None

    def serviceStarted(self):
        log.info("Service started")
        connection.SSHConnection.serviceStarted(self)
        if not self.service_stop_defer.called:
            self.service_start_defer.callback(self)

    def serviceStopped(self):
        log.info("Service stopped")
        connection.SSHConnection.serviceStopped(self)
        if not self.service_stop_defer.called:
            self.service_stop_defer.callback(self)

    def channelClosed(self, channel):
        if not channel.conn:
            log.warning("Channel %r failed to open", channel.id)
            # Channel has no connection, so we were still trying to open it The
            # normal error handling won't notify us since the channel never
            # successfully opened.
            channel.openFailed(None)

        connection.SSHConnection.channelClosed(self, channel)

    def ssh_CHANNEL_REQUEST(self, packet):
        """
        The other side is sending a request to a channel.  Payload::
            uint32  local channel number
            string  request name
            bool    want reply
            <request specific data>

        Handles missing local channel.
        """
        localChannel = struct.unpack('>L', packet[: 4])[0]
        if localChannel not in self.channels:
            requestType, _ = common.getNS(packet[4:])
            host = self.transport.transport.getHost()
            msg = "Missing channel: %s, request_type: %s, host: %s"
            log.warn(msg, localChannel, requestType, host)
            return
        connection.SSHConnection.ssh_CHANNEL_REQUEST(self, packet)

class ExecChannel(channel.SSHChannel):

    name = 'session'
    exit_defer = None
    start_defer = None

    command = None
    exit_status = None
    running = False

    def __init__(self, *args, **kwargs):
        channel.SSHChannel.__init__(self, *args, **kwargs)
        self.output_callbacks = []
        self.end_callbacks = []
        self.error_callbacks = []
        self.data = []

    def channelOpen(self, data):
        self.data = []
        self.running = True

        if self.start_defer:
            log.debug("Channel %s is open, calling deferred", self.id)
            self.start_defer.callback(self)

            # Unicode commands will cause the connection to fail
            self.command = str(self.command)

            req = self.conn.sendRequest(self, 'exec',
                                        common.NS(self.command),
                                        wantReply=True)
            req.addCallback(self._cbExecSendRequest)
        else:
            # A missing start defer means that we are no longer expected to do
            # anything when the channel opens It probably means we gave up on
            # this connection and failed the job, but later the channel opened
            # up correctly.
            log.warning("Channel open delayed, giving up and closing")
            self.loseConnection()

    def addOutputCallback(self, output_callback):
        self.output_callbacks.append(output_callback)

    def addErrorCallback(self, error_callback):
        self.error_callbacks.append(error_callback)

    def addEndCallback(self, end_callback):
        self.end_callbacks.append(end_callback)

    def openFailed(self, reason):
        log.error("Open failed due to %r", reason)
        if self.start_defer:
            self.start_defer.errback(self)

    def _cbExecSendRequest(self, ignored):
        self.conn.sendEOF(self)

    def request_exit_status(self, data):
        # exit status is a 32-bit unsigned int in network byte format
        status = struct.unpack_from(">L", data, 0)[0]

        log.debug("Received exit status request: %d", status)
        self.exit_status = status
        self.exit_defer.callback(self)
        self.running = False
        return True

    def dataReceived(self, data):
        self.data.append(data)
        for callback in self.output_callbacks:
            callback(data)

    def extReceived(self, dataType, data):
        self.data.append(data)
        for callback in self.error_callbacks:
            callback(data)

    def getStdout(self):
        return "".join(self.data)

    def closed(self):
        if (self.exit_status is None and
            self.running and
            self.exit_defer and
            not self.exit_defer.called):
            log.warning("Channel has been closed without receiving an exit"
                        " status")
            f = failure.Failure(exc_value=ChannelClosedEarlyError())
            self.exit_defer.errback(f)

        for callback in self.end_callbacks:
            callback()
        # TODO: this is triggered by loseConnection, we shouldn't need to call it
        # again here
        self.loseConnection()

########NEW FILE########
__FILENAME__ = trondaemon
"""
 Daemonize trond.
"""
import logging
import logging.config
import os
import pkg_resources
import daemon

from twisted.internet import pollreactor, defer
from twisted.internet.main import installReactor

import lockfile
from tron.utils import flockfile
import signal
from twisted.python import log as twisted_log
import tron


log = logging.getLogger(__name__)


class PIDFile(object):
    """Create and check for a PID file for the daemon."""

    def __init__(self, filename):
        self.lock = flockfile.FlockFile(filename)
        self.check_if_pidfile_exists()

    @property
    def filename(self):
        return self.lock.path

    def check_if_pidfile_exists(self):
        self.lock.acquire()

        try:
            with open(self.filename, 'r') as fh:
                pid = int(fh.read().strip())
        except (IOError, ValueError):
            pid = None

        if self.is_process_running(pid):
            self._try_unlock()
            raise SystemExit("Daemon running as %s" % pid)

        if pid:
            self._try_unlock()
            raise SystemExit("Daemon was running as %s. Remove PID file." % pid)

    def is_process_running(self, pid):
        """Return True if the process is still running."""
        if not pid:
            return False
        try:
            os.kill(pid, 0)
            return True
        except OSError:
            return False

    def __enter__(self):
        print >>self.lock.file, os.getpid()
        self.lock.file.flush()

    def _try_unlock(self):
        try:
            self.lock.release()
        except lockfile.NotLocked:
            log.warn("Lockfile was already unlocked.")

    def __exit__(self, *args):
        self._try_unlock()
        try:
            os.unlink(self.filename)
        except OSError:
            log.warn("Failed to remove pidfile: %s" % self.filename)


def setup_logging(options):
    default = pkg_resources.resource_filename(tron.__name__, 'logging.conf')
    logfile = options.log_conf or default

    level = twist_level = None
    if options.verbose > 0:
        level = logging.INFO
        twist_level = logging.WARNING
    if options.verbose > 1:
        level = logging.DEBUG
        twist_level = logging.INFO
    if options.verbose > 2:
        twist_level = logging.DEBUG

    tron_logger = logging.getLogger('tron')
    twisted_logger = logging.getLogger('twisted')

    logging.config.fileConfig(logfile)
    if level is not None:
        tron_logger.setLevel(level)
    if twist_level is not None:
        twisted_logger.setLevel(twist_level)

    # Hookup twisted to standard logging
    twisted_log.PythonLoggingObserver().start()

    # Show stack traces for errors in twisted deferreds.
    if options.debug:
        defer.setDebugging(True)


class NoDaemonContext(object):
    """A mock DaemonContext for running trond without being a daemon."""

    def __init__(self, **kwargs):
        self.signal_map     = kwargs.pop('signal_map', {})
        self.pidfile        = kwargs.pop('pidfile', None)
        self.working_dir    = kwargs.pop('working_directory', '.')
        self.signal_map[signal.SIGUSR1] = self._handle_debug

    def _handle_debug(self, *args):
        import ipdb
        ipdb.set_trace()

    def __enter__(self):
        for signum, handler in self.signal_map.iteritems():
            signal.signal(signum, handler)

        os.chdir(self.working_dir)
        if self.pidfile:
            self.pidfile.__enter__()

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.pidfile:
            self.pidfile.__exit__(exc_type, exc_val, exc_tb)

    def terminate(self, *args):
        pass

class TronDaemon(object):
    """Daemonize and run the tron daemon."""

    WAIT_SECONDS = 5

    def __init__(self, options):
        self.options    = options
        self.mcp        = None
        nodaemon        = self.options.nodaemon
        context_class   = NoDaemonContext if nodaemon else daemon.DaemonContext
        self.context    = self._build_context(options, context_class)
        self.reactor    = None

    def _build_context(self, options, context_class):
        signal_map = {
            signal.SIGHUP:  self._handle_reconfigure,
            signal.SIGINT:  self._handle_graceful_shutdown,
            signal.SIGTERM: self._handle_shutdown,
        }
        pidfile = PIDFile(options.pid_file)
        return context_class(
            working_directory=options.working_dir,
            umask=0o022,
            pidfile=pidfile,
            signal_map=signal_map,
            files_preserve=[pidfile.lock.file]
        )

    def run(self):
        with self.context:
            self.setup_reactor()
            setup_logging(self.options)
            self._run_mcp()
            self._run_www_api()
            self._run_reactor()


    def setup_reactor(self):
        self.reactor = pollreactor.PollReactor()
        installReactor(self.reactor)

    def _run_www_api(self):
        # Local import required because of reactor import in server and www
        from tron.api import resource
        site = resource.TronSite.create(self.mcp, self.options.web_path)
        port = self.options.listen_port
        self.reactor.listenTCP(port, site, interface=self.options.listen_host)

    def _run_mcp(self):
        # Local import required because of reactor import in mcp
        from tron import mcp
        working_dir         = self.options.working_dir
        config_path         = self.options.config_path
        self.mcp            = mcp.MasterControlProgram(working_dir, config_path)

        try:
            self.mcp.initial_setup()
        except Exception, e:
            msg = "Error in configuration %s: %s"
            log.exception(msg % (config_path, e))
            raise SystemExit("Failed to configure MCP")

    def _run_reactor(self):
        """Run the twisted reactor."""
        self.reactor.run()

    def _handle_shutdown(self, sig_num, stack_frame):
        log.info("Shutdown requested: sig %s" % sig_num)
        if self.mcp:
            self.mcp.shutdown()
        self.reactor.stop()
        self.context.terminate(sig_num, stack_frame)

    def _handle_graceful_shutdown(self, sig_num, stack_frame):
        """Gracefully shutdown by waiting for Jobs to finish."""
        log.info("Graceful Shutdown requested: sig %s" % sig_num)
        if not self.mcp:
            self._handle_shutdown(sig_num, stack_frame)
            return
        self.mcp.graceful_shutdown()
        self._wait_for_jobs()

    def _wait_for_jobs(self):
        if self.mcp.jobs.is_shutdown:
            self._handle_shutdown(None, None)
            return

        log.info("Waiting for jobs to shutdown.")
        self.reactor.callLater(self.WAIT_SECONDS, self._wait_for_jobs)

    def _handle_reconfigure(self, _signal_number, _stack_frame):
        log.info("Reconfigure requested by SIGHUP.")
        self.reactor.callLater(0, self.mcp.reconfigure)

########NEW FILE########
__FILENAME__ = collections
"""Utilities for working with collections."""
import logging


log = logging.getLogger(__name__)


class MappingCollection(dict):
    """Dictionary like object for managing collections of items. Item is
    expected to support the following interface, and should be hashable.

    class Item(object):

        def get_name(self): ...

        def restore_state(self, state_data): ...

        def disable(self): ...

        def __eq__(self, other): ...

    """

    def __init__(self, item_name):
        dict.__init__(self)
        self.item_name = item_name

    def filter_by_name(self, names):
        for name in set(self) - set(names):
            self.remove(name)

    def remove(self, name):
        if name not in self:
            raise ValueError("%s %s unknown" % (self.item_name, name))

        log.info("Removing %s %s", self.item_name, name)
        self.pop(name).disable()

    def restore_state(self, state_data):
        for name, state in state_data.iteritems():
            self[name].restore_state(state)
        log.info("Loaded state for %d %s", len(state_data), self.item_name)

    def contains_item(self, item, handle_update_func):
        if item == self.get(item.get_name()):
            return True

        return handle_update_func(item) if item.get_name() in self else False

    def add(self, item, update_func):
        if self.contains_item(item, update_func):
            return False

        log.info("Adding new %s" % item)
        self[item.get_name()] = item
        return True

    def replace(self, item):
        return self.add(item, self.remove_item)

    def remove_item(self, item):
        return self.remove(item.get_name())


class Enum(object):
    """Enumeration of values."""

    def __init__(self, values):
        self.values = set(values)

    @classmethod
    def create(cls, *values):
        return cls(values)

    def __contains__(self, item):
        return item in self.values

    def __getattr__(self, name):
        if name in self.values:
            return name
        raise AttributeError(name)

    def __iter__(self):
        return iter(self.values)
########NEW FILE########
__FILENAME__ = crontab
"""Parse a crontab entry and return a dictionary."""

import calendar
import itertools
import re


PREDEFINED_SCHEDULE = {
    "@yearly":  "0 0 1 1 *",
    "@anually": "0 0 1 1 *",
    "@monthly": "0 0 1 * *",
    "@weekly":  "0 0 * * 0",
    "@daily":   "0 0 * * *",
    "@midnight":"0 0 * * *",
    "@hourly":  "0 * * * *"}


def convert_predefined(line):
    if not line.startswith('@'):
        return line

    if line not in PREDEFINED_SCHEDULE:
        raise ValueError("Unknown predefine: %s" % line)
    return PREDEFINED_SCHEDULE[line]


class FieldParser(object):
    """Parse and validate a field in a crontab entry."""

    name   = None
    bounds = None
    range_pattern = re.compile(r'''
        (?P<min>\d+|\*)         # Initial value
        (?:-(?P<max>\d+))?      # Optional max upper bound
        (?:/(?P<step>\d+))?     # Optional step increment
        ''', re.VERBOSE)

    def normalize(self, source):
        return source.strip()

    def get_groups(self, source):
        return source.split(',')

    def parse(self, source):
        if source == '*':
            return None

        groups  = [self.get_values(group) for group in self.get_groups(source)]
        return sorted(set(itertools.chain.from_iterable(groups)))

    def get_match_groups(self, source):
        match = self.range_pattern.match(source)
        if not match:
            raise ValueError("Unknown expression: %s" % source)
        return match.groupdict()

    def get_values(self, source):
        source               = self.normalize(source)
        match_groups         = self.get_match_groups(source)
        step                 = 1
        min_value, max_value = self.get_value_range(match_groups)

        if match_groups['step']:
            step = self.validate_bounds(match_groups['step'])
        return self.get_range(min_value, max_value, step)

    def get_value_range(self, match_groups):
        if match_groups['min'] == '*':
            return self.bounds

        min_value = self.validate_bounds(match_groups['min'])
        if match_groups['max']:
            # Cron expressions are inclusive, range is exclusive on upper bound
            max_value = self.validate_bounds(match_groups['max']) + 1
            return min_value, max_value

        return min_value, min_value + 1

    def get_range(self, min_value, max_value, step):
        if min_value < max_value:
            return range(min_value, max_value, step)

        min_bound, max_bound = self.bounds
        diff = (max_bound - min_value) + (max_value - min_bound)
        return [(min_value + i) % max_bound for i in xrange(0, diff, step)]

    def validate_bounds(self, value):
        min_value, max_value = self.bounds
        value = int(value)
        if not min_value <= value < max_value:
            raise ValueError("%s value out of range: %s" % (self.name, value))
        return value


class MinuteFieldParser(FieldParser):
    name    = 'minutes'
    bounds  = (0, 60)

class HourFieldParser(FieldParser):
    name    = 'hours'
    bounds  = (0, 24)

class MonthdayFieldParser(FieldParser):
    name    = 'monthdays'
    bounds  = (1, 32)

    def get_values(self, source):
        # Handle special case for last day of month
        source = self.normalize(source)
        if source == 'L':
            return ['LAST']

        return super(MonthdayFieldParser, self).get_values(source)

class MonthFieldParser(FieldParser):
    name        = 'months'
    bounds      = (1, 13)
    month_names = calendar.month_abbr[1:]

    def normalize(self, month):
        month = super(MonthFieldParser, self).normalize(month)
        month = month.lower()
        for month_num, month_name in  enumerate(self.month_names, start=1):
            month = month.replace(month_name.lower(), str(month_num))
        return month

class WeekdayFieldParser(FieldParser):
    name        = 'weekdays'
    bounds      = (0, 7)
    day_names   = ['sun', 'mon', 'tue', 'wed', 'thu', 'fri', 'sat']

    def normalize(self, day_of_week):
        day_of_week = super(WeekdayFieldParser, self).normalize(day_of_week)
        day_of_week = day_of_week.lower()
        for dow_num, dow_name in enumerate(self.day_names):
            day_of_week = day_of_week.replace(dow_name, str(dow_num))
        return day_of_week.replace('7', '0').replace('?', '*')


minute_parser   = MinuteFieldParser()
hour_parser     = HourFieldParser()
monthday_parser = MonthdayFieldParser()
month_parser    = MonthdayFieldParser()
weekday_parser  = WeekdayFieldParser()


# TODO: support L (for dow), W, #
def parse_crontab(line):
    line = convert_predefined(line)
    minutes, hours, dom, months, dow = line.split(None, 4)

    return {
        'minutes':      minute_parser.parse(minutes),
        'hours':        hour_parser.parse(hours),
        'monthdays':    monthday_parser.parse(dom),
        'months':       month_parser.parse(months),
        'weekdays':     weekday_parser.parse(dow),
        'ordinals':     None}

########NEW FILE########
__FILENAME__ = dicts
"""Data structures used in tron."""
from __future__ import absolute_import
from collections import Mapping
import itertools


def invert_dict_list(dictionary):
    """Invert a dictionary of lists. All values in the lists should be unique.
    """
    def invert(key, seq):
        for item in seq:
            yield item, key

    seq = (invert(k, v) for k, v in dictionary.iteritems())
    return dict(itertools.chain.from_iterable(seq))

class FrozenDict(Mapping):
    """Simple implementation of an immutable dictionary so we can freeze the
    command context, set of jobs/services, actions, etc.

    from http://stackoverflow.com/questions/2703599/what-would-be-a-frozen-dict
    """

    def __init__(self, *args, **kwargs):
        if hasattr(self, '_d'):
            raise Exception("Can't call __init__ twice")
        self._d = dict(*args, **kwargs)
        self._hash = None

    def __repr__(self):
        return 'FrozenDict(%r)' % self._d

    def __iter__(self):
        return iter(self._d)

    def __len__(self):
        return len(self._d)

    def __getitem__(self, key):
        return self._d[key]

    def __hash__(self):
        # It would have been simpler and maybe more obvious to
        # use hash(tuple(sorted(self._d.iteritems()))) from this discussion
        # so far, but this solution is O(n). I don't know what kind of
        # n we are going to run into, but sometimes it's hard to resist the
        # urge to optimize when it will gain improved algorithmic performance.
        if self._hash is None:
            self._hash = 0
            for key, value in self.iteritems():
                self._hash ^= hash(key)
                self._hash ^= hash(value)
        return self._hash


## http://code.activestate.com/recipes/576693/ (r9)
# Backport of OrderedDict() class that runs on Python 2.4, 2.5, 2.6, 2.7 and pypy.
# Passes Python2.7's test suite and incorporates all the latest updates.

try:
    from thread import get_ident as _get_ident
    assert _get_ident # pyflakes
except ImportError:
    from dummy_thread import get_ident as _get_ident

try:
    from _abcoll import KeysView, ValuesView, ItemsView
except ImportError:
    pass


class OrderedDict(dict):
    """Dictionary that remembers insertion order"""
    # An inherited dict maps keys to values.
    # The inherited dict provides __getitem__, __len__, __contains__, and get.
    # The remaining methods are order-aware.
    # Big-O running times for all methods are the same as for regular dictionaries.

    # The internal self.__map dictionary maps keys to links in a doubly linked list.
    # The circular doubly linked list starts and ends with a sentinel element.
    # The sentinel element never gets deleted (this simplifies the algorithm).
    # Each link is stored as a list of length three:  [PREV, NEXT, KEY].

    def __init__(self, *args, **kwds):
        '''Initialize an ordered dictionary.  Signature is the same as for
        regular dictionaries, but keyword arguments are not recommended
        because their insertion order is arbitrary.

        '''
        if len(args) > 1:
            raise TypeError('expected at most 1 arguments, got %d' % len(args))
        try:
            self.__root
        except AttributeError:
            self.__root = root = []                     # sentinel node
            root[:] = [root, root, None]
            self.__map = {}
        self.__update(*args, **kwds)

    def __setitem__(self, key, value, dict_setitem=dict.__setitem__):
        'od.__setitem__(i, y) <==> od[i]=y'
        # Setting a new item creates a new link which goes at the end of the linked
        # list, and the inherited dictionary is updated with the new key/value pair.
        if key not in self:
            root = self.__root
            last = root[0]
            last[1] = root[0] = self.__map[key] = [last, root, key]
        dict_setitem(self, key, value)

    def __delitem__(self, key, dict_delitem=dict.__delitem__):
        'od.__delitem__(y) <==> del od[y]'
        # Deleting an existing item uses self.__map to find the link which is
        # then removed by updating the links in the predecessor and successor nodes.
        dict_delitem(self, key)
        link_prev, link_next, key = self.__map.pop(key)
        link_prev[1] = link_next
        link_next[0] = link_prev

    def __iter__(self):
        'od.__iter__() <==> iter(od)'
        root = self.__root
        curr = root[1]
        while curr is not root:
            yield curr[2]
            curr = curr[1]

    def __reversed__(self):
        'od.__reversed__() <==> reversed(od)'
        root = self.__root
        curr = root[0]
        while curr is not root:
            yield curr[2]
            curr = curr[0]

    def clear(self):
        'od.clear() -> None.  Remove all items from od.'
        try:
            for node in self.__map.itervalues():
                del node[:]
            root = self.__root
            root[:] = [root, root, None]
            self.__map.clear()
        except AttributeError:
            pass
        dict.clear(self)

    def popitem(self, last=True):
        '''od.popitem() -> (k, v), return and remove a (key, value) pair.
        Pairs are returned in LIFO order if last is true or FIFO order if false.

        '''
        if not self:
            raise KeyError('dictionary is empty')
        root = self.__root
        if last:
            link = root[0]
            link_prev = link[0]
            link_prev[1] = root
            root[0] = link_prev
        else:
            link = root[1]
            link_next = link[1]
            root[1] = link_next
            link_next[0] = root
        key = link[2]
        del self.__map[key]
        value = dict.pop(self, key)
        return key, value

    # -- the following methods do not depend on the internal structure --

    def keys(self):
        'od.keys() -> list of keys in od'
        return list(self)

    def values(self):
        'od.values() -> list of values in od'
        return [self[key] for key in self]

    def items(self):
        'od.items() -> list of (key, value) pairs in od'
        return [(key, self[key]) for key in self]

    def iterkeys(self):
        'od.iterkeys() -> an iterator over the keys in od'
        return iter(self)

    def itervalues(self):
        'od.itervalues -> an iterator over the values in od'
        for k in self:
            yield self[k]

    def iteritems(self):
        'od.iteritems -> an iterator over the (key, value) items in od'
        for k in self:
            yield (k, self[k])

    def update(*args, **kwds):
        '''od.update(E, **F) -> None.  Update od from dict/iterable E and F.

        If E is a dict instance, does:           for k in E: od[k] = E[k]
        If E has a .keys() method, does:         for k in E.keys(): od[k] = E[k]
        Or if E is an iterable of items, does:   for k, v in E: od[k] = v
        In either case, this is followed by:     for k, v in F.items(): od[k] = v

        '''
        if len(args) > 2:
            raise TypeError('update() takes at most 2 positional '
                            'arguments (%d given)' % (len(args),))
        elif not args:
            raise TypeError('update() takes at least 1 argument (0 given)')
        self = args[0]
        # Make progressively weaker assumptions about "other"
        other = ()
        if len(args) == 2:
            other = args[1]
        if isinstance(other, dict):
            for key in other:
                self[key] = other[key]
        elif hasattr(other, 'keys'):
            for key in other.keys():
                self[key] = other[key]
        else:
            for key, value in other:
                self[key] = value
        for key, value in kwds.items():
            self[key] = value

    __update = update  # let subclasses override update without breaking __init__

    __marker = object()

    def pop(self, key, default=__marker):
        '''od.pop(k[,d]) -> v, remove specified key and return the corresponding value.
        If key is not found, d is returned if given, otherwise KeyError is raised.

        '''
        if key in self:
            result = self[key]
            del self[key]
            return result
        if default is self.__marker:
            raise KeyError(key)
        return default

    def setdefault(self, key, default=None):
        'od.setdefault(k[,d]) -> od.get(k,d), also set od[k]=d if k not in od'
        if key in self:
            return self[key]
        self[key] = default
        return default

    def __repr__(self, _repr_running={}):
        'od.__repr__() <==> repr(od)'
        call_key = id(self), _get_ident()
        if call_key in _repr_running:
            return '...'
        _repr_running[call_key] = 1
        try:
            if not self:
                return '%s()' % (self.__class__.__name__,)
            return '%s(%r)' % (self.__class__.__name__, self.items())
        finally:
            del _repr_running[call_key]

    def __reduce__(self):
        'Return state information for pickling'
        items = [[k, self[k]] for k in self]
        inst_dict = vars(self).copy()
        for k in vars(OrderedDict()):
            inst_dict.pop(k, None)
        if inst_dict:
            return (self.__class__, (items,), inst_dict)
        return self.__class__, (items,)

    def copy(self):
        'od.copy() -> a shallow copy of od'
        return self.__class__(self)

    @classmethod
    def fromkeys(cls, iterable, value=None):
        '''OD.fromkeys(S[, v]) -> New ordered dictionary with keys from S
        and values equal to v (which defaults to None).

        '''
        d = cls()
        for key in iterable:
            d[key] = value
        return d

    def __eq__(self, other):
        '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive
        while comparison to a regular mapping is order-insensitive.

        '''
        if isinstance(other, OrderedDict):
            return len(self)==len(other) and self.items() == other.items()
        return dict.__eq__(self, other)

    def __ne__(self, other):
        return not self == other

    # -- the following methods are only used in Python 2.7 --

    def viewkeys(self):
        "od.viewkeys() -> a set-like object providing a view on od's keys"
        return KeysView(self)

    def viewvalues(self):
        "od.viewvalues() -> an object providing a view on od's values"
        return ValuesView(self)

    def viewitems(self):
        "od.viewitems() -> a set-like object providing a view on od's items"
        return ItemsView(self)
    ## end of http://code.activestate.com/recipes/576693/ }}}

########NEW FILE########
__FILENAME__ = emailer
"""General email sending utilities"""

import logging
import smtplib
from email.mime.text import MIMEText
import getpass
import socket

log = logging.getLogger("tron.emailer")


class Error(Exception):
    pass


class Emailer(object):
    def __init__(self, smtp_host, notification_address):
        self.smtp_host = smtp_host
        self.to_addr = notification_address

    @property
    def from_addr(self):
        # TODO: Should probably allow this to be configured
        username = getpass.getuser()
        hostname = socket.gethostname()
        return "@".join((username, hostname))

    def send(self, content):
        msg = MIMEText(content)
        msg['Subject'] = "Tron Exception"
        msg['To'] = self.to_addr

        port = 25
        host_parts = self.smtp_host.split(":")
        host = host_parts.pop(0)
        if host_parts:
            port = int(host_parts.pop(0))
        if host_parts:
            raise Error("Invalid host name")

        log.info("Connecting to SMTP host %r %r", self.smtp_host, (host, port))

        s = smtplib.SMTP()
        s.connect(host, port)
        s.sendmail(self.from_addr, self.to_addr, msg.as_string())
        s.close()

########NEW FILE########
__FILENAME__ = flockfile
import fcntl
import lockfile
import logging
import os

log = logging.getLogger(__name__)

class FlockFile(object):
    """
    A lockfile (matching the specification of the builtin lockfile class)
    based off of flock. Single lockfile per process (no thread support)..
    """
    def __init__(self, path):
        self.path       = path
        self.lock_file  = None
        try:
            self.lock_file = open(self.path, 'a')
        except (IOError, OSError), e:
            raise lockfile.LockFailed(e, self.path)
        self._has_lock = False

    @property
    def file(self):
        """Get a handle to the underlying lock file (to write out data to)"""
        return self.lock_file

    def acquire(self):
        log.debug("Locking %s", self.path)
        try:
            fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX|fcntl.LOCK_NB)
            self._has_lock = True
        except IOError, e:
            raise lockfile.AlreadyLocked(e, self.path)
        log.debug("Locked %s", self.path)

    def break_lock(self):
        """Can't break posix locks, sorry man"""
        raise lockfile.LockError()

    @property
    def i_am_locking(self):
        return self._has_lock

    def is_locked(self):
        if self._has_lock:
            return True
        try:
            fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_EX|fcntl.LOCK_NB)
            fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)
            return False
        except IOError:
            return True

    def release(self):
        log.debug("Releasing lock on %s", self.path)
        if self.i_am_locking:
            fcntl.flock(self.lock_file.fileno(), fcntl.LOCK_UN)
            self._has_lock = False
        else:
            raise lockfile.NotLocked(self.path)
        log.debug("Unlocked %s", self.path)

    def destroy(self):
        try:
            if self.i_am_locking:
                self.release()
            self.lock_file.close()
        finally:
            if os.path.exists(self.path):
                os.unlink(self.path)

    def __enter__(self):
        self.acquire()
        return self

    def __exit__(self, *args):
        self.release()

########NEW FILE########
__FILENAME__ = iteration
"""Iteration utility functions."""


def build_filtered_func(func):
    def filtered_func(seq):
        """Filter out Nones and return the return of func."""
        if not seq:
            return None
        seq = filter(None, seq)
        if not seq:
            return None
        return func(seq)
    return filtered_func

min_filter = build_filtered_func(min)
max_filter = build_filtered_func(max)


def list_all(seq):
    """Create a list from the sequence then evaluate all the entries using
    all(). This differs from the built-in all() which will short circuit
    on the first False.
    """
    return all(list(seq))
########NEW FILE########
__FILENAME__ = observer
"""Implements the Observer/Observable pattern,"""
import logging

log = logging.getLogger(__name__)

class Observable(object):
    """An Observable in the Observer/Observable pattern. It stores
    specifications and Observers which can be notified of changes by calling
    notify.
    """

    def __init__(self):
        self._observers = dict()

    def attach(self, watch_spec, observer):
        """Attach another observer to the listen_spec.

        Listener Spec matches on:
            True                    Matches everything
            <string>                Matches only that event
            <sequence of strings>   Matches any of the events in the sequence
        """
        if isinstance(watch_spec, (basestring, bool)):
            self._observers.setdefault(watch_spec, []).append(observer)
            return

        for spec in watch_spec:
            self._observers.setdefault(spec, []).append(observer)

    def clear_observers(self, watch_spec=None):
        """Remove all observers for a given watch_spec. Removes all
        observers if listen_spec is None
        """
        if watch_spec is None or watch_spec is True:
            self._observers.clear()
            return

        del self._observers[watch_spec]

    def remove_observer(self, observer):
        """Remove an observer from all watch_specs."""
        for observers in self._observers.values():
            if observer in observers:
                observers.remove(observer)

    def _get_handlers_for_event(self, event):
        """Returns the complete list of handlers for the event."""
        return self._observers.get(True, []) + self._observers.get(event, [])

    def notify(self, event):
        """Notify all observers of the event."""
        log.debug("Notifying listeners for new event %r", event)
        for handler in self._get_handlers_for_event(event):
            handler.handler(self, event)


class Observer(object):
    """An observer in the Observer/Observable pattern.  Given an observable
    object will watch for notify calls.  Override handler to act on those
    notifications.
    """

    def watch(self, observable, event=True):
        """Adds this Observer as a watcher of the observable."""
        observable.attach(event, self)

    def watch_all(self, observables, event=True):
        for observable in observables:
            self.watch(observable, event)

    def handler(self, observable, event):
        """Override this method to call a method to handle events."""
        pass

    def stop_watching(self, observable):
        observable.remove_observer(self)
########NEW FILE########
__FILENAME__ = proxy
"""Utilities for creating classes that proxy function calls."""


class CollectionProxy(object):
    """Proxy attribute lookups to a sequence of objects."""

    def __init__(self, obj_list_getter, definition_list=None):
        """See add() for a description of proxy definitions."""
        self.obj_list_getter    = obj_list_getter
        self._defs              = {}
        for definition in definition_list or []:
            self.add(*definition)

    def add(self, attribute_name, aggregate_func, is_callable):
        """Add attributes to proxy, the aggregate function to use on the
        sequence of returned values, and a boolean identifying if this
        attribute is a callable or not.

            attribute_name - the name of the attribute to proxy
            aggregate_func - a function that takes a sequence as its only argument
            callable       - if this attribute is a callable on every object in
                             the obj_list (boolean)
        """
        self._defs[attribute_name] = (aggregate_func, is_callable)

    def perform(self, name):
        """Attempt to perform the proxied lookup.  Raises AttributeError if
        the name is not defined.
        """
        if name not in self._defs:
            raise AttributeError(name)

        obj_list = self.obj_list_getter
        aggregate_func, is_callable = self._defs[name]

        if not is_callable:
            return aggregate_func(getattr(i, name) for i in obj_list())

        def func(*args, **kwargs):
            return aggregate_func(
                getattr(item, name)(*args, **kwargs) for item in obj_list())
        return func


def func_proxy(name, func):
    return name, func, True

def attr_proxy(name, func):
    return name, func, False


class AttributeProxy(object):
    """Proxy attribute lookups to another object."""

    def __init__(self, dest_obj, attribute_list=None):
        self._attributes = set(attribute_list or [])
        self.dest_obj = dest_obj

    def add(self, attribute_name):
        self._attributes.add(attribute_name)

    def perform(self, attribute_name):
        if attribute_name not in self._attributes:
            raise AttributeError(attribute_name)

        return getattr(self.dest_obj, attribute_name)
########NEW FILE########
__FILENAME__ = state
import logging
from tron.utils.observer import Observable


class Error(Exception):
    pass


class InvalidRuleError(Error):
    pass


class CircularTransitionError(Error):
    pass


log = logging.getLogger(__name__)


class NamedEventState(dict):
    """A dict like object with a name that acts as a state. The dict stores
    valid transition actions and the destination state.
    """

    def __init__(self, name, short_name=None, short_chars=4, **kwargs):
        self.name = name
        self._short_name = short_name
        self._short_chars = short_chars
        super(NamedEventState, self).__init__(**kwargs)

    def __eq__(self, other):
        try:
            return self.name == other.name
        except AttributeError:
            return False

    def __hash__(self):
        return hash(self.name)

    def __nonzero__(self):
        return bool(self.name)

    def __str__(self):
        return self.name

    def __repr__(self):
        return "<%r %s>" % (self.__class__.__name__, self.name)

    @property
    def short_name(self):
        """If a short_name was given use that, otherwise return the first
        self._short_letters letters of the name in caps.
        """
        if self._short_name:
            return self._short_name
        return self.name[:self._short_chars].upper()


def traverse(starting_state, match_func):
    visited                 = set()
    state_pairs             = [(None, starting_state)]
    pair_with_name          = lambda p: (p[0], p[1].name)

    while state_pairs:
        transition_state_pair = state_pairs.pop()
        _, cur_state = transition_state_pair
        visited.add(pair_with_name(transition_state_pair))

        if match_func(*transition_state_pair):
            yield transition_state_pair

        for next_pair in cur_state.iteritems():
            if pair_with_name(next_pair) not in visited:
                state_pairs.append(next_pair)


def named_event_by_name(starting_state, name):
    name_match = lambda t, s: s.name == name
    try:
        _, state = traverse(starting_state, name_match).next()
        return state
    except StopIteration:
        raise ValueError("State %s not found." % name)


def get_transitions(starting_state):
    transition_match = lambda t, s: bool(t)
    return [trans for trans, _ in traverse(starting_state, transition_match)]


class StateMachine(Observable):
    """StateMachine is a class that can be used for managing state machines.

    A state machine is made up of a State() and a target. The target is the
    where all the input comes from for making decision about state changes,
    whatever that may be.

    A State is really just a fancy container for a set of rules for
    transitioning to other states based on the target.
    """

    def __init__(self, initial_state, delegate=None, force_state=None):
        super(StateMachine, self).__init__()
        self.initial_state = initial_state
        self.state = force_state or self.initial_state
        self._state_by_name = None
        self.delegate = delegate

    def check(self, target):
        """Check if the state can be transitioned to target. Returns the
        destination state if target is a valid state to transition to,
        None otherwise.
        """
        log.debug("Checking for transition from %s to %s", self.state, target)
        return self.state.get(target, None)

    @property
    def transitions(self):
        return get_transitions(self.initial_state)

    def transition(self, target, stop_item=None):
        """Check our current state for a transition based on the input 'target'

        Returns True or False based on whether a transition has indeed taken
        place.  Listeners for this change will also be notified before
        returning.
        """

        next_state = self.check(target)
        if next_state is None:
            return False

        prev_state = self.state
        log.debug("Transitioning from %s to %s", self.state, next_state)

        # Check if we are doing some circular transition.
        if stop_item is not None and next_state is stop_item:
            raise CircularTransitionError()

        self.state = next_state
        self.notify(self.state)

        # We always call recursively after a state change incase there are
        # multiple steps to take.
        self.transition(target, stop_item=(stop_item or prev_state))
        return True

    def notify(self, event):
        """Notify observers."""
        watched = self.delegate if self.delegate else self
        for handler in self._get_handlers_for_event(event):
            handler.handler(watched, event)

########NEW FILE########
__FILENAME__ = timeutils
"""Functions for working with dates and timestamps."""
from __future__ import division
import datetime
import re
import time


def current_time():
    """Return the current datetime."""
    return datetime.datetime.now()


def current_timestamp():
    """Return the current time as a timestamp."""
    return to_timestamp(current_time())


def to_timestamp(time_val):
    """Generate a unix timestamp for the given datetime instance"""
    return time.mktime(time_val.timetuple())


def delta_total_seconds(td):
    """Equivalent to timedelta.total_seconds() available in Python 2.7.
    """
    microseconds, seconds, days = td.microseconds, td.seconds, td.days
    return (microseconds + (seconds + days * 24 * 3600) * 10**6) / 10**6


def macro_timedelta(start_date, years=0, months=0, days=0):
    """Since datetime doesn't provide timedeltas at the year or month level,
    this function generates timedeltas of the appropriate sizes.
    """
    delta = datetime.timedelta(days=days)

    new_month = start_date.month + months
    while new_month > 12:
        new_month -= 12
        years += 1
    while new_month < 1:
        new_month += 12
        years -= 1

    end_date = datetime.datetime(
        start_date.year + years, new_month, start_date.day)
    delta += end_date - start_date

    return delta


def duration(start_time, end_time=None):
    """Get a timedelta between end_time and start_time, where end_time defaults
    to now().
    """
    if not start_time:
        return None
    last_time = end_time if end_time else current_time()
    return last_time - start_time


class DateArithmetic(object):
    """Parses a string which contains a date arithmetic pattern and returns
    a date with the delta added or subtracted.
    """

    DATE_TYPE_PATTERN = re.compile(r'(\w+)([+-]\d+)?')

    DATE_FORMATS = {
        'year':                 '%Y',
        'month':                '%m',
        'day':                  '%d',
        'shortdate':            '%Y-%m-%d'
    }

    @classmethod
    def parse(cls, date_str, dt=None):
        """Parse a date arithmetic pattern (Ex: 'shortdate-1'). Supports
        date strings: shortdate, year, month, day, unixtime, daynumber.
        Supports subtraction and addition operations of integers. Time unit is
        based on date format (Ex: seconds for unixtime, days for day).
        """
        dt = dt or current_time()
        match = cls.DATE_TYPE_PATTERN.match(date_str)
        if not match:
            return
        attr, value = match.groups()
        delta = int(value) if value else 0

        if attr in ('shortdate', 'year', 'month', 'day'):
            if delta:
                kwargs = {'days' if attr == 'shortdate' else attr + 's': delta}
                dt += macro_timedelta(dt, **kwargs)
            return dt.strftime(cls.DATE_FORMATS[attr])

        if attr == 'unixtime':
            return int(to_timestamp(dt)) + delta

        if attr == 'daynumber':
            return dt.toordinal() + delta
########NEW FILE########
__FILENAME__ = tool_utils
import contextlib
import os

@contextlib.contextmanager
def working_dir(path):
    """Change the working directory and revert back to previous working
    directory.

    WARNING: This decorator manipulates global state (current directory) and
    should not be used in any code that is run in the tron daemon. This
    decorator should only be used in short lived scripts under tools/.
    """
    cwd = os.getcwd()
    os.chdir(path)
    try:
        yield
    finally:
        os.chdir(cwd)

########NEW FILE########
__FILENAME__ = trontimespec
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""A complete time specification based on the Google App Engine GROC spec."""


import calendar
import datetime
import itertools

try:
    import pytz
    assert pytz
except ImportError:
    pytz = None

HOURS = 'hours'
MINUTES = 'minutes'

try:
    from pytz import NonExistentTimeError
    assert NonExistentTimeError
    from pytz import AmbiguousTimeError
    assert AmbiguousTimeError
except ImportError:
    class NonExistentTimeError(Exception):
        pass

    class AmbiguousTimeError(Exception):
        pass


def get_timezone(timezone_string):
    """Converts a timezone string to a pytz timezone object.

    Arguments:
      timezone_string: a string representing a timezone, or None

    Returns:
      a pytz timezone object, or None if the input timezone_string is None

    Raises:
      ValueError: if timezone_string is not None and the pytz module could not be
          loaded
    """
    if timezone_string:
        if pytz is None:
            raise ValueError('need pytz in order to specify a timezone')
        return pytz.timezone(timezone_string)
    else:
        return None


def to_timezone(t, tzinfo):
    """Converts 't' to the time zone 'tzinfo'.

    Arguments:
      t: a datetime object.  It may be in any pytz time zone, or it may be
          timezone-naive (interpreted as UTC).
      tzinfo: a pytz timezone object, or None (interpreted as UTC).

    Returns:
      a datetime object in the time zone 'tzinfo'
    """
    if pytz is None:
        return t.replace(tzinfo=tzinfo)
    elif tzinfo:
        if not t.tzinfo:
            t = pytz.utc.localize(t)
        return tzinfo.normalize(t.astimezone(tzinfo))
    elif t.tzinfo:
        return pytz.utc.normalize(t.astimezone(pytz.utc)).replace(tzinfo=None)
    else:
        return t


def get_time(time_string):
    """Converts a string to a datetime.time object.

    Arguments:
      time_string: a string representing a time ('hours:minutes')

    Returns:
      a datetime.time object
    """
    try:
        return datetime.datetime.strptime(time_string, "%H:%M").time()
    except ValueError:
        return None


TOKEN_LAST = 'LAST'

ordinal_range  = range(1, 6)
weekday_range  = range(0, 7)
month_range    = range(1, 13)
monthday_range = range(1, 32)
hour_range     = range(0, 24)
minute_range   = second_range = range(0, 60)


def validate_spec(source, value_range, type, default=None, allow_last=False):
    default = default if default is not None else value_range
    if not source:
        return default

    for item in source:
        if allow_last and item == TOKEN_LAST:
            continue
        if item not in value_range:
            raise ValueError("%s not in range %s" % (type, value_range))
    return sorted(set(source))


class TimeSpecification(object):
    """TimeSpecification determines the next time which matches the
    configured pattern.
    """

    def __init__(self,
            ordinals=None,
            weekdays=None,
            months=None,
            monthdays=None,
            timestr=None,
            timezone=None,
            minutes=None,
            hours=None,
            seconds=None):

        if weekdays and monthdays:
            raise ValueError('cannot supply both monthdays and weekdays')

        if timestr and (minutes or hours or seconds):
            raise ValueError('cannot supply both timestr and h/m/s')

        if not any((timestr, minutes, hours, seconds)):
            timestr = '00:00'

        if timestr:
            time    = get_time(timestr)
            hours   = [time.hour]
            minutes = [time.minute]
            seconds = [0]

        self.hours      = validate_spec(hours, hour_range, 'hour')
        self.minutes    = validate_spec(minutes, minute_range, 'minute')
        self.seconds    = validate_spec(seconds, second_range, 'second')
        self.ordinals   = validate_spec(ordinals, ordinal_range, 'ordinal')
        self.weekdays   = validate_spec(
            weekdays, weekday_range, 'weekdays', allow_last=True)
        self.months     = validate_spec(months, month_range, 'month')
        self.monthdays  = validate_spec(
            monthdays, monthday_range, 'monthdays', [], True)
        self.timezone   = get_timezone(timezone)

    def next_day(self, first_day, year, month):
        """Returns matching days for the given year and month.
        """
        first_day_of_month, last_day_of_month = calendar.monthrange(year, month)

        map_last   = lambda day: last_day_of_month if day == TOKEN_LAST else day
        day_filter = lambda day: first_day <= day <= last_day_of_month
        sort_days  = lambda days: sorted(itertools.ifilter(day_filter, days))

        if self.monthdays:
            return sort_days(map_last(day) for day in self.monthdays)

        start_day = (first_day_of_month + 1) % 7
        def days_from_weekdays():
            for ordinal in self.ordinals:
                week = (ordinal - 1) * 7
                for weekday in self.weekdays:
                    yield ((weekday - start_day) % 7) + week + 1

        return sort_days(days_from_weekdays())

    def next_month(self, start_date):
        """Create a generator which yields valid months after the start month.
        """
        current     = start_date.month
        potential   = [m for m in self.months if m >= current]
        year_wraps   = 0

        while True:
            if not potential:
                year_wraps += 1
                potential = list(self.months)

            yield potential.pop(0), start_date.year + year_wraps

    def next_time(self, start_date, is_start_day):
        """Return the next valid time."""
        start_hour   = start_date.time().hour
        hour_filter  = lambda hour: not is_start_day or hour >= start_hour

        for hour in itertools.ifilter(hour_filter, self.hours):
            for minute in self.minutes:
                for second in self.seconds:
                    candidate = datetime.time(hour, minute, second)

                    if is_start_day and start_date.time() >= candidate:
                        continue

                    return candidate

    def get_match(self, start):
        """Returns the next datetime match after start."""
        start_date  = to_timezone(start, self.timezone).replace(tzinfo=None)

        def get_first_day(month, year):
            if (month, year) != (start_date.month, start_date.year):
                return 1
            return start_date.day

        for month, year in self.next_month(start_date):
            first_day = get_first_day(month, year)

            for day in self.next_day(first_day, year, month):
                is_start_day = start_date.timetuple()[:3] == (year, month, day)

                time = self.next_time(start_date, is_start_day)
                if time is None:
                    continue

                candidate = start_date.replace(year, month, day, time.hour,
                    time.minute, second=time.second, microsecond=0)
                candidate = self.handle_timezone(candidate, start.tzinfo)
                if not candidate:
                    continue
                return candidate

    # TODO: test
    def handle_timezone(self, out, tzinfo):
        if self.timezone and pytz is not None:
            try:
                out = self.timezone.localize(out, is_dst=None)
            except AmbiguousTimeError:
                out = self.timezone.localize(out)
            except NonExistentTimeError:
                # TODO: this is duplicated in the scheduler
                for _ in range(24):
                    out += datetime.timedelta(minutes=60)
                    try:
                        out = self.timezone.localize(out)
                    except NonExistentTimeError:
                        return None
        return to_timezone(out, tzinfo)

    def __eq__(self, other):
        attrs = [
            'hours',
            'minutes',
            'seconds',
            'ordinals',
            'weekdays',
            'months',
            'monthdays',
            'timezone']
        return all(
            getattr(other, attr, None) == getattr(self, attr, None)
                for attr in attrs)

    def __ne__(self, other):
        return not self == other

########NEW FILE########
__FILENAME__ = twistedutils
from twisted.internet import defer, reactor
from twisted.python import failure


class Error(Exception):
    pass


def _cancel(deferred):
    """Re-implementing what's available in newer twisted in a crappy, but
    workable way."""

    if not deferred.called:
        deferred.errback(failure.Failure(Error()))
    elif isinstance(deferred.result, defer.Deferred):
        _cancel(deferred.result)


def defer_timeout(deferred, timeout):
    try:
        reactor.callLater(timeout, deferred.cancel)
    except AttributeError:
        reactor.callLater(timeout, lambda: _cancel(deferred))

########NEW FILE########
