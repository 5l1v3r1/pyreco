__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# particle documentation build configuration file, created by
# sphinx-quickstart on Wed Dec 25 21:19:20 2013.
#
# This file is execfile()d with the current directory set to its
# containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys
import os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration ------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'particle'
copyright = u'2013, Brian Abelson'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.0'
# The full version, including alpha/beta/rc tags.
release = '1.0'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all
# documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
#keep_warnings = False


# -- Options for HTML output ----------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'kr'

html_theme_path = ['_themes']
# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Custom sidebar templates, maps document names to template names.
html_sidebars = {
    'index': ['sidebarlogo.html', 'sourcelink.html', 'searchbox.html']
}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# Add any extra paths that contain custom files (such as robots.txt or
# .htaccess) here, relative to this directory. These files are copied
# directly to the root of the documentation.
#html_extra_path = []

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'particledoc'


# -- Options for LaTeX output ---------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
  ('index', 'particle.tex', u'particle Documentation',
   u'Brian Abelson', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output ---------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'particle', u'particle Documentation',
     [u'Brian Abelson'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output -------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'particle', u'particle Documentation',
   u'Brian Abelson', 'particle', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

# If true, do not generate a @detailmenu in the "Top" node's menu.
#texinfo_no_detailmenu = False

########NEW FILE########
__FILENAME__ = flask_theme_support
# flasky extensions.  flasky pygments style based on tango style
from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Punctuation, Other, Literal


class FlaskyStyle(Style):
    background_color = "#f8f8f8"
    default_style = ""

    styles = {
        # No corresponding class for the following:
        #Text:                     "", # class:  ''
        Whitespace:                "underline #f8f8f8",      # class: 'w'
        Error:                     "#a40000 border:#ef2929", # class: 'err'
        Other:                     "#000000",                # class 'x'

        Comment:                   "italic #8f5902", # class: 'c'
        Comment.Preproc:           "noitalic",       # class: 'cp'

        Keyword:                   "bold #004461",   # class: 'k'
        Keyword.Constant:          "bold #004461",   # class: 'kc'
        Keyword.Declaration:       "bold #004461",   # class: 'kd'
        Keyword.Namespace:         "bold #004461",   # class: 'kn'
        Keyword.Pseudo:            "bold #004461",   # class: 'kp'
        Keyword.Reserved:          "bold #004461",   # class: 'kr'
        Keyword.Type:              "bold #004461",   # class: 'kt'

        Operator:                  "#582800",   # class: 'o'
        Operator.Word:             "bold #004461",   # class: 'ow' - like keywords

        Punctuation:               "bold #000000",   # class: 'p'

        # because special names such as Name.Class, Name.Function, etc.
        # are not recognized as such later in the parsing, we choose them
        # to look the same as ordinary variables.
        Name:                      "#000000",        # class: 'n'
        Name.Attribute:            "#c4a000",        # class: 'na' - to be revised
        Name.Builtin:              "#004461",        # class: 'nb'
        Name.Builtin.Pseudo:       "#3465a4",        # class: 'bp'
        Name.Class:                "#000000",        # class: 'nc' - to be revised
        Name.Constant:             "#000000",        # class: 'no' - to be revised
        Name.Decorator:            "#888",           # class: 'nd' - to be revised
        Name.Entity:               "#ce5c00",        # class: 'ni'
        Name.Exception:            "bold #cc0000",   # class: 'ne'
        Name.Function:             "#000000",        # class: 'nf'
        Name.Property:             "#000000",        # class: 'py'
        Name.Label:                "#f57900",        # class: 'nl'
        Name.Namespace:            "#000000",        # class: 'nn' - to be revised
        Name.Other:                "#000000",        # class: 'nx'
        Name.Tag:                  "bold #004461",   # class: 'nt' - like a keyword
        Name.Variable:             "#000000",        # class: 'nv' - to be revised
        Name.Variable.Class:       "#000000",        # class: 'vc' - to be revised
        Name.Variable.Global:      "#000000",        # class: 'vg' - to be revised
        Name.Variable.Instance:    "#000000",        # class: 'vi' - to be revised

        Number:                    "#990000",        # class: 'm'

        Literal:                   "#000000",        # class: 'l'
        Literal.Date:              "#000000",        # class: 'ld'

        String:                    "#4e9a06",        # class: 's'
        String.Backtick:           "#4e9a06",        # class: 'sb'
        String.Char:               "#4e9a06",        # class: 'sc'
        String.Doc:                "italic #8f5902", # class: 'sd' - like a comment
        String.Double:             "#4e9a06",        # class: 's2'
        String.Escape:             "#4e9a06",        # class: 'se'
        String.Heredoc:            "#4e9a06",        # class: 'sh'
        String.Interpol:           "#4e9a06",        # class: 'si'
        String.Other:              "#4e9a06",        # class: 'sx'
        String.Regex:              "#4e9a06",        # class: 'sr'
        String.Single:             "#4e9a06",        # class: 's1'
        String.Symbol:             "#4e9a06",        # class: 'ss'

        Generic:                   "#000000",        # class: 'g'
        Generic.Deleted:           "#a40000",        # class: 'gd'
        Generic.Emph:              "italic #000000", # class: 'ge'
        Generic.Error:             "#ef2929",        # class: 'gr'
        Generic.Heading:           "bold #000080",   # class: 'gh'
        Generic.Inserted:          "#00A000",        # class: 'gi'
        Generic.Output:            "#888",           # class: 'go'
        Generic.Prompt:            "#745334",        # class: 'gp'
        Generic.Strong:            "bold #000000",   # class: 'gs'
        Generic.Subheading:        "bold #800080",   # class: 'gu'
        Generic.Traceback:         "bold #a40000",   # class: 'gt'
    }

########NEW FILE########
__FILENAME__ = nytimes
from particle import Particle

p = Particle('nytimes.yml')
p.run()

########NEW FILE########
__FILENAME__ = propublica
from particle import Particle

p = Particle('propublica.yml')
p.run()

########NEW FILE########
__FILENAME__ = app
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from thready import threaded
import yaml, json
import logging

from particle.facebook import facebook
from particle.twitter import twitter
from particle.promopages import promopages
from particle.rssfeeds import rssfeeds
from particle.facebook import fb
from particle.twitter import twt
from particle.common import DEBUG, db
from particle.helpers import *

urllib3_logger = logging.getLogger('urllib3')
urllib3_logger.setLevel(logging.CRITICAL)

log = logging.getLogger('particle')

class Particle:
  def __init__(self, config, tasks=None):

    self.CONFIG, self.TASKS = load_and_validate_config(config)
    if tasks is not None:
      self.TASKS = tasks

    if 'twitter' in self.TASKS:
      twt.generate_lists(self.CONFIG)
    if 'facebook' in self.TASKS:
      self.CONFIG = fb.generate_extended_access_token(self.CONFIG)

  def _execute(self, task):
    if task=="twitter":
      twitter.run(self.CONFIG)
    elif task=="facebook":
      facebook.run(self.CONFIG)
    elif task=="rssfeeds":
      rssfeeds.run(self.CONFIG)
    elif task=="promopages":
      promopages.run(self.CONFIG)

  def run(self, 
          num_threads=4, 
          max_queue=4):
    
    threaded_or_serial(self.TASKS, self._execute, num_threads, max_queue)

    def __repr__(self):
        return '<Particle Object>'
########NEW FILE########
__FILENAME__ = cli
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json, yaml, os, sys
from argparse import ArgumentParser

from particle.web import api
from particle.app import Particle


# sub-command functions
def particle_runner(args):
  tasks = [t for t in args.tasks.split(',') if t != '' and t is not None]
  particle = Particle(config=args.config, tasks=tasks)
  particle.run()

def api_runner(args):
  api(port = args.port, debug = args.debug)

# command line interface
def cli():
  parser = ArgumentParser(
    prog = 'particle', 
    usage = '%(prog)s [options]', 
    description='Run particle from the command line.'
  )
  subparsers = parser.add_subparsers()
  
  # run particle
  parser_run = subparsers.add_parser('run')
  parser_run.add_argument(
    "-c", '--config', 
    dest="config", 
    default="particle.yml", 
    help="The path to your configuration file.\r\ndefault = particle.yml"
  )
  parser_run.add_argument(
    "-t", '--tasks', 
    dest="tasks", 
    default="twitter,facebook,rssfeeds,promopages", 
    help = 'A comma-separated list of tasks to run.\r\ndefault = twitter,facebook,rssfeeds,promopages'
  )
  parser_run.set_defaults(func=particle_runner)

  parser_api = subparsers.add_parser('api')
  parser_api.add_argument(
    "-p", '--port', 
    dest = "port", 
    default = 3030, 
    help = 'The port on which to serve the API.\r\ndefault = 3030'
  )
  parser_api.add_argument(
    "-d", '--debug', 
    dest = "debug", 
    action='store_true', 
    help = 'Whether or not to run the API in debug mode.\r\ndefault = True'
  )
  parser_api.set_defaults(func=api_runner)

  # parse args and run function
  args = parser.parse_args()
  args.func(args)

########NEW FILE########
__FILENAME__ = common
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import redis

# GLOBAL settings
DEBUG = False
APP_DEBUG = True

# initialize redis
db = redis.StrictRedis(host='localhost', port=6379, db=0)


########NEW FILE########
__FILENAME__ = facebook
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json
from thready import threaded
from pprint import pprint
from datetime import datetime
import logging

from particle.common import db, DEBUG
from particle.facebook import fb
from particle.helpers import *


FB_DATE_FORMAT = "%Y-%m-%dT%H:%M:%S+0000"

log = logging.getLogger('particle')


def is_insights(page_id, config):
  """
  Determine whether we can collect insights for a page
  """
  if config['facebook'].has_key('insights_pages'):
    return page_id in set(config['facebook']['insights_pages'])
  else:
    return False


def get_fb_link(post_data, config, unshorten=False):
  """
  parse fb_post data for links
  """
  if post_data.has_key('link'):
    if unshorten:
      return parse_url(unshorten_link(post_data['link'], config))
    else:
      return parse_url(post_data['link'])
  elif post_data.has_key('source'):
    if unshorten:
      return parse_url(unshorten_link(post_data['source'], config))
    else:
      return parse_url(post_data['source'])
  else:
    return None


def parse_message_urls(message, config):
  """
  parse facebook message for links
  """
  if message is not None:
    message_urls = extract_urls(message, config)
    if len(message_urls)>0:
      return [u for u in message_urls]
    else:
      return []
  else:
    return [] 


def get_message_urls(article_urls, message, config):
  """
  determine whether we should get message_urls
  """
  if len(article_urls)==0:
    return parse_messge_urls(message, config)
  elif article_urls[0] is not None and is_facebook_link(article_urls[0]):
    return parse_message_urls(message, config)
  else:
    return []


def get_insights_data(api, page_id, post_id):
  """
  Get insights data if indicated so by the config file
  """
  graph_results = api.get(post_id + "/insights", page=False, retry=5)
  data = graph_results['data']
  insights = {}
  insights['includes_insights'] = True  
  
  for d in data:
    val = d['values'][0]['value']
    if isinstance(val, dict):
      for k, v in val.iteritems():
        insights[k] = v

    else:
      insights[d['name']] = val

  return insights


def insert_new_post(post_arg_set):
  """
  insert new post into redis
  """
  api, post_data, acct_data, page_id, config = post_arg_set

  try:
    post_id = post_data['id'] if post_data.has_key('id') else None

  except Exception as e:
    log.error( e )

  else:

    # parse date
    if post_data.has_key('created_time') and post_data['created_time'] is not None:  
      dt = datetime.strptime(post_data['created_time'], FB_DATE_FORMAT)
      date_time = tz_adj(dt, config)
      time_bucket = round_datetime(date_time, config)
      raw_timestamp = int(date_time.strftime("%s"))
    
    else:
      time_bucket = None
      raw_timestamp = None
    
    # extract message so we can find links within the msg if not in url
    article_urls = [get_fb_link(post_data, config, unshorten=True)]
    message = post_data['message'].encode('utf-8') if post_data.has_key('message') else None
    message_urls = get_message_urls(article_urls, message, config)

    # detect article links, unshorten and parse
    article_urls = [
      parse_url(unshorten_link(url, config)) \
      for url in article_urls + message_urls
      if url is not None
    ]

    article_urls = [url for url in article_urls if is_article(url, config)]

    if article_urls:
      for article_url in set(article_urls):

        # sluggify url
        article_slug = sluggify(article_url)

        # format data
        post_value = {
          'article_slug': article_slug,
          'article_url': article_url,
          'time_bucket': time_bucket,
          'fb_post_created': raw_timestamp,
          'raw_timestamp': raw_timestamp,
          'fb_raw_link' : get_fb_link(post_data, config=config),
          'fb_page_id': page_id,
          'fb_post_id': post_id,
          'fb_page_likes': acct_data['likes'] if acct_data.has_key('likes') else None,
          'fb_page_talking_about': acct_data['talking_about_count'] if acct_data.has_key('talking_about_count') else None,
          'fb_type': post_data['type'] if post_data.has_key('type') else None,
          'fb_status_type': post_data['status_type'] if post_data.has_key('status_type') else None,
          'fb_message': message
        }
          
        # always insert insights data
        if is_insights(page_id, config):
          
          log.info( "INSIGHTS\tAdding data from %s re: %s" % (page_id, article_slug) )

          # fetch data
          insights_value = get_insights_data(api, page_id, post_id)

          # create datasource name
          data_source = "facebook_insights_%s" % page_id 
          
          # upsert url
          upsert_url(article_url, article_slug, data_source, config)

          # insert id
          db.sadd('facebook_post_ids', post_id)

          # format time bucket
          current_time_bucket = gen_time_bucket(config)
          insights_value['time_bucket'] =  current_time_bucket
          post_value.pop('time_bucket', None)
          
          value = json.dumps({
            data_source : dict(post_value.items() + insights_value.items())
          })

          # upload data to redis
          db.zadd(article_slug, current_time_bucket, value)        
            
        # only insert new posts
        if not db.sismember('facebook_post_ids', post_id):
          
          log.info( "FACEBOOK\tNew post %s\t%s" % (post_id, article_url) )
          
          # insert id
          db.sadd('facebook_post_ids', post_id)     
          
          # upsert url
          data_source = "facebook_%s" % page_id
          upsert_url(article_url, article_slug, data_source, config)

          value = json.dumps( {data_source : post_value} )


          # upload data to redis
          db.zadd(article_slug, time_bucket, value)


def get_new_data_for_page(page_arg_set):
  """
  get all new posts on a page
  """
  api, page_id, config = page_arg_set

  log.info( "FACEBOOK\tGetting new data for facebook.com/%s" % page_id )
  
  # fetch account data so we can associate the number of likes with the account AT THAT TIME
  try:
    acct_data = api.get(page_id)
  except Exception as e:
    log.error('FACEBOOK\t%s does not exist' % page_id)
    return None
  else:
    # determine limit
    if is_insights(page_id, config):
      if config['facebook'].has_key('insights_limit'):
        limit = config['facebook']['insights_limit']
      else:
        limit = 200

    else:
      if config['facebook'].has_key('page_limit'):
        limit = config['facebook']['page_limit']
      else:
        limit = 10

    # get last {limit} articles for this page
    page = api.get(page_id + "/posts", page=False, retry=5, limit=limit)
    post_arg_sets = [(api, post_data, acct_data, page_id, config) for post_data in page['data']]
    
    threaded_or_serial(post_arg_sets, insert_new_post, 30, 200)


def run(config):
  """
  get all new posts on all pages
  """
  page_ids = config['facebook']['pages']
  api = fb.connect(config)
  page_arg_sets = [(api, page_id, config) for page_id in set(page_ids)]
  
  threaded_or_serial(page_arg_sets, get_new_data_for_page, 5, 100)


########NEW FILE########
__FILENAME__ = fb
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import facepy
import yaml
from datetime import datetime, timedelta
from urlparse import parse_qs
import logging

from particle.common import DEBUG

log = logging.getLogger('particle')

def connect(config):
    return facepy.GraphAPI(config['facebook']['stable_access_token'])

def generate_extended_access_token(config):
    """
    Get an extended OAuth access token.

    :param access_token: A string describing an OAuth access token.
    :param application_id: An icdsnteger describing the Facebook application's ID.
    :param application_secret_key: A string describing the Facebook application's secret key.

    Returns a tuple with a string describing the extended access token and a datetime instance
    describing when it expires.
    """

    if not config['facebook'].has_key('stable_access_token') or \
       config['facebook']['stable_access_token'] is None:

        # access tokens
        default_access_token = facepy.get_application_access_token(
            application_id = config['facebook']['app_id'],  
            application_secret_key = config['facebook']['app_secret']
        )
        graph = facepy.GraphAPI(default_access_token)

        response = graph.get(
            path='oauth/access_token',
            client_id = config['facebook']['app_id'],
            client_secret = config['facebook']['app_secret'],
            grant_type = 'fb_exchange_token',
            fb_exchange_token = config['facebook']['temp_access_token']
        )

        components = parse_qs(response)

        token = components['access_token'][0]
        expires_at = datetime.now() + timedelta(seconds=int(components['expires'][0]))

        config['facebook']['stable_access_token'] = token
        config['facebook']['stable_access_token_expires_at'] =  int(expires_at.strftime("%s"))
        
        log.info("FACEBOOK\tThis is your new stable facebook access token: %s" % token)
        log.info("FACEBOOK\tIt will expire on %s" % expires_at.strftime("%s"))
        
        return config

    else:
        return config

########NEW FILE########
__FILENAME__ = helpers
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from datetime import datetime, timedelta
import re
from particle.common import db
from HTMLParser import HTMLParser
from urlparse import urlparse
import pytz
import requests
from pprint import pprint
import string
import json
import logging
import sys
import yaml
from bs4 import BeautifulSoup
from thready import threaded

from particle.common import DEBUG

requests_logger = logging.getLogger('requests')
requests_logger.setLevel(logging.CRITICAL)

log = logging.getLogger('particle')

# config validation
req_items = {
  'global': ['bucket', 'newsroom_timezone', 'phantomjs', 'content_regexes'],
  'facebook': ['app_id', 'app_secret', 'temp_access_token', 'page_limit', 'pages'],
  'twitter': ['access_token', 'access_token_secret', 'consumer_key', 'consumer_secret', 'lists']
}

class ParticleConfigException(Exception):
    pass

def raise_error_message(message):
    err_message = """%s
      See the documentation at http://particle.rtfd.org/""" % message

    raise ParticleConfigException(err_message)

def raise_missing_items_error_message(field, missing_items):
    message = """
        The config's '%s' field  must include items for: 
        %s.""" % (field, ", ".join( missing_items)) 
    raise_error_message(message)

def validate_items(config, field):
  return [r for r in req_items[field] if r not in config[field].keys()]

def validate_field(config, field):
  if req_items.has_key(field):
    missing_items = validate_items(config, field)
    if len( missing_items) > 0 :
      raise_missing_items_error_message(field, missing_items)

def load_config(config):
    # initialize CONFIG
    if isinstance(config, basestring):
      if config.endswith('.yml'):
        c = yaml.safe_load(open(config))
      elif config.endswith('.json'):
        c = json.load(open(config))
    elif isinstance(config, dict):
      c = config
    else:
      raise raise_error_message('config must be a filepath or a dictionary.')

    return c
  
def load_and_validate_config(config):
  config = load_config(config)
  tasks = config.keys()

  if 'global' not in tasks:
    raise raise_error_message("The config must include a 'global' field.")
  
  else:
    for field in tasks:
      validate_field(config, field)
    
  # return list of tasks
  tasks = [ t for t in tasks if t != 'global' ]
  return config, tasks

# wrapper for thready
def threaded_or_serial(tasks, func, num_threads, max_queue):
  if DEBUG:
    for t in tasks:
      func(t)
  else:
    threaded(tasks, func, num_threads, max_queue)

# debug msg
def print_output(article_url, time_bucket, value):
  log.info( "key: %s" % article_url )
  log.info( "rank: %s" % time_bucket )
  log.info( value )


# DATE HELPERS #
def round_datetime(dt, config):
  """
  round dateime object to set bucket
  return as timestamp integer
  """
  bucket = int(config['global']['bucket'])
  dt = dt - timedelta(
          minutes = dt.minute % bucket,
          seconds = dt.second,
          microseconds = dt.microsecond
      )
  return int(dt.strftime('%s'))

def tz_adj(dt, config):
  tz = config['global']['newsroom_timezone']
  utc = pytz.timezone("UTC")
  mytz = pytz.timezone(tz)
  try:
      dt = dt.replace(tzinfo=utc)
  except:
      return None
  else:
      return mytz.normalize(dt.astimezone(mytz))
# database
def current_datetime(config=None):
  """
  generate datetime bucket for sorted set ranking
  """
  try:
    tz = config['global']['newsroom_timezone']
  except:
    tz = 'UTC'

  mytz = pytz.timezone(tz)
  return datetime.now(tz=mytz)

def current_timestamp(config=None):
  """
  generate datetime bucket for sorted set ranking
  """
  try:
    tz = config['global']['newsroom_timezone']
  except:
    tz = 'UTC'
    
  mytz = pytz.timezone(tz)
  return int(datetime.now(tz=mytz).strftime('%s'))

def gen_time_bucket(config):
  """
  generate datetime bucket for sorted set ranking
  """
  return round_datetime(current_datetime(config), config)

# DATABASE HELPERS

def sluggify(url):
  regex = re.compile('[%s]' % re.escape(string.punctuation))
  o = urlparse(url)
  out = regex.sub(' ', o.path).strip()
  if out.endswith('html'):
    out = out[:-4].strip()
  return re.sub(r"\s+", "-", out).lower()

def upsert_url(article_url, article_slug, data_source, config):
  if not db.sismember('article_set', article_url):
    # add it to the set
    db.sadd('article_set', article_url)

    # insert metadata
    ts = current_timestamp(config)
    value = json.dumps({
      "url" : article_url,
      "slug": article_slug,
      "timestamp" : ts,
      "data_source": data_source
      })
    
    db.zadd('article_sorted_set', ts, value)

def upsert_rss_pub(article_url, article_slug, value):
  if not db.sismember('article_set', article_url):
    # add it to the set
    db.sadd('article_set', article_url)
    
  key = "%s:article" % article_slug
  db.set(key, value)

# urls
def parse_url(url):
  """
  remove url query strings
  """
  o = urlparse(url)
  return  "%s://%s%s" % (o.scheme, o.netloc, o.path)

def is_article(link_url, config):
  patterns = [str(p) for p in config['global']['content_regexes'] if p != '' and p is not None]

  if len(patterns)>0:
    return any([re.search(p, link_url) for p in patterns])
  else:
    return True

# html stripping
class MLStripper(HTMLParser):
  def __init__(self):
    self.reset()
    self.fed = []

  def handle_data(self, d):
    self.fed.append(d)

  def get_data(self):
    return ''.join(self.fed)

def strip_tags(html):
    s = MLStripper()
    s.feed(html)
    raw_text = s.get_data()
    raw_text = re.sub(r'\n|\t', ' ', raw_text)
    return re.sub('\s+', ' ', raw_text).strip()

def is_short_link(url):
  re_short_links = [
    re.compile(r".*bit\.do.*"),
    re.compile(r".*t\.co.*"),
    re.compile(r".*go2\.do.*"),
    re.compile(r".*adf\.ly.*"),
    re.compile(r".*goo\.gl.*"),
    re.compile(r".*bitly\.com.*"),
    re.compile(r".*bit\.ly.*"),
    re.compile(r".*tinyurl\.com.*"),
    re.compile(r".*ow\.ly.*"),
    re.compile(r".*bit\.ly.*"),
    re.compile(r".*adcrun\.ch.*"),
    re.compile(r".*zpag\.es.*"),
    re.compile(r".*ity\.im.*"),
    re.compile(r".*q\.gs.*"),
    re.compile(r".*lnk\.co.*"),
    re.compile(r".*viralurl\.com.*"),
    re.compile(r".*is\.gd.*"),
    re.compile(r".*vur\.me.*"),
    re.compile(r".*bc\.vc.*"),
    re.compile(r".*yu2\.it.*"),
    re.compile(r".*twitthis\.com.*"),
    re.compile(r".*u\.to.*"),
    re.compile(r".*j\.mp.*"),
    re.compile(r".*bee4\.biz.*"),
    re.compile(r".*adflav\.com.*"),
    re.compile(r".*buzurl\.com.*"),
    re.compile(r".*xlinkz\.info.*"),
    re.compile(r".*cutt\.us.*"),
    re.compile(r".*u\.bb.*"),
    re.compile(r".*yourls\.org.*"),
    re.compile(r".*fun\.ly.*"),
    re.compile(r".*hit\.my.*"),
    re.compile(r".*nov\.io.*"),
    re.compile(r".*crisco\.com.*"),
    re.compile(r".*x\.co.*"),
    re.compile(r".*shortquik\.com.*"),
    re.compile(r".*prettylinkpro\.com.*"),
    re.compile(r".*viralurl\.biz.*"),
    re.compile(r".*longurl\.org.*"),
    re.compile(r".*tota2\.com.*"),
    re.compile(r".*adcraft\.co.*"),
    re.compile(r".*virl\.ws.*"),
    re.compile(r".*scrnch\.me.*"),
    re.compile(r".*filoops\.info.*"),
    re.compile(r".*linkto\.im.*"),
    re.compile(r".*vurl\.bz.*"),
    re.compile(r".*fzy\.co.*"),
    re.compile(r".*vzturl\.com.*"),
    re.compile(r".*picz\.us.*"),
    re.compile(r".*lemde\.fr.*"),
    re.compile(r".*golinks\.co.*"),
    re.compile(r".*xtu\.me.*"),
    re.compile(r".*qr\.net.*"),
    re.compile(r".*1url\.com.*"),
    re.compile(r".*tweez\.me.*"),
    re.compile(r".*sk\.gy.*"),
    re.compile(r".*gog\.li.*"),
    re.compile(r".*cektkp\.com.*"),
    re.compile(r".*v\.gd.*"),
    re.compile(r".*p6l\.org.*"),
    re.compile(r".*id\.tl.*"),
    re.compile(r".*dft\.ba.*"),
    re.compile(r".*aka\.gr.*")
  ]
  return any([r.search(url) for r in re_short_links])


def is_facebook_link(link):
  if re.search("facebook", link):
    return True
  else:
    return False

def test_for_short_link(link, config):
  if config['global'].has_key('short_regexes'):
    patterns = config['global']['short_regexes']
    if any([re.search(p, link) for p in patterns]):
      return True
  elif is_short_link(link):
    return True
  else:
    return False

def unshorten_link(link, config):
  """
  recursively unshorten a link
  """
  l = link
  test = test_for_short_link(l, config)
  if test:
    tries = 0
    while test:
      
      try:
        r = requests.get(l)
      
      except Exception as e:
        log.warning("unshorten_link, %s" % e)
        
        # quit
        return l
        break

      else:
        
        # quit
        if r.status_code != 200:
          return l
          break
        
        # give it one more shot!
        else:
          l = r.url
          test = test_for_short_link(l, config)
          tries +=1
          
          # if we've tried 10 times, give up
          if tries==10:
            return l
            break

    # return link at final state
    return l
  else:
    return link

def extract_urls(string, config, text=True):
    """
    get urls from input string
    """
    if string is not None:
      if text:
        p = "(https?://|([a-z]+\.[a-z]+)[^\s\'\"]+)"
        return list(set([parse_url(unshorten_link(l, config)) for l in re.findall(p, string) if len(l)>6]))
      
      else:
        soup = BeautifulSoup(string)
        links = []
        for a in soup.find_all('a'):
          if a.has_key('href') and a.attrs['href'] != '':
            link = parse_url(unshorten_link(a.attrs['href'], config))
            links.append(link)

        return list(set(links))

def extract_imgs(string):
    soup = BeautifulSoup(string)
    imgs = []
    for i in soup.find_all('img'):
      if i.has_key('src') and i.attrs['src'] != '':
        imgs.append(i.attrs['src'])

    return list(set(imgs))

########NEW FILE########
__FILENAME__ = promopages
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from selenium import webdriver
from selenium.webdriver.support.wait import WebDriverWait
from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException
from datetime import datetime, timedelta
from thready import threaded
import json, yaml
import socket

from particle.common import db, DEBUG
from particle.helpers import *

log = logging.getLogger('particle')

# set the socket timeout
socket.setdefaulttimeout(20)


def get_image_for_a_link(link):
  try:
    img = link.find_element_by_tag_name("img")
  except NoSuchElementException:
    img = None
  if img is not None:
    w = int(img.get_attribute("width"))
    h = int(img.get_attribute("height"))
    
    return dict(
      pp_is_img = 1,
      pp_img_width = w,
      pp_img_height = h,
      pp_img_area = w*h,
      pp_img_src = img.get_attribute("src")
    )
  else:
    return dict(pp_is_img = 0)


def scrape_link(link_arg_set):
  promo_url, link, time_bucket, data_source, config = link_arg_set

  try:
    link_url = link.get_attribute("href")

  except StaleElementReferenceException:
    pass

  else:                  
    # continue under specified conditions
    if isinstance(link_url, basestring) and is_article(link_url, config):
      
      # parse link text
      try:
        link_text = link.text.encode('utf-8').strip()
      except:
        link_text = None

      if link_text is not None  and link_text is not '':

        pos_x = int(link.location['x'])
        pos_y = int(link.location['y'])

        if pos_x > 0 and pos_y > 0:
          
          # get image
          img_dict = get_image_for_a_link(link)

          # parse link
          link_url = link_url.encode('utf-8')
          article_url = parse_url(link_url)

          # sluggify
          article_slug = sluggify(article_url)

          # scrape
          log.info("PROMOPAGE\tLink detected on %s\t%s" % (promo_url, article_url))

          link_dict = {
            'article_slug' : article_slug,
            'article_url': article_url,
            'time_bucket': time_bucket,
            'raw_timestamp': int(datetime.now().strftime("%s")),
            'pp_promo_url' : promo_url,
            'pp_link_url': link_url,
            'pp_headline' : link_text,
            'pp_font_size' : int(link.value_of_css_property('font-size')[:-2]),
            'pp_pos_x' : pos_x,
            'pp_pos_y' : pos_y
          }

          value = json.dumps({data_source : dict(img_dict.items() + link_dict.items())})

          # upsert url
          upsert_url(article_url, article_slug, data_source, config)

          # upload data to redis
          db.zadd(article_slug, time_bucket, value)


def scrape_links(links_arg_set):    
  
  b, promo_url, data_source, config = links_arg_set
  log.info( "PROMOPAGE\t%s" % promo_url )
  time_bucket = gen_time_bucket(config)
  links = b.find_elements_by_tag_name("a")
  link_arg_sets = [(promo_url, l, time_bucket, data_source, config) for l in links]
  
  threaded_or_serial(link_arg_sets, scrape_link, 5, 25)
  # for link_arg_set in link_arg_sets:
  #   scrape_link(link_arg_set)

def readystate_complete(d):
    # AFAICT Selenium offers no better way to wait for the document to be loaded,
    # if one is in ignorance of its contents.
    return d.execute_script("return document.readyState") == "complete"


def get_url_safely(b, url):
  tries = 0
  try:
    b.get(url)
    WebDriverWait(b, 30).until(readystate_complete)
  
  except TimeoutException:
    d.execute_script("window.stop();")

  except socket.timeout:
    while 1:

      try:
        b.get(url)
        WebDriverWait(b, 5).until(readystate_complete)
  
      except TimeoutException:
        d.execute_script("window.stop();")

      except:
        tries += 1
        if tries == 20:
          return b
      else:
        return b

  except Exception as e:
    return b

  else:
    return b


def scrape_promo_page(page_arg_set):
  promo_url, data_source, config = page_arg_set
  b = webdriver.PhantomJS(config['global']['phantomjs'])
  b = get_url_safely(b, promo_url)
  links_arg_set = (b, promo_url, data_source, config)
  scrape_links(links_arg_set)


def run(config):
  pages = config['promopages']
  page_arg_sets = [(url, slug, config) for slug, url in pages.iteritems()] 
  threaded_or_serial(page_arg_sets, scrape_promo_page, 5, 100)


########NEW FILE########
__FILENAME__ = article_extractor
#!/usr/bin/env python
# -*- coding: utf-8 -*-

from boilerpipe.extract import Extractor
from readability.readability import Document
import requests

from particle.helpers import *
import logging


def extract_article(url, config):
  
  r = requests.get(url)
  
  # the the url exists, continue
  if r.status_code == 200:
    
    # extract and parse response url
    url = parse_url(r.url)

    # extract html
    html = r.content.decode('utf-8', errors='ignore')

    # run boilerpipe
    BP = Extractor(html=html)

    # run readability
    Rdb = Document(html)

    # return article data
    return {
      'extracted_title': Rdb.short_title().strip(),
      'extracted_content': strip_tags(BP.getText()),
      'extracted_html': Rdb.summary()
    }

  # otherwise return an empty dict
  else:
    return {}

########NEW FILE########
__FILENAME__ = rssfeeds
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import feedparser
from thready import threaded
from datetime import datetime
from dateutil import parser
import json
import requests
import logging

# custom modules:
from particle.common import db, DEBUG
from particle.rssfeeds.article_extractor import extract_article
from particle.helpers import *

log = logging.getLogger('particle')

def parse_rss_date(entry):
  if entry.has_key('updated_parsed'):
    dt = entry['updated_parsed']
    return datetime(
        dt.tm_year, 
        dt.tm_mon, 
        dt.tm_mday, 
        dt.tm_hour, 
        dt.tm_min, 
        dt.tm_sec
      )
  elif entry.has_key('updated'):
    return parser.parse(entry('updated'))
  else:
    return None

def parse_one_entry(entry_arg_set):
  """
  parse an entry in an rss feed
  """
  entry, data_source, full_text, config = entry_arg_set

  # open url to get actual link
  if entry.has_key('link') and entry['link'] != '' and entry['link'] is not None:
    r = requests.get(entry['link'])
    if r.status_code == 200:
      # parse and sluggify url
      article_url = parse_url(r.url)
      article_slug = sluggify(article_url)

    else:
      article_url = None
      article_slug = None

    # check if this is a relevant article
    if is_article(article_url, config):
      
      # check if this key exists
      if not db.exists(article_slug + ":article"):

        # parse date
        dt = parse_rss_date(entry)
        if dt is not None:
          time_bucket = round_datetime(dt, config)
          raw_timestamp = int(dt.strftime('%s'))
          pub_date = dt.strftime('%Y-%m-%d %H:%M:%S')

        else:
          time_bucket = gen_time_bucket(config)
          raw_timestamp = current_timestamp(config)
          pub_date = None

        # parse_title
        if entry.has_key('title'):
          rss_title = entry['title']

        else:
          rss_title = None

        # parse content
        if entry.has_key('summary'):
          rss_content = strip_tags(entry['summary'])
          rss_html = entry['summary']
        else:
          rss_content = None

      # if feed is not full text, extract the article content
        # by crawling the page
        if not full_text:
          article_datum = extract_article(article_url, config)
          extracted_urls = extract_urls(article_datum['extracted_html'], config, False)
          extracted_imgs = extract_imgs(article_datum['extracted_html'])
        else:
          article_datum = {}
          extracted_urls = extract_urls(rss_html, config, False)
          extracted_imgs = extract_imgs(rss_html)

        # rss datum
        rss_datum = dict(
          article_slug = article_slug,
          article_url = article_url,
          time_bucket = time_bucket,
          raw_timestamp = raw_timestamp,
          rss_pub_date = pub_date,
          rss_raw_link = r.url,
          rss_title = rss_title,
          rss_content = rss_content,
          rss_html = rss_html,
          extracted_urls = extracted_urls,
          extracted_imgs = extracted_imgs
        )
        
        # merge data
        complete_datum = dict(
          rss_datum.items() + 
          article_datum.items()
        )

        value = json.dumps({data_source: complete_datum})
        log.info( "RSSFEEDS\tNew post on %s\t%s" % (data_source, article_url) )
        
        # upsert the data
        upsert_rss_pub(article_url, article_slug, value)
  

def parse_one_feed(feed_arg_set):
  feed_url, data_source, full_text, config = feed_arg_set
  """
  parse all the items in an rss feed
  """
  
  feed_data = feedparser.parse(feed_url)
  
  entries = feed_data['entries']
  entry_arg_sets = [(entry, data_source, full_text, config) for entry in entries]

  threaded_or_serial(entry_arg_sets, parse_one_entry, 30, 100)

def run(config):
  """
  parse all teh feedz
  """
  # generate args from config
  if not isinstance(config['rssfeeds'], dict):
    raise ParticleConfigException("""'rssfeeds' field must be a set of key/value pairs.

      """)

  feed_arg_sets = []
  for data_source, v in config['rssfeeds'].iteritems():
    if not v.has_key('full_text'):
      v['full_text'] = False
    feed_arg = (v['feed_url'], data_source, v['full_text'], config)
    feed_arg_sets.append(feed_arg)

  # thread that shit!
  threaded_or_serial(feed_arg_sets, parse_one_feed, 5, 100)

########NEW FILE########
__FILENAME__ = tumblr
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import pytumblr
import logging

def connect(config):
  return pytumblr.TumblrRestClient(
    config['tumblr']['consumer_key'], 
    config['tumblr']['consumer_secret'], 
    config['tumblr']['oauth_token'], 
    config['tumblr']['oauth_token_secret']
  )

########NEW FILE########
__FILENAME__ = twitter
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json
import tweepy
from thready import threaded
from datetime import datetime
import logging

from particle.twitter import twt
from particle.common import db, DEBUG
from particle.helpers import *

TWT_DATE_FORMAT = "%Y-%m-%d %H:%M:%S"

log = logging.getLogger('particle')


def parse_tweet(tweet_arg_set):
  slug, t, config = tweet_arg_set

  # check if id exists
  twt_id = t.id_str
  if not db.sismember('twitter_twt_ids', twt_id):

    # if not, add id to id_set
    db.sadd('twitter_twt_ids', twt_id)
    
    # check for relevant urls
    raw_urls = [u['expanded_url'] for u in t.entities['urls']]

    # parse urls
    article_urls = set([parse_url(unshorten_link(u, config)) for u in raw_urls])

    if any([is_article(u, config) for u in article_urls]):

      # parse dates
      # sometimes t.created_at is a datetime object
      if isinstance(t.created_at, datetime):
        dt = t.created_at
      else:
        dt = datetime.strptrim(t.created_at, TWT_DATE_FORMAT)
      
      date_time = tz_adj(dt, config)
      time_bucket = round_datetime(date_time, config) if date_time is not None else None
      
      raw_timestamp = int(date_time.strftime('%s'))
      

      for article_url in article_urls:
        # sluggify url
        article_slug = sluggify(article_url)
        screen_name = t.user.screen_name
        log.info( "TWITTER\tNew Tweet %s/%s\t%s" % (screen_name, twt_id, article_url) )

      # format data
        value = {
          'article_slug': article_slug,
          'article_url': article_url,
          'time_bucket': time_bucket,
          'raw_timestamp' :  raw_timestamp,
          'twt_list' : slug,
          'twt_post_created': raw_timestamp,
          'twt_id': twt_id,
          'twt_screen_name': t.user.screen_name,
          'twt_text': t.text,
          'twt_followers': t.author.followers_count,
          'twt_friends': t.author.friends_count,
          'twt_lang': t.lang,    
          'twt_raw_links': raw_urls,
          'twt_hashtags': t.entities['hashtags'],
          'twt_user_mentions': t.entities['user_mentions'],
          'twt_in_reply_to_screen_name': t.in_reply_to_screen_name,
          'twt_in_reply_to_status_id_str': t.in_reply_to_status_id_str
        }
        
        data_source = "twitter_%s" % slug
        
        # upsert url
        upsert_url(article_url, article_slug, data_source, config)

        value = json.dumps({ data_source : value})
        
        # add data to redis
        db.zadd(article_slug, time_bucket, value)


def parse_tweets(tweet_arg_sets):
  threaded_or_serial(tweet_arg_sets, parse_tweet, 5, 200)


def run(config):
  try:
    tweet_arg_sets = [
      (slug, t, config) 
      for l in twt.get_list_timelines(config) 
      for slug, tweets in l.iteritems()
      for t in tweets
    ]

  except tweepy.error.TweepError as e:
    log.error(e)

  else:
    parse_tweets(tweet_arg_sets)


########NEW FILE########
__FILENAME__ = twt
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import tweepy
import logging

from particle.helpers import *

log = logging.getLogger('particle')

def connect(config):
  """
  Given 4 Environmental Variables, Connect to Twitter
  """
  
  # load credentials
  consumer_key = config['twitter']['consumer_key']
  consumer_secret = config['twitter']['consumer_secret']
  access_token = config['twitter']['access_token']
  access_token_secret = config['twitter']['access_token_secret']

  # authenticate
  auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
  auth.set_access_token(access_token, access_token_secret)
  api = tweepy.API(auth)

  return api


def add_list_member(list_member_arg_set):

  screen_name, slug, owner_screen_name, api = list_member_arg_set

  try:
    api.add_list_member(
      owner_screen_name = owner_screen_name,
      slug = slug,
      user_id = screen_name
    )

  except:
    log.info( "%s doesn't exist" % screen_name )

  else:
    log.info( "TWT\tadding %s to list: %s for user: %s" % (screen_name, slug, owner_screen_name) )


def generate_list(api, slug, list_dict):

  # parse handles
  if isinstance(list_dict['screen_names'], basestring):
    screen_names = [
      sn.strip() 
      for sn in open(list_dict['screen_names']).read().split("\n") 
      if sn != '' and sn is not None 
    ]

  else:
    screen_names = [sn for sn in list_dict['screen_names'] if sn != '' and sn is not None]
  
  owner_screen_name = list_dict['owner']
  
  try:
    api.create_list(slug)

  except tweepy.error.TweepError as e:
    
    log.error( "ERROR\tTWT\t%s Already Exists for user %s" % (slug, owner_screen_name) )
    log.error( e )
    return None

  else:
    # get current list members and member ids:
    members = []
    for member in tweepy.Cursor(api.list_members, owner_screen_name, slug).items():
      
      members.append(member.screen_name)
      members.append(member.id_str)
    
    # add members if they aren't already in list
    list_member_arg_sets = [
      (screen_name, slug, owner_screen_name, api) 
      for screen_name in screen_names if screen_name not in members
    ]

    threaded_or_serial(list_member_arg_sets, add_list_member, 30, 200)


def generate_lists(config):
  
  api = connect(config)
  for slug, list_dict in config['twitter']['lists'].iteritems():
    if list_dict.has_key('screen_names'):
      log.info('TWITTER\tUpdating list %s' % slug)
      generate_list(api, slug, list_dict)


def get_list_timelines(config):
  api = connect(config)
  list_list = []
  for slug, list_dict in config['twitter']['lists'].iteritems():
    log.info( "TWITTER\tGetting new data for twitter.com/%s/lists/%s" % (list_dict['owner'], slug) )
    tweets = api.list_timeline(
            owner_screen_name = list_dict['owner'], 
            slug =  slug,
            count = list_dict['limit']
          )
    list_list.append({slug: [t for t in tweets]})

  return list_list

########NEW FILE########
__FILENAME__ = web
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import flask
import redis
import json
import re
from flask import Response, request
import dateutil.parser
import logging

from particle.helpers import *
from particle.common import db

urllib3_logger = logging.getLogger('urllib3')
urllib3_logger.setLevel(logging.CRITICAL)

app = flask.Flask(__name__)

class JSONEncoder(json.JSONEncoder):
  """ This encoder will serialize all entities that have a to_dict
  method by calling that method and serializing the result. """

  def encode(self, obj):
    if hasattr(obj, 'to_dict'):
      obj = obj.to_dict()
    return super(JSONEncoder, self).encode(obj)

  def default(self, obj):

    if hasattr(obj, 'as_dict'):
      return obj.as_dict()

    if isinstance(obj, datetime):
      return obj.isoformat()

    if hasattr(obj, 'to_dict'):
      return obj.to_dict()

    raise TypeError("%r is not JSON serializable" % obj)


def jsonify(obj, status=200, headers=None):
  """ Custom JSONificaton to support obj.to_dict protocol. """
  jsondata = json.dumps(obj, cls=JSONEncoder)

  if 'callback' in request.args:
    jsondata = '%s(%s)' % (request.args.get('callback'), jsondata)

  return Response(jsondata, headers=headers, status=status, mimetype='application/json')

@app.route("/")
def query():

  # parse args
  article_slug = sluggify(request.args.get('url', ''))
  data_sources = request.args.get('data_sources', 'all').split(",")
  start = request.args.get('start', 0)
  end = request.args.get('end', 1e11)
  timestamp = request.args.get('timestamp', None)
  order = request.args.get('order', 'desc')
  include_article = request.args.get('include_article', 'true').lower()
  include_keys = request.args.get('include_keys', 'true').lower()
  
  # pesky boolian problem
  if include_keys=='true':
    include_keys = True
  elif include_keys=='false':
    include_keys = False

  # parse article arg
  if include_article == 'true':
    include_article = True
  elif include_article =='false':
    include_article = False

  # timestamp override
  if timestamp is not None:
    start = timestamp
    end = timestamp

  # fetch data
  results = db.zrangebyscore(article_slug, min = start, max = end)

  # optionally filter out particular datasources
  if data_sources[0] != "all":
    filtered_results = []
    data = [json.loads(r) for r in results]
    for src in data_sources:
      for d in data:
        if d.has_key(src):
          if not include_keys:
            filtered_results.append(json.dumps(d.values()))
          else:
            filtered_results.append(json.dumps(d))
        else:
          for key in d.keys():
            if re.search(src, key):
              if not include_keys:
                filtered_results.append(json.dumps(d.values()))
              else:
                filtered_results.append(json.dumps(d))              

      results = filtered_results

  # optionally order database
  if order=='desc':
    results = [r for r in reversed(results)]
  
  # turn it into a json list
  results = "[%s]" % ",".join(results)

  # create article dict if requested
  if include_article:
    return_pattern = "{\"article\" : %s, \"events\" : %s }"
    article_data = db.get(article_slug+":article")
    
    if article_data is None:
      article_data = "{}"
    
    return return_pattern % (article_data, results)

  # otherwise just return json list
  else: 
    return results

@app.route("/recent-articles/")
def recent():
  
  # parse arg
  limit = request.args.get('limit', 50)

  # just look two days back
  end = current_timestamp()
  start = end - 24*60*60*2
  
  # fetch data
  key = 'article_sorted_set'
  results = db.zrangebyscore(key, min = start, max = end)

  # turn it into a json list
  return "[%s]" % ",".join(results[0:limit+1])

def api(host='0.0.0.0', port=3030, debug=True):
  app.debug = debug
  app.run(host=host, port=port)

########NEW FILE########
