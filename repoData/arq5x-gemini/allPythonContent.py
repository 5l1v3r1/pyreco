__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys
import os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('../'))

from gemini import __version__ as version
# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
# needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.doctest', 'sphinx.ext.intersphinx', 'sphinx.ext.todo',
              'sphinx.ext.coverage', 'sphinx.ext.pngmath', 'sphinx.ext.ifconfig', 'sphinx.ext.viewcode']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
# source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'gemini'
copyright = u'2012,2013'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = version
# The full version, including alpha/beta/rc tags.
release = version

print version
# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
# language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
# today = ''
# Else, today_fmt is used as the format for a strftime call.
# today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
# default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
# add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
# add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
# show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
# modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'rtd'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
# html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ["themes"]

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
html_short_title = project + " v" + release

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
html_logo = 'gemini.png'

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
html_favicon = 'gemini.png'
html_style = 'labibi.css'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
# html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
html_sidebars = {
    'index': ['sidebar-intro.html', 'sourcelink.html', 'searchbox.html']
}


# Additional templates that should be rendered to pages, maps page names to
# template names.
# html_additional_pages = {}

# If false, no module index is generated.
# html_domain_indices = True

# If false, no index is generated.
# html_use_index = True

# If true, the index is split into individual pages for each letter.
# html_split_index = False

# If true, links to the reST sources are added to the pages.
# html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
html_show_sphinx = False

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
# html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
# html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
# html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'gemini-docs'

# Google analytics
# googleanalytics_id = "UA-24167610-15"

# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
# latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
# latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
    ('index', 'gemini.tex', u'gemini Documentation', u'Quinlan lab @ UVa', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
# latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
# latex_use_parts = False

# If true, show page references after internal links.
# latex_show_pagerefs = False

# If true, show URL addresses after external links.
# latex_show_urls = False

# Additional stuff for the LaTeX preamble.
# latex_preamble = ''

# Documents to append as an appendix to all manuals.
# latex_appendices = []

# If false, no module index is generated.
# latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'gemini', u'gemini Documentation', [u'UVa'], 1)
]


# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {'http://docs.python.org/': None}


class Mock(object):
    def __init__(self, *args, **kwargs):
        pass

    def __call__(self, *args, **kwargs):
        return Mock()

    @classmethod
    def __getattr__(cls, name):
        if name in ('__file__', '__path__'):
            return '/dev/null'
        elif name[0] == name[0].upper():
            return type(name, (), {})
        else:
            return Mock()

MOCK_MODULES = ['numpy', 'matplotlib', 'matplotlib.pyplot',
                'matplotlib.sphinxext', 'matplotlib.sphinxext.plot_directive']
for mod_name in MOCK_MODULES:
    sys.modules[mod_name] = Mock()

########NEW FILE########
__FILENAME__ = annotations
#!/usr/bin/env python
import pysam
import sqlite3
import os
import sys
import collections
import re
from unidecode import unidecode
from bx.bbi.bigwig_file import BigWigFile
from gemini.config import read_gemini_config

# dictionary of anno_type -> open Tabix file handles
annos = {}

def get_anno_files():
    config = read_gemini_config()
    anno_dirname = config["annotation_dir"]
    # Default annotations -- always found
    annos = {
     'pfam_domain': os.path.join(anno_dirname, 'hg19.pfam.ucscgenes.bed.gz'),
     'cytoband': os.path.join(anno_dirname, 'hg19.cytoband.bed.gz'),
     'dbsnp': os.path.join(anno_dirname, 'dbsnp.138.vcf.gz'),
     'clinvar': os.path.join(anno_dirname, 'clinvar_20140303.vcf.gz'),
     'gwas': os.path.join(anno_dirname, 'hg19.gwas.bed.gz'),
     'rmsk': os.path.join(anno_dirname, 'hg19.rmsk.bed.gz'),
     'segdup': os.path.join(anno_dirname, 'hg19.segdup.bed.gz'),
     'conserved': os.path.join(anno_dirname, '29way_pi_lods_elements_12mers.chr_specific.fdr_0.1_with_scores.txt.hg19.merged.bed.gz'),
     'cpg_island': os.path.join(anno_dirname, 'hg19.CpG.bed.gz'),
     'dgv': os.path.join(anno_dirname, 'hg19.dgv.bed.gz'),
     'esp': os.path.join(anno_dirname,
                         'ESP6500SI.all.snps_indels.vcf.gz'),
     '1000g': os.path.join(anno_dirname,
                           'ALL.wgs.integrated_phase1_v3.20101123.snps_indels_sv.sites.2012Oct12.vcf.gz'),
     'recomb': os.path.join(anno_dirname,
                            'genetic_map_HapMapII_GRCh37.gz'),
     'gms': os.path.join(anno_dirname,
                         'GRCh37-gms-mappability.vcf.gz'),
     'grc': os.path.join(anno_dirname, 'GRC_patch_regions.bed.gz'),
     'cse': os.path.join(anno_dirname, "cse-hiseq-8_4-2013-02-20.bed.gz"),
     'encode_tfbs': os.path.join(anno_dirname,
                                 'wgEncodeRegTfbsClusteredV2.cell_count.20130213.bed.gz'),
     'encode_dnase1': os.path.join(anno_dirname,
                                   'stam.125cells.dnaseI.hg19.bed.gz'),
     'encode_consensus_segs': os.path.join(anno_dirname,
                                           'encode.6celltypes.consensus.bedg.gz'),
     'gerp_elements': os.path.join(anno_dirname, 'hg19.gerp.elements.bed.gz'),
     'vista_enhancers': os.path.join(anno_dirname, 'hg19.vista.enhancers.20131108.bed.gz'),
     'cosmic': os.path.join(anno_dirname, 'hg19.cosmic.v67.20131024.gz')
    }
    # optional annotations
    if os.path.exists(os.path.join(anno_dirname, 'hg19.gerp.bw')):
        annos['gerp_bp'] = os.path.join(anno_dirname, 'hg19.gerp.bw')
    if os.path.exists(os.path.join(anno_dirname, 'whole_genome_SNVs.tsv.compressed.gz')):
        annos['cadd_score'] = os.path.join(anno_dirname, 'whole_genome_SNVs.tsv.compressed.gz')
    return annos

class ClinVarInfo(object):
    def __init__(self):
        self.clinvar_dbsource = None
        self.clinvar_dbsource_id = None
        self.clinvar_origin = None
        self.clinvar_sig = None
        self.clinvar_dsdb = None
        self.clinvar_dsdbid = None
        self.clinvar_disease_name = None
        self.clinvar_disease_acc = None
        self.clinvar_in_omim = None
        self.clinvar_in_locus_spec_db = None
        self.clinvar_on_diag_assay = None

        self.origin_code_map = {'0': 'unknown',
                                '1': 'germline',
                                '2': 'somatic',
                                '4': 'inherited',
                                '8': 'paternal',
                                '16': 'maternal',
                                '32': 'de-novo',
                                '64': 'biparental',
                                '128': 'uniparental',
                                '256': 'not-tested',
                                '512': 'tested-inconclusive',
                                '1073741824': 'other'}

        self.sig_code_map = {'0': 'unknown',
                             '1': 'untested',
                             '2': 'non-pathogenic',
                             '3': 'probable-non-pathogenic',
                             '4': 'probable-pathogenic',
                             '5': 'pathogenic',
                             '6': 'drug-response',
                             '7': 'histocompatibility',
                             '255': 'other'}

    def __repr__(self):
        return '\t'.join([self.clinvar_dbsource,
                          self.clinvar_dbsource_id,
                          self.clinvar_origin,
                          self.clinvar_sig,
                          self.clinvar_dsdb,
                          self.clinvar_dsdbid,
                          self.clinvar_disease_name,
                          self.clinvar_disease_acc,
                          str(self.clinvar_in_omim),
                          str(self.clinvar_in_locus_spec_db),
                          str(self.clinvar_on_diag_assay)])

    def lookup_clinvar_origin(self, origin_code):
        try:
            return self.origin_code_map[origin_code]
        except KeyError:
            return None

    def lookup_clinvar_significance(self, sig_code):
        if "|" not in sig_code:
            try:
                return self.sig_code_map[sig_code]
            except KeyError:
                return None
        else:
            sigs = set(sig_code.split('|'))
            # e.g., 255|255|255
            if len(sigs) == 1:
                try:
                    return self.sig_code_map[sigs.pop()]
                except KeyError:
                    return None
            # e.g., 1|5|255
            else:
                return "mixed"


ESPInfo = collections.namedtuple("ESPInfo",
                                 "found \
                                  aaf_EA \
                                  aaf_AA \
                                  aaf_ALL \
                                  exome_chip")
ENCODEDnaseIClusters = collections.namedtuple("ENCODEDnaseIClusters",
                                              "cell_count \
                                         cell_list")
ENCODESegInfo = collections.namedtuple("ENCODESegInfo",
                                       "gm12878 \
                                         h1hesc \
                                         helas3 \
                                         hepg2 \
                                         huvec \
                                         k562")
ThousandGInfo = collections.namedtuple("ThousandGInfo",
                                       "found \
                                        aaf_ALL \
                                        aaf_AMR \
                                        aaf_ASN \
                                        aaf_AFR \
                                        aaf_EUR")

def load_annos():
    """
    Populate a dictionary of Tabixfile handles for
    each annotation file.  Other modules can then
    access a given handle and fetch data from it
    as follows:

    dbsnp_handle = annotations.annos['dbsnp']
    hits = dbsnp_handle.fetch(chrom, start, end)
    """
    anno_files = get_anno_files()
    for anno in anno_files: 
        try:
            # .gz denotes Tabix files.
            if anno_files[anno].endswith(".gz"):
                annos[anno] = pysam.Tabixfile(anno_files[anno])
            # .bw denotes BigWig files.
            elif anno_files[anno].endswith(".bw"):
                annos[anno] = BigWigFile( open( anno_files[anno] ) )

        except IOError:
            sys.exit("Gemini cannot open this annotation file: %s. \n"
                     "Have you installed the annotation files?  If so, "
                     "have they been moved or deleted? Exiting...\n\n"
                     "For more details:\n\t"
                     "http://gemini.readthedocs.org/en/latest/content/"
                     "#installation.html\#installing-annotation-files\n"
                     % anno_files[anno])

# ## Standard access to Tabix indexed files


def _get_hits(coords, annotation, parser_type):
    """Retrieve BED information, recovering if BED annotation file does have a chromosome.
    """
    if parser_type == "bed":
        parser = pysam.asBed()
    elif parser_type == "vcf":
        parser = pysam.asVCF()
    elif parser_type == "tuple":
        parser = pysam.asTuple()
    elif parser_type is None:
        parser = None
    else:
        raise ValueError("Unexpected parser type: %s" % parser)
    chrom, start, end = coords
    try:
        hit_iter = annotation.fetch(str(chrom), start, end, parser=parser)
    # catch invalid region errors raised by ctabix
    except ValueError:
        hit_iter = []
    # recent versions of pysam return KeyError
    except KeyError:
        hit_iter = []
    return hit_iter

def _get_bw_summary(coords, annotation):
    """Return summary of BigWig scores in an interval
    """
    chrom, start, end = coords
    try:
        return annotation.summarize(str(chrom), start, end, end-start).min_val[0]
    except AttributeError:
        return None


def _get_chr_as_grch37(chrom):
    if chrom in ["chrM"]:
        return "MT"
    return chrom if not chrom.startswith("chr") else chrom[3:]


def _get_chr_as_ucsc(chrom):
    return chrom if chrom.startswith("chr") else "chr" + chrom


def guess_contig_naming(anno):
    """Guess which contig naming scheme a given annotation file uses.
    """
    chr_names = [x for x in anno.contigs if x.startswith("chr")]
    if len(chr_names) > 0:
        return "ucsc"
    else:
        return "grch37"


def _get_var_coords(var, naming):
    """Retrieve variant coordinates from multiple input objects.
    """
    if isinstance(var, dict) or isinstance(var, sqlite3.Row):
        chrom = var["chrom"]
        start = int(var["start"])
        end = int(var["end"])
    else:
        chrom = var.CHROM
        start = var.start
        end = var.end
    if naming == "ucsc":
        chrom = _get_chr_as_ucsc(chrom)
    elif naming == "grch37":
        chrom = _get_chr_as_grch37(chrom)
    return chrom, start, end

def _get_cadd_scores(var, labels, hit):
    """
    get cadd scores
    """
    raw = hit[3].split(",")
    scaled = hit[4].split(",")
    
    p = re.compile(str(var.ALT[0]))
    for m in p.finditer(str(labels[hit[2]])):
        pos = m.start()
        return raw[pos], scaled[pos]
        

def annotations_in_region(var, anno, parser_type=None, naming="ucsc"):
    """Iterator of annotations found in a genomic region.

    - var: PyVCF object or database query with chromosome, start and end.
    - anno: pysam Tabix annotation file or string to reference
            a standard annotation
    - parser_type: string specifying the filetype of the tabix file
    - naming: chromosome naming scheme used, ucsc or grch37
    """
    coords = _get_var_coords(var, naming)
    if isinstance(anno, basestring):
        anno = annos[anno]
    return _get_hits(coords, anno, parser_type)


def bigwig_summary(var, anno, naming="ucsc"):
    coords = _get_var_coords(var, naming)
    if isinstance(anno, basestring):
        anno = annos[anno]
    return _get_bw_summary(coords, anno)


# ## Track-specific annotations
def get_cpg_island_info(var):
    """
    Returns a boolean indicating whether or not the
    variant overlaps a CpG island
    """
    for hit in annotations_in_region(var, "cpg_island", "bed"):
        return True
    return False

# def get_dbNSFP_info(var, impacts):
#     """
#     Returns Polyphen, SIFT, etc. from dbNSFP annotation file.
#     One prediction per transcript.
    
#     LIMITATION: only handles bi-allelic loci
#     """

#     # is this variant predicted to be nonsynonymous for any of the transcripts?
#     # if not, we can skip dnNSFP.
#     non_syn_impacts = [imp for imp in impacts \
#                                if imp.consequence == 'non_syn_coding']

#     if len(non_syn_impacts) > 0:
#         for hit in annotations_in_region(var, "dbnsfp", parser_type="tuple", naming="grch37"):

#             if var.POS == int(hit[1]) and \
#                var.REF == hit[2] and \
#                var.ALT[0] == hit[3]:
                
#                 transcripts = hit[7].split(';')
#                 aapos = hit[8].split(';')
#                 pp_scores = hit[11].split(';')

#                 if len(transcripts) != len(pp_scores):
#                     print var.POS, var.REF, var.ALT[0], [i.transcript for i in non_syn_impacts], \
#                           [i.polyphen_pred for i in non_syn_impacts], [i.polyphen_score for i in non_syn_impacts], \
#                           hit[7], hit[8], hit[11], hit[12]
#     else:
#         pass

def get_cyto_info(var):
    """
    Returns a comma-separated list of the chromosomal
    cytobands that a variant overlaps.
    """
    cyto_band = ''
    for hit in annotations_in_region(var, "cytoband", "bed"):
        if len(cyto_band) > 0:
            cyto_band += "," + hit.contig + hit.name
        else:
            cyto_band += hit.contig + hit.name
    return cyto_band if len(cyto_band) > 0 else None

def get_gerp_bp(var):
    """
    Returns a summary of the GERP scores for the variant.
    """
    if "gerp_bp" not in annos:
        raise IOError("Need to download BigWig file with GERP scores per base pair. "
                      "Run `gemini update --dataonly --extra gerp_bp")
    gerp = bigwig_summary(var, "gerp_bp")
    return gerp

def get_gerp_elements(var):
    """
    Returns the GERP element information.
    """
    p_vals = []
    for hit in annotations_in_region(var, "gerp_elements", "tuple"):
        p_vals.append(hit[3])
    if len(p_vals) == 1:
        return p_vals[0]
    elif len(p_vals) > 1:
        return min(float(p) for p in p_vals)
    else:
        return None

def get_vista_enhancers(var):
    """
    Returns the VISTA enhancer information.
    """
    vista_enhancers = []
    for hit in annotations_in_region(var, "vista_enhancers", "tuple"):
        vista_enhancers.append(hit[4])
    return ",".join(vista_enhancers) if len(vista_enhancers) > 0 else None


def get_cadd_scores(var):
    """
    Returns the C-raw scores & scaled scores (CADD) to predict deleterious
    variants. Implemented only for SNV's
    """
    if "cadd_score" not in annos:
        raise IOError("Need to download the CADD data file for deleteriousness."
                      "Run `gemini update --dataonly --extra cadd_score")

    cadd_raw = cadd_scaled = None
    labels = {"A":"CGT", "C":"AGT", "G":"ACT", "T":"ACG", "R":"ACGT", "M":"ACGT"}
    
    for hit in annotations_in_region(var, "cadd_score", "tuple", "grch37"):
        # we want exact position mapping here and not a range (end-start) as
        # returned in hit (e.g. indels) & we do not want to consider del & ins
        if str(hit[1]) == str(var.POS) and len(var.REF) == 1 and \
           len(var.ALT[0]) == 1:
           
            if str(hit[2]) == var.REF and str(var.ALT[0]) in labels[hit[2]]:
               (cadd_raw, cadd_scaled) = _get_cadd_scores(var, labels, hit)
            
            # consider ref cases with ambiguity codes R (G,A) and M (A,C)
            elif ((str(hit[2]) == 'R'  and var.REF in('G','A')) or \
                (str(hit[2]) == 'M'  and var.REF in('A','C'))) and \
                str(var.ALT[0]) in labels[hit[2]]:
                (cadd_raw, cadd_scaled) = _get_cadd_scores(var, labels, hit)
    
    return (cadd_raw, cadd_scaled)
     
    
def get_pfamA_domains(var):
    """
    Returns pfamA domains that a variant overlaps
    """
    pfam_domain = []
    for hit in annotations_in_region(var, "pfam_domain", "bed"):
        pfam_domain.append(hit.name)
    return ",".join(pfam_domain) if len(pfam_domain) > 0 else None


def get_cosmic_info(var):
    """
    Returns a list of COSMIC ids associated with given variant

    E.g. from COSMIC VCF
    #CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO
    chrM    1747    COSN408408      G       A       .       .       .
    chrM    2700    COSN408409      G       A       .       .       .
    chr1    42880262    COSM464635  G   C   .   .   AA=p.D224H;CDS=c.670G>C;CNT=1;GENE=RIMKLA;STRAND=+
    chr1    42880269    COSM909628  G   A   .   .   AA=p.G226D;CDS=c.677G>A;CNT=1;GENE=RIMKLA;STRAND=+
    chr1    42880284    COSM1502979 G   T   .   .   AA=p.C231F;CDS=c.692G>T;CNT=1;GENE=RIMKLA;STRAND=+
    chr1    42880318    COSM681351  T   A   .   .   AA=p.F242L;CDS=c.726T>A;CNT=1;GENE=RIMKLA;STRAND=+
    chr1    42880337    COSM464636  G   A   .   .   AA=p.D249N;CDS=c.745G>A;CNT=1;GENE=RIMKLA;STRAND=+
    chr1    42880384    COSM909629  T   C   .   .   AA=p.N264N;CDS=c.792T>C;CNT=1;GENE=RIMKLA;STRAND=+
    chr1    42880415    COSM909630  G   C   .   .   AA=p.G275R;CDS=c.823G>C;CNT=1;GENE=RIMKLA;STRAND=+
    """
    # report the first overlapping ClinVar variant Most often, just one).
    cosmic_ids = []
    for hit in annotations_in_region(var, "cosmic", "vcf", "ucsc"):
        cosmic_ids.append(hit.id)
    return ",".join(cosmic_ids) if len(cosmic_ids) > 0 else None


def get_clinvar_info(var):
    """
    Returns a suite of annotations from ClinVar

    ClinVarInfo named_tuple:
    --------------------------------------------------------------------------
    # clinvar_dbsource         = CLNSRC=OMIM Allelic Variant;
    # clinvar_dbsource_id      = CLNSRCID=103320.0001;
    # clinvar_origin           = CLNORIGIN=1
    # clinvar_sig              = CLNSIG=5
    # clinvar_dsdb             = CLNDSDB=GeneReviews:NCBI:OMIM:Orphanet;
    # clinvar_dsdbid           = CLNDSDBID=NBK1168:C1850792:254300:590;
    # clinvar_disease_name     = CLNDBN=Myasthenia\x2c limb-girdle\x2c familial;
    # clinvar_disease_acc      = CLNACC=RCV000019902.1
    # clinvar_in_omim          = OM
    # clinvar_in_locus_spec_db = LSD
    # clinvar_on_diag_assay    = CDA
    """

    clinvar = ClinVarInfo()

    # report the first overlapping ClinVar variant Most often, just one).
    for hit in annotations_in_region(var, "clinvar", "vcf", "grch37"):
        # load each VCF INFO key/value pair into a DICT
        info_map = {}
        for info in hit.info.split(";"):
            if info.find("=") > 0:
                (key, value) = info.split("=")
                info_map[key] = value
            else:
                info_map[info] = True

        raw_dbsource = info_map['CLNSRC'] or None
        #interpret 8-bit strings and convert to plain text
        clinvar.clinvar_dbsource = unidecode(raw_dbsource.decode('utf-8'))
        clinvar.clinvar_dbsource_id = info_map['CLNSRCID'] or None
        clinvar.clinvar_origin           = \
            clinvar.lookup_clinvar_origin(info_map['CLNORIGIN'])
        clinvar.clinvar_sig              = \
            clinvar.lookup_clinvar_significance(info_map['CLNSIG'])
        clinvar.clinvar_dsdb = info_map['CLNDSDB'] or None
        clinvar.clinvar_dsdbid = info_map['CLNDSDBID'] or None
        # Remap all unicode characters into plain text string replacements
        raw_disease_name = info_map['CLNDBN'] or None
        clinvar.clinvar_disease_name = unidecode(raw_disease_name.decode('utf-8'))
        # Clinvar represents commas as \x2c.  Make them commas.
        clinvar.clinvar_disease_name = clinvar.clinvar_disease_name.decode('string_escape')

        clinvar.clinvar_disease_acc = info_map['CLNACC'] or None
        clinvar.clinvar_in_omim = 1 if 'OM' in info_map else 0
        clinvar.clinvar_in_locus_spec_db = 1 if 'LSD' in info_map else 0
        clinvar.clinvar_on_diag_assay = 1 if 'CDA' in info_map else 0

    return clinvar


def get_dbsnp_info(var):
    """
    Returns a suite of annotations from dbSNP
    """
    rs_ids = []
    for hit in annotations_in_region(var, "dbsnp", "vcf", "grch37"):
        rs_ids.append(hit.id)
        # load each VCF INFO key/value pair into a DICT
        info_map = {}
        for info in hit.info.split(";"):
            if info.find("=") > 0:
                (key, value) = info.split("=")
                info_map[key] = value

    return ",".join(rs_ids) if len(rs_ids) > 0 else None


def get_esp_info(var):
    """
    Returns a suite of annotations from the ESP project

    ESP reports the minor allele frequency (MAF), not the
    alternate allele frequency (AAF). We must therefore figure
    out whther the reference or alternate allele is the minor allele.

    1       69496   rs150690004     G       A       .       PASS    DBSNP=dbSNP_134;EA_AC=2,6764;AA_AC=23,3785;TAC=25,10549;MAF=0.0296,0.604,0.2364;GTS=AA,AG,GG;EA_GTC=0,2,3381;AA_GTC=5,13,1886;GTC=5,15,5267;DP=91;GL=OR4F5;CP=0.5;CG=2.3;AA=G;CA=.;EXOME_CHIP=no;GWAS_PUBMED=.;GM=NM_001005484.1;FG=missense;AAC=SER/GLY;PP=136/306;CDP=406;GS=56;PH=benign
    1       69511   rs75062661      A       G       .       PASS    DBSNP=dbSNP_131;EA_AC=5337,677;AA_AC=1937,1623;TAC=7274,2300;MAF=11.2571,45.5899,24.0234;GTS=GG,GA,AA;EA_GTC=2430,477,100;AA_GTC=784,369,627;GTC=3214,846,727;DP=69;GL=OR4F5;CP=1.0;CG=1.1;AA=G;CA=.;EXOME_CHIP=no;GWAS_PUBMED=.;GM=NM_001005484.1;FG=missense;AAC=ALA/THR;PP=141/306;CDP=421;GS=58;PH=benign
    """
    aaf_EA = aaf_AA = aaf_ALL = None
    maf = fetched = con = []
    exome_chip = False
    found = False
    info_map = {}
    for hit in annotations_in_region(var, "esp", "vcf", "grch37"):
        if hit.contig not in ['Y']:
            fetched.append(hit)
            # We need a single ESP entry for a variant
            if fetched != None and len(fetched) == 1 and \
                    hit.alt == var.ALT[0] and hit.ref == var.REF:
                found = True
                # loads each VCF INFO key/value pair into a DICT
                for info in hit.info.split(";"):
                    if info.find("=") > 0:
                    # splits on first occurence of '='
                    # useful to handle valuerror: too many values to unpack (e.g (a,b) = split(",", (a,b,c,d)) for cases like
                    # SA=http://www.ncbi.nlm.nih.gov/sites/varvu?gene=4524&amp%3Brs=1801131|http://omim.org/entry/607093#0004
                        (key, value) = info.split("=", 1)
                        info_map[key] = value

                # get the allele counts so that we can compute alternate allele frequencies
                # example: EA_AC=2,6764;AA_AC=23,3785;TAC=25,10549
                if info_map.get('EA_AC') is not None:
                    lines = info_map['EA_AC'].split(",")
                    aaf_EA = float(lines[0]) / (float(lines[0]) + float(lines[1]))

                if info_map.get('AA_AC') is not None:
                    lines = info_map['AA_AC'].split(",")
                    aaf_AA = float(lines[0]) / (float(lines[0]) + float(lines[1]))

                if info_map.get('TAC') is not None:
                    lines = info_map['TAC'].split(",")
                    aaf_ALL = float(lines[0]) / (float(lines[0]) + float(lines[1]))

                # Is the SNP on an human exome chip?
                if info_map.get('EXOME_CHIP') is not None and \
                        info_map['EXOME_CHIP'] == "no":
                    exome_chip = 0
                elif info_map.get('EXOME_CHIP') is not None and \
                        info_map['EXOME_CHIP'] == "yes":
                    exome_chip = 1

    return ESPInfo(found, aaf_EA, aaf_AA, aaf_ALL, exome_chip)


def get_1000G_info(var):
    """
    Returns a suite of annotations from the 1000 Genomes project
    """
    #fetched = []
    info_map = {}
    found = False

    for hit in annotations_in_region(var, "1000g", "vcf", "grch37"):
        # We need to ensure we are dealing with the exact sample variant
        # based on position and the alleles present.
        if var.start == hit.pos and \
           var.ALT[0] == hit.alt and \
           hit.ref == var.REF:
            for info in hit.info.split(";"):
                if info.find("=") > 0:
                    (key, value) = info.split("=", 1)
                    info_map[key] = value
            found = True

    return ThousandGInfo(found, info_map.get('AF'), info_map.get('AMR_AF'),
                         info_map.get('ASN_AF'), info_map.get('AFR_AF'),
                         info_map.get('EUR_AF'))


def get_rmsk_info(var):
    """
    Returns a comma-separated list of annotated repeats
    that overlap a variant.  Derived from the UCSC rmsk track
    """
    rmsk_hits = []
    for hit in annotations_in_region(var, "rmsk", "bed"):
        rmsk_hits.append(hit.name)
    return ",".join(rmsk_hits) if len(rmsk_hits) > 0 else None


def get_segdup_info(var):
    """
    Returns a boolean indicating whether or not the
    variant overlaps a known segmental duplication.
    """
    for hit in annotations_in_region(var, "segdup", "bed"):
        return True
    return False


def get_conservation_info(var):
    """
    Returns a boolean indicating whether or not the
    variant overlaps a conserved region as defined
    by the 29-way mammalian conservation study.
    http://www.nature.com/nature/journal/v478/n7370/full/nature10530.html

    Data file provenance:
    http://www.broadinstitute.org/ftp/pub/assemblies/mammals/29mammals/ \
    29way_pi_lods_elements_12mers.chr_specific.fdr_0.1_with_scores.txt.gz

    # Script to convert for gemini:
    gemini/annotation_provenance/make-29way-conservation.sh
    """
    for hit in annotations_in_region(var, "conserved", "bed"):
        return True
    return False


def get_recomb_info(var):
    """
    Returns the mean recombination rate at the site.
    """
    count = 0
    tot_rate = 0.0
    for hit in annotations_in_region(var, "recomb", "bed"):
        if hit.contig not in ['chrY']:
        # recomb rate file is in bedgraph format.
        # pysam will store the rate in the "name" field
            count += 1
            tot_rate += float(hit.name)

    return float(tot_rate) / float(count) if count > 0 else None


def _get_first_vcf_hit(hit_iter):
    if hit_iter is not None:
        hits = list(hit_iter)
        if len(hits) > 0:
            return hits[0]


def _get_vcf_info_attrs(hit):
    info_map = {}
    for info in hit.info.split(";"):
        if info.find("=") > 0:
            (key, value) = info.split("=", 1)
            info_map[key] = value
    return info_map


def get_gms(var):
    """Return Genome Mappability Scores for multiple technologies.
    """
    techs = ["illumina", "solid", "iontorrent"]
    GmsTechs = collections.namedtuple("GmsTechs", techs)
    hit = _get_first_vcf_hit(
        annotations_in_region(var, "gms", "vcf", "grch37"))
    attr_map = _get_vcf_info_attrs(hit) if hit is not None else {}
    return apply(GmsTechs,
                 [attr_map.get("GMS_{0}".format(x), None) for x in techs])


def get_grc(var):
    """Return GRC patched genome regions.
    """
    regions = set()
    for hit in annotations_in_region(var, "grc", "bed", "grch37"):
        regions.add(hit.name)
    return ",".join(sorted(list(regions))) if len(regions) > 0 else None

def get_cse(var):
    """Return if a variant is in a CSE: Context-specific error region.
    """
    for hit in annotations_in_region(var, "cse", "bed", "grch37"):
        return True
    return False

def get_encode_tfbs(var):
    """
    Returns a comma-separated list of transcription factors that were
    observed to bind DNA in this region.  Each hit in the list is constructed
    as TF_CELLCOUNT, where:
      TF is the transcription factor name
      CELLCOUNT is the number of cells tested that had nonzero signals

    NOTE: the annotation file is in BED format, but pysam doesn't
    tolerate BED files with more than 12 fields, so we just use the base
    tuple parser and grab the name column (4th column)
    """
    tfbs = []
    for hit in annotations_in_region(var, "encode_tfbs", "tuple"):
        tfbs.append(hit[3] + "_" + hit[4])
    if len(tfbs) > 0:
        return ','.join(tfbs)
    else:
        return None


def get_encode_dnase_clusters(var):
    """
    If a variant overlaps a DnaseI cluster, return the number of cell types
    that were found to have DnaseI HS at in the given interval, as well
    as a comma-separated list of each cell type:

    Example data:
    chr1	20042385	20042535	4	50.330600	8988t;K562;Osteobl;hTH1
    chr1	20043060	20043210	3	12.450500	Gm12891;T47d;hESCT0
    chr1	20043725	20043875	2	5.948180	Fibrobl;Fibrop
    chr1	20044125	20044275	3	6.437350	HESC;Ips;hTH1
    """
    for hit in annotations_in_region(var, "encode_dnase1", "tuple"):
        return ENCODEDnaseIClusters(hit[3], hit[5])
    return ENCODEDnaseIClusters(None, None)


def get_encode_consensus_segs(var):
    """
    Queries a meta-BEDGRAPH of consensus ENCODE segmentations for 6 cell types:
    gm12878, h1hesc, helas3, hepg2, huvec, k562

    Returns a 6-tuple of the predicted chromatin state of each cell type for the
    region overlapping the variant.

    CTCF: CTCF-enriched element
    E:    Predicted enhancer
    PF:   Predicted promoter flanking region
    R:    Predicted repressed or low-activity region
    TSS:  Predicted promoter region including TSS
    T:    Predicted transcribed region
    WE:   Predicted weak enhancer or open chromatin cis-regulatory element
    """
    for hit in annotations_in_region(var, "encode_consensus_segs", "tuple"):
        return ENCODESegInfo(hit[3], hit[4], hit[5], hit[6], hit[7], hit[8])

    return ENCODESegInfo(None, None, None, None, None, None)


def get_encode_segway_segs(var):
    """
    Queries a meta-BEDGRAPH of SegWay ENCODE segmentations for 6 cell types:
    gm12878, h1hesc, helas3, hepg2, huvec, k562

    Returns a 6-tuple of the predicted chromatin state of each cell type for the
    region overlapping the variant.
    """
    for hit in annotations_in_region(var, "encode_segway_segs", "tuple"):
        return ENCODESegInfo(hit[3], hit[4], hit[5], hit[6], hit[7], hit[8])

    return ENCODESegInfo(None, None, None, None, None, None)


def get_encode_chromhmm_segs(var):
    """
    Queries a meta-BEDGRAPH of SegWay ENCODE segmentations for 6 cell types:
    gm12878, h1hesc, helas3, hepg2, huvec, k562

    Returns a 6-tuple of the predicted chromatin state of each cell type for the
    region overlapping the variant.
    """
    for hit in annotations_in_region(var, "encode_chromhmm_segs", "tuple"):
        return ENCODESegInfo(hit[3], hit[4], hit[5], hit[6], hit[7], hit[8])

    return ENCODESegInfo(None, None, None, None, None, None)


def get_resources():
    """Retrieve list of annotation resources loaded into gemini.
    """
    anno_files = get_anno_files()
    return [(n, os.path.basename(anno_files[n])) for n in sorted(anno_files.keys())]

########NEW FILE########
__FILENAME__ = combined_gene_table
"""
For a detailed gene table and a summary gene table
"""

#!/usr/bin/env python

import sys
import os
import itertools
from collections import defaultdict
from sets import Set


filename = 'detailed_gene_table_v75'
detailed_out = open(filename, 'w')

file = 'summary_gene_table_v75'
summary_out = open(file, 'w')

# write out files for detailed and summary gene table
detailed_out.write("\t".join(["Chromosome","Gene_name","Is_hgnc","Ensembl_gene_id","Ensembl_transcript_id","Biotype",
                              "Transcript_status","CCDS_id","HGNC_id","CDS_length","Protein_length",
                              "Transcript_start","Transcript_end","strand","Synonyms", 
                              "Rvis_pct","entrez_gene_id","mammalian_phenotype_id"]))     
detailed_out.write("\n")

summary_out.write("\t".join(["Chromosome","Gene_name","Is_hgnc","Ensembl_gene_id",
                         "HGNC_id","Synonyms", "Rvis_pct","Strand","Transcript_min_start","Transcript_max_end","Mammalian_phenotype_id"]))     
summary_out.write("\n")


mouse_phenotype = defaultdict(list)
genic_intolerance = defaultdict(list)
keygene = list_hgnc = []

#initializing values for the summary gene table
transcript_min = defaultdict(list)
transcript_max = defaultdict(list)
lines_seen = set()


for line in open("genic_intolerance_dataset2", 'r'):
    if line.startswith("#") is False:
        field = line.strip().split("\t")
        name = str(field[0])
        score =  str(field[1])
        percentile = str(field[2])
        (key,value) = (name, percentile)
        genic_intolerance[name].append(percentile)

#Phenotype data from MGI - Jax
for row in open("HMD_HumanPhenotype", 'r'):
    col = row.strip().split("\t")
    #Remove leading white spaces in the column
    entrez_id = str(col[1]).lstrip()
    #Remove leading white spaces in the column & join MP terms with a comma
    mph = str(col[5]).lstrip().replace(' ',',') if str(col[5]) != '' else None
    (key,value) = (entrez_id, mph)
    mouse_phenotype[entrez_id].append(mph)

# Dictionary for summary gene table to handle transcript min, max co-ordinates
for each in open("raw_gene_table", 'r'):
    if each.startswith("Chromosome") is False:
        k = each.strip().split("\t")
        chr = "chr"+str((k[0]))
        ens = str(k[2])
        start = str(k[10])
        end = str(k[11])
        transcript_min[(chr,ens)].append(start)
        transcript_max[(chr,ens)].append(end)


for each in open("raw_gene_table", 'r'):
    if each.startswith("Chromosome") is False:
        k = each.strip().split("\t")
        chrom = "chr"+str((k[0]))
        hgnc = str(k[1])
        ens_geneid = str(k[2])
        ens_transid = str(k[3])
        trans_biotype = str(k[4])
        status = str(k[5])
        ccds_id = str(k[6]) #these id's are unique to transcripts
        hgnc_id = str(k[7])
        cds_len = str(k[8])
        protein_len = str(k[9])
        transcript_start = str(k[10])
        transcript_end = str(k[11])
        strand = str(k[12])
        #remove space between names
        previous =  str(k[13]).replace(" ","")
        synonyms = str(k[14]).replace(" ","")
        entrez = str(k[15])
        # sort all transcript start and end positions for a gene (use ens_geneid, since HGNC is not always true)
        # Capture the first and the last position from the sorted list to give min, max
        if (chrom,ens_geneid) in transcript_min:
            minmum = sorted(transcript_min[(chrom,ens_geneid)])[0]
        if (chrom,ens_geneid) in transcript_max:
            maxmum = sorted(transcript_max[(chrom,ens_geneid)])[-1]
        
        
        rvis = genic_intolerance[hgnc][0] if hgnc in genic_intolerance else None
        pheno = mouse_phenotype[entrez] if entrez in mouse_phenotype else None
        
        if pheno is not None and len(pheno) == 1:
            phenotype = pheno[0]
        elif pheno is None:
            phenotype = "None"
        else:
            if len(pheno) > 1:
                #convert the list to a string
                string = ",".join(pheno)
                # store a None for multiple Nones
                if "None" in string and "MP:" not in string:
                    phenotype = None
                #remove redundancy in MP terms  
                if "None" not in string and "MP:" in string:
                    phenotype = ",".join(set(string.split(",")))
                #remove nones when MP terms are available
                if "None" in string and "MP:" in string:
                    phen = string.split(",")
                    phenotype = ",".join([x for x in phen if x != "None"])
        
        if hgnc != "None":
            list_hgnc.append(hgnc)
        #we don't want string of Nones
        if "None" in previous and "None" in synonyms and "None" in hgnc:
            string = None
        else:
            # We would like all genes names to be put together
            gene_string = hgnc+","+previous+","+synonyms
            
            #get rid of Nones in gene strings
            if gene_string.startswith("None"):
                string = gene_string.replace("None,","")
            else:
                string = gene_string.replace(",None","")

        #Nonetype object has no attribute split
        if string is not None:
            genes = set(string.split(","))
            if len(genes) > 1:
            # We would like to represent each member of the gene list as a key and the remainder as synonyms each time
                for each in genes:
                    keygene = set([each])
                    synonym = genes.difference(keygene)
                    gene_name = ','.join(keygene)
                    other_names = ','.join(synonym)
                    hgnc_flag = "1" if gene_name in list_hgnc else "0"
                    # only when the gene is a HGNC name, it would have an hgnc id
                    is_hgnc_id = hgnc_id if gene_name in list_hgnc else "None"
                    
                    # handling duplicate lines (due to transcripts) in summary table (which we don't care for in this table)
                    # writing to outfile for the summary gene table
                    line = "\t".join([chrom,gene_name,hgnc_flag,ens_geneid,is_hgnc_id,
                                         other_names,str(rvis),strand,minmum,maxmum,str(phenotype)])
                    if line not in lines_seen:
                        summary_out.write(line)
                        summary_out.write("\n")
                        lines_seen.add(line)
                    
                    
                    # Writing to out for detailed gene table
                    detailed_out.write("\t".join([chrom,gene_name,hgnc_flag,ens_geneid,ens_transid,trans_biotype,
                                                  status,ccds_id,is_hgnc_id,cds_len,protein_len,transcript_start,
                                                  transcript_end,strand,other_names,str(rvis),entrez,str(phenotype)]))
                    detailed_out.write("\n")
            
            # if there is one gene name in the list, we just want it to be the key
            elif len(genes) == 1:
                gene_name = ','.join(genes)
                other_names = "None"
                hgnc_flag = "1" if gene_name in list_hgnc else "0"
                is_hgnc_id = hgnc_id if gene_name in list_hgnc else "None"
                
                
                # handling duplicate lines (due to transcripts) in summary table (which we don't care for in this table)
                # writing to outfile for the summary gene table
                line = "\t".join([chrom,str(gene_name),hgnc_flag,ens_geneid,is_hgnc_id,
                                  other_names,str(rvis),strand,minmum,maxmum,str(phenotype)])
                                  
                if line not in lines_seen:
                    summary_out.write(line)
                    summary_out.write("\n")
                    lines_seen.add(line)
                
                
                # write to out for detailed gene table
                detailed_out.write("\t".join([chrom,str(gene_name),hgnc_flag,ens_geneid,ens_transid,trans_biotype,
                                              status,ccds_id,is_hgnc_id,cds_len,protein_len,transcript_start,
                                              transcript_end,strand,other_names,str(rvis),entrez,str(phenotype)]))
                detailed_out.write("\n")
        # if there are no HGNC, previous or synonyms names for an ensembl entry, just return None
        elif string is None:
            gene_name = "None"
            other_names = "None"
            hgnc_flag = "0"
            is_hgnc_id = "None"
            
            #handling duplicate lines (due to transcripts) in summary table (which we don't care for in this table)
            #writing to outfile for the summary gene table
            line = "\t".join([chrom,gene_name,hgnc_flag,ens_geneid,is_hgnc_id,
                              other_names,str(rvis),strand,minmum,maxmum,str(phenotype)])
            if line not in lines_seen:
                summary_out.write(line)
                summary_out.write("\n")
                lines_seen.add(line)
            
            # probably we still want to print these lines where gene is none since ensembl gene id has value
            detailed_out.write("\t".join([chrom,gene_name,hgnc_flag,ens_geneid,ens_transid,trans_biotype,status,
                                          ccds_id,is_hgnc_id,cds_len,protein_len,transcript_start,transcript_end,
                                          strand,other_names,str(rvis),entrez,str(phenotype)]))
            detailed_out.write("\n")
            
detailed_out.close()
summary_out.close()

########NEW FILE########
__FILENAME__ = ensembl
#!/usr/bin/env python

import sys
import os
import itertools
from collections import defaultdict

filename = 'ensembl_format'
out = open(filename, 'w')

out.write("\t".join(["Chromosome","HGNC_symbol","Ensembl_gene_id","Ensembl_transcript_id","Biotype","Transcript_status",
                     "CCDS_id","HGNC_id","CDS_length","Protein_length","transcript_start","transcript_end","strand"]))     
out.write("\n")

ccdslen = defaultdict(list)
ensemb = defaultdict(list)


for each in open("ensembl75_2", 'r'):
    if each.startswith("Ensembl") is False:
        col = each.strip().split("\t")
        if col[2] != "None":
            Protein_length = int(col[2])/int(3) - 1
        else:
            Protein_length = "None"
        (key, value) = (col[1], (col[2],str(Protein_length)))
        ccdslen[key].append(value)

for line in open("ensembl75_1", 'r'):
    if line.startswith("Chromosome") is False:
        k = line.strip().split("\t")
        chrom = str((k[0]))
        hgnc = str(k[1])
        ens_geneid = str(k[2])
        ens_transid = str(k[3])
        trans_biotype = str(k[4])
        status = str(k[5])
        ccds_id = str(k[6])
        hgnc_id = str(k[7])
        transcript_start = str(k[8])
        transcript_end = str(k[9])
        strand = str(k[10])
    
        if ens_transid in ccdslen:
            for each in ccdslen[ens_transid]:
                cds_len = each[0]
                protein_len = each[1]
            string = [chrom,hgnc,ens_geneid,ens_transid,trans_biotype,status,ccds_id,hgnc_id,cds_len,protein_len,transcript_start,transcript_end,strand]
        else:
            print "line fail"
        out.write("\t".join(string))
        out.write("\n")
        
out.close()

########NEW FILE########
__FILENAME__ = map_entrez
#!/usr/bin/env python

import sys
import os
import itertools
from collections import defaultdict

files = 'raw_gene_table'
outfile = open(files, 'w')

outfile.write("\t".join(["Chromosome","HGNC_symbol","Ensembl_gene_id","Ensembl_transcript_id","Biotype","Transcript_status",
                         "CCDS_id","HGNC_id","CDS_length","Protein_length","transcript_start","transcript_end",
                         "strand","Previous_symbol","Synonymous","entrez_id"]))
outfile.write("\n")

entrez = defaultdict(list)

for lines in open("ensembl75_3",'r'):
    if lines.startswith("Ensembl") is False:
        seq = lines.strip().split("\t")
        (key,value) = (seq[1],seq[2])
        entrez[key].append(value)
  
with open("gene_table") as f:
    for each in f:
        if each.startswith("Chromosome") is False:
            field = each.strip().split("\t")
            transcript = field[3]
            if transcript in entrez:
                for value in entrez[transcript]:
                    sequence = [field[0],field[1],field[2],field[3],field[4],field[5],field[6],field[7],field[8],field[9],field[10],field[11],field[12],field[13],field[14],value]
            else:
                # return none for entrez where there is no mapping
                value = "None"
                sequence = [field[0],field[1],field[2],field[3],field[4],field[5],field[6],field[7],field[8],field[9],field[10],field[11],field[12],field[13],field[14],value]
            
            outfile.write("\t".join(sequence))
            outfile.write("\n")                
            
outfile.close()

########NEW FILE########
__FILENAME__ = synonym
#!/usr/bin/env python

import sys
import os
import itertools
from collections import defaultdict

files = 'gene_table'
outfile = open(files, 'w')

outfile.write("\t".join(["Chromosome","HGNC_symbol","Ensembl_gene_id","Ensembl_transcript_id","Biotype","Transcript_status",
                         "CCDS_id","HGNC_id","CDS_length","Protein_length","transcript_start","transcript_end","strand","Previous_symbol","Synonymous"]))
outfile.write("\n")

hgncfile = defaultdict(list)

for lines in open("hgnc_file",'r'):
    if lines.startswith("HGNC") is False:
        seq = lines.strip().split("\t")
        (key,value) = (seq[0],(seq[2],seq[3]))
        hgncfile[key].append(value)
    
with open("ensembl_format") as f:
    for each in f:
        if each.startswith("Chromosome") is False:
            field = each.strip().split("\t")
            hgncid = field[7]
            if hgncid in hgncfile:
                for values in hgncfile[hgncid]:
                    #print values[0], values[1]
                    previous_name = values[0]
                    synonymous = values[1]
                sequence = [field[0],field[1],field[2],field[3],field[4],field[5],field[6],field[7],field[8],field[9],field[10],field[11],field[12],previous_name, synonymous]
            else:
                # withdrawn entries (e.g 40184) or genes that do not have an HGNC symbol (None) will not have entry in hgnc_file
                previous_name = "None"
                synonymous = "None"
                sequence = [field[0],field[1],field[2],field[3],field[4],field[5],field[6],field[7],field[8],field[9],field[10],field[11],field[12],previous_name, synonymous]
            
            outfile.write("\t".join(sequence))
            outfile.write("\n")                
            
outfile.close()

########NEW FILE########
__FILENAME__ = hprd_graph
#!/usr/bin/env python

import cPickle
from pygraph.classes.graph import graph
from pygraph.classes.exceptions import AdditionError

"""
This script converts a binary protein-protein interaction
file from HPRD (http://http://www.hprd.org/) into a cPickled
graph generated by the python-graph library.  The resulting
file (hprd_interaction_graph) can be subsequently explored
by gemini for identifying interacting (direct or indirect) genes
with certain classes of genetic variation in one or more samples.

Input:  BINARY_PROTEIN_PROTEIN_INTERACTIONS.txt
    - from: http://www.hprd.org/download/HPRD_Release9_041310.tar.gz
Output: hprd_interaction_graph

"""

gr = graph()
output = open('hprd_interaction_graph', 'wb')

for line in open("BINARY_PROTEIN_PROTEIN_INTERACTIONS.txt", 'r'):
    fields=line.strip().split("\t")
    first = str(fields[0])
    second = str(fields[3])
    if first != "-":
        try:
            gr.add_nodes([first])
        except AdditionError:
            pass;
    if second != "-":
        try:
            gr.add_nodes([second])
        except AdditionError:
            pass;

    if (first == "-" or second == "-" or first == second):
        pass;
    else:
        try:
            gr.add_edge((first, second))
        except AdditionError:
            pass;

cPickle.dump(gr, output)
output.close()
              

########NEW FILE########
__FILENAME__ = unihsagene
#!/usr/bin/env python

############################################################
# Gives a final pathway map of uniprot -> hsa -> gene for ensembl
#########################################################
import sys
import os
import itertools
from collections import defaultdict

filename = 'map_uni2hsa2gene'
out = open(filename, 'w')

filep = 'add_path'
outf = open(filep, 'w') 

unihsa = defaultdict(list)# dict of uniprot to hsa map
hsagene = defaultdict(list) # dict of hsa to gene map
map = defaultdict(list) # 
pathway = defaultdict(list)
add = defaultdict(list)


for line in open("uniprot_hsa_map", 'r'):
    r = line.strip().split("\t")
    (key, value) = (r[0], r[1])
    unihsa[key].append(value)

for each in open("kegg_hsa_to_gene_fmt", 'r'):
    l = each.strip().split("\t")
    (key, value) = (l[0], l[1])
    hsagene[key] = value

prim_value_is_key = [(k, each, hsagene[each]) for k, v in unihsa.iteritems() for each in v if each in hsagene]
for each in prim_value_is_key:
    genes = each[2].split(",")
    for gene in genes:
        g = gene.lstrip() # remove leading white spaces before gene names
        string = [str(each[0]), str(each[1]), str(g)]
        out.write("\t".join(string))
        out.write('\n')
out.close()

for line in open("map_uni2hsa2gene", 'r'):
    p = line.strip().split("\t")
    (key, value) = ((str(p[0]), str(p[2])), str(p[1])) 
    map[key].append(value)

for path in open("kegg_pathway", 'r'):
    k = path.strip().split("\t")
    (key, value) = (k[0], k[1])
    pathway[key] = value

for lines in open("kegg_hsa_to_path", 'r'):
    u = lines.strip().split("\t")
    hs = str(u[0])
    pathcode = str(u[1])
    thread = [hs, pathcode, pathway[pathcode]]
    outf.write("\t".join(thread))
    outf.write("\n")
outf.close()

for rows in open("add_path", 'r'):
    t = rows.strip().split("\t")
    (key, value) = (str(t[0]), (str(t[1]), str(t[2])))
    add[key].append(value)
    
for line in open("ensembl_genes66", 'r'):
    k = line.strip().split("\t")
    uniprot = str(k[0])
    agn = str(k[1])
    hgnc = str(k[2])
    ens_geneid = str(k[3])
    ens_transid = str(k[4])
    
    hsa = [(uniprot, agn, hgnc, ens_geneid, ens_transid, v) for k, v in map.iteritems() if (k[1] == agn and k[0] == uniprot) or (k[1] == hgnc and k[0] == uniprot)]
    if not hsa:
        print uniprot + "\t" + agn + "\t" + hgnc + "\t" + ens_geneid + "\t" + ens_transid + "\t" + "None" + "\t" + "None"   
    
    elif hsa: # if list is not empty
        for each in hsa:
            for keggid in each[5]:
                if keggid in add:
                    for eachvalue in add[keggid]:
                        pathstring = ";".join(eachvalue)
                        print str(each[0]) + "\t" + str(each[1]) + "\t" + str(each[2]) + "\t" + str(each[3]) + "\t" + str(each[4]) + "\t" + str(keggid) + "\t" + pathstring
                else:
                    print str(each[0]) + "\t" + str(each[1]) + "\t" + str(each[2]) + "\t" + str(each[3]) + "\t" + str(each[4]) + "\t" + str(keggid) + "\tNone"

########NEW FILE########
__FILENAME__ = make-ncbi-grc-patches
#!/usr/bin/env python
"""Retrieve information on patches and fixes to GRCh37 from GRC website.

Converts information on these regions into a BED file for Gemini import.

http://www.ncbi.nlm.nih.gov/projects/genome/assembly/grc/human/
"""
import urllib2
from collections import namedtuple
from operator import attrgetter
from contextlib import closing

patch = "p8"
base_url = "ftp://ftp.ncbi.nlm.nih.gov/genbank/genomes/Eukaryotes/" \
           "vertebrates_mammals/Homo_sapiens/GRCh37.{0}/".format(patch)
sub_url = "/alt_scaffolds/alt_scaffold_placement.txt"
dirs = ["PATCHES"] + ["ALT_REF_LOCI_%s" % i for i in range(1, 10)]

def main():
    out_file = "GRC_patch_regions.bed"
    regions = []
    for dname in dirs:
        cur_url = base_url + dname + sub_url
        for region in grc_regions_from_url(cur_url):
            regions.append(region)
    regions.sort(key=attrgetter("chrom", "start", "end"))
    with open(out_file, "w") as out_handle:
        for region in regions:
            out_handle.write("{chrom}\t{start}\t{end}\t{name}\n".format(
                    **vars(region)))

def grc_regions_from_url(url):
    GrcRegion = namedtuple("GrcRegion", "chrom,start,end,name")
    with closing(urllib2.urlopen(url)) as in_handle:
        header = in_handle.next()
        for parts in (l.split("\t") for l in in_handle):
            try:
                chrom = int(parts[5])
            except ValueError:
                chrom = parts[5]
            yield GrcRegion(chrom, int(parts[11]) - 1, int(parts[12]),
                            "grc_%s" % ("fix" if parts[2].endswith("PATCH") else "novel"))
        

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = compression
import zlib
import cPickle
import sqlite3
import numpy
import collections

from gemini_utils import OrderedDict

# http://stackoverflow.com/questions/695794/more-efficient-way-to-
# pickle-a-string


def pack_blob(obj):
    return sqlite3.Binary(zdumps(obj))

def unpack_genotype_blob(blob):
    return numpy.array(cPickle.loads(zlib.decompress(blob)))
    
def unpack_ordereddict_blob(blob):
	blob_val = cPickle.loads(zlib.decompress(blob))
	if blob_val is not None:
	    return OrderedDict(blob_val)
	return None

def zdumps(obj):
    return zlib.compress(cPickle.dumps(obj, cPickle.HIGHEST_PROTOCOL), 9)

def zloads(obj):
    return cPickle.loads(zlib.decompress(obj))

########NEW FILE########
__FILENAME__ = config
"""Configuration YAML files for Gemini.

Provide Gemini configuration files in alternative locations:

- Installer based: gemini-virtualenv/../data or gemini-virtualenv/../gemini/data
- Global:    /usr/local/share/gemini/gemini-config.yaml
- User only: $HOME/.gemini/gemini-config.yaml

Prefer installer based or global if you have system level permissions for
installation since it will work for all system users.
"""
import os
import yaml

CONFIG_FILE = "gemini-config.yaml"

def get_config_dirs():
    virtualenv_loc = __file__.find("gemini-virtualenv")
    anaconda_loc = __file__.find("anaconda")
    if anaconda_loc >= 0:
        base = __file__[:anaconda_loc]
        dirs = [os.path.join(base), os.path.join(base, "gemini")]
    elif virtualenv_loc >= 0:
        base = __file__[:virtualenv_loc]
        dirs = [os.path.join(base), os.path.join(base, "gemini")]
    else:
        dirs = []
    dirs.append("/usr/local/share/gemini")
    dirs.append(os.path.join(os.environ["HOME"], ".gemini"))
    return dirs

def _get_config_file(dirs=None):
    dirs = [] if dirs is None else dirs
    dnames = dirs + get_config_dirs()
    for dname in dnames:
        fname = os.path.join(dname, CONFIG_FILE)
        if os.path.exists(fname):
            return fname
    raise ValueError("GEMINI configuration file {0} not found in {1}.\n"
                     "Please ensure the GEMINI data is installed using the install-data.py script\n"
                     "http://gemini.readthedocs.org/en/latest/content/installation.html"
                     .format(CONFIG_FILE, dnames))

def read_gemini_config(dirs=None, allow_missing=False):
    try:
        fname = _get_config_file(dirs)
    except ValueError:
        if allow_missing:
            return {}
        else:
            raise
    with open(fname) as in_handle:
        return yaml.load(in_handle)

def _find_best_config_file(dirs=None):
    dirs = [] if dirs is None else dirs
    dnames = dirs + get_config_dirs()
    for dname in dnames:
        if os.access(dname, os.W_OK) or \
                os.access(os.path.dirname(dname), os.W_OK):
            return os.path.join(dname, CONFIG_FILE)

    raise ValueError("Gemini configuration: "
                     "Could not find writeable directory: {0}".format(dnames))

def write_gemini_config(new_config, dirs=None):
    try:
        fname = _get_config_file(dirs)
    except ValueError:
        fname = _find_best_config_file(dirs)
    if not os.path.exists(os.path.dirname(fname)):
        os.makedirs(os.path.dirname(fname))
    with open(fname, "w") as out_handle:
        yaml.dump(new_config, out_handle, allow_unicode=False,
                  default_flow_style=False)

########NEW FILE########
__FILENAME__ = database
#!/usr/bin/env python

import sqlite3
import sys
from itertools import repeat
import contextlib

from ped import get_ped_fields, default_ped_fields


def index_variation(cursor):
    cursor.execute('''create index var_chr_start_idx on\
                      variants(chrom, start)''')
    cursor.execute('''create index var_type_idx on variants(type)''')
    cursor.execute('''create index var_gt_counts_idx on \
                      variants(num_hom_ref, num_het, \
                               num_hom_alt, num_unknown)''')
    cursor.execute('''create index var_aaf_idx on variants(aaf)''')
    cursor.execute('''create index var_in_dbsnp_idx on variants(in_dbsnp)''')
    cursor.execute('''create index var_in_call_rate_idx on variants(call_rate)''')
    cursor.execute('''create index var_exonic_idx on variants(is_exonic)''')
    cursor.execute('''create index var_coding_idx on variants(is_coding)''')
    cursor.execute('''create index var_lof_idx on variants(is_lof)''')
    cursor.execute('''create index var_som_idx on variants(is_somatic)''')
    cursor.execute('''create index var_depth_idx on variants(depth)''')
    cursor.execute('''create index var_gene_idx on variants(gene)''')
    cursor.execute('''create index var_trans_idx on variants(transcript)''')
    cursor.execute('''create index var_impact_idx on variants(impact)''')
    cursor.execute('''create index var_impact_severity_idx on variants(impact_severity)''')
    cursor.execute('''create index var_esp_idx on variants(aaf_esp_all)''')
    cursor.execute('''create index var_1kg_idx on variants(aaf_1kg_all)''')
    cursor.execute('''create index var_qual_idx on variants(qual)''')
    cursor.execute('''create index var_homref_idx on variants(num_hom_ref)''')
    cursor.execute('''create index var_homalt_idx on variants(num_hom_alt)''')
    cursor.execute('''create index var_het_idx on variants(num_het)''')
    cursor.execute('''create index var_unk_idx on variants(num_unknown)''')
    cursor.execute('''create index var_omim_idx on variants(in_omim)''')
    cursor.execute('''create index var_cadd_raw_idx on variants(cadd_raw)''')
    cursor.execute('''create index var_cadd_scaled_idx on variants(cadd_scaled)''')


def index_variation_impacts(cursor):
    cursor.execute('''create index varimp_exonic_idx on \
                      variant_impacts(is_exonic)''')
    cursor.execute('''create index varimp_coding_idx on \
                      variant_impacts(is_coding)''')
    cursor.execute(
        '''create index varimp_lof_idx on variant_impacts(is_lof)''')
    cursor.execute('''create index varimp_impact_idx on \
                      variant_impacts(impact)''')
    cursor.execute('''create index varimp_trans_idx on \
                      variant_impacts(transcript)''')
    cursor.execute('''create index varimp_gene_idx on \
                      variant_impacts(gene)''')


def index_samples(cursor):
    cursor.execute('''create unique index sample_name_idx on samples(name)''')


def index_gene_detailed(cursor):
    cursor.execute('''create index gendet_chrom_gene_idx on \
                       gene_detailed(chrom, gene)''')
    cursor.execute('''create index gendet_rvis_idx on \
                       gene_detailed(rvis_pct)''')
    cursor.execute('''create index gendet_transcript_idx on \
                       gene_detailed(transcript)''')
    cursor.execute('''create index gendet_ccds_idx on \
                       gene_detailed(ccds_id)''')

def index_gene_summary(cursor):
    cursor.execute('''create index gensum_chrom_gene_idx on \
                       gene_summary(chrom, gene)''')
    cursor.execute('''create index gensum_rvis_idx on \
                      gene_summary(rvis_pct)''')

def create_indices(cursor):
    """
    Index our master DB tables for speed
    """
    index_variation(cursor)
    index_variation_impacts(cursor)
    index_samples(cursor)
    index_gene_detailed(cursor)
    index_gene_summary(cursor)


def create_tables(cursor):
    """
    Create our master DB tables
    """
    cursor.execute('''create table if not exists variants  (    \
                    chrom text,                                 \
                    start integer,                              \
                    end integer,                                \
                    vcf_id text,                                \
                    variant_id integer,                         \
                    anno_id integer,                            \
                    ref text,                                   \
                    alt text,                                   \
                    qual float,                                 \
                    filter text,                                \
                    type text,                                  \
                    sub_type text,                              \
                    gts blob,                                   \
                    gt_types blob,                              \
                    gt_phases blob,                             \
                    gt_depths blob,                             \
                    gt_ref_depths blob,                         \
                    gt_alt_depths blob,                         \
                    gt_quals blob,                              \
                    call_rate float,                            \
                    in_dbsnp bool,                              \
                    rs_ids text default NULL,                   \
                    in_omim bool,                               \
                    clinvar_sig text default NULL,              \
                    clinvar_disease_name text default NULL,     \
                    clinvar_dbsource text default NULL,         \
                    clinvar_dbsource_id text default NULL,      \
                    clinvar_origin text default NULL,           \
                    clinvar_dsdb text default NULL,             \
                    clinvar_dsdbid text default NULL,           \
                    clinvar_disease_acc text default NULL,      \
                    clinvar_in_locus_spec_db bool,              \
                    clinvar_on_diag_assay bool,                 \
                    pfam_domain text,                           \
                    cyto_band text default NULL,                \
                    rmsk text default NULL,                     \
                    in_cpg_island bool,                         \
                    in_segdup bool,                             \
                    is_conserved bool,                          \
                    gerp_bp_score float,                        \
                    gerp_element_pval float,                    \
                    num_hom_ref integer,                        \
                    num_het integer,                            \
                    num_hom_alt integer,                        \
                    num_unknown integer,                        \
                    aaf real,                                   \
                    hwe decimal(2,7),                           \
                    inbreeding_coeff decimal(2,7),              \
                    pi decimal(2,7),                            \
                    recomb_rate decimal(2,7),                   \
                    gene text,                                  \
                    transcript text,                            \
                    is_exonic bool,                             \
                    is_coding bool,                             \
                    is_lof bool,                                \
                    exon text,                                  \
                    codon_change text,                          \
                    aa_change text,                             \
                    aa_length text,                             \
                    biotype text,                               \
                    impact text default NULL,                   \
                    impact_so text default NULL,                \
                    impact_severity text,                       \
                    polyphen_pred text,                         \
                    polyphen_score float,                       \
                    sift_pred text,                             \
                    sift_score float,                           \
                    anc_allele text,                            \
                    rms_bq float,                               \
                    cigar text,                                 \
                    depth integer default NULL,                 \
                    strand_bias float default NULL,             \
                    rms_map_qual float default NULL,            \
                    in_hom_run integer default NULL,            \
                    num_mapq_zero integer default NULL,         \
                    num_alleles integer default NULL,           \
                    num_reads_w_dels float default NULL,        \
                    haplotype_score float default NULL,         \
                    qual_depth float default NULL,              \
                    allele_count integer default NULL,          \
                    allele_bal float default NULL,              \
                    in_hm2 bool,                                \
                    in_hm3 bool,                                \
                    is_somatic,                                 \
                    in_esp bool,                                \
                    aaf_esp_ea decimal(2,7),                    \
                    aaf_esp_aa decimal(2,7),                    \
                    aaf_esp_all decimal(2,7),                   \
                    exome_chip bool,                            \
                    in_1kg bool,                                \
                    aaf_1kg_amr decimal(2,7),                   \
                    aaf_1kg_asn decimal(2,7),                   \
                    aaf_1kg_afr decimal(2,7),                   \
                    aaf_1kg_eur decimal(2,7),                   \
                    aaf_1kg_all decimal(2,7),                   \
                    grc text default NULL,                      \
                    gms_illumina float,                         \
                    gms_solid float,                            \
                    gms_iontorrent float,                       \
                    in_cse bool,                                \
                    encode_tfbs text,                           \
                    encode_dnaseI_cell_count integer,           \
                    encode_dnaseI_cell_list text,               \
                    encode_consensus_gm12878 text,              \
                    encode_consensus_h1hesc text,               \
                    encode_consensus_helas3 text,               \
                    encode_consensus_hepg2 text,                \
                    encode_consensus_huvec text,                \
                    encode_consensus_k562 text,                 \
                    vista_enhancers text,                       \
                    cosmic_ids text,                            \
                    info blob,                                  \
                    cadd_raw float,                             \
                    cadd_scaled float,                          \
                    PRIMARY KEY(variant_id ASC))''')

    cursor.execute('''create table if not exists variant_impacts  (   \
                    variant_id integer,                               \
                    anno_id integer,                                  \
                    gene text,                                        \
                    transcript text,                                  \
                    is_exonic bool,                                   \
                    is_coding bool,                                   \
                    is_lof bool,                                      \
                    exon text,                                        \
                    codon_change text,                                \
                    aa_change text,                                   \
                    aa_length text,                                   \
                    biotype text,                                     \
                    impact text,                                      \
                    impact_so text,                                   \
                    impact_severity text,                             \
                    polyphen_pred text,                               \
                    polyphen_score float,                             \
                    sift_pred text,                                   \
                    sift_score float,                                 \
                    PRIMARY KEY(variant_id ASC, anno_id ASC))''')

    cursor.execute('''create table if not exists sample_genotypes (  \
                    sample_id integer,                               \
                    gt_types BLOB,                                   \
                    PRIMARY KEY(sample_id ASC))''')

    cursor.execute('''create table if not exists sample_genotype_counts ( \
                     sample_id integer,                                   \
                     num_hom_ref integer,                                 \
                     num_het integer,                                     \
                     num_hom_alt integer,                                 \
                     num_unknown integer,                                 \
                     PRIMARY KEY(sample_id ASC))''')

    cursor.execute('''create table if not exists resources ( \
                     name text,                              \
                     resource text)''')

    cursor.execute('''create table if not exists version (version text)''')
    
    cursor.execute('''create table if not exists gene_detailed (       \
                   uid integer,                                        \
                   chrom text,                                         \
                   gene text,                                          \
                   is_hgnc bool,                                       \
                   ensembl_gene_id text,                               \
                   transcript text,                                    \
                   biotype text,                                       \
                   transcript_status text,                             \
                   ccds_id text,                                       \
                   hgnc_id text,                                       \
                   entrez_id text,                                     \
                   cds_length text,                                    \
                   protein_length text,                                \
                   transcript_start text,                              \
                   transcript_end text,                                \
                   strand text,                                        \
                   synonym text,                                       \
                   rvis_pct float,                                     \
                   mam_phenotype_id text,                              \
                   PRIMARY KEY(uid ASC))''')
                   
    cursor.execute('''create table if not exists gene_summary (     \
                    uid integer,                                    \
                    chrom text,                                     \
                    gene text,                                      \
                    is_hgnc bool,                                   \
                    ensembl_gene_id text,                           \
                    hgnc_id text,                                   \
                    transcript_min_start text,                      \
                    transcript_max_end text,                        \
                    strand text,                                    \
                    synonym text,                                   \
                    rvis_pct float,                                 \
                    mam_phenotype_id text,                          \
                    in_cosmic_census bool,                          \
                    PRIMARY KEY(uid ASC))''')

def create_sample_table(cursor, args):
    NUM_BUILT_IN = 6
    fields = get_ped_fields(args.ped_file)
    required = "sample_id integer"
    optional_fields = ["family_id", "name", "paternal_id", "maternal_id",
                       "sex", "phenotype"]
    optional_fields += fields[NUM_BUILT_IN:] + ["PRIMARY KEY(sample_id ASC)"]
    optional = " text default NULL,".join(optional_fields)
    structure = '''{0}, {1}'''.format(required, optional)
    creation = "create table if not exists samples ({0})".format(structure)
    cursor.execute(creation)

def _insert_variation_one_per_transaction(cursor, buffer):
    for variant in buffer:
        try:
            cursor.execute("BEGIN TRANSACTION")
            cursor.execute('insert into variants values     (?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?,?,?,?,?,?,?, \
                                                             ?,?,?,?)', variant)
            cursor.execute("END TRANSACTION")
        # skip repeated keys until we get to the failed variant
        except sqlite3.IntegrityError, e:
            cursor.execute("END TRANSACTION")
            continue
        except sqlite3.ProgrammingError, e:
            print variant
            print "Error %s:" % (e.args[0])
            sys.exit(1)

def insert_variation(cursor, buffer):
    """
    Populate the variants table with each variant in the buffer.
    """
    try:
        cursor.execute("BEGIN TRANSACTION")
        cursor.executemany('insert into variants values (?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?)', buffer)

        cursor.execute("END TRANSACTION")
    except sqlite3.ProgrammingError:
        cursor.execute("END TRANSACTION")
        _insert_variation_one_per_transaction(cursor, buffer)


def insert_variation_impacts(cursor, buffer):
    """
    Populate the variant_impacts table with each variant in the buffer.
    """
    cursor.execute("BEGIN TRANSACTION")
    cursor.executemany('insert into variant_impacts values (?,?,?,?,?,?,?,?, \
                                                            ?,?,?,?,?,?,?,?, \
                                                            ?,?,?)',
                       buffer)
    cursor.execute("END")


def insert_sample(cursor, sample_list):
    """
    Populate the samples with sample ids, names, and
    other indicative information.
    """
    placeholders = ",".join(list(repeat("?", len(sample_list))))
    cursor.execute("BEGIN TRANSACTION")
    cursor.execute("insert into samples values "
                   "({0})".format(placeholders), sample_list)
    cursor.execute("END")

def insert_gene_detailed(cursor, table_contents):
    cursor.execute("BEGIN TRANSACTION")
    cursor.executemany('insert into gene_detailed values (?,?,?,?,?,?,?,?,?, \
                                                          ?,?,?,?,?,?,?,?,?, \
                                                          ?)',
                        table_contents)
    cursor.execute("END")
    

def insert_gene_summary(cursor, contents):
    cursor.execute("BEGIN TRANSACTION")
    cursor.executemany('insert into gene_summary values (?,?,?,?,?,?,?,?, \
                                                         ?,?,?,?,?)', 
                        contents)
    cursor.execute("END")
    
def insert_resources(cursor, resources):
    """Populate table of annotation resources used in this database.
    """
    cursor.execute("BEGIN TRANSACTION")
    cursor.executemany('''insert into resources values (?,?)''', resources)
    cursor.execute("END")

def insert_version(cursor, version):
    """Populate table of documenting which
    gemini version was used for this database.
    """
    cursor.execute("BEGIN TRANSACTION")
    cursor.execute('''insert into version values (?)''', (version,))
    cursor.execute("END")


def close_and_commit(cursor, connection):
    """
    Commit changes to the DB and close out DB cursor.
    """
    connection.commit()
    cursor.close


def empty_tables(cursor):
    cursor.execute('''delete * from variation''')
    cursor.execute('''delete * from samples''')


def update_gene_summary_w_cancer_census(cursor, genes):
    update_qry = "UPDATE gene_summary SET in_cosmic_census = ? "
    update_qry += " WHERE gene = ? and chrom = ?"
    cursor.executemany(update_qry, genes)

@contextlib.contextmanager
def database_transaction(db):
    conn = sqlite3.connect(db)
    conn.isolation_level = None
    cursor = conn.cursor()
    cursor.execute('PRAGMA synchronous = OFF')
    cursor.execute('PRAGMA journal_mode=MEMORY')
    yield cursor
    conn.commit
    cursor.close()

########NEW FILE########
__FILENAME__ = dgidb
import urllib2
import json

def query_dgidb(genes):
    """
    Batch query DGIdb for drug-gene interaction data for
    a set of genes.
    """

    def convert(input):
        """
        Convert JSON UNICODE to plain ole strings.
        """
        if isinstance(input, dict):
            return {convert(key): convert(value) for key, value in input.iteritems()}
        elif isinstance(input, list):
            return [convert(element) for element in input]
        elif isinstance(input, unicode):
            return input.encode('utf-8')
        else:
            return input

    # make a single request to DGIdb for all of the genes requested
    dgidb_url = 'http://dgidb.genome.wustl.edu/api/v1/interactions.json?genes='
    
    # None is present by default. Make sure we have more than None
    if len(genes) > 1:
        query = dgidb_url + ','.join(genes.keys())
        
        response = urllib2.urlopen(query)
        data = convert(json.load(response))
        matches = data['matchedTerms']

        # store the results for all of the genes. if there are no matches
        # in DGIdb, the result will be None.
        gene_dgidb_info = {}
        for gene in genes:
            gene_dgidb_info[gene] = None

        for match in matches:
            gene = match['searchTerm']
            gene_dgidb_info[gene] = dict(match)

        return gene_dgidb_info
    else:
        return None
########NEW FILE########
__FILENAME__ = func_impact
import sys
import snpEff
import vep


def interpret_impact(args, var):
    """
    Interpret the report from SnpEff or VEP to determine the impact of the variant.

    SnpEff examples:
    0    NON_SYNONYMOUS_CODING(MODERATE|MISSENSE|Aca/Gca|T/A|OR4F5|protein_coding|CODING|ENST00000335137|exon_1_69091_70008),
    1    NON_SYNONYMOUS_CODING(MODERATE|MISSENSE|Aca/Gca|T/A|OR4F5|protein_coding|CODING|ENST00000534990|exon_1_69037_69829)

    VEP examples
    CSQ: Consequence|Codons|Amino_acids|Gene|hgnc|Feature|EXON|polyphen|sift
    non_synonymous_codon|gaT/gaG|D/E|ENSG00000116254|CHD5|ENST00000378006|18/25|benign(0.011)|tolerated(0.3)
    nc_transcript_variant|||ENSG00000116254|CHD5|ENST00000491020|5/6||
    """
    impact_all = []
        # holds a list of all the transcript impacts for this variant
    effect_strings_str = ""
    effect_strings = []
    counter = 0  # counter for anno_id
    if args.anno_type == "snpEff":
        try:
            effect_strings_str = var.INFO["EFF"]
            effect_strings = effect_strings_str.split(",")
        except KeyError:
            if "SNPEFF_EFFECT" in var.INFO:
                impact_all.append(snpEff.gatk_effect_details(var.INFO))
            else:
                sys.stderr.write("WARNING: The input VCF has no snpEFF annotations. "
                                 "Variant impact will be set to unknown\n")

        for effect_string in effect_strings:
            counter += 1
            eff_pieces = snpEff.eff_search.findall(effect_string)
            for piece in eff_pieces:
                impact_string = piece[0]
                    # the predicted inpact, which is outside the ()
                impact_detail = piece[1]
                    # all the other information, which is inside the ()
                try:
                    impact_info = snpEff.effect_map[impact_string]
                    impact_details = snpEff.EffectDetails(impact_string,
                                                      impact_info.priority,
                                                      impact_detail,
                                                      counter,
                                                      args.maj_version)
                except KeyError:
                    impact_details = snpEff.EffectDetails(impact_string,
                                                    None,
                                                    impact_detail,
                                                    counter,
                                                    args.maj_version)
                                                    
                impact_all.append(impact_details)

    elif args.anno_type == "VEP":
        try:
            effect_strings_str = var.INFO["CSQ"]
            effect_strings = effect_strings_str.split(",")
        except KeyError:
            sys.stderr.write("WARNING: The input VCF has no VEP annotations. \
                             Variant impact will be set to unknown\n")

        for effect_string in effect_strings:

             # nc_transcript_variant&intron_variant|||ENSG00000243485|MIR1302-11|ENST00000
            each_string = effect_string.split("|")
            if "&" in each_string[0]:
                impact_strings = each_string[0].split("&")
                # impact_strings will be [nc_transcript_variant,
                # intron_variant]
                for impact_string in impact_strings:
                    counter += 1
                    try:
                        impact_info = vep.effect_map[impact_string]
                        impact_details = vep.EffectDetails(
                            impact_string, impact_info.priority, effect_string, counter)
                    except KeyError:
                        impact_details = vep.EffectDetails(
                            impact_string, None, effect_string, counter)
                    impact_all.append(impact_details)
            # we expect VEP to produce a valid impact label for each_string[0]
            elif "&" not in each_string[0]:
                counter += 1
                impact_string = each_string[0]
                impact_info = vep.effect_map.get(impact_string)
                try:
                    impact_details = vep.EffectDetails(
                        impact_string, impact_info.priority, effect_string, counter)
                except AttributeError:
                    impact_details = vep.EffectDetails(
                        impact_string, None, effect_string, counter)
                impact_all.append(impact_details)
    else:
        # should not get here, as the valid -t options should be handled
        # in main()
        sys.exit("ERROR: Unsupported variant annotation type.\n")

    return impact_all

########NEW FILE########
__FILENAME__ = GeminiQuery
#!/usr/bin/env python

import os
import sys
import sqlite3
import re
import itertools
import collections
import json
import abc
import re
import numpy as np

# gemini imports
import gemini_utils as util
from gemini_constants import *
from gemini_utils import OrderedSet, OrderedDict, itersubclasses, partition
import compression
from sql_utils import ensure_columns, get_select_cols_and_rest
from gemini_subjects import get_subjects


class RowFormat:
    """A row formatter to output rows in a custom format.  To provide
    a new output format 'foo', implement the class methods and set the
    name field to foo.  This will automatically add support for 'foo' to
    anything accepting the --format option via --format foo.
    """

    __metaclass__ = abc.ABCMeta

    @abc.abstractproperty
    def name(self):
        return

    @abc.abstractmethod
    def format(self, row):
        """ return a string representation of a GeminiRow object
        """
        return '\t'.join([str(row.row[c]) for c in row.row])

    @abc.abstractmethod
    def format_query(self, query):
        """ augment the query with columns necessary for the format or else just
        return the untouched query
        """
        return query

    @abc.abstractmethod
    def predicate(self, row):
        """ the row must pass this additional predicate to be output. Just
        return True if there is no additional predicate"""
        return True

    @abc.abstractmethod
    def header(self, fields):
        """ return a header for the row """
        return "\t".join(fields)


class DefaultRowFormat(RowFormat):

    name = "default"

    def __init__(self, args):
        pass

    def format(self, row):
        return '\t'.join([str(row.row[c]) for c in row.row])

    def format_query(self, query):
        return query

    def predicate(self, row):
        return True

    def header(self, fields):
        """ return a header for the row """
        return "\t".join(fields)

class CarrierSummary(RowFormat):
    """
    Generates a count of the carrier/noncarrier status of each feature in a given
    column of the sample table

    Assumes None == unknown.
    """
    name = "carrier_summary"

    def __init__(self, args):
        subjects = get_subjects(args)
        self.carrier_summary = args.carrier_summary

        # get the list of all possible values in the column
        # but don't include None, since we are treating that as unknown.
        self.column_types = list(set([getattr(x, self.carrier_summary)
                                      for x in subjects.values()]))
        self.column_types = [i for i in self.column_types if i is not None]
        self.column_counters = {None: set()}
        for ct in self.column_types:
            self.column_counters[ct] = set([k for (k, v) in subjects.items() if
                                            getattr(v, self.carrier_summary) == ct])


    def format(self, row):
        have_variant = set(row.variant_samples)
        have_reference = set(row.HOM_REF_samples)
        unknown = len(set(row.UNKNOWN_samples).union(self.column_counters[None]))
        carrier_counts = []
        for ct in self.column_types:
            counts = len(self.column_counters[ct].intersection(have_variant))
            carrier_counts.append(counts)
        for ct in self.column_types:
            counts = len(self.column_counters[ct].intersection(have_reference))
            carrier_counts.append(counts)

        carrier_counts.append(unknown)
        carrier_counts = map(str, carrier_counts)
        return '\t'.join([str(row.row[c]) for c in row.row] + carrier_counts)

    def format_query(self, query):
        return query

    def predicate(self, row):
        return True

    def header(self, fields):
        """ return a header for the row """
        header_columns = self.column_types
        if self.carrier_summary == "affected":
            header_columns = self._rename_affected()
        carriers = [x + "_carrier" for x in map(str, header_columns)]
        noncarriers = [ x + "_noncarrier" for x in map(str, header_columns)]
        fields += carriers
        fields += noncarriers
        fields += ["unknown"]
        return "\t".join(fields)

    def _rename_affected(self):
        header_columns = []
        for ct in self.column_types:
            if ct == True:
                header_columns.append("affected")
            elif ct == False:
                header_columns.append("unaffected")
        return header_columns



class TPEDRowFormat(RowFormat):

    name = "tped"
    NULL_GENOTYPES = ["."]

    def __init__(self, args):
        pass

    def format(self, row):
        VALID_CHROMOSOMES = map(str, range(1, 23)) + ["X", "Y", "XY", "MT"]
        chrom = row['chrom'].split("chr")[1]
        chrom = chrom if chrom in VALID_CHROMOSOMES else "0"
        start = str(row.row['start'])
        end = str(row.row['end'])
        geno = [re.split('\||/', x) for x in row.row['gts'].split(",")]
        geno = [["0", "0"] if any([y in self.NULL_GENOTYPES for y in x])
                else x for x in geno]
        genotypes = " ".join(list(flatten(geno)))
        alleles = "|".join(set(list(flatten(geno))).difference("0"))
        name = chrom + ":" +  start + "-" + end + ":" + alleles
        return " ".join([chrom, name, "0", start, genotypes])

    def format_query(self, query):
        NEED_COLUMNS = ["chrom", "rs_ids", "start", "gts", "type", "variant_id"]
        return ensure_columns(query, NEED_COLUMNS)

    def predicate(self, row):
        geno = [re.split("\||/", x) for x in row['gts']]
        geno = list(flatten(geno))
        num_alleles = len(set(geno).difference(self.NULL_GENOTYPES))
        return num_alleles > 0 and num_alleles <= 2 and row['type'] != "sv"

    def header(self, fields):
        return None


class JSONRowFormat(RowFormat):

    name = "json"

    def __init__(self, args):
        pass

    def format(self, row):
        """Emit a JSON representation of a given row
        """
        return json.dumps(row.row)

    def format_query(self, query):
        return query

    def predicate(self, row):
        return True

    def header(self, fields):
        return None


class GeminiRow(object):

    def __init__(self, row, gts=None, gt_types=None,
                 gt_phases=None, gt_depths=None,
                 gt_ref_depths=None, gt_alt_depths=None,
                 gt_quals=None, variant_samples=None,
                 HET_samples=None, HOM_ALT_samples=None,
                 HOM_REF_samples=None, UNKNOWN_samples=None,
                 info=None,formatter=DefaultRowFormat(None)):
        self.row = row
        self.gts = gts
        self.info = info
        self.gt_types = gt_types
        self.gt_phases = gt_phases
        self.gt_depths = gt_depths
        self.gt_ref_depths = gt_ref_depths
        self.gt_alt_depths = gt_alt_depths
        self.gt_quals = gt_quals
        self.gt_cols = ['gts', 'gt_types', 'gt_phases',
                        'gt_depths', 'gt_ref_depths', 'gt_alt_depths',
                        'gt_quals', "variant_samples", "HET_samples", "HOM_ALT_samples", "HOM_REF_samples"]
        self.formatter = formatter
        self.variant_samples = variant_samples
        self.HET_samples = HET_samples
        self.HOM_ALT_samples = HOM_ALT_samples
        self.HOM_REF_samples = HOM_REF_samples
        self.UNKNOWN_samples = UNKNOWN_samples

    def __getitem__(self, val):
        if val not in self.gt_cols:
            return self.row[val]
        else:
            return getattr(self, val)

    def __iter__(self):
        return self

    def __repr__(self):
        return self.formatter.format(self)

    def next(self):
        try:
            return self.row.keys()
        except:
            raise StopIteration


class GeminiQuery(object):

    """
    An interface to submit queries to an existing Gemini database
    and iterate over the results of the query.

    We create a GeminiQuery object by specifying database to which to
    connect::
        from gemini import GeminiQuery
        gq = GeminiQuery("my.db")

    We can then issue a query against the database and iterate through
    the results by using the ``run()`` method::


        for row in gq:
            print row

    Instead of printing the entire row, one access print specific columns::

        gq.run("select chrom, start, end from variants")
        for row in gq:
            print row['chrom']

    Also, all of the underlying numpy genotype arrays are
    always available::

        gq.run("select chrom, start, end from variants")
        for row in gq:
            gts = row.gts
            print row['chrom'], gts
            # yields "chr1" ['A/G' 'G/G' ... 'A/G']

    The ``run()`` methods also accepts genotype filter::

        query = "select chrom, start, end" from variants"
        gt_filter = "gt_types.NA20814 == HET"
        gq.run(query)
        for row in gq:
            print row

    Lastly, one can use the ``sample_to_idx`` and ``idx_to_sample``
    dictionaries to gain access to sample-level genotype information
    either by sample name or by sample index::

        # grab dict mapping sample to genotype array indices
        smp2idx = gq.sample_to_idx

        query  = "select chrom, start, end from variants"
        gt_filter  = "gt_types.NA20814 == HET"
        gq.run(query, gt_filter)

        # print a header listing the selected columns
        print gq.header
        for row in gq:
            # access a NUMPY array of the sample genotypes.
            gts = row['gts']
            # use the smp2idx dict to access sample genotypes
            idx = smp2idx['NA20814']
            print row, gts[idx]
    """

    def __init__(self, db, include_gt_cols=False,
                 out_format=DefaultRowFormat(None)):
        assert os.path.exists(db), "%s does not exist." % db

        self.db = db
        self.query_executed = False
        self.for_browser = False
        self.include_gt_cols = include_gt_cols

        # try to connect to the provided database
        self._connect_to_database()

        # extract the column names from the sample table.
        # needed for gt-filter wildcard support.
        self._collect_sample_table_columns()

        # map sample names to indices. e.g. self.sample_to_idx[NA20814] -> 323
        self.sample_to_idx = util.map_samples_to_indices(self.c)
        # and vice versa. e.g., self.idx_to_sample[323] ->  NA20814
        self.idx_to_sample = util.map_indices_to_samples(self.c)
        self.idx_to_sample_object = util.map_indices_to_sample_objects(self.c)
        self.formatter = out_format
        self.predicates = [self.formatter.predicate]


    def _set_gemini_browser(self, for_browser):
        self.for_browser = for_browser

    def run(self, query, gt_filter=None, show_variant_samples=False,
            variant_samples_delim=',', predicates=None,
            needs_genotypes=False, needs_genes=False,
            show_families=False):
        """
        Execute a query against a Gemini database. The user may
        specify:

            1. (reqd.) an SQL `query`.
            2. (opt.) a genotype filter.
        """
        self.query = self.formatter.format_query(query)
        self.gt_filter = gt_filter
        if self._is_gt_filter_safe() is False:
            sys.exit("ERROR: invalid --gt-filter command.")

        self.show_variant_samples = show_variant_samples
        self.variant_samples_delim = variant_samples_delim
        self.needs_genotypes = needs_genotypes
        self.needs_genes = needs_genes
        self.show_families = show_families
        if predicates:
            self.predicates += predicates

        # make sure the SELECT columns are separated by a 
        # comma and a space. then tokenize by spaces.
        self.query = self.query.replace(',', ', ')
        self.query_pieces = self.query.split()
        if not any(s.startswith("gt") for s in self.query_pieces) and \
           not any(s.startswith("(gt") for s in self.query_pieces) and \
           not any(".gt" in s for s in self.query_pieces):
            if self.gt_filter is None:
                self.query_type = "no-genotypes"
            else:
                self.gt_filter = self._correct_genotype_filter()
                self.query_type = "filter-genotypes"
        else:
            if self.gt_filter is None:
                self.query_type = "select-genotypes"
            else:
                self.gt_filter = self._correct_genotype_filter()
                self.query_type = "filter-genotypes" 
        
        self._apply_query()
        self.query_executed = True


    def __iter__(self):
        return self

    @property
    def header(self):
        """
        Return a header describing the columns that
        were selected in the query issued to a GeminiQuery object.
        """
        if self.query_type == "no-genotypes":
            h = [col for col in self.all_query_cols]
        else:
            h = [col for col in self.all_query_cols] + \
                [col for col in OrderedSet(self.all_columns_orig)
                 - OrderedSet(self.select_columns)]
        if self.show_variant_samples:
            h += ["variant_samples", "HET_samples", "HOM_ALT_samples"]
        if self.show_families:
            h += ["families"]
        return self.formatter.header(h)

    @property
    def sample2index(self):
        """
        Return a dictionary mapping sample names to
        genotype array offsets::

            gq = GeminiQuery("my.db")
            s2i = gq.sample2index

            print s2i['NA20814']
            # yields 1088
        """
        return self.sample_to_idx

    @property
    def index2sample(self):
        """
        Return a dictionary mapping sample names to
        genotype array offsets::

            gq = GeminiQuery("my.db")
            i2s = gq.index2sample

            print i2s[1088]
            # yields "NA20814"
        """
        return self.idx_to_sample

    def next(self):
        """
        Return the GeminiRow object for the next query result.
        """
        # we use a while loop since we may skip records based upon
        # genotype filters.  if we need to skip a record, we just
        # throw a continue and keep trying. the alternative is to just
        # recursively call self.next() if we need to skip, but this
        # can quickly exceed the stack.
        while (1):
            try:
                row = self.c.next()
            except Exception as e:
                self.conn.close()
                raise StopIteration
            gts = None
            gt_types = None
            gt_phases = None
            gt_depths = None
            gt_ref_depths = None
            gt_alt_depths = None
            gt_quals = None
            variant_names = []
            het_names = []
            hom_alt_names = []
            hom_ref_names = []
            unknown_names = []
            info = None
            
            if 'info' in self.report_cols:
                info = compression.unpack_ordereddict_blob(row['info'])
            
            if self._query_needs_genotype_info():
                gts = compression.unpack_genotype_blob(row['gts'])
                gt_types = \
                    compression.unpack_genotype_blob(row['gt_types'])
                gt_phases = \
                    compression.unpack_genotype_blob(row['gt_phases'])
                gt_depths = \
                    compression.unpack_genotype_blob(row['gt_depths'])
                gt_ref_depths = \
                    compression.unpack_genotype_blob(row['gt_ref_depths'])
                gt_alt_depths = \
                    compression.unpack_genotype_blob(row['gt_alt_depths'])
                gt_quals = \
                    compression.unpack_genotype_blob(row['gt_quals'])
                variant_samples = [x for x, y in enumerate(gt_types) if y == HET or
                                   y == HOM_ALT]
                variant_names = [self.idx_to_sample[x] for x in variant_samples]
                het_samples = [x for x, y in enumerate(gt_types) if y == HET]
                het_names = [self.idx_to_sample[x] for x in het_samples]
                hom_alt_samples = [x for x, y in enumerate(gt_types) if y == HOM_ALT]
                hom_alt_names = [self.idx_to_sample[x] for x in hom_alt_samples]
                hom_ref_samples = [x for x, y in enumerate(gt_types) if y == HOM_REF]
                hom_ref_names = [self.idx_to_sample[x] for x in hom_ref_samples]
                unknown_samples = [x for x, y in enumerate(gt_types) if y == UNKNOWN]
                unknown_names = [self.idx_to_sample[x] for x in unknown_samples]
                families = map(str, list(set([self.idx_to_sample_object[x].family_id
                            for x in variant_samples])))

                # skip the record if it does not meet the user's genotype filter
                if self.gt_filter and not eval(self.gt_filter):
                    continue

            fields = OrderedDict()

            for idx, col in enumerate(self.report_cols):
                if col == "*":
                    continue
                if not col.startswith("gt") and not col.startswith("GT") and not col == "info":
                    fields[col] = row[col]
                elif col == "info":
                    fields[col] = self._info_dict_to_string(info)
                else:
                    # reuse the original column name user requested
                    # e.g. replace gts[1085] with gts.NA20814
                    if '[' in col:
                        orig_col = self.gt_idx_to_name_map[col]
                        val = eval(col.strip())
                        if type(val) in [np.int8, np.int32, np.bool_]:
                            fields[orig_col] = int(val)
                        elif type(val) in [np.float32]:
                            fields[orig_col] = float(val)
                        else:
                            fields[orig_col] = val
                    else:
                        # asked for "gts" or "gt_types", e.g.
                        if col == "gts":
                            fields[col] = ','.join(gts)
                        elif col == "gt_types":
                            fields[col] = \
                                ','.join(str(t) for t in gt_types)
                        elif col == "gt_phases":
                            fields[col] = \
                                ','.join(str(p) for p in gt_phases)
                        elif col == "gt_depths":
                            fields[col] = \
                                ','.join(str(d) for d in gt_depths)
                        elif col == "gt_quals":
                            fields[col] = \
                                ','.join(str(d) for d in gt_quals)
                        elif col == "gt_ref_depths":
                            fields[col] = \
                                ','.join(str(d) for d in gt_ref_depths)
                        elif col == "gt_alt_depths":
                            fields[col] = \
                                ','.join(str(d) for d in gt_alt_depths)

            if self.show_variant_samples:
                fields["variant_samples"] = \
                    self.variant_samples_delim.join(variant_names)
                fields["HET_samples"] = \
                    self.variant_samples_delim.join(het_names)
                fields["HOM_ALT_samples"] = \
                    self.variant_samples_delim.join(hom_alt_names)
            if self.show_families:
                fields["families"] = self.variant_samples_delim.join(families)

            gemini_row = GeminiRow(fields, gts, gt_types, gt_phases,
                                   gt_depths, gt_ref_depths, gt_alt_depths,
                                   gt_quals, variant_names, het_names, hom_alt_names,
                                   hom_ref_names, unknown_names, info,
                                   formatter=self.formatter)

            if not all([predicate(gemini_row) for predicate in self.predicates]):
                continue

            if not self.for_browser:
                return gemini_row
            else:
                return fields

    def _connect_to_database(self):
        """
        Establish a connection to the requested Gemini database.
        """
        # open up a new database
        if os.path.exists(self.db):
            self.conn = sqlite3.connect(self.db)
            self.conn.isolation_level = None
            # allow us to refer to columns by name
            self.conn.row_factory = sqlite3.Row
            self.c = self.conn.cursor()

    def _collect_sample_table_columns(self):            
        """
        extract the column names in the samples table into a list
        """
        self.c.execute('select * from samples limit 1')
        self.sample_column_names = [tup[0] for tup in self.c.description]

    def _is_gt_filter_safe(self):
        """
        Test to see if the gt_filter string is potentially malicious.
        
        A future improvement would be to use pyparsing to
        traverse and directly validate the string.
        """
        if self.gt_filter is None:
            return True

        # avoid builtins
        # http://nedbatchelder.com/blog/201206/eval_really_is_dangerous.html
        if "__" in self.gt_filter:
            return False

        # avoid malicious commands
        evil = [" rm ", "os.system"]
        if any(s in self.gt_filter for s in evil):
            return False

        # make sure a "gt" col is in the string
        valid_cols = ["gts.", "gt_types.", "gt_phases.", "gt_quals.",
                      "gt_depths.", "gt_ref_depths.", "gt_alt_depths.",
                      "(gts).", "(gt_types).", "(gt_phases).", "(gt_quals).",
                      "(gt_depths).", "(gt_ref_depths).", "(gt_alt_depths)."]
        if any(s in self.gt_filter for s in valid_cols):
            return True

        # assume the worst
        return False

    def _execute_query(self):
        try:
            self.c.execute(self.query)
        except sqlite3.OperationalError as e:
            print "SQLite error: {0}".format(e)
            sys.exit("The query issued (%s) has a syntax error." % self.query)

    def _apply_query(self):
        """
        Execute a query. Intercept gt* columns and
        replace sample names with indices where necessary.
        """
        if self.needs_genes:
            self.query = self._add_gene_col_to_query()
 
        if self._query_needs_genotype_info():
            # break up the select statement into individual
            # pieces and replace genotype columns using sample
            # names with sample indices
            self._split_select()

            # we only need genotype information if the user is
            # querying the variants table
            self.query = self._add_gt_cols_to_query()

            self._execute_query()

            self.all_query_cols = [str(tuple[0]) for tuple in self.c.description
                                   if not tuple[0].startswith("gt") \
                                      and ".gt" not in tuple[0]]

            if "*" in self.select_columns:
                self.select_columns.remove("*")
                self.all_columns_orig.remove("*")
                self.all_columns_new.remove("*")
                self.select_columns += self.all_query_cols

            self.report_cols = self.all_query_cols + \
                list(OrderedSet(self.all_columns_new) - OrderedSet(self.select_columns))
        # the query does not involve the variants table
        # and as such, we don't need to do anything fancy.
        else:
            self._execute_query()
            self.all_query_cols = [str(tuple[0]) for tuple in self.c.description
                                   if not tuple[0].startswith("gt")]
            self.report_cols = self.all_query_cols

    def _correct_genotype_col(self, raw_col):
        """
        Convert a _named_ genotype index to a _numerical_
        genotype index so that the appropriate value can be
        extracted for the sample from the genotype numpy arrays.

        These lookups will be eval()'ed on the resuting rows to
        extract the appropriate information.

        For example, convert gt_types.1478PC0011 to gt_types[11]
        """
        if raw_col == "*":
            return raw_col.lower()
        # e.g., "gts.NA12878"
        elif '.' in raw_col:
            (column, sample) = raw_col.split('.', 1)
            corrected = column.lower() + "[" + str(self.sample_to_idx[sample]).lower() + "]"
        else:
            # e.g. "gts" - do nothing
            corrected = raw_col
        return corrected

    def _get_matching_sample_ids(self, wildcard):
        """
        Helper function to convert a sample wildcard
        to a list of tuples reflecting the sample indices 
        and sample names so that the wildcard
        query can be applied to the gt_* columns.
        """
        query = 'SELECT sample_id, name FROM samples '
        if wildcard != "*":
           query += ' WHERE ' + wildcard
    
        sample_info = [] # list of sample_id/name tuples
        self.c.execute(query)
        for row in self.c:
            # sample_ids are 1-based but gt_* indices are 0-based
            sample_info.append((int(row['sample_id']) - 1, str(row['name'])))
        return sample_info

    def _correct_genotype_filter(self):
        """
        This converts a raw genotype filter that contains 
        'wildcard' statements into a filter that can be eval()'ed.
        Specifically, we must convert a _named_ genotype index 
        to a _numerical_ genotype index so that the appropriate 
        value can be extracted for the sample from the genotype 
        numpy arrays.

        For example, without WILDCARDS, this converts:
        --gt-filter "(gt_types.1478PC0011 == 1)"
        
        to:
        (gt_types[11] == 1)

        With WILDCARDS, this converts things like:
            "(gt_types).(phenotype==1).(==HET)"

        to:
            "gt_types[2] == HET and gt_types[5] == HET"
        """
        corrected_gt_filter = []

        # first try to identify wildcard rules.
        wildcard_tokens = re.split(r'(\(.+?\)\.\(.+?\)\.\(.+?\))', str(self.gt_filter))
        for token in wildcard_tokens:
            # NOT a WILDCARD
            # We must then split on whitespace and
            # correct the gt_* columns:
            # e.g., "gts.NA12878" or "and gt_types.M10500 == HET"
            if (token.find("gt") >= 0 or token.find("GT") >= 0) \
                and not '.(' in token and not ')self.' in token:
                tokens = re.split(r'[\s+]+', str(token))
                for t in tokens:
                    if len(t) == 0:
                        continue
                    if (t.find("gt") >= 0 or t.find("GT") >= 0):
                        corrected = self._correct_genotype_col(t)
                        corrected_gt_filter.append(corrected)
                    else:
                        corrected_gt_filter.append(t)
            # IS a WILDCARD
            # e.g., "gt_types.(affected==1).(==HET)"
            elif (token.find("gt") >= 0 or token.find("GT") >= 0) \
                and '.(' in token and ').' in token:
                # break the wildcard into its pieces. That is:
                # (COLUMN).(WILDCARD).(WILDCARD_RULE)
                (column, wildcard, wildcard_rule) = token.split('.')

                # remove the syntactic parentheses
                column = column.strip('(').strip(')')
                wildcard = wildcard.strip('(').strip(')')
                wildcard_rule = wildcard_rule.strip('(').strip(')')
                
                # convert "gt_types.(affected==1).(==HET)"
                # to, e.g.,: gt_types[3] == HET and gt_types[9] == HET
                sample_info = self._get_matching_sample_ids(wildcard)
                for (idx, sample) in enumerate(sample_info):
                    if idx < len(sample_info) - 1:
                        rule = column + '[' + str(sample[0]) + '] ' + wildcard_rule + ' and '
                    else:
                        rule = column + '[' + str(sample[0]) + '] ' + wildcard_rule
                    corrected_gt_filter.append(rule)
            else:
                if len(token) > 0:
                    corrected_gt_filter.append(token)

        return " ".join(corrected_gt_filter)


    def _add_gt_cols_to_query(self):
        """
        We have to modify the raw query to select the genotype
        columns in order to support the genotype filters.  That is,
        if the user wants to limit the rows returned based upon, for example,
        "gts.joe == 1", then we need to select the full gts BLOB column in
        order to enforce that limit.  The user wouldn't have selected gts as a
        columns, so therefore, we have to modify the select statement to add
        it.

        In essence, when a gneotype filter has been requested, we always add
        the gts, gt_types and gt_phases columns.
        """

        if "from" not in self.query.lower():
            sys.exit("Malformed query: expected a FROM keyword.")

        (select_tokens, rest_of_query) = get_select_cols_and_rest(self.query)

        # remove any GT columns
        select_clause_list = []
        for token in select_tokens:
            if not token.startswith("gt") and \
               not token.startswith("GT") and \
               not ".gt" in token and \
               not ".GT" in token and \
               not token.startswith("(gt") and \
               not token.startswith("(GT"):
                select_clause_list.append(token)

        # reconstruct the query with the GT* columns added
        if len(select_clause_list) > 0:
            select_clause = ",".join(select_clause_list) + \
                    ", gts, gt_types, gt_phases, gt_depths, \
                       gt_ref_depths, gt_alt_depths, gt_quals "

        else:
            select_clause = ",".join(select_clause_list) + \
                    " gts, gt_types, gt_phases, gt_depths, \
                      gt_ref_depths, gt_alt_depths, gt_quals "

        self.query = "select " + select_clause + rest_of_query

        # extract the original select columns
        return self.query

    def _add_gene_col_to_query(self):
        """
        Add the gene column to the list of SELECT'ed columns
        in a query.
        """
        if "from" not in self.query.lower():
            sys.exit("Malformed query: expected a FROM keyword.")

        (select_tokens, rest_of_query) = get_select_cols_and_rest(self.query)

        if not any("gene" in s for s in select_tokens):

            select_clause = ",".join(select_tokens) + \
                        ", gene "
            
            self.query = "select " + select_clause + rest_of_query

        return self.query

    def _split_select(self):
        """
        Build a list of _all_ columns in the SELECT statement
        and segregated the non-genotype specific SELECT columns.

        This is used to control how to report the results, as the
        genotype-specific columns need to be eval()'ed whereas others
        do not.

        For example: "SELECT chrom, start, end, gt_types.1478PC0011"
        will populate the lists as follows:

        select_columns = ['chrom', 'start', 'end']
        all_columns = ['chrom', 'start', 'end', 'gt_types[11]']
        """
        self.select_columns = []
        self.all_columns_new = []
        self.all_columns_orig = []
        self.gt_name_to_idx_map = {}
        self.gt_idx_to_name_map = {}

        # iterate through all of the select columns andclear
        # distinguish the genotype-specific columns from the base columns
        if "from" not in self.query.lower():
            sys.exit("Malformed query: expected a FROM keyword.")

        (select_tokens, rest_of_query) = get_select_cols_and_rest(self.query)

        for token in select_tokens:
            
            # it is a WILDCARD
            if (token.find("gt") >= 0 or token.find("GT") >= 0) \
                and '.(' in token and ').' in token:
                # break the wildcard into its pieces. That is:
                # (COLUMN).(WILDCARD)
                (column, wildcard) = token.split('.')

                # remove the syntactic parentheses
                wildcard = wildcard.strip('(').strip(')')
                column = column.strip('(').strip(')')

                # convert "gt_types.(affected==1)"
                # to: gt_types[3] == HET and gt_types[9] == HET
                sample_info = self._get_matching_sample_ids(wildcard)
                
                # maintain a list of the sample indices that should
                # be displayed as a result of the SELECT'ed wildcard
                wildcard_indices = []
                for (idx, sample) in enumerate(sample_info):
                    wildcard_display_col = column + '.' + str(sample[1])
                    wildcard_mask_col = column + '[' + str(sample[0]) + ']'
                    wildcard_indices.append(sample[0])

                    new_col = wildcard_mask_col
                    self.all_columns_new.append(new_col)
                    self.all_columns_orig.append(wildcard_display_col)
                    self.gt_name_to_idx_map[wildcard_display_col] = wildcard_mask_col
                    self.gt_idx_to_name_map[wildcard_mask_col] = wildcard_display_col

            # it is a basic genotype column
            elif (token.find("gt") >= 0 or token.find("GT") >= 0) \
                and '.(' not in token and not ').' in token:
                new_col = self._correct_genotype_col(token)

                self.all_columns_new.append(new_col)
                self.all_columns_orig.append(token)
                self.gt_name_to_idx_map[token] = new_col
                self.gt_idx_to_name_map[new_col] = token

            # it is neither
            else:
                self.select_columns.append(token)
                self.all_columns_new.append(token)
                self.all_columns_orig.append(token)

    def _info_dict_to_string(self, info):
        """
        Flatten the VCF info-field OrderedDict into a string, 
        including all arrays for allelic-level info.
        """
        if info is not None:
            return ';'.join(['%s=%s' % (key, value) if not isinstance(value, list) \
                             else '%s=%s' % (key, ','.join([str(v) for v in value])) \
                             for (key, value) in info.items()])
        else:
            return None
    
    def _tokenize_query(self):
        tokens = list(flatten([x.split(",") for x in self.query.split(" ")]))
        return tokens

    def _query_needs_genotype_info(self):
        tokens = self._tokenize_query()
        requested_genotype = "variants" in tokens and \
                            (any([x.startswith("gt") for x in tokens]) or \
                             any([x.startswith("(gt") for x in tokens]) or \
                             any(".gt" in x for x in tokens))   
        return requested_genotype or \
               self.include_gt_cols or \
               self.show_variant_samples or \
               self.needs_genotypes

def select_formatter(args):
    SUPPORTED_FORMATS = {x.name.lower(): x for x in
                         itersubclasses(RowFormat)}

    if hasattr(args, 'carrier_summary') and args.carrier_summary:
        return SUPPORTED_FORMATS["carrier_summary"](args)

    if not args.format in SUPPORTED_FORMATS:
        raise NotImplementedError("Conversion to %s not supported. Valid "
                                  "formats are %s."
                                  % (args.format, SUPPORTED_FORMATS))
    else:
        return SUPPORTED_FORMATS[args.format](args)


def flatten(l):
    """
    flatten an irregular list of lists
    example: flatten([[[1, 2, 3], [4, 5]], 6]) -> [1, 2, 3, 4, 5, 6]
    lifted from: http://stackoverflow.com/questions/2158395/

    """
    for el in l:
        if isinstance(el, collections.Iterable) and not isinstance(el,
                                                                   basestring):
            for sub in flatten(el):
                yield sub
        else:
            yield el

if __name__ == "__main__":

    db = sys.argv[1]

    gq = GeminiQuery(db)

    print "test a basic query with no genotypes"
    query = "select chrom, start, end from variants limit 5"
    gq.run(query)
    for row in gq:
        print row

    print "test a basic query with no genotypes using a header"
    query = "select chrom, start, end from variants limit 5"
    gq.run(query)
    print gq.header
    for row in gq:
        print row

    print "test query that selects a sample genotype"
    query = "select chrom, start, end, gts.NA20814 from variants limit 5"
    gq.run(query)
    for row in gq:
        print row

    print "test query that selects a sample genotype and uses a header"
    query = "select chrom, start, end, gts.NA20814 from variants limit 5"
    gq.run(query)
    print gq.header
    for row in gq:
        print row

    print "test query that selects and _filters_ on a sample genotype"
    query = "select chrom, start, end, gts.NA20814 from variants limit 50"
    db_filter = "gt_types.NA20814 == HET"
    gq.run(query, db_filter)
    for row in gq:
        print row

    print "test query that selects and _filters_ on a sample genotype and uses a filter"
    query = "select chrom, start, end, gts.NA20814 from variants limit 50"
    db_filter = "gt_types.NA20814 == HET"
    gq.run(query, db_filter)
    print gq.header
    for row in gq:
        print row

    print "test query that selects and _filters_ on a sample genotype and uses a filter and a header"
    query = "select chrom, start, end, gts.NA20814 from variants limit 50"
    db_filter = "gt_types.NA20814 == HET"
    gq.run(query, db_filter)
    print gq.header
    for row in gq:
        print row

    print "demonstrate accessing individual columns"
    query = "select chrom, start, end, gts.NA20814 from variants limit 50"
    db_filter = "gt_types.NA20814 == HET"
    gq.run(query, db_filter)
    for row in gq:
        print row['chrom'], row['start'], row['end'], row['gts.NA20814']

########NEW FILE########
__FILENAME__ = gemini_actionable_mutations
#!/usr/bin/env python
import sqlite3
from collections import defaultdict
from gemini_constants import *
import gemini_subjects
from dgidb import query_dgidb
import GeminiQuery

def get_tumor_normal_pairs(args):
    conn = sqlite3.connect(args.db)
    conn.isolation_level = None
    conn.row_factory = sqlite3.Row
    c = conn.cursor()
    return gemini_subjects.get_families(c)

def get_actionable_mutations(parser, args):

    t_n_pairs = get_tumor_normal_pairs(args)

    query = "SELECT variants.chrom, start, end, ref, alt, \
                    variants.gene, impact, is_somatic, \
                    gene_summary.in_cosmic_census \
             FROM variants, gene_summary \
             WHERE variants.is_somatic = 1 \
             AND variants.is_exonic = 1 \
             AND variants.chrom = gene_summary.chrom \
             AND variants.gene = gene_summary.gene \
             AND gene_summary.in_cosmic_census = 1"


    # collect the relevant genes and query DGIDB
    gq = GeminiQuery.GeminiQuery(args.db)
    gq.run(query)

    genes = defaultdict()
    for row in gq:
      genes[row['gene']] = True
    # collect info from DGIdb
    dgidb_info = query_dgidb(genes)


    # now rerun the query and report actionable mutations per DGIDB and COSMIC census.
    gq = GeminiQuery.GeminiQuery(args.db)
    gq.run(query)
    print'\t'.join(['tum_name', 'chrom', 'start', 'end', 'ref', 'alt', \
                    'gene', 'impact', 'is_somatic', 'in_cosmic_census', 'dgidb_info'])
    for row in gq:

        for pair in t_n_pairs:
            samples = pair.subjects
            if len(samples) != 2:
                continue

            tumor = pair.subjects[0]
            normal = pair.subjects[1]
            # swap if we guessed the tumor incorrectly
            if tumor.affected is False:
                tumor, normal = normal, tumor

            print'\t'.join(str(s) for s in [tumor.name, row['chrom'], \
                                            row['start'], row['end'], \
                                            row['ref'], row['alt'], \
                                            row['gene'], row['impact'], \
                                            row['is_somatic'], \
                                            row['in_cosmic_census'], \
                                            str(dgidb_info[row['gene']])])

########NEW FILE########
__FILENAME__ = gemini_amend
import GeminiQuery
from gemini_subjects import get_subjects
from ped import load_ped_file, get_ped_fields
from gemini_utils import quote_string
import sqlite3
from database import database_transaction

def amend(parser, args):
    if args.db is None:
        parser.print_help()
        exit("ERROR: amend needs a database file.")
    if args.sample:
        amend_sample(args)

def amend_sample(args):
    loaded_subjects = get_subjects(args)
    ped_dict = load_ped_file(args.sample)
    header = get_ped_fields(args.sample)
    with database_transaction(args.db) as c:
        for k, v in loaded_subjects.items():
            if k in ped_dict:
                item_list = map(quote_string, ped_dict[k])
                sample = zip(header, item_list)
                set_str = ",".join([str(x) + "=" + str(y) for (x, y) in sample])
                sql_query = "update samples set {0} where sample_id={1}"
                c.execute(sql_query.format(set_str, v.sample_id))


########NEW FILE########
__FILENAME__ = gemini_annotate
    #!/usr/bin/env python

import os
import sys
import sqlite3
from collections import defaultdict
import numpy as np
from scipy.stats import mode
import pysam

from gemini.annotations import annotations_in_region, guess_contig_naming
from database import database_transaction

def add_requested_columns(args, update_cursor, col_names, col_types=None):
    """
    Attempt to add new, user-defined columns to the
    variants table.  Warn if the column already exists.
    """

    if args.anno_type in ["count", "boolean"]:

        col_name = col_names[0]
        col_type = "integer"

        try:
            alter_qry = "ALTER TABLE variants ADD COLUMN " \
                        + col_name \
                        + " " \
                        + col_type \
                        + " " \
                        + "DEFAULT NULL"
            update_cursor.execute(alter_qry)
        except sqlite3.OperationalError:
            sys.stderr.write("WARNING: Column \"("
                             + col_name
                             + ")\" already exists in variants table. Overwriting values.\n")
    elif args.anno_type == "extract":

        for col_name, col_type in zip(col_names, col_types):

            try:
                alter_qry = "ALTER TABLE variants ADD COLUMN " \
                            + col_name \
                            + " " \
                            + col_type \
                            + " " \
                            + "DEFAULT NULL"
                update_cursor.execute(alter_qry)
            except sqlite3.OperationalError:
                sys.stderr.write("WARNING: Column \"("
                                 + col_name
                                 + ")\" already exists in variants table. Overwriting values.\n")
    else:
        sys.exit("Unknown annotation type: %s\n" % args.anno_type)


def _annotate_variants(args, conn, get_val_fn, col_names=None, col_types=None, col_ops=None):
    """Generalized annotation of variants with a new column.

    get_val_fn takes a list of annotations in a region and returns
    the value for that region to update the database with.

    Separates selection and identification of values from update,
    to avoid concurrent database access errors from sqlite3, especially on
    NFS systems. The retained to_update list is small, but batching
    could help if memory issues emerge.
    """
    # For each, use Tabix to detect overlaps with the user-defined
    # annotation file.  Update the variant row with T/F if overlaps found.
    anno = pysam.Tabixfile(args.anno_file)
    naming = guess_contig_naming(anno)
    select_cursor = conn.cursor()
    update_cursor = conn.cursor()
    add_requested_columns(args, select_cursor, col_names, col_types)

    last_id = 0
    current_id = 0
    total = 0
    CHUNK_SIZE = 100000
    to_update = []

    select_cursor.execute('''SELECT chrom, start, end, variant_id FROM variants''')
    while True:
        for row in select_cursor.fetchmany(CHUNK_SIZE):

            # update_data starts out as a list of the values that should
            # be used to populate the new columns for the current row.
            update_data = get_val_fn(annotations_in_region(row,
                                                    anno,
                                                    "tuple",
                                                    naming))
            # were there any hits for this row?
            if len(update_data) > 0:
                # we add the primary key to update_data for the
                # where clause in the SQL UPDATE statement.
                update_data.append(str(row["variant_id"]))
                to_update.append(tuple(update_data))

            current_id = row["variant_id"]

        if current_id <= last_id:
            break
        else:
            update_cursor.execute("BEGIN TRANSACTION")
            _update_variants(to_update, col_names, update_cursor)
            update_cursor.execute("END TRANSACTION")

            total += len(to_update)
            print "updated", total, "variants"
            last_id = current_id
        to_update = []

def _update_variants(to_update, col_names, cursor):
        update_qry = "UPDATE variants SET "

        update_cols = ",".join(col_name + " = ?" for col_name in col_names)
        update_qry += update_cols
        update_qry += " WHERE variant_id = ?"
        cursor.executemany(update_qry, to_update)


def annotate_variants_bool(args, conn, col_names):
    """
    Populate a new, user-defined column in the variants
    table with a BOOLEAN indicating whether or not
    overlaps were detected between the variant and the
    annotation file.
    """
    def has_hit(hits):
        for hit in hits:
            return [1]
        return [0]

    return _annotate_variants(args, conn, has_hit, col_names)


def annotate_variants_count(args, conn, col_names):
    """
    Populate a new, user-defined column in the variants
    table with a INTEGER indicating the count of overlaps
    between the variant and the
    annotation file.
    """
    def get_hit_count(hits):
        return [len(list(hits))]

    return _annotate_variants(args, conn, get_hit_count, col_names)


def annotate_variants_extract(args, conn, col_names, col_types, col_ops, col_idxs):
    """
    Populate a new, user-defined column in the variants
    table based on the value(s) from a specific column.
    in the annotation file.
    """

    def _map_list_types(hit_list, col_type):
        try:
            if col_type == "int":
                return [int(h) for h in hit_list]
            elif col_type == "float":
                return [float(h) for h in hit_list]
        except ValueError:
            sys.exit('Non-numeric value found in annotation file: %s\n' % (','.join(hit_list)))

    def summarize_hits(hits):

        hits = list(hits)
        if len(hits) == 0:
            return []

        hit_list = defaultdict(list)
        for hit in hits:
            try:
                for idx, col_idx in enumerate(col_idxs):
                    hit_list[idx].append(hit[int(col_idx) - 1])
            except IndexError:
                sys.exit("EXITING: Column " + args.col_extracts + " exceeds "
                          "the number of columns in your "
                          "annotation file.\n")

        vals = []
        for idx, op in enumerate(col_ops):
            # more than one overlap, must summarize
            if op == "mean":
                val = np.average(_map_list_types(hit_list[idx], col_types[idx]))
            elif op == 'list':
                val = ",".join(hit_list[idx])
            elif op == 'uniq_list':
                val = ",".join(set(hit_list[idx]))
            elif op == 'median':
                val = np.median(_map_list_types(hit_list[idx], col_types[idx]))
            elif op == 'min':
                val = np.min(_map_list_types(hit_list[idx], col_types[idx]))
            elif op == 'max':
                val = np.max(_map_list_types(hit_list[idx], col_types[idx]))
            elif op == 'mode':
                val = mode(_map_list_types(hit_list[idx], col_types[idx]))[0][0]
            elif op == 'first':
                val = hit_list[idx][0]
            elif op == 'last':
                val = hit_list[idx][-1]
            else:
                sys.exit("EXITING: Operation (-o) \"" + op + "\" not recognized.\n")

            if col_types[idx] == "int":
                try:
                    vals.append(int(val))
                except ValueError:
                    sys.exit ('Non-integer value found in annotation file: %s\n' % (val))
            elif col_types[idx] == "float":
                try:
                    vals.append(float(val))
                except ValueError:
                    sys.exit ('Non-float value found in annotation file: %s\n' % (val))
            else:
                vals.append(val)

        return vals


    return _annotate_variants(args, conn, summarize_hits,
                              col_names, col_types, col_ops)




def annotate(parser, args):

    def _validate_args(args):
        if (args.col_operations or args.col_types or args.col_extracts):
            sys.exit('EXITING: You may only specify a column name (-c) when '
                     'using \"-a boolean\" or \"-a count\".\n')

        col_names = args.col_names.split(',')
        if len(col_names) > 1:
            sys.exit('EXITING: You may only specify a single column name (-c) '
                     'when using \"-a boolean\" or \"-a count\".\n')
        return col_names

    def _validate_extract_args(args):
        col_ops = args.col_operations.split(',')
        col_names = args.col_names.split(',')
        col_types = args.col_types.split(',')
        col_idxs  = args.col_extracts.split(',')

        supported_types = ['text', 'float', 'integer']
        for col_type in col_types:
            if col_type not in supported_types:
                sys.exit('EXITING: Column type [%s] not supported.\n' % \
                         (col_type))

        supported_ops = ['mean', 'median', 'mode', 'min', 'max', 'first', \
                         'last', 'list', 'uniq_list']
        for col_op in col_ops:
            if col_op not in supported_ops:
                sys.exit('EXITING: Column operation [%s] not supported.\n' % \
                         (col_op))

        if not (len(col_ops) == len(col_names) == \
                len(col_types) == len(col_idxs)):
            sys.exit('EXITING: The number of column names, numbers, types, and '
                     'operations must match: [%s], [%s], [%s], [%s]\n' % \
                     (args.col_names, args.col_extracts, args.col_types, args.col_operations))

        return col_names, col_types, col_ops, col_idxs



    if (args.db is None):
        parser.print_help()
        exit(1)
    if not os.path.exists(args.db):
        sys.stderr.write("Error: cannot find database file.")
        exit(1)
    if not os.path.exists(args.anno_file):
        sys.stderr.write("Error: cannot find annotation file.")
        exit(1)

    conn = sqlite3.connect(args.db)
    conn.row_factory = sqlite3.Row  # allow us to refer to columns by name
    conn.isolation_level = None

    if args.anno_type == "boolean":
        col_names = _validate_args(args)
        annotate_variants_bool(args, conn, col_names)
    elif args.anno_type == "count":
        col_names = _validate_args(args)
        annotate_variants_count(args, conn, col_names)
    elif args.anno_type == "extract":
        if args.col_extracts is None:
            sys.exit("You must specify which column to "
                     "extract from your annotation file.")
        else:
            col_names, col_types, col_ops, col_idxs = _validate_extract_args(args)
            annotate_variants_extract(args, conn, col_names, col_types, col_ops, col_idxs)
    else:
        sys.exit("Unknown column type requested. Exiting.")

    conn.close()

    # index on the newly created columns
    for col_name in col_names:
        with database_transaction(args.db) as c:
            c.execute('''create index %s on variants(%s)''' % (col_name + "idx", col_name))

########NEW FILE########
__FILENAME__ = gemini_browser
import os
import warnings
from collections import namedtuple

import GeminiQuery

import tool_de_novo_mutations as de_novo_tool
import tool_autosomal_recessive as recessive_tool
import tool_autosomal_dominant as dominant_tool

# based upon bottle example here:
# https://bitbucket.org/timtan/bottlepy-in-real-case

# -- determine where I launch python and config lib path
# base_dir = os.path.dirname(__file__)
# third_party_path = os.path.abspath(os.path.join(base_dir, 'third_party' ))
# sys.path.insert(0, third_party_path)

# -- common bottle importation
with warnings.catch_warnings():
    warnings.simplefilter("ignore")
    from bottle import TEMPLATE_PATH, Bottle, run, static_file, debug, request
    from bottle import jinja2_view as view, jinja2_template as template

debug(True)


base_dir = os.path.dirname(__file__)
TEMPLATE_PATH.append(os.path.abspath(os.path.join(base_dir, 'views')))

# -- the instance app is important
app = Bottle()

# -- serve static files, files located in static
static_folder = 'static'
_static_folder = os.path.join(os.path.dirname(__file__), static_folder)

@app.route('/stats/region/:chrom',method='GET')
def stats_region(chrom):
    # Note: chrom is give as an argument

    # we then extract start and end using HTML GET    
    start = request.GET.get('start', '').strip()
    end = request.GET.get('end', '').strip()

    # construct a query
    query =  "SELECT start, end from variants"
    query += " WHERE chrom = '" + chrom + "'"
    query += " AND start >= " + start
    query += " AND end <= " + end

    # issue the query
    gq = GeminiQuery.GeminiQuery(database)
    gq._set_gemini_browser(True)
    gq.run(query)

    # return query results in JSON format
    return{'features': [dict(row) for row in gq]}


@app.route('/static/<filepath:path>')
def server_static(filepath):
    return static_file(filepath, root=_static_folder)
# -- end of static folder configuration

# -- index page routing


@app.route('/index')
@app.route('/')
def index():
    return template('index.j2')

@app.route('/query_json', method='GET')
def query_json():
    query = request.GET.get('query', '').strip()
    
    gq = GeminiQuery.GeminiQuery(database)
    gq._set_gemini_browser(True)
    gq.run(query)
    
    return {'gemini_results': [dict(row) for row in gq]}


@app.route('/query', method='GET')
def query():

    def _get_fields():
        query = request.GET.get('query', '').strip()
        gt_filter = request.GET.get('gt_filter', '').strip()
        use_header = request.GET.get('use_header')
        igv_links = request.GET.get('igv_links')
        return query, gt_filter, use_header, igv_links
    
    # user clicked the "submit" button
    if request.GET.get('submit', '').strip():

        (query, gt_filter, use_header, igv_links) = _get_fields()

        if use_header: use_header = True
        if igv_links: igv_links = True

        gq = GeminiQuery.GeminiQuery(database)
        gq._set_gemini_browser(True)
        gq.run(query, gt_filter)


        if len(query) == 0:
            return template('query.j2', dbfile=database)

        if igv_links and ('chrom' not in query.lower()
                          or 'start' not in query.lower()
                          or 'end' not in query.lower()):
            return template('query.j2', dbfile=database,
                            rows=gq,
                            igv_links=igv_links,
                            igv_links_error=True,
                            use_header=use_header,
                            gt_filter=gt_filter,
                            query=query)
        else:
            return template('query.j2', dbfile=database,
                            rows=gq,
                            igv_links=igv_links,
                            igv_links_error=False,
                            use_header=use_header,
                            gt_filter=gt_filter,
                            query=query)

    # user clicked the "save to file" button
    elif request.GET.get('save', '').strip():

        (query, gt_filter, use_header, igv_links) = _get_fields()

        gq = GeminiQuery.GeminiQuery(database)
        gq.run(query, gt_filter)

        if len(query) == 0:
            return template('query.j2', dbfile=database)

        # dump the results to a text file.  this will be
        # stored in /static and a link will be given to
        # the user.
        tmp_file = '/tmp.txt'
        tmp = open(_static_folder + tmp_file, 'w')
        for row in gq:
            tmp.write('\t'.join(str(c) for c in row) + '\n')
        tmp.close()

        return template('query.j2', dbfile=database,
                        tmp_file=tmp_file,
                        igv_links=igv_links,
                        igv_links_error=True,
                        use_header=use_header,
                        gt_filter=gt_filter,
                        query=query)
    # user did nothing.
    else:
        return template('query.j2', dbfile=database)




@app.route('/de_novo', method='GET')
def de_novo():

    Arguments = namedtuple('Arguments', ['db', 'min_sample_depth'], verbose=True)
    # user clicked the "submit" button
    if request.GET.get('submit', '').strip():

        min_sample_depth = str(request.GET.get('min-depth', '').strip())
        igv_links = request.GET.get('igv_links')

        args = Arguments(db=database, min_sample_depth=min_sample_depth)

        de_novo_factory = \
            GeminiInheritanceModelFactory(args, model="de_novo")
        de_novo_factory.get_candidates()

        if len(min_sample_depth) == 0:
            row_iter = \
                de_novo_tool.get_de_novo_candidates(gq.c)
        else:
            row_iter = \
                de_novo_tool.get_de_novo_candidates(gq.c, int(min_sample_depth))

        return template('de_novo.j2', dbfile=database,
                        rows=row_iter,
                        igv_links=igv_links)

    else:
        return template('de_novo.j2', dbfile=database)


@app.route('/auto_rec', method='GET')
def auto_rec():

    # user clicked the "submit" button
    if request.GET.get('submit', '').strip():

        c = connect_to_db(database)

        row_iter = \
            recessive_tool.get_auto_recessive_candidates(c)

        return template('auto_rec.j2', dbfile=database, rows=row_iter)

    else:
        return template('auto_rec.j2', dbfile=database)


@app.route('/auto_dom', method='GET')
def auto_dom():

    # user clicked the "submit" button
    if request.GET.get('submit', '').strip():

        c = connect_to_db(database)

        row_iter = \
            dominant_tool.get_auto_dominant_candidates(c)

        return template('auto_dom.j2', dbfile=database, rows=row_iter)

    else:
        return template('auto_dom.j2', dbfile=database)


@app.route('/db_schema', method='GET')
def db_schema():
    return template('db_schema.j2')


def browser_main(parser, args):
    global database

    print "!!!!!"
    print "NOTE: open a browser and point it to http://localhost:8088/query"
    print "!!!!!"

    database = args.db

    run(app, host='localhost', port=8088,
        reloader=True, debug=True)

########NEW FILE########
__FILENAME__ = gemini_constants
#!/usr/bin/env python

VARIANTS_KEY = "variant_id"
BUFFER_SIZE  = 10000


# genotype encoding.
# 0 / 00000000 hom ref
# 1 / 00000001 het
# 2 / 00000010 unknown
# 3 / 00000011 hom alt
HOM_REF = 0
HET = 1
UNKNOWN = 2
HOM_ALT = 3

MISSING = None
UNAFFECTED = 1
AFFECTED = 2

########NEW FILE########
__FILENAME__ = gemini_dbinfo
#!/usr/bin/env python
import sqlite3
import os
import gemini_utils as util


def get_table_info(c, table_name, out_template):
    """
    Report the column names and types for a given database table
    """
    query = "PRAGMA table_info('" + table_name + "')"
    c.execute(query)

    for row in c:
        rec = (table_name, str(row['name']), str(row['type']))
        print out_template.format(*rec)


def db_info(parser, args):

    if os.path.exists(args.db):
        conn = sqlite3.connect(args.db)
        conn.isolation_level = None
        conn.row_factory = sqlite3.Row
        c = conn.cursor()

        # column widths for the output
        out_template = "{0:20}{1:30}{2:10}"

        # header
        print out_template.format("table_name", "column_name", "type")
        for table in ['variants', 'variant_impacts', 'samples', 'gene_detailed', 'gene_summary']:
            get_table_info(c, table, out_template)

########NEW FILE########
__FILENAME__ = gemini_dump
#!/usr/bin/env python
import sqlite3
import numpy as np
import zlib
import re
import os
import cPickle

import gemini_utils as util
from GeminiQuery import GeminiQuery


def get_variants(c, args):
    """
    Report all columns in the variant table, except for the
    genotype vectors.
    """
    query = "SELECT * FROM variants \
             ORDER BY chrom, start"
    c.execute(query)

    # build a list of all the column indices that are NOT
    # gt_* columns.  These will be the columns reported
    (col_names, non_gt_idxs) = \
        util.get_col_names_and_indices(c.description, ignore_gt_cols=True)

    if args.use_header:
        print args.separator.join(col for col in col_names)
    for row in c:
        print args.separator.join(str(row[i]) if row[i] is not None else "." \
                                              for i in non_gt_idxs )


def get_genotypes(c, args):
    """For each variant, report each sample's genotype
       on a separate line.
    """
    idx_to_sample = util.map_indices_to_samples(c)

    query = "SELECT  v.chrom, v.start, v.end, \
                     v.ref, v.alt, \
                     v.type, v.sub_type, \
                     v.aaf, v.in_dbsnp, v.gene, \
                     v.gts \
             FROM    variants v \
             ORDER BY chrom, start"
    c.execute(query)

    # build a list of all the column indices that are NOT
    # gt_* columns.  These will be the columns reported
    (col_names, non_gt_idxs) = \
        util.get_col_names_and_indices(c.description, ignore_gt_cols=True)
    col_names.append('sample')
    col_names.append('genotype')

    if args.use_header:
        print args.separator.join(col for col in col_names)
    for row in c:
        gts = np.array(cPickle.loads(zlib.decompress(row['gts'])))
        for idx, gt in enumerate(gts):
            # xrange(len(row)-1) to avoid printing v.gts
            print args.separator.join(str(row[i]) for i in xrange(len(row)-1)),
            print args.separator.join([idx_to_sample[idx], gt])


def get_samples(c, args):
    """
    Report all of the information about the samples in the DB
    """
    query = "SELECT * FROM samples"
    c.execute(query)

    (col_names, col_idxs) = util.get_col_names_and_indices(c.description)
    if args.use_header:
        print args.separator.join(col_names)
    for row in c:
        print args.separator.join(str(row[i]) if row[i] is not None else "." \
                                              for i in xrange(len(row)) )


def tfam(args):
    """
    Report the information about the samples in the DB in TFAM format:
    http://pngu.mgh.harvard.edu/~purcell/plink/data.shtml
    """

    query = ("select family_id, name, paternal_id, maternal_id, "
             "sex, phenotype from samples")
    gq = GeminiQuery(args.db)
    gq.run(query)
    for row in gq:
        print " ".join(map(str, [row['family_id'], row['name'], row['paternal_id'],
                        row['maternal_id'], row['sex'], row['phenotype']]))


def dump(parser, args):

    if os.path.exists(args.db):
        conn = sqlite3.connect(args.db)
        conn.isolation_level = None
        conn.row_factory = sqlite3.Row
        c = conn.cursor()

        if args.variants:
            get_variants(c, args)
        elif args.genotypes:
            get_genotypes(c, args)
        elif args.samples:
            get_samples(c, args)
        elif args.tfam:
            tfam(args)



########NEW FILE########
__FILENAME__ = gemini_inheritance_model_utils
#!/usr/bin/env python
import sqlite3
import os
import sys
from collections import defaultdict
import GeminiQuery
import sql_utils
from gemini_constants import *
import gemini_subjects as subjects


class GeminiInheritanceModelFactory(object):

    def __init__(self, args, model):
        self.args = args
        self.model = model
        self.gq = GeminiQuery.GeminiQuery(args.db, include_gt_cols=True)


    def get_candidates(self):
        """
        Report candidate variants that meet the requested inheritance model.
        """
        if self.model in ["auto_dom", "auto_rec"]:
            self._get_gene_only_candidates()
        else:
            self._get_all_candidates()


    def _report_candidates(self):
        """
        Print variants that meet the user's requirements
        """
        num_families = self.candidates.keys()

        if len(num_families) >= self.args.min_kindreds:

            for (gene, family_id) in self.candidates:
                for (row, family_gt_label, family_gt_cols, family_dp_cols) \
                    in self.candidates[(gene, family_id)]:

                    gt_types = row['gt_types']
                    gts = row['gts']
                    gt_depths = row['gt_depths']

                    print str(family_id) + "\t" + \
                        ",".join([str(s) for s in family_gt_label]) + \
                        "\t", \
                        ",".join([str(eval(s)) for s in family_gt_cols]) + \
                        "\t", \
                        ",".join([str(eval(s)) for s in family_dp_cols]) + \
                        "\t",
                    print row

    def _get_family_info(self):
        """
        Extract the relevant genotype filters, as well all labels
        for each family in the database.
        """
        families = subjects.get_families(self.args.db)
        self.family_ids = []
        self.family_masks = []
        self.family_gt_labels = []
        self.family_gt_columns = []
        self.family_dp_columns = []
        for family in families:

            family_filter = None
            
            if self.model == "auto_rec":
                family_filter = family.get_auto_recessive_filter()
            elif self.model == "auto_dom":
                family_filter = family.get_auto_dominant_filter()
            elif self.model == "de_novo":
                family_filter = family.get_de_novo_filter()

            if family_filter != "False" and family_filter is not None:
                self.family_masks.append(family_filter)
                self.family_gt_labels.append(family.get_genotype_labels())
                self.family_gt_columns.append(family.get_genotype_columns())
                self.family_dp_columns.append(family.get_genotype_depths())
                self.family_ids.append(family.family_id)

    def _construct_query(self):
        """
        Construct the relevant query based on the user's requests.
        """
        if self.args.columns is not None:
            # the user only wants to report a subset of the columns
            self.query = "SELECT " + str(self.args.columns) + \
                    " FROM variants "
        else:
            # report the kitchen sink
            self.query = "SELECT *" + \
                    ", gts, gt_types, gt_phases, gt_depths, \
                    gt_ref_depths, gt_alt_depths, gt_quals" + \
                    " FROM variants "

        # add any non-genotype column limits to the where clause
        if self.args.filter:
            self.query += " WHERE " + self.args.filter
        
        # auto_rec and auto_dom candidates should be limited to
        # variants affecting genes.
        if self.model == "auto_rec" or self.model == "auto_dom":

            # we require the "gene" column for the auto_* tools
            self.query = sql_utils.ensure_columns(self.query, ['gene'])
            if self.args.filter:
                self.query += " AND gene is not NULL ORDER BY gene"
            else:
                self.query += " WHERE gene is not NULL ORDER BY gene"

    def _get_gene_only_candidates(self):
        """
        Identify candidates that meet the user's criteria AND affect genes.
        """
        # collect family info
        self._get_family_info()

        # run the query applying any genotype filters provided by the user.
        self._construct_query()
        self.gq.run(self.query)

        # print a header
        print "family_id\tfamily_members\tfamily_genotypes\tfamily_genotype_depths\t",
        print self.gq.header

        # yield the resulting variants for this familiy
        self.candidates = defaultdict(list)
        prev_gene = None
        for row in self.gq:

            curr_gene = row['gene']
        
            # report any candidates for the previous gene
            if curr_gene != prev_gene and prev_gene is not None:
                self._report_candidates()
                # reset for the next gene
                self.candidates = defaultdict(list)

            # test the variant for each family in the db
            for idx, fam_id in enumerate(self.family_ids):
                family_genotype_mask = self.family_masks[idx]
                family_gt_labels = self.family_gt_labels[idx]
                family_gt_cols = self.family_gt_columns[idx]
                family_dp_cols = self.family_dp_columns[idx]

                # interrogate the genotypes present in each family member to 
                # conforming to the genetic model being tested
                gt_types = row['gt_types']
                gts = row['gts']
                gt_depths = row['gt_depths']

                # skip if the variant doesn't meet a recessive model
                # for this family
                if not eval(family_genotype_mask):
                    continue

                # make sure each sample's genotype had sufficient coverage.
                # otherwise, ignore
                insufficient_depth = False
                for col in family_dp_cols:
                    depth = int(eval(col))
                    if depth < self.args.min_sample_depth:
                        insufficient_depth = True
                        break
                if insufficient_depth is True:
                    continue

                # if it meets a recessive model, add it to the list
                # of candidates for this gene.
                self.candidates[(curr_gene, fam_id)].append((row,
                                                        family_gt_labels, 
                                                        family_gt_cols,
                                                        family_dp_cols))

            prev_gene = curr_gene
    
        # report any candidates for the last gene
        self._report_candidates()
        
    def _get_all_candidates(self):
        """
        Identify candidates that meet the user's criteria no matter where
        they occur in the genome.
        """
        """
        Identify candidates that meet the user's criteria AND affect genes.
        """
        # collect family info
        self._get_family_info()

        # run the query applying any genotype filters provided by the user.
        self._construct_query()
        self.gq.run(self.query)

        # print a header
        print "family_id\tfamily_members\tfamily_genotypes\tfamily_genotype_depths\t",
        print self.gq.header

        for row in self.gq:

            # test the variant for each family in the db
            for idx, fam_id in enumerate(self.family_ids):
                family_genotype_mask = self.family_masks[idx]
                family_gt_labels = self.family_gt_labels[idx]
                family_gt_cols = self.family_gt_columns[idx]
                family_dp_cols = self.family_dp_columns[idx]

                # interrogate the genotypes present in each family member to 
                # conforming to the genetic model being tested
                gt_types = row['gt_types']
                gts = row['gts']
                gt_depths = row['gt_depths']

                # skip if the variant doesn't meet a recessive model
                # for this family
                if not eval(family_genotype_mask):
                    continue

                # make sure each sample's genotype had sufficient coverage.
                # otherwise, ignore
                insufficient_depth = False
                for col in family_dp_cols:
                    depth = int(eval(col))
                    if depth < self.args.min_sample_depth:
                        insufficient_depth = True
                        break
                if insufficient_depth is True:
                    continue

                print str(fam_id) + "\t" + \
                    ",".join([str(s) for s in family_gt_labels]) + \
                    "\t", \
                    ",".join([str(eval(s)) for s in family_gt_cols]) + \
                    "\t", \
                    ",".join([str(eval(s)) for s in family_dp_cols]) + \
                    "\t",
                print row
########NEW FILE########
__FILENAME__ = gemini_load
#!/usr/bin/env python

# native Python imports
import os.path
import sys
import sqlite3

import annotations
import subprocess
from cluster_helper.cluster import cluster_view
import database as gemini_db
from gemini_load_chunk import GeminiLoader
import uuid
import time
import datetime


def load(parser, args):
    if (args.db is None or args.vcf is None):
        parser.print_help()
        exit("ERROR: load needs both a VCF file and a database file\n")
    if args.skip_cadd is False:
        sys.stdout.write("CADD is being loaded (to skip use:--skip-cadd).\n")
    if args.skip_gerp_bp is False:
        sys.stdout.write("GERP per bp is being loaded (to skip use:--skip-gerp-bp).\n")
    # collect of the the add'l annotation files
    annotations.load_annos()

    if args.scheduler:
        load_ipython(args)
    elif args.cores > 1:
        load_multicore(args)
    else:
        load_singlecore(args)

def load_singlecore(args):
    # create a new gemini loader and populate
    # the gemini db and files from the VCF
    gemini_loader = GeminiLoader(args)
    gemini_loader.store_resources()
    gemini_loader.store_version()
    gemini_loader.populate_from_vcf()

    if not args.skip_gene_tables and not args.test_mode:
        gemini_loader.update_gene_table()
    if not args.test_mode:
        gemini_loader.build_indices_and_disconnect()

    if not args.no_genotypes and not args.no_load_genotypes:
        gemini_loader.store_sample_gt_counts()

def load_multicore(args):
    grabix_file = bgzip(args.vcf)
    chunks = load_chunks_multicore(grabix_file, args)
    merge_chunks_multicore(chunks, args.db)

def load_ipython(args):
    grabix_file = bgzip(args.vcf)
    with cluster_view(*get_ipython_args(args)) as view:
        chunks = load_chunks_ipython(grabix_file, args, view)
        merge_chunks_ipython(chunks, args.db, view)

def merge_chunks(chunks, db):
    cmd = get_merge_chunks_cmd(chunks, db)
    print "Merging chunks."
    subprocess.check_call(cmd, shell=True)
    cleanup_temp_db_files(chunks)
    return db

def get_merge_chunks_cmd(chunks, db):
    chunk_names = ""
    for chunk in chunks:
        chunk_names += " --chunkdb  " + chunk
    return "gemini merge_chunks {chunk_names} --db {db}".format(**locals())

def merge_chunks_ipython(chunks, db, view):
    if len(chunks) == 1:
        os.rename(chunks[0], db)
        return db
    else:
        sub_merges = get_chunks_to_merge(chunks)
        tmp_dbs = get_temp_dbs(len(sub_merges), os.path.dirname(sub_merges[0][0]))
        view.map(merge_chunks, sub_merges, tmp_dbs)
        merge_chunks_ipython(tmp_dbs, db, view)

def merge_chunks_multicore(chunks, db):
    if len(chunks) <= 1:
        ts = time.time()
        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
        print st, "indexing final database."

        os.rename(chunks[0], db)
        main_conn = sqlite3.connect(db)
        main_conn.isolation_level = None
        main_curr = main_conn.cursor()
        main_curr.execute('PRAGMA synchronous = OFF')
        main_curr.execute('PRAGMA journal_mode=MEMORY')
        
        gemini_db.create_indices(main_curr)
        
        main_conn.commit()
        main_curr.close()
        return db
    else:
        ts = time.time()
        st = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')
        print st, "merging", len(chunks), "chunks."
        procs = []
        sub_merges = get_chunks_to_merge(chunks)
        tmp_dbs = get_temp_dbs(len(sub_merges), os.path.dirname(sub_merges[0][0]))
        for sub_merge, tmp_db in zip(sub_merges, tmp_dbs):
            cmd = get_merge_chunks_cmd(sub_merge, tmp_db)
            procs.append(subprocess.Popen(cmd, shell=True))
        wait_until_finished(procs)
        cleanup_temp_db_files(chunks)
        merge_chunks_multicore(tmp_dbs, db)

def get_chunks_to_merge(chunks):
    sublist = list_to_sublists(chunks, 2)
    if len(sublist[-1]) > 1:
        return sublist
    else:
        sublist[-2].extend(sublist[-1])
        return sublist[:-1]

def list_to_sublists(l, n):
    """ convert list l to sublists of length n """
    return [l[i:i+n] for i in xrange(0, len(l), n)]

def get_temp_dbs(n, tmp_dir):
    return [os.path.join(tmp_dir, str(uuid.uuid4())) + ".db" for _ in xrange(n)]

def get_chunk_name(chunk):
    return "--chunkdb " + chunk

def load_chunks_multicore(grabix_file, args):
    cores = args.cores

    # specify the PED file if given one
    ped_file = ""
    if args.ped_file is not None:
        ped_file = "-p " + args.ped_file

    # specify the annotation type if given one
    anno_type = ""
    if args.anno_type is not None:
        anno_type = "-t " + args.anno_type

    no_genotypes = ""
    if args.no_genotypes is True:
        no_genotypes = "--no-genotypes"

    no_load_genotypes = ""
    if args.no_load_genotypes is True:
        no_load_genotypes = "--no-load-genotypes"

    skip_gerp_bp = ""
    if args.skip_gerp_bp is True:
        skip_gerp_bp = "--skip-gerp-bp"

    skip_gene_tables = ""
    if args.skip_gene_tables is True:
        skip_gene_tables = "--skip-gene-tables"

    skip_cadd = ""
    if args.skip_cadd is True:
        skip_cadd = "--skip-cadd"
    
    test_mode = ""
    if args.test_mode is True:
        test_mode = "--test-mode"

    passonly = ""
    if args.passonly is True:
        passonly = "--passonly"

    skip_info_string = ""
    if args.skip_info_string is True:
        skip_info_string = "--skip-info-string"

    submit_command = "{cmd}"
    vcf, _ = os.path.splitext(grabix_file)
    chunk_steps = get_chunk_steps(grabix_file, args)
    chunk_vcfs = []
    chunk_dbs = []
    procs = []

    for chunk_num, chunk in chunk_steps:
        start, stop = chunk
        print "Loading chunk " + str(chunk_num) + "."
        gemini_load = gemini_pipe_load_cmd().format(**locals())
        procs.append(subprocess.Popen(submit_command.format(cmd=gemini_load),
                                      shell=True))

        chunk_vcf = vcf + ".chunk" + str(chunk_num)
        chunk_vcfs.append(chunk_vcf)
        chunk_dbs.append(chunk_vcf + ".db")

    wait_until_finished(procs)
    print "Done loading {0} variants in {1} chunks.".format(stop, chunk_num+1)
    return chunk_dbs

def load_chunks_ipython(grabix_file, args, view):
    # specify the PED file if given one
    ped_file = ""
    if args.ped_file is not None:
        ped_file = "-p " + args.ped_file

    # specify the annotation type if given one
    anno_type = ""
    if args.anno_type is not None:
        anno_type = "-t " + args.anno_type

    no_genotypes = ""
    if args.no_genotypes is True:
        no_genotypes = "--no-genotypes"

    no_load_genotypes = ""
    if args.no_load_genotypes is True:
        no_load_genotypes = "--no-load-genotypes"

    skip_gerp_bp = ""
    if args.skip_gerp_bp is True:
        skip_gerp_bp = "--skip-gerp-bp"

    skip_gene_tables = ""
    if args.skip_gene_tables is True:
        skip_gene_tables = "--skip-gene-tables"
    
    skip_cadd = ""
    if args.skip_cadd is True:
        skip_cadd = "--skip-cadd"
    
    test_mode = ""
    if args.test_mode is True:
        test_mode = "--test-mode"

    passonly = ""
    if args.passonly is True:
        passonly = "--passonly"

    skip_info_string = ""
    if args.skip_info_string is True:
        skip_info_string = "--skip-info-string"


    vcf, _ = os.path.splitext(grabix_file)
    chunk_steps = get_chunk_steps(grabix_file, args)
    total_chunks = len(chunk_steps)
    scheduler, queue, cores = get_ipython_args(args)
    load_args = {"ped_file": ped_file,
                 "anno_type": anno_type,
                 "vcf": vcf,
                 "grabix_file": grabix_file,
                 "no_genotypes": no_genotypes,
                 "no_load_genotypes": no_load_genotypes,
                 "skip_gerp_bp": skip_gerp_bp,
                 "skip_gene_tables": skip_gene_tables,
                 "skip_cadd": skip_cadd,
                 "test_mode": test_mode,
                 "passonly": passonly,
                 "skip_info_string": skip_info_string}
    chunk_dbs = view.map(load_chunk, chunk_steps, [load_args] * total_chunks)

    print "Done loading variants in {0} chunks.".format(total_chunks)
    return chunk_dbs

def load_chunk(chunk_step, kwargs):
    chunk_num, chunk = chunk_step
    start, stop = chunk
    args = combine_dicts(locals(), kwargs)
    gemini_load = gemini_pipe_load_cmd().format(**args)
    subprocess.check_call(gemini_load, shell=True)
    chunk_db = args["vcf"] + ".chunk" + str(chunk_num) + ".db"
    return chunk_db

def wait_until_finished(procs):
    [p.wait() for p in procs]

def cleanup_temp_db_files(chunk_dbs):
    for chunk_db in chunk_dbs:
        os.remove(chunk_db)

def gemini_pipe_load_cmd():
    grabix_cmd = "grabix grab {grabix_file} {start} {stop}"
    gemini_load_cmd = ("gemini load_chunk -v - {anno_type} {ped_file}"
                       " {no_genotypes} {no_load_genotypes} {no_genotypes}"
                       " {skip_gerp_bp} {skip_gene_tables} {skip_cadd}"
                       " {passonly} {skip_info_string} {test_mode}"
                       " -o {start} {vcf}.chunk{chunk_num}.db")
    return " | ".join([grabix_cmd, gemini_load_cmd])

def get_chunk_steps(grabix_file, args):
    index_file = grabix_index(grabix_file)
    num_lines = get_num_lines(index_file)
    chunk_size = int(num_lines) / int(args.cores)
    print "Breaking {0} into {1} chunks.".format(grabix_file, args.cores)

    starts = []
    stops = []
    for chunk in range(0, int(args.cores)):
        start = (chunk * chunk_size) + 1
        stop  = start + chunk_size - 1
        # make sure the last chunk covers the remaining lines
        if chunk >= (args.cores - 1) and stop < num_lines:
            stop = num_lines
        starts.append(start)
        stops.append(stop)
    return list(enumerate(zip(starts, stops)))

def get_num_lines(index_file):
    with open(index_file) as index_handle:
        index_handle.next()
        num_lines = int(index_handle.next().strip())
    print "Loading %d variants." % (num_lines)
    return num_lines

def grabix_index(fname):
    if not which("grabix"):
        print_cmd_not_found_and_exit("grabix")
    index_file = fname + ".gbi"
    if file_exists(index_file):
        return index_file
    print "Indexing {0} with grabix.".format(fname)
    subprocess.check_call("grabix index {fname}".format(fname=fname), shell=True)
    return index_file

def bgzip(fname):

    if not which("bgzip"):
        print_cmd_not_found_and_exit("bgzip")

    if is_gz_file(fname):
        return fname

    vcf_time = os.path.getmtime(fname)
    bgzip_file = fname + ".gz"

    if not file_exists(bgzip_file) or \
       (file_exists(bgzip_file) and os.path.getmtime(bgzip_file) < vcf_time):
        print "Bgzipping {0} into {1}.".format(fname, fname + ".gz")
        subprocess.check_call("bgzip -c {fname} > \
                              {fname}.gz".format(fname=fname),
                              shell=True)
    elif file_exists(bgzip_file) and os.path.getmtime(bgzip_file) > vcf_time:
        print "Loading with existing bgzip ({0}) version of {1}.".format(fname + ".gz", fname)

    return bgzip_file


def is_gz_file(fname):
    _, ext = os.path.splitext(fname)
    if ext == ".gz":
        return True
    else:
        return False

def get_submit_command(args):
    return "{cmd}"


def file_exists(fname):
    """Check if a file exists and is non-empty.
    """
    return os.path.exists(fname) and os.path.getsize(fname) > 0

def which(program):
    """ returns the path to an executable or None if it can't be found
     http://stackoverflow.com/questions/377017/test-if-executable-exists-in-python
     """

    def is_exe(fpath):
        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)

    fpath, fname = os.path.split(program)
    if fpath:
        if is_exe(program):
            return program
    else:
        for path in os.environ["PATH"].split(os.pathsep):
            exe_file = os.path.join(path, program)
            if is_exe(exe_file):
                return exe_file

    return None

def combine_dicts(d1, d2):
    return dict(d1.items() + d2.items())

def get_ipython_args(args):
    return (args.scheduler, args.queue, args.cores)

def print_cmd_not_found_and_exit(cmd):
    sys.stderr.write("Cannot find {cmd}, install it or put it in your "
                     "path.".format(cmd=cmd))
    exit(1)

def use_scheduler(args):
    return bool(args.scheduler)

########NEW FILE########
__FILENAME__ = gemini_load_chunk
#!/usr/bin/env python

# native Python imports
import os.path
import sys
import sqlite3
import numpy as np
import datetime
from itertools import repeat

# third-party imports
import cyvcf as vcf

# gemini modules
import version
from ped import get_ped_fields, default_ped_fields, load_ped_file
import gene_table
import infotag
import database
import annotations
import func_impact
import severe_impact
import popgen
from gemini_constants import *
from compression import pack_blob
from gemini.config import read_gemini_config


class GeminiLoader(object):
    """
    Object for creating and populating a gemini
    database and auxillary data files.
    """
    def __init__(self, args, buffer_size=10000):
        self.args = args

        # create the gemini database
        self._create_db()
        # create a reader for the VCF file
        self.vcf_reader = self._get_vcf_reader()
        # load sample information

        if not self.args.no_genotypes and not self.args.no_load_genotypes:
            # load the sample info from the VCF file.
            self._prepare_samples()
            # initialize genotype counts for each sample
            self._init_sample_gt_counts()
            self.num_samples = len(self.samples)
        else:
            self.num_samples = 0

        self.buffer_size = buffer_size
        self._get_anno_version()
        self._get_gene_detailed()
        self._get_gene_summary()

        if self.args.anno_type == "VEP":
            if not self._is_proper_vep_input():
                error = "\nERROR: Check gemini docs for the recommended VCF annotation with VEP"\
                        "\nhttp://gemini.readthedocs.org/en/latest/content/functional_annotation.html#stepwise-installation-and-usage-of-vep"
                sys.exit(error)

    def store_resources(self):
        """Create table of annotation resources used in this gemini database.
        """
        database.insert_resources(self.c, annotations.get_resources())

    def store_version(self):
        """Create table documenting which gemini version was used for this db.
        """
        database.insert_version(self.c, version.__version__)

    def _get_vid(self):
        if hasattr(self.args, 'offset'):
            v_id = int(self.args.offset)
        else:
            v_id = 1
        return v_id

    def populate_from_vcf(self):
        """
        """
        self.v_id = self._get_vid()
        self.counter = 0
        self.var_buffer = []
        self.var_impacts_buffer = []
        buffer_count = 0
        self.skipped = 0

        # process and load each variant in the VCF file
        for var in self.vcf_reader:
            if self.args.passonly and (var.FILTER is not None and var.FILTER != "."):
                self.skipped += 1
                continue
            (variant, variant_impacts) = self._prepare_variation(var)
            # add the core variant info to the variant buffer
            self.var_buffer.append(variant)
            # add each of the impact for this variant (1 per gene/transcript)
            for var_impact in variant_impacts:
                self.var_impacts_buffer.append(var_impact)

            buffer_count += 1
            # buffer full - time to insert into DB
            if buffer_count >= self.buffer_size:
                sys.stderr.write("pid " + str(os.getpid()) + ": " +
                                 str(self.counter) + " variants processed.\n")
                database.insert_variation(self.c, self.var_buffer)
                database.insert_variation_impacts(self.c,
                                                  self.var_impacts_buffer)
                # binary.genotypes.append(var_buffer)
                # reset for the next batch
                self.var_buffer = []
                self.var_impacts_buffer = []
                buffer_count = 0
            self.v_id += 1
            self.counter += 1
        # final load to the database
        self.v_id -= 1
        database.insert_variation(self.c, self.var_buffer)
        database.insert_variation_impacts(self.c, self.var_impacts_buffer)
        sys.stderr.write("pid " + str(os.getpid()) + ": " +
                         str(self.counter) + " variants processed.\n")
        if self.args.passonly:
            sys.stderr.write("pid " + str(os.getpid()) + ": " +
                             str(self.skipped) + " skipped due to having the "
                             "FILTER field set.\n")


    def build_indices_and_disconnect(self):
        """
        Create the db table indices and close up
        db connection
        """
        # index our tables for speed
        database.create_indices(self.c)
        # commit data and close up
        database.close_and_commit(self.c, self.conn)

    def _get_vcf_reader(self):
        # the VCF is a proper file
        if self.args.vcf != "-":
            if self.args.vcf.endswith(".gz"):
                return vcf.VCFReader(open(self.args.vcf), 'rb', compressed=True)
            else:
                return vcf.VCFReader(open(self.args.vcf), 'rb')
        # the VCF is being passed in via STDIN
        else:
            return vcf.VCFReader(sys.stdin, 'rb')

    def _get_anno_version(self):
        """
        Extract the snpEff or VEP version used
        to annotate the VCF
        """
        # default to unknown version
        self.args.version = None

        if self.args.anno_type == "snpEff":
            try:
                version_string = self.vcf_reader.metadata['SnpEffVersion']
            except KeyError:
                error = ("\nWARNING: VCF is not annotated with snpEff, check documentation at:\n"\
                "http://gemini.readthedocs.org/en/latest/content/functional_annotation.html#stepwise-installation-and-usage-of-snpeff\n")
                sys.exit(error)

            # e.g., "SnpEff 3.0a (build 2012-07-08), by Pablo Cingolani"
            # or "3.3c (build XXXX), by Pablo Cingolani"

            version_string = version_string.replace('"', '')  # No quotes

            toks = version_string.split()

            if "SnpEff" in toks[0]:
                self.args.raw_version = toks[1]  # SnpEff *version*, etc
            else:
                self.args.raw_version = toks[0]  # *version*, etc
            # e.g., 3.0a -> 3
            self.args.maj_version = int(self.args.raw_version.split('.')[0])

        elif self.args.anno_type == "VEP":
            pass

    def _is_proper_vep_input(self):
        """
        Test whether the VCF header meets expectations for
        proper execution of VEP for use with Gemini.
        """
        #support versions 73-75
        format = "Consequence|Codons|Amino_acids|Gene|SYMBOL|Feature|EXON|PolyPhen|SIFT|Protein_position|BIOTYPE".upper()

        if 'CSQ' in self.vcf_reader.infos and \
            format in str(self.vcf_reader.infos['CSQ']).upper():
            return True
        return False

    def _create_db(self):
        """
        private method to open a new DB
        and create the gemini schema.
        """
        # open up a new database
        if os.path.exists(self.args.db):
            os.remove(self.args.db)
        self.conn = sqlite3.connect(self.args.db)
        self.conn.isolation_level = None
        self.c = self.conn.cursor()
        self.c.execute('PRAGMA synchronous = OFF')
        self.c.execute('PRAGMA journal_mode=MEMORY')
        # create the gemini database tables for the new DB
        database.create_tables(self.c)
        database.create_sample_table(self.c, self.args)

    def _prepare_variation(self, var):
        """
        private method to collect metrics for
        a single variant (var) in a VCF file.
        """
        # these metric require that genotypes are present in the file
        call_rate = None
        hwe_p_value = None
        pi_hat = None
        inbreeding_coeff = None
        hom_ref = het = hom_alt = unknown = None

        # only compute certain metrics if genoypes are available
        if not self.args.no_genotypes and not self.args.no_load_genotypes:
            hom_ref = var.num_hom_ref
            hom_alt = var.num_hom_alt
            het = var.num_het
            unknown = var.num_unknown
            call_rate = var.call_rate
            aaf = var.aaf
            hwe_p_value, inbreeding_coeff = \
                popgen.get_hwe_likelihood(hom_ref, het, hom_alt, aaf)
            pi_hat = var.nucl_diversity
        else:
            aaf = infotag.extract_aaf(var)

        ############################################################
        # collect annotations from gemini's custom annotation files
        ############################################################
        pfam_domain = annotations.get_pfamA_domains(var)
        cyto_band = annotations.get_cyto_info(var)
        rs_ids = annotations.get_dbsnp_info(var)
        clinvar_info = annotations.get_clinvar_info(var)
        in_dbsnp = 0 if rs_ids is None else 1
        rmsk_hits = annotations.get_rmsk_info(var)
        in_cpg = annotations.get_cpg_island_info(var)
        in_segdup = annotations.get_segdup_info(var)
        is_conserved = annotations.get_conservation_info(var)
        esp = annotations.get_esp_info(var)
        thousandG = annotations.get_1000G_info(var)
        recomb_rate = annotations.get_recomb_info(var)
        gms = annotations.get_gms(var)
        grc = annotations.get_grc(var)
        in_cse = annotations.get_cse(var)
        encode_tfbs = annotations.get_encode_tfbs(var)
        encode_dnaseI = annotations.get_encode_dnase_clusters(var)
        encode_cons_seg = annotations.get_encode_consensus_segs(var)
        gerp_el = annotations.get_gerp_elements(var)
        vista_enhancers = annotations.get_vista_enhancers(var)
        cosmic_ids = annotations.get_cosmic_info(var)
        
        #load CADD scores by default
        if self.args.skip_cadd is False:
            (cadd_raw, cadd_scaled) = annotations.get_cadd_scores(var)
        else:
            (cadd_raw, cadd_scaled) =  (None, None)
        
        # load the GERP score for this variant by default.
        gerp_bp = None
        if self.args.skip_gerp_bp is False:
            gerp_bp = annotations.get_gerp_bp(var)

        # impact is a list of impacts for this variant
        impacts = None
        severe_impacts = None
        # impact terms initialized to None for handling unannotated vcf's
        # anno_id in variants is for the trans. with the most severe impact term
        gene = transcript = exon = codon_change = aa_change = aa_length = \
            biotype = consequence = consequence_so = effect_severity = None
        is_coding = is_exonic = is_lof = None
        polyphen_pred = polyphen_score = sift_pred = sift_score = anno_id = None

        if self.args.anno_type is not None:
            impacts = func_impact.interpret_impact(self.args, var)
            severe_impacts = \
                severe_impact.interpret_severe_impact(self.args, var)
            if severe_impacts:
                gene = severe_impacts.gene
                transcript = severe_impacts.transcript
                exon = severe_impacts.exon
                codon_change = severe_impacts.codon_change
                aa_change = severe_impacts.aa_change
                aa_length = severe_impacts.aa_length
                biotype = severe_impacts.biotype
                consequence = severe_impacts.consequence
                effect_severity = severe_impacts.effect_severity
                polyphen_pred = severe_impacts.polyphen_pred
                polyphen_score = severe_impacts.polyphen_score
                sift_pred = severe_impacts.sift_pred
                sift_score = severe_impacts.sift_score
                anno_id = severe_impacts.anno_id
                is_exonic = severe_impacts.is_exonic
                is_coding = severe_impacts.is_coding
                is_lof = severe_impacts.is_lof
                consequence_so = severe_impacts.so

        # construct the filter string
        filter = None
        if var.FILTER is not None and var.FILTER != ".":
            if isinstance(var.FILTER, list):
                filter = ";".join(var.FILTER)
            else:
                filter = var.FILTER

        vcf_id = None
        if var.ID is not None and var.ID != ".":
            vcf_id = var.ID

        # build up numpy arrays for the genotype information.
        # these arrays will be pickled-to-binary, compressed,
        # and loaded as SqlLite BLOB values (see compression.pack_blob)
        if not self.args.no_genotypes and not self.args.no_load_genotypes:
            gt_bases = np.array(var.gt_bases, np.str)  # 'A/G', './.'
            gt_types = np.array(var.gt_types, np.int8)  # -1, 0, 1, 2
            gt_phases = np.array(var.gt_phases, np.bool)  # T F F
            gt_depths = np.array(var.gt_depths, np.int32)  # 10 37 0
            gt_ref_depths = np.array(var.gt_ref_depths, np.int32)  # 2 21 0 -1
            gt_alt_depths = np.array(var.gt_alt_depths, np.int32)  # 8 16 0 -1
            gt_quals = np.array(var.gt_quals, np.float32)  # 10.78 22 99 -1

            # tally the genotypes
            self._update_sample_gt_counts(gt_types)
        else:
            gt_bases = None
            gt_types = None
            gt_phases = None
            gt_depths = None
            gt_ref_depths = None
            gt_alt_depths = None
            gt_quals = None
        
        if self.args.skip_info_string is False:
            info = var.INFO
        else:
            info = None

        # were functional impacts predicted by SnpEFF or VEP?
        # if so, build up a row for each of the impacts / transcript
        variant_impacts = []
        if impacts is not None:
            for idx, impact in enumerate(impacts):
                var_impact = [self.v_id, (idx + 1), impact.gene,
                              impact.transcript, impact.is_exonic,
                              impact.is_coding, impact.is_lof,
                              impact.exon, impact.codon_change,
                              impact.aa_change, impact.aa_length,
                              impact.biotype, impact.consequence,
                              impact.so, impact.effect_severity,
                              impact.polyphen_pred, impact.polyphen_score, 
                              impact.sift_pred, impact.sift_score]
                variant_impacts.append(var_impact)

        # construct the core variant record.
        # 1 row per variant to VARIANTS table
        chrom = var.CHROM if var.CHROM.startswith("chr") else "chr" + var.CHROM
        variant = [chrom, var.start, var.end,
                   vcf_id, self.v_id, anno_id, var.REF, ','.join(var.ALT),
                   var.QUAL, filter, var.var_type,
                   var.var_subtype, pack_blob(gt_bases), pack_blob(gt_types),
                   pack_blob(gt_phases), pack_blob(gt_depths),
                   pack_blob(gt_ref_depths), pack_blob(gt_alt_depths),
                   pack_blob(gt_quals),
                   call_rate, in_dbsnp,
                   rs_ids,
                   clinvar_info.clinvar_in_omim,
                   clinvar_info.clinvar_sig,
                   clinvar_info.clinvar_disease_name,
                   clinvar_info.clinvar_dbsource,
                   clinvar_info.clinvar_dbsource_id,
                   clinvar_info.clinvar_origin,
                   clinvar_info.clinvar_dsdb,
                   clinvar_info.clinvar_dsdbid,
                   clinvar_info.clinvar_disease_acc,
                   clinvar_info.clinvar_in_locus_spec_db,
                   clinvar_info.clinvar_on_diag_assay,
                   pfam_domain, cyto_band, rmsk_hits, in_cpg,
                   in_segdup, is_conserved, gerp_bp, gerp_el,
                   hom_ref, het, hom_alt, unknown,
                   aaf, hwe_p_value, inbreeding_coeff, pi_hat,
                   recomb_rate, gene, transcript, is_exonic,
                   is_coding, is_lof, exon, codon_change, aa_change,
                   aa_length, biotype, consequence, consequence_so, effect_severity,
                   polyphen_pred, polyphen_score, sift_pred, sift_score,
                   infotag.get_ancestral_allele(var), infotag.get_rms_bq(var),
                   infotag.get_cigar(var),
                   infotag.get_depth(var), infotag.get_strand_bias(var),
                   infotag.get_rms_map_qual(var), infotag.get_homopol_run(var),
                   infotag.get_map_qual_zero(var),
                   infotag.get_num_of_alleles(var),
                   infotag.get_frac_dels(var),
                   infotag.get_haplotype_score(var),
                   infotag.get_quality_by_depth(var),
                   infotag.get_allele_count(var), infotag.get_allele_bal(var),
                   infotag.in_hm2(var), infotag.in_hm3(var),
                   infotag.is_somatic(var),
                   esp.found, esp.aaf_EA,
                   esp.aaf_AA, esp.aaf_ALL, 
                   esp.exome_chip, thousandG.found,
                   thousandG.aaf_AMR, thousandG.aaf_ASN, 
                   thousandG.aaf_AFR, thousandG.aaf_EUR, 
                   thousandG.aaf_ALL, grc,
                   gms.illumina, gms.solid, 
                   gms.iontorrent, in_cse,
                   encode_tfbs,
                   encode_dnaseI.cell_count,
                   encode_dnaseI.cell_list,
                   encode_cons_seg.gm12878,
                   encode_cons_seg.h1hesc,
                   encode_cons_seg.helas3,
                   encode_cons_seg.hepg2,
                   encode_cons_seg.huvec,
                   encode_cons_seg.k562,
                   vista_enhancers,
                   cosmic_ids,
                   pack_blob(info),
                   cadd_raw,
                   cadd_scaled]
        
        return variant, variant_impacts

    def _prepare_samples(self):
        """
        private method to load sample information
        """
        if not self.args.no_genotypes:
            self.samples = self.vcf_reader.samples
            self.sample_to_id = {}
            for idx, sample in enumerate(self.samples):
                self.sample_to_id[sample] = idx + 1

        self.ped_hash = {}
        if self.args.ped_file is not None:
            self.ped_hash = load_ped_file(self.args.ped_file)

        sample_list = []
        for sample in self.samples:
            i = self.sample_to_id[sample]
            if sample in self.ped_hash:
                fields = self.ped_hash[sample]
                sample_list = [i] + fields
            elif len(self.ped_hash) > 0:
                sys.exit("EXITING: sample %s found in the VCF but "
                                 "not in the PED file.\n" % (sample))
            else:
                # if there is no ped file given, just fill in the name and
                # sample_id and set the other required fields to None
                sample_list = [i, None, sample]
                sample_list += list(repeat(None, len(default_ped_fields) - 2))
            database.insert_sample(self.c, sample_list)
            
    def _get_gene_detailed(self):
        """
        define a gene detailed table
        """
        #unique identifier for each entry
        i = 0
        table_contents = detailed_list = []
        
        config = read_gemini_config()
        path_dirname = config["annotation_dir"]
        file_handle = os.path.join(path_dirname, 'detailed_gene_table_v75')
        
        for line in open(file_handle, 'r'):
            field = line.strip().split("\t")
            if not field[0].startswith("Chromosome"):
                i += 1
                table = gene_table.gene_detailed(field)
                detailed_list = [str(i),table.chrom,table.gene,table.is_hgnc,
                                 table.ensembl_gene_id,table.ensembl_trans_id, 
                                 table.biotype,table.trans_status,table.ccds_id, 
                                 table.hgnc_id,table.entrez,table.cds_length,table.protein_length, 
                                 table.transcript_start,table.transcript_end,
                                 table.strand,table.synonym,table.rvis,table.mam_phenotype]
                table_contents.append(detailed_list)
        database.insert_gene_detailed(self.c, table_contents)
        
    def _get_gene_summary(self):
        """
        define a gene summary table
        """
        #unique identifier for each entry
        i = 0
        contents = summary_list = []
        
        config = read_gemini_config()
        path_dirname = config["annotation_dir"]
        file = os.path.join(path_dirname, 'summary_gene_table_v75')
        
        for line in open(file, 'r'):
            col = line.strip().split("\t")
            if not col[0].startswith("Chromosome"):
                i += 1
                table = gene_table.gene_summary(col)
                # defaul cosmic census to False
                cosmic_census = 0
                summary_list = [str(i),table.chrom,table.gene,table.is_hgnc,
                                table.ensembl_gene_id,table.hgnc_id,
                                table.transcript_min_start,
                                table.transcript_max_end,table.strand,
                                table.synonym,table.rvis,table.mam_phenotype,
                                cosmic_census]
                contents.append(summary_list)
        database.insert_gene_summary(self.c, contents)

    def update_gene_table(self):
        """
        """
        gene_table.update_cosmic_census_genes(self.c)

    def _init_sample_gt_counts(self):
        """
        Initialize a 2D array of counts for tabulating
        the count of each genotype type for eaxh sample.

        The first dimension is one bucket for each sample.
        The second dimension (size=4) is a count for each gt type.
           Index 0 == # of hom_ref genotypes for the sample
           Index 1 == # of het genotypes for the sample
           Index 2 == # of missing genotypes for the sample
           Index 3 == # of hom_alt genotypes for the sample
        """
        self.sample_gt_counts = np.array(np.zeros((len(self.samples), 4)),
                                         dtype='uint32')

    def _update_sample_gt_counts(self, gt_types):
        """
        Update the count of each gt type for each sample
        """
        for idx, gt_type in enumerate(gt_types):
            self.sample_gt_counts[idx][gt_type] += 1

    def store_sample_gt_counts(self):
        """
        Update the count of each gt type for each sample
        """
        self.c.execute("BEGIN TRANSACTION")
        for idx, gt_counts in enumerate(self.sample_gt_counts):
            self.c.execute("""insert into sample_genotype_counts values \
                            (?,?,?,?,?)""",
                           [idx,
                            int(gt_counts[HOM_REF]),  # hom_ref
                            int(gt_counts[HET]),  # het
                            int(gt_counts[HOM_ALT]),  # hom_alt
                            int(gt_counts[UNKNOWN])])  # missing
        self.c.execute("END")


def load(parser, args):
    if (args.db is None or args.vcf is None):
        parser.print_help()
        exit("ERROR: load needs both a VCF file and a database file\n")
    if args.anno_type not in ['snpEff', 'VEP', None]:
        parser.print_help()
        exit("\nERROR: Unsupported selection for -t\n")

    # collect of the the add'l annotation files
    annotations.load_annos()

    # create a new gemini loader and populate
    # the gemini db and files from the VCF
    gemini_loader = GeminiLoader(args)
    gemini_loader.store_resources()
    gemini_loader.store_version()

    gemini_loader.populate_from_vcf()
    gemini_loader.update_gene_table()
    # gemini_loader.build_indices_and_disconnect()

    if not args.no_genotypes and not args.no_load_genotypes:
        gemini_loader.store_sample_gt_counts()

########NEW FILE########
__FILENAME__ = gemini_main
#!/usr/bin/env python

import os.path
import sys
import argparse
import gemini_load
import gemini_load_chunk
import gemini_query
import \
    gemini_region, gemini_stats, gemini_dump, \
    gemini_annotate, gemini_windower, \
    gemini_browser, gemini_dbinfo, gemini_merge_chunks, gemini_update, \
    gemini_amend, gemini_set_somatic, gemini_actionable_mutations

import gemini.version

import tool_compound_hets
import tool_autosomal_recessive
import tool_autosomal_dominant
import tool_de_novo_mutations
import tool_pathways
import tool_lof_sieve
import tool_interactions
import tool_burden_tests


def examples(parser, args):

    print
    print "[load] - load a VCF file into a gemini database:"
    print "   gemini load -v my.vcf my.db"
    print "   gemini load -v my.vcf -t snpEff my.db"
    print "   gemini load -v my.vcf -t VEP my.db"
    print

    print "[stats] - report basic statistics about your variants:"
    print "   gemini stats --tstv my.db"
    print "   gemini stats --tstv-coding my.db"
    print "   gemini stats --sfs my.db"
    print "   gemini stats --snp-counts my.db"
    print

    print "[query] - explore the database with ad hoc queries:"
    print "   gemini query -q \"select * from variants where is_lof = 1 and aaf <= 0.01\" my.db"
    print "   gemini query -q \"select chrom, pos, gt_bases.NA12878 from variants\" my.db"
    print "   gemini query -q \"select chrom, pos, in_omim, clin_sigs from variants\" my.db"
    print

    print "[dump] - convenient \"data dumps\":"
    print "   gemini dump --variants my.db"
    print "   gemini dump --genotypes my.db"
    print "   gemini dump --samples my.db"
    print

    print "[region] - access variants in specific genomic regions:"
    print "   gemini region --reg chr1:100-200 my.db"
    print "   gemini region --gene TP53 my.db"
    print

    print "[tools] - there are also many specific tools available"
    print "   1. Find compound heterozygotes."
    print "     gemini comp_hets my.db"
    print

    exit()


def main():

    #########################################
    # create the top-level parser
    #########################################
    parser = argparse.ArgumentParser(prog='gemini', formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("-v", "--version", help="Installed gemini version",
                        action="version",
                        version="%(prog)s " + str(gemini.version.__version__))
    subparsers = parser.add_subparsers(title='[sub-commands]', dest='command')

    #########################################
    # $ gemini examples
    #########################################
    parser_examples = subparsers.add_parser('examples',
                                            help='show usage examples')
    parser_examples.set_defaults(func=examples)

    #########################################
    # $ gemini load
    #########################################
    parser_load = subparsers.add_parser('load',
                                        help='load a VCF file in gemini database')
    parser_load.add_argument('db', metavar='db',
                             help='The name of the database to be created.')
    parser_load.add_argument('-v', dest='vcf',
                             help='The VCF file to be loaded.')
    parser_load.add_argument('-t', dest='anno_type',
                             default=None, choices=["snpEff", "VEP"],
                             help="The annotations to be used with the input vcf.")
    parser_load.add_argument('-p', dest='ped_file',
                             help='Sample information file in PED+ format.',
                             default=None)
    parser_load.add_argument('--skip-gerp-bp',
                             dest='skip_gerp_bp',
                             action='store_true',
                             help='Do not load GERP scores at base pair resolution. Loaded by default.',
                             default=False)
    parser_load.add_argument('--skip-cadd',
                             dest='skip_cadd',
                             action='store_true',
                             help='Do not load CADD scores. Loaded by default',
                             default=False)
    parser_load.add_argument('--skip-gene-tables',
                             dest='skip_gene_tables',
                             action='store_true',
                             help='Do not load gene tables. Loaded by default.',
                             default=False)
    parser_load.add_argument('--skip-info-string',
                             dest='skip_info_string',
                             action='store_true',
                             help='Do not load INFO string from VCF file to reduce DB size. Loaded by default',
                             default=False)
    parser_load.add_argument('--no-load-genotypes',
                             dest='no_load_genotypes',
                             action='store_true',
                             help='Genotypes exist in the file, but should not be stored.',
                             default=False)
    parser_load.add_argument('--no-genotypes',
                             dest='no_genotypes',
                             action='store_true',
                             help='There are no genotypes in the file (e.g. some 1000G VCFs)',
                             default=False)
    parser_load.add_argument('--cores', dest='cores',
                             default=1,
                             type=int,
                             help="Number of cores to use to load in parallel.")
    parser_load.add_argument('--scheduler', dest='scheduler', default=None,
                             choices=["lsf", "sge", "slurm", "torque"],
                             action=IPythonAction,
                             help='Cluster scheduler to use.')
    parser_load.add_argument('--queue', dest='queue',
                             default=None, help='Cluster queue to use.')
    parser_load.add_argument('--passonly',
                             dest='passonly',
                             default=False,
                             action='store_true',
                             help="Keep only variants that pass all filters.")
    parser_load.add_argument('--test-mode',
                         dest='test_mode',
                         action='store_true',
                         help='Load in test mode (faster)',
                         default=False)

    parser_load.set_defaults(func=gemini_load.load)
    #########################################
    # $ gemini amend
    #########################################
    parser_amend = subparsers.add_parser('amend',
                                         help="Amend an already loaded GEMINI database.")
    parser_amend.add_argument('db',
                              metavar='db',
                              help='The name of the database to be amended.')
    parser_amend.add_argument('--sample',
                              metavar='sample',
                              default=None,
                              help='New sample information file to load')
    parser_amend.set_defaults(func=gemini_amend.amend)

    #########################################
    # $ gemini load_chunk
    #########################################
    parser_loadchunk = subparsers.add_parser('load_chunk',
                                             help='load a VCF file in gemini database')
    parser_loadchunk.add_argument('db',
                                  metavar='db',
                                  help='The name of the database to be created.')
    parser_loadchunk.add_argument('-v',
                                  dest='vcf',
                                  help='The VCF file to be loaded.')
    parser_loadchunk.add_argument('-t',
                                  dest='anno_type',
                                  default=None,
                                  metavar='STRING',
                                  help="The annotations to be used with the input vcf. Options are:\n"
                                  "  snpEff  - Annotations as reported by snpEff.\n"
                                  "  VEP     - Annotations as reported by VEP.\n"
                                  )
    parser_loadchunk.add_argument('-o',
                                  dest='offset',
                                  help='The starting number for the variant_ids',
                                  default=None)
    parser_loadchunk.add_argument('-p',
                                  dest='ped_file',
                                  help='Sample information file in PED+ format.',
                                  default=None)
    parser_loadchunk.add_argument('--no-load-genotypes',
                                  dest='no_load_genotypes',
                                  action='store_true',
                                  help='Genotypes exist in the file, but should not be stored.',
                                  default=False)
    parser_loadchunk.add_argument('--no-genotypes',
                                  dest='no_genotypes',
                                  action='store_true',
                                  help='There are no genotypes in the file (e.g. some 1000G VCFs)',
                                  default=False)
    parser_loadchunk.add_argument('--skip-gerp-bp',
                                  dest='skip_gerp_bp',
                                  action='store_true',
                                  help='Do not load GERP scores at base pair resolution. Loaded by default.',
                                  default=False)
    parser_loadchunk.add_argument('--skip-cadd',
                                 dest='skip_cadd',
                                 action='store_true',
                                 help='Do not load CADD scores. Loaded by default',
                                 default=False)
    parser_loadchunk.add_argument('--skip-gene-tables',
                             dest='skip_gene_tables',
                             action='store_true',
                             help='Do not load gene tables. Loaded by default.',
                             default=False)
    parser_loadchunk.add_argument('--skip-info-string',
                                  dest='skip_info_string',
                                  action='store_true',
                                  help='Do not load INFO string from VCF file to reduce DB size. Loaded by default',
                                  default=False)
    parser_loadchunk.add_argument('--passonly',
                                  dest='passonly',
                                  default=False,
                                  action='store_true',
                                  help="Keep only variants that pass all filters.")
    parser_loadchunk.add_argument('--test-mode',
                         dest='test_mode',
                         action='store_true',
                         help='Load in test mode (faster)',
                         default=False)
    parser_loadchunk.set_defaults(func=gemini_load_chunk.load)

    #########################################
    # $ gemini merge_chunks
    #########################################
    parser_mergechunks = subparsers.add_parser('merge_chunks',
            help='combine intermediate db files into the final gemini ')
    parser_mergechunks.add_argument('--db',
            dest='db',
            help='The name of the final database to be loaded.')
    parser_mergechunks.add_argument('--chunkdb',
            nargs='*',
            dest='chunkdbs',
            action='append')

    parser_mergechunks.set_defaults(func=gemini_merge_chunks.merge_chunks)

    #########################################
    # $ gemini query
    #########################################
    parser_query = subparsers.add_parser('query',
            help='issue ad hoc SQL queries to the DB')
    parser_query.add_argument('db',
            metavar='db',
            help='The name of the database to be queried.')
    parser_query.add_argument('-q',
            dest='query',
            metavar='QUERY_STR',
            help='The query to be issued to the database')
    parser_query.add_argument('--gt-filter',
            dest='gt_filter',
            metavar='STRING',
            help='Restrictions to apply to genotype values')
    parser_query.add_argument('--show-samples',
                              dest='show_variant_samples',
                              action='store_true',
                              default=False,
                              help=('Add a column of all sample names with a variant to each '
                                    'variant.'))
    parser_query.add_argument('--show-families',
                              dest='show_families',
                              action='store_true',
                              default=False,
                              help=('Add a column listing all of the families '
                                    'with a variant to each variant.'))
    parser_query.add_argument('--family-wise',
                              dest='family_wise',
                              default=False,
                              action='store_true',
                              help=('Perform the sample-filter on a family-wise '
                                    'basis.'))
    parser_query.add_argument('--min-kindreds',
                              dest='min_kindreds',
                              default=1,
                              type=int,
                              help=('Minimum number of families for a variant passing '
                                    'a family-wise filter to be in.'))
    parser_query.add_argument('--sample-delim',
            dest='sample_delim',
            metavar='STRING',
            help='The delimiter to be used with the --show-samples option.',
            default=',')

    parser_query.add_argument('--header',
            dest='use_header',
            action='store_true',
            help='Add a header of column names to the output.',
            default=False)
    parser_query.add_argument('--sample-filter',
                              dest='sample_filter',
                              help='SQL filter to use to filter the sample table',
                              default=None)
    parser_query.add_argument('--in',
                              dest='in_subject',
                              nargs='*',
                              help=('A variant must be in either all, none or any '
                                    'samples passing the --sample-query filter.'),
                              choices=['all', 'none', 'any', 'only'],
                              default=['any'])
    parser_query.add_argument('--format',
                              dest='format',
                              default='default',
                              help='Format of output (JSON, TPED or default)')
    parser_query.add_argument('--region',
                              dest='region',
                              default=None,
                              help=('Restrict query to this region, '
                                    'e.g. chr1:10-20.'))
    parser_query.add_argument('--carrier-summary-by-phenotype',
                              dest='carrier_summary',
                              default=None,
                              help=('Output columns of counts of carriers and '
                                    'non-carriers stratified by the given '
                                    'sample phenotype column'))
    parser_query.add_argument('--dgidb',
                              dest='dgidb',
                              action='store_true',
                              help='Request drug-gene interaction info from DGIdb.',
                              default=False)

    parser_query.set_defaults(func=gemini_query.query)

    #########################################
    # $ gemini dump
    #########################################
    parser_dump = subparsers.add_parser('dump',
            help='shortcuts for extracting data from the DB')
    parser_dump.add_argument('db',
            metavar='db',
            help='The name of the database to be queried.')
    parser_dump.add_argument('--variants',
            dest='variants',
            action='store_true',
            help='Report all rows/columns from the variants table.',
            default=False)
    parser_dump.add_argument('--genotypes',
            dest='genotypes',
            action='store_true',
            help='Report all rows/columns from the variants table \nwith one line per sample/genotype.',
            default=False)
    parser_dump.add_argument('--samples',
            dest='samples',
            action='store_true',
            help='Report all rows/columns from the samples table.',
            default=False)
    parser_dump.add_argument('--header',
            dest='use_header',
            action='store_true',
            help='Add a header of column names to the output.',
            default=False)
    parser_dump.add_argument('--sep',
            dest='separator',
            metavar='STRING',
            help='Output column separator',
            default="\t")
    parser_dump.add_argument('--tfam',
                             dest='tfam',
                             action='store_true',
                             default=False,
                             help='Output sample information to TFAM format.')
    parser_dump.set_defaults(func=gemini_dump.dump)

    #########################################
    # $ gemini region
    #########################################
    parser_region = subparsers.add_parser('region',
            help='extract variants from specific genomic loci')
    parser_region.add_argument('db',
            metavar='db',
            help='The name of the database to be queried.')
    parser_region.add_argument('--reg',
            dest='region',
            metavar='STRING',
            help='Specify a chromosomal region chr:start-end')
    parser_region.add_argument('--gene',
            dest='gene',
            metavar='STRING',
            help='Specify a gene of interest')
    parser_region.add_argument('--header',
            dest='use_header',
            action='store_true',
            help='Add a header of column names to the output.',
            default=False)
    parser_region.add_argument('--columns',
            dest='columns',
            metavar='STRING',
            help='A list of columns that you would like returned. Def. = "*"',
            )
    parser_region.add_argument('--filter',
            dest='filter',
            metavar='STRING',
            help='Restrictions to apply to variants (SQL syntax)')
    parser_region.add_argument('--show-samples',
                               dest='show_variant_samples',
                               action='store_true',
                               default=False,
                                help=('Add a column of all sample names with a variant to each '
                                      'variant.'))
    parser_region.add_argument('--format',
                              dest='format',
                              default='default',
                              help='Format of output (JSON, TPED or default)')
    parser_region.set_defaults(func=gemini_region.region)

    #########################################
    # $ gemini stats
    #########################################
    parser_stats = subparsers.add_parser('stats',
            help='compute useful variant stastics')
    parser_stats.add_argument('db',
            metavar='db',
            help='The name of the database to be queried.')
    parser_stats.add_argument('--tstv',
            dest='tstv',
            action='store_true',
            help='Report the overall ts/tv ratio.',
            default=False)
    parser_stats.add_argument('--tstv-coding',
            dest='tstv_coding',
            action='store_true',
            help='Report the ts/tv ratio in coding regions.',
            default=False)
    parser_stats.add_argument('--tstv-noncoding',
            dest='tstv_noncoding',
            action='store_true',
            help='Report the ts/tv ratio in non-coding regions.',
            default=False)
    parser_stats.add_argument('--snp-counts',
            dest='snp_counts',
            action='store_true',
            help='Report the count of each type of SNP (A->G, G->T, etc.).',
            default=False)
    parser_stats.add_argument('--sfs',
            dest='sfs',
            action='store_true',
            help='Report the site frequency spectrum of the variants.',
            default=False)
    parser_stats.add_argument('--mds',
            dest='mds',
            action='store_true',
            help='Report the pairwise genetic distance between the samples.',
            default=False)
    parser_stats.add_argument('--vars-by-sample',
            dest='variants_by_sample',
            action='store_true',
            help='Report the number of variants observed in each sample.',
            default=False)
    parser_stats.add_argument('--gts-by-sample',
            dest='genotypes_by_sample',
            action='store_true',
            help='Report the count of each genotype class obs. per sample.',
            default=False)
    parser_stats.add_argument('--summarize',
            dest='query',
            metavar='QUERY_STR',
            default=None,
            help='The query to be issued to the database to summarize')
    parser_stats.add_argument('--gt-filter',
            dest='gt_filter',
            metavar='STRING',
            help='Restrictions to apply to genotype values')
    parser_stats.set_defaults(func=gemini_stats.stats)

    #########################################
    # gemini annotate
    #########################################
    parser_get = subparsers.add_parser('annotate',
            help='Add new columns for custom annotations')
    parser_get.add_argument('db',
            metavar='db',
            help='The name of the database to be updated.')
    parser_get.add_argument('-f',
            dest='anno_file',
            help='The TABIX\'ed BED file containing the annotations')
    parser_get.add_argument('-c',
            dest='col_names',
            help='The name(s) of the column(s) to be added to the variant table.')
    parser_get.add_argument('-a',
            dest='anno_type',
            help='How should the annotation file be used? (def. extract)',
            default="extract",
            choices=['boolean', 'count', 'extract'])
    parser_get.add_argument('-e',
            dest='col_extracts',
            help='Column(s) to extract information from for list annotations.')
    parser_get.add_argument('-t',
            dest='col_types',
            help='What data type(s) should be used to represent the new values '
                 'in the database? '
                 'Any of {integer, float, text}')
    parser_get.add_argument('-o',
            dest='col_operations',
            help='Operation(s) to apply to the extract column values '
                  'in the event that a variant overlaps multiple annotations '
                  'in your annotation file (-f).'
                  'Any of {mean, median, min, max, mode, list, uniq_list, first, last}')
    parser_get.set_defaults(func=gemini_annotate.annotate)

    #########################################
    # gemini windower
    #########################################
    parser_get = subparsers.add_parser('windower',
            help='Compute statistics across genome \"windows\"')
    parser_get.add_argument('db',
            metavar='db',
            help='The name of the database to be updated.')
    parser_get.add_argument('-w',
            dest='window_size',
            default=1000000,
            help='The name of the column to be added to the variant table.')
    parser_get.add_argument('-s',
            dest='step_size',
            default=0,
            help="The step size for the windows in bp.\n")
    parser_get.add_argument('-t',
            dest='analysis_type',
            help='The type of windowed analysis requested.',
            choices=['nucl_div', 'hwe'],
            default='hwe')
    parser_get.add_argument('-o',
            dest='op_type',
            help='The operation that should be applied to the -t values.',
            choices=['mean', 'median', 'min', 'max', 'collapse'],
            default='mean')
    parser_get.set_defaults(func=gemini_windower.windower)

    #########################################
    # gemini db_info
    #########################################
    parser_get = subparsers.add_parser('db_info',
            help='Get the names and types of cols. database tables')
    parser_get.add_argument('db',
            metavar='db',
            help='The name of the database to be updated.')
    parser_get.set_defaults(func=gemini_dbinfo.db_info)

    #########################################
    # $ gemini comp_hets
    #########################################
    parser_comp_hets = subparsers.add_parser('comp_hets',
            help='Identify compound heterozygotes')
    parser_comp_hets.add_argument('db',
            metavar='db',
            help='The name of the database to be created.')
    parser_comp_hets.add_argument('--columns',
            dest='columns',
            metavar='STRING',
            help='A list of columns that you would like returned. Def. = "*"',
            )
    parser_comp_hets.add_argument('--filter',
            dest='filter',
            metavar='STRING',
            help='Restrictions to apply to variants (SQL syntax)')
    parser_comp_hets.add_argument('--only-affected',
            dest='only_affected',
            action='store_true',
            help='Report solely those compund heterozygotes impacted a sample \
                  labeled as affected.',
            default=False)
    parser_comp_hets.add_argument('--ignore-phasing',
            dest='ignore_phasing',
            action='store_true',
            help='Ignore phasing when screening for compound hets. \
                  Candidates are inherently _putative_.',
            default=False)
    parser_comp_hets.set_defaults(func=tool_compound_hets.run)

    #########################################
    # $ gemini pathways
    #########################################
    parser_pathway = subparsers.add_parser('pathways',
            help='Map genes and variants to KEGG pathways')
    parser_pathway.add_argument('db',
            metavar='db',
            help='The name of the database to be queried')
    parser_pathway.add_argument('-v',
            dest='version',
            default='68',
            metavar='STRING',
            help="Version of ensembl genes to use. "
                 "Supported versions: 66 to 71\n"
            )
    parser_pathway.add_argument('--lof',
            dest='lof',
            action='store_true',
            help='Report pathways for indivs/genes/sites with LoF variants',
            default=False)
    parser_pathway.set_defaults(func=tool_pathways.pathways)

    #########################################
    # $ gemini lof_sieve
    #########################################
    parser_lof_sieve = subparsers.add_parser('lof_sieve',
            help='Prioritize LoF mutations')
    parser_lof_sieve.add_argument('db',
            metavar='db',
            help='The name of the database to be queried')
    parser_lof_sieve.set_defaults(func=tool_lof_sieve.lof_sieve)

    #########################################
    # $ gemini burden
    #########################################
    burden_help = ("Gene-level genetic burden tests. By default counts all "
                   "variants with high impact in coding regions "
                   "as contributing to burden.")

    parser_burden = subparsers.add_parser('burden',
                                          help=burden_help)
    parser_burden.add_argument('--nonsynonymous', action='store_true',
                               default=False,
                               help=("Count all nonsynonymous variants as "
                                     "contributing burden."))
    parser_burden.add_argument('--cases',
                               dest='cases',
                               nargs='*',
                               help=('Space separated list of cases for '
                                     'association testing.'))
    parser_burden.add_argument('--controls',
                               nargs='*',
                               dest='controls',
                               help=('Space separated list of controls for '
                                     'association testing.'))
    parser_burden.add_argument('--calpha',
                               action='store_true',
                               default=False,
                               help="Run the C-alpha association test.")
    parser_burden.add_argument('--permutations',
                               default=0,
                               type=int,
                               help=("Number of permutations to run for the "
                                     "C-alpha test (try 1000 to start)."))
    parser_burden.add_argument('--min-aaf',
                               dest='min_aaf',
                               type=float,
                               default=0.0,
                               help='The min. alt. allele frequency for a '
                                     'variant to be included.')
    parser_burden.add_argument('--max-aaf',
                               dest='max_aaf',
                               type=float,
                               default=1.0,
                               help='The max. alt. allele frequency for a '
                                     'variant to be included.')
    parser_burden.add_argument('--save_tscores', default=False,
                               action='store_true',
                               help='Save the permuted T-scores to a file.')
    parser_burden.add_argument('db',
                               metavar='db',
                               help='The name of the database to be queried.')

    parser_burden.set_defaults(func=tool_burden_tests.burden)

    #########################################
    # $ gemini interactions
    #########################################
    parser_interaction = subparsers.add_parser('interactions',
            help='Find interaction partners for a gene in sample variants(default mode)')
    parser_interaction.add_argument('db',
            metavar='db',
            help='The name of the database to be queried')
    parser_interaction.add_argument('-g',
            dest='gene',
            help='Gene to be used as a root in BFS/shortest_path')
    parser_interaction.add_argument('-r',
            dest='radius',
            type=int,
            help="Set filter for BFS:\n"
                 "valid numbers starting from 0")
    parser_interaction.add_argument('--var',
            dest='var_mode',
            help='var mode: Returns variant info (e.g. impact, biotype) for interacting genes',
            action='store_true',
            default=False)
    parser_interaction.set_defaults(func=tool_interactions.genequery)

    #########################################
    # gemini lof_interactions
    #########################################
    parser_interaction = subparsers.add_parser('lof_interactions',
            help='Find interaction partners for a lof gene in sample variants(default mode)')
    parser_interaction.add_argument('db',
            metavar='db',
            help='The name of the database to be queried')
    parser_interaction.add_argument('-r',
            dest='radius',
            type=int,
            help="set filter for BFS:\n")
    parser_interaction.add_argument('--var',
            dest='var_mode',
            help='var mode: Returns variant info (e.g. impact, biotype) for interacting genes',
            action='store_true',
            default=False)
    parser_interaction.set_defaults(func=tool_interactions.lofgenequery)

    #########################################
    # $ gemini autosomal_recessive
    #########################################
    parser_auto_rec = subparsers.add_parser('autosomal_recessive',
            help='Identify variants meeting an autosomal \
                  recessive inheritance model')
    parser_auto_rec.add_argument('db',
            metavar='db',
            help='The name of the database to be queried.')
    parser_auto_rec.add_argument('--columns',
            dest='columns',
            metavar='STRING',
            help='A list of columns that you would like returned. Def. = "*"',
            )
    parser_auto_rec.add_argument('--filter',
            dest='filter',
            metavar='STRING',
            help='Restrictions to apply to variants (SQL syntax)')
    parser_auto_rec.add_argument('--min-kindreds',
            dest='min_kindreds',
            type=int,
            default=1,
            help='The min. number of kindreds that must have a candidate variant in a gene.')
    parser_auto_rec.add_argument('-d',
            dest='min_sample_depth',
            type=int,
            help="The minimum aligned\
              sequence depth (genotype DP) req'd for\
              each sample (def. = 0)",
            default=0)
    #parser_auto_rec.add_argument('--gt-filter',
    #        dest='gt_filter',
    #        metavar='STRING',
    #        help='Restrictions to apply to genotype values (Python syntax)')
    parser_auto_rec.set_defaults(func=tool_autosomal_recessive.run)

    #########################################
    # $ gemini autosomal_dominant
    #########################################
    parser_auto_dom = subparsers.add_parser('autosomal_dominant',
            help='Identify variants meeting an autosomal \
                  dominant inheritance model')
    parser_auto_dom.add_argument('db',
            metavar='db',
            help='The name of the database to be queried.')
    parser_auto_dom.add_argument('--columns',
            dest='columns',
            metavar='STRING',
            help='A list of columns that you would like returned. Def. = "*"',
            )
    parser_auto_dom.add_argument('--filter',
            dest='filter',
            metavar='STRING',
            help='Restrictions to apply to variants (SQL syntax)')
    parser_auto_dom.add_argument('--min-kindreds',
            dest='min_kindreds',
            type=int,
            default=1,
            help='The min. number of kindreds that must have a candidate variant in a gene.')
    parser_auto_dom.add_argument('-d',
            dest='min_sample_depth',
            type=int,
            help="The minimum aligned\
              sequence depth (genotype DP) req'd for\
              each sample (def. = 0)",
            default=0)
    #parser_auto_dom.add_argument('--gt-filter',
    #        dest='gt_filter',
    #        metavar='STRING',
    #        help='Restrictions to apply to genotype values (Python syntax)')
    parser_auto_dom.set_defaults(func=tool_autosomal_dominant.run)

    #########################################
    # $ gemini de_novo
    #########################################
    parser_de_novo = subparsers.add_parser('de_novo',
            help='Identify candidate de novo mutations')
    parser_de_novo.add_argument('db',
            metavar='db',
            help='The name of the database to be queried.')
    parser_de_novo.add_argument('--columns',
            dest='columns',
            metavar='STRING',
            help='A list of columns that you would like returned. Def. = "*"',
            )
    parser_de_novo.add_argument('--filter',
            dest='filter',
            metavar='STRING',
            help='Restrictions to apply to variants (SQL syntax)')
    #parser_de_novo.add_argument('--gt-filter',
    #        dest='gt_filter',
    #        metavar='STRING',
    #        help='Restrictions to apply to genotype values (Python syntax)')
    parser_de_novo.add_argument('-d',
            dest='min_sample_depth',
            type=int,
            help="The minimum aligned\
                  sequence depth (genotype DP) req'd for\
                  each sample (def. = 0)",
            default=0)
    parser_de_novo.set_defaults(func=tool_de_novo_mutations.run)

    #########################################
    # $ gemini browser
    #########################################
    parser_browser = subparsers.add_parser('browser',
            help='Browser interface to gemini')
    parser_browser.add_argument('db', metavar='db',
            help='The name of the database to be queried.')
    parser_browser.set_defaults(func=gemini_browser.browser_main)



    #########################################
    # $ gemini set_somatic
    #########################################
    parser_set_somatic = subparsers.add_parser("set_somatic",
                          help="Tag somatic mutations (is_somatic) by comparint tumor/normal pairs.")
    parser_set_somatic.add_argument('db', metavar='db',
            help='The name of the database to be updated.')
    parser_set_somatic.set_defaults(func=gemini_set_somatic.set_somatic)

    parser_set_somatic.add_argument('--min-depth',
            dest='min_depth',
            type=float,
            default=30,
            help='The min combined depth for tumor + normal (def: %(default)s).')

    parser_set_somatic.add_argument('--min-qual',
            dest='min_qual',
            type=float,
            default=30,
            help='The min variant quality (VCF QUAL) (def: %(default)s).')

    parser_set_somatic.add_argument('--max-norm-alt-freq',
            dest='max_norm_alt_freq',
            type=float,
            default=0.03,
            help='The max freq. of the alt. allele in the normal sample (def: %(default)s).')

    parser_set_somatic.add_argument('--max-norm-alt-count',
            dest='max_norm_alt_count',
            type=int,
            default=2,
            help='The max count. of the alt. allele in the normal sample (def: %(default)s).')

    parser_set_somatic.add_argument('--min-norm-depth',
            dest='min_norm_depth',
            type=int,
            default=10,
            help='The minimum depth allowed in the normal sample to believe somatic (def: %(default)s).')

    parser_set_somatic.add_argument('--min-tumor-alt-freq',
            dest='min_tumor_alt_freq',
            type=float,
            default=0.05,
            help='The min freq. of the alt. allele in the tumor sample (def: %(default)s).')

    parser_set_somatic.add_argument('--min-tumor-alt-count',
            dest='min_tumor_alt_count',
            type=int,
            default=2,
            help='The min count. of the alt. allele in the tumor sample (def: %(default)s).')

    parser_set_somatic.add_argument('--min-tumor-depth',
            dest='min_tumor_depth',
            type=int,
            default=10,
            help='The minimum depth allowed in the tumor sample to believe somatic (def: %(default)s).')

    parser_set_somatic.add_argument('--chrom',
            dest='chrom',
            metavar='STRING',
            help='A specific chromosome on which to tag somatic mutations. (def: %(default)s).',
            default=None,
            )

    parser_set_somatic.add_argument('--dry-run',
            dest='dry_run',
            action='store_true',
            help='Don\'t set the is_somatic flag, just report what _would_ be set. For testing parameters.',
            default=False)


    #########################################
    # $ gemini actionable_mutations
    #########################################
    parser_actionable_mut = subparsers.add_parser("actionable_mutations",
                          help="Retriev genes with actionable somatic mutations via COSMIC and DGIdb.")
    parser_actionable_mut.add_argument('db', metavar='db',
            help='The name of the database to be queried.')
    parser_actionable_mut.set_defaults(func=gemini_actionable_mutations.get_actionable_mutations)


    #########################################
    # $ gemini update
    #########################################
    parser_update = subparsers.add_parser("update", help="Update gemini software and data files.")
    parser_update.add_argument("--devel", help="Get the latest development version instead of the release",
                               action="store_true", default=False)
    parser_update.add_argument("--dataonly", help="Only update data, not the underlying libraries.",
                               action="store_true", default=False)
    parser_update.add_argument("--extra", help="Add additional non-standard genome annotations to include",
                               action="append", default=[], choices=["gerp_bp","cadd_score"])
    parser_update.set_defaults(func=gemini_update.release)


    #######################################################
    # parse the args and call the selected function
    #######################################################
    args = parser.parse_args()

    # make sure database is found if provided
    if len(sys.argv) > 2 and sys.argv[1] not in \
       ["load", "merge_chunks", "load_chunk"]:
        if hasattr(args, "db") and args.db is not None and not os.path.exists(args.db):
            sys.stderr.write("Requested GEMINI database (%s) not found. "
                             "Please confirm the provided filename.\n"
                             % args.db)

    try:
        args.func(parser, args)
    except IOError, e:
        if e.errno != 32:  # ignore SIGPIPE
            raise

class IPythonAction(argparse.Action):
    def __call__(self, parser, args, values, option = None):
        args.scheduler = values
        if xor(args.scheduler, args.queue):
            parser.error("If you are using the IPython parallel loading, you "
                         "must specify both a scheduler with --scheduler and a "
                         "queue to use with --queue.")

def xor(arg1, arg2):
    return bool(arg1) ^ bool(arg2)


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = gemini_merge_chunks
#!/usr/bin/env python
import sqlite3
import os
import database as gemini_db
import gemini_utils as util


def append_variant_info(main_curr, chunk_db):
    """
    Append the variant and variant_info data from a chunk_db
    to the main database.
    """

    cmd = "attach ? as toMerge"
    main_curr.execute(cmd, (chunk_db, ))

    main_curr.execute("BEGIN TRANSACTION")
    cmd = "INSERT INTO variants SELECT * FROM toMerge.variants"
    main_curr.execute(cmd)

    cmd = \
        "INSERT INTO variant_impacts SELECT * FROM toMerge.variant_impacts"
    main_curr.execute(cmd)
    main_curr.execute("END TRANSACTION")

    cmd = "detach toMerge"
    main_curr.execute(cmd)


def append_sample_genotype_counts(main_curr, chunk_db):
    """
    Append the sample_genotype_counts from a chunk_db
    to the main database.
    """
    cmd = "attach ? as toMerge"
    main_curr.execute(cmd, (chunk_db, ))

    cmd = "INSERT INTO sample_genotype_counts \
           SELECT * FROM toMerge.sample_genotype_counts"
    main_curr.execute(cmd)

    cmd = "detach toMerge"
    main_curr.execute(cmd)


def append_sample_info(main_curr, chunk_db):
    """
    Append the sample info from a chunk_db
    to the main database.
    """
    cmd = "attach ? as toMerge"
    main_curr.execute(cmd, (chunk_db, ))

    cmd = "create table samples as select * from toMerge.samples where 1=0"
    main_curr.execute(cmd)

    cmd = "INSERT INTO samples SELECT * FROM toMerge.samples"
    main_curr.execute(cmd)

    cmd = "detach toMerge"
    main_curr.execute(cmd)


def append_resource_info(main_curr, chunk_db):
    """
    Append the resource info from a chunk_db
    to the main database.
    """
    cmd = "attach ? as toMerge"
    main_curr.execute(cmd, (chunk_db, ))

    cmd = "INSERT INTO resources SELECT * FROM toMerge.resources"
    main_curr.execute(cmd)

    cmd = "detach toMerge"
    main_curr.execute(cmd)


def append_version_info(main_curr, chunk_db):
    """
    Append the version info from a chunk_db
    to the main database.
    """
    cmd = "attach ? as toMerge"
    main_curr.execute(cmd, (chunk_db, ))

    cmd = "INSERT INTO version SELECT * FROM toMerge.version"
    main_curr.execute(cmd)

    cmd = "detach toMerge"
    main_curr.execute(cmd)

def append_gene_summary(main_curr, chunk_db):
    """
    Append the gene_summary from a chunk_db
    to the main database.
    """
    cmd = "attach ? as toMerge"
    main_curr.execute(cmd, (chunk_db, ))

    cmd = "INSERT INTO gene_summary SELECT * FROM toMerge.gene_summary"
    main_curr.execute(cmd)

    cmd = "detach toMerge"
    main_curr.execute(cmd)

def append_gene_detailed(main_curr, chunk_db):
    """
    Append the gene_detailed from a chunk_db
    to the main database.
    """
    cmd = "attach ? as toMerge"
    main_curr.execute(cmd, (chunk_db, ))

    cmd = "INSERT INTO gene_detailed SELECT * FROM toMerge.gene_detailed"
    main_curr.execute(cmd)

    cmd = "detach toMerge"
    main_curr.execute(cmd)

def update_sample_genotype_counts(main_curr, chunk_db):
    """
    Update the main sample_genotype_counts table with the
    counts observed in one of the chunked databases (chunk_db)
    """
    curr_db_conn = sqlite3.connect(chunk_db)
    curr_db_conn.isolation_level = None
    curr_db_conn.row_factory = sqlite3.Row
    curr_db_curr = curr_db_conn.cursor()

    cmd = "SELECT sample_id, num_hom_ref, \
                  num_het, num_hom_alt, \
                  num_unknown FROM sample_genotype_counts"
    curr_db_curr.execute(cmd)
    for row in curr_db_curr:
        main_curr.execute("""UPDATE sample_genotype_counts
                          SET num_hom_ref = num_hom_ref + ?,
                              num_het = num_het + ?,
                              num_hom_alt = num_hom_alt + ?,
                              num_unknown = num_unknown + ?
                          WHERE sample_id= ? """,
                          (row['num_hom_ref'],
                           row['num_het'],
                           row['num_hom_alt'],
                           row['num_unknown'],
                           row['sample_id']))
    curr_db_curr.close()


def merge_db_chunks(args):

    # open up a new database
    if os.path.exists(args.db):
        os.remove(args.db)

    main_conn = sqlite3.connect(args.db)
    main_conn.isolation_level = None
    main_curr = main_conn.cursor()
    main_curr.execute('PRAGMA synchronous = OFF')
    main_curr.execute('PRAGMA journal_mode=MEMORY')
    # create the gemini database tables for the new DB
    gemini_db.create_tables(main_curr)

    databases = []
    for database in args.chunkdbs:
        databases.append(database)

    for idx, database in enumerate(databases):

        db = database[0]

        append_variant_info(main_curr, db)

        # we only need to add these tables from one of the chunks.
        if idx == 0:
            append_sample_genotype_counts(main_curr, db)
            append_sample_info(main_curr, db)
            append_resource_info(main_curr, db)
            append_version_info(main_curr, db)
            append_gene_summary(main_curr, db)
            append_gene_detailed(main_curr, db)
        else:
            update_sample_genotype_counts(main_curr, db)

    main_conn.commit()
    main_curr.close()


def merge_chunks(parser, args):
    merge_db_chunks(args)

########NEW FILE########
__FILENAME__ = gemini_plot
#!/usr/bin/env python

import os
import sys
import sqlite3


def plot(parser, args):
    """
    To do.
    """
    pass

if __name__ == "__main__":
    plot()

########NEW FILE########
__FILENAME__ = gemini_query
#!/usr/bin/env python
import os
import sys
from itertools import tee, ifilterfalse
from collections import defaultdict

# gemini imports
import GeminiQuery
from GeminiQuery import select_formatter
from gemini_constants import *
from gemini_region import add_region_to_query
from gemini_subjects import (Subject, get_subjects, get_subjects_in_family,
                             get_family_dict)
from gemini_utils import itersubclasses
from dgidb import query_dgidb

def all_samples_predicate(args):
    """ returns a predicate that returns True if, for a variant,
    the only samples that have the variant have a given phenotype
    """
    subjects = get_subjects(args).values()
    return select_subjects_predicate(subjects, args)

def family_wise_predicate(args):
    formatter = select_formatter(args)
    families = get_family_dict(args)
    gq = GeminiQuery.GeminiQuery(args.db, out_format=formatter)
    predicates = []
    for f in families.values():
        family_names = [x.name for x in f]
        subjects = get_subjects_in_family(args, f).values()
        predicates.append(select_subjects_predicate(subjects, args,
                                                    family_names))
    def predicate(row):
        return sum([p(row) for p in predicates]) >= args.min_kindreds
    return predicate

def select_subjects_predicate(subjects, args, subset=None):
    subjects = set([s.name for s in subjects])
    predicates = []
    if "all" in args.in_subject:
        predicates.append(variant_in_all_subjects(subjects))
    if "none" in args.in_subject:
        predicates.append(variant_not_in_subjects(subjects))
    if "only" in args.in_subject:
        predicates.append(variant_only_in_subjects(subjects, subset))
    if "any" in args.in_subject:
        predicates.append(variant_in_any_subject(subjects))
    def predicate(row):
        return all([p(row) for p in predicates])
    return predicate

def variant_in_any_subject(subjects):
    def predicate(row):
        return subjects.intersection(samples_with_variant(row)) != set()
    return predicate

def variant_in_all_subjects(subjects):
    def predicate(row):
        return subjects.issubset(samples_with_variant(row))
    return predicate

def variant_only_in_subjects(subjects, subset=None):
    def predicate(row):
        if subset:
            check = set(subset).intersection(samples_with_variant(row))
        else:
            check = samples_with_variant(row)
        return check and subjects.issuperset(check)
    return predicate

def variant_not_in_subjects(subjects):
    def predicate(row):
        return subjects.intersection(samples_with_variant(row)) == set()
    return predicate

def samples_with_variant(row):
    return row['variant_samples']

def queries_variants(query):
    return "variants" in query.lower()

def get_row_predicates(args):
    """
    generate a list of predicates a row must pass in order to be
    returned from a query
    """
    predicates = []
    if args.family_wise:
        predicates.append(family_wise_predicate(args))
    elif args.sample_filter:
        predicates.append(all_samples_predicate(args))
    return predicates


def needs_genotypes(args):
    return (args.show_variant_samples or 
            args.family_wise or 
            args.sample_filter or 
            args.carrier_summary or
            args.show_families or 
            args.gt_filter)


def needs_gene(args):
    return (args.dgidb)


def add_required_columns_to_query(args):
    if args.region:
        add_region_to_query(args)


def run_query(args):
    predicates = get_row_predicates(args)
    add_required_columns_to_query(args)
    formatter = select_formatter(args)
    genotypes_needed = needs_genotypes(args)
    gene_needed = needs_gene(args)
    gq = GeminiQuery.GeminiQuery(args.db, out_format=formatter)
    gq.run(args.query, args.gt_filter, args.show_variant_samples,
           args.sample_delim, predicates, genotypes_needed,
           gene_needed, args.show_families)

    if args.use_header and gq.header:
        print gq.header

    if not args.dgidb:
        for row in gq:
            print row
    else:
        # collect a list of all the genes that need to be queried
        # from DGIdb
        genes = defaultdict()
        for row in gq:
            genes[row['gene']] = True

        # collect info from DGIdb
        dgidb_info = query_dgidb(genes)

        # rerun the query (the cursor is now consumed)
        gq = GeminiQuery.GeminiQuery(args.db, out_format=formatter)
        gq.run(args.query, args.gt_filter, args.show_variant_samples,
           args.sample_delim, predicates, genotypes_needed,
           gene_needed, args.show_families)

        # report the query results with DGIdb info added at the end.
        for row in gq:  
            print str(row) + "\t" + str(dgidb_info[row['gene']])


def query(parser, args):

    if (args.db is None):
        parser.print_help()

    if os.path.exists(args.db):
        run_query(args)

if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = gemini_region
#!/usr/bin/env python
import sqlite3
import re
import os
import sys

import GeminiQuery
from GeminiQuery import select_formatter

def _report_results(args, query, gq):
    # report the results of the region query
    gq.run(query, show_variant_samples=args.show_variant_samples)
    if args.use_header and gq.header:
        print gq.header

    for row in gq:
        print row


def get_region(args, gq):
    region_regex = re.compile("(\S+):(\d+)-(\d+)")

    try:
        region = region_regex.findall(args.region)[0]
    except IndexError:
        sys.exit("Malformed region (--reg) string")

    if len(region) != 3:
        sys.exit("Malformed region (--reg) string")

    chrom = region[0]
    start = region[1]
    end = region[2]

    if args.columns is not None:
        query = "SELECT " + str(args.columns) + \
                    " FROM variants "
    else:
        query = "SELECT * FROM variants "

    query += "WHERE chrom = " + "'" + chrom + "'" + \
        " AND ((start BETWEEN " + start + " AND " + end + ")" +\
        " OR (end BETWEEN " + start + " AND " + end + "))"

    if args.filter:
        query += " AND " + args.filter

    query += " ORDER BY chrom, start"

    _report_results(args, query, gq)



def get_gene(args, gq):
    """
    Report all variants in a specific gene.
    """
    if args.columns is not None:
        query = "SELECT " + str(args.columns) + \
                    " FROM variants "
    else:
        query = "SELECT * FROM variants "

    query += "WHERE gene = " + "'" + args.gene + "' "

    if args.filter:
        query += " AND " + args.filter

    query += " ORDER BY chrom, start"

    _report_results(args, query, gq)

def add_region_to_query(args):
    region_regex = re.compile("(\S+):(\d+)-(\d+)")


    try:
        region = region_regex.findall(args.region)[0]
    except IndexError:
        sys.exit("Malformed region (--reg) string")

    if len(region) != 3:
        sys.exit("Malformed region (--reg) string")

    chrom = region[0]
    start = region[1]
    end = region[2]

    where_clause = " chrom = " + "'" + chrom + "'" + \
        " AND ((start BETWEEN " + start + " AND " + end + ")" +\
        " OR (end BETWEEN " + start + " AND " + end + "))"

    args.query = _add_to_where_clause(args.query, where_clause)


def _add_to_where_clause(query, where_clause):
    where_index = query.lower().find("where")
    prefix = query[0:where_index]
    suffix = query[where_index + len("where"):]
    if where_index == -1:
        query += " WHERE " + where_clause
    else:
        query = "{0} WHERE ({1}) AND ({2})".format(prefix, suffix, where_clause)
    return query



def region(parser, args):

    if os.path.exists(args.db):

        formatter = select_formatter(args)
        gq = GeminiQuery.GeminiQuery(args.db, out_format=formatter)

        if args.region is not None and args.gene is not None:
            sys.exit('EXITING: Choose either --reg or --gene, not both.\n')
        elif args.region is not None:
            get_region(args, gq)
        elif args.gene is not None:
            get_gene(args, gq)

########NEW FILE########
__FILENAME__ = gemini_set_somatic
#!/usr/bin/env python
import sqlite3
from collections import Counter
from gemini_constants import *
import gemini_subjects
import GeminiQuery

def get_tumor_normal_pairs(args):
    conn = sqlite3.connect(args.db)
    conn.isolation_level = None
    conn.row_factory = sqlite3.Row
    c = conn.cursor()
    return gemini_subjects.get_families(c)

def tag_somatic_mutations(args):

    t_n_pairs = get_tumor_normal_pairs(args)

    gq = GeminiQuery.GeminiQuery(args.db)

    if args.chrom is None:
        query = "SELECT variant_id, chrom, start, end, \
                        ref, alt, gene, impact, gts, gt_types, \
                        gt_ref_depths, gt_alt_depths \
                 FROM variants \
                 WHERE depth >= " + str(args.min_depth) + \
                 " AND   qual >= " + str(args.min_qual) 
    else:
        query = "SELECT variant_id, chrom, start, end, \
                ref, alt, gene, impact, gts, gt_types, \
                gt_ref_depths, gt_alt_depths \
         FROM variants \
         WHERE depth >= " + str(args.min_depth) + \
         " AND qual >= " + str(args.min_qual) + \
         " AND chrom = \'" + args.chrom + "\'"

    gq.run(query)
    smp2idx = gq.sample_to_idx

    somatic_counter = 0
    somatic_v_ids = []

    if args.dry_run:
        print'\t'.join(['tum_name', 'tum_gt', 'tum_alt_freq', 'tum_alt_depth', 'tum_depth', \
                        'nrm_name', 'nrm_gt', 'nrm_alt_freq', 'nrm_alt_depth', 'nrm_depth',
                        'chrom', 'start', 'end', 'ref', 'alt', 'gene'])

    for row in gq:

        # we can skip varinats where all genotypes are identical
        if len(set(row['gt_types'])) == 1:
            continue

        for pair in t_n_pairs:

            samples = pair.subjects
            if len(samples) != 2:
                continue

            tumor = pair.subjects[0]
            normal = pair.subjects[1]
            # swap if we guessed the tumor incorrectly
            if tumor.affected is False:
                tumor, normal = normal, tumor

            tum_idx = smp2idx[tumor.name]
            nrm_idx = smp2idx[normal.name]

            tum_gt = row['gts'][tum_idx]
            nrm_gt = row['gts'][nrm_idx]

            tum_gt_type = row['gt_types'][tum_idx]
            nrm_gt_type = row['gt_types'][nrm_idx]

            if nrm_gt_type == tum_gt_type:
                continue

            if nrm_gt_type == UNKNOWN or tum_gt_type == UNKNOWN:
                continue

            # the genotypes pass the smell test for somatic
            # mutations if in this block.
            if (nrm_gt_type == HOM_REF and tum_gt_type != HOM_REF):

               
               tum_ref_depth = row['gt_ref_depths'][tum_idx]
               nrm_ref_depth = row['gt_ref_depths'][nrm_idx]

               tum_alt_depth = row['gt_alt_depths'][tum_idx]
               nrm_alt_depth = row['gt_alt_depths'][nrm_idx]

               # total observed depth
               nrm_depth = nrm_alt_depth + nrm_ref_depth
               tum_depth = tum_alt_depth + tum_ref_depth

               if (nrm_depth < args.min_norm_depth \
                  or \
                  tum_depth < args.min_tumor_depth):
                  continue

               tum_alt_freq = float(tum_alt_depth) / \
                              (float(tum_alt_depth) + float(tum_ref_depth))

               nrm_alt_freq = float(nrm_alt_depth) / \
                              (float(nrm_alt_depth) + float(nrm_ref_depth))

               # apply evidence thresholds.
               if nrm_alt_freq > args.max_norm_alt_freq \
                  or \
                  nrm_alt_depth > args.max_norm_alt_count:
                  continue

               somatic_counter += 1
               somatic_v_ids.append((1, row['variant_id']))
               
               print'\t'.join(str(s) for s in [tumor.name,  tum_gt, tum_alt_freq, tum_alt_depth, tum_depth, \
                                   normal.name, nrm_gt, nrm_alt_freq, nrm_alt_depth, nrm_depth, \
                                   row['chrom'], row['start'], row['end'], row['ref'], row['alt'], row['gene']])

    if not args.dry_run:
        conn = sqlite3.connect(args.db)
        conn.isolation_level = None
        c = conn.cursor()

        # now set the identified mutations to True.
        update_qry = "UPDATE variants SET is_somatic = ? "
        update_qry += " WHERE variant_id = ?"
        c.executemany(update_qry, somatic_v_ids)
        print "Identified and set", somatic_counter, "somatic mutations"
    else:
        print "Would have identified and set", somatic_counter, "somatic mutations"

def set_somatic(parser, args):
    
    tag_somatic_mutations(args)


########NEW FILE########
__FILENAME__ = gemini_stats
#!/usr/bin/env python
import sqlite3
import os
import numpy as np
import cPickle
import zlib
import collections
from collections import Counter

import gemini_utils as util
from gemini_constants import *
import GeminiQuery


def get_tstv(c, args):
    """
    Report the transition / transversion ratio.
    """
    ts_cmd = "SELECT count(1) \
           FROM  variants \
           WHERE type = \'snp\' \
           AND   sub_type = \'ts\'"
    tv_cmd = "SELECT count(1) \
          FROM  variants v \
          WHERE type = \'snp\' \
          AND   sub_type = \'tv\'"
    # get the number of transitions
    c.execute(ts_cmd)
    ts = c.fetchone()[0]
    # get the number of transversions
    c.execute(tv_cmd)
    tv = c.fetchone()[0]
    # report the transitions, transversions, and the ts/tv ratio
    print "ts" + '\t' + \
          "tv" + '\t' + "ts/tv"
    print str(ts) + '\t' + \
        str(tv) + '\t' + \
        str(tstv(ts,tv))


def get_tstv_coding(c, args):
    """
    Report the transition / transversion ratio in coding regions.
    """
    ts_cmd = "SELECT count(1) \
           FROM variants v \
           WHERE v.type = \'snp\' \
           AND v.sub_type = \'ts\' \
           AND v.is_coding = 1"
    tv_cmd = "SELECT count(1) \
          FROM variants v \
          WHERE v.type = \'snp\' \
          AND v.sub_type = \'tv\' \
          AND v.is_coding = 1"
    # get the number of transitions
    c.execute(ts_cmd)
    ts = c.fetchone()[0]

    # get the number of transversions
    c.execute(tv_cmd)
    tv = c.fetchone()[0]

    # report the transitions, transversions, and the ts/tv ratio
    print "ts" + '\t' + \
          "tv" + '\t' + "ts/tv"
    print str(ts) + '\t' + \
        str(tv) + '\t' + \
        str(tstv(ts,tv))


def get_tstv_noncoding(c, args):
    """
    Report the transition / transversion ratio in non-coding regions.
    """
    ts_cmd = "SELECT count(1) \
           FROM variants v \
           WHERE v.type = \'snp\' \
           AND v.sub_type = \'ts\' \
           AND v.is_coding = 0"
    tv_cmd = "SELECT count(1) \
          FROM variants v \
          WHERE v.type = \'snp\' \
          AND v.sub_type = \'tv\' \
          AND v.is_coding = 0"
    # get the number of transitions
    c.execute(ts_cmd)
    ts = c.fetchone()[0]

    # get the number of transversions
    c.execute(tv_cmd)
    tv = c.fetchone()[0]

    # report the transitions, transversions, and the ts/tv ratio
    print "ts" + '\t' + \
          "tv" + '\t' + "ts/tv"
    print str(ts) + '\t' + \
        str(tv) + '\t' + \
        str(tstv(ts,tv))


def tstv(ts, tv):
    """
    Calculate ts/tv, and avoid division by zero error
    """
    try:
        return round(float(ts) / float(tv), 4)
    except ZeroDivisionError:
        return 0

def get_snpcounts(c, args):
    """
    Report the count of each type of SNP.
    """
    query = "SELECT ref, alt, count(1) \
             FROM   variants \
             WHERE  type = \'snp\' \
             GROUP BY ref, alt"

    # get the ref and alt alleles for all snps.
    c.execute(query)
    print '\t'.join(['type', 'count'])
    for row in c:
        print '\t'.join([str(row['ref']) + "->" + str(row['alt']),
                         str(row['count(1)'])])


def get_sfs(c, args):
    """
    Report the site frequency spectrum
    """
    precision = 3
    query = "SELECT round(aaf," + str(precision) + "), count(1) \
             FROM variants \
             GROUP BY round(aaf," + str(precision) + ")"

    c.execute(query)
    print '\t'.join(['aaf', 'count'])
    for row in c:
        print '\t'.join([str(row[0]), str(row[1])])


def get_mds(c, args):
    """
    Compute the pairwise genetic distance between each sample.
    """
    idx_to_sample = {}
    c.execute("select sample_id, name from samples")
    for row in c:
        idx_to_sample[int(row['sample_id']) - 1] = row['name']

    query = "SELECT DISTINCT v.variant_id, v.gt_types\
    FROM variants v\
    WHERE v.type = 'snp'"
    c.execute(query)

    # keep a list of numeric genotype values
    # for each sample
    genotypes = collections.defaultdict(list)
    for row in c:

        gt_types = np.array(cPickle.loads(zlib.decompress(row['gt_types'])))

        # at this point, gt_types is a numpy array
        # idx:  0 1 2 3 4 5 6 .. #samples
        # type [0 1 2 1 2 0 0 ..         ]
        for idx, gt_type in enumerate(gt_types):
            sample = idx_to_sample[idx]
            genotypes[sample].append(gt_type)

    mds = collections.defaultdict(float)
    # convert the genotype list for each sample
    # to a numpy array for performance.
    # masks stores an array of T/F indicating which genotypes are
    # known (True, [0,1,2]) and unknown (False [-1]).
    masks = {}
    for s in genotypes:
        sample = str(s)
        x = np.array(genotypes[sample])
        genotypes[sample] = x
        masks[sample] = \
            np.ma.masked_where(genotypes[sample] != UNKNOWN,
                               genotypes[sample]).mask

    # compute the euclidean distance for each s1/s2 combination
    # using numpy's vectorized sum() and square() operations.
    # we use the mask arrays to identify the indices of known genotypes
    # for each sample.  by doing a bitwise AND of the mask arrays for the
    # two samples, we have a mask array of variants where __both__ samples
    # were called.
    for sample1 in genotypes:
        for sample2 in genotypes:
            pair = (sample1, sample2)
            # which variants have known genotypes for both samples?
            both_mask = masks[str(sample1)] & masks[str(sample2)]
            genotype1 = genotypes[sample1]
            genotype2 = genotypes[sample2]

            # distance between s1 and s2:
            eucl_dist = float(np.sum(np.square((genotype1 - genotype2)[both_mask]))) \
                / \
                float(np.sum(both_mask))

            mds[pair] = eucl_dist

    # report the pairwise MDS for each sample pair.
    print "sample1\tsample2\tdistance"
    for pair in mds:
        print "\t".join([str(pair[0]), str(pair[1]), str(round(mds[pair], 4))])


def get_variants_by_sample(c, args):
    """
    Report the number of variants observed for each sample
    where the sample had a non-ref genotype
    """
    idx_to_sample = util.map_indices_to_samples(c)

    # report.
    print '\t'.join(['sample', 'total'])

    query = "SELECT sample_id, \
             (num_het + num_hom_alt) as total \
             FROM sample_genotype_counts"
    c.execute(query)
    for row in c:
        sample = idx_to_sample[row['sample_id']]
        print "\t".join(str(s) for s in [sample,
                                         row['total']])


def get_gtcounts_by_sample(c, args):
    """
    Report the count of each genotype class
    observed for each sample.
    """
    idx_to_sample = util.map_indices_to_samples(c)

    # report.
    print '\t'.join(['sample', 'num_hom_ref', 'num_het',
                     'num_hom_alt', 'num_unknown', 'total'])

    query = "SELECT *, \
             (num_hom_ref + num_het + num_hom_alt + num_unknown) as total \
             FROM sample_genotype_counts"
    c.execute(query)
    # count the number of each genotype type obs. for each sample.
    for row in c:
        sample = idx_to_sample[row['sample_id']]
        print "\t".join(str(s) for s in [sample,
                                         row['num_hom_ref'],
                                         row['num_het'],
                                         row['num_hom_alt'],
                                         row['num_unknown'],
                                         row['total']])


def summarize_query_by_sample(args):
    gq = GeminiQuery.GeminiQuery(args.db)
    gq.run(args.query, show_variant_samples=True, gt_filter=args.gt_filter)
    total_counts = Counter()
    het_counts = Counter()
    hom_alt_counts = Counter()
    hom_ref_counts = Counter()
    print "\t".join(["sample", "total", "num_het", "num_hom_alt", "num_hom_ref"])
    for row in gq:
        total_counts.update(row["variant_samples"])
        het_counts.update(row["HET_samples"])
        hom_alt_counts.update(row["HOM_ALT_samples"])
        hom_ref_counts.update(row["HOM_REF_samples"])
    for key in total_counts.keys():
        count_row = [key, total_counts.get(key, 0), het_counts.get(key, 0),
                     hom_alt_counts.get(key, 0), hom_ref_counts.get(key, 0)]
        print "\t".join(map(str, count_row))


def stats(parser, args):

    if os.path.exists(args.db):
        conn = sqlite3.connect(args.db)
        conn.isolation_level = None
        conn.row_factory = sqlite3.Row
        c = conn.cursor()

        if args.tstv:
            get_tstv(c, args)
        elif args.tstv_coding:
            get_tstv_coding(c, args)
        elif args.tstv_noncoding:
            get_tstv_noncoding(c, args)
        elif args.snp_counts:
            get_snpcounts(c, args)
        elif args.sfs:
            get_sfs(c, args)
        elif args.variants_by_sample:
            get_variants_by_sample(c, args)
        elif args.genotypes_by_sample:
            get_gtcounts_by_sample(c, args)
        elif args.mds:
            get_mds(c, args)
        elif args.query:
            summarize_query_by_sample(args)

########NEW FILE########
__FILENAME__ = gemini_subjects
#!/usr/bin/env python
import sqlite3
import os
import sys
import numpy as np
from collections import defaultdict

from gemini_constants import *
import GeminiQuery



class Subject(object):

    """
    Describe a single subject in the the samples table.
    """
    def __init__(self, row):
        self._set_fields_from_row(row)

    def __repr__(self):
        return "\t".join(map(str, [self.name, self.paternal_id,
                                   self.maternal_id, self.phenotype]))

    def set_father(self):
        self.father = True

    def set_mother(self):
        self.mother = True

    def _set_fields_from_row(self, row):
        [setattr(self, k, v) for (k, v) in zip(row.keys(), list(row))]
        self.phenotype = int(self.phenotype) if self._has_phenotype() else None
        self._set_affected_status(row)

    def _has_phenotype(self):
        if hasattr(self, 'phenotype') and self.phenotype is not None:
            return True

    def _set_affected_status(self, row):
        # 1 = unaffected
        # 2 = affected
        # 0 or -9 is unknown.
        # http://pngu.mgh.harvard.edu/~purcell/plink/data.shtml#ped
        if str(self.phenotype) == "2":
            self.affected = True
        elif str(self.phenotype) == "1":
            self.affected = False
        # distinguish unknown from known to be unaffected.
        else:
            self.affected = None


class Family(object):

    """
    Describe the relationships among multiple subjects in a family.
    """
    def __init__(self, subjects):
        self.subjects = subjects
        self.father = None
        self.mother = None
        self.family_id = self.subjects[0].family_id
        self.children = []
        self.is_constructed = False

        self.find_parents()

    def has_an_affected(self):
        """
        Return True if the Family has at least one affected individual.
        Otherwise return False.
        """
        for subject in self.subjects:
            if subject.affected:
                return True
        return False

    def has_an_affected_child(self):
        """
        Return True if the Family has at least one affected child.
        Otherwise return False.
        """
        if not self.is_constructed:
            self.find_parents()

        for child in self.children:
            if child.affected:
                return True
        return False

    def find_parents(self):
        """
        Screen for children with parental ids so that
        we can identify the parents in this family.

        NOTE: assumes at most a 2 generation family.
        """

        # build only if the family has not already been built.
        if self.is_constructed is False:

            self.father_name = None
            self.mother_name = None
            for subject in self.subjects:
                # if mom and dad are found, we know this is the child
                if subject.maternal_id is not None and \
                   str(subject.maternal_id) != "-9" and \
                   str(subject.maternal_id) != "0" and \
                   str(subject.paternal_id) is not None and \
                   str(subject.paternal_id) != "-9" and \
                   subject.paternal_id != "0":
                    self.father_name = str(subject.paternal_id)
                    self.mother_name = str(subject.maternal_id)
                    self.children.append(subject)

            # now track the actual sampleIds for the parents
            for subject in self.subjects:
                if self.father_name is not None and \
                   subject.name == self.father_name:
                    self.father = subject
                elif self.mother_name is not None and \
                   subject.name == self.mother_name:
                    self.mother = subject

            # prevent reconstructing family every time function is called.
            self.is_constructed = True

        if self.father is not None and self.mother is not None:
            return True
        else:
            return False

    def get_auto_recessive_filter(self):
        """
        Generate an autosomal recessive eval() filter to apply for this family.
        For example:

        '(gt_types[57] == HET and \  # mom
          gt_types[58] == HET and \  # dad
          gt_types[11] == HOM_ALT)'  # affected child
        """

        parents_found = self.find_parents()
        affected_found = self.has_an_affected()
        # identify which samples are the parents in the family.
        # Fail if both parents are not found
        if not parents_found and not affected_found:
            sys.stderr.write("WARNING: Unable to identify at least one "
                             "affected individual for family (%s). "
                             "Consequently, GEMINI will not screen for "
                             "variants in this family.\n"
                 % self.family_id)
            return "False"

        elif not parents_found and affected_found:
            sys.stderr.write("WARNING: Unable to identify parents for family (%s). "
                             "Consequently, GEMINI will solely place genotype "
                             "requirements on subjects based on their phenotype.\n"
                             % self.family_id)

            mask = "("
            for i, subject in enumerate(self.subjects):
                if subject.affected:
                    mask += 'gt_types[' + str(subject.sample_id - 1) + "] == " + \
                        str(HOM_ALT)
                else:
                    mask += 'gt_types[' + str(subject.sample_id - 1) + "] != " + \
                        str(HOM_ALT)

                if i < (len(self.subjects) - 1):
                    mask += " and "
            mask += ")"

            return mask

        elif parents_found:
            # if either parent is affected, this family cannot satisfy
            # a recessive model, as the parents should be carriers.
            if self.father.affected == True or self.mother.affected == True:
                return "False"


            mask = "("
            mask += 'gt_types[' + str(self.father.sample_id - 1) + "] == " + \
                str(HET)
            mask += " and "
            mask += 'gt_types[' + str(self.mother.sample_id - 1) + "] == " + \
                str(HET)

            if self.has_an_affected_child():
                for i, child in enumerate(self.children):
                    if child.affected is True:
                        mask += " and "
                        mask += 'gt_types[' + str(child.sample_id - 1) + "] == " + \
                            str(HOM_ALT)
                    # only allow an unaffected if there are other affected children
                    elif child.affected is False and self.has_an_affected_child():
                        mask += " and "
                        mask += 'gt_types[' + str(child.sample_id - 1) + "] != " + \
                            str(HOM_ALT)
                    elif child.affected is None:
                        # assume just testing for inheritance patterns
                        mask += " and "
                        mask += 'gt_types[' + str(child.sample_id - 1) + "] == " + \
                            str(HOM_ALT)
            else:
                for i, child in enumerate(self.children):
                    mask += " and "
                    mask += 'gt_types[' + str(child.sample_id - 1) + "] == " + \
                        str(HOM_ALT)

            mask += ")"
            return mask

    def get_auto_dominant_filter(self):
        """
        Generate an autosomal dominant eval() filter to apply for this family.
        For example:
        '(
          ((bool(gt_types[57] == HET)         # mom
            != \
            bool(gt_types[58] == HET)) and \  # dad
            gt_types[11] == HET               # affected child
        )'

        NOTE: the bool(dad) != bool(mom) is an XOR requiring that one and
        only one of the parents is heterozygous
        """

        parents_found = self.find_parents()
        affected_found = self.has_an_affected()

        # identify which samples are the parents in the family.
        # Fail if both parents are not found
        if not parents_found and not affected_found:
            sys.stderr.write("WARNING: Unable to identify at least one "
                             "affected individual for family (%s). "
                             "Consequently, GEMINI will not screen for "
                             "variants in this family.\n"
                 % self.family_id)
            return "False"

        elif not parents_found and affected_found:
            sys.stderr.write("WARNING: Unable to identify parents for family (%s). "
                             "Consequently, GEMINI will solely place genotype "
                             "requirements on subjects based on their phenotype.\n"
                             % self.family_id)

            mask = "("
            for i, subject in enumerate(self.subjects):
                if subject.affected:
                    mask += 'gt_types[' + str(subject.sample_id - 1) + "] == " + \
                        str(HET)
                else:
                    mask += 'gt_types[' + str(subject.sample_id - 1) + "] == " + \
                        str(HOM_REF)

                if i < (len(self.subjects) - 1):
                    mask += " and "
            mask += ")"

            return mask

        elif parents_found:
            mask = ""
            if self.father.affected is True and self.mother.affected is True:
                # doesn't meet an auto. dominant model if both parents are affected
                # [*]---(*)
                #     |
                #    (*)
                return "False"
            elif ((self.father.affected is False and self.mother.affected is False)
                 or
                 (self.father.affected is None and self.mother.affected is None)):
                # if neither parents are affected, or the affection status is
                # unknown for both, we can just screen for variants where one and
                # only one of the parents are hets and and the child is also a het
                # []---()
                #    |
                #   (*)
                mask = "((bool("
                mask += 'gt_types[' + str(self.father.sample_id - 1) + "] == " + \
                    str(HET)
                mask += ") != "
                mask += 'bool(gt_types[' + \
                        str(self.mother.sample_id - 1) + "] == " + \
                        str(HET)
                mask += ")) and "
                for i, child in enumerate(self.children):
                    if child.affected:
                        mask += 'gt_types[' + str(child.sample_id - 1) + "] == " + \
                            str(HET)
                    else:
                        mask += 'gt_types[' + str(child.sample_id - 1) + "] == " + \
                            str(HOM_REF)

                    if i < (len(self.children) - 1):
                        mask += " and "
                mask += ")"
                return mask
            elif (self.father.affected is True and
                  self.mother.affected is not True):
                # if only Dad is known to be affected, we must enforce
                # that only the affected child and Dad have the
                # same heterozygous genotype.
                # [*]---()
                #     |
                #    (*)
                mask = "(("
                mask += 'gt_types[' + str(self.father.sample_id - 1) + "] == " + \
                    str(HET)
                mask += " and "
                mask += 'gt_types[' + str(self.mother.sample_id - 1) + "] != " + \
                    str(HET)
                mask += ") and "
                for i, child in enumerate(self.children):
                    if child.affected:
                        mask += 'gt_types[' + str(child.sample_id - 1) + "] == " + \
                              str(HET)
                    else:
                        mask += 'gt_types[' + str(child.sample_id - 1) + "] == " + \
                              str(HOM_REF)
                    if i < (len(self.children) - 1):
                        mask += " and "
                mask += ")"
                return mask
            elif (self.father.affected is not True
                  and self.mother.affected is True):
                # if only Mom is known to be affected, we must enforce
                # that only the affected child and Mom have the
                # same heterozygous genotype.
                # []---(*)
                #    |
                #   (*)
                mask = "(("
                mask += 'gt_types[' + str(self.mother.sample_id - 1) + "] == " + \
                    str(HET)
                mask += " and "
                mask += 'gt_types[' + str(self.father.sample_id - 1) + "] != " + \
                    str(HET)
                mask += ") and "
                for i, child in enumerate(self.children):
                    if child.affected:
                        mask += 'gt_types[' + str(child.sample_id - 1) + "] == " + \
                              str(HET)
                    else:
                        mask += 'gt_types[' + str(child.sample_id - 1) + "] == " + \
                              str(HOM_REF)
                    if i < (len(self.children) - 1):
                        mask += " and "
                mask += ")"
                return mask



    def get_de_novo_filter(self):
        """
        Generate aa de novo mutation eval() filter to apply for this family.
        For example:

        '(gt_types[57] == HOM_REF and \  # mom
          gt_types[58] == HOM_REF and \  # dad
          gt_types[11] == HET)'          # affected child

          # [G/G]---(G/G)
          #       |
          #     (A/G)
        """

        # identify which samples are the parents in the family.
        # Fail if both parents are not found
        if not self.find_parents():
            sys.stderr.write("WARNING: Unable to find parents for family (%s). "
                 "GEMINI is currently only able to identify candidates "
                 "from two generational families.\n"
                 % self.family_id)
            return "False"

        mask = "("

        mask += "("
        mask += 'gt_types[' + str(self.father.sample_id - 1) + "] == " + \
            str(HOM_REF)
        mask += " and "
        mask += 'gt_types[' + str(self.mother.sample_id - 1) + "] == " + \
            str(HOM_REF)
        mask += ")"

        mask += " or "

        mask += "("
        mask += 'gt_types[' + str(self.father.sample_id - 1) + "] == " + \
            str(HOM_ALT)
        mask += " and "
        mask += 'gt_types[' + str(self.mother.sample_id - 1) + "] == " + \
            str(HOM_ALT)
        mask += ")"

        mask += ")"

        mask += " and ("

        if len(self.children) == 1:
            mask += 'gt_types[' + str(self.children[0].sample_id - 1) + "] == " + \
                str(HET)
        else:      
            for i, child in enumerate(self.children):
                mask += 'gt_types[' + str(child.sample_id - 1) + "] == " + \
                        str(HET)
                if i < (len(self.children) - 1):
                    mask += " or "
        mask += " )"
        return mask

    def get_genotype_columns(self):
        """
        Return the indices into the gts numpy array for the parents
        and the children.
        """
        columns = []

        if not self.find_parents():
            for subject in self.subjects:
                columns.append('gts[' + str(subject.sample_id - 1) + ']')
        else:
            columns.append('gts[' + str(self.father.sample_id - 1) + ']')
            columns.append('gts[' + str(self.mother.sample_id - 1) + ']')
            for child in self.children:
                columns.append('gts[' + str(child.sample_id - 1) + ']')

        return columns

    def get_genotype_depths(self):
        """
        Return the indices into the gt_depths numpy array for the parents
        and the children.
        """
        columns = []
        if not self.find_parents():
            for subject in self.subjects:
                columns.append('gt_depths[' + str(subject.sample_id - 1) + ']')
        else:
            columns.append('gt_depths[' + str(self.father.sample_id - 1) + ']')
            columns.append('gt_depths[' + str(self.mother.sample_id - 1) + ']')
            for child in self.children:
                columns.append('gt_depths[' + str(child.sample_id - 1) + ']')

        return columns

    def get_genotype_labels(self):
        """
        Return header genotype labels for the parents and the children.
        """
        labels = []

        # these are just anonymous affected and unaffected i
        # individuals in the same family
        if not self.find_parents():
            for subject in self.subjects:
                if subject.affected is True:
                    labels.append(subject.name + "(affected)")
                elif subject.affected is False:
                    labels.append(subject.name + "(unaffected)")
                elif subject.affected is None:
                    labels.append(subject.name + "(unknown)")
        else:

            if self.father.affected is True:
                labels.append(self.father.name + "(father; affected)")
            elif self.father.affected is False:
                labels.append(self.father.name + "(father; unaffected)")
            elif self.father.affected is None:
                labels.append(self.father.name + "(father; unknown)")

            if self.mother.affected is True:
                labels.append(self.mother.name + "(mother; affected)")
            elif self.mother.affected is False:
                labels.append(self.mother.name + "(mother; unaffected)")
            elif self.mother.affected is None:
                labels.append(self.mother.name + "(mother; unknown)")

            # handle the childrem
            for child in self.children:
                if child.affected is True:
                    labels.append(child.name + "(child; affected)")
                elif child.affected is False:
                    labels.append(child.name + "(child; unaffected)")
                elif child.affected is None:
                    labels.append(child.name + "(child; unknown)")

        return labels

    def get_subject_depth_labels(self):
        """
        Return header depth labels for the parents and the children.
        """
        subjects = []
        subjects.append(self.father.name + "(depth)")
        subjects.append(self.mother.name + "(depth)")
        for child in self.children:
            subjects.append(child.name + "(depth)")

        return subjects


def get_families(db):
    """
    Query the samples table to return a list of Family
    objects that each contain all of the Subjects in a Family.
    """
    conn = sqlite3.connect(db)
    conn.isolation_level = None
    conn.row_factory = sqlite3.Row
    c = conn.cursor()
    
    query = "SELECT * FROM samples \
             WHERE family_id is not NULL \
             ORDER BY family_id"
    c.execute(query)

    families_dict = {}
    for row in c:
        subject = Subject(row)
        family_id = subject.family_id
        if family_id in families_dict:
            families_dict[family_id].append(subject)
        else:
            families_dict[family_id] = []
            families_dict[family_id].append(subject)

    families = []
    for fam in families_dict:
        family = Family(families_dict[fam])
        families.append(family)
    return families

def get_family_dict(args):
    families = defaultdict(list)
    subjects = get_subjects(args)
    for subject in subjects.values():
        families[subject.family_id].append(subject)

    return families

def get_subjects(args):
    """
    return a dictionary of subjects, optionally using the
    subjects_query argument to filter them.
    """
    gq = GeminiQuery.GeminiQuery(args.db)
    query = "SELECT * FROM samples"
    if hasattr(args, 'sample_filter') and args.sample_filter:
        query += " WHERE " + args.sample_filter
    gq.c.execute(query)
    samples_dict = {}
    for row in gq.c:
        subject = Subject(row)
        samples_dict[subject.name] = subject
    return samples_dict

def get_subjects_in_family(args, family):
    subjects = get_subjects(args)
    family_names = [f.name for f in family]
    subject_dict = {}
    for subject in subjects:
        if subject in family_names:
            subject_dict[subject] = subjects[subject]
    return subject_dict

########NEW FILE########
__FILENAME__ = gemini_update
"""Perform in-place updates of gemini and databases when installed into virtualenv.
"""
import os
import shutil
import subprocess
import sys

import gemini.config

def release(parser, args):
    """Update gemini to the latest release, along with associated data files.
    """
    url = "https://raw.github.com/arq5x/gemini/master/requirements.txt"
    repo = "https://github.com/arq5x/gemini.git"
    # update locally isolated python
    gemini_cmd = os.path.join(os.path.dirname(sys.executable), "gemini")
    pip_bin = os.path.join(os.path.dirname(sys.executable), "pip")
    ei_bin = os.path.join(os.path.dirname(sys.executable), "easy_install")
    activate_bin = os.path.join(os.path.dirname(sys.executable), "activate")
    conda_bin = os.path.join(os.path.dirname(sys.executable), "conda")
    if not args.dataonly:
        # Work around issue with distribute where asks for 'distribute==0.0'
        # try:
        #     subprocess.check_call([ei_bin, "--upgrade", "distribute"])
        # except subprocess.CalledProcessError:
        #     try:
        #         subprocess.check_call([pip_bin, "install", "--upgrade", "distribute"])
        #     except subprocess.CalledProcessError:
        #         pass
        if os.path.exists(conda_bin):
            pkgs = ["cython", "ipython", "jinja2", "nose", "numpy",
                    "pip", "pycrypto", "pyparsing", "pysam", "pyyaml",
                    "pyzmq", "pandas", "scipy"]
            subprocess.check_call([conda_bin, "install", "--yes", "numpy"])
            subprocess.check_call([conda_bin, "install", "--yes"] + pkgs)
        elif os.path.exists(activate_bin):
            pass
        else:
            raise NotImplementedError("Can only upgrade gemini installed in anaconda or virtualenv")
        # allow downloads excluded in recent pip (1.5 or greater) versions
        try:
            p = subprocess.Popen([pip_bin, "--version"], stdout=subprocess.PIPE)
            pip_version = p.communicate()[0].split()[1]
        except:
            pip_version = ""
        pip_compat = []
        if pip_version >= "1.5":
            for req in ["python-graph-core", "python-graph-dot"]:
                pip_compat += ["--allow-external", req, "--allow-unverified", req]
        # update libraries
        subprocess.check_call([pip_bin, "install"] + pip_compat + ["-r", url])
        if args.devel:
            print("Installing latest GEMINI development version")
            subprocess.check_call([pip_bin, "install", "--upgrade", "--no-deps",
                                   "git+%s" % repo])
        print "Gemini upgraded to latest version"
    # update datafiles
    config = gemini.config.read_gemini_config()
    extra_args = ["--extra=%s" % x for x in args.extra]
    subprocess.check_call([sys.executable, _get_install_script(), config["annotation_dir"]] + extra_args)
    print "Gemini data files updated"
    # update tests
    if not args.dataonly:
        test_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(pip_bin))),
                                "gemini")
        if not os.path.exists(test_dir) or os.path.isdir(test_dir):
            _update_testbase(test_dir, repo, gemini_cmd)
            print "Run test suite with: cd %s && bash master-test.sh" % test_dir

def _get_install_script():
    try:
        import pkg_resources
        return pkg_resources.resource_filename(__name__, "install-data.py")
    except ImportError:
        return os.path.join(os.path.dirname(__file__), "install-data.py")

def _update_testbase(repo_dir, repo, gemini_cmd):
    cur_dir = os.getcwd()
    needs_git = True
    if os.path.exists(repo_dir):
        os.chdir(repo_dir)
        try:
            subprocess.check_call(["git", "pull", "origin", "master", "--tags"])
            needs_git = False
        except:
            os.chdir(cur_dir)
            shutil.rmtree(repo_dir)
    if needs_git:
        os.chdir(os.path.split(repo_dir)[0])
        subprocess.check_call(["git", "clone", repo])
    os.chdir(repo_dir)
    _update_testdir_revision(gemini_cmd)
    os.chdir(cur_dir)

def _update_testdir_revision(gemini_cmd):
    """Update test directory to be in sync with a tagged installed version or development.
    """
    try:
        p = subprocess.Popen([gemini_cmd, "--version"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        gversion = p.communicate()[0].split()[1]
    except:
        gversion = ""
    tag = ""
    if gversion:
        try:
            p = subprocess.Popen("git tag -l | grep %s" % gversion, stdout=subprocess.PIPE, shell=True)
            tag = p.communicate()[0].strip()
        except:
            tag = ""
    if tag:
        subprocess.check_call(["git", "checkout", "tags/%s" % tag])
        pass
    else:
        subprocess.check_call(["git", "reset", "--hard", "HEAD"])

########NEW FILE########
__FILENAME__ = gemini_utils
#!/usr/bin/env python
import sqlite3
import collections
from itertools import tee, ifilterfalse
from gemini_subjects import Subject


def map_samples_to_indices(c):
    """Return a dict mapping samples names (key)
       to sample indices in the numpy genotype arrays (value).
    """
    sample_to_idx = {}
    c.execute("select sample_id, name from samples")
    for row in c:
        name = str(row['name'])
        idx = row['sample_id'] - 1
        sample_to_idx[name] = idx
    return sample_to_idx


def map_indices_to_samples(c):
    """Return a dict mapping samples indices in the
       numpy arrays (key) to sample names.
    """
    return {k: v.name for (k, v) in map_indices_to_sample_objects(c).items()}
    # c.execute("select sample_id, name from samples")
    # for row in c:
    #     name = str(row['name'])
    #     idx = row['sample_id'] - 1
    #     idx_to_sample[idx] = name
    # return idx_to_sample

def map_indices_to_sample_objects(c):
    idx_to_sample_object = {}
    c.execute("select * from samples")
    for row in c:
        idx = row['sample_id'] - 1
        idx_to_sample_object[idx] = Subject(row)
    return idx_to_sample_object


def get_col_names_and_indices(sqlite_description, ignore_gt_cols=False):
    """Return a list of column namanes and a list of the row indices.
       Optionally exclude gt_* columns.
    """
    col_indices = []
    col_names = []
    for idx, col_tup in enumerate(sqlite_description):
        # e.g., each col in sqlite desc is a tuple like:
        # ('variant_id', None, None, None, None, None, None)
        col_name = col_tup[0]
        if ((not ignore_gt_cols) or
           (ignore_gt_cols and not col_name.startswith('gt'))):
            col_indices.append(idx)
            col_names.append(col_name)
    return col_names, col_indices


# http://code.activestate.com/recipes/576694/
class OrderedSet(collections.MutableSet):

    def __init__(self, iterable=None):
        self.end = end = []
        end += [None, end, end]         # sentinel node for doubly linked list
        self.map = {}                   # key --> [key, prev, next]
        if iterable is not None:
            self |= iterable

    def __len__(self):
        return len(self.map)

    def __contains__(self, key):
        return key in self.map

    def add(self, key):
        if key not in self.map:
            end = self.end
            curr = end[1]
            curr[2] = end[1] = self.map[key] = [key, curr, end]

    def discard(self, key):
        if key in self.map:
            key, prev, next = self.map.pop(key)
            prev[2] = next
            next[1] = prev

    def __iter__(self):
        end = self.end
        curr = end[2]
        while curr is not end:
            yield curr[0]
            curr = curr[2]

    def __reversed__(self):
        end = self.end
        curr = end[1]
        while curr is not end:
            yield curr[0]
            curr = curr[1]

    def pop(self, last=True):
        if not self:
            raise KeyError('set is empty')
        key = self.end[1][0] if last else self.end[2][0]
        self.discard(key)
        return key

    def __repr__(self):
        if not self:
            return '%s()' % (self.__class__.__name__,)
        return '%s(%r)' % (self.__class__.__name__, list(self))

    def __eq__(self, other):
        if isinstance(other, OrderedSet):
            return len(self) == len(other) and list(self) == list(other)
        return set(self) == set(other)

# Backport of OrderedDict() class that runs on Python 2.4, 2.5, 2.6, 2.7 and pypy.
# Passes Python2.7's test suite and incorporates all the latest updates.

try:
    from thread import get_ident as _get_ident
except ImportError:
    from dummy_thread import get_ident as _get_ident

try:
    from _abcoll import KeysView, ValuesView, ItemsView
except ImportError:
    pass


# from: http://code.activestate.com/recipes/576693/
class OrderedDict(dict):
    'Dictionary that remembers insertion order'
    # An inherited dict maps keys to values.
    # The inherited dict provides __getitem__, __len__, __contains__, and get.
    # The remaining methods are order-aware.
    # Big-O running times for all methods are the same as for regular dictionaries.

    # The internal self.__map dictionary maps keys to links in a doubly linked list.
    # The circular doubly linked list starts and ends with a sentinel element.
    # The sentinel element never gets deleted (this simplifies the algorithm).
    # Each link is stored as a list of length three:  [PREV, NEXT, KEY].

    def __init__(self, *args, **kwds):
        '''Initialize an ordered dictionary.  Signature is the same as for
        regular dictionaries, but keyword arguments are not recommended
        because their insertion order is arbitrary.

        '''
        if len(args) > 1:
            raise TypeError('expected at most 1 arguments, got %d' % len(args))
        try:
            self.__root
        except AttributeError:
            self.__root = root = []                     # sentinel node
            root[:] = [root, root, None]
            self.__map = {}
        self.__update(*args, **kwds)

    def __setitem__(self, key, value, dict_setitem=dict.__setitem__):
        'od.__setitem__(i, y) <==> od[i]=y'
        # Setting a new item creates a new link which goes at the end of the linked
        # list, and the inherited dictionary is updated with the new key/value pair.
        if key not in self:
            root = self.__root
            last = root[0]
            last[1] = root[0] = self.__map[key] = [last, root, key]
        dict_setitem(self, key, value)

    def __delitem__(self, key, dict_delitem=dict.__delitem__):
        'od.__delitem__(y) <==> del od[y]'
        # Deleting an existing item uses self.__map to find the link which is
        # then removed by updating the links in the predecessor and successor nodes.
        dict_delitem(self, key)
        link_prev, link_next, key = self.__map.pop(key)
        link_prev[1] = link_next
        link_next[0] = link_prev

    def __iter__(self):
        'od.__iter__() <==> iter(od)'
        root = self.__root
        curr = root[1]
        while curr is not root:
            yield curr[2]
            curr = curr[1]

    def __reversed__(self):
        'od.__reversed__() <==> reversed(od)'
        root = self.__root
        curr = root[0]
        while curr is not root:
            yield curr[2]
            curr = curr[0]

    def clear(self):
        'od.clear() -> None.  Remove all items from od.'
        try:
            for node in self.__map.itervalues():
                del node[:]
            root = self.__root
            root[:] = [root, root, None]
            self.__map.clear()
        except AttributeError:
            pass
        dict.clear(self)

    def popitem(self, last=True):
        '''od.popitem() -> (k, v), return and remove a (key, value) pair.
        Pairs are returned in LIFO order if last is true or FIFO order if false.

        '''
        if not self:
            raise KeyError('dictionary is empty')
        root = self.__root
        if last:
            link = root[0]
            link_prev = link[0]
            link_prev[1] = root
            root[0] = link_prev
        else:
            link = root[1]
            link_next = link[1]
            root[1] = link_next
            link_next[0] = root
        key = link[2]
        del self.__map[key]
        value = dict.pop(self, key)
        return key, value

    # -- the following methods do not depend on the internal structure --

    def keys(self):
        'od.keys() -> list of keys in od'
        return list(self)

    def values(self):
        'od.values() -> list of values in od'
        return [self[key] for key in self]

    def items(self):
        'od.items() -> list of (key, value) pairs in od'
        return [(key, self[key]) for key in self]

    def iterkeys(self):
        'od.iterkeys() -> an iterator over the keys in od'
        return iter(self)

    def itervalues(self):
        'od.itervalues -> an iterator over the values in od'
        for k in self:
            yield self[k]

    def iteritems(self):
        'od.iteritems -> an iterator over the (key, value) items in od'
        for k in self:
            yield (k, self[k])

    def update(*args, **kwds):
        '''od.update(E, **F) -> None.  Update od from dict/iterable E and F.

        If E is a dict instance, does:           for k in E: od[k] = E[k]
        If E has a .keys() method, does:         for k in E.keys(): od[k] = E[k]
        Or if E is an iterable of items, does:   for k, v in E: od[k] = v
        In either case, this is followed by:     for k, v in F.items(): od[k] = v

        '''
        if len(args) > 2:
            raise TypeError('update() takes at most 2 positional '
                            'arguments (%d given)' % (len(args),))
        elif not args:
            raise TypeError('update() takes at least 1 argument (0 given)')
        self = args[0]
        # Make progressively weaker assumptions about "other"
        other = ()
        if len(args) == 2:
            other = args[1]
        if isinstance(other, dict):
            for key in other:
                self[key] = other[key]
        elif hasattr(other, 'keys'):
            for key in other.keys():
                self[key] = other[key]
        else:
            for key, value in other:
                self[key] = value
        for key, value in kwds.items():
            self[key] = value

    __update = update  # let subclasses override update without breaking __init__

    __marker = object()

    def pop(self, key, default=__marker):
        '''od.pop(k[,d]) -> v, remove specified key and return the corresponding value.
        If key is not found, d is returned if given, otherwise KeyError is raised.

        '''
        if key in self:
            result = self[key]
            del self[key]
            return result
        if default is self.__marker:
            raise KeyError(key)
        return default

    def setdefault(self, key, default=None):
        'od.setdefault(k[,d]) -> od.get(k,d), also set od[k]=d if k not in od'
        if key in self:
            return self[key]
        self[key] = default
        return default

    def __repr__(self, _repr_running={}):
        'od.__repr__() <==> repr(od)'
        call_key = id(self), _get_ident()
        if call_key in _repr_running:
            return '...'
        _repr_running[call_key] = 1
        try:
            if not self:
                return '%s()' % (self.__class__.__name__,)
            return '%s(%r)' % (self.__class__.__name__, self.items())
        finally:
            del _repr_running[call_key]

    def __reduce__(self):
        'Return state information for pickling'
        items = [[k, self[k]] for k in self]
        inst_dict = vars(self).copy()
        for k in vars(OrderedDict()):
            inst_dict.pop(k, None)
        if inst_dict:
            return (self.__class__, (items,), inst_dict)
        return self.__class__, (items,)

    def copy(self):
        'od.copy() -> a shallow copy of od'
        return self.__class__(self)

    @classmethod
    def fromkeys(cls, iterable, value=None):
        '''OD.fromkeys(S[, v]) -> New ordered dictionary with keys from S
        and values equal to v (which defaults to None).

        '''
        d = cls()
        for key in iterable:
            d[key] = value
        return d

    def __eq__(self, other):
        '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive
        while comparison to a regular mapping is order-insensitive.

        '''
        if isinstance(other, OrderedDict):
            return len(self)==len(other) and self.items() == other.items()
        return dict.__eq__(self, other)

    def __ne__(self, other):
        return not self == other

    # -- the following methods are only used in Python 2.7 --

    def viewkeys(self):
        "od.viewkeys() -> a set-like object providing a view on od's keys"
        return KeysView(self)

    def viewvalues(self):
        "od.viewvalues() -> an object providing a view on od's values"
        return ValuesView(self)

    def viewitems(self):
        "od.viewitems() -> a set-like object providing a view on od's items"
        return ItemsView(self)


def itersubclasses(cls, _seen=None):
    """
    snagged from:  http://code.activestate.com/recipes/576949/
    itersubclasses(cls)

    Generator over all subclasses of a given class, in depth first order.

    >>> list(itersubclasses(int)) == [bool]
    True
    >>> class A(object): pass
    >>> class B(A): pass
    >>> class C(A): pass
    >>> class D(B,C): pass
    >>> class E(D): pass
    >>>
    >>> for cls in itersubclasses(A):
    ...     print(cls.__name__)
    B
    D
    E
    C
    >>> # get ALL (new-style) classes currently defined
    >>> [cls.__name__ for cls in itersubclasses(object)] #doctest: +ELLIPSIS
    ['type', ...'tuple', ...]
    """

    if not isinstance(cls, type):
        raise TypeError('itersubclasses must be called with '
                        'new-style classes, not %.100r' % cls)
    if _seen is None:
        _seen = set()
    try:
        subs = cls.__subclasses__()
    except TypeError:  # fails only when cls is type
        subs = cls.__subclasses__(cls)
    for sub in subs:
        if sub not in _seen:
            _seen.add(sub)
            yield sub
            for sub in itersubclasses(sub, _seen):
                yield sub

def partition(pred, iterable):
    'Use a predicate to partition entries into false entries and true entries'
    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9
    t1, t2 = tee(iterable)
    return list(ifilterfalse(pred, t1)), list(filter(pred, t2))

def quote_string(item):
    """ if the item is a string, put quotes around it else leave it """
    if isinstance(item, basestring):
        item = "\"" + item + "\""
    return item

########NEW FILE########
__FILENAME__ = gemini_windower
#!/usr/bin/env python
import sqlite3
import os
import gemini_utils as util

import pybedtools as pbt


def get_window_data(c, analysis_type, temp_file):
    """
    Create a temp file of the requested statistic for each variant.

    Execute a query against the variants table
    that extracts the requested column for each variant.
    save the results to '.temp.pid', which will be loaded
    into a pybedtools BedTool for use with the bedtools map
    function.  This will compute the requested statistic
    for each variant in the variants table
    """
    if analysis_type == "hwe":
        column = 'hwe'
    elif analysis_type == "nucl_div":
        column = 'pi'

    t = open(temp_file, 'w')
    query = "SELECT chrom,start,end," + \
        column + \
        " FROM variants ORDER BY chrom,start"
    c.execute(query)
    for row in c:
        if row[column] is not None:
            t.write('%s\t%d\t%d\t%f\n' % (str(row['chrom']),
                                          int(row['start']),
                                          int(row['end']),
                                          float(row[column])))
    t.close()
    # Tell bedtools map that the statistic is in the fourth column.
    # Parameterized for future mods,
    return 4


def make_windows(c, args, temp_file):
    """
    Compute the requested statistic for the user-defined windows.
    """
    # create our windows with pybedtools
    window = pbt.BedTool()
    windows = window.window_maker(genome='hg19',
                                  w=args.window_size,
                                  s=args.step_size)

    # create a temp file ('.temp.pid') storing the requested stat
    # for each variant. Load this into a pybedtools BedTool
    op_col = get_window_data(c, args.analysis_type, temp_file)
    window_data = pbt.BedTool(temp_file)

    # Use bedtools map to summarize and report
    # the requested statistic for each window
    windowed_analysis = windows.map(window_data, o=args.op_type, c=op_col)
    for window in windowed_analysis:
        each = str(window).strip().split("\t")
        if args.op_type == "collapse" or each[3] is ".":
            print "\t".join(each[0:])
        else:
            print "\t".join(each[0:3])+"\t"+str(round(float(each[3]),4))
        
    # cleanup
    os.remove(temp_file)


def windower(parser, args):

    if os.path.exists(args.db):
        conn = sqlite3.connect(args.db)
        conn.isolation_level = None
        conn.row_factory = sqlite3.Row
        c = conn.cursor()
        # on y va
        pid = os.getpid()
        temp_file = ".".join(['.temp', str(pid)])
        make_windows(c, args, temp_file)

########NEW FILE########
__FILENAME__ = gene_table
import sys
import os
import database
from gemini.config import read_gemini_config

class gene_detailed:

    def __init__(self, field):
        
        self.fields = field[:]
        self.chrom = field[0]
        self.gene = field[1]
        self.is_hgnc = field[2]
        self.ensembl_gene_id = field[3]
        self.ensembl_trans_id = field[4]
        self.biotype = field[5]
        self.trans_status = field[6]
        self.ccds_id = field[7]
        self.hgnc_id = field[8]
        self.cds_length = field[9]
        self.protein_length = field[10]
        self.transcript_start = field[11]
        self.transcript_end = field[12]
        self.strand = field[13]
        self.synonym = field[14]
        self.rvis = field[15]
        self.entrez = field[16]
        self.mam_phenotype = field[17]
        
    def __str__(self):
        return ",".join([self.chrom, self.gene, self.is_hgnc, self.ensembl_gene_id, self.ensembl_trans_id, self.biotype, self.trans_status,
                         self.ccds_id, self.hgnc_id, self.cds_length, self.protein_length, self.transcript_start, self.transcript_end, self.strand, 
                         str(self.synonym), self.rvis, self.entrez, self.mam_phenotype])
                                             
class gene_summary:
    
    def __init__(self, col):
         self.columns = col[:]
         self.chrom = col[0]
         self.gene = col[1]
         self.is_hgnc = col[2]
         self.ensembl_gene_id = col[3]
         self.hgnc_id = col[4]
         self.synonym = col[5]
         self.rvis = col[6]
         self.strand = col[7]
         self.transcript_min_start = col[8]
         self.transcript_max_end = col[9]
         self.mam_phenotype = col[10]
         
    def __str__(self):
        return ",".join([self.chrom, self.gene, self.is_hgnc, self.ensembl_gene_id, self.hgnc_id, self.synonym, self.rvis, 
                         self.strand, self.transcript_min_start, self.transcript_max_end, self.mam_phenotype])
         
def update_cosmic_census_genes(cursor):
    """
    Update the gene summary table with
    whether or not a given gene is in the
    COSMIC cancer gene census
    """
    config = read_gemini_config()
    path_dirname = config["annotation_dir"]
    file = os.path.join(path_dirname, 'cancer_gene_census.20140120.tsv')
    
    cosmic_census_genes = []
    for line in open(file, 'r'):
        fields = line.strip().split("\t")
        gene = fields[0]
        chrom = "chr" + fields[3]
        cosmic_census_genes.append((1,gene,chrom))

    database.update_gene_summary_w_cancer_census(cursor, cosmic_census_genes)
         
        

########NEW FILE########
__FILENAME__ = infotag
"""
    Utility functions for extracting specific attributes
    from the VCF INFO field.
"""


def _safe_single_attr(x):
    """Handle cleaning of attributes after extraction:
      - '.'s to indicate None
      - tuples where we expect a single value
    """
    if x is None or x is ".":
        return None
    elif isinstance(x, (list, tuple)):
        return x[0]
    else:
        return x


def extract_aaf(var):
    """
    Extract the AAF directly from the INFO field.
    """
    return var.INFO.get('AF')


def get_ancestral_allele(var):
    """
    Return the reported ancestral allele if there is one
    """
    return _safe_single_attr(var.INFO.get('AA'))


def get_rms_bq(var):
    """
    Return the RMS base quality at this position
    """
    return var.INFO.get('BQ')


def get_cigar(var):
    """
    Return the cigar string describing how to align an
    alternate allele to the reference allele
    """
    return var.INFO.get('CIGAR')


def in_hm2(var):
    """
    Return whether the variant was part of HapMap2
    """
    return var.INFO.get('H2')


def in_hm3(var):
    """
    Return whether the variant was part of HapMap3
    """
    return var.INFO.get('H3')


def is_somatic(var):
    """
    Return whether the variant is somatically acquired
    """
    return var.INFO.get('SOMATIC')


def get_depth(var):
    """
    Return the depth of aligned sequences for a given variant,
    or None if it isn't present in the VCF.
    """
    return _safe_single_attr(var.INFO.get('DP'))


def get_strand_bias(var):
    """
    Return the likelihood of strand bias,
    or None if it isn't present in the VCF.
    """
    return var.INFO.get('SB')


def get_rms_map_qual(var):
    """
    Return the RMS mapping quality,
    or None if it isn't present in the VCF.
    """
    return _safe_single_attr(var.INFO.get('MQ'))


def get_homopol_run(var):
    """
    Return the largest contiguous homopolymer run of the variant allele,
    or None if it isn't present in the VCF.
    """
    return var.INFO.get('HRun')


def get_map_qual_zero(var):
    """
    Return the total counts of mapping quality zero reads,
    or None if it isn't present in the VCF.
    """
    return var.INFO.get('MQ0')


def get_num_of_alleles(var):
    """
    return the total number of alleles in called genotypes,
    or None if it isn't present in the VCF.
    """
    return _safe_single_attr(var.INFO.get('AN'))


def get_frac_dels(var):
    """
    Returns the fraction of reads containing spanning deletions,
    or None if it isn't present in the VCF.
    """
    return var.INFO.get('Dels')


def get_haplotype_score(var):
    """
    Returns consistency of the site with two segregating haplotypes,
    or None if it isn't present in the VCF.
    """
    return var.INFO.get('HaplotypeScore')


def get_quality_by_depth(var):
    """
    Returns the quality by depth or the variant confidence,
    or None if it isn't present in the VCF.
    """
    return var.INFO.get('QD')


def get_allele_count(var):
    """
    Returns allele counts in genotypes,
    or None if it isn't present in the VCF.
    """
    return _safe_single_attr(var.INFO.get('AC'))


def get_allele_bal(var):
    """
    Returns allele balance for hets,
    or None if it isn't present in the VCF.
    """
    return _safe_single_attr(var.INFO.get('AB'))

########NEW FILE########
__FILENAME__ = install-data
#!/usr/bin/env python
import argparse
import sys
import os
import shutil
import subprocess
import time

from gemini.config import read_gemini_config, write_gemini_config

"""Install annotation data and update Gemini configuration with location.

The recommended Gemini install location is /usr/local/share/gemini.

    This installation script was inspired by a helpful suggestion from
    Brad Chapman, and is based on his code at:
    https://github.com/chapmanb/bcbb/blob/master/nextgen/tests/test_automated_analysis.py
"""

anno_files = \
['dbsnp.138.vcf.gz',
'clinvar_20140303.vcf.gz',
'29way_pi_lods_elements_12mers.chr_specific.fdr_0.1_with_scores.txt.hg19.merged.bed.gz',
'hg19.CpG.bed.gz',
'hg19.pfam.ucscgenes.bed.gz',
'hg19.gerp.elements.bed.gz',
'hg19.cytoband.bed.gz',
'hg19.dgv.bed.gz',
'hg19.gwas.bed.gz',
'hg19.rmsk.bed.gz',
'hg19.segdup.bed.gz',
'ESP6500SI.all.snps_indels.vcf.gz',
'ALL.wgs.integrated_phase1_v3.20101123.snps_indels_sv.sites.2012Oct12.vcf.gz',
'genetic_map_HapMapII_GRCh37.gz',
'wgEncodeRegTfbsClusteredV2.cell_count.20130213.bed.gz',
'encode.6celltypes.consensus.bedg.gz',
'stam.125cells.dnaseI.hg19.bed.gz',
'GRCh37-gms-mappability.vcf.gz',
'GRC_patch_regions.bed.gz',
'kegg_pathways_ensembl66',
'kegg_pathways_ensembl67',
'kegg_pathways_ensembl68',
'kegg_pathways_ensembl69',
'kegg_pathways_ensembl70',
'kegg_pathways_ensembl71',
'hprd_interaction_graph',
'cse-hiseq-8_4-2013-02-20.bed.gz',
'hg19.vista.enhancers.20131108.bed.gz',
'hg19.cosmic.v67.20131024.gz',
'detailed_gene_table_v75',
'summary_gene_table_v75',
'cancer_gene_census.20140120.tsv'
]
extra_anno_files = {"gerp_bp": "hg19.gerp.bw", "cadd_score": "whole_genome_SNVs.tsv.compressed.gz"}

toadd_anno_files = []

anno_versions = {
    "GRCh37-gms-mappability.vcf.gz": 2,
    "dbsnp.138.vcf.gz": 2,
    "clinvar_20140303.vcf.gz": 3,
    "hg19.rmsk.bed.gz": 2,
    "detailed_gene_table_v75": 2,
    "summary_gene_table_v75": 2}

def install_annotation_files(anno_root_dir, dl_files=False, extra=None):
    """Download required annotation files.
    """
    # create the full gemini data path based on
    # the root dir the user provided
    if anno_root_dir.endswith(("gemini/data", "gemini/data/", "gemini_data")):
        anno_dir = anno_root_dir
    elif anno_root_dir.endswith(("gemini", "gemini/")):
        anno_dir = os.path.join(anno_root_dir, "data")
    else:
        anno_dir = os.path.join(anno_root_dir, "gemini", "data")

    cur_config = read_gemini_config(allow_missing=True)
    cur_config["annotation_dir"] = os.path.abspath(anno_dir)
    cur_config["versions"] = anno_versions
    write_gemini_config(cur_config)

    if dl_files:
        if not os.path.exists(anno_dir):
            os.makedirs(anno_dir)
        if not os.path.isdir(anno_dir):
            sys.exit(anno_dir + " is not a valid directory.")
        _check_dependencies()
        to_dl = anno_files[:]
        if extra:
            to_dl += [extra_anno_files[x] for x in extra]
        _download_anno_files("https://s3.amazonaws.com/gemini-annotations",
                             to_dl, anno_dir, cur_config)
    #_download_anno_files("https://s3.amazonaws.com/chapmanb/gemini",
    #                     toadd_anno_files, cur_config)

def _check_dependencies():
    """Ensure required tools for download are present.
    """
    print "Checking required dependencies..."
    for cmd, url in [("curl", "http://curl.haxx.se/")]:
        try:
            retcode = subprocess.call([cmd, "--version"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        except OSError:
            retcode = 127
        if retcode == 127:
            raise OSError("gemini requires %s (%s)" % (cmd, url))
        else:
            print " %s found" % cmd

def _download_anno_files(base_url, file_names, anno_dir, cur_config):
    """Download and install each of the annotation files
    """
    for orig in file_names:
        if orig.endswith(".gz"):
            dls = [orig, "%s.tbi" % orig]
        else:
            dls = [orig]
        for dl in dls:
            url = "{base_url}/{fname}".format(fname=dl, base_url=base_url)
            _download_to_dir(url, anno_dir, anno_versions.get(orig, 1),
                             cur_config.get("versions", {}).get(orig, 1))

def _download_to_dir(url, dirname, version, cur_version):
    """
    Grab an annotation file and place in /usr/share/gemini/data
    """
    print "* downloading " + url + " to " + dirname + "\n"
    stub = os.path.basename(url)
    dest = os.path.join(dirname, stub)
    if not os.path.exists(dest) or version > cur_version:
        # download data file to staging directory instead of current
        # direction which may not have space
        dl_dir = os.path.join(dirname, "tmpdownload")
        if not os.path.exists(dl_dir):
            os.makedirs(dl_dir)
        orig_dir = os.getcwd()
        os.chdir(dl_dir)
        # download file, allowing retries for network errors
        max_retries = 2
        retries = 0
        while 1:
            cmd = ["curl", "-C", "-", "-OL", url]
            retcode = subprocess.call(cmd)
            if retcode == 0:
                break
            else:
                print "Curl failed with non-zero exit code %s. Retrying" % retcode
                if retries >= max_retries:
                    raise
                time.sleep(10)
                retries += 1
        with open(stub) as in_handle:
            line1 = in_handle.readline()
            line2 = in_handle.readline()
            if "?xml" in line1 and "Error" in line2 and "AccessDenied" in line2:
                raise ValueError("Could not download annotation file. Permission denied error: %s" % url)
        # move to system directory (/usr/share/gemini/data) and remove from pwd
        shutil.move(stub, dest)
        os.chdir(orig_dir)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("anno_dir", help="Directory to write annotation files.")
    parser.add_argument("--nodata", dest="dl_files", default=True, action="store_false")
    parser.add_argument("--extra", help="Add additional non-standard genome annotations to include",
                        action="append", default=[])
    args = parser.parse_args()
    install_annotation_files(args.anno_dir, args.dl_files, args.extra)

########NEW FILE########
__FILENAME__ = ped
default_ped_fields = ["family_id", "name", "paternal_id", "maternal_id",
                      "sex", "phenotype"]

def get_ped_fields(ped_file):
    if not ped_file:
        return default_ped_fields

    with open(ped_file) as in_handle:
        possible_header = in_handle.readline()

    if possible_header.startswith("#"):
        header = possible_header.replace("#", "").split()
        # rename the standard fields to a common name
        header = default_ped_fields + header[len(default_ped_fields):]
        return possible_header.replace("#", "").split()
    else:
        return default_ped_fields

def load_ped_file(ped_file):
    ped_dict = {}
    header = get_ped_fields(ped_file)
    for line in open(ped_file, 'r'):
        if line.startswith("#") or len(line) == 0:
            continue
        fields = line.split()
        ped_dict[fields[1]] = fields
    return ped_dict

########NEW FILE########
__FILENAME__ = popgen
import stats


def get_hwe_likelihood(obs_hom_ref, obs_het, obs_hom_alt, aaf):
    """
    Compute the likelihood of deviation from HWE using X^2,
    as well as the inbreeding coefficient.
    """
    # Bail out if aaf is undefined. This occurs
    # when there are multiple alternate alleles
    if aaf is None:
        return (None, None)

    # how many total genotypes?
    sum = (float(obs_hom_ref) + float(obs_het) + float(obs_hom_alt))
    # get the reference allele freq
    raf = 1.0 - float(aaf)
    # compute the expected number of each genotype based on p and q
    exp_hom_ref = (raf ** 2) * sum
    exp_het = (2.0 * (raf * aaf)) * sum
    exp_hom_alt = (aaf ** 2) * sum
    # get the X^2 statistcs for each genotype class.
    x2_hom_ref = ((obs_hom_ref - exp_hom_ref) ** 2) / \
        exp_hom_ref if exp_hom_ref > 0 else 0
    x2_hom_alt = ((obs_hom_alt - exp_hom_alt) ** 2) / \
        exp_hom_alt if exp_hom_alt > 0 else 0
    x2_het = ((obs_het - exp_het) ** 2) / exp_het if exp_het > 0 else 0
    x2_statistic = x2_hom_ref + x2_hom_alt + x2_het
    # return the p-value (null hyp. is that the genotypes are in HWE)
    # 1 degree of freedom b/c 3 genotypes, 2 alleles (3-2)
    # estimate the inbreeding coefficient (F_hat):
    # F_hat = 1 - O_hets / E_hets
    inbreeding_coeff = (
        1.0 - (float(obs_het) / (float(exp_het)))) if obs_het > 0 else None
    return stats.lchisqprob(x2_statistic, 1), inbreeding_coeff

########NEW FILE########
__FILENAME__ = gemini_install
#!/usr/bin/env python
"""Installer for gemini: a lightweight db framework for disease and population genetics.

https://github.com/arq5x/gemini

Handles installation of:

- Required third party software
- Required Python libraries
- Gemini application
- Associated data files

Requires: Python 2.7, git, and compilers (gcc, g++)

Run gemini_install.py -h for usage.
"""
import argparse
import platform
import os
import shutil
import subprocess
import sys

remotes = {"requirements":
           "https://raw.github.com/arq5x/gemini/master/requirements.txt",
           "cloudbiolinux":
           "https://github.com/chapmanb/cloudbiolinux.git",
           "gemini":
           "https://github.com/arq5x/gemini.git",
           "anaconda":
           "http://repo.continuum.io/miniconda/Miniconda-3.3.0-%s-x86_64.sh"}

def main(args):
    check_dependencies()
    work_dir = os.path.join(os.getcwd(), "tmpgemini_install")
    if not os.path.exists(work_dir):
        os.makedirs(work_dir)
    os.chdir(work_dir)
    print "Installing isolated base python installation"
    make_dirs(args)
    anaconda = install_anaconda_python(args, remotes)
    print "Installing gemini..."
    install_conda_pkgs(anaconda)
    gemini = install_gemini(anaconda, remotes, args.datadir, args.tooldir, args.sudo)
    cbl = get_cloudbiolinux(remotes["cloudbiolinux"])
    fabricrc = write_fabricrc(cbl["fabricrc"], args.tooldir, args.datadir,
                              "ubuntu", args.sudo)
    if args.install_tools:
        print "Installing associated tools..."
        install_tools(gemini["fab"], cbl["tool_fabfile"], fabricrc)
    os.chdir(work_dir)
    install_data(gemini["python"], gemini["data_script"], args)
    os.chdir(work_dir)
    test_script = install_testbase(args.datadir, remotes["gemini"], gemini)
    print "Finished: gemini, tools and data installed"
    print " Tools installed in:\n  %s" % args.tooldir
    print " Data installed in:\n  %s" % args.datadir
    print " Run tests with:\n  cd %s && bash %s" % (os.path.dirname(test_script),
                                                    os.path.basename(test_script))
    print " NOTE: be sure to add %s/bin to your PATH." % args.tooldir
    print " NOTE: Install data files for GERP_bp & CADD_scores (not installed by default).\n "

    shutil.rmtree(work_dir)

def install_gemini(anaconda, remotes, datadir, tooldir, use_sudo):
    """Install gemini plus python dependencies inside isolated Anaconda environment.
    """
    # Work around issue with distribute where asks for 'distribute==0.0'
    # try:
    #     subprocess.check_call([anaconda["easy_install"], "--upgrade", "distribute"])
    # except subprocess.CalledProcessError:
    #     try:
    #         subprocess.check_call([anaconda["pip"], "install", "--upgrade", "distribute"])
    #     except subprocess.CalledProcessError:
    #         pass
    # Ensure latest version of fabric for running CloudBioLinux
    subprocess.check_call([anaconda["pip"], "install", "fabric>=1.7.0"])
    # Install problem dependency separately: bx-python
    subprocess.check_call([anaconda["pip"], "install", "--upgrade",
                           "https://bitbucket.org/james_taylor/bx-python/get/tip.tar.bz2"])
    # allow downloads excluded in recent pip (1.5 or greater) versions
    try:
        p = subprocess.Popen([anaconda["pip"], "--version"], stdout=subprocess.PIPE)
        pip_version = p.communicate()[0].split()[1]
    except:
        pip_version = ""
    pip_compat = []
    if pip_version >= "1.5":
        for req in ["python-graph-core", "python-graph-dot"]:
            pip_compat += ["--allow-external", req, "--allow-unverified", req]
    subprocess.check_call([anaconda["pip"], "install"] + pip_compat + ["-r", remotes["requirements"]])
    python_bin = os.path.join(anaconda["dir"], "bin", "python")
    _cleanup_problem_files(anaconda["dir"])
    _add_missing_inits(python_bin)
    for final_name, ve_name in [("gemini", "gemini"), ("gemini_python", "python"),
                                ("gemini_pip", "pip")]:
        final_script = os.path.join(tooldir, "bin", final_name)
        ve_script = os.path.join(anaconda["dir"], "bin", ve_name)
        sudo_cmd = ["sudo"] if use_sudo else []
        if os.path.lexists(final_script):
            subprocess.check_call(sudo_cmd + ["rm", "-f", final_script])
        else:
            subprocess.check_call(sudo_cmd + ["mkdir", "-p", os.path.dirname(final_script)])
        cmd = ["ln", "-s", ve_script, final_script]
        subprocess.check_call(sudo_cmd + cmd)
    library_loc = subprocess.check_output("%s -c 'import gemini; print gemini.__file__'" % python_bin,
                                          shell=True)
    return {"fab": os.path.join(anaconda["dir"], "bin", "fab"),
            "data_script": os.path.join(os.path.dirname(library_loc.strip()), "install-data.py"),
            "python": python_bin,
            "cmd": os.path.join(anaconda["dir"], "bin", "gemini")}

def install_conda_pkgs(anaconda):
    pkgs = ["cython", "ipython", "jinja2", "nose", "numpy",
            "pip", "pycrypto", "pyparsing", "pysam", "pyyaml",
            "pyzmq", "pandas", "scipy"]
    subprocess.check_call([anaconda["conda"], "install", "--yes", "numpy"])
    subprocess.check_call([anaconda["conda"], "install", "--yes"] + pkgs)

def install_anaconda_python(args, remotes):
    """Provide isolated installation of Anaconda python.
    http://docs.continuum.io/anaconda/index.html
    """
    anaconda_dir = os.path.join(args.datadir, "anaconda")
    bindir = os.path.join(anaconda_dir, "bin")
    conda = os.path.join(bindir, "conda")
    if platform.mac_ver()[0]:
        distribution = "macosx"
    else:
        distribution = "linux"
    if not os.path.exists(anaconda_dir) or not os.path.exists(conda):
        if os.path.exists(anaconda_dir):
            shutil.rmtree(anaconda_dir)
        url = remotes["anaconda"] % ("MacOSX" if distribution == "macosx" else "Linux")
        if not os.path.exists(os.path.basename(url)):
            subprocess.check_call(["wget", url])
        subprocess.check_call("bash %s -b -p %s" %
                              (os.path.basename(url), anaconda_dir), shell=True)
    return {"conda": conda,
            "pip": os.path.join(bindir, "pip"),
            "easy_install": os.path.join(bindir, "easy_install"),
            "dir": anaconda_dir}

def _add_missing_inits(python_bin):
    """pip/setuptools strips __init__.py files with namespace declarations.
    I have no idea why, but this adds them back.
    """
    library_loc = subprocess.check_output("%s -c 'import pygraph.classes.graph; "
                                          "print pygraph.classes.graph.__file__'" % python_bin,
                                          shell=True)
    pygraph_init = os.path.normpath(os.path.join(os.path.dirname(library_loc.strip()), os.pardir,
                                                 "__init__.py"))
    if not os.path.exists(pygraph_init):
        with open(pygraph_init, "w") as out_handle:
            out_handle.write("__import__('pkg_resources').declare_namespace(__name__)\n")

def _cleanup_problem_files(venv_dir):
    """Remove problem bottle items in PATH which conflict with site-packages
    """
    for cmd in ["bottle.py", "bottle.pyc"]:
        bin_cmd = os.path.join(venv_dir, "bin", cmd)
        if os.path.exists(bin_cmd):
            os.remove(bin_cmd)

def install_tools(fab_cmd, fabfile, fabricrc):
    """Install 3rd party tools used by Gemini using a custom CloudBioLinux flavor.
    """
    tools = ["tabix", "grabix", "samtools", "bedtools"]
    flavor_dir = os.path.join(os.getcwd(), "gemini-flavor")
    if not os.path.exists(flavor_dir):
        os.makedirs(flavor_dir)
    with open(os.path.join(flavor_dir, "main.yaml"), "w") as out_handle:
        out_handle.write("packages:\n")
        out_handle.write("  - bio_nextgen\n")
        out_handle.write("libraries:\n")
    with open(os.path.join(flavor_dir, "custom.yaml"), "w") as out_handle:
        out_handle.write("bio_nextgen:\n")
        for tool in tools:
            out_handle.write("  - %s\n" % tool)
    cmd = [fab_cmd, "-f", fabfile, "-H", "localhost", "-c", fabricrc,
           "install_biolinux:target=custom,flavor=%s" % flavor_dir]
    subprocess.check_call(cmd)

def install_data(python_cmd, data_script, args):
    """Install biological data used by gemini.
    """
    data_dir = os.path.join(args.datadir, "gemini_data") if args.sharedpy else args.datadir
    cmd = [python_cmd, data_script, data_dir]
    if args.install_data:
        print "Installing gemini data..."
    else:
        cmd.append("--nodata")
    subprocess.check_call(cmd)

def install_testbase(datadir, repo, gemini):
    """Clone or update gemini code so we have the latest test suite.
    """
    gemini_dir = os.path.join(datadir, "gemini")
    cur_dir = os.getcwd()
    needs_git = True
    if os.path.exists(gemini_dir):
        os.chdir(gemini_dir)
        try:
            subprocess.check_call(["git", "pull", "origin", "master", "--tags"])
            needs_git = False
        except:
            os.chdir(cur_dir)
            shutil.rmtree(gemini_dir)
    if needs_git:
        os.chdir(os.path.split(gemini_dir)[0])
        subprocess.check_call(["git", "clone", repo])
    os.chdir(gemini_dir)
    _update_testdir_revision(gemini["cmd"])
    os.chdir(cur_dir)
    return os.path.join(gemini_dir, "master-test.sh")

def _update_testdir_revision(gemini_cmd):
    """Update test directory to be in sync with a tagged installed version or development.
    """
    try:
        p = subprocess.Popen([gemini_cmd, "--version"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        gversion = p.communicate()[0].split()[1]
    except:
        gversion = ""
    tag = ""
    if gversion:
        try:
            p = subprocess.Popen("git tag -l | grep %s" % gversion, stdout=subprocess.PIPE, shell=True)
            tag = p.communicate()[0].strip()
        except:
            tag = ""
    if tag:
        subprocess.check_call(["git", "checkout", "tags/%s" % tag])
        pass
    else:
        subprocess.check_call(["git", "reset", "--hard", "HEAD"])

def write_fabricrc(base_file, tooldir, datadir, distribution, use_sudo):
    out_file = os.path.join(os.getcwd(), os.path.basename(base_file))
    with open(base_file) as in_handle:
        with open(out_file, "w") as out_handle:
            for line in in_handle:
                if line.startswith("system_install"):
                    line = "system_install = %s\n" % tooldir
                elif line.startswith("local_install"):
                    line = "local_install = %s/install\n" % tooldir
                elif line.startswith("data_files"):
                    line = "data_files = %s\n" % datadir
                elif line.startswith("distribution"):
                    line = "distribution = %s\n" % distribution
                elif line.startswith("use_sudo"):
                    line = "use_sudo = %s\n" % use_sudo
                elif line.startswith("edition"):
                    line = "edition = minimal\n"
                elif line.startswith("#galaxy_home"):
                    line = "galaxy_home = %s\n" % os.path.join(datadir, "galaxy")
                out_handle.write(line)
    return out_file

def make_dirs(args):
    sudo_cmd = ["sudo"] if args.sudo else []
    for dname in [args.datadir, args.tooldir]:
        if not os.path.exists(dname):
            subprocess.check_call(sudo_cmd + ["mkdir", "-p", dname])
            username = subprocess.check_output("echo $USER", shell=True).strip()
            subprocess.check_call(sudo_cmd + ["chown", username, dname])

def get_cloudbiolinux(repo):
    base_dir = os.path.join(os.getcwd(), "cloudbiolinux")
    if not os.path.exists(base_dir):
        subprocess.check_call(["git", "clone", repo])
    return {"fabricrc": os.path.join(base_dir, "config", "fabricrc.txt"),
            "tool_fabfile": os.path.join(base_dir, "fabfile.py")}

def check_dependencies():
    """Ensure required tools for installation are present.
    """
    print "Checking required dependencies..."
    for cmd, url in [("git", "http://git-scm.com/"),
                     ("wget", "http://www.gnu.org/software/wget/"),
                     ("curl", "http://curl.haxx.se/")]:
        try:
            retcode = subprocess.call([cmd, "--version"], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        except OSError:
            retcode = 127
        if retcode == 127:
            raise OSError("gemini requires %s (%s)" % (cmd, url))
        else:
            print " %s found" % cmd

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Automated installer for gemini framework.")
    parser.add_argument("tooldir", help="Directory to install 3rd party software tools",
                        type=os.path.abspath)
    parser.add_argument("datadir", help="Directory to install gemini data files",
                        type=os.path.abspath)
    parser.add_argument("--nosudo", help="Specify we cannot use sudo for commands",
                        dest="sudo", action="store_false", default=True)
    parser.add_argument("--notools", help="Do not install tool dependencies",
                        dest="install_tools", action="store_false", default=True)
    parser.add_argument("--nodata", help="Do not install data dependencies",
                        dest="install_data", action="store_false", default=True)
    parser.add_argument("--sharedpy", help=("Indicate we share an Anaconda Python directory with "
                                            "another project. Creates unique gemini data directory."),
                        action="store_true", default=False)
    if len(sys.argv) == 1:
        parser.print_help()
    else:
        main(parser.parse_args())

########NEW FILE########
__FILENAME__ = severe_impact
import sys
import snpEff
import vep


def interpret_severe_impact(args, var):
    """
    Interpret the report from SnpEff or VEP to determine the impact of the variant.

    SnpEff examples:
    0    NON_SYNONYMOUS_CODING(MODERATE|MISSENSE|Aca/Gca|T/A|OR4F5|protein_coding|CODING|ENST00000335137|exon_1_69091_70008),
    1    NON_SYNONYMOUS_CODING(MODERATE|MISSENSE|Aca/Gca|T/A|OR4F5|protein_coding|CODING|ENST00000534990|exon_1_69037_69829)

    VEP examples
    CSQ: Consequence|Codons|Amino_acids|Gene|hgnc|Feature|EXON|polyphen|sift
    non_synonymous_codon|gaT/gaG|D/E|ENSG00000116254|CHD5|ENST00000378006|18/25|benign(0.011)|tolerated(0.3)
    nc_transcript_variant|||ENSG00000116254|CHD5|ENST00000491020|5/6||
    """

    effect_strings_str = ""
    effect_strings = impact_all = []
    impact_details = impact_features = None
    max_severity = 9  # initialize to a value greater than the largest value in impact info priority code
    counter = 0  # initialize counter for anno_id
    
    if args.anno_type == "snpEff":
        try:
            effect_strings_str = var.INFO["EFF"]
            effect_strings = effect_strings_str.split(",")
            
        except KeyError:
            if "SNPEFF_EFFECT" in var.INFO:
                impact_details = snpEff.gatk_effect_details(var.INFO)
            else:
                sys.stderr.write("WARNING: The input VCF has no snpEFF annotations. "
                                 "Variant impact will be set to unknown\n")

        for effect_string in effect_strings:
            counter += 1
            eff_pieces = snpEff.eff_search.findall(effect_string)
            for piece in eff_pieces:
                # the predicted inpact, which is outside the ()
                impact_string = piece[0]
                # all the other information, which is inside the ()  
                impact_detail = piece[1]
                    
                try:
                    impact_info = snpEff.effect_map[impact_string]
                    # update the impact stored if a higher or an equal severity transcript
                    # is encountered
                    if impact_info.priority_code <= max_severity:
                        impact_details = snpEff.EffectDetails(impact_string,
                                                          impact_info.priority,
                                                          impact_detail,
                                                          counter,
                                                          args.maj_version)
                        impact_all.append(impact_details)
                        # store the current "winning" severity for the next iteration
                        max_severity = impact_info.priority_code 
                        # This would store the highest priority for the next outer loop
                        top_severity =  impact_info.priority
                except KeyError:
                    pass
                    
        # prioritizing biotype: initialize flags to a high value
        set_flag = flag = 4
        for idx, impact in enumerate(impact_all):
            if impact.effect_severity == str(top_severity) and impact.biotype == "protein_coding":    
                set_flag = 0    
            elif impact.effect_severity == str(top_severity) and impact.biotype != "protein_coding":
                set_flag = 1
            if impact.effect_severity == str(top_severity) and set_flag < flag:
                flag = set_flag
                impact_features = impact
        return impact_features


    elif args.anno_type == "VEP":
        try:
            effect_strings_str = var.INFO["CSQ"]
            effect_strings = effect_strings_str.split(",")
        except KeyError:
            sys.stderr.write("WARNING: The input VCF has no VEP annotations. \
                             Variant impact will be set to unknown\n")

        for effect_string in effect_strings:
            # nc_transcript_variant&intron_variant|||ENSG00000243485|MIR1302-11|ENST00000
            each_string = effect_string.split("|")

            if "&" in each_string[0]:
                impact_strings = each_string[0].split("&")
                # impact_strings will be [nc_transcript_variant,
                # intron_variant]
                for impact_string in impact_strings:
                    counter += 1
                    try:
                        impact_info = vep.effect_map[impact_string]
                        # update the impact stored only if a higher severity
                        # transcript is encountered
                        if impact_info.priority_code <= max_severity:
                            impact_details = vep.EffectDetails(
                                impact_string, impact_info.priority, effect_string, counter)
                            impact_all.append(impact_details)
                            max_severity = impact_info.priority_code  # store the current "winning" severity for the next iteration.
                            top_severity =  impact_info.priority
                    except KeyError:
                        pass

            # we expect VEP to produce a valid impact label for each_string[0]
            elif "&" not in each_string[0]:
                counter += 1
                impact_string = each_string[0]
                impact_info = vep.effect_map.get(impact_string)
                if impact_info is None:
                    pass
                elif impact_info.priority_code <= max_severity:
                    impact_details = vep.EffectDetails(
                        impact_string, impact_info.priority, effect_string, counter)
                    impact_all.append(impact_details)
                    max_severity = impact_info.priority_code  # initialize the max_severity to the former value of priority code
                    top_severity =  impact_info.priority
        
        # prioritize biotype
        set_flag = flag = 4
        for idx, impact in enumerate(impact_all):
            if impact.effect_severity == str(top_severity) and impact.biotype == "protein_coding":    
                set_flag = 0    
            elif impact.effect_severity == str(top_severity) and impact.biotype != "protein_coding":
                set_flag = 1
            if impact.effect_severity == str(top_severity) and set_flag < flag:
                flag = set_flag
                impact_features = impact
        
    else:
        # should not get here, as the valid -t options should be handled
        # in main()
        sys.exit("ERROR: Unsupported variant annotation type.\n")
    
    return impact_features

########NEW FILE########
__FILENAME__ = snpEff
#!/usr/bin/env python
#############################################
# consequence as per snpeff v3:
# Effect ( Effect_Impact | Functional_Class | Codon_Change | Amino_Acid_change| Amino_Acid_length | Gene_Name | Gene_BioType | Coding | Transcript | Exon
# NON_SYNONYMOUS_CODING(MODERATE|MISSENSE|aCg/aTg|T143M|459|XKR3|protein_coding|CODING|ENST00000331428|exon_22_17280661_17280914

#############################################

import re
from collections import namedtuple
from collections import defaultdict


class EffectDetails(object):
    def __init__(self, name, severity, detail_string, counter, snp_eff_version):
        fields = detail_string.split("|")
        self.effect_name = name
        self.anno_id = counter
        self.effect_severity = severity
        self.impact = fields[1] if fields[1] != '' else None
        self.codon = fields[2] if fields[2] != '' else None
        self.aa_change = fields[3] if fields[3] != '' else None

        # snpEff >= v3.0 includes aa_length
        if snp_eff_version is not None and snp_eff_version >= 3:
            self.aa_length = fields[4] if fields[4] != '' else None
            self.gene = fields[5] if fields[5] != '' else None
            self.biotype = fields[6] if fields[6] != '' else None
            self.coding = fields[7] if fields[7] != '' else None
            self.transcript = fields[8] if fields[8] != '' else None
            self.exon = fields[9] if fields[9] != '' else None
            self.warnings = None
            if len(fields) > 9:
                self.warnings = fields[9]
        else:
            self.aa_length = None
            self.gene = fields[4] if fields[4] != '' else None
            self.biotype = fields[5] if fields[5] != '' else None
            self.coding = fields[6] if fields[6] != '' else None
            self.transcript = fields[7] if fields[7] != '' else None
            self.exon = fields[8] if fields[8] != '' else None
            self.warnings = None
            if len(fields) > 8:
                self.warnings = fields[8]

        # Handling only codon change and not distance(snpEff v3.3)
        if self.effect_name not in ("DOWNSTREAM", "UPSTREAM"):
            self.codon_change = self.codon
        else:
            self.codon_change =  None
        
        # rules for being exonic.
        # 1. must be protein_coding
        # 2. the impact must be in a list of impacts
        #    that are known to be exonic.
        if self.biotype == "protein_coding" and \
                self.effect_name in exonic_impacts:
            self.is_exonic = 1
        else:
            self.is_exonic = 0

        # rules for being coding.
        # 1. must be protein_coding
        # 2. must be exonic, yet must not be a UTR
        self.is_coding = 0
        if self.is_exonic and not (self.effect_name == "START_GAINED" or
                                   self.effect_name.startswith("UTR_")):
            self.is_coding = 1

        # rules for being loss-of-function (lof).
        # 1. must be protein_coding
        # 2. must be a coding variant with HIGH impact
        # 3. must affect a protein_coding transcript
        self.is_lof = 0
        if self.effect_severity == "HIGH" and self.biotype == "protein_coding":
            self.is_lof = 1

        self.polyphen_pred = None
        self.polyphen_score = None
        self.sift_pred = None
        self.sift_score = None
        self.consequence = effect_dict[self.effect_name] if self.effect_severity != None else self.effect_name
        self.so = effect_so[self.effect_name] if self.effect_severity != None else self.effect_name

    def __str__(self):
        return "\t".join([self.consequence, self.effect_severity,
                          str(self.impact), str(self.codon_change),
                          str(self.aa_change), str(self.aa_length), str(self.gene),
                          str(self.biotype), str(self.is_exonic),
                          str(self.is_coding), str(self.transcript),
                          str(self.exon), str(self.anno_id), str(self.so)])

    def __repr__(self):
        return self.__str__()


exonic_impacts = ["CODON_CHANGE",
                  "CODON_CHANGE_PLUS_CODON_DELETION",
                  "CODON_CHANGE_PLUS_CODON_INSERTION",
                  "CODON_DELETION",
                  "CODON_INSERTION",
                  "EXON",
                  "EXON_DELETED",
                  "FRAME_SHIFT",
                  "GENE",
                  "NON_SYNONYMOUS_CODING",
                  "RARE_AMINO_ACID",
                  "START_GAINED",
                  "START_LOST",
                  "STOP_GAINED",
                  "STOP_LOST",
                  "SYNONYMOUS_CODING",
                  "SYNONYMOUS_START",
                  "SYNONYMOUS_STOP",
                  "TRANSCRIPT",
                  "UTR_3_DELETED",
                  "UTR_3_PRIME",
                  "UTR_5_DELETED",
                  "UTR_5_PRIME",
                  "NON_SYNONYMOUS_START",
                  "CHROMOSOME_LARGE_DELETION"]


effect_names = ["CDS",
                "CODON_CHANGE",
                "CODON_CHANGE_PLUS_CODON_DELETION",
                "CODON_CHANGE_PLUS_CODON_INSERTION",
                "CODON_DELETION",
                "CODON_INSERTION",
                "DOWNSTREAM",
                "EXON",
                "EXON_DELETED",
                "FRAME_SHIFT",
                "GENE",
                "INTERGENIC",
                "INTERGENIC_CONSERVED",
                "INTRAGENIC",
                "INTRON",
                "INTRON_CONSERVED",
                "NON_SYNONYMOUS_CODING",
                "RARE_AMINO_ACID",
                "SPLICE_SITE_ACCEPTOR",
                "SPLICE_SITE_DONOR",
                "SPLICE_SITE_REGION",
                "START_GAINED",
                "START_LOST",
                "STOP_GAINED",
                "STOP_LOST",
                "SYNONYMOUS_CODING",
                "SYNONYMOUS_START",
                "SYNONYMOUS_STOP",
                "TRANSCRIPT",
                "UPSTREAM",
                "UTR_3_DELETED",
                "UTR_3_PRIME",
                "UTR_5_DELETED",
                "UTR_5_PRIME",
                "NON_SYNONYMOUS_START",
                "NONE",
                "CHROMOSOME_LARGE_DELETION"]
                
effect_so = defaultdict()
effect_so = {'CDS': 'coding_sequence_variant',
             'CODON_CHANGE': 'coding_sequence_variant',
             'CODON_CHANGE_PLUS_CODON_DELETION': 'disruptive_inframe_deletion',
             'CODON_CHANGE_PLUS_CODON_INSERTION': 'disruptive_inframe_insertion',
             'CODON_DELETION': 'inframe_deletion',
             'CODON_INSERTION': 'inframe_insertion',
             'DOWNSTREAM': 'downstream_gene_variant',
             'EXON': 'exon_variant',
             'EXON_DELETED': 'exon_loss_variant',
             'FRAME_SHIFT': 'frameshift_variant',
             'GENE': 'gene_variant',
             'INTERGENIC': 'intergenic_variant',
             'INTERGENIC_CONSERVED': 'conserved_intergenic_variant',
             'INTRAGENIC': 'intragenic_variant',
             'INTRON': 'intron_variant',
             'INTRON_CONSERVED': 'conserved_intron_variant',
             'NON_SYNONYMOUS_CODING': 'missense_variant',
             'RARE_AMINO_ACID': 'rare_amino_acid_variant',
             'SPLICE_SITE_ACCEPTOR': 'splice_acceptor_variant',
             'SPLICE_SITE_DONOR': 'splice_donor_variant',
             'SPLICE_SITE_REGION': 'splice_region_variant',
             'START_GAINED': '5_prime_UTR_premature_start_codon_gain_variant',
             'START_LOST': 'start_lost',
             'STOP_GAINED': 'stop_gained',
             'STOP_LOST': 'stop_lost',
             'SYNONYMOUS_CODING': 'synonymous_variant',
             'SYNONYMOUS_START': 'start_retained',
             'SYNONYMOUS_STOP': 'stop_retained_variant',
             'TRANSCRIPT': 'transcript_variant',
             'UPSTREAM': 'upstream_gene_variant',
             'UTR_3_DELETED': '3_prime_UTR_truncation_+_exon_loss_variant',
             'UTR_3_PRIME': '3_prime_UTR_variant',
             'UTR_5_DELETED': '5_prime_UTR_truncation_+_exon_loss_variant',
             'UTR_5_PRIME': '5_prime_UTR_variant',
             'NON_SYNONYMOUS_START': 'initiator_codon_variant',
             'NONE': 'None',
             'CHROMOSOME_LARGE_DELETION': 'chromosomal_deletion'}

effect_dict = defaultdict()
effect_dict = {'CDS': 'CDS',
               'CODON_CHANGE': 'inframe_codon_change',
               'CODON_CHANGE_PLUS_CODON_DELETION': 'codon_change_del',
               'CODON_CHANGE_PLUS_CODON_INSERTION': 'codon_change_ins',
               'CODON_DELETION': 'inframe_codon_loss',
               'CODON_INSERTION': 'inframe_codon_gain',
               'DOWNSTREAM': 'downstream',
               'EXON': 'exon',
               'EXON_DELETED': 'exon_deleted',
               'FRAME_SHIFT': 'frame_shift',
               'GENE': 'gene',
               'INTERGENIC': 'intergenic',
               'INTERGENIC_CONSERVED': 'intergenic_conserved',
               'INTRAGENIC': 'intragenic',
               'INTRON': 'intron',
               'INTRON_CONSERVED': 'intron_conserved',
               'NON_SYNONYMOUS_CODING': 'non_syn_coding',
               'RARE_AMINO_ACID': 'rare_amino_acid',
               'SPLICE_SITE_ACCEPTOR': 'splice_acceptor',
               'SPLICE_SITE_DONOR': 'splice_donor',
               'SPLICE_SITE_REGION': 'splice_region',
               'START_GAINED': 'start_gain',
               'START_LOST': 'start_loss',
               'STOP_GAINED': 'stop_gain',
               'STOP_LOST': 'stop_loss',
               'SYNONYMOUS_CODING': 'synonymous_coding',
               'SYNONYMOUS_START': 'synonymous_start',
               'SYNONYMOUS_STOP': 'synonymous_stop',
               'TRANSCRIPT': 'transcript',
               'UPSTREAM': 'upstream',
               'UTR_3_DELETED': 'UTR_3_del',
               'UTR_3_PRIME': 'UTR_3_prime',
               'UTR_5_DELETED': 'UTR_5_del',
               'UTR_5_PRIME': 'UTR_5_prime',
               'NON_SYNONYMOUS_START': 'non_synonymous_start',
               'NONE': 'None',
               'CHROMOSOME_LARGE_DELETION': 'chrom_large_del'}

effect_desc = ["The variant hits a CDS.",
               "One or many codons are changed",
               "One codon is changed and one or more codons are deleted",
               "One codon is changed and one or many codons are inserted",
               "One or many codons are deleted",
               "One or many codons are inserted",
               "Downstream of a gene (default length: 5K bases)",
               "The variant hits an exon.",
               "A deletion removes the whole exon.",
               "Insertion or deletion causes a frame shift",
               "The variant hits a gene.",
               "The variant is in an intergenic region.",
               "The variant is in a highly conserved intergenic region.",
               "The variant hits a gene, but no transcripts within \
               the gene.",
               "Variant hits an intron.",
               "The variant is in a highly conserved intronic region.",
               "Variant causes a codon that produces a different \
               amino acid.",
               "The variant hits a rare amino acid thus is likely to \
                produce protein loss of function",
               "The variant hits a splice acceptor site (defined as two \
               bases before exon start, except for the first exon).",
               "The variant hits a Splice donor site (defined as two \
               bases after coding exon end, except for the last exon).",
               "The variant lies within a splice site region (1-3bps into \
                an exon or 3-8bps into an intron)",
               "A variant in 5'UTR region produces a three base sequence \
                that can be a START codon.",
               "Variant causes start codon to be mutated into a non-start \
                codon.",
               "Variant causes a STOP codon.",
               "Variant causes stop codon to be mutated into a non-stop \
                codon.",
               "Variant causes a codon that produces the same amino acid",
               "Variant causes start codon to be mutated into another \
               start codon.",
               "Variant causes stop codon to be mutated into another stop \
                codon.",
               "The variant hits a transcript.",
               "Upstream of a gene (default length: 5K bases).",
               "The variant deletes and exon which is in the 3'UTR of the \
                transcript.",
               "Variant hits 3'UTR region.",
               "The variant deletes and exon which is in the 5'UTR of the \
                transcript.",
               "Variant hits 5'UTR region.",
               "The variant causes a start codon to be changed into a \
                different codon",
                "Unknown",
                "A large region of the chromosome deleted (over 1%)"]

effect_priorities = ["LOW",
                     "MED",
                     "MED",
                     "MED",
                     "MED",
                     "MED",
                     "LOW",
                     "LOW",
                     "HIGH",
                     "HIGH",
                     "LOW",
                     "LOW",
                     "LOW",
                     "LOW",
                     "LOW",
                     "LOW",
                     "MED",
                     "HIGH",
                     "HIGH",
                     "HIGH",
                     "MED",
                     "LOW",
                     "HIGH",
                     "HIGH",
                     "HIGH",
                     "LOW",
                     "LOW",
                     "LOW",
                     "LOW",
                     "LOW",
                     "MED",
                     "LOW",
                     "MED",
                     "LOW",
                     "HIGH",
                     "LOW",
                     "HIGH"]

effect_priority_codes = [3,
                         2,
                         2,
                         2,
                         2,
                         2,
                         3,
                         3,
                         1,
                         1,
                         3,
                         3,
                         3,
                         3,
                         3,
                         3,
                         2,
                         1,
                         1,
                         1,
                         2,
                         3,
                         1,
                         1,
                         1,
                         3,
                         3,
                         3,
                         3,
                         3,
                         2,
                         3,
                         2,
                         3,
                         1,
                         3,
                         1]

effect_ids = range(1, 38)
effect_map = {}
EffectInfo = namedtuple(
    'EffectInfo', ['id', 'priority', 'priority_code', 'desc'])

for i, effect_name in enumerate(effect_names):
    info = EffectInfo(effect_ids[i], effect_priorities[i],
                      effect_priority_codes[i], effect_desc[i])
    effect_map[effect_name] = info

eff_pattern = '(\S+)[(](\S+)[)]'
eff_search = re.compile(eff_pattern)


def gatk_effect_details(info):
    """Convert GATK prepared snpEff effect details into standard EffectDetails.
    """
    name = info.get("SNPEFF_EFFECT", None)
    if name is not None:
        effect = effect_map[name]
        detail_string = "|{impact}|{codon_change}|{aa_change}|{gene}|{biotype}|{coding}|{transcript}|{exon}".format(
            impact=info.get("SNPEFF_IMPACT", ""),
            codon_change=info.get("SNPEFF_CODON_CHANGE", ""),
            aa_change=info.get("SNPEFF_AMINO_ACID_CHANGE", ""),
            gene=info.get("SNPEFF_GENE_NAME", ""),
            biotype=info.get("SNPEFF_GENE_BIOTYPE", ""),
            coding="",
            transcript=info.get("SNPEFF_TRANSCRIPT", ""),
            exon=info.get("SNPEFF_EXON_ID", ""))
        return EffectDetails(name, effect.priority, detail_string, 0)

########NEW FILE########
__FILENAME__ = sql
# simpleSQL.py
#
# simple demo of using the parsing library to do simple-minded SQL parsing
# could be extended to include where clauses etc.
#
# Copyright (c) 2003, Paul McGuire
# Modified by Aaron Quinlan, 2012
#
from pyparsing import Literal, CaselessLiteral, Word, Upcase, delimitedList, Optional, \
    Combine, Group, alphas, nums, alphanums, ParseException, Forward, oneOf, quotedString, \
    ZeroOrMore, restOfLine, Keyword

# define SQL tokens
selectStmt = Forward()
selectToken = Keyword("select", caseless=True)
fromToken = Keyword("from", caseless=True)

# ARQ 2012-Feb-10: allow struct-like column names, e.,g. gt_types.sample1 (add + ".$")
ident = Word( alphas, alphanums + "_$" + ".$" ).setName("identifier")

columnName = Upcase( delimitedList( ident, ".", combine=True ) )
columnNameList = Group( delimitedList( columnName ) )
tableName = Upcase( delimitedList( ident, ".", combine=True ) )
tableNameList = Group( delimitedList( tableName ) )

whereExpression = Forward()
and_ = Keyword("and", caseless=True)
or_ = Keyword("or", caseless=True)
in_ = Keyword("in", caseless=True)
# ARQ 2012-Feb-10: add "like" as an operator
like_ = Keyword("like", caseless=True)

E = CaselessLiteral("E")
# ARQ 2012-Feb-10: add "like" as a binop
binop = oneOf("= != < > >= <= eq ne lt le gt ge like", caseless=True)
arithSign = Word("+-",exact=1)
realNum = Combine( Optional(arithSign) + ( Word( nums ) + "." + Optional( Word(nums) ) |
                                                         ( "." + Word(nums) ) ) +
            Optional( E + Optional(arithSign) + Word(nums) ) )
intNum = Combine( Optional(arithSign) + Word( nums ) +
            Optional( E + Optional("+") + Word(nums) ) )

columnRval = realNum | intNum | quotedString | columnName # need to add support for alg expressions
whereCondition = Group(
    ( columnName + binop + columnRval ) |
    ( columnName + in_ + "(" + delimitedList( columnRval ) + ")" ) |
    ( columnName + in_ + "(" + selectStmt + ")" ) |
    ( "(" + whereExpression + ")" )
    )
whereExpression << whereCondition + ZeroOrMore( ( and_ | or_ ) + whereExpression )

# define the grammar
selectStmt << ( selectToken +
                   ( '*' | columnNameList ).setResultsName( "columns" ) +
                   fromToken +
                   tableNameList.setResultsName( "tables" ) +
                   Optional( Group( CaselessLiteral("where") + whereExpression ), "" ).setResultsName("where") )

simpleSQL = selectStmt

# ARQ 2012-Feb-10: define SQL Lite comment format, and ignore them
sqlLiteComment = "#" + restOfLine
simpleSQL.ignore( sqlLiteComment )

def parse_sql(str):
    try:
        return simpleSQL.parseString( str )
    except ParseException, err:
        print " "*err.loc + "^\n" + err.msg
        print err


def test( str ):
    print str,"->"
    try:
        tokens = simpleSQL.parseString( str )
        print "tokens = ", tokens
        print "tokens.columns =", tokens.columns
        print "tokens.tables =", tokens.tables
        print "tokens.where =", tokens.where
    except ParseException, err:
        print " "*err.loc + "^\n" + err.msg
        print err
    print

# test( "SELECT * from XYZZY, ABC" )
# test( "select * from SYS.XYZZY" )
# test( "Select A from Sys.dual" )
# test( "Select A,B,C from Sys.dual" )
# test( "Select A, B, C from Sys.dual" )
# test( "Select A, B, C from Sys.dual, Table2 " )
# test( "Xelect A, B, C from Sys.dual" )
# test( "Select A, B, C frox Sys.dual" )
# test( "Select" )
# test( "Select &&& frox Sys.dual" )
# test( "Select A from Sys.dual where a in ('RED','GREEN','BLUE')" )
# test( "Select A from Sys.dual where a in ('RED','GREEN','BLUE') and b in (10,20,30)" )
# test( "Select A,b from table1,table2 where table1.id eq table2.id and chrom = 1 and (a>2 or b <1)" )
# 
# """
# Test output:
# >pythonw -u simpleSQL.py
# SELECT * from XYZZY, ABC ->
# tokens = ['select', '*', 'from', ['XYZZY', 'ABC']]
# tokens.columns = *
# tokens.tables = ['XYZZY', 'ABC']
# 
# select * from SYS.XYZZY ->
# tokens = ['select', '*', 'from', ['SYS.XYZZY']]
# tokens.columns = *
# tokens.tables = ['SYS.XYZZY']
# 
# Select A from Sys.dual ->
# tokens = ['select', ['A'], 'from', ['SYS.DUAL']]
# tokens.columns = ['A']
# tokens.tables = ['SYS.DUAL']
# 
# Select A,B,C from Sys.dual ->
# tokens = ['select', ['A', 'B', 'C'], 'from', ['SYS.DUAL']]
# tokens.columns = ['A', 'B', 'C']
# tokens.tables = ['SYS.DUAL']
# 
# Select A, B, C from Sys.dual ->
# tokens = ['select', ['A', 'B', 'C'], 'from', ['SYS.DUAL']]
# tokens.columns = ['A', 'B', 'C']
# tokens.tables = ['SYS.DUAL']
# 
# Select A, B, C from Sys.dual, Table2 ->
# tokens = ['select', ['A', 'B', 'C'], 'from', ['SYS.DUAL', 'TABLE2']]
# tokens.columns = ['A', 'B', 'C']
# tokens.tables = ['SYS.DUAL', 'TABLE2']
# 
# Xelect A, B, C from Sys.dual ->
# ^
# Expected 'select'
# Expected 'select' (0), (1,1)
# 
# Select A, B, C frox Sys.dual ->
# ^
# Expected 'from'
# Expected 'from' (15), (1,16)
# 
# Select ->
# ^
# Expected '*'
# Expected '*' (6), (1,7)
# 
# Select &&& frox Sys.dual ->
# ^
# Expected '*'
# Expected '*' (7), (1,8)
# 
# >Exit code: 0
# """
########NEW FILE########
__FILENAME__ = sql_extended
# select_parser.py
# Copyright 2010, Paul McGuire
#
# a simple SELECT statement parser, taken from SQLite's SELECT statement
# definition at http://www.sqlite.org/lang_select.html
#
from pyparsing import *

LPAR,RPAR,COMMA = map(Suppress,"(),")
select_stmt = Forward().setName("select statement")

# keywords
(UNION, ALL, AND, INTERSECT, EXCEPT, COLLATE, ASC, DESC, ON, USING, NATURAL, INNER, 
 CROSS, LEFT, OUTER, JOIN, AS, INDEXED, NOT, SELECT, DISTINCT, FROM, WHERE, GROUP, BY,
 HAVING, ORDER, BY, LIMIT, OFFSET) =  map(CaselessKeyword, """UNION, ALL, AND, INTERSECT, 
 EXCEPT, COLLATE, ASC, DESC, ON, USING, NATURAL, INNER, CROSS, LEFT, OUTER, JOIN, AS, INDEXED, NOT, SELECT, 
 DISTINCT, FROM, WHERE, GROUP, BY, HAVING, ORDER, BY, LIMIT, OFFSET""".replace(",","").split())
(CAST, ISNULL, NOTNULL, NULL, IS, BETWEEN, ELSE, END, CASE, WHEN, THEN, EXISTS,
 COLLATE, IN, LIKE, GLOB, REGEXP, MATCH, ESCAPE, CURRENT_TIME, CURRENT_DATE, 
 CURRENT_TIMESTAMP) = map(CaselessKeyword, """CAST, ISNULL, NOTNULL, NULL, IS, BETWEEN, ELSE, 
 END, CASE, WHEN, THEN, EXISTS, COLLATE, IN, LIKE, GLOB, REGEXP, MATCH, ESCAPE, 
 CURRENT_TIME, CURRENT_DATE, CURRENT_TIMESTAMP""".replace(",","").split())
keyword = MatchFirst((UNION, ALL, INTERSECT, EXCEPT, COLLATE, ASC, DESC, ON, USING, NATURAL, INNER, 
 CROSS, LEFT, OUTER, JOIN, AS, INDEXED, NOT, SELECT, DISTINCT, FROM, WHERE, GROUP, BY,
 HAVING, ORDER, BY, LIMIT, OFFSET, CAST, ISNULL, NOTNULL, NULL, IS, BETWEEN, ELSE, END, CASE, WHEN, THEN, EXISTS,
 COLLATE, IN, LIKE, GLOB, REGEXP, MATCH, ESCAPE, CURRENT_TIME, CURRENT_DATE, 
 CURRENT_TIMESTAMP))
 
identifier = ~keyword + Word(alphas, alphanums+"_")
collation_name = identifier.copy()
column_name = identifier.copy()
column_alias = identifier.copy()
table_name = identifier.copy()
table_alias = identifier.copy()
index_name = identifier.copy()
function_name = identifier.copy()
parameter_name = identifier.copy()
database_name = identifier.copy()

# expression
expr = Forward().setName("expression")

integer = Regex(r"[+-]?\d+")
numeric_literal = Regex(r"\d+(\.\d*)?([eE][+-]?\d+)?")
string_literal = QuotedString("'")
blob_literal = Combine(oneOf("x X") + "'" + Word(hexnums) + "'")
literal_value = ( numeric_literal | string_literal | blob_literal |
    NULL | CURRENT_TIME | CURRENT_DATE | CURRENT_TIMESTAMP )
bind_parameter = (
    Word("?",nums) |
    Combine(oneOf(": @ $") + parameter_name)
    )
type_name = oneOf("TEXT REAL INTEGER BLOB NULL")

expr_term = (
    CAST + LPAR + expr + AS + type_name + RPAR |
    EXISTS + LPAR + select_stmt + RPAR |
    function_name + LPAR + Optional(delimitedList(expr)) + RPAR |
    literal_value |
    bind_parameter |
    identifier
    )

UNARY,BINARY,TERNARY=1,2,3
expr << operatorPrecedence(expr_term,
    [
    (oneOf('- + ~') | NOT, UNARY, opAssoc.LEFT),
    ('||', BINARY, opAssoc.LEFT),
    (oneOf('* / %'), BINARY, opAssoc.LEFT),
    (oneOf('+ -'), BINARY, opAssoc.LEFT),
    (oneOf('<< >> & |'), BINARY, opAssoc.LEFT),
    (oneOf('< <= > >='), BINARY, opAssoc.LEFT),
    (oneOf('= == != <>') | IS | IN | LIKE | GLOB | MATCH | REGEXP, BINARY, opAssoc.LEFT),
    ('||', BINARY, opAssoc.LEFT),
    ((BETWEEN,AND), TERNARY, opAssoc.LEFT),
    ])

compound_operator = (UNION + Optional(ALL) | INTERSECT | EXCEPT)

ordering_term = expr + Optional(COLLATE + collation_name) + Optional(ASC | DESC)

join_constraint = Optional(ON + expr | USING + LPAR + Group(delimitedList(column_name)) + RPAR)

join_op = COMMA | (Optional(NATURAL) + Optional(INNER | CROSS | LEFT + OUTER | LEFT | OUTER) + JOIN)

join_source = Forward()
single_source = ( (Group(database_name("database") + "." + table_name("table")) | table_name("table")) + 
                    Optional(Optional(AS) + table_alias("table_alias")) +
                    Optional(INDEXED + BY + index_name("name") | NOT + INDEXED)("index") | 
                  (LPAR + select_stmt + RPAR + Optional(Optional(AS) + table_alias)) | 
                  (LPAR + join_source + RPAR) )

join_source << single_source + ZeroOrMore(join_op + single_source + join_constraint)

result_column = "*" | table_name + "." + "*" | (expr + Optional(Optional(AS) + column_alias))

# ARQ 2012-Feb-10: add "like" as a binop
whereExpression = Forward()
and_ = Keyword("and", caseless=True)
or_ = Keyword("or", caseless=True)
in_ = Keyword("in", caseless=True)
like_ = Keyword("like", caseless=True)
E = CaselessLiteral("E")
binop = oneOf("= != < > >= <= eq ne lt le gt ge like", caseless=True)
arithSign = Word("+-",exact=1)
realNum = Combine( Optional(arithSign) + ( Word( nums ) + "." + Optional( Word(nums) ) |
                                                         ( "." + Word(nums) ) ) +
            Optional( E + Optional(arithSign) + Word(nums) ) )
intNum = Combine( Optional(arithSign) + Word( nums ) +
            Optional( E + Optional("+") + Word(nums) ) )

columnRval = realNum | intNum | quotedString | result_column # need to add support for alg expressions
ident = Word( alphas, alphanums + "_$" + ".$" ).setName("identifier")
columnName = delimitedList( ident, ".", combine=True ).setParseAction( upcaseTokens )
fromToken = Keyword("from", caseless=True)
tableName = delimitedList( ident, ".", combine=True ).setParseAction( upcaseTokens )
tableNameList = Group( delimitedList( tableName ) )
whereCondition = Group(
    ( columnName + binop + columnRval ) |
    ( columnName + in_ + "(" + delimitedList( columnRval ) + ")" ) |
    ( columnName + in_ + "(" + select_stmt + ")" ) |
    ( "(" + whereExpression + ")" )
    )
whereExpression << whereCondition + ZeroOrMore( ( and_ | or_ ) + whereExpression )

# Combine the grammar rules to define how a SQL statement should be parsed.
select_core = (SELECT + Optional(DISTINCT | ALL) + 
                ( '*' | Group(delimitedList(columnName)))("select") +
                fromToken +
                tableNameList.setResultsName("tables") +
                Optional(WHERE + Group(delimitedList(whereExpression))("where")))

# handle any order by, group by, having, or limit clauses
select_stmt << (select_core + ZeroOrMore(compound_operator + select_core) +
                Optional(ORDER + BY + Group(delimitedList(ordering_term))("orderby")) +
                Optional(GROUP + BY + Group(delimitedList(ordering_term))("groupby")) +
                Optional(HAVING + expr("having")) +
                Optional(LIMIT + (integer + OFFSET + integer | integer + COMMA + integer)))

simpleSQL = select_stmt

# ARQ 2012-Feb-10: define SQL Lite comment format, and ignore them
sqlLiteComment = "#" + restOfLine
simpleSQL.ignore( sqlLiteComment )

def parse_sql(str):
    try:
        return simpleSQL.parseString( str )
    except ParseException, err:
        print " "*err.loc + "^\n" + err.msg
        print err


# tests = """\
#     select * from variants
#     select * from variants where ref = alt
#     select * from xyzzy where z > 100 order by zz, yy
#     select * from xyzzy where z > 100 and x < 100 order by zz
#     select distinct chrom, start from variants where x > 100 and y < 200 order by zz
#     select distinct chrom, start from variants where x > 100 and y < 200 order by yy group by zz,xx having ww > 10
#     select *, start from variants where x > 100 and y < 200 order by zz
#     select * from xyzzy""".splitlines()
# for t in tests:
#     print t
#     try:
#         print select_stmt.parseString(t).dump()
#     except ParseException, pe:
#         print pe.msg
#     print

########NEW FILE########
__FILENAME__ = sql_utils
"""
these are utilities to parse and transform SQL statements
"""

import re


def get_select_cols_and_rest(query):
    """
    Separate the a list of selected columns from
    the rest of the query

    Returns:
        1. a list of the selected columns
        2. a string of the rest of the query after the SELECT
    """
    from_loc = query.lower().find("from")

    raw_select_clause = query[0:from_loc].rstrip()
    rest_of_query = query[from_loc:len(query)]

    # remove the SELECT keyword from the query
    select_pattern = re.compile("select", re.IGNORECASE)
    raw_select_clause = select_pattern.sub('', raw_select_clause)

    # now create and iterate through a list of of the SELECT'ed columns
    selected_columns = raw_select_clause.split(',')
    selected_columns = [c.strip() for c in selected_columns]

    return selected_columns, rest_of_query


def ensure_columns(query, cols):
    """
    if a query is missing any of these list of columns, add them
    and return the new query string
    """
    sel_cols, rest = get_select_cols_and_rest(query)
    sel_cols = [x.lower() for x in sel_cols]
    for c in cols:
        c = c.lower()
        if c not in sel_cols:
            sel_cols += [c]

    sel_string = ", ".join(sel_cols)
    return "select {sel_string} {rest}".format(**locals())

########NEW FILE########
__FILENAME__ = stats
#!/usr/bin/env python

"""
Adapted from Gary Strangman's stats.py package: 
http://www.nmr.mgh.harvard.edu/Neural_Systems_Group/gary/python/stats.py
"""
import math

def zprob(z):
    """
    Returns the area under the normal curve 'to the left of' the given z value.
    Thus, 
        for z<0, zprob(z) = 1-tail probability
        for z>0, 1.0-zprob(z) = 1-tail probability
        for any z, 2.0*(1.0-zprob(abs(z))) = 2-tail probability
    Adapted from z.c in Gary Perlman's |Stat.

    Usage:   lzprob(z)
    """
    Z_MAX = 6.0    # maximum meaningful z-value
    if z == 0.0:
        x = 0.0
    else:
        y = 0.5 * math.fabs(z)
        if y >= (Z_MAX*0.5):
            x = 1.0
        elif (y < 1.0):
            w = y*y
            x = ((((((((0.000124818987 * w
                        -0.001075204047) * w +0.005198775019) * w
                      -0.019198292004) * w +0.059054035642) * w
                    -0.151968751364) * w +0.319152932694) * w
                  -0.531923007300) * w +0.797884560593) * y * 2.0
        else:
            y = y - 2.0
            x = (((((((((((((-0.000045255659 * y
                             +0.000152529290) * y -0.000019538132) * y
                           -0.000676904986) * y +0.001390604284) * y
                         -0.000794620820) * y -0.002034254874) * y
                       +0.006549791214) * y -0.010557625006) * y
                     +0.011630447319) * y -0.009279453341) * y
                   +0.005353579108) * y -0.002141268741) * y
                 +0.000535310849) * y +0.999936657524
    if z > 0.0:
        prob = ((x+1.0)*0.5)
    else:
        prob = ((1.0-x)*0.5)
    return prob
    
    
def lchisqprob(chisq,df):
    """
    Returns the (1-tailed) probability value associated with the provided
    chi-square value and df.  Adapted from chisq.c in Gary Perlman's |Stat.

    Usage:   lchisqprob(chisq,df)
    """
    BIG = 20.0
    def ex(x):
        BIG = 20.0
        if x < -BIG:
            return 0.0
        else:
            return math.exp(x)

    if chisq <=0 or df < 1:
        return 1.0
    a = 0.5 * chisq
    if df%2 == 0:
        even = 1
    else:
        even = 0
    if df > 1:
        y = ex(-a)
    if even:
        s = y
    else:
        s = 2.0 * zprob(-math.sqrt(chisq))
    if (df > 2):
        chisq = 0.5 * (df - 1.0)
        if even:
            z = 1.0
        else:
            z = 0.5
        if a > BIG:
            if even:
                e = 0.0
            else:
                e = math.log(math.sqrt(math.pi))
            c = math.log(a)
            while (z <= chisq):
                e = math.log(z) + e
                s = s + ex(c*z-a-e)
                z = z + 1.0
            return s
        else:
            if even:
                e = 1.0
            else:
                e = 1.0 / math.sqrt(math.pi) / math.sqrt(a)
            c = 0.0
            while (z <= chisq):
                e = e * (a/float(z))
                c = c + e
                z = z + 1.0
            return (c*y+s)
    else:
        return s
########NEW FILE########
__FILENAME__ = tool_autosomal_dominant
#!/usr/bin/env python
import os
import sys
from gemini_inheritance_model_utils import GeminiInheritanceModelFactory

def run(parser, args):
    if os.path.exists(args.db):
        auto_dominant_factory = \
            GeminiInheritanceModelFactory(args, model="auto_dom")
        auto_dominant_factory.get_candidates()


########NEW FILE########
__FILENAME__ = tool_autosomal_recessive
#!/usr/bin/env python
import os
import sys
from gemini_inheritance_model_utils import GeminiInheritanceModelFactory

def run(parser, args):
    if os.path.exists(args.db):
        auto_recessive_factory = \
            GeminiInheritanceModelFactory(args, model="auto_rec")
        auto_recessive_factory.get_candidates()

########NEW FILE########
__FILENAME__ = tool_burden_tests
import math
from collections import Counter, defaultdict
import numpy as np
from scipy.stats import binom, norm, chi2
from pandas import DataFrame
import sys
import random
from math import pow
from itertools import ifilterfalse, islice
from scipy.misc import comb


import GeminiQuery


def burden_by_gene(args):
    """
    calculates per sample the total genetic burden for each gene
    """
    query = ("SELECT gene from variants WHERE "
             "is_coding=1 and (impact_severity = 'HIGH' or "
             "polyphen_pred = 'probably_damaging')")
    _summarize_by_gene_and_sample(args, query)


def nonsynonymous_by_gene(args):
    """
    calculates per sample the total genetic burden for each gene
    """
    query = ("SELECT variant_id, gene from variants WHERE "
             "codon_change != 'None'")
    _summarize_by_gene_and_sample(args, query)

def get_calpha(args):
    """
    Calculate the C-alpha statistic for each gene based on the observed
    counts of variants in cases and controls.

    From Neale et al, PLoS Genetics, 2011.
    http://www.plosgenetics.org/article/info%3Adoi%2F10.1371%2Fjournal.pgen.1001322
    """
    db = args.db
    if not (args.controls and args.cases):
        case, control = _get_case_and_control_samples(args)
    else:
        case = args.cases
        control = args.controls
    assert (case and control), ("Phenotypes not found in the database and "
                                "--cases and --controls are not set.")

    samples = control + case
    # p_0 = the fraction of samples that are cases (used for weighting)
    p_0 = float(len(case)) / float(len(samples))

    if args.nonsynonymous:
        ns = _nonsynonymous_variants(args)
    else:
        ns = _medium_or_high_impact_variants(args)

    variants_in_gene, variants = _calculate_counts(ns, samples)
    header = ["gene", "T", "c", "Z", "p_value"]
    print "\t".join(header)

    if args.permutations > 0:
        perms = permute_cases(samples, args.permutations, case)

    for gene in variants_in_gene:
        vig = variants_in_gene[gene]

        # m = the number of variants observed for this gene
        m = len(vig.keys())

        # m_n is the number of variants with n copies (i.e., samples with the variant)
        #m_n = Counter([len(x) for x in vig.values()])

        # n_i is a list reflecting the total number of samples
        # having each variant
        n_i = [len(x) for x in vig.values()]

        # y_i is a list reflecting the total number of __cases__
        # having each variant
        y_i = [len(filter(lambda a: a in case, x)) for x in vig.values()]

        # "The C-alpha test statistic T contrasts the variance of each observed
        # count with the expected variance, assuming the binomial distribution."
        # In other words, given that we have n total samples and p_0 * n of them
        # are cases, we _expect_ the variant copies to be distributed among the
        # samples following a binomal distribution.  The T statistic contrasts
        # the observed count distributions with the expected:
        #
        # T = SUM{i=(1,m)} [(y_i - n_i*p_0)^2 - n_i*p_0(1 - p_0)]
        #
        T = _calculate_T(m, p_0, n_i, y_i)

        # Calculate the variance of T in order to normalize it
        c = _calculate_c(n_i, p_0)

        # The final test statistic, Z, id just the original test statistic divided
        # by its standard deviation. "We reject the null when Z is larger than expected
        # using a one-tailed standard normal distribution for reference.
        if c == 0:
            Z = np.NaN
            p_value = np.NaN
            print "\t".join([gene, str(T), str(c), str(Z), str(p_value)])
            continue
        else:
            Z = T / math.sqrt(c)

        if args.permutations == 0:
            # sf is the survival function ... same as 1 - CDF.
            p_value = norm.sf(Z)
        else:
            # this permutes the cases without replacement, important for
            # calculating an exact p-value
            T_scores = []
            for perm_case in perms:
                y_i = [len(filter(lambda a: a in perm_case, x)) for x in vig.values()]
                T_permuted = _calculate_T(m, p_0, n_i, y_i)
                T_scores.append(T_permuted)
            if args.save_tscores:
                with open("permutated_t_scores.txt", "a") as out_handle:
                    out_handle.write("\t".join([gene] + map(str, T_scores)) + "\n")
            false_hits = sum([x >= T for x in T_scores])
            # the + 1 to make it an unbiased estimator
            # Permutation P-values Should Never Be Zero: Calculating Exact
            # P-values When Permutations Are Randomly Drawn
            # http://www.degruyter.com/view/j/sagmb.2010.9.1/sagmb.2010.9.1.1585/sagmb.2010.9.1.1585.xml
            p_value = (float(false_hits) + 1) / (float(args.permutations + 1))

        print "\t".join([gene, str(T), str(c), str(Z), str(p_value)])


def permute_cases(samples, permutations, case):
    max_permutations = comb(len(samples), len(case))
    if permutations > max_permutations:
        sys.stderr.write("Permutations set to greater than the maximum number of "
                         "unique permutations of cases labels. Setting it to "
                         "%d\n." % (max_permutations))
        permutations = max_permutations

    perms = take(permutations, unique_permutations(samples, len(case)))
    return perms

def unique_permutations(iterable, length):
    """
    returns random permutations from an iterable without repeating a set
    take(unique_permutations([1,2,3,4,5], 2), 3) => [3,4], [1,6], [3,5]
    """
    seen = set()
    while True:
        element = tuple(sorted(random.sample(iterable, length)))
        if element not in seen:
            seen.add(element)
            yield list(element)

def take(n, iterable):
    "Return first n items of the iterable as a list"
    return list(islice(iterable, n))


def _get_case_and_control_samples(args):
    query = ("SELECT * from samples")
    gq = GeminiQuery.GeminiQuery(args.db)
    gq.run(query)
    cases = []
    controls = []
    for row in gq:
        if int(row["phenotype"]) == 1:
            controls.append(row["name"])
        elif int(row["phenotype"]) == 2:
            cases.append(row["name"])
    return cases, controls


def _calculate_c(n_i, p_0):
    c = 0.0
    singleton_n = 0
    for n in n_i:
        if n < 2:
            singleton_n += n
            continue
        for u in xrange(n + 1):
            c += _C_term(u, n, p_0)
    if singleton_n >= 2:
        for u in xrange(singleton_n + 1):
            c += _C_term(u, singleton_n, p_0)
    return c


def _C_term(u, n, p_0):
    p_obs_u = binom(n, p_0).pmf(u)
    return ((u - n * p_0)**2 - n * p_0 * (1 - p_0))**2 * p_obs_u


def _calculate_T(m, p_0, n_i, y_i):
    T = 0.0
    singleton_n = 0
    singleton_y = 0
    for n, y in zip(n_i, y_i):
        if n < 2:
            singleton_n += n
            singleton_y += y
            continue
        T += _variant_T_term(p_0, n, y)
    if singleton_n >= 2:
        T += _variant_T_term(p_0, singleton_n, singleton_y)
    return T

def _variant_T_term(p_0, n_i, y_i):
    return (y_i - n_i * p_0)**2 - n_i * p_0 * (1 - p_0)


def _nonsynonymous_variants(db):
    query = ("SELECT variant_id, gene from variants WHERE "
             "codon_change != 'None'")
    gq = GeminiQuery.GeminiQuery(db)
    gq.run(query, show_variant_samples=True)
    return gq

def _medium_or_high_impact_variants(args):
    query = ("SELECT variant_id, gene from variants"
             " WHERE impact_severity != 'LOW'"
             " AND aaf >= %s"
             " AND aaf <= %s" % (str(args.min_aaf), str(args.max_aaf)))

    gq = GeminiQuery.GeminiQuery(args.db)
    gq.run(query, show_variant_samples=True)
    return gq

def _calculate_counts(gq, samples):
    variants = defaultdict(Counter)
    variants_in_gene = defaultdict(defaultdict)
    for row in gq:
        gene_name = row['gene']
        samples_with_variant = [x for x in row["variant_samples"] if
                                x in samples]
        if not gene_name or not samples_with_variant:
            continue
        variants_in_gene[gene_name].update({row['variant_id']:
                                            samples_with_variant})
        new_counts = Counter(samples_with_variant)
        del new_counts['']
        variants[gene_name] += new_counts
    return variants_in_gene, variants


def _summarize_by_gene_and_sample(args, query):
    gq = GeminiQuery.GeminiQuery(args.db)
    gq.run(query, show_variant_samples=True)
    burden = defaultdict(Counter)
    for row in gq:
        gene_name = row['gene']
        if not gene_name:
            continue
        new_counts = Counter(row["HET_samples"])
        # Counter can't do scalar multiplication
        new_counts = new_counts + Counter(row["HOM_ALT_samples"])
        new_counts = new_counts + Counter(row["HOM_ALT_samples"])

        del new_counts['']
        burden[gene_name] += new_counts

    df = DataFrame({})
    for gene_name, counts in burden.items():
        df = df.append(DataFrame(counts, columns=counts.keys(),
                                 index=[gene_name]))
    df = df.replace(np.NaN, 0)
    df.to_csv(sys.stdout, float_format="%d", sep="\t", index_label='gene')


def burden(parser, args):
    if args.nonsynonymous and not args.calpha:
        nonsynonymous_by_gene(args)
    elif args.calpha:
        get_calpha(args)
    else:
        burden_by_gene(args)


# unit tests of the underlying calculations
def _test_calculate_C():
    nn = [4, 10, 5]
    yy = [2, 8, 0]
    correct = 15.250000000000007
    calc  = _calculate_c(nn, 0.5)
    assert correct == calc

def _test_calculate_T():
    nn = [4, 10, 5]
    yy = [2, 8, 0]
    correct = 10.5

    calc = sum([_variant_T_term(0.5, n, y) for n, y in zip(nn, yy)])
    assert correct == calc

########NEW FILE########
__FILENAME__ = tool_compound_hets
#!/usr/bin/env python
import sqlite3
import os
import sys
import collections
import re
from copy import copy

import GeminiQuery
import gemini_utils as util
from gemini_constants import *
import gemini_subjects as subjects

class Site(object):
    def __init__(self, row):
        self.row = row
        self.phased = None
        self.gt = None

    def __eq__(self, other):
        return self.row['chrom'] == other.row['chrom'] and \
               self.row['start'] == other.row['start']

    def __repr__(self):
        return ",".join([self.row['chrom'], 
                         str(self.row['start']), 
                         str(self.row['end'])])




def _add_necessary_columns(args, custom_columns):
    """
    Convenience function to tack on columns that are necessary for
    the functionality of the tool but yet have not been specifically
    requested by the user.
    """
    # we need to add the variant's chrom, start and gene if 
    # not already there.
    if custom_columns.find("gene") < 0:
        custom_columns += ", gene"
    if custom_columns.find("start") < 0:
        custom_columns += ", start"
        
    return custom_columns

def get_compound_hets(args):
    """
    Report candidate compound heterozygous mutations.
    """
    gq = GeminiQuery.GeminiQuery(args.db, include_gt_cols=True)
    idx_to_sample = gq.idx_to_sample
    subjects_dict = subjects.get_subjects(args)
    
    if args.columns is not None:
        custom_columns = _add_necessary_columns(args, str(args.columns))        
        query = "SELECT " + custom_columns + \
                " FROM variants " + \
                " WHERE (is_exonic = 1 or impact_severity != 'LOW') "
    else:
        # report the kitchen sink
        query = "SELECT *" + \
                ", gts, gt_types, gt_phases, gt_depths, \
                gt_ref_depths, gt_alt_depths, gt_quals" + \
                " FROM variants " + \
                " WHERE (is_exonic = 1 or impact_severity != 'LOW') "

    # add any non-genotype column limits to the where clause
    if args.filter:
        query += " AND " + args.filter

    # run the query applying any genotype filters provided by the user.
    gq.run(query)

    comp_hets = collections.defaultdict(lambda: collections.defaultdict(list))

    for row in gq:
        gt_types = row['gt_types']
        gts = row['gts']
        gt_bases = row['gts']
        gt_phases = row['gt_phases']
        
        site = Site(row)

        # track each sample that is heteroyzgous at this site.
        for idx, gt_type in enumerate(gt_types):
            if gt_type == HET:
                sample = idx_to_sample[idx]
                
                if args.only_affected and not subjects_dict[sample].affected:
                    continue

                # sample = "NA19002"
                sample_site = copy(site)
                sample_site.phased = gt_phases[idx]

                # require phased genotypes
                if not sample_site.phased and not args.ignore_phasing:
                    continue

                sample_site.gt = gt_bases[idx]
                # add the site to the list of candidates
                # for this sample/gene
                comp_hets[sample][site.row['gene']].append(sample_site)

    # header
    print "family\tsample\tcomp_het_id\t" + str(gq.header)

    # step 2.  now, cull the list of candidate heterozygotes for each
    # gene/sample to those het pairs where the alternate alleles
    # were inherited on opposite haplotypes.    
    comp_het_id = 1
    for sample in comp_hets:
        for gene in comp_hets[sample]:

            # we only care about combinations, not permutations
            # (e.g. only need site1,site2, not site1,site2 _and site2,site1)
            # thus we can do this in a ~ linear pass instead of a ~ N^2 pass
            for idx, site1 in enumerate(comp_hets[sample][gene]):
                for site2 in comp_hets[sample][gene][idx + 1:]:

                    # expand the genotypes for this sample
                    # at each site into it's composite
                    # alleles.  e.g. A|G -> ['A', 'G']
                    alleles_site1 = []
                    alleles_site2 = []
                    if not args.ignore_phasing:
                        alleles_site1 = site1.gt.split('|')
                        alleles_site2 = site2.gt.split('|')
                    else:
                        # split on phased (|) or unphased (/) genotypes
                        alleles_site1 = re.split('\||/', site1.gt)
                        alleles_site2 = re.split('\||/', site2.gt)
                    
                    # it is only a true compound heterozygote IFF
                    # the alternates are on opposite haplotypes.
                    if not args.ignore_phasing:
                        # return the haplotype on which the alternate
                        # allele was observed for this sample at each
                        # candidate het. site.
                        # e.g., if ALT=G and alleles_site1=['A', 'G']
                        # then alt_hap_1 = 1.  if ALT=A, then alt_hap_1 = 0
                        if "," in str(site1.row['alt']) or \
                           "," in str(site2.row['alt']):
                            sys.stderr.write("WARNING: Skipping candidate for sample"
                                             " %s b/c variants with mult. alt."
                                             " alleles are not yet supported. The sites are:"
                                             " %s and %s.\n" % (sample, site1, site2))
                            continue

                        alt_hap_1 = alleles_site1.index(site1.row['alt'])
                        alt_hap_2 = alleles_site2.index(site2.row['alt'])

                    # report if 
                    #   1. phasing is considered AND the alt alleles are on
                    #      different haplotypes
                    # OR
                    #   2. the user doesn't care about phasing.
                    if (not args.ignore_phasing and alt_hap_1 != alt_hap_2) \
                        or args.ignore_phasing:
                            print \
                               "\t".join([str(subjects_dict[sample].family_id), 
                                          sample,
                                         str(comp_het_id),
                                         str(site1.row)])
                            print \
                               "\t".join([str(subjects_dict[sample].family_id), 
                                          sample,
                                          str(comp_het_id),
                                          str(site2.row)])

                    comp_het_id += 1

def run(parser, args):
    if os.path.exists(args.db):
        get_compound_hets(args)
        

########NEW FILE########
__FILENAME__ = tool_de_novo_mutations
#!/usr/bin/env python
import os
import sys
from gemini_inheritance_model_utils import GeminiInheritanceModelFactory


def run(parser, args):
    if os.path.exists(args.db):
        de_novo_factory = \
            GeminiInheritanceModelFactory(args, model="de_novo")
        de_novo_factory.get_candidates()


########NEW FILE########
__FILENAME__ = tool_interactions
#!/usr/bin/env python

###########################################################################################################################################
#1. user defined root(may or may not be a mutated gene for the sample); get all interacting partners from the variant list of the sample
#2. For a root fixed to a lof gene in each sample, get the interacting partners
#3. a network graph of mutated genes for each sample, a subnetwork for any gene in that list (not executed here)
###########################################################################################################################################

import os
import sys
import sqlite3
import numpy as np
import zlib
import cPickle
from gemini.config import read_gemini_config
from pygraph.classes.graph import graph
from pygraph.readwrite.dot import write
from pygraph.algorithms.searching import breadth_first_search
from pygraph.algorithms.minmax import shortest_path
from pygraph.classes.exceptions import AdditionError
from pygraph.algorithms.filters.radius import radius
from pygraph.classes.digraph import digraph
import gemini_utils as util
from gemini_constants import *
from collections import defaultdict

def get_variant_genes(c, args, idx_to_sample):
    samples = defaultdict(list)
    for r in c:
        gt_types = np.array(cPickle.loads(zlib.decompress(r['gt_types'])))
        gts      = np.array(cPickle.loads(zlib.decompress(r['gts'])))
        var_id = str(r['variant_id'])
        chrom = str(r['chrom'])
        start = str(r['start'])
        end = str(r['end'])
        gene     = str(r['gene'])
        impact = str(r['impact'])
        biotype = str(r['biotype'])
        in_dbsnp = str(r['in_dbsnp'])
        clinvar_sig = str(r['clinvar_sig'])
        clinvar_disease_name = str(r['clinvar_disease_name'])
        aaf_1kg_all = str(r['aaf_1kg_all'])
        aaf_esp_all = str(r['aaf_esp_all'])

        for idx, gt_type in enumerate(gt_types):
            if (gt_type == HET or gt_type == HOM_ALT):
                if gene != "None":
                    (key, value) = (idx_to_sample[idx], \
                                   (gene,var_id,chrom,start,end,impact, \
                                   biotype,in_dbsnp,clinvar_sig, \
                                   clinvar_disease_name,aaf_1kg_all, \
                                   aaf_esp_all))
                    samples[idx_to_sample[idx]].append(value)
    return samples

def get_lof_genes(c, args, idx_to_sample):
    lof = defaultdict(list)
    for r in c:
        gt_types = np.array(cPickle.loads(zlib.decompress(r['gt_types'])))
        gts      = np.array(cPickle.loads(zlib.decompress(r['gts'])))
        gene     = str(r['gene'])

        for idx, gt_type in enumerate(gt_types):
            if (gt_type == HET or gt_type == HOM_ALT):
                if gene != "None":
                    (key, value) = (idx_to_sample[idx], gene)
                    lof[idx_to_sample[idx]].append(gene)
    return lof

def sample_gene_interactions(c, args, idx_to_sample):
    out = open("file.dot", 'w')
    #fetch variant gene dict for all samples
    samples = get_variant_genes(c, args, idx_to_sample)
    #file handle for fetching the hprd graph
    config = read_gemini_config()
    path_dirname = config["annotation_dir"]
    file_graph = os.path.join(path_dirname, 'hprd_interaction_graph')
    #load the graph using cPickle and close file handle
    gr = graph()
    f = open(file_graph, 'rb')
    gr = cPickle.load(f)
    f.close()
    k = []
    variants = []
    #calculate nodes from the graph
    hprd_genes = gr.nodes()
    if args.gene == None or args.gene not in hprd_genes:
        sys.stderr.write("Gene name not found or")
        sys.stderr.write(" gene not in p-p interaction file\n")

    elif args.gene in hprd_genes:
        x, y = \
            breadth_first_search(gr,root=args.gene,filter=radius(args.radius))
        gst = digraph()
        gst.add_spanning_tree(x)
        dot = write(gst)
        out.write(dot)
        st, sd = shortest_path(gst, args.gene)

        if args.var_mode:
            for sample in samples.iterkeys():
                var = samples[str(sample)]
                #for each level return interacting genes if they are
                # variants in the sample.
                # 0th order would be returned if the user chosen
                # gene is a variant in the sample
                for x in range(0, (args.radius+1)):
                    for each in var:
                        for key, value in sd.iteritems():
                            if value == x and key == each[0]:
                                print "\t".join([str(sample),str(args.gene), \
                                          str(x), \
                                          str(key), \
                                          str(each[1]), \
                                          str(each[2]), \
                                          str(each[3]), \
                                          str(each[4]), \
                                          str(each[5]), \
                                          str(each[6]), \
                                          str(each[7]), \
                                          str(each[8]), \
                                          str(each[9]), \
                                          str(each[10]), \
                                          str(each[11])])
        elif (not args.var_mode):
            for sample in samples.iterkeys():
                for each in samples[str(sample)]:
                    variants.append(each[0])
                for x in range(0, (args.radius+1)):
                    for key, value in sd.iteritems():
                        if value == x and key in set(variants):
                            k.append(key)
                    if k:
                        print "\t".join([str(sample), str(args.gene), \
                                 str(x)+"_order:",
                                 ",".join(k)])
                    else:
                        print "\t".join([str(sample), str(args.gene), \
                                         str(x)+"_order:", "none"])
                    #initialize keys for next iteration
                    k = []
                #initialize variants list for next iteration
                variants = []


def sample_lof_interactions(c, args, idx_to_sample, samples):
    lof = get_lof_genes(c, args, idx_to_sample)
    #file handle for fetching the hprd graph
    config = read_gemini_config()
    path_dirname = config["annotation_dir"]
    file_graph = os.path.join(path_dirname, 'hprd_interaction_graph')
    #load the graph using cPickle and close file handle
    gr = graph()
    f = open(file_graph, 'rb')
    gr = cPickle.load(f)
    f.close()
    #calculate nodes from the graph
    hprd_genes = gr.nodes()
    #initialize keys
    k = []
    variants = []

    if (not args.var_mode):
        for sample in lof.iterkeys():
            lofvariants = list(set(lof[str(sample)]))
            for each in samples[str(sample)]:
                variants.append(each[0])
            for gene in lofvariants:
                if gene in hprd_genes:
                    x, y = \
                        breadth_first_search(gr,root=gene,\
                        filter=radius(args.radius))

                    gst = digraph()
                    gst.add_spanning_tree(x)
                    st, sd = shortest_path(gst, gene)
                    # for each level return interacting genes
                    # if they are variants in the sample.
                    for rad in range(1, (args.radius+1)):
                        for key, value in sd.iteritems():
                            if (value == rad) and key in set(variants):
                                k.append(key)
                        if k:
                            print "\t".join([str(sample), \
                                       str(gene), \
                                       str(rad)+"_order:",
                                       ",".join(k)])
                        else:
                            print "\t".join([str(sample), \
                                       str(gene), \
                                       str(rad)+"_order:", \
                                       "none"])
                        #initialize k
                        k = []
            #initialize variants list for next iteration
            variants = []
    elif args.var_mode:
        for sample in lof.iterkeys():
            lofvariants = list(set(lof[str(sample)]))
            var = samples[str(sample)]
            for gene in lofvariants:
                if gene in hprd_genes:
                    x, y = \
                         breadth_first_search(gr,root=gene, \
                         filter=radius(args.radius))
                    gst = digraph()
                    gst.add_spanning_tree(x)
                    st, sd = shortest_path(gst, gene)
                    for rad in range(1, (args.radius+1)):
                        for each in var:
                            for key, value in sd.iteritems():
                                if value == rad and key == each[0]:
                                    print "\t".join([str(sample), \
                                               str(gene), \
                                               str(rad), \
                                               str(key), \
                                               str(each[1]), \
                                               str(each[2]), \
                                               str(each[3]), \
                                               str(each[4]), \
                                               str(each[5]), \
                                               str(each[6]), \
                                               str(each[7]), \
                                               str(each[8]), \
                                               str(each[9]), \
                                               str(each[10]), \
                                               str(each[11])])


def sample_variants(c, args):
    idx_to_sample = util.map_indices_to_samples(c)
    query = "SELECT variant_id, gt_types, gts, gene, impact, biotype, \
                    in_dbsnp, clinvar_sig, clinvar_disease_name, aaf_1kg_all, aaf_esp_all, chrom, \
                    start, end  \
             FROM variants"
    c.execute(query)

    if args.command == 'interactions':
        #header
        if args.var_mode:
            print "\t".join(['sample','gene','order_of_interaction', \
                             'interacting_gene', 'var_id', 'chrom', 'start', \
                             'end', 'impact', 'biotype', 'in_dbsnp', \
                             'clinvar_sig', 'clinvar_disease_name', 'aaf_1kg_all', \
                             'aaf_esp_all'])

        if (not args.var_mode):
            print "\t".join(['sample','gene','order_of_interaction', \
                     'interacting_gene'])
        sample_gene_interactions(c, args, idx_to_sample)

    elif args.command == 'lof_interactions':
        samples = get_variant_genes(c, args, idx_to_sample)
        return samples


def sample_lof_variants(c, args, samples):
    idx_to_sample = util.map_indices_to_samples(c)
    query = "SELECT chrom, start, end, \
                             gt_types, gts, gene \
             FROM variants \
             WHERE is_lof='1'"
    c.execute(query)

    #header
    if args.var_mode:
        print "\t".join(['sample','lof_gene','order_of_interaction', \
                    'interacting_gene', 'var_id', 'chrom', 'start', \
                    'end', 'impact','biotype','in_dbsnp', 'clinvar_sig', \
                    'clinvar_disease_name', 'aaf_1kg_all','aaf_esp_all'])

    elif (not args.var_mode):
        print "\t".join(['sample','lof_gene','order_of_interaction', \
                         'interacting_gene'])

    sample_lof_interactions(c, args, idx_to_sample, samples)


def genequery(parser, args):
    if os.path.exists(args.db):
        conn = sqlite3.connect(args.db)
        conn.isolation_level = None
        conn.row_factory = sqlite3.Row
        c = conn.cursor()
        sample_variants(c, args)

def lofgenequery(parser, args):
    if os.path.exists(args.db):
        conn = sqlite3.connect(args.db)
        conn.isolation_level = None
        conn.row_factory = sqlite3.Row
        c = conn.cursor()
        samples = sample_variants(c, args)
        sample_lof_variants(c, args, samples)

########NEW FILE########
__FILENAME__ = tool_lof_sieve
#!/usr/bin/env python

import os
import re
import sqlite3
import gemini_utils as util
from gemini_constants import *
import numpy as np
import cPickle
import zlib


def get_ind_lof(c, args):
    
    idx_to_sample = util.map_indices_to_samples(c)

    query = "SELECT v.chrom, v.start, v.end, v.ref, v.alt, \
                             v.impact, v.aa_change, v.aa_length, \
                             v.gt_types, v.gts, i.gene, \
                             i.transcript,  i.biotype\
             FROM variants v, variant_impacts i \
             WHERE v.variant_id = i.variant_id \
             AND i.is_lof='1' \
             AND v.type = 'snp'"

    c.execute(query)

    # header
    print '\t'.join(['chrom', 'start', 'end', 'ref', 'alt',
                     'highest_impact', 'aa_change', 'var_trans_pos',
                     'trans_aa_length', 'var_trans_pct',
                     'sample', 'genotype', 'gene', 'transcript', 'trans_type'])

    for r in c:
        gt_types = np.array(cPickle.loads(zlib.decompress(r['gt_types'])))
        gts = np.array(cPickle.loads(zlib.decompress(r['gts'])))
        gene = str(r['gene'])
        trans = str(r['transcript'])

        aa_change = str(r['aa_change'])
        aa_length = str(r['aa_length'])
        transcript_pos = None
        transcript_pct = None
        if aa_change != 'None':
            try:
                #transcript_pos for snpEff annotated VCF
                transcript_pos = re.findall('\S(\d+)\S', aa_change)[0]
            except IndexError:
                #transcript_pos for VEP annotated VCF
                if aa_length != 'None' and \
                        aa_length.split("/")[0] != "-":
                    transcript_pos = aa_length.split("/")[0] 
        #transcript_pct for snpEff annotated VCF        
        if aa_length != 'None' and "/" not in aa_length:
            transcript_pct = float(transcript_pos) / float(aa_length)
        #transcript_pct for VEP annotated VCF
        elif aa_length != 'None' and "/" in aa_length:
            transcript_pct = float(transcript_pos) / float(aa_length.split("/")[1])

        for idx, gt_type in enumerate(gt_types):
            if gt_type == HET or gt_type == HOM_ALT:
                print "\t".join([r['chrom'], str(r['start']),
                                 str(r['end']), r['ref'], r['alt'],
                                 r['impact'],
                                 r['aa_change'] or 'None',
                                 transcript_pos or 'None',
                                 r['aa_length'] or 'None',
                                 str(transcript_pct) or 'None',
                                 idx_to_sample[idx],
                                 gts[idx], gene, trans, r['biotype'] or 'None'])


def lof_sieve(parser, args):
    if os.path.exists(args.db):
        conn = sqlite3.connect(args.db)
        conn.isolation_level = None
        conn.row_factory = sqlite3.Row
        c = conn.cursor()

        get_ind_lof(c, args)

########NEW FILE########
__FILENAME__ = tool_pathways
#!/usr/bin/env python

import os
import sys
import sqlite3
import numpy as np
import cPickle
import zlib
from collections import defaultdict
from gemini.config import read_gemini_config
import gemini_utils as util
from gemini_constants import *



def get_pathways(args):

    version_dic = defaultdict()
    version_dic = {'66': 'kegg_pathways_ensembl66', '67': 'kegg_pathways_ensembl67',
                   '68': 'kegg_pathways_ensembl68', '69': 'kegg_pathways_ensembl69',
                   '70': 'kegg_pathways_ensembl70', '71': 'kegg_pathways_ensembl71'}

    config = read_gemini_config()
    path_dirname = config["annotation_dir"]
    if args.version in version_dic:
        path_file = os.path.join(path_dirname, version_dic[args.version])

    else:
        sys.exit("Unsupported Ensembl gene version.\n")

    agn_paths = defaultdict(list)
    hgnc_paths = defaultdict(list)
    ensembl_paths = defaultdict(list)

    for line in open(path_file, 'r'):
        fields=line.strip().split("\t")
        uniprot = fields[0]
        agn = fields[1]
        hgnc = fields[2]
        ensid = fields[3]
        ens_transcript = fields[4]
        hsa = fields[5]
        path = fields[6] if fields[6] != 'None' else None

        # clean up the pathways such that this:
        # path:hsa00260;Glycine_serine_and_threonine_metabolism
        # becomes this:
        # hsa00260:Glycine_serine_and_threonine_metabolism
        if path is not None and path.startswith("path:"):
            path = path[5:]
            path = path.replace(";", ":")

        # build gene/transcript -> pathway mappings using
        # all three gene naming conventions
        agn_paths[(agn, ens_transcript)].append(path)
        hgnc_paths[(hgnc, ens_transcript)].append(path)
        ensembl_paths[(ensid, ens_transcript)].append(path)

    return agn_paths, hgnc_paths, ensembl_paths

def _get_pathways(gene, transcript, pathways, allow_none=True):
    # get distinct pathways
    pathways = set(pathways)
    if None in pathways:
        # remove "None" if a valid pathway exists.
        if len(pathways) > 1 or allow_none is False:
           pathways.remove(None)
    return pathways

def _report_variant_pathways(c, args, idx_to_sample):

    (agn_paths, hgnc_paths, ensembl_paths) = get_pathways(args)

    for r in c:
        gt_types = np.array(cPickle.loads(zlib.decompress(r['gt_types'])))
        gts      = np.array(cPickle.loads(zlib.decompress(r['gts'])))
        gene     = str(r['gene'])
        trans    = str(r['transcript'])

        pathways = []
        if (gene, trans) in agn_paths:
            pathways = _get_pathways(gene, trans, agn_paths[(gene, trans)],
                            allow_none=False)
        elif (gene, trans) in hgnc_paths:
            pathways = _get_pathways(gene, trans, hgnc_paths[(gene, trans)],
                            allow_none=False)
        elif (gene, trans) in ensembl_paths:
            pathways = _get_pathways(gene, trans, ensembl_paths[(gene, trans)],
                            allow_none=False)
        pathlist = ",".join(pathways)
        for idx, gt_type in enumerate(gt_types):
            if (gt_type == HET or gt_type == HOM_ALT) and \
                len(pathways) > 0:
                print "\t".join([r['chrom'], str(r['start']), str(r['end']), \
                                 r['ref'], r['alt'], r['impact'], \
                                 idx_to_sample[idx], gts[idx], gene, trans, \
                                 pathlist])

def get_ind_pathways(c, args):

    idx_to_sample = util.map_indices_to_samples(c)

    query = "SELECT v.chrom, v.start, v.end, v.ref, v.alt, \
                             i.impact, v.gt_types, v.gts, i.gene, \
                             i.transcript \
             FROM variants v, variant_impacts i \
             WHERE v.variant_id = i.variant_id"

    c.execute(query)

    # header
    print '\t'.join(['chrom', 'start', 'end', 'ref', 'alt', \
                     'impact', 'sample', 'genotype', \
                     'gene', 'transcript', 'pathway'])

    _report_variant_pathways(c, args, idx_to_sample)


def get_ind_lof_pathways(c, args):

    idx_to_sample = util.map_indices_to_samples(c)

    query = "SELECT v.chrom, v.start, v.end, v.ref, v.alt, \
                             i.impact, v.gt_types, v.gts, i.gene, \
                             i.transcript \
             FROM variants v, variant_impacts i \
             WHERE v.variant_id = i.variant_id \
             AND i.is_lof='1'"

    c.execute(query)

    # header
    print '\t'.join(['chrom', 'start', 'end', 'ref', 'alt', \
                     'impact', 'sample', 'genotype', \
                     'gene', 'transcript', 'pathway'])

    _report_variant_pathways(c, args, idx_to_sample)



def pathways(parser, args):
    if os.path.exists(args.db):
        conn = sqlite3.connect(args.db)
        conn.isolation_level = None
        conn.row_factory = sqlite3.Row
        c = conn.cursor()

        if (not args.lof):
            get_ind_pathways(c, args)
        else:
            get_ind_lof_pathways(c, args)





########NEW FILE########
__FILENAME__ = vep
#!/usr/bin/env python

#############

# CSQ: Consequence|Codons|Amino_acids|Gene|hgnc|Feature|EXON|polyphen|sift|Protein_position|BIOTYPE
# missense_variant|gAg/gTg|E/V|ENSG00000188157||ENST00000379370|12/36|probably_damaging(0.932)|deleterious(0.02)|728/2045_protein_coding
# nc_transcript_variant|||ENSG00000116254|CHD5|ENST00000491020|5/6|||||
#############

import re
from collections import namedtuple
from collections import defaultdict


class EffectDetails(object):
    def __init__(self, impact_string, severity, detail_string, counter):
        fields = detail_string.split("|")
        self.effect_severity = severity
        self.effect_name = impact_string
        self.anno_id = counter
        self.codon_change = fields[1] if fields[1] != '' else None
        self.aa_change = fields[2] if fields[2] != '' else None
        self.ensembl_gene = fields[3] if fields[3] != '' else None
        self.hgnc = fields[4] if fields[4] != '' else None
        self.gene = self.hgnc if fields[4] != '' else self.ensembl_gene
        self.transcript = fields[5] if fields[5] != '' else None
        self.exon = fields[6] if fields[6] != '' else None
        self.polyphen = fields[7] if fields[7] != '' else None
        self.sift = fields[8] if fields[8] != '' else None
        self.aa_length = fields[9] if fields[9] != '' else None
        self.biotype = fields[10] if fields[10] != '' else None
        self.warnings = None
        self.consequence = effect_dict[
            self.effect_name] if self.effect_severity != None else self.effect_name
        self.so = self.effect_name #VEP impacts are SO by default
        
        if len(fields) > 11:
            self.warnings = fields[11]

        # rules for being exonic.
        # 1. the impact must be in the list of exonic impacts
        # 3. must be protein_coding
        self.is_exonic = 0
        if self.effect_name in exonic_impacts and \
                self.biotype == "protein_coding":
            self.is_exonic = 1
        
        # rules for being loss-of-function (lof).
        #  must be protein_coding
        #  must be a coding variant with HIGH impact
        self.is_lof = 0 
        if self.effect_severity == "HIGH" and self.biotype == "protein_coding":
            self.is_lof = 1
        
        # Rules for being coding
        # must be protein_coding
        # Exonic but not UTR's
        self.is_coding = 0
        if self.is_exonic and not (self.effect_name == "5_prime_UTR_variant" or \
                                   self.effect_name == "3_prime_UTR_variant"):
            self.is_coding = 1
            
        # parse Polyphen predictions
        if self.polyphen is not None:
            self.polyphen_b = self.polyphen.split("(")
            self.polyphen_pred = self.polyphen_b[0]
            self.polyphen2 = self.polyphen_b[1].split(")")
            self.polyphen_score = self.polyphen2[0]
        else:
            self.polyphen_pred = None
            self.polyphen_score = None
        # parse SIFT predictions
        if self.sift is not None:
            self.sift_b = self.sift.split("(")
            self.sift_pred = self.sift_b[0]
            self.sift2 = self.sift_b[1].split(")")
            self.sift_score = self.sift2[0]
        else:
            self.sift_pred = None
            self.sift_score = None

    def __str__(self):

        return "\t".join([self.consequence, self.effect_severity, str(self.codon_change),
                          str(self.aa_change), str(self.aa_length), str(self.biotype),
                          str(self.ensembl_gene), str(self.gene), str(self.transcript),
                          str(self.exon), str(self.is_exonic), str(self.anno_id), str(self.polyphen_pred),
                          str(self.polyphen_score), str(self.sift_pred), str(self.sift_score),
                          str(self.is_coding), str(self.is_lof), str(self.so)])

    def __repr__(self):
        return self.__str__()


exonic_impacts = ["stop_gained",
                  "stop_lost",
                  "frameshift_variant",
                  "initiator_codon_variant",
                  "inframe_deletion",
                  "inframe_insertion",
                  "missense_variant",
                  "incomplete_terminal_codon_variant",
                  "stop_retained_variant",
                  "synonymous_variant",
                  "coding_sequence_variant",
                  "5_prime_UTR_variant",
                  "3_prime_UTR_variant",
                  "transcript_ablation",
                  "transcript_amplification",
                  "feature_elongation",
                  "feature_truncation"]


effect_names = ["splice_acceptor_variant", "splice_donor_variant",
                "stop_gained", "stop_lost",
                "non_coding_exon_variant", "frameshift_variant",
                "initiator_codon_variant", "inframe_deletion",
                "inframe_insertion", "missense_variant",
                "splice_region_variant", "incomplete_terminal_codon_variant",
                "stop_retained_variant", "synonymous_variant",
                "coding_sequence_variant", "mature_miRNA_variant",
                "5_prime_UTR_variant", "3_prime_UTR_variant",
                "intron_variant", "NMD_transcript_variant",
                "nc_transcript_variant", "upstream_gene_variant",
                "downstream_gene_variant", "regulatory_region_variant",
                "TF_binding_site_variant", "intergenic_variant",
                "regulatory_region_ablation", "regulatory_region_amplification",
                "transcript_ablation", "transcript_amplification",
                "TFBS_ablation", "TFBS_amplification",
                "feature_elongation", "feature_truncation"]

effect_dict = defaultdict()
effect_dict = {
    'splice_acceptor_variant': 'splice_acceptor', 'splice_donor_variant': 'splice_donor',
    'stop_gained': 'stop_gain', 'stop_lost': 'stop_loss',
    'non_coding_exon_variant': 'nc_exon', 'frameshift_variant': 'frame_shift',
    'initiator_codon_variant': 'transcript_codon_change', 'inframe_deletion': 'inframe_codon_loss',
    'inframe_insertion': 'inframe_codon_gain', 'missense_variant': 'non_syn_coding',
    'splice_region_variant': 'splice_region', 'incomplete_terminal_codon_variant': 'incomplete_terminal_codon',
    'stop_retained_variant': 'synonymous_stop', 'synonymous_variant': 'synonymous_coding',
    'coding_sequence_variant': 'CDS', 'mature_miRNA_variant': 'mature_miRNA',
    '5_prime_UTR_variant': 'UTR_5_prime', '3_prime_UTR_variant': 'UTR_3_prime',
    'intron_variant': 'intron', 'NMD_transcript_variant': 'NMD_transcript',
    'nc_transcript_variant': 'nc_transcript', 'upstream_gene_variant': 'upstream',
    'downstream_gene_variant': 'downstream', 'regulatory_region_variant': 'regulatory_region',
    'TF_binding_site_variant': 'TF_binding_site', 'intergenic_variant': 'intergenic',
    'regulatory_region_ablation': 'regulatory_region_ablation', 'regulatory_region_amplification': 'regulatory_region_amplification',
    'transcript_ablation': 'transcript_ablation', 'transcript_amplification': 'transcript_amplification',
    'TFBS_ablation': 'TFBS_ablation', 'TFBS_amplification': 'TFBS_amplification',
    'feature_elongation': 'feature_elongation', 'feature_truncation': 'feature_truncation'}

effect_desc = ["The variant hits the splice acceptor site (2 basepair region at 3' end of an intron)", "The variant hits the splice donor site (2 basepair region at 5'end of an intron)",
               "Variant causes a STOP codon", "Variant causes stop codon to be mutated into a non-stop codon",
               "Variant causes a change in the non coding exon sequence", "Insertion or deletion causes a frame shift in coding sequence",
               "Variant causes atleast one base change in the first codon of a transcript", "An inframe non-syn variant that deletes bases from the coding sequence",
               "An inframe non-syn variant that inserts bases in the coding sequence", "The variant causes a different amino acid in the coding sequence",
               "Variant causes a change within the region of a splice site (1-3bps into an exon or 3-8bps into an intron)", "The variant hits the incomplete codon of a transcript whose end co-ordinate is not known",
               "The variant causes stop codon to be mutated into another stop codon", "The variant causes no amino acid change in coding sequence",
               "Variant hits coding sequence with indeterminate effect", "The variant hits a microRNA",
               "Variant hits the 5 prime untranslated region", "Variant hits the 3 prime untranslated region",
               "Variant hits an intron", "A variant hits a transcript that is predicted to undergo nonsense mediated decay",
                   "Variant hits a gene that does not code for a protein", "The variant hits upstream of a gene (5' of a gene)",
                   "The variant hits downstream of a gene (3' of a gene)", "Variant hits the regulatory region annotated by Ensembl(e.g promoter)",
                   "Variant falls in a transcription factor binding motif within an Ensembl regulatory region", "The variant is located in the intergenic region, between genes",
                   "SV causes ablation of a regulatory region", "SV results in an amplification of a regulatory region",
                   "SV causes an ablation/deletion of a transcript feature", "SV causes an amplification of a transcript feature",
                   "SV results in a deletion of the TFBS", "SV results in an amplification of a region containing TFBS",
                   "SV causes an extension of a genomic feature wrt reference", "SV causes a reduction of a genomic feature compared to reference"]

effect_priorities = ["HIGH", "HIGH",
                     "HIGH", "HIGH",
                     "LOW", "HIGH",
                     "HIGH", "MED",
                     "MED", "MED",
                     "MED", "LOW",
                     "LOW", "LOW",
                     "LOW", "MED",
                     "LOW", "LOW",
                     "LOW", "LOW",
                     "LOW", "LOW",
                     "LOW", "MED",
                     "MED", "LOW",
                     "MED", "MED",
                     "LOW", "LOW",
                     "MED", "MED",
                     "LOW", "LOW"]

effect_priority_codes = [1, 1,
                        1, 1,
                        3, 1,
                        1, 2,
                        2, 2,
                        2, 3,
                        3, 3,
                        3, 2,
                        3, 3,
                        3, 3,
                        3, 3,
                        3, 2,
                        2, 3,
                        2, 2,
                        3, 3,
                        2, 2,
                        3, 3]

effect_ids = range(1, 35)

effect_map = {}
EffectInfo = namedtuple(
    'EffectInfo', ['id', 'priority', 'priority_code', 'desc'])
for i, effect_name in enumerate(effect_names):
    info = EffectInfo(effect_ids[i], effect_priorities[i],
                      effect_priority_codes[i], effect_desc[i])
    effect_map[effect_name] = info

########NEW FILE########
__FILENAME__ = version
__version__="0.7.1a"


########NEW FILE########
