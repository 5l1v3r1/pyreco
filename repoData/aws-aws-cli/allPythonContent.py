__FILENAME__ = argparser
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import argparse
import sys
import six
from difflib import get_close_matches


class CLIArgParser(argparse.ArgumentParser):
    Formatter = argparse.RawTextHelpFormatter

    # When displaying invalid choice error messages,
    # this controls how many options to show per line.
    ChoicesPerLine = 2

    def _check_value(self, action, value):
        """
        It's probably not a great idea to override a "hidden" method
        but the default behavior is pretty ugly and there doesn't
        seem to be any other way to change it.
        """
        # converted value must be one of the choices (if specified)
        if action.choices is not None and value not in action.choices:
            msg = ['Invalid choice, valid choices are:\n']
            for i in range(len(action.choices))[::self.ChoicesPerLine]:
                current = []
                for choice in action.choices[i:i+self.ChoicesPerLine]:
                    current.append('%-40s' % choice)
                msg.append(' | '.join(current))
            possible = get_close_matches(value, action.choices, cutoff=0.8)
            if possible:
                extra = ['\n\nInvalid choice: %r, maybe you meant:\n' % value]
                for word in possible:
                    extra.append('  * %s' % word)
                msg.extend(extra)
            raise argparse.ArgumentError(action, '\n'.join(msg))

    def parse_known_args(self, args, namespace=None):
        parsed, remaining = super(CLIArgParser, self).parse_known_args(args, namespace)
        terminal_encoding = getattr(sys.stdin, 'encoding', 'utf-8')
        if terminal_encoding is None:
            # In some cases, sys.stdin won't have an encoding set,
            # (e.g if it's set to a StringIO).  In this case we just
            # default to utf-8.
            terminal_encoding = 'utf-8'
        for arg, value in vars(parsed).items():
            if isinstance(value, six.binary_type):
                setattr(parsed, arg, value.decode(terminal_encoding))
            elif isinstance(value, list):
                encoded = []
                for v in value:
                    if isinstance(v, six.binary_type):
                        encoded.append(v.decode(terminal_encoding))
                    else:
                        encoded.append(v)
                setattr(parsed, arg, encoded)
        return parsed, remaining


class MainArgParser(CLIArgParser):
    Formatter = argparse.RawTextHelpFormatter

    def __init__(self, command_table, version_string,
                 description, usage, argument_table):
        super(MainArgParser, self).__init__(
            formatter_class=self.Formatter,
            add_help=False,
            conflict_handler='resolve',
            description=description,
            usage=usage)
        self._build(command_table, version_string, argument_table)

    def _create_choice_help(self, choices):
        help_str = ''
        for choice in sorted(choices):
            help_str += '* %s\n' % choice
        return help_str

    def _build(self, command_table, version_string, argument_table):
        for argument_name in argument_table:
            argument = argument_table[argument_name]
            argument.add_to_parser(self)
        self.add_argument('--version', action="version",
                          version=version_string,
                          help='Display the version of this tool')
        self.add_argument('command', choices=list(command_table.keys()))


class ServiceArgParser(CLIArgParser):

    Usage = ("aws [options] <command> <subcommand> [parameters]")

    def __init__(self, operations_table, service_name):
        super(ServiceArgParser, self).__init__(
            formatter_class=argparse.RawTextHelpFormatter,
            add_help=False,
            conflict_handler='resolve',
            usage=self.Usage)
        self._build(operations_table)
        self._service_name = service_name

    def _build(self, operations_table):
        self.add_argument('operation', choices=list(operations_table.keys()))


class ArgTableArgParser(CLIArgParser):
    """CLI arg parser based on an argument table."""
    Usage = ("aws [options] <command> <subcommand> [parameters]")

    def __init__(self, argument_table, command_table=None):
        # command_table is an optional subcommand_table.  If it's passed
        # in, then we'll update the argparse to parse a 'subcommand' argument
        # and populate the choices field with the command table keys.
        super(ArgTableArgParser, self).__init__(
            formatter_class=self.Formatter,
            add_help=False,
            usage=self.Usage,
            conflict_handler='resolve')
        if command_table is None:
            command_table = {}
        self._build(argument_table, command_table)

    def _build(self, argument_table, command_table):
        for arg_name in argument_table:
            argument = argument_table[arg_name]
            argument.add_to_parser(self)
        if command_table:
            self.add_argument('subcommand', choices=list(command_table.keys()),
                              nargs='?')

    def parse_known_args(self, args, namespace=None):
        if len(args) == 1 and args[0] == 'help':
            namespace = argparse.Namespace()
            namespace.help = 'help'
            return namespace, []
        else:
            return super(ArgTableArgParser, self).parse_known_args(
                args, namespace)

########NEW FILE########
__FILENAME__ = argprocess
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""Module for processing CLI args."""
import os
import logging
import six

from botocore.compat import OrderedDict, json

from awscli import utils
from awscli import SCALAR_TYPES, COMPLEX_TYPES
from awscli.paramfile import get_paramfile, ResourceLoadingError


LOG = logging.getLogger('awscli.argprocess')


class ParamError(Exception):
    def __init__(self, param, message):
        full_message = ("Error parsing parameter '%s': %s" %
                        (param.cli_name, message))
        super(ParamError, self).__init__(full_message)
        self.param = param
        self.message = message


class ParamSyntaxError(Exception):
    pass


class ParamUnknownKeyError(Exception):
    def __init__(self, param, key, valid_keys):
        valid_keys = ', '.join(valid_keys)
        full_message = (
            "Unknown key '%s' for parameter %s, valid choices "
            "are: %s" % (key, param.cli_name, valid_keys))
        super(ParamUnknownKeyError, self).__init__(full_message)


def unpack_argument(session, service_name, operation_name, param, value):
    """
    Unpack an argument's value from the commandline. This is part one of a two
    step process in handling commandline arguments. Emits the load-cli-arg
    event with service, operation, and parameter names. Example::

        load-cli-arg.ec2.describe-instances.foo

    """
    param_name = getattr(param, 'name', 'anonymous')

    value_override = session.emit_first_non_none_response(
        'load-cli-arg.%s.%s.%s' % (service_name,
                                   operation_name,
                                   param_name),
        param=param, value=value, service_name=service_name,
        operation_name=operation_name)

    if value_override is not None:
        value = value_override

    return value


def uri_param(param, value, **kwargs):
    """Handler that supports param values from URIs.
    """
    # Some params have a 'no_paramfile' attribute in their JSON
    # models which means that we should not allow any uri based params
    # for this argument.
    if getattr(param, 'no_paramfile', False):
        return
    else:
        return _check_for_uri_param(param, value)


def _check_for_uri_param(param, value):
    if isinstance(value, list) and len(value) == 1:
        value = value[0]
    try:
        return get_paramfile(value)
    except ResourceLoadingError as e:
        raise ParamError(param, six.text_type(e))


def detect_shape_structure(param):
    if param.type in SCALAR_TYPES:
        return 'scalar'
    elif param.type == 'structure':
        sub_types = [detect_shape_structure(p)
                     for p in param.members]
        # We're distinguishing between structure(scalar)
        # and structure(scalars), because for the case of
        # a single scalar in a structure we can simplify
        # more than a structure(scalars).
        if len(sub_types) == 1 and all(p == 'scalar' for p in sub_types):
            return 'structure(scalar)'
        elif len(sub_types) > 1 and all(p == 'scalar' for p in sub_types):
            return 'structure(scalars)'
        else:
            return 'structure(%s)' % ', '.join(sorted(set(sub_types)))
    elif param.type == 'list':
        return 'list-%s' % detect_shape_structure(param.members)
    elif param.type == 'map':
        if param.members.type in SCALAR_TYPES:
            return 'map-scalar'
        else:
            return 'map-%s' % detect_shape_structure(param.members)


class ParamShorthand(object):

    # To add support for a new shape:
    #
    #  * Add it to SHORTHAND_SHAPES below, key is the shape structure
    #    value is the name of the method to call.
    #  * Implement parse method.
    #  * Implement _doc_<parse_method_name>.  This is used to generate
    #    the docs for this shorthand syntax.

    SHORTHAND_SHAPES = {
        'structure(scalars)': '_key_value_parse',
        'structure(scalar)': '_special_key_value_parse',
        'map-scalar': '_key_value_parse',
        'list-structure(scalar)': '_list_scalar_parse',
        'list-structure(scalars)': '_list_key_value_parse',
        'list-structure(list-scalar, scalar)': '_list_scalar_list_parse',
    }

    def __init__(self):
        pass

    def __call__(self, param, value, **kwargs):
        """Attempt to parse shorthand syntax for values.

        This is intended to be hooked up as an event handler (hence the
        **kwargs).  Given ``param`` object and its string ``value``,
        figure out if we can parse it.  If we can parse it, we return
        the parsed value (typically some sort of python dict).

        :type param: :class:`botocore.parameters.Parameter`
        :param param: The parameter object (includes various metadata
            about the parameter).

        :type value: str
        :param value: The value for the parameter type on the command
            line, e.g ``--foo this_value``, value would be ``"this_value"``.

        :returns: If we can parse the value we return the parsed value.
            If it looks like JSON, we return None (which tells the event
            emitter to use the default ``unpack_cli_arg`` provided that
            no other event handlers can parsed the value).  If we
            run into an error parsing the value, a ``ParamError`` will
            be raised.

        """
        parse_method = self.get_parse_method_for_param(param, value)
        if parse_method is None:
            return
        else:
            try:
                LOG.debug("Using %s for param %s", parse_method, param)
                parsed = getattr(self, parse_method)(param, value)
            except ParamSyntaxError as e:
                doc_fn = self._get_example_fn(param)
                # Try to give them a helpful error message.
                if doc_fn is None:
                    raise e
                else:
                    raise ParamError(param, "should be: %s" % doc_fn(param))
            return parsed

    def get_parse_method_for_param(self, param, value=None):
        # We first need to make sure this is a parameter that qualifies
        # for simplification.  The first short-circuit case is if it looks
        # like json we immediately return.
        if isinstance(value, list):
            check_val = value[0]
        else:
            check_val = value
        if isinstance(check_val, six.string_types) and check_val.strip().startswith(
                ('[', '{')):
            LOG.debug("Param %s looks like JSON, not considered for "
                      "param shorthand.", param.py_name)
            return
        structure = detect_shape_structure(param)
        # If this looks like shorthand then we log the detected structure
        # to help with debugging why the shorthand may not work, for
        # example list-structure(list-structure(scalars))
        LOG.debug('Detected structure: {0}'.format(structure))
        parse_method = self.SHORTHAND_SHAPES.get(structure)
        return parse_method

    def _get_example_fn(self, param):
        doc_fn = None
        shape_structure = detect_shape_structure(param)
        method = self.SHORTHAND_SHAPES.get(shape_structure)
        if method:
            doc_fn = getattr(self, '_docs' + method, None)
        return doc_fn

    def add_example_fn(self, arg_name, help_command, **kwargs):
        """
        Adds a callable to the ``example_fn`` attribute of the parameter
        if the parameter type is supported by shorthand syntax.  This
        callable should return a string containing just the example and
        not any of the ReST formatting that might be required in the docs.
        """
        argument = help_command.arg_table[arg_name]
        if hasattr(argument, 'argument_object') and argument.argument_object:
            param = argument.argument_object
            LOG.debug('Adding example fn for: %s' % param.name)
            doc_fn = self._get_example_fn(param)
            param.example_fn = doc_fn

    def _list_scalar_list_parse(self, param, value):
        # Think something like ec2.DescribeInstances.Filters.
        # We're looking for key=val1,val2,val3,key2=val1,val2.
        args = {}
        for arg in param.members.members:
            # Arg name -> arg object lookup
            args[arg.name] = arg
        parsed = []
        for v in value:
            parts = self._split_on_commas(v)
            current_parsed = {}
            current_key = None
            for part in parts:
                current = part.split('=', 1)
                if len(current) == 2:
                    # This is a key/value pair.
                    current_key = current[0].strip()
                    if current_key not in args:
                        raise ParamUnknownKeyError(param, current_key,
                                                   args.keys())
                    current_value = unpack_scalar_cli_arg(args[current_key],
                                                          current[1].strip())
                    if args[current_key].type == 'list':
                        current_parsed[current_key] = current_value.split(',')
                    else:
                        current_parsed[current_key] = current_value
                elif current_key is not None:
                    # This is a value which we associate with the current_key,
                    # so key1=val1,val2
                    #               ^
                    #               |
                    #             val2 is associated with key1.
                    current_value = unpack_scalar_cli_arg(args[current_key],
                                                          current[0])
                    current_parsed[current_key].append(current_value)
                else:
                    raise ParamSyntaxError(part)
            parsed.append(current_parsed)
        return parsed

    def _list_scalar_parse(self, param, value):
        single_param = param.members.members[0]
        parsed = []
        # We know that value is a list in this case.
        for v in value:
            parsed.append({single_param.name: v})
        return parsed

    def _list_key_value_parse(self, param, value):
        # param is a list param.
        # param.member is the struct param.
        struct_param = param.members
        parsed = []
        for v in value:
            single_struct_param = self._key_value_parse(struct_param, v)
            parsed.append(single_struct_param)
        return parsed

    def _special_key_value_parse(self, param, value):
        # This is a special key value parse that can do the normal
        # key=value parsing, *but* supports a few additional conveniences
        # when working with a structure with a single element.
        # Precondition: param is a shape of structure(scalar)
        if len(param.members) == 1 and param.members[0].name == 'Value' and \
                '=' not in value:
            # We have an even shorter shorthand syntax for structure
            # of scalars of a single element with a member name of
            # 'Value'.
            return {'Value': value}
        else:
            return self._key_value_parse(param, value)

    def _key_value_parse(self, param, value):
        # The expected structure is:
        #  key=value,key2=value
        # that is, csv key value pairs, where the key and values
        # are separated by '='.  All of this should be whitespace
        # insensitive.
        parsed = OrderedDict()
        parts = self._split_on_commas(value)
        valid_names = self._create_name_to_params(param)
        for part in parts:
            try:
                key, value = part.split('=', 1)
            except ValueError:
                raise ParamSyntaxError(part)
            key = key.strip()
            value = value.strip()
            if valid_names and key not in valid_names:
                raise ParamUnknownKeyError(param, key, valid_names)
            if valid_names:
                sub_param = valid_names[key]
                if sub_param is not None:
                    value = unpack_scalar_cli_arg(sub_param, value)
            parsed[key] = value
        return parsed

    def _create_name_to_params(self, param):
        if param.type == 'structure':
            return dict([(p.name, p) for p in param.members])
        elif param.type == 'map' and hasattr(param.keys, 'enum'):
            return dict([(v, None) for v in param.keys.enum])

    def _docs_list_scalar_list_parse(self, param):
        s = 'Key value pairs, where values are separated by commas.\n'
        s += '%s ' % param.cli_name
        inner_params = param.members.members
        scalar_params = [p for p in inner_params if p.type in SCALAR_TYPES]
        list_params = [p for p in inner_params if p.type == 'list']
        for param in scalar_params:
            s += '%s=%s1,' % (param.name, param.type)
        for param in list_params[:-1]:
            param_type = param.members.type
            s += '%s=%s1,%s2,' % (param.name, param_type, param_type)
        last_param = list_params[-1]
        param_type = last_param.members.type
        s += '%s=%s1,%s2' % (last_param.name, param_type, param_type)
        return s

    def _docs_list_scalar_parse(self, param):
        name = param.members.members[0].name
        return '%s %s1 %s2 %s3' % (param.cli_name, name, name, name)

    def _docs_list_key_value_parse(self, param):
        s = "Key value pairs, with multiple values separated by a space.\n"
        s += '%s ' % param.cli_name
        s += ','.join(['%s=%s' % (sub_param.name, sub_param.type)
                       for sub_param in param.members.members])
        return s

    def _docs_special_key_value_parse(self, param):
        if len(param.members) == 1 and param.members[0].name == 'Value':
            # Returning None will indicate that we don't have
            # any examples to generate, and the entire examples section
            # should be skipped for this arg.
            return None
        else:
            return self._docs_key_value_parse(param)

    def _docs_key_value_parse(self, param):
        s = '%s ' % param.cli_name
        if param.type == 'structure':
            s += ','.join(['%s=value' % sub_param.name
                            for sub_param in param.members])
        elif param.type == 'map':
            s += 'key_name=string,key_name2=string'
            if param.keys.type == 'string' and hasattr(param.keys, 'enum'):
                s += '\nWhere valid key names are:\n'
                for value in param.keys.enum:
                    s += '  %s\n' % value
        return s

    def _split_on_commas(self, value):
        try:
            return utils.split_on_commas(value)
        except ValueError as e:
            raise ParamSyntaxError(six.text_type(e))


def unpack_cli_arg(parameter, value):
    """
    Parses and unpacks the encoded string command line parameter
    and returns native Python data structures that can be passed
    to the Operation.

    :type parameter: :class:`botocore.parameter.Parameter`
    :param parameter: The parameter object containing metadata about
        the parameter.

    :param value: The value of the parameter.  This can be a number of
        different python types (str, list, etc).  This is the value as
        it's specified on the command line.

    :return: The "unpacked" argument than can be sent to the `Operation`
        object in python.
    """
    if parameter.type in SCALAR_TYPES:
        return unpack_scalar_cli_arg(parameter, value)
    elif parameter.type in COMPLEX_TYPES:
        return unpack_complex_cli_arg(parameter, value)
    else:
        return six.text_type(value)


def unpack_complex_cli_arg(parameter, value):
    if parameter.type == 'structure' or parameter.type == 'map':
        if value.lstrip()[0] == '{':
            try:
                return json.loads(value, object_pairs_hook=OrderedDict)
            except ValueError as e:
                raise ParamError(
                    parameter, "Invalid JSON: %s\nJSON received: %s"
                    % (e, value))
        raise ParamError(parameter, "Invalid JSON:\n%s" % value)
    elif parameter.type == 'list':
        if isinstance(value, six.string_types):
            if value.lstrip()[0] == '[':
                return json.loads(value, object_pairs_hook=OrderedDict)
        elif isinstance(value, list) and len(value) == 1:
            single_value = value[0].strip()
            if single_value and single_value[0] == '[':
                return json.loads(value[0], object_pairs_hook=OrderedDict)
        try:
            return [unpack_cli_arg(parameter.members, v) for v in value]
        except ParamError as e:
            # The list params don't have a name/cli_name attached to them
            # so they will have bad error messages.  We're going to
            # attach the parent parameter to this error message to provide
            # a more helpful error message.
            raise ParamError(parameter, e.message)


def unpack_scalar_cli_arg(parameter, value):
    if parameter.type == 'integer' or parameter.type == 'long':
        return int(value)
    elif parameter.type == 'float' or parameter.type == 'double':
        # TODO: losing precision on double types
        return float(value)
    elif parameter.type == 'blob' and parameter.payload and parameter.streaming:
        file_path = os.path.expandvars(value)
        file_path = os.path.expanduser(file_path)
        if not os.path.isfile(file_path):
            msg = 'Blob values must be a path to a file.'
            raise ParamError(parameter, msg)
        return open(file_path, 'rb')
    elif parameter.type == 'boolean':
        if isinstance(value, six.string_types) and value.lower() == 'false':
            return False
        return bool(value)
    else:
        return value

########NEW FILE########
__FILENAME__ = arguments
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""Abstractions for CLI arguments.

This module contains abstractions for representing CLI arguments.
This includes how the CLI argument parser is created, how arguments
are serialized, and how arguments are bound (if at all) to operation
arguments.

The BaseCLIArgument is the interface for all arguments.  This is the interface
expected by objects that work with arguments.  If you want to implement your
own argument subclass, make sure it implements everything in BaseCLIArgument.

Arguments generally fall into one of several categories:

* global argument.  These arguments may influence what the CLI does,
  but aren't part of the input parameters needed to make an API call.  For
  example, the ``--region`` argument specifies which region to send the request
  to.  The ``--output`` argument specifies how to display the response to the
  user.  The ``--query`` argument specifies how to select specific elements
  from a response.
* operation argument.  These are arguments that influence the parameters we
  send to a service when making an API call.  Some of these arguments are
  automatically created directly from introspecting the JSON service model.
  Sometimes customizations may provide a pseudo-argument that takes the
  user input and maps the input value to several API parameters.

"""
import logging

from botocore import xform_name
from botocore.parameters import ListParameter, StructParameter

from awscli.argprocess import unpack_cli_arg
from awscli.schema import SchemaTransformer


LOG = logging.getLogger('awscli.arguments')


class UnknownArgumentError(Exception):
    pass


class BaseCLIArgument(object):
    """Interface for CLI argument.

    This class represents the interface used for representing CLI
    arguments.

    """

    def __init__(self, name):
        self._name = name

    def add_to_arg_table(self, argument_table):
        """Add this object to the argument_table.

        The ``argument_table`` represents the argument for the operation.
        This is called by the ``ServiceOperation`` object to create the
        arguments associated with the operation.

        :type argument_table: dict
        :param argument_table: The argument table.  The key is the argument
            name, and the value is an object implementing this interface.
        """
        argument_table[self.name] = self

    def add_to_parser(self, parser):
        """Add this object to the parser instance.

        This method is called by the associated ``ArgumentParser``
        instance.  This method should make the relevant calls
        to ``add_argument`` to add itself to the argparser.

        :type parser: ``argparse.ArgumentParser``.
        :param parser: The argument parser associated with the operation.

        """
        pass

    def add_to_params(self, parameters, value):
        """Add this object to the parameters dict.

        This method is responsible for taking the value specified
        on the command line, and deciding how that corresponds to
        parameters used by the service/operation.

        :type parameters: dict
        :param parameters: The parameters dictionary that will be
            given to ``botocore``.  This should match up to the
            parameters associated with the particular operation.

        :param value: The value associated with the CLI option.

        """
        pass

    @property
    def name(self):
        return self._name

    @property
    def cli_name(self):
        return '--' + self._name

    @property
    def cli_type_name(self):
        raise NotImplementedError("cli_type_name")

    @property
    def required(self):
        raise NotImplementedError("required")

    @property
    def documentation(self):
        raise NotImplementedError("documentation")

    @property
    def cli_type(self):
        raise NotImplementedError("cli_type")

    @property
    def py_name(self):
        return self._name.replace('-', '_')

    @property
    def choices(self):
        """List valid choices for argument value.

        If this value is not None then this should return a list of valid
        values for the argument.

        """
        return None

    @name.setter
    def name(self, value):
        self._name = value

    @property
    def group_name(self):
        """Get the group name associated with the argument.

        An argument can be part of a group.  This property will
        return the name of that group.

        This base class has no default behavior for groups, code
        that consumes argument objects can use them for whatever
        purposes they like (documentation, mutually exclusive group
        validation, etc.).

        """
        return None


class CustomArgument(BaseCLIArgument):
    """
    Represents a CLI argument that is configured from a dictionary.

    For example, the "top level" arguments used for the CLI
    (--region, --output) can use a CustomArgument argument,
    as these are described in the cli.json file as dictionaries.

    This class is also useful for plugins/customizations that want to
    add additional args.

    """

    def __init__(self, name, help_text='', dest=None, default=None,
                 action=None, required=None, choices=None, nargs=None,
                 cli_type_name=None, group_name=None, positional_arg=False,
                 no_paramfile=False, schema=None):
        self._name = name
        self._help = help_text
        self._dest = dest
        self._default = default
        self._action = action
        self._required = required
        self._nargs = nargs
        self._cli_type_name = cli_type_name
        self._group_name = group_name
        self._positional_arg = positional_arg
        if choices is None:
            choices = []
        self._choices = choices
        self.no_paramfile = no_paramfile
        self._schema = schema

        # If the top level element is a list then set nargs to
        # accept multiple values seperated by a space.
        if self._schema and self._schema.get('type', None) == 'array':
            self._nargs = '+'

        # TODO: We should eliminate this altogether.
        # You should not have to depend on an argument_object
        # as part of the interface.  Currently the argprocess
        # and docs code relies on this object.
        self.argument_object = None

    @property
    def cli_name(self):
        if self._positional_arg:
            return self._name
        else:
            return '--' + self._name

    def add_to_parser(self, parser):
        """

        See the ``BaseCLIArgument.add_to_parser`` docs for more information.

        """
        cli_name = self.cli_name
        kwargs = {}
        if self._dest is not None:
            kwargs['dest'] = self._dest
        if self._action is not None:
            kwargs['action'] = self._action
        if self._default is not None:
            kwargs['default'] = self._default
        if self._choices:
            kwargs['choices'] = self._choices
        if self._required is not None:
            kwargs['required'] = self._required
        if self._nargs is not None:
            kwargs['nargs'] = self._nargs
        parser.add_argument(cli_name, **kwargs)

    def create_argument_object(self):
        """
        Create an argument object based on the JSON schema if one is set.
        After calling this method, ``parameter.argument_object`` is available
        e.g. for generating docs.
        """
        transformer = SchemaTransformer(self._schema)
        transformed = transformer.transform()

        # Set the parameter name from the parsed arg key name
        transformed.update({'name': self.name})

        LOG.debug('Custom parameter schema for {0}: {1}'.format(
            self.name, transformed))

        # Select the correct top level type
        if transformed['type'] == 'structure':
            self.argument_object = StructParameter(None, **transformed)
        elif transformed['type'] == 'list':
            self.argument_object = ListParameter(None, **transformed)
        else:
            raise ValueError('Invalid top level type {0}!'.format(
                transformed['type']))

    @property
    def required(self):
        if self._required is None:
            return False
        return self._required

    @required.setter
    def required(self, value):
        self._required = value

    @property
    def documentation(self):
        return self._help

    @property
    def cli_type_name(self):
        if self._cli_type_name is not None:
            return self._cli_type_name
        elif self._action in ['store_true', 'store_false']:
            return 'boolean'
        elif self.argument_object is not None:
            return self.argument_object.type
        else:
            # Default to 'string' type if we don't have any
            # other info.
            return 'string'

    @property
    def cli_type(self):
        cli_type = str
        if self._action in ['store_true', 'store_false']:
            cli_type = bool
        return cli_type

    @property
    def choices(self):
        return self._choices

    @property
    def group_name(self):
        return self._group_name


class CLIArgument(BaseCLIArgument):
    """Represents a CLI argument that maps to a service parameter.

    """

    TYPE_MAP = {
        'structure': str,
        'map': str,
        'timestamp': str,
        'list': str,
        'string': str,
        'float': float,
        'integer': str,
        'long': int,
        'boolean': bool,
        'double': float,
        'blob': str
    }

    def __init__(self, name, argument_object, operation_object):
        """

        :type name: str
        :param name: The name of the argument in "cli" form
            (e.g.  ``min-instances``).

        :type argument_object: ``botocore.parameter.Parameter``
        :param argument_object: The parameter object to associate with
            this object.

        :type operation_object: ``botocore.operation.Operation``
        :param operation_object: The operation object associated with
            this object.

        """
        self._name = name
        self.argument_object = argument_object
        self.operation_object = operation_object

    @property
    def py_name(self):
        return self._name.replace('-', '_')

    @property
    def required(self):
        return self.argument_object.required

    @required.setter
    def required(self, value):
        self.argument_object.required = value

    @property
    def documentation(self):
        return self.argument_object.documentation

    @property
    def cli_type_name(self):
        return self.argument_object.type

    @property
    def cli_type(self):
        return self.TYPE_MAP.get(self.argument_object.type, str)

    def add_to_parser(self, parser):
        """

        See the ``BaseCLIArgument.add_to_parser`` docs for more information.

        """
        cli_name = self.cli_name
        parser.add_argument(
            cli_name,
            help=self.documentation,
            type=self.cli_type,
            required=self.required)

    def add_to_params(self, parameters, value):
        if value is None:
            return
        else:
            # This is a two step process.  First is the process of converting
            # the command line value into a python value.  Normally this is
            # handled by argparse directly, but there are cases where extra
            # processing is needed.  For example, "--foo name=value" the value
            # can be converted from "name=value" to {"name": "value"}.  This is
            # referred to as the "unpacking" process.  Once we've unpacked the
            # argument value, we have to decide how this is converted into
            # something that can be consumed by botocore.  Many times this is
            # just associating the key and value in the params dict as down
            # below.  Sometimes this can be more complicated, and subclasses
            # can customize as they need.
            unpacked = self._unpack_argument(value)
            LOG.debug('Unpacked value of "%s" for parameter "%s": %s', value,
                      self.argument_object.py_name, unpacked)
            parameters[self.argument_object.py_name] = unpacked

    def _unpack_argument(self, value):
        service_name = self.operation_object.service.endpoint_prefix
        operation_name = xform_name(self.operation_object.name, '-')
        override = self._emit_first_response('process-cli-arg.%s.%s' % (
            service_name, operation_name), param=self.argument_object,
            value=value,
            operation=self.operation_object)
        if override is not None:
            # A plugin supplied an alternate conversion,
            # use it instead.
            return override
        else:
            # Fall back to the default arg processing.
            return unpack_cli_arg(self.argument_object, value)

    def _emit(self, name, **kwargs):
        session = self.operation_object.service.session
        return session.emit(name, **kwargs)

    def _emit_first_response(self, name, **kwargs):
        session = self.operation_object.service.session
        return session.emit_first_non_none_response(name, **kwargs)


class ListArgument(CLIArgument):

    def add_to_parser(self, parser):
        cli_name = self.cli_name
        parser.add_argument(cli_name,
                            nargs='*',
                            type=self.cli_type,
                            required=self.required)


class BooleanArgument(CLIArgument):
    """Represent a boolean CLI argument.

    A boolean parameter is specified without a value::

        aws foo bar --enabled

    For cases wher the boolean parameter is required we need to add
    two parameters::

        aws foo bar --enabled
        aws foo bar --no-enabled

    We use the capabilities of the CLIArgument to help achieve this.

    """

    def __init__(self, name, argument_object, operation_object,
                 action='store_true', dest=None, group_name=None,
                 default=None):
        super(BooleanArgument, self).__init__(name, argument_object,
                                              operation_object)
        self._mutex_group = None
        self._action = action
        if dest is None:
            self._destination = self.py_name
        else:
            self._destination = dest
        if group_name is None:
            self._group_name = self.name
        else:
            self._group_name = group_name
        self._default = default

    def add_to_params(self, parameters, value):
        # If a value was explicitly specified (so value is True/False
        # but *not* None) then we add it to the params dict.
        # If the value was not explicitly set (value is None)
        # we don't add it to the params dict.
        if value is not None:
            parameters[self.py_name] = value

    def add_to_arg_table(self, argument_table):
        # Boolean parameters are a bit tricky.  For a single boolean parameter
        # we actually want two CLI params, a --foo, and a --no-foo.  To do this
        # we need to add two entries to the argument table.  So we can add
        # ourself as the positive option (--no), and then create a clone of
        # ourselves for the negative service.  We then insert both into the
        # arg table.
        argument_table[self.name] = self
        negative_name = 'no-%s' % self.name
        negative_version = self.__class__(negative_name, self.argument_object,
                                          self.operation_object,
                                          action='store_false',
                                          dest=self._destination,
                                          group_name=self.group_name)
        argument_table[negative_name] = negative_version

    def add_to_parser(self, parser):
        parser.add_argument(self.cli_name,
                            help=self.documentation,
                            action=self._action,
                            default=self._default,
                            dest=self._destination)

    @property
    def group_name(self):
        return self._group_name

########NEW FILE########
__FILENAME__ = clidocs
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import logging
from bcdoc.docevents import DOC_EVENTS

from awscli import SCALAR_TYPES

LOG = logging.getLogger(__name__)


class CLIDocumentEventHandler(object):

    def __init__(self, help_command):
        self.help_command = help_command
        self.register(help_command.session, help_command.event_class)
        self.help_command.doc.translation_map = self.build_translation_map()
        self._arg_groups = self._build_arg_table_groups(help_command)
        self._documented_arg_groups = []

    def _build_arg_table_groups(self, help_command):
        arg_groups = {}
        for name, arg in help_command.arg_table.items():
            if arg.group_name is not None:
                arg_groups.setdefault(arg.group_name, []).append(arg)
        return arg_groups

    def build_translation_map(self):
        return dict()

    def _map_handlers(self, session, event_class, mapfn):
        for event in DOC_EVENTS:
            event_handler_name = event.replace('-', '_')
            if hasattr(self, event_handler_name):
                event_handler = getattr(self, event_handler_name)
                format_string = DOC_EVENTS[event]
                num_args = len(format_string.split('.')) - 2
                format_args = (event_class,) + ('*',) * num_args
                event_string = event + format_string % format_args
                unique_id = event_class + event_handler_name
                mapfn(event_string, event_handler, unique_id)

    def register(self, session, event_class):
        """
        The default register iterates through all of the
        available document events and looks for a corresponding
        handler method defined in the object.  If it's there, that
        handler method will be registered for the all events of
        that type for the specified ``event_class``.
        """
        self._map_handlers(session, event_class, session.register)

    def unregister(self):
        """
        The default unregister iterates through all of the
        available document events and looks for a corresponding
        handler method defined in the object.  If it's there, that
        handler method will be unregistered for the all events of
        that type for the specified ``event_class``.
        """
        self._map_handlers(self.help_command.session,
                           self.help_command.event_class,
                           self.help_command.session.unregister)

    # These are default doc handlers that apply in the general case.

    def doc_title(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.h1(help_command.name)

    def doc_description(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.h2('Description')
        doc.include_doc_string(help_command.description)
        doc.style.new_paragraph()

    def doc_synopsis_start(self, help_command, **kwargs):
        self._documented_arg_groups = []
        doc = help_command.doc
        doc.style.h2('Synopsis')
        doc.style.start_codeblock()
        doc.writeln('%s' % help_command.name)

    def doc_synopsis_option(self, arg_name, help_command, **kwargs):
        doc = help_command.doc
        argument = help_command.arg_table[arg_name]
        if argument.group_name in self._arg_groups:
            if argument.group_name in self._documented_arg_groups:
                # This arg is already documented so we can move on.
                return
            option_str = ' | '.join(
                [a.cli_name for a in
                 self._arg_groups[argument.group_name]])
            self._documented_arg_groups.append(argument.group_name)
        else:
            option_str = '%s <value>' % argument.cli_name
        if not argument.required:
            option_str = '[%s]' % option_str
        doc.writeln('%s' % option_str)

    def doc_synopsis_end(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.end_codeblock()
        # Reset the documented arg groups for other sections
        # that may document args (the detailed docs following
        # the synopsis).
        self._documented_arg_groups = []

    def doc_options_start(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.h2('Options')
        if not help_command.arg_table:
            doc.write('*None*\n')

    def doc_option(self, arg_name, help_command, **kwargs):
        doc = help_command.doc
        argument = help_command.arg_table[arg_name]
        if argument.group_name in self._arg_groups:
            if argument.group_name in self._documented_arg_groups:
                # This arg is already documented so we can move on.
                return
            name = ' | '.join(
                ['``%s``' % a.cli_name for a in
                 self._arg_groups[argument.group_name]])
            self._documented_arg_groups.append(argument.group_name)
        else:
            name = '``%s``' % argument.cli_name
        doc.write('%s (%s)\n' % (name, argument.cli_type_name))
        doc.style.indent()
        doc.include_doc_string(argument.documentation)
        doc.style.dedent()
        doc.style.new_paragraph()


class ProviderDocumentEventHandler(CLIDocumentEventHandler):

    def doc_synopsis_start(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.h2('Synopsis')
        doc.style.codeblock(help_command.synopsis)
        doc.include_doc_string(help_command.help_usage)

    def doc_synopsis_option(self, arg_name, help_command, **kwargs):
        pass

    def doc_synopsis_end(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.new_paragraph()

    def doc_options_start(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.h2('Options')

    def doc_option(self, arg_name, help_command, **kwargs):
        doc = help_command.doc
        argument = help_command.arg_table[arg_name]
        doc.writeln('``%s`` (%s)' % (argument.cli_name,
                                     argument.cli_type_name))
        doc.include_doc_string(argument.documentation)
        if argument.choices:
            doc.style.start_ul()
            for choice in argument.choices:
                doc.style.li(choice)
            doc.style.end_ul()

    def doc_subitems_start(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.h2('Available Services')
        doc.style.toctree()

    def doc_subitem(self, command_name, help_command, **kwargs):
        doc = help_command.doc
        file_name = '%s/index' % command_name
        doc.style.tocitem(command_name, file_name=file_name)


class ServiceDocumentEventHandler(CLIDocumentEventHandler):

    def build_translation_map(self):
        d = {}
        for op in self.help_command.obj.operations:
            d[op.name] = op.cli_name
        return d

    # A service document has no synopsis.
    def doc_synopsis_start(self, help_command, **kwargs):
        pass

    def doc_synopsis_option(self, arg_name, help_command, **kwargs):
        pass

    def doc_synopsis_end(self, help_command, **kwargs):
        pass

    # A service document has no option section.
    def doc_options_start(self, help_command, **kwargs):
        pass

    def doc_option(self, arg_name, help_command, **kwargs):
        pass

    def doc_option_example(self, arg_name, help_command, **kwargs):
        pass

    def doc_options_end(self, help_command, **kwargs):
        pass

    def doc_title(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.h1(help_command.name)

    def doc_description(self, help_command, **kwargs):
        doc = help_command.doc
        service = help_command.obj
        doc.style.h2('Description')
        doc.include_doc_string(service.documentation)

    def doc_subitems_start(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.h2('Available Commands')
        doc.style.toctree()

    def doc_subitem(self, command_name, help_command, **kwargs):
        doc = help_command.doc
        doc.style.tocitem(command_name)


class OperationDocumentEventHandler(CLIDocumentEventHandler):

    def build_translation_map(self):
        LOG.debug('build_translation_map')
        operation = self.help_command.obj
        d = {}
        for param in operation.params:
            d[param.name] = param.cli_name
        for operation in operation.service.operations:
            d[operation.name] = operation.cli_name
        return d

    def doc_breadcrumbs(self, help_command, event_name, **kwargs):
        doc = help_command.doc
        if doc.target != 'man':
            l = event_name.split('.')
            if len(l) > 1:
                service_name = l[1]
                doc.write('[ ')
                doc.style.ref('aws', '../index')
                doc.write(' . ')
                doc.style.ref(service_name, 'index')
                doc.write(' ]')

    def doc_title(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.h1(help_command.name)

    def doc_description(self, help_command, **kwargs):
        doc = help_command.doc
        operation = help_command.obj
        doc.style.h2('Description')
        doc.include_doc_string(operation.documentation)

    def _json_example_value_name(self, param, include_enum_values=True):
        # If include_enum_values is True, then the valid enum values
        # are included as the sample JSON value.
        if param.type == 'string':
            if hasattr(param, 'enum') and include_enum_values:
                choices = param.enum
                return '|'.join(['"%s"' % c for c in choices])
            else:
                return '"string"'
        elif param.type == 'boolean':
            return 'true|false'
        else:
            return '%s' % param.type

    def _json_example(self, doc, param):
        if param.type == 'list':
            doc.write('[')
            if param.members.type in SCALAR_TYPES:
                doc.write('%s, ...' % self._json_example_value_name(param.members))
            else:
                doc.style.indent()
                doc.style.new_line()
                self._json_example(doc, param.members)
                doc.style.new_line()
                doc.write('...')
                doc.style.dedent()
                doc.style.new_line()
            doc.write(']')
        elif param.type == 'map':
            doc.write('{')
            doc.style.indent()
            key_string = self._json_example_value_name(param.keys)
            doc.write('%s: ' % key_string)
            if param.members.type in SCALAR_TYPES:
                doc.write(self._json_example_value_name(param.members))
            else:
                doc.style.indent()
                self._json_example(doc, param.members)
                doc.style.dedent()
            doc.style.new_line()
            doc.write('...')
            doc.style.dedent()
            doc.write('}')
        elif param.type == 'structure':
            doc.write('{')
            doc.style.indent()
            doc.style.new_line()
            for i, member in enumerate(param.members):
                if member.type in SCALAR_TYPES:
                    doc.write('"%s": %s' % (member.name,
                        self._json_example_value_name(member)))
                elif member.type == 'structure':
                    doc.write('"%s": ' % member.name)
                    self._json_example(doc, member)
                elif member.type == 'map':
                    doc.write('"%s": ' % member.name)
                    self._json_example(doc, member)
                elif member.type == 'list':
                    doc.write('"%s": ' % member.name)
                    self._json_example(doc, member)
                if i < len(param.members) - 1:
                    doc.write(',')
                    doc.style.new_line()
                else:
                    doc.style.dedent()
                    doc.style.new_line()
            doc.write('}')

    def doc_option_example(self, arg_name, help_command, **kwargs):
        doc = help_command.doc
        argument = help_command.arg_table[arg_name]
        if argument.group_name in self._arg_groups:
            if argument.group_name in self._documented_arg_groups:
                # Args with group_names (boolean args) don't
                # need to generate example syntax.
                return
        param = argument.argument_object
        if param and param.example_fn:
            # TODO: bcdoc should not know about shorthand syntax. This
            # should be pulled out into a separate handler in the
            # awscli.customizations package.
            example_syntax = param.example_fn(param)
            if example_syntax is None:
                # If the shorthand syntax returns a value of None,
                # this indicates to us that there is no example
                # needed for this param so we can immediately
                # return.
                return
            doc.style.new_paragraph()
            doc.write('Shorthand Syntax')
            doc.style.start_codeblock()
            for example_line in example_syntax.splitlines():
                doc.writeln(example_line)
            doc.style.end_codeblock()
        if param is not None and param.type == 'list' and \
                param.members.type in SCALAR_TYPES:
            # A list of scalars is special.  While you *can* use
            # JSON ( ["foo", "bar", "baz"] ), you can also just
            # use the argparse behavior of space separated lists.
            # "foo" "bar" "baz".  In fact we don't even want to
            # document the JSON syntax in this case.
            doc.style.new_paragraph()
            doc.write('Syntax')
            doc.style.start_codeblock()
            example_type = self._json_example_value_name(
                param.members, include_enum_values=False)
            doc.write('%s %s ...' % (example_type, example_type))
            if hasattr(param.members, 'enum'):
                # If we have enum values, we can tell the user
                # exactly what valid values they can provide.
                self._write_valid_enums(doc, param.members.enum)
            doc.style.end_codeblock()
            doc.style.new_paragraph()
        elif argument.cli_type_name not in SCALAR_TYPES:
            doc.style.new_paragraph()
            doc.write('JSON Syntax')
            doc.style.start_codeblock()
            self._json_example(doc, param)
            doc.style.end_codeblock()
            doc.style.new_paragraph()

    def _write_valid_enums(self, doc, enum_values):
        doc.style.new_paragraph()
        doc.write("Where valid values are:\n")
        for value in enum_values:
            doc.write("    %s\n" % value)
        doc.write("\n")

    def _doc_member(self, doc, member_name, member):
        docs = member.get('documentation', '')
        if member_name:
            doc.write('%s -> (%s)' % (member_name, member['type']))
        else:
            doc.write('(%s)' % member['type'])
        doc.style.indent()
        doc.style.new_paragraph()
        doc.include_doc_string(docs)
        doc.style.new_paragraph()
        if member['type'] == 'structure':
            for sub_name in member['members']:
                sub_member = member['members'][sub_name]
                self._doc_member(doc, sub_name, sub_member)
        elif member['type'] == 'map':
            keys = member['keys']
            self._doc_member(doc, keys.get('xmlname', 'key'), keys)
            members = member['members']
            self._doc_member(doc, members.get('xmlname', 'value'), members)
        elif member['type'] == 'list':
            self._doc_member(doc, '', member['members'])
        doc.style.dedent()
        doc.style.new_paragraph()

    def doc_output(self, help_command, event_name, **kwargs):
        doc = help_command.doc
        doc.style.h2('Output')
        operation = help_command.obj
        output = operation.output
        if output is None:
            doc.write('None')
        else:
            for member_name in output['members']:
                member = output['members'][member_name]
                self._doc_member(doc, member_name, member)

########NEW FILE########
__FILENAME__ = clidriver
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import sys
import logging

import botocore.session
from botocore import __version__ as botocore_version
from botocore.hooks import HierarchicalEmitter
from botocore import xform_name
from botocore.compat import copy_kwargs, OrderedDict
from botocore.exceptions import NoCredentialsError
from botocore.exceptions import NoRegionError

from awscli import EnvironmentVariables, __version__
from awscli.formatter import get_formatter
from awscli.plugin import load_plugins
from awscli.argparser import MainArgParser
from awscli.argparser import ServiceArgParser
from awscli.argparser import ArgTableArgParser
from awscli.help import ProviderHelpCommand
from awscli.help import ServiceHelpCommand
from awscli.help import OperationHelpCommand
from awscli.arguments import CustomArgument
from awscli.arguments import ListArgument
from awscli.arguments import BooleanArgument
from awscli.arguments import CLIArgument
from awscli.arguments import UnknownArgumentError
from awscli.argprocess import unpack_argument


LOG = logging.getLogger('awscli.clidriver')


def main():
    driver = create_clidriver()
    return driver.main()


def create_clidriver():
    emitter = HierarchicalEmitter()
    session = botocore.session.Session(EnvironmentVariables, emitter)
    _set_user_agent_for_session(session)
    load_plugins(session.full_config.get('plugins', {}),
                 event_hooks=emitter)
    driver = CLIDriver(session=session)
    return driver


def _set_user_agent_for_session(session):
    session.user_agent_name = 'aws-cli'
    session.user_agent_version = __version__


class CLIDriver(object):

    def __init__(self, session=None):
        if session is None:
            self.session = botocore.session.get_session(EnvironmentVariables)
            _set_user_agent_for_session(self.session)
        else:
            self.session = session
        self._cli_data = None
        self._command_table = None
        self._argument_table = None

    def _get_cli_data(self):
        # Not crazy about this but the data in here is needed in
        # several places (e.g. MainArgParser, ProviderHelp) so
        # we load it here once.
        if self._cli_data is None:
            self._cli_data = self.session.get_data('cli')
        return self._cli_data

    def _get_command_table(self):
        if self._command_table is None:
            self._command_table = self._build_command_table()
        return self._command_table

    def _get_argument_table(self):
        if self._argument_table is None:
            self._argument_table = self._build_argument_table()
        return self._argument_table

    def _build_command_table(self):
        """
        Create the main parser to handle the global arguments.

        :rtype: ``argparser.ArgumentParser``
        :return: The parser object

        """
        command_table = self._build_builtin_commands(self.session)
        self.session.emit('building-command-table.main',
                          command_table=command_table,
                          session=self.session)
        return command_table

    def _build_builtin_commands(self, session):
        commands = OrderedDict()
        services = session.get_available_services()
        for service_name in services:
            commands[service_name] = ServiceCommand(cli_name=service_name,
                                                    session=self.session,
                                                    service_name=service_name)
        return commands

    def _build_argument_table(self):
        argument_table = OrderedDict()
        cli_data = self._get_cli_data()
        cli_arguments = cli_data.get('options', None)
        for option in cli_arguments:
            option_params = copy_kwargs(cli_arguments[option])
            # Special case the 'choices' param.  Allows choices
            # to reference a variable from the session.
            if 'choices' in option_params:
                choices = option_params['choices']
                if not isinstance(choices, list):
                    # Assume it's a reference like
                    # "{provider}/_regions", so first resolve
                    # the provider.
                    provider = self.session.get_config_variable('provider')
                    # The grab the var from the session
                    choices_path = choices.format(provider=provider)
                    choices = list(self.session.get_data(choices_path))
                option_params['choices'] = choices
            argument_object = self._create_argument_object(option,
                                                           option_params)
            argument_object.add_to_arg_table(argument_table)
        # Then the final step is to send out an event so handlers
        # can add extra arguments or modify existing arguments.
        self.session.emit('building-top-level-params',
                          argument_table=argument_table)
        return argument_table

    def _create_argument_object(self, option_name, option_params):
        return CustomArgument(
            option_name, help_text=option_params.get('help', ''),
            dest=option_params.get('dest'),default=option_params.get('default'),
            action=option_params.get('action'),
            required=option_params.get('required'),
            choices=option_params.get('choices'))

    def create_help_command(self):
        cli_data = self._get_cli_data()
        return ProviderHelpCommand(self.session, self._get_command_table(),
                                   self._get_argument_table(),
                                   cli_data.get('description', None),
                                   cli_data.get('synopsis', None),
                                   cli_data.get('help_usage', None))

    def _create_parser(self):
        # Also add a 'help' command.
        command_table = self._get_command_table()
        command_table['help'] = self.create_help_command()
        cli_data = self._get_cli_data()
        parser = MainArgParser(
            command_table, self.session.user_agent(),
            cli_data.get('description', None),
            cli_data.get('synopsis', None),
            self._get_argument_table())
        return parser

    def main(self, args=None):
        """

        :param args: List of arguments, with the 'aws' removed.  For example,
            the command "aws s3 list-objects --bucket foo" will have an
            args list of ``['s3', 'list-objects', '--bucket', 'foo']``.

        """
        if args is None:
            args = sys.argv[1:]
        parser = self._create_parser()
        command_table = self._get_command_table()
        parsed_args, remaining = parser.parse_known_args(args)
        self._handle_top_level_args(parsed_args)
        try:
            return command_table[parsed_args.command](remaining, parsed_args)
        except UnknownArgumentError as e:
            sys.stderr.write(str(e) + '\n')
            return 255
        except NoRegionError as e:
            msg = ('%s You can also configure your region by running '
                   '"aws configure".' % e)
            self._show_error(msg)
            return 255
        except NoCredentialsError as e:
            msg = ('%s. You can configure credentials by running '
                   '"aws configure".' % e)
            self._show_error(msg)
            return 255
        except Exception as e:
            LOG.debug("Exception caught in main()", exc_info=True)
            LOG.debug("Exiting with rc 255")
            sys.stderr.write("\n")
            sys.stderr.write("%s\n" % e)
            return 255

    def _show_error(self, msg):
        LOG.debug(msg, exc_info=True)
        sys.stderr.write(msg)
        sys.stderr.write('\n')

    def _handle_top_level_args(self, args):
        self.session.emit(
            'top-level-args-parsed', parsed_args=args, session=self.session)
        if args.profile:
            self.session.profile = args.profile
        if args.debug:
            # TODO:
            # Unfortunately, by setting debug mode here, we miss out
            # on all of the debug events prior to this such as the
            # loading of plugins, etc.
            self.session.set_debug_logger(logger_name='botocore')
            self.session.set_debug_logger(logger_name='awscli')
            LOG.debug("CLI version: %s, botocore version: %s",
                      self.session.user_agent(),
                      botocore_version)
        else:
            self.session.set_stream_logger(logger_name='awscli',
                                           log_level=logging.ERROR)


class CLICommand(object):
    """Interface for a CLI command.

    This class represents a top level CLI command
    (``aws ec2``, ``aws s3``, ``aws config``).

    """

    @property
    def name(self):
        # Subclasses must implement a name.
        raise NotImplementedError("name")

    @name.setter
    def name(self, value):
        # Subclasses must implement setting/changing the cmd name.
        raise NotImplementedError("name")

    def __call__(self, args, parsed_globals):
        """Invoke CLI operation.

        :type args: str
        :param args: The remaining command line args.

        :type parsed_globals: ``argparse.Namespace``
        :param parsed_globals: The parsed arguments so far.

        :rtype: int
        :return: The return code of the operation.  This will be used
            as the RC code for the ``aws`` process.

        """
        # Subclasses are expected to implement this method.
        pass

    def create_help_command(self):
        # Subclasses are expected to implement this method if they want
        # help docs.
        return None

    @property
    def arg_table(self):
        return {}


class ServiceCommand(CLICommand):
    """A service command for the CLI.

    For example, ``aws ec2 ...`` we'd create a ServiceCommand
    object that represents the ec2 service.

    """

    def __init__(self, cli_name, session, service_name=None):
        # The cli_name is the name the user types, the name we show
        # in doc, etc.
        # The service_name is the name we used internally with botocore.
        # For example, we have the 's3api' as the cli_name for the service
        # but this is actually bound to the 's3' service name in botocore,
        # i.e. we load s3.json from the botocore data dir.  Most of
        # the time these are the same thing but in the case of renames,
        # we want users/external things to be able to rename the cli name
        # but *not* the service name, as this has to be exactly what
        # botocore expects.
        self._name = cli_name
        self.session = session
        self._command_table = None
        self._service_object = None
        if service_name is None:
            # Then default to using the cli name.
            self._service_name = cli_name
        else:
            self._service_name = service_name

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, value):
        self._name = value

    def _get_command_table(self):
        if self._command_table is None:
            self._command_table = self._create_command_table()
        return self._command_table

    def _get_service_object(self):
        if self._service_object is None:
            self._service_object = self.session.get_service(self._service_name)
        return self._service_object

    def __call__(self, args, parsed_globals):
        # Once we know we're trying to call a service for this operation
        # we can go ahead and create the parser for it.  We
        # can also grab the Service object from botocore.
        service_parser = self._create_parser()
        parsed_args, remaining = service_parser.parse_known_args(args)
        command_table = self._get_command_table()
        return command_table[parsed_args.operation](remaining, parsed_globals)

    def _create_command_table(self):
        command_table = OrderedDict()
        service_object = self._get_service_object()
        for operation_object in service_object.operations:
            cli_name = xform_name(operation_object.name, '-')
            command_table[cli_name] = ServiceOperation(
                name=cli_name,
                parent_name=self._name,
                operation_object=operation_object,
                operation_caller=CLIOperationCaller(self.session),
                service_object=service_object)
        self.session.emit('building-command-table.%s' % self._name,
                          command_table=command_table,
                          session=self.session)
        return command_table

    def create_help_command(self):
        command_table = self._get_command_table()
        service_object = self._get_service_object()
        return ServiceHelpCommand(session=self.session,
                                  obj=service_object,
                                  command_table=command_table,
                                  arg_table=None,
                                  event_class='Operation',
                                  name=self._name)

    def _create_parser(self):
        command_table = self._get_command_table()
        # Also add a 'help' command.
        command_table['help'] = self.create_help_command()
        return ServiceArgParser(
            operations_table=command_table, service_name=self._name)


class ServiceOperation(object):
    """A single operation of a service.

    This class represents a single operation for a service, for
    example ``ec2.DescribeInstances``.

    """

    ARG_TYPES = {
        'list': ListArgument,
        'boolean': BooleanArgument,
    }
    DEFAULT_ARG_CLASS = CLIArgument

    def __init__(self, name, parent_name, operation_object, operation_caller,
                 service_object):
        """

        :type name: str
        :param name: The name of the operation/subcommand.

        :type parent_name: str
        :param parent_name: The name of the parent command.

        :type operation_object: ``botocore.operation.Operation``
        :param operation_object: The operation associated with this subcommand.

        :type operation_caller: ``CLIOperationCaller``
        :param operation_caller: An object that can properly call the
            operation.

        :type service_object: ``botocore.service.Service``
        :param service_object: The service associated wtih the object.

        """
        self._arg_table = None
        self._name = name
        # These is used so we can figure out what the proper event
        # name should be <parent name>.<name>.
        self._parent_name = parent_name
        self._operation_object = operation_object
        self._operation_caller = operation_caller
        self._service_object = service_object

    @property
    def arg_table(self):
        if self._arg_table is None:
            self._arg_table = self._create_argument_table()
        return self._arg_table

    def __call__(self, args, parsed_globals):
        # Once we know we're trying to call a particular operation
        # of a service we can go ahead and load the parameters.
        operation_parser = self._create_operation_parser(self.arg_table)
        self._add_help(operation_parser)
        parsed_args, remaining = operation_parser.parse_known_args(args)
        if parsed_args.help == 'help':
            op_help = self.create_help_command()
            return op_help(parsed_args, parsed_globals)
        elif parsed_args.help:
            remaining.append(parsed_args.help)
        if remaining:
            raise UnknownArgumentError(
                "Unknown options: %s" % ', '.join(remaining))
        event = 'operation-args-parsed.%s.%s' % (self._parent_name,
                                                 self._name)
        self._emit(event, parsed_args=parsed_args,
                   parsed_globals=parsed_globals)
        call_parameters = self._build_call_parameters(parsed_args,
                                                      self.arg_table)
        return self._operation_caller.invoke(
            self._operation_object, call_parameters, parsed_globals)

    def create_help_command(self):
        return OperationHelpCommand(
            self._service_object.session, self._service_object,
            self._operation_object, arg_table=self.arg_table,
            name=self._name, event_class=self._parent_name)

    def _add_help(self, parser):
        # The 'help' output is processed a little differently from
        # the provider/operation help because the arg_table has
        # CLIArguments for values.
        parser.add_argument('help', nargs='?')

    def _build_call_parameters(self, args, arg_table):
        # We need to convert the args specified on the command
        # line as valid **kwargs we can hand to botocore.
        service_params = {}
        # args is an argparse.Namespace object so we're using vars()
        # so we can iterate over the parsed key/values.
        parsed_args = vars(args)
        for arg_name, arg_object in arg_table.items():
            py_name = arg_object.py_name
            if py_name in parsed_args:
                value = parsed_args[py_name]
                value = self._unpack_arg(arg_object, value)
                arg_object.add_to_params(service_params, value)
        return service_params

    def _unpack_arg(self, arg_object, value):
        # Unpacks a commandline argument into a Python value by firing the
        # load-cli-arg.service-name.operation-name event.
        session = self._service_object.session
        service_name = self._service_object.endpoint_prefix
        operation_name = xform_name(self._operation_object.name, '-')

        param = arg_object
        if hasattr(param, 'argument_object') and param.argument_object:
            param = param.argument_object

        return unpack_argument(session, service_name, operation_name,
                               param, value)

    def _create_argument_table(self):
        argument_table = OrderedDict()
        # Arguments are treated a differently than service and
        # operations.  Instead of doing a get_parameter() we just
        # load all the parameter objects up front for the operation.
        # We could potentially do the same thing as service/operations
        # but botocore already builds all the parameter objects
        # when calling an operation so we'd have to optimize that first
        # before using get_parameter() in the cli would be advantageous
        for argument in self._operation_object.params:
            cli_arg_name = argument.cli_name[2:]
            arg_class = self.ARG_TYPES.get(argument.type,
                                           self.DEFAULT_ARG_CLASS)
            arg_object = arg_class(cli_arg_name, argument,
                                   self._operation_object)
            arg_object.add_to_arg_table(argument_table)
        LOG.debug(argument_table)
        self._emit('building-argument-table.%s.%s' % (self._parent_name,
                                                      self._name),
                   operation=self._operation_object,
                   argument_table=argument_table)
        return argument_table

    def _emit(self, name, **kwargs):
        session = self._service_object.session
        return session.emit(name, **kwargs)

    def _create_operation_parser(self, arg_table):
        parser = ArgTableArgParser(arg_table)
        return parser


class CLIOperationCaller(object):
    """
    Call an AWS operation and format the response.

    This class handles the non-error path.  If an HTTP error occurs
    on the call to the service operation, it will be detected and
    handled by the :class:`awscli.errorhandler.ErrorHandler` which
    is registered on the ``after-call`` event.
    """

    def __init__(self, session):
        self._session = session

    def invoke(self, operation_object, parameters, parsed_globals):
        # We could get an error from get_endpoint() about not having
        # a region configured.  Before this happens we want to check
        # for credentials so we can give a good error message.
        if not self._session.get_credentials():
            raise NoCredentialsError()
        endpoint = operation_object.service.get_endpoint(
            region_name=parsed_globals.region,
            endpoint_url=parsed_globals.endpoint_url,
            verify=parsed_globals.verify_ssl)
        if operation_object.can_paginate and parsed_globals.paginate:
            pages = operation_object.paginate(endpoint, **parameters)
            self._display_response(operation_object, pages,
                                   parsed_globals)
        else:
            http_response, response_data = operation_object.call(endpoint,
                                                                 **parameters)
            self._display_response(operation_object, response_data,
                                   parsed_globals)
        return 0

    def _display_response(self, operation, response, args):
        output = args.output
        if output is None:
            output = self._session.get_config_variable('output')
        formatter = get_formatter(output, args)
        formatter(operation, response)

########NEW FILE########
__FILENAME__ = compat
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.

# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at

#     http://aws.amazon.com/apache2.0/

# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import sys
import six

if six.PY3:
    import locale

    def get_stdout_text_writer():
        return sys.stdout

    def compat_open(filename, mode='r', encoding=None):
        """Back-port open() that accepts an encoding argument.

        In python3 this uses the built in open() and in python2 this
        uses the io.open() function.

        If the file is not being opened in binary mode, then we'll
        use locale.getpreferredencoding() to find the preferred
        encoding.

        """
        if 'b' not in mode:
            encoding = locale.getpreferredencoding()
        return open(filename, mode, encoding=encoding)

else:
    import codecs
    import locale
    import io

    def get_stdout_text_writer():
        # In python3, all the sys.stdout/sys.stderr streams are in text
        # mode.  This means they expect unicode, and will encode the
        # unicode automatically before actually writing to stdout/stderr.
        # In python2, that's not the case.  In order to provide a consistent
        # interface, we can create a wrapper around sys.stdout that will take
        # unicode, and automatically encode it to the preferred encoding.
        # That way consumers can just call get_stdout_text_writer() and write
        # unicode to the returned stream.  Note that get_stdout_text_writer
        # just returns sys.stdout in the PY3 section above because python3
        # handles this.
        return codecs.getwriter(locale.getpreferredencoding())(sys.stdout)

    def compat_open(filename, mode='r', encoding=None):
        # See docstring for compat_open in the PY3 section above.
        if 'b' not in mode:
            encoding = locale.getpreferredencoding()
        return io.open(filename, mode, encoding=encoding)

########NEW FILE########
__FILENAME__ = completer
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at

#     http://aws.amazon.com/apache2.0/

# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import awscli.clidriver
import sys
import logging
import copy

LOG = logging.getLogger(__name__)


class Completer(object):

    def __init__(self):
        self.driver = awscli.clidriver.create_clidriver()
        self.main_hc = self.driver.create_help_command()
        self.main_options = self._documented(self.main_hc.arg_table)
        self.cmdline = None
        self.point = None
        self.command_hc = None
        self.subcommand_hc = None
        self.command_name = None
        self.subcommand_name = None
        self.current_word = None
        self.previous_word = None
        self.non_options = None

    def _complete_option(self, option_name):
        if option_name == '--region':
            return self.driver.session.get_data('aws/_regions').keys()
        if option_name == '--endpoint-url':
            return []
        if option_name == '--output':
            cli_data = self.driver.session.get_data('cli')
            return cli_data['options']['output']['choices']
        if option_name == '--profile':
            return self.driver.session.available_profiles
        return []

    def _complete_provider(self):
        retval = []
        if self.current_word.startswith('-'):
            cw = self.current_word.lstrip('-')
            l = ['--' + n for n in self.main_options
                 if n.startswith(cw)]
            retval = l
        elif self.current_word == 'aws':
            retval = self._documented(self.main_hc.command_table)
        else:
            # Otherwise, see if they have entered a partial command name
            retval = self._documented(self.main_hc.command_table,
                                      startswith=self.current_word)
        return retval

    def _complete_command(self):
        retval = []
        if self.current_word == self.command_name:
            if self.command_hc:
                retval = self._documented(self.command_hc.command_table)
        elif self.current_word.startswith('-'):
            retval = self._find_possible_options()
        else:
            # See if they have entered a partial command name
            if self.command_hc:
                retval = self._documented(self.command_hc.command_table,
                                          startswith=self.current_word)
        return retval

    def _documented(self, table, startswith=None):
        names = []
        for key, command in table.items():
            if getattr(command, '_UNDOCUMENTED', False):
                # Don't tab complete undocumented commands/params
                continue
            if startswith is not None and not key.startswith(startswith):
                continue
            names.append(key)
        return names

    def _complete_subcommand(self):
        retval = []
        if self.current_word == self.subcommand_name:
            retval = []
        elif self.current_word.startswith('-'):
            retval = self._find_possible_options()
        return retval

    def _find_possible_options(self):
        all_options = copy.copy(self.main_options)
        if self.subcommand_hc:
            all_options = all_options + self._documented(self.subcommand_hc.arg_table)
        for opt in self.options:
            # Look thru list of options on cmdline. If there are
            # options that have already been specified and they are
            # not the current word, remove them from list of possibles.
            if opt != self.current_word:
                stripped_opt = opt.lstrip('-')
                if stripped_opt in all_options:
                    all_options.remove(stripped_opt)
        cw = self.current_word.lstrip('-')
        possibles = ['--' + n for n in all_options if n.startswith(cw)]
        if len(possibles) == 1 and possibles[0] == self.current_word:
            return self._complete_option(possibles[0])
        return possibles

    def _process_command_line(self):
        # Process the command line and try to find:
        #     - command_name
        #     - subcommand_name
        #     - words
        #     - current_word
        #     - previous_word
        #     - non_options
        #     - options
        self.command_name = None
        self.subcommand_name = None
        self.words = self.cmdline[0:self.point].split()
        self.current_word = self.words[-1]
        if len(self.words) >= 2:
            self.previous_word = self.words[-2]
        else:
            self.previous_word = None
        self.non_options = [w for w in self.words if not w.startswith('-')]
        self.options = [w for w in self.words if w.startswith('-')]
        # Look for a command name in the non_options
        for w in self.non_options:
            if w in self.main_hc.command_table:
                self.command_name = w
                cmd_obj = self.main_hc.command_table[self.command_name]
                self.command_hc = cmd_obj.create_help_command()
                if self.command_hc and self.command_hc.command_table:
                    # Look for subcommand name
                    for w in self.non_options:
                        if w in self.command_hc.command_table:
                            self.subcommand_name = w
                            cmd_obj = self.command_hc.command_table[self.subcommand_name]
                            self.subcommand_hc = cmd_obj.create_help_command()
                            break
                break

    def complete(self, cmdline, point):
        self.cmdline = cmdline
        self.command_name = None
        if point is None:
            point = len(cmdline)
        self.point = point
        self._process_command_line()
        if not self.command_name:
            # If we didn't find any command names in the cmdline
            # lets try to complete provider options
            return self._complete_provider()
        if self.command_name and not self.subcommand_name:
            return self._complete_command()
        return self._complete_subcommand()


def complete(cmdline, point):
    choices = Completer().complete(cmdline, point)
    print(' \n'.join(choices))


if __name__ == '__main__':
    if len(sys.argv) == 3:
        cmdline = sys.argv[1]
        point = int(sys.argv[2])
    elif len(sys.argv) == 2:
        cmdline = sys.argv[1]
    else:
        print('usage: %s <cmdline> <point>' % sys.argv[0])
        sys.exit(1)
    print(complete(cmdline, point))

########NEW FILE########
__FILENAME__ = addexamples
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
Add authored examples to MAN and HTML documentation
---------------------------------------------------

This customization allows authored examples in ReST format to be
inserted into the generated help for an Operation.  To get this to
work you need to:

* Register the ``add_examples`` function below with the
  ``doc-examples.*.*`` event.
* Create a file containing ReST format fragment with the examples.
  The file needs to be created in the ``examples/<service_name>``
  directory and needs to be named ``<service_name>-<op_name>.rst``.
  For example, ``examples/ec2/ec2-create-key-pair.rst``.

"""
import os
import logging


LOG = logging.getLogger(__name__)


def add_examples(help_command, **kwargs):
    doc_path = os.path.join(
        os.path.dirname(
            os.path.dirname(
                os.path.abspath(__file__))), 'examples')
    doc_path = os.path.join(doc_path,
                            help_command.event_class)
    file_name = '%s.rst' % (help_command.name)
    doc_path = os.path.join(doc_path, file_name)
    LOG.debug("Looking for example file at: %s", doc_path)
    if os.path.isfile(doc_path):
        help_command.doc.style.h2('Examples')
        fp = open(doc_path)
        for line in fp.readlines():
            help_command.doc.write(line)

########NEW FILE########
__FILENAME__ = argrename
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
"""

from awscli.customizations import utils


ARGUMENT_RENAMES = {
    # Mapping of original arg to renamed arg.
    # The key is <service>.<operation>.argname
    # The first part of the key is used for event registration
    # so if you wanted to rename something for an entire service you
    # could say 'ec2.*.dry-run': 'renamed-arg-name', or if you wanted
    # to rename across all services you could say '*.*.dry-run': 'new-name'.
    'ec2.create-image.no-no-reboot': 'reboot',
    'ec2.*.no-egress': 'ingress',
    'ec2.*.no-disable-api-termination': 'enable-api-termination',
    'opsworks.*.region': 'stack-region',
    'elastictranscoder.*.output': 'job-output',
    'swf.register-activity-type.version': 'activity-version',
    'swf.register-workflow-type.version': 'workflow-version',
    'datapipeline.*.query': 'objects-query',
}


def register_arg_renames(cli):
    for original, new_name in ARGUMENT_RENAMES.items():
        event_portion, original_arg_name = original.rsplit('.', 1)
        cli.register('building-argument-table.%s' % event_portion,
                     rename_arg(original_arg_name, new_name))


def rename_arg(original_arg_name, new_name):
    def _rename_arg(argument_table, **kwargs):
        if original_arg_name in argument_table:
            utils.rename_argument(argument_table, original_arg_name, new_name)
    return _rename_arg

########NEW FILE########
__FILENAME__ = cloudsearch
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

import logging

from awscli.customizations.flatten import FlattenArguments, SEP
from botocore.compat import OrderedDict

LOG = logging.getLogger(__name__)

DEFAULT_VALUE_TYPE_MAP = {
    'Int': int,
    'Double': float,
    'IntArray': int,
    'DoubleArray': float
}


def index_hydrate(params, container, cli_type, key, value):
    """
    Hydrate an index-field option value to construct something like::

        {
            'index_field': {
                'DoubleOptions': {
                    'DefaultValue': 0.0
                }
            }
        }
    """
    if 'index_field' not in params:
        params['index_field'] = {}

    if 'IndexFieldType' not in params['index_field']:
        raise RuntimeError('You must pass the --type option!')

    # Find the type and transform it for the type options field name
    # E.g: int-array => IntArray
    _type = params['index_field']['IndexFieldType']
    _type = ''.join([i.capitalize() for i in _type.split('-')])

    # Transform string value to the correct type?
    if key.split(SEP)[-1] == 'DefaultValue':
        value = DEFAULT_VALUE_TYPE_MAP.get(_type, lambda x: x)(value)

    # Set the proper options field
    if _type + 'Options' not in params['index_field']:
        params['index_field'][_type + 'Options'] = {}

    params['index_field'][_type + 'Options'][key.split(SEP)[-1]] = value


FLATTEN_CONFIG = {
    "define-expression": {
        "expression": {
            "keep": False,
            "flatten": OrderedDict([
                # Order is crucial here!  We're
                # flattening ExpressionValue to be "expression",
                # but this is the name ("expression") of the our parent
                # key, the top level nested param.
                ("ExpressionName", {"name": "name"}),
                ("ExpressionValue", {"name": "expression"}),]),
        }
    },
    "define-index-field": {
        "index-field": {
            "keep": False,
            # We use an ordered dict because `type` needs to be parsed before
            # any of the <X>Options values.
            "flatten": OrderedDict([
                ("IndexFieldName", {"name": "name"}),
                ("IndexFieldType", {"name": "type"}),
                ("IntOptions.DefaultValue", {"name": "default-value",
                                             "type": "string",
                                             "hydrate": index_hydrate}),
                ("IntOptions.FacetEnabled", { "name": "facet-enabled",
                                             "hydrate": index_hydrate }),
                ("IntOptions.SearchEnabled", {"name": "search-enabled",
                                              "hydrate": index_hydrate}),
                ("IntOptions.ReturnEnabled", {"name": "return-enabled",
                                              "hydrate": index_hydrate}),
                ("IntOptions.SortEnabled", {"name": "sort-enabled",
                                            "hydrate": index_hydrate}),
                ("TextOptions.HighlightEnabled", {"name": "highlight-enabled",
                                                  "hydrate": index_hydrate}),
                ("TextOptions.AnalysisScheme", {"name": "analysis-scheme",
                                                "hydrate": index_hydrate})
            ])
        }
    }
}


def initialize(cli):
    """
    The entry point for CloudSearch customizations.
    """
    flattened = FlattenArguments('cloudsearch', FLATTEN_CONFIG)
    flattened.register(cli)

########NEW FILE########
__FILENAME__ = cloudtrail
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import json
import logging
import sys

from awscli.customizations.commands import BasicCommand
from awscli.customizations.service import Service
from botocore.vendored import requests


LOG = logging.getLogger(__name__)
S3_POLICY_TEMPLATE = 'policy/S3/AWSCloudTrail-S3BucketPolicy-2013-11-01.json'
SNS_POLICY_TEMPLATE = 'policy/SNS/AWSCloudTrail-SnsTopicPolicy-2013-11-01.json'


def initialize(cli):
    """
    The entry point for CloudTrail high level commands.
    """
    cli.register('building-command-table.cloudtrail', inject_commands)


def inject_commands(command_table, session, **kwargs):
    """
    Called when the CloudTrail command table is being built. Used to inject new
    high level commands into the command list. These high level commands
    must not collide with existing low-level API call names.
    """
    command_table['create-subscription'] = CloudTrailSubscribe(session)
    command_table['update-subscription'] = CloudTrailUpdate(session)


class CloudTrailSubscribe(BasicCommand):
    """
    Subscribe/update a user account to CloudTrail, creating the required S3 bucket,
    the optional SNS topic, and starting the CloudTrail monitoring and logging.
    """
    NAME = 'create-subscription'
    DESCRIPTION = ('Creates and configures the AWS resources necessary to use'
                   ' CloudTrail, creates a trail using those resources, and '
                   'turns on logging.')
    SYNOPSIS = ('aws cloudtrail create-subscription'
                ' (--s3-use-bucket|--s3-new-bucket) bucket-name'
                ' [--sns-new-topic topic-name]\n')

    ARG_TABLE = [
        {'name': 'name', 'required': True, 'help_text': 'Cloudtrail name'},
        {'name': 's3-new-bucket',
         'help_text': 'Create a new S3 bucket with this name'},
        {'name': 's3-use-bucket',
         'help_text': 'Use an existing S3 bucket with this name'},
        {'name': 's3-prefix', 'help_text': 'S3 object prefix'},
        {'name': 'sns-new-topic',
         'help_text': 'Create a new SNS topic with this name'},
        {'name': 'include-global-service-events',
         'help_text': 'Whether to include global service events'},
        {'name': 's3-custom-policy',
         'help_text': 'Optional URL to a custom S3 policy template'},
        {'name': 'sns-custom-policy',
         'help_text': 'Optional URL to a custom SNS policy template'}
    ]

    UPDATE = False

    def _run_main(self, args, parsed_globals):
        endpoint_args = {
            'region_name': None,
            'endpoint_url': None
        }
        if 'region' in parsed_globals:
            endpoint_args['region_name'] = parsed_globals.region
        if 'endpoint_url' in parsed_globals:
            endpoint_args['endpoint_url'] = parsed_globals.endpoint_url

        # Initialize services
        LOG.debug('Initializing S3, SNS and CloudTrail...')
        self.iam = Service('iam', session=self._session)
        self.s3 = Service('s3', endpoint_args['region_name'],
                          session=self._session)
        self.sns = Service('sns', endpoint_args['region_name'],
                           session=self._session)
        self.cloudtrail = Service('cloudtrail', endpoint_args=endpoint_args,
                                  session=self._session)

        # Run the command and report success
        self._call(args, parsed_globals)

        return 1

    def _call(self, options, parsed_globals):
        """
        Run the command. Calls various services based on input options and
        outputs the final CloudTrail configuration.
        """
        gse = options.include_global_service_events
        if gse:
            if gse.lower() == 'true':
                gse = True
            elif gse.lower() == 'false':
                gse = False
            else:
                raise ValueError('You must pass either true or false to'
                                 ' --include-global-service-events.')

        bucket = options.s3_use_bucket

        if options.s3_new_bucket:
            bucket = options.s3_new_bucket

            if self.UPDATE and options.s3_prefix is None:
                # Prefix was not passed and this is updating the S3 bucket,
                # so let's find the existing prefix and use that if possible
                res = self.cloudtrail.DescribeTrails(
                    trail_name_list=[options.name])
                trail_info = res['trailList'][0]

                if 'S3KeyPrefix' in trail_info:
                    LOG.debug('Setting S3 prefix to {0}'.format(
                        trail_info['S3KeyPrefix']))
                    options.s3_prefix = trail_info['S3KeyPrefix']

            self.setup_new_bucket(bucket, options.s3_prefix,
                                  options.s3_custom_policy)
        elif not bucket and not self.UPDATE:
            # No bucket was passed for creation.
            raise ValueError('You must pass either --s3-use-bucket or'
                             ' --s3-new-bucket to create.')

        if options.sns_new_topic:
            try:
                topic_result = self.setup_new_topic(options.sns_new_topic,
                                                    options.sns_custom_policy)
            except Exception:
                # Roll back any S3 bucket creation
                if options.s3_new_bucket:
                    self.s3.DeleteBucket(bucket=options.s3_new_bucket)
                raise

        try:
            cloudtrail_config = self.upsert_cloudtrail_config(
                options.name,
                bucket,
                options.s3_prefix,
                options.sns_new_topic,
                gse
            )
        except Exception:
            # Roll back any S3 bucket / SNS topic creations
            if options.s3_new_bucket:
                self.s3.DeleteBucket(bucket=options.s3_new_bucket)
            if options.sns_new_topic:
                self.sns.DeleteTopic(topic_arn=topic_result['TopicArn'])
            raise

        sys.stdout.write('CloudTrail configuration:\n{config}\n'.format(
            config=json.dumps(cloudtrail_config, indent=2)))

        if not self.UPDATE:
            # If the configure call command above completes then this should
            # have a really high chance of also completing
            self.start_cloudtrail(options.name)

            sys.stdout.write(
                'Logs will be delivered to {bucket}:{prefix}\n'.format(
                    bucket=bucket, prefix=options.s3_prefix or ''))

    def setup_new_bucket(self, bucket, prefix, policy_url=None):
        """
        Creates a new S3 bucket with an appropriate policy to let CloudTrail
        write to the prefix path.
        """
        sys.stdout.write(
            'Setting up new S3 bucket {bucket}...\n'.format(bucket=bucket))

        # Who am I?
        response = self.iam.GetUser()
        account_id = response['User']['Arn'].split(':')[4]

        # Clean up the prefix - it requires a trailing slash if set
        if prefix and not prefix.endswith('/'):
            prefix += '/'

        # Fetch policy data from S3 or a custom URL
        if policy_url:
            policy = requests.get(policy_url).text
        else:
            data = self.s3.GetObject(bucket='awscloudtrail',
                                     key=S3_POLICY_TEMPLATE)
            policy = data['Body'].read().decode('utf-8')

        policy = policy.replace('<BucketName>', bucket)\
                       .replace('<CustomerAccountID>', account_id)

        if '<Prefix>/' in policy:
            policy = policy.replace('<Prefix>/', prefix or '')
        else:
            policy = policy.replace('<Prefix>', prefix or '')

        LOG.debug('Bucket policy:\n{0}'.format(policy))

        # Make sure bucket doesn't already exist
        # Warn but do not fail if ListBucket permissions
        # are missing from the IAM role
        try:
            buckets = self.s3.ListBuckets()['Buckets']
        except Exception:
            buckets = []
            LOG.warn('Unable to list buckets, continuing...')

        if [b for b in buckets if b['Name'] == bucket]:
            raise Exception('Bucket {bucket} already exists.'.format(
                bucket=bucket))

        data = self.s3.CreateBucket(bucket=bucket)

        try:
            self.s3.PutBucketPolicy(bucket=bucket, policy=policy)
        except Exception:
            # Roll back bucket creation
            self.s3.DeleteBucket(bucket=bucket)
            raise

        return data

    def setup_new_topic(self, topic, policy_url=None):
        """
        Creates a new SNS topic with an appropriate policy to let CloudTrail
        post messages to the topic.
        """
        sys.stdout.write(
            'Setting up new SNS topic {topic}...\n'.format(topic=topic))

        # Who am I?
        response = self.iam.GetUser()
        account_id = response['User']['Arn'].split(':')[4]

        # Make sure topic doesn't already exist
        # Warn but do not fail if ListTopics permissions
        # are missing from the IAM role?
        try:
            topics = self.sns.ListTopics()['Topics']
        except Exception:
            topics = []
            LOG.warn('Unable to list topics, continuing...')

        if [t for t in topics if t['TopicArn'].split(':')[-1] == topic]:
            raise Exception('Topic {topic} already exists.'.format(
                topic=topic))

        region = self.sns.endpoint.region_name

        # Get the SNS topic policy information to allow CloudTrail
        # write-access.
        if policy_url:
            policy = requests.get(policy_url).text
        else:
            data = self.s3.GetObject(bucket='awscloudtrail',
                                     key=SNS_POLICY_TEMPLATE)
            policy = data['Body'].read().decode('utf-8')

        policy = policy.replace('<Region>', region)\
                       .replace('<SNSTopicOwnerAccountId>', account_id)\
                       .replace('<SNSTopicName>', topic)

        topic_result = self.sns.CreateTopic(name=topic)

        try:
            # Merge any existing topic policy with our new policy statements
            topic_attr = self.sns.GetTopicAttributes(
                topic_arn=topic_result['TopicArn'])

            policy = self.merge_sns_policy(topic_attr['Attributes']['Policy'],
                                           policy)

            LOG.debug('Topic policy:\n{0}'.format(policy))

            # Set the topic policy
            self.sns.SetTopicAttributes(topic_arn=topic_result['TopicArn'],
                                        attribute_name='Policy',
                                        attribute_value=policy)
        except Exception:
            # Roll back topic creation
            self.sns.DeleteTopic(topic_arn=topic_result['TopicArn'])
            raise

        return topic_result

    def merge_sns_policy(self, left, right):
        """
        Merge two SNS topic policy documents. The id information from
        ``left`` is used in the final document, and the statements
        from ``right`` are merged into ``left``.

        http://docs.aws.amazon.com/sns/latest/dg/BasicStructure.html

        :type left: string
        :param left: First policy JSON document
        :type right: string
        :param right: Second policy JSON document
        :rtype: string
        :return: Merged policy JSON
        """
        left_parsed = json.loads(left)
        right_parsed = json.loads(right)

        left_parsed['Statement'] += right_parsed['Statement']

        return json.dumps(left_parsed)

    def upsert_cloudtrail_config(self, name, bucket, prefix, topic, gse):
        """
        Either create or update the CloudTrail configuration depending on
        whether this command is a create or update command.
        """
        sys.stdout.write('Creating/updating CloudTrail configuration...\n')
        config = {
            'name': name
        }

        if bucket is not None:
            config['s3_bucket_name'] = bucket

        if prefix is not None:
            config['s3_key_prefix'] = prefix

        if topic is not None:
            config['sns_topic_name'] = topic

        if gse is not None:
            config['include_global_service_events'] = gse

        if not self.UPDATE:
            self.cloudtrail.CreateTrail(**config)
        else:
            self.cloudtrail.UpdateTrail(**config)

        return self.cloudtrail.DescribeTrails()

    def start_cloudtrail(self, name):
        """
        Start the CloudTrail service, which begins logging.
        """
        sys.stdout.write('Starting CloudTrail service...\n')
        return self.cloudtrail.StartLogging(name=name)


class CloudTrailUpdate(CloudTrailSubscribe):
    """
    Like subscribe above, but the update version of the command.
    """
    NAME = 'update-subscription'
    UPDATE = True

    DESCRIPTION = ('Updates any of the trail configuration settings, and'
                   ' creates and configures any new AWS resources specified.')

    SYNOPSIS = ('aws cloudtrail update-subscription'
                ' [(--s3-use-bucket|--s3-new-bucket) bucket-name]'
                ' [--sns-new-topic topic-name]\n')

########NEW FILE########
__FILENAME__ = commands
import logging
import os

import bcdoc.docevents
from botocore.compat import OrderedDict

import awscli
from awscli.clidocs import OperationDocumentEventHandler
from awscli.argparser import ArgTableArgParser
from awscli.argprocess import unpack_argument, unpack_cli_arg
from awscli.clidriver import CLICommand
from awscli.arguments import CustomArgument
from awscli.help import HelpCommand


LOG = logging.getLogger(__name__)
_open = open


class _FromFile(object):
    def __init__(self, *paths, **kwargs):
        """
        ``**kwargs`` can contain a ``root_module`` argument
        that contains the root module where the file contents
        should be searched.  This is an optional argument, and if
        no value is provided, will default to ``awscli``.  This means
        that by default we look for examples in the ``awscli`` module.

        """
        self.filename = None
        if paths:
            self.filename = os.path.join(*paths)
        if 'root_module' in kwargs:
            self.root_module = kwargs['root_module']
        else:
            self.root_module = awscli


class BasicCommand(CLICommand):
    """Basic top level command with no subcommands.

    If you want to create a new command, subclass this and
    provide the values documented below.

    """

    # This is the name of your command, so if you want to
    # create an 'aws mycommand ...' command, the NAME would be
    # 'mycommand'
    NAME = 'commandname'
    # This is the description that will be used for the 'help'
    # command.
    DESCRIPTION = 'describe the command'
    # This is optional, if you are fine with the default synopsis
    # (the way all the built in operations are documented) then you
    # can leave this empty.
    SYNOPSIS = ''
    # If you want to provide some hand written examples, you can do
    # so here.  This is written in RST format.  This is optional,
    # you don't have to provide any examples, though highly encouraged!
    EXAMPLES = ''
    # If your command has arguments, you can specify them here.  This is
    # somewhat of an implementation detail, but this is a list of dicts
    # where the dicts match the kwargs of the CustomArgument's __init__.
    # For example, if I want to add a '--argument-one' and an
    # '--argument-two' command, I'd say:
    #
    # ARG_TABLE = [
    #     {'name': 'argument-one', 'help_text': 'This argument does foo bar.',
    #      'action': 'store', 'required': False, 'cli_type_name': 'string',},
    #     {'name': 'argument-two', 'help_text': 'This argument does some other thing.',
    #      'action': 'store', 'choices': ['a', 'b', 'c']},
    # ]
    #
    # A `schema` parameter option is available to accept a custom JSON
    # structure as input. See the file `awscli/schema.py` for more info.
    ARG_TABLE = []
    # If you want the command to have subcommands, you can provide a list of
    # dicts.  We use a list here because we want to allow a user to provide
    # the order they want to use for subcommands.
    # SUBCOMMANDS = [
    #     {'name': 'subcommand1', 'command_class': SubcommandClass},
    #     {'name': 'subcommand2', 'command_class': SubcommandClass2},
    # ]
    # The command_class must subclass from ``BasicCommand``.
    SUBCOMMANDS = []

    FROM_FILE = _FromFile
    # You can set the DESCRIPTION, SYNOPSIS, and EXAMPLES to FROM_FILE
    # and we'll automatically read in that data from the file.
    # This is useful if you have a lot of content and would prefer to keep
    # the docs out of the class definition.  For example:
    #
    # DESCRIPTION = FROM_FILE
    #
    # will set the DESCRIPTION value to the contents of
    # awscli/examples/<command name>/_description.rst
    # The naming conventions for these attributes are:
    #
    # DESCRIPTION = awscli/examples/<command name>/_description.rst
    # SYNOPSIS = awscli/examples/<command name>/_synopsis.rst
    # EXAMPLES = awscli/examples/<command name>/_examples.rst
    #
    # You can also provide a relative path and we'll load the file
    # from the specified location:
    #
    # DESCRIPTION = awscli/examples/<filename>
    #
    # For example:
    #
    # DESCRIPTION = FROM_FILE('command, 'subcommand, '_description.rst')
    # DESCRIPTION = 'awscli/examples/command/subcommand/_description.rst'
    #

    # At this point, the only other thing you have to implement is a _run_main
    # method (see the method for more information).

    def __init__(self, session):
        self._session = session

    def __call__(self, args, parsed_globals):
        # args is the remaining unparsed args.
        # We might be able to parse these args so we need to create
        # an arg parser and parse them.
        subcommand_table = self._build_subcommand_table()
        arg_table = self.arg_table
        parser = ArgTableArgParser(arg_table, subcommand_table)
        parsed_args, remaining = parser.parse_known_args(args)

        # Unpack arguments
        for key, value in vars(parsed_args).items():
            param = None

            # Convert the name to use dashes instead of underscore
            # as these are how the parameters are stored in the
            # `arg_table`.
            xformed = key.replace('_', '-')
            if xformed in arg_table:
                param = arg_table[xformed]

            value = unpack_argument(
                self._session,
                'custom',
                self.name,
                param,
                value
            )

            # If this parameter has a schema defined, then allow plugins
            # a chance to process and override its value.
            if param and getattr(param, 'argument_object', None) is not None \
               and value is not None:
                param_object = param.argument_object

                # Allow a single event handler to process the value
                override = self._session\
                    .emit_first_non_none_response(
                        'process-cli-arg.%s.%s' % ('custom', self.name),
                        param=param_object, value=value, operation=None)

                if override is not None:
                    # A plugin supplied a conversion
                    value = override
                else:
                    # Unpack the argument, which is a string, into the
                    # correct Python type (dict, list, etc)
                    value = unpack_cli_arg(param_object, value)

                # Validate param types, required keys, etc
                param_object.validate(value)

            setattr(parsed_args, key, value)

        if hasattr(parsed_args, 'help'):
            self._display_help(parsed_args, parsed_globals)
        elif getattr(parsed_args, 'subcommand', None) is None:
            # No subcommand was specified so call the main
            # function for this top level command.
            return self._run_main(parsed_args, parsed_globals)
        else:
            return subcommand_table[parsed_args.subcommand](remaining, parsed_globals)

    def _run_main(self, parsed_args, parsed_globals):
        # Subclasses should implement this method.
        # parsed_globals are the parsed global args (things like region,
        # profile, output, etc.)
        # parsed_args are any arguments you've defined in your ARG_TABLE
        # that are parsed.  These will come through as whatever you've
        # provided as the 'dest' key.  Otherwise they default to the
        # 'name' key.  For example: ARG_TABLE[0] = {"name": "foo-arg", ...}
        # can be accessed by ``parsed_args.foo_arg``.
        raise NotImplementedError("_run_main")

    def _build_subcommand_table(self):
        subcommand_table = OrderedDict()
        for subcommand in self.SUBCOMMANDS:
            subcommand_name = subcommand['name']
            subcommand_class = subcommand['command_class']
            subcommand_table[subcommand_name] = subcommand_class(self._session)
        self._session.emit('building-command-table.%s' % self.NAME,
                           command_table=subcommand_table,
                           session=self._session)
        return subcommand_table

    def _display_help(self, parsed_args, parsed_globals):
        help_command = self.create_help_command()
        help_command(parsed_args, parsed_globals)

    def create_help_command(self):
        return BasicHelp(self._session, self, command_table={},
                         arg_table=self.arg_table)

    @property
    def arg_table(self):
        arg_table = OrderedDict()
        for arg_data in self.ARG_TABLE:
            custom_argument = CustomArgument(**arg_data)

            # If a custom schema was passed in, create the argument object
            # so that it can be validated and docs can be generated
            if 'schema' in arg_data:
                custom_argument.create_argument_object()

            arg_table[arg_data['name']] = custom_argument
        return arg_table

    @classmethod
    def add_command(cls, command_table, session, **kwargs):
        command_table[cls.NAME] = cls(session)

    @property
    def name(self):
        return self.NAME


class BasicHelp(HelpCommand):
    event_class = 'command'

    def __init__(self, session, command_object, command_table, arg_table,
                 event_handler_class=None):
        super(BasicHelp, self).__init__(session, command_object,
                                        command_table, arg_table)
        # This is defined in HelpCommand so we're matching the
        # casing here.
        if event_handler_class is None:
            event_handler_class=BasicDocHandler
        self.EventHandlerClass = event_handler_class

        # These are public attributes that are mapped from the command
        # object.  These are used by the BasicDocHandler below.
        self._description = command_object.DESCRIPTION
        self._synopsis = command_object.SYNOPSIS
        self._examples = command_object.EXAMPLES

    @property
    def name(self):
        return self.obj.NAME

    @property
    def description(self):
        return self._get_doc_contents('_description')

    @property
    def synopsis(self):
        return self._get_doc_contents('_synopsis')

    @property
    def examples(self):
        return self._get_doc_contents('_examples')

    def _get_doc_contents(self, attr_name):
        value = getattr(self, attr_name)
        if isinstance(value, BasicCommand.FROM_FILE):
            if value.filename is not None:
                trailing_path = value.filename
            else:
                trailing_path = os.path.join(self.name, attr_name + '.rst')
            root_module = value.root_module
            doc_path = os.path.join(
                os.path.abspath(os.path.dirname(root_module.__file__)), 'examples',
                trailing_path)
            with _open(doc_path) as f:
                return f.read()
        else:
            return value

    def __call__(self, args, parsed_globals):
        # Create an event handler for a Provider Document
        instance = self.EventHandlerClass(self)
        # Now generate all of the events for a Provider document.
        # We pass ourselves along so that we can, in turn, get passed
        # to all event handlers.
        bcdoc.docevents.generate_events(self.session, self)
        self.renderer.render(self.doc.getvalue())
        instance.unregister()


class BasicDocHandler(OperationDocumentEventHandler):
    def __init__(self, help_command):
        super(BasicDocHandler, self).__init__(help_command)
        self.doc = help_command.doc

    def build_translation_map(self):
        return {}

    def doc_description(self, help_command, **kwargs):
        self.doc.style.h2('Description')
        self.doc.write(help_command.description)
        self.doc.style.new_paragraph()

    def doc_synopsis_start(self, help_command, **kwargs):
        if not help_command.synopsis:
            super(BasicDocHandler, self).doc_synopsis_start(
                help_command=help_command, **kwargs)
        else:
            self.doc.style.h2('Synopsis')
            self.doc.style.start_codeblock()
            self.doc.writeln(help_command.synopsis)

    def doc_synopsis_option(self, arg_name, help_command, **kwargs):
        if not help_command.synopsis:
            super(BasicDocHandler, self).doc_synopsis_option(
                arg_name=arg_name,
                help_command=help_command, **kwargs)
        else:
            # A synopsis has been provided so we don't need to write
            # anything here.
            pass

    def doc_synopsis_end(self, help_command, **kwargs):
        if not help_command.synopsis:
            super(BasicDocHandler, self).doc_synopsis_end(
                help_command=help_command, **kwargs)
        else:
            self.doc.style.end_codeblock()

    def doc_examples(self, help_command, **kwargs):
        if help_command.examples:
            self.doc.style.h2('Examples')
            self.doc.write(help_command.examples)

    def doc_subitems_start(self, help_command, **kwargs):
        pass

    def doc_subitem(self, command_name, help_command, **kwargs):
        pass

    def doc_subitems_end(self, help_command, **kwargs):
        pass

    def doc_output(self, help_command, event_name, **kwargs):
        pass

########NEW FILE########
__FILENAME__ = configure
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
import re
import sys
import logging

from botocore.exceptions import ProfileNotFound

from awscli.customizations.commands import BasicCommand


try:
    raw_input = raw_input
except NameError:
    raw_input = input


logger = logging.getLogger(__name__)
NOT_SET = '<not set>'


def register_configure_cmd(cli):
    cli.register('building-command-table.main',
                 ConfigureCommand.add_command)


class ConfigValue(object):
    def __init__(self, value, config_type, config_variable):
        self.value = value
        self.config_type = config_type
        self.config_variable = config_variable

    def mask_value(self):
        if self.value is NOT_SET:
            return
        self.value = _mask_value(self.value)


class SectionNotFoundError(Exception):
    pass


def _mask_value(current_value):
    if current_value is None:
        return 'None'
    else:
        return ('*' * 16) +  current_value[-4:]


class InteractivePrompter(object):
    def get_value(self, current_value, config_name, prompt_text=''):
        if config_name in ('aws_access_key_id', 'aws_secret_access_key'):
            current_value = _mask_value(current_value)
        response = raw_input("%s [%s]: " % (prompt_text, current_value))
        if not response:
            # If the user hits enter, we return a value of None
            # instead of an empty string.  That way we can determine
            # whether or not a value has changed.
            response = None
        return response


class ConfigFileWriter(object):
    SECTION_REGEX = re.compile(r'\[(?P<header>[^]]+)\]')
    OPTION_REGEX = re.compile(
        r'(?P<option>[^:=\s][^:=]*)'
        r'\s*(?P<vi>[:=])\s*'
        r'(?P<value>.*)$'
    )

    def update_config(self, new_values, config_filename):
        section_name = new_values.pop('__section__', 'default')
        if not os.path.isfile(config_filename):
            self._create_file(config_filename)
            self._write_new_section(section_name, new_values, config_filename)
            return
        with open(config_filename, 'r') as f:
            contents = f.readlines()
        # We can only update a single section at a time so we first need
        # to find the section in question
        try:
            self._update_section_contents(contents, section_name, new_values)
            with open(config_filename, 'w') as f:
                f.write(''.join(contents))
        except SectionNotFoundError:
            self._write_new_section(section_name, new_values, config_filename)

    def _create_file(self, config_filename):
        # Create the file as well as the parent dir if needed.
        dirname, basename = os.path.split(config_filename)
        if not os.path.isdir(dirname):
            os.makedirs(dirname)
        with os.fdopen(os.open(config_filename,
                               os.O_WRONLY|os.O_CREAT, 0o600), 'w'):
            pass

    def _write_new_section(self, section_name, new_values, config_filename):
        with open(config_filename, 'a') as f:
            f.write('[%s]\n' % section_name)
            for key, value in new_values.items():
                f.write('%s = %s\n' % (key, value))

    def _update_section_contents(self, contents, section_name, new_values):
        new_values = new_values.copy()
        # contents is a list of file line contents.
        for i in range(len(contents)):
            line = contents[i]
            if line.strip().startswith(('#', ';')):
                # This is a comment, so we can safely ignore this line.
                continue
            match = self.SECTION_REGEX.search(line)
            if match is not None and self._matches_section(match,
                                                           section_name):
                break
        else:
            raise SectionNotFoundError(section_name)
        # If we get here, then we've found the section.  We now need
        # to figure out if we're updating a value or adding a new value.
        i += 1
        last_matching_line = i
        for j in range(i, len(contents)):
            line = contents[j]
            match = self.OPTION_REGEX.search(line)
            if match is not None:
                last_matching_line = j
                key_name = match.group(1).strip()
                if key_name in new_values:
                    new_line = '%s = %s\n' % (key_name, new_values[key_name])
                    contents[j] = new_line
                    del new_values[key_name]
            elif self.SECTION_REGEX.search(line) is not None:
                # We've hit a new section which means the config key is
                # not in the section.  We need to add it here.
                self._insert_new_values(line_number=last_matching_line,
                                        contents=contents,
                                        new_values=new_values)
                return

        if new_values:
            if not contents[-1].endswith('\n'):
                contents.append('\n')
            self._insert_new_values(line_number=last_matching_line + 1,
                                    contents=contents,
                                    new_values=new_values)

    def _insert_new_values(self, line_number, contents, new_values):
        new_contents = []
        for key, value in new_values.items():
            new_contents.append('%s = %s\n' % (key, value))
        contents.insert(line_number + 1, ''.join(new_contents))

    def _matches_section(self, match, section_name):
        parts = section_name.split(' ')
        unquoted_match = match.group(0) == '[%s]' % section_name
        if len(parts) > 1:
            quoted_match = match.group(0) == '[%s "%s"]' % (
                parts[0], ' '.join(parts[1:]))
            return unquoted_match or quoted_match
        return unquoted_match


class ConfigureListCommand(BasicCommand):
    NAME = 'list'
    DESCRIPTION = (
        'List the AWS CLI configuration data.  This command will '
        'show you the current configuration data.  For each configuration '
        'item, it will show you the value, where the configuration value '
        'was retrieved, and the configuration variable name.  For example, '
        'if you provide the AWS region in an environment variable, this '
        'command will show you the name of the region you\'ve configured, '
        'it will tell you that this value came from an environment '
        'variable, and it will tell you the name of the environment '
        'variable.\n'
    )
    SYNOPSIS = 'aws configure list [--profile profile-name]'
    EXAMPLES = (
        'To show your current configuration values::\n'
        '\n'
        '  $ aws configure list\n'
        '        Name                    Value             Type    Location\n'
        '        ----                    -----             ----    --------\n'
        '     profile                <not set>             None    None\n'
        '  access_key     ****************ABCD      config_file    ~/.aws/config\n'
        '  secret_key     ****************ABCD      config_file    ~/.aws/config\n'
        '      region                us-west-2              env    AWS_DEFAULT_REGION\n'
        '\n'
    )

    def __init__(self, session, stream=sys.stdout):
        super(ConfigureListCommand, self).__init__(session)
        self._stream = stream

    def _run_main(self, args, parsed_globals):
        self._display_config_value(ConfigValue('Value', 'Type', 'Location'),
                                   'Name')
        self._display_config_value(ConfigValue('-----', '----', '--------'),
                                   '----')

        if self._session.profile is not None:
            profile = ConfigValue(self._session.profile, 'manual',
                                  '--profile')
        else:
            profile = self._lookup_config('profile')
        self._display_config_value(profile, 'profile')

        access_key, secret_key = self._lookup_credentials()
        self._display_config_value(access_key, 'access_key')
        self._display_config_value(secret_key, 'secret_key')

        region = self._lookup_config('region')
        self._display_config_value(region, 'region')

    def _display_config_value(self, config_value, config_name):
        self._stream.write('%10s %24s %16s    %s\n' % (
            config_name, config_value.value, config_value.config_type,
            config_value.config_variable))

    def _lookup_credentials(self):
        # First try it with _lookup_config.  It's possible
        # that we don't find credentials this way (for example,
        # if we're using an IAM role).
        access_key = self._lookup_config('access_key')
        if access_key.value is not NOT_SET:
            secret_key = self._lookup_config('secret_key')
            access_key.mask_value()
            secret_key.mask_value()
            return access_key, secret_key
        else:
            # Otherwise we can try to use get_credentials().
            # This includes a few more lookup locations
            # (IAM roles, some of the legacy configs, etc.)
            credentials = self._session.get_credentials()
            if credentials is None:
                no_config = ConfigValue(NOT_SET, None, None)
                return no_config, no_config
            else:
                # For the ConfigValue, we don't track down the
                # config_variable because that info is not
                # visible from botocore.credentials.  I think
                # the credentials.method is sufficient to show
                # where the credentials are coming from.
                access_key = ConfigValue(credentials.access_key,
                                        credentials.method, '')
                secret_key = ConfigValue(credentials.secret_key,
                                        credentials.method, '')
                access_key.mask_value()
                secret_key.mask_value()
                return access_key, secret_key

    def _lookup_config(self, name):
        # First try to look up the variable in the env.
        value = self._session.get_config_variable(name, methods=('env',))
        if value is not None:
            return ConfigValue(value, 'env', self._session.session_var_map[name][1])
        # Then try to look up the variable in the config file.
        value = self._session.get_config_variable(name, methods=('config',))
        if value is not None:
            return ConfigValue(value, 'config-file',
                               self._session.get_config_variable('config_file'))
        else:
            return ConfigValue(NOT_SET, None, None)

class ConfigureSetCommand(BasicCommand):
    NAME = 'set'
    DESCRIPTION = BasicCommand.FROM_FILE('configure', 'set',
                                         '_description.rst')
    SYNOPSIS = 'aws configure set varname value [--profile profile-name]'
    EXAMPLES = BasicCommand.FROM_FILE('configure', 'set', '_examples.rst')
    ARG_TABLE = [
        {'name': 'varname',
         'help_text': 'The name of the config value to set.',
         'action': 'store',
         'cli_type_name': 'string', 'positional_arg': True},
        {'name': 'value',
         'help_text': 'The value to set.',
         'action': 'store',
         'cli_type_name': 'string', 'positional_arg': True},
    ]

    def __init__(self, session, config_writer=None):
        super(ConfigureSetCommand, self).__init__(session)
        if config_writer is None:
            config_writer = ConfigFileWriter()
        self._config_writer = config_writer

    def _run_main(self, args, parsed_globals):
        varname = args.varname
        value = args.value
        section = 'default'
        if '.' not in varname:
            # unqualified name, scope it to the current
            # profile (or leave it as the 'default' section if
            # no profile is set).
            if self._session.profile is not None:
                section = 'profile %s' % self._session.profile
        else:
            # It's either section.config-name,
            # of profile.profile-name.config-name (we
            # don't support arbitrary.thing.config-name).
            num_dots = varname.count('.')
            if num_dots == 1:
                section, varname = varname.split('.')
            elif num_dots == 2 and varname.startswith('profile'):
                dotted_section, varname = varname.rsplit('.', 1)
                profile = dotted_section.split('.')[1]
                section = 'profile %s' % profile
        config_filename = os.path.expanduser(
            self._session.get_config_variable('config_file'))
        updated_config = {'__section__': section, varname: value}
        self._config_writer.update_config(updated_config, config_filename)


class ConfigureGetCommand(BasicCommand):
    NAME = 'get'
    DESCRIPTION = BasicCommand.FROM_FILE('configure', 'get',
                                         '_description.rst')
    SYNOPSIS = ('aws configure get varname [--profile profile-name]')
    EXAMPLES = BasicCommand.FROM_FILE('configure', 'get', '_examples.rst')
    ARG_TABLE = [
        {'name': 'varname',
         'help_text': 'The name of the config value to retrieve.',
         'action': 'store',
         'cli_type_name': 'string', 'positional_arg': True},
    ]

    def __init__(self, session, stream=sys.stdout):
        super(ConfigureGetCommand, self).__init__(session)
        self._stream = stream

    def _run_main(self, args, parsed_globals):
        varname = args.varname
        value = None
        if '.' not in varname:
            # get_scoped_config() returns the config variables in the config
            # file (not the logical_var names), which is what we want.
            config = self._session.get_scoped_config()
            value = config.get(varname)
        else:
            num_dots = varname.count('.')
            if num_dots == 1:
                full_config = self._session.full_config
                section, config_name = varname.split('.')
                value = full_config.get(section, {}).get(config_name)
                if value is None:
                    # Try to retrieve it from the profile config.
                    value = full_config['profiles'].get(
                        section, {}).get(config_name)
            elif num_dots == 2 and varname.startswith('profile'):
                # We're hard coding logic for profiles here.  Really
                # we could support any generic format of [section subsection],
                # but we'd need some botocore.session changes for that,
                # and nothing would immediately use that feature.
                dot_section, config_name = varname.rsplit('.', 1)
                start, profile_name = dot_section.split('.')
                self._session.profile = profile_name
                config = self._session.get_scoped_config()
                value = config.get(config_name)
        if value is not None:
            self._stream.write(value)
            self._stream.write('\n')
            return 0
        else:
            return 1


class ConfigureCommand(BasicCommand):
    NAME = 'configure'
    DESCRIPTION = BasicCommand.FROM_FILE()
    SYNOPSIS = ('aws configure [--profile profile-name]')
    EXAMPLES = (
        'To create a new configuration::\n'
        '\n'
        '    $ aws configure\n'
        '    AWS Access Key ID [None]: accesskey\n'
        '    AWS Secret Access Key [None]: secretkey\n'
        '    Default region name [None]: us-west-2\n'
        '    Default output format [None]:\n'
        '\n'
        'To update just the region name::\n'
        '\n'
        '    $ aws configure\n'
        '    AWS Access Key ID [****]:\n'
        '    AWS Secret Access Key [****]:\n'
        '    Default region name [us-west-1]: us-west-2\n'
        '    Default output format [None]:\n'
    )
    SUBCOMMANDS = [
        {'name': 'list', 'command_class': ConfigureListCommand},
        {'name': 'get', 'command_class': ConfigureGetCommand},
        {'name': 'set', 'command_class': ConfigureSetCommand},
    ]

    # If you want to add new values to prompt, update this list here.
    VALUES_TO_PROMPT = [
        # (logical_name, config_name, prompt_text)
        ('aws_access_key_id', "AWS Access Key ID"),
        ('aws_secret_access_key', "AWS Secret Access Key"),
        ('region', "Default region name"),
        ('output', "Default output format"),
    ]

    def __init__(self, session, prompter=None, config_writer=None):
        self._session = session
        if prompter is None:
            prompter = InteractivePrompter()
        self._prompter = prompter
        if config_writer is None:
            config_writer = ConfigFileWriter()
        self._config_writer = config_writer

    def _run_main(self, parsed_args, parsed_globals):
        # Called when invoked with no args "aws configure"
        new_values = {}
        if parsed_globals.profile is not None:
            self._session.profile = parsed_globals.profile
        # This is the config from the config file scoped to a specific
        # profile.
        try:
            config = self._session.get_scoped_config()
        except ProfileNotFound:
            config = {}
        for config_name, prompt_text in self.VALUES_TO_PROMPT:
            current_value = config.get(config_name)
            new_value = self._prompter.get_value(current_value, config_name,
                                                 prompt_text)
            if new_value is not None and new_value != current_value:
                new_values[config_name] = new_value
        config_filename = os.path.expanduser(
            self._session.get_config_variable('config_file'))
        if new_values:
            if parsed_globals.profile is not None:
                new_values['__section__'] = (
                    'profile %s' % parsed_globals.profile)
            self._config_writer.update_config(new_values, config_filename)

########NEW FILE########
__FILENAME__ = translator
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import json


class PipelineDefinitionError(Exception):
    def __init__(self, msg, definition):
        full_msg = (
            "Error in pipeline definition: %s\n" % msg)
        super(PipelineDefinitionError, self).__init__(full_msg)
        self.msg = msg
        self.definition = definition


def definition_to_api(definition):
    if 'objects' not in definition:
        raise PipelineDefinitionError('Missing "objects" key', definition)
    api_elements = []
    # To convert to the structure expected by the service,
    # we convert the existing structure to a list of dictionaries.
    # Each dictionary has a 'fields', 'id', and 'name' key.
    for element in definition['objects']:
        try:
            element_id = element.pop('id')
        except KeyError:
            raise PipelineDefinitionError('Missing "id" key of element: %s' %
                                          json.dumps(element), definition)
        api_object = {'id': element_id}
        # If a name is provided, then we use that for the name,
        # otherwise the id is used for the name.
        name = element.pop('name', element_id)
        api_object['name'] = name
        # Now we need the field list.  Each element in the field list is a dict
        # with a 'key', 'stringValue'|'refValue'
        fields = []
        for key, value in sorted(element.items()):
            if isinstance(value, list):
                for item in value:
                    fields.append(_convert_single_field(key, item))
            else:
                fields.append(_convert_single_field(key, value))
        api_object['fields'] = fields
        api_elements.append(api_object)
    return api_elements


def _convert_single_field(key, value):
    field = {'key': key}
    if isinstance(value, dict) and list(value.keys()) == ['ref']:
        field['refValue'] = value['ref']
    else:
        field['stringValue'] = value
    return field


def api_to_definition(api_response):
    # When we're translating from api_response -> definition
    # we have to be careful *not* to mutate the existing
    # response as other code might need to the original
    # api_response.
    pipeline_objs = []
    for element in api_response:
        current = {
            'id': element['id'],
            'name': element['name'],
        }
        for field in element['fields']:
            key = field['key']
            if 'stringValue' in field:
                value = field['stringValue']
            else:
                value = {'ref': field['refValue']}
            if key not in current:
                current[key] = value
            elif isinstance(current[key], list):
                # Dupe keys result in values aggregating
                # into a list.
                current[key].append(value)
            else:
                converted_list = [current[key], value]
                current[key] = converted_list
        pipeline_objs.append(current)
    return {'objects': pipeline_objs}

########NEW FILE########
__FILENAME__ = dryrundocs
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
Add a docstring to all --dryrun parameters.
"""

DOCS = ('<p>Checks whether you have the required permissions for the '
        'action, without actually making the request.  '
        'Using this option will result in one of two possible error '
        'responses.  If you have the required permissions, the error '
        'response will be <code>DryRunOperation</code>.  Otherwise '
        'it will be <code>UnauthorizedOperation</code>.</p>')


def register_dryrun_docs(cli):
    cli.register('doc-option-example.ec2.*.dry-run', add_docs)


def add_docs(help_command, **kwargs):
    help_command.doc.include_doc_string(DOCS)

########NEW FILE########
__FILENAME__ = ec2addcount
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import logging

from awscli.arguments import BaseCLIArgument
from botocore.parameters import StringParameter

logger = logging.getLogger(__name__)


HELP = """
<p>Number of instances to launch. If a single number is provided, it
is assumed to be the minimum to launch (defaults to 1).  If a range is
provided in the form <code>min:max</code> then the first number is
interpreted as the minimum number of instances to launch and the second
is interpreted as the maximum number of instances to launch.</p>"""


def ec2_add_count(argument_table, operation, **kwargs):
    argument_table['count'] = CountArgument(operation, 'count')
    del argument_table['min-count']
    del argument_table['max-count']


class CountArgument(BaseCLIArgument):

    def __init__(self, operation, name):
        param = StringParameter(operation, name='count', type='string')
        self.argument_object = param
        self._operation = operation
        self._name = name

    @property
    def cli_name(self):
        return '--' + self._name

    @property
    def cli_type_name(self):
        return 'string'

    @property
    def required(self):
        return False

    @property
    def documentation(self):
        return HELP

    def add_to_parser(self, parser):
        parser.add_argument(self.cli_name, metavar=self.py_name,
                            help='Number of instances to launch',
                            default='1')

    def add_to_params(self, parameters, value):
        try:
            if ':' in value:
                minstr, maxstr = value.split(':')
            else:
                minstr, maxstr = (value, value)
            parameters['min_count'] = int(minstr)
            parameters['max_count'] = int(maxstr)
        except:
            msg = ('count parameter should be of '
                   'form min[:max] (e.g. 1 or 1:10)')
            raise ValueError(msg)

########NEW FILE########
__FILENAME__ = ec2bundleinstance
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

import logging
from hashlib import sha1
import hmac
import base64
import datetime

import six

from awscli.arguments import CustomArgument

logger = logging.getLogger('ec2bundleinstance')

# This customization adds the following scalar parameters to the
# bundle-instance operation:

# --bucket:
BUCKET_DOCS = ('The bucket in which to store the AMI.  '
               'You can specify a bucket that you already own or '
               'a new bucket that Amazon EC2 creates on your behalf.  '
               'If you specify a bucket that belongs to someone else, '
               'Amazon EC2 returns an error.')

# --prefix:
PREFIX_DOCS = ('The prefix for the image component names being stored '
               'in Amazon S3.')

# --owner-akid
OWNER_AKID_DOCS = 'The access key ID of the owner of the Amazon S3 bucket.'

# --policy
POLICY_DOCS = ("An Amazon S3 upload policy that gives "
               "Amazon EC2 permission to upload items into Amazon S3 "
               "on the user's behalf. If you provide this parameter, "
               "you must also provide "
               "your secret access key, so we can create a policy "
               "signature for you (the secret access key is not passed "
               "to Amazon EC2). If you do not provide this parameter, "
               "we generate an upload policy for you automatically. "
               "For more information about upload policies see the "
               "sections about policy construction and signatures in the "
               '<a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/HTTPPOSTForms.html">'
               'Amazon Simple Storage Service Developer Guide</a>.')

# --owner-sak
OWNER_SAK_DOCS = ('The AWS secret access key for the owner of the '
                  'Amazon S3 bucket specified in the --bucket '
                  'parameter. This parameter is required so that a '
                  'signature can be computed for the policy.')


def _add_params(argument_table, operation, **kwargs):
    # Add the scalar parameters and also change the complex storage
    # param to not be required so the user doesn't get an error from
    # argparse if they only supply scalar params.
    storage_arg = argument_table.get('storage')
    storage_param = storage_arg.argument_object
    storage_param.required = False
    arg = BundleArgument(storage_param='Bucket',
                         name='bucket',
                         help_text=BUCKET_DOCS)
    argument_table['bucket'] = arg
    arg = BundleArgument(storage_param='Prefix',
                         name='prefix',
                         help_text=PREFIX_DOCS)
    argument_table['prefix'] = arg
    arg = BundleArgument(storage_param='AWSAccessKeyId',
                         name='owner-akid',
                         help_text=OWNER_AKID_DOCS)
    argument_table['owner-akid'] = arg
    arg = BundleArgument(storage_param='_SAK',
                         name='owner-sak',
                         help_text=OWNER_SAK_DOCS)
    argument_table['owner-sak'] = arg
    arg = BundleArgument(storage_param='UploadPolicy',
                         name='policy',
                         help_text=POLICY_DOCS)
    argument_table['policy'] = arg


def _check_args(parsed_args, **kwargs):
    # This function checks the parsed args.  If the user specified
    # the --ip-permissions option with any of the scalar options we
    # raise an error.
    logger.debug(parsed_args)
    arg_dict = vars(parsed_args)
    if arg_dict['storage']:
        for key in ('bucket', 'prefix', 'owner-akid',
                    'owner-sak', 'policy'):
            if arg_dict[key]:
                msg = ('Mixing the --storage option '
                       'with the simple, scalar options is '
                       'not recommended.')
                raise ValueError(msg)

POLICY = ('{{"expiration": "{expires}",'
          '"conditions": ['
          '{{"bucket": "{bucket}"}},'
          '{{"acl": "ec2-bundle-read"}},'
          '["starts-with", "$key", "{prefix}"]'
          ']}}'
          )


def _generate_policy(params):
    # Called if there is no policy supplied by the user.
    # Creates a policy that provides access for 24 hours.
    delta = datetime.timedelta(hours=24)
    expires = datetime.datetime.utcnow() + delta
    expires_iso = expires.strftime("%Y-%m-%dT%H:%M:%S.%fZ")
    policy = POLICY.format(expires=expires_iso,
                           bucket=params['Bucket'],
                           prefix=params['Prefix'])
    params['UploadPolicy'] = policy


def _generate_signature(params):
    # If we have a policy and a sak, create the signature.
    policy = params.get('UploadPolicy')
    sak = params.get('_SAK')
    if policy and sak:
        policy = base64.b64encode(six.b(policy)).decode('utf-8')
        new_hmac = hmac.new(sak.encode('utf-8'), digestmod=sha1)
        new_hmac.update(six.b(policy))
        ps = base64.encodestring(new_hmac.digest()).strip().decode('utf-8')
        params['UploadPolicySignature'] = ps
        del params['_SAK']


def _check_params(**kwargs):
    # Called just before call but prior to building the params.
    # Adds information not supplied by the user.
    logger.debug(kwargs)
    storage = kwargs['params']['storage']['S3']
    if 'UploadPolicy' not in storage:
        _generate_policy(storage)
    if 'UploadPolicySignature' not in storage:
        _generate_signature(storage)


EVENTS = [
    ('building-argument-table.ec2.bundle-instance', _add_params),
    ('operation-args-parsed.ec2.bundle-instance', _check_args),
    ('before-parameter-build.ec2.BundleInstance', _check_params),
    ]


def register_bundleinstance(event_handler):
    # Register all of the events for customizing BundleInstance
    for event, handler in EVENTS:
        event_handler.register(event, handler)


class BundleArgument(CustomArgument):

    def __init__(self, storage_param, *args, **kwargs):
        super(BundleArgument, self).__init__(*args, **kwargs)
        self._storage_param = storage_param

    def _build_storage(self, params, value):
        # Build up the Storage data structure
        if 'storage' not in params:
            params['storage'] = {'S3': {}}
        params['storage']['S3'][self._storage_param] = value


    def add_to_params(self, parameters, value):
        if value:
            self._build_storage(parameters, value)

########NEW FILE########
__FILENAME__ = ec2decryptpassword
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import logging
import os
import base64
import rsa
import six

from awscli.arguments import BaseCLIArgument
from botocore.parameters import StringParameter

logger = logging.getLogger(__name__)

HELP = """<p>The file that contains the private key used to launch
the instance (e.g. windows-keypair.pem).  If this is supplied, the
password data sent from EC2 will be decrypted before display.</p>"""


def ec2_add_priv_launch_key(argument_table, operation, **kwargs):
    """
    This handler gets called after the argument table for the
    operation has been created.  It's job is to add the
    ``priv-launch-key`` parameter.
    """
    argument_table['priv-launch-key'] = LaunchKeyArgument(operation,
                                                          'priv-launch-key')

class LaunchKeyArgument(BaseCLIArgument):

    def __init__(self, operation, name):
        param = StringParameter(operation,
                                name=name,
                                type='string')
        self._name = name
        self.argument_object = param
        self._operation = operation
        self._name = name
        self._key_path = None

    @property
    def cli_type_name(self):
        return 'string'

    @property
    def required(self):
        return False

    @property
    def documentation(self):
        return HELP

    def add_to_parser(self, parser):
        parser.add_argument(self.cli_name, dest=self.py_name,
                            help='SSH Private Key file')

    def add_to_params(self, parameters, value):
        """
        This gets called with the value of our ``--priv-launch-key``
        if it is specified.  It needs to determine if the path
        provided is valid and, if it is, it stores it in the instance
        variable ``_key_path`` for use by the decrypt routine.
        """
        if value:
            path = os.path.expandvars(value)
            path = os.path.expanduser(path)
            if os.path.isfile(path):
                self._key_path = path
                service_name = self._operation.service.endpoint_prefix
                event = 'after-call.%s.%s' % (service_name,
                                              self._operation.name)
                self._operation.session.register(event,
                                                 self._decrypt_password_data)
            else:
                msg = ('priv-launch-key should be a path to the '
                       'local SSH private key file used to launch '
                       'the instance.')
                raise ValueError(msg)

    def _decrypt_password_data(self, http_response, parsed, **kwargs):
        """
        This handler gets called after the GetPasswordData command has
        been executed.  It is called with the ``http_response`` and
        the ``parsed`` data.  It checks to see if a private launch
        key was specified on the command.  If it was, it tries to use
        that private key to decrypt the password data and replace it
        in the returned data dictionary.
        """
        if self._key_path is not None:
            logger.debug("Decrypting password data using: %s", self._key_path)
            value = parsed.get('PasswordData')
            if not value:
                return
            try:
                with open(self._key_path) as pk_file:
                    pk_contents = pk_file.read()
                    private_key = rsa.PrivateKey.load_pkcs1(six.b(pk_contents))
                    value = base64.b64decode(value)
                    value = rsa.decrypt(value, private_key)
                    logger.debug(parsed)
                    parsed['PasswordData'] = value.decode('utf-8')
                    logger.debug(parsed)
            except Exception:
                logger.debug('Unable to decrypt PasswordData', exc_info=True)
                msg = ('Unable to decrypt password data using '
                       'provided private key file.')
                raise ValueError(msg)

########NEW FILE########
__FILENAME__ = ec2protocolarg
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
This customization allows the user to specify the values "tcp", "udp",
or "icmp" as values for the --protocol parameter.  The actual Protocol
parameter of the operation accepts only integer protocol numbers.
"""

def _fix_args(operation, endpoint, params, **kwargs):
    if 'protocol' in params:
        if params['protocol'] == 'tcp':
            params['protocol'] = '6'
        elif params['protocol'] == 'udp':
            params['protocol'] = '17'
        elif params['protocol'] == 'icmp':
            params['protocol'] = '1'
        elif params['protocol'] == 'all':
            params['protocol'] = '-1'


def register_protocol_args(cli):
    cli.register('before-parameter-build.ec2.CreateNetworkAclEntry',
                 _fix_args)
    cli.register('before-parameter-build.ec2.ReplaceNetworkAclEntry',
                 _fix_args)
    

########NEW FILE########
__FILENAME__ = ec2runinstances
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
This customization adds two new parameters to the ``ec2 run-instance``
command.  The first, ``--secondary-private-ip-addresses`` allows a list
of IP addresses within the specified subnet to be associated with the
new instance.  The second, ``--secondary-ip-address-count`` allows you
to specify how many additional IP addresses you want but the actual
address will be assigned for you.

This functionality (and much more) is also available using the
``--network-interfaces`` complex argument.  This just makes two of
the most commonly used features available more easily.
"""
from awscli.arguments import CustomArgument

# --secondary-private-ip-address
SECONDARY_PRIVATE_IP_ADDRESSES_DOCS = (
    '[EC2-VPC] A secondary private IP address for the network interface '
    'or instance. You can specify this multiple times to assign multiple '
    'secondary IP addresses.  If you want additional private IP addresses '
    'but do not need a specific address, use the '
    '--secondary-private-ip-address-count option.')

# --secondary-private-ip-address-count
SECONDARY_PRIVATE_IP_ADDRESS_COUNT_DOCS = (
    '[EC2-VPC] The number of secondary IP addresses to assign to '
    'the network interface or instance.')

# --associate-public-ip-address
ASSOCIATE_PUBLIC_IP_ADDRESS_DOCS = (
    '[EC2-VPC] If specified a public IP address will be assigned '
    'to the new instance in a VPC.')

def _add_params(argument_table, operation, **kwargs):
    arg = SecondaryPrivateIpAddressesArgument(
        name='secondary-private-ip-addresses',
        help_text=SECONDARY_PRIVATE_IP_ADDRESSES_DOCS)
    argument_table['secondary-private-ip-addresses'] = arg
    arg = SecondaryPrivateIpAddressCountArgument(
        name='secondary-private-ip-address-count',
        help_text=SECONDARY_PRIVATE_IP_ADDRESS_COUNT_DOCS)
    argument_table['secondary-private-ip-address-count'] = arg
    arg = AssociatePublicIpAddressArgument(
        name='associate-public-ip-address',
        help_text=ASSOCIATE_PUBLIC_IP_ADDRESS_DOCS,
        action='store_true', group_name='associate_public_ip')
    argument_table['associate-public-ip-address'] = arg
    arg = NoAssociatePublicIpAddressArgument(
        name='no-associate-public-ip-address',
        help_text=ASSOCIATE_PUBLIC_IP_ADDRESS_DOCS,
        action='store_false', group_name='associate_public_ip')
    argument_table['no-associate-public-ip-address'] = arg


def _check_args(parsed_args, **kwargs):
    # This function checks the parsed args.  If the user specified
    # the --network-interfaces option with any of the scalar options we
    # raise an error.
    arg_dict = vars(parsed_args)
    if arg_dict['network_interfaces']:
        for key in ('secondary_private_ip_addresses',
                    'secondary_private_ip_address_count',
                    'associate_public_ip_address'):
            if arg_dict[key]:
                msg = ('Mixing the --network-interfaces option '
                       'with the simple, scalar options is '
                       'not supported.')
                raise ValueError(msg)


def _fix_args(operation, endpoint, params, **kwargs):
    # The RunInstances request provides some parameters
    # such as --subnet-id and --security-group-id that can be specified
    # as separate options only if the request DOES NOT include a
    # NetworkInterfaces structure.  In those cases, the values for
    # these parameters must be specified inside the NetworkInterfaces
    # structure.  This function checks for those parameters
    # and fixes them if necessary.
    # NOTE: If the user is a default VPC customer, RunInstances
    # allows them to specify the security group by name or by id.
    # However, in this scenario we can only support id because
    # we can't place a group name in the NetworkInterfaces structure.
    if 'network_interfaces' in params:
        ni = params['network_interfaces']
        if 'AssociatePublicIpAddress' in ni[0]:
            if 'subnet_id' in params:
                ni[0]['SubnetId'] = params['subnet_id']
                del params['subnet_id']
            if 'security_group_ids' in params:
                ni[0]['Groups'] = params['security_group_ids']
                del params['security_group_ids']
            if 'private_ip_address' in params:
                ip_addr = {'PrivateIpAddress': params['private_ip_address'],
                           'Primary': True}
                ni[0]['PrivateIpAddresses'] = [ip_addr]
                del params['private_ip_address']


EVENTS = [
    ('building-argument-table.ec2.run-instances', _add_params),
    ('operation-args-parsed.ec2.run-instances', _check_args),
    ('before-parameter-build.ec2.RunInstances', _fix_args),
    ]


def register_runinstances(event_handler):
    # Register all of the events for customizing BundleInstance
    for event, handler in EVENTS:
        event_handler.register(event, handler)


def _build_network_interfaces(params, key, value):
    # Build up the NetworkInterfaces data structure
    if 'network_interfaces' not in params:
        params['network_interfaces'] = [{'DeviceIndex': 0}]

    if key == 'PrivateIpAddresses':
        if 'PrivateIpAddresses' not in params['network_interfaces'][0]:
            params['network_interfaces'][0]['PrivateIpAddresses'] = value
    else:
        params['network_interfaces'][0][key] = value


class SecondaryPrivateIpAddressesArgument(CustomArgument):

    def add_to_parser(self, parser, cli_name=None):
        parser.add_argument(self.cli_name, dest=self.py_name,
                            default=self._default, nargs='*')

    def add_to_params(self, parameters, value):
        if value:
            value = [{'PrivateIpAddress': v, 'Primary': False} for
                     v in value]
            _build_network_interfaces(parameters,
                                      'PrivateIpAddresses',
                                      value)


class SecondaryPrivateIpAddressCountArgument(CustomArgument):

    def add_to_parser(self, parser, cli_name=None):
        parser.add_argument(self.cli_name, dest=self.py_name,
                            default=self._default, type=int)

    def add_to_params(self, parameters, value):
        if value:
            _build_network_interfaces(parameters,
                                      'SecondaryPrivateIpAddressCount',
                                      value)


class AssociatePublicIpAddressArgument(CustomArgument):

    def add_to_params(self, parameters, value):
        if value is True:
            _build_network_interfaces(parameters,
                                      'AssociatePublicIpAddress',
                                      value)


class NoAssociatePublicIpAddressArgument(CustomArgument):

    def add_to_params(self, parameters, value):
        if value is False:
            _build_network_interfaces(parameters,
                                      'AssociatePublicIpAddress',
                                      value)

########NEW FILE########
__FILENAME__ = ec2secgroupsimplify
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
This customization adds the following scalar parameters to the
authorize operations:

* --protocol: tcp | udp | icmp or any protocol number
* --port:  A single integer or a range (min-max). You can specify ``all``
  to mean all ports (for example, port range 0-65535)
* --source-group-name
* --source-group-id
* --cidr -  The CIDR range. Cannot be used when specifying a source or
  destination security group.
"""

from awscli.arguments import CustomArgument


def _add_params(argument_table, operation, **kwargs):
    arg = ProtocolArgument('protocol',
                           help_text=PROTOCOL_DOCS)
    argument_table['protocol'] = arg
    arg = PortArgument('port', help_text=PORT_DOCS)
    argument_table['port'] = arg
    arg = CidrArgument('cidr', help_text=CIDR_DOCS)
    argument_table['cidr'] = arg
    arg = SourceGroupArgument('source-group',
                              help_text=SOURCEGROUP_DOCS)
    argument_table['source-group'] = arg
    arg = GroupOwnerArgument('group-owner',
                             help_text=GROUPOWNER_DOCS)
    argument_table['group-owner'] = arg


def _check_args(parsed_args, **kwargs):
    # This function checks the parsed args.  If the user specified
    # the --ip-permissions option with any of the scalar options we
    # raise an error.
    arg_dict = vars(parsed_args)
    if arg_dict['ip_permissions']:
        for key in ('protocol', 'port', 'cidr',
                    'source_group', 'group_owner'):
            if arg_dict[key]:
                msg = ('The --%s option is not compatible '
                       'with the --ip-permissions option ') % key
                raise ValueError(msg)

def _add_docs(help_command, **kwargs):
    doc = help_command.doc
    doc.style.new_paragraph()
    doc.style.start_note()
    msg = ('To specify multiple rules in a single command '
           'use the <code>--ip-permissions</code> option')
    doc.include_doc_string(msg)
    doc.style.end_note()


EVENTS = [
    ('building-argument-table.ec2.authorize-security-group-ingress', _add_params),
    ('building-argument-table.ec2.authorize-security-group-egress', _add_params),
    ('building-argument-table.ec2.revoke-security-group-ingress', _add_params),
    ('building-argument-table.ec2.revoke-security-group-egress', _add_params),
    ('operation-args-parsed.ec2.authorize-security-group-ingress', _check_args),
    ('operation-args-parsed.ec2.authorize-security-group-egress', _check_args),
    ('operation-args-parsed.ec2.revoke-security-group-ingress', _check_args),
    ('operation-args-parsed.ec2.revoke-security-group-egress', _check_args),
    ('doc-description.ec2.authorize-security-group-ingress', _add_docs),
    ('doc-description.ec2.authorize-security-group-egress', _add_docs),
    ('doc-description.ec2.revoke-security-group-ingress', _add_docs),
    ('doc-description.ec2.revoke-security-groupdoc-ingress', _add_docs),
    ]
PROTOCOL_DOCS = ('<p>The IP protocol of this permission.</p>'
                 '<p>Valid protocol values: <code>tcp</code>, '
                 '<code>udp</code>, <code>icmp</code></p>')
PORT_DOCS = ('<p>For TCP or UDP: The range of ports to allow.'
             '  A single integer or a range (min-max). You can '
             'specify <code>all</code> to mean all ports</p>')
CIDR_DOCS = '<p>The CIDR IP range.</p>'
SOURCEGROUP_DOCS = ('<p>The name of the source security group. '
                    'Cannot be used when specifying a CIDR IP address.')
GROUPOWNER_DOCS = ('<p>The AWS account ID that owns the source security '
                   'group. Cannot be used when specifying a CIDR IP '
                   'address.</p>')

def register_secgroup(event_handler):
    for event, handler in EVENTS:
        event_handler.register(event, handler)


def _build_ip_permissions(params, key, value):
    if 'ip_permissions' not in params:
        params['ip_permissions'] = [{}]
    if key == 'CidrIp':
        if 'IpRanges' not in params['ip_permissions'][0]:
            params['ip_permissions'][0]['IpRanges'] = []
        params['ip_permissions'][0]['IpRanges'].append(value)
    elif key in ('GroupId', 'GroupName', 'UserId'):
        if 'UserIdGroupPairs' not in params['ip_permissions'][0]:
            params['ip_permissions'][0]['UserIdGroupPairs'] = [{}]
        params['ip_permissions'][0]['UserIdGroupPairs'][0][key] = value
    else:
        params['ip_permissions'][0][key] = value


class ProtocolArgument(CustomArgument):

    def add_to_params(self, parameters, value):
        if value:
            try:
                int_value = int(value)
                if (int_value < 0 or int_value > 255) and int_value != -1:
                    msg = ('protocol numbers must be in the range 0-255 '
                           'or -1 to specify all protocols')
                    raise ValueError(msg)
            except ValueError:
                if value not in ('tcp', 'udp', 'icmp', 'all'):
                    msg = ('protocol parameter should be one of: '
                           'tcp|udp|icmp|all or any valid protocol number.')
                    raise ValueError(msg)
                if value == 'all':
                    value = '-1'
            _build_ip_permissions(parameters, 'IpProtocol', value)


class PortArgument(CustomArgument):

    def add_to_params(self, parameters, value):
        if value:
            try:
                if value == '-1' or value == 'all':
                    fromstr = '-1'
                    tostr = '-1'
                elif '-' in value:
                    fromstr, tostr = value.split('-')
                else:
                    fromstr, tostr = (value, value)
                _build_ip_permissions(parameters, 'FromPort', int(fromstr))
                _build_ip_permissions(parameters, 'ToPort', int(tostr))
            except ValueError:
                msg = ('port parameter should be of the '
                       'form <from[-to]> (e.g. 22 or 22-25)')
                raise ValueError(msg)


class CidrArgument(CustomArgument):

    def add_to_params(self, parameters, value):
        if value:
            value = [{'CidrIp': value}]
            _build_ip_permissions(parameters, 'IpRanges', value)


class SourceGroupArgument(CustomArgument):

    def add_to_params(self, parameters, value):
        if value:
            if value.startswith('sg-'):
                _build_ip_permissions(parameters, 'GroupId', value)
            else:
                _build_ip_permissions(parameters, 'GroupName', value)


class GroupOwnerArgument(CustomArgument):

    def add_to_params(self, parameters, value):
        if value:
            _build_ip_permissions(parameters, 'UserId', value)

########NEW FILE########
__FILENAME__ = flatten
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

import logging

from awscli.arguments import CustomArgument

LOG = logging.getLogger(__name__)

# Nested argument member separator
SEP = '.'


class FlattenedArgument(CustomArgument):
    """
    A custom argument which has been flattened from an existing structure. When
    added to the call params it is hydrated back into the structure.

    Supports both an object and a list of objects, in which case the flattened
    parameters will hydrate a list with a single object in it.
    """
    def __init__(self, name, container, prop, help_text='', required=None,
                 type=None, hydrate=None, hydrate_value=None):
        super(FlattenedArgument, self).__init__(name=name, help_text=help_text,
                                                required=required)
        self.type = type
        self._container = container
        self._property = prop
        self._hydrate = hydrate
        self._hydrate_value = hydrate_value

    @property
    def cli_type_name(self):
        return self.type

    def add_to_params(self, parameters, value):
        """
        Hydrate the original structure with the value of this flattened
        argument.

        TODO: This does not hydrate nested structures (``XmlName1.XmlName2``)!
              To do this for now you must provide your own ``hydrate`` method.
        """
        container = self._container.py_name
        cli_type = self._container.cli_type_name
        key = self._property

        LOG.debug('Hydrating {0}[{1}]'.format(container, key))

        if value is not None:
            # Convert type if possible
            if self.type == 'boolean':
                value = not value.lower() == 'false'
            elif self.type in ['integer', 'long']:
                value = int(value)
            elif self.type in ['float', 'double']:
                value = float(value)

            if self._hydrate:
                self._hydrate(parameters, container, cli_type, key, value)
            else:
                if container not in parameters:
                    if cli_type == 'list':
                        parameters[container] = [{}]
                    else:
                        parameters[container] = {}

                if self._hydrate_value:
                    value = self._hydrate_value(value)

                if cli_type == 'list':
                    parameters[container][0][key] = value
                else:
                    parameters[container][key] = value


class FlattenArguments(object):
    """
    Flatten arguments for one or more commands for a particular service from
    a given configuration which maps service call parameters to flattened
    names. Takes in a configuration dict of the form::

        {
            "command-cli-name": {
                "argument-cli-name": {
                    "keep": False,
                    "flatten": {
                        "XmlName": {
                            "name": "flattened-cli-name",
                            "type": "Optional custom type",
                            "required": "Optional custom required",
                            "help_text": "Optional custom docs",
                            "hydrate_value": Optional function to hydrate value,
                            "hydrate": Optional function to hydrate
                        },
                        ...
                    }
                },
                ...
            },
            ...
        }

    The ``type``, ``required`` and ``help_text`` arguments are entirely
    optional and by default are pulled from the model. You should only set them
    if you wish to override the default values in the model.

    The ``keep`` argument determines whether the original command is still
    accessible vs. whether it is removed. It defaults to ``False`` if not
    present, which removes the original argument.

    The keys inside of ``flatten`` (e.g. ``XmlName`` above) can include nested
    references to structures via a colon. For example, ``XmlName1:XmlName2``
    for the following structure::

        {
            "XmlName1": {
                "XmlName2": ...
            }
        }

    The ``hydrate_value`` function takes in a value and should return a value.
    It is only called when the value is not ``None``. Example::

        "hydrate_value": lambda (value): value.upper()

    The ``hydrate`` function takes in a list of existing parameters, the name
    of the container, its type, the name of the container key and its set
    value. For the example above, the container would be
    ``'argument-cli-name'``, the key would be ``'XmlName'`` and the value
    whatever the user passed in. Example::

        def my_hydrate(params, container, cli_type, key, value):
            if container not in params:
                params[container] = {'default': 'values'}

            params[container][key] = value

    It's possible for ``cli_type`` to be ``list``, in which case you should
    ensure that a list of one or more objects is hydrated rather than a
    single object.
    """
    def __init__(self, service_name, configs):
        self.configs = configs
        self.service_name = service_name

    def register(self, cli):
        """
        Register with a CLI instance, listening for events that build the
        argument table for operations in the configuration dict.
        """
        # Flatten each configured operation when they are built
        service = self.service_name
        for operation in self.configs:
            cli.register('building-argument-table.{0}.{1}'.format(service,
                                                                  operation),
                         self.flatten_args)

    def flatten_args(self, operation, argument_table, **kwargs):
        # For each argument with a bag of parameters
        for name, argument in self.configs[operation.cli_name].items():
            argument_from_table = argument_table[name]
            overwritten = False

            LOG.debug('Flattening {0} argument {1} into {2}'.format(
                operation, name,
                ', '.join([v['name'] for k, v in argument['flatten'].items()])
            ))

            # For each parameter to flatten out
            for sub_argument, new_config in argument['flatten'].items():
                config = new_config.copy()
                config['container'] = argument_from_table
                config['prop'] = sub_argument

                # Handle nested arguments
                _arg = self._find_nested_arg(
                    argument_from_table.argument_object, sub_argument
                )

                # Pull out docs and required attribute
                self._merge_member_config(_arg, sub_argument, config)

                # Create and set the new flattened argument
                new_arg = FlattenedArgument(**config)
                argument_table[new_config['name']] = new_arg

                if name == new_config['name']:
                    overwritten = True

            # Delete the original argument?
            if not overwritten and ('keep' not in argument or
                                    not argument['keep']):
                del argument_table[name]

    def _find_nested_arg(self, argument, name):
        """
        Find and return a nested argument, if it exists. If no nested argument
        is requested then the original argument is returned. If the nested
        argument cannot be found, then a ValueError is raised.
        """
        if SEP in name:
            # Find the actual nested argument to pull out
            LOG.debug('Finding nested argument in {0}'.format(name))
            for piece in name.split(SEP)[:-1]:
                for member in argument.members:
                    if member.name == piece:
                        argument = member
                        break
                else:
                    raise ValueError('Invalid piece {0}'.format(piece))

        return argument

    def _merge_member_config(self, argument, name, config):
        """
        Merges an existing config taken from the configuration dict with an
        existing member of an existing argument object. This pulls in
        attributes like ``required`` and ``help_text`` if they have not been
        overridden in the configuration dict. Modifies the config in-place.
        """
        # Pull out docs and required attribute
        for member in argument.members:
            if member.name == name.split(SEP)[-1]:
                if 'help_text' not in config:
                    config['help_text'] = member.documentation

                if 'required' not in config:
                    config['required'] = member.required

                if 'type' not in config:
                    config['type'] = member.type

                break

########NEW FILE########
__FILENAME__ = globalargs
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import sys
import os

import jmespath


def register_parse_global_args(cli):
    cli.register('top-level-args-parsed', resolve_types)


def resolve_types(parsed_args, **kwargs):
    # This emulates the "type" arg from argparse, but does so in a way
    # that plugins can also hook into this process.
    _resolve_arg(parsed_args, 'query')
    _resolve_arg(parsed_args, 'verify_ssl')


def _resolve_arg(parsed_args, name):
    value = getattr(parsed_args, name, None)
    if value is not None:
        new_value = getattr(sys.modules[__name__], '_resolve_%s' % name)(value)
        setattr(parsed_args, name, new_value)


def _resolve_query(value):
    try:
        return jmespath.compile(value)
    except Exception as e:
        raise ValueError("Bad value for --query %s: %s" % (value, str(e)))


def _resolve_verify_ssl(value):
    verify = None
    if not value:
        verify = False
    else:
        verify = os.environ.get('AWS_CA_BUNDLE')
    return verify

########NEW FILE########
__FILENAME__ = iamvirtmfa
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
This customization makes it easier to deal with the bootstrapping
data returned by the ``iam create-virtual-mfa-device`` command.
You can choose to bootstrap via a QRCode or via a Base32String.
You specify your choice via the ``--bootstrap-method`` option
which should be either "QRCodePNG" or "Base32StringSeed".  You
then specify the path to where you would like your bootstrapping
data saved using the ``--outfile`` option.  The command will
pull the appropriate data field out of the response and write it
to the specified file.  It will also remove the two bootstrap data
fields from the response.
"""
import base64
import os

from awscli.arguments import CustomArgument

CHOICES = ('QRCodePNG', 'Base32StringSeed')
OUTPUT_HELP = ('The output path and file name where the bootstrap '
               'information will be stored.')
BOOTSTRAP_HELP = ('Method to use to seed the virtual MFA.  '
                  'Valid values are: %s | %s' % CHOICES)


class StatefulArgument(CustomArgument):

    def __init__(self, *args, **kwargs):
        super(StatefulArgument, self).__init__(*args, **kwargs)
        self._value = None

    def add_to_params(self, parameters, value):
        self._value = value

    @property
    def value(self):
        return self._value


class FileArgument(StatefulArgument):

    def add_to_params(self, parameters, value):
        # Validate the file here so we can raise an error prior
        # calling the service.
        outfile = os.path.expandvars(value)
        outfile = os.path.expanduser(outfile)
        if not os.access(os.path.dirname(outfile), os.W_OK):
            raise ValueError('Unable to write to file: %s' % outfile)
        self._value = outfile
        


class IAMVMFAWrapper(object):

    def __init__(self, event_handler):
        self._event_handler = event_handler
        self._operation = None
        self._outfile = FileArgument(
            'outfile', help_text=OUTPUT_HELP, required=True)
        self._method = StatefulArgument(
            'bootstrap-method', help_text=BOOTSTRAP_HELP,
            choices=CHOICES, required=True)
        self._event_handler.register(
            'building-argument-table.iam.create-virtual-mfa-device',
            self._add_options)
        self._event_handler.register(
            'after-call.iam.CreateVirtualMFADevice', self._save_file)

    def _add_options(self, argument_table, operation, **kwargs):
        self._operation = operation
        argument_table['outfile'] = self._outfile
        argument_table['bootstrap-method'] = self._method

    def _save_file(self, http_response, parsed, **kwargs):
        method = self._method.value
        outfile = self._outfile.value
        if method in parsed['VirtualMFADevice']:
            body = parsed['VirtualMFADevice'][method]
            with open(outfile, 'wb') as fp:
                fp.write(base64.b64decode(body))
            for choice in CHOICES:
                if choice in parsed['VirtualMFADevice']:
                    del parsed['VirtualMFADevice'][choice]

########NEW FILE########
__FILENAME__ = paginate
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""This module has customizations to unify paging paramters.

For any operation that can be paginated, we will:

    * Hide the service specific pagination params.  This can vary across
    services and we're going to replace them with a consistent set of
    arguments.  The arguments will still work, but they are not
    documented.  This allows us to add a pagination config after
    the fact and still remain backwards compatible with users that
    were manually doing pagination.
    * Add a ``--starting-token`` and a ``--max-items`` argument.

"""
import logging
from functools import partial

from awscli.arguments import BaseCLIArgument
from botocore.parameters import StringParameter

logger = logging.getLogger(__name__)


STARTING_TOKEN_HELP = """
<p>A token to specify where to start paginating.  This is the
<code>NextToken</code> from a previously truncated response.</p>
"""

MAX_ITEMS_HELP = """
<p>The total number of items to return.  If the total number
of items available is more than the value specified in
max-items then a <code>NextToken</code> will
be provided in the output that you can use to resume pagination.
"""


def register_pagination(event_handlers):
    event_handlers.register('building-argument-table',
                            unify_paging_params)


def unify_paging_params(argument_table, operation, event_name, **kwargs):
    if not operation.can_paginate:
        # We only apply these customizations to paginated responses.
        return
    logger.debug("Modifying paging parameters for operation: %s", operation)
    _remove_existing_paging_arguments(argument_table, operation)
    parsed_args_event = event_name.replace('building-argument-table.',
                                           'operation-args-parsed.')
    operation.session.register(
        parsed_args_event,
        partial(check_should_enable_pagination,
                list(_get_all_cli_input_tokens(operation))))
    argument_table['starting-token'] = PageArgument('starting-token',
                                                    STARTING_TOKEN_HELP,
                                                    operation,
                                                    parse_type='string')
    # Try to get the pagination parameter type
    limit_param = None
    if 'limit_key' in operation.pagination:
        for param in operation.params:
            if param.name == operation.pagination['limit_key']:
                limit_param = param
                break

    type_ = limit_param and limit_param.type or 'integer'
    if limit_param and limit_param.type not in PageArgument.type_map:
        raise TypeError(('Unsupported pagination type {0} for operation {1}'
                         ' and parameter {2}').format(type_, operation.name,
                                                      limit_param.name))

    argument_table['max-items'] = PageArgument('max-items', MAX_ITEMS_HELP,
                                               operation, parse_type=type_)


def check_should_enable_pagination(input_tokens, parsed_args, parsed_globals,
                                   **kwargs):
    normalized_paging_args = ['start_token', 'max_items']
    for token in input_tokens:
        py_name = token.replace('-', '_')
        if getattr(parsed_args, py_name) is not None and \
                py_name not in normalized_paging_args:
            # The user has specified a manual (undocumented) pagination arg.
            # We need to automatically turn pagination off.
            logger.debug("User has specified a manual pagination arg. "
                         "Automatically setting --no-paginate.")
            parsed_globals.paginate = False


def _remove_existing_paging_arguments(argument_table, operation):
    for cli_name in _get_all_cli_input_tokens(operation):
        argument_table[cli_name]._UNDOCUMENTED = True


def _get_all_cli_input_tokens(operation):
    # Get all input tokens including the limit_key
    # if it exists.
    tokens = _get_input_tokens(operation)
    for token_name in tokens:
        cli_name = _get_cli_name(operation.params, token_name)
        yield cli_name
    if 'limit_key' in operation.pagination:
        key_name = operation.pagination['limit_key']
        cli_name = _get_cli_name(operation.params, key_name)
        yield cli_name


def _get_input_tokens(operation):
    config = operation.pagination
    tokens = config['input_token']
    if not isinstance(tokens, list):
        return [tokens]
    return tokens


def _get_cli_name(param_objects, token_name):
    for param in param_objects:
        if param.name == token_name:
            return param.cli_name.lstrip('-')


class PageArgument(BaseCLIArgument):
    type_map = {
        'string': str,
        'integer': int,
    }

    def __init__(self, name, documentation, operation, parse_type):
        param = StringParameter(operation, name=name, type=parse_type)
        self._name = name
        self.argument_object = param
        self._name = name
        self._documentation = documentation
        self._parse_type = parse_type

    @property
    def cli_name(self):
        return '--' + self._name

    @property
    def cli_type_name(self):
        return self._parse_type

    @property
    def required(self):
        return False

    @property
    def documentation(self):
        return self._documentation

    def add_to_parser(self, parser):
        parser.add_argument(self.cli_name, dest=self.py_name,
                            type=self.type_map[self._parse_type])

    def add_to_params(self, parameters, value):
        if value is not None:
            parameters[self.py_name] = value

########NEW FILE########
__FILENAME__ = preview
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""This module enables the preview-mode customization.

If a service is marked as being in preview mode, then any attempts
to call operations on that service will print a message pointing
the user to alternate solutions.  A user can still access this
service by enabling the service in their config file via:

    [preview]
    servicename=true

or by running:

    aws configure set preview.servicename true

Also any service that is marked as being in preview will *not*
be listed in the help docs, unless the service has been enabled
in the config file as shown above.

"""
import logging
import sys
import textwrap

from awscli.clidriver import CLICommand


logger = logging.getLogger(__name__)

# Mapping of service name to help text to print
# when a user tries to invoke a service marked as preview.
CLOUDSEARCH_HELP = """
CloudSearch has a specialized command line tool available at
http://aws.amazon.com/tools#cli. The AWS CLI does not yet
support all of the features of the CloudSearch CLI. Until these features
are added to the AWS CLI, you may have a more complete
experience using the CloudSearch CLI.
"""

EMR_HELP = """
EMR has a specialized command line tool available at
http://aws.amazon.com/tools#cli. The AWS CLI does not yet
support all of the features of the EMR CLI. Until these features
are added to the AWS CLI, you may have a more complete
experience using the EMR CLI.
"""


GENERAL_HELP = """
This service is only available as a preview service.
"""


PREVIEW_SERVICES = {
    'cloudfront': GENERAL_HELP,
    'emr': EMR_HELP,
    'sdb': GENERAL_HELP,
}


def register_preview_commands(events):
    events.register('building-command-table.main', mark_as_preview)


def mark_as_preview(command_table, session, **kwargs):
    # These are services that are marked as preview but are
    # explicitly enabled in the config file.
    allowed_services = _get_allowed_services(session)
    for preview_service in PREVIEW_SERVICES:
        if preview_service in allowed_services:
            # Then we don't need to swap it as a preview
            # service, the user has specifically asked to
            # enable this service.
            logger.debug("Preview service enabled through config file: %s",
                         preview_service)
            continue
        command_table[preview_service] = PreviewModeCommand(
            preview_service, PREVIEW_SERVICES[preview_service])


def _get_allowed_services(session):
    # For a service to be marked as preview, it must be in the
    # [preview] section and it must have a value of 'true'
    # (case insensitive).
    allowed = []
    preview_services = session.full_config.get('preview', {})
    for preview, value in preview_services.items():
        if value == 'true':
            allowed.append(preview)
    return allowed


class PreviewModeCommand(CLICommand):
    # This is a hidden attribute that tells the doc system
    # not to document this command in the provider help.
    # This is an internal implementation detail.
    _UNDOCUMENTED = True

    ENABLE_DOCS = textwrap.dedent("""\
    However, if you'd like to use a basic set of {service} commands with the
    AWS CLI, you can enable this service by adding the following to your CLI
    config file:

        [preview]
        {service}=true

    or by running:

        aws configure set preview.{service} true

    """)

    def __init__(self, service_name, service_help):
        self._service_name = service_name
        self._service_help = service_help

    def __call__(self, args, parsed_globals):
        sys.stderr.write(self._service_help)
        sys.stderr.write("\n")
        # Then let them know how to enable this service.
        sys.stderr.write(self.ENABLE_DOCS.format(service=self._service_name))
        return 1

########NEW FILE########
__FILENAME__ = putmetricdata
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
This customization adds the following scalar parameters to the
cloudwatch put-metric-data operation:

* --metric-name
* --dimensions
* --timestamp
* --value
* --statistic-values
* --unit

"""

from awscli.arguments import CustomArgument
from awscli.utils import split_on_commas
from awscli.customizations.utils import validate_mutually_exclusive_handler


def register_put_metric_data(event_handler):
    event_handler.register('building-argument-table.cloudwatch.put-metric-data',
                           _promote_args)
    event_handler.register(
        'operation-args-parsed.cloudwatch.put-metric-data',
        validate_mutually_exclusive_handler(
            ['metric_data'], ['metric_name', 'timestamp', 'unit', 'value',
                              'dimensions', 'statistic_values']))


def _promote_args(argument_table, operation, **kwargs):
    # We're providing top level params for metric-data.  This means
    # that metric-data is now longer a required arg.  We do need
    # to check that either metric-data or the complex args we've added
    # have been provided.
    argument_table['metric-data'].required = False

    argument_table['metric-name'] = PutMetricArgument(
        'metric-name', help_text='The name of the metric.')
    argument_table['timestamp'] = PutMetricArgument(
        'timestamp', help_text='The time stamp used for the metric.  '
                               'If not specified, the default value is '
                               'set to the time the metric data was '
                               'received.')
    argument_table['unit'] = PutMetricArgument(
        'unit', help_text='The unit of metric.')
    argument_table['value'] = PutMetricArgument(
        'value', help_text='The value for the metric.  Although the --value '
                           'parameter accepts numbers of type Double, '
                           'Amazon CloudWatch truncates values with very '
                           'large exponents.  Values with base-10 exponents '
                           'greater than 126 (1 x 10^126) are truncated.  '
                           'Likewise, values with base-10 exponents less '
                           'than -130 (1 x 10^-130) are also truncated.')

    argument_table['dimensions'] = PutMetricArgument(
        'dimensions', help_text=(
            'The --dimension argument further expands '
            'on the identity of a metric using a Name=Value'
            'pair, separated by commas, for example: '
            '<code>--dimensions User=SomeUser,Stack=Test</code>'))
    argument_table['statistic-values'] = PutMetricArgument(
        'statistic-values', help_text='A set of statistical values describing '
                                      'the metric.')


def insert_first_element(name):
    def _wrap_add_to_params(func):
        def _add_to_params(self, parameters, value):
            if value is None:
                return
            if name not in parameters:
                # We're taking a shortcut here and assuming that the first
                # element is a struct type, hence the default value of
                # a dict.  If this was going to be more general we'd need
                # to have this paramterized, i.e. you pass in some sort of
                # factory function that creates the initial starting value.
                parameters[name] = [{}]
            first_element = parameters[name][0]
            return func(self, first_element, value)
        return _add_to_params
    return _wrap_add_to_params


class PutMetricArgument(CustomArgument):
    def add_to_params(self, parameters, value):
        method_name = '_add_param_%s' % self.name.replace('-', '_')
        return getattr(self, method_name)(parameters, value)

    @insert_first_element('metric_data')
    def _add_param_metric_name(self, first_element, value):
        first_element['MetricName'] = value

    @insert_first_element('metric_data')
    def _add_param_unit(self, first_element, value):
        first_element['Unit'] = value

    @insert_first_element('metric_data')
    def _add_param_timestamp(self, first_element, value):
        first_element['Timestamp'] = value

    @insert_first_element('metric_data')
    def _add_param_value(self, first_element, value):
        first_element['Value'] = value

    @insert_first_element('metric_data')
    def _add_param_dimensions(self, first_element, value):
        # Dimensions needs a little more processing.  We support
        # the key=value,key2=value syntax so we need to parse
        # that.
        dimensions = []
        for pair in split_on_commas(value):
            key, value = pair.split('=')
            dimensions.append({'Name': key, 'Value': value})
        first_element['Dimensions'] = dimensions

    @insert_first_element('metric_data')
    def _add_param_statistic_values(self, first_element, value):
        # StatisticValues is a struct type so we are parsing
        # a csv keyval list into a dict.
        statistics = {}
        for pair in split_on_commas(value):
            key, value = pair.split('=')
            statistics[key] = value
        first_element['StatisticValues'] = statistics

########NEW FILE########
__FILENAME__ = rds
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
This customization splits the modify-option-group into two separate commands:

* ``add-option-group``
* ``remove-option-group``

In both commands the ``--options-to-remove`` and ``--options-to-add`` args will
be renamed to just ``--options``.

All the remaining args will be available in both commands (which proxy
modify-option-group).

"""

from awscli.clidriver import ServiceOperation
from awscli.clidriver import CLIOperationCaller
from awscli.customizations import utils


def register_rds_modify_split(cli):
    cli.register('building-command-table.rds', _building_command_table)
    cli.register('building-argument-table.rds.add-option-to-option-group',
                 _rename_add_option)
    cli.register('building-argument-table.rds.remove-option-from-option-group',
                 _rename_remove_option)


def _rename_add_option(argument_table, **kwargs):
    utils.rename_argument(argument_table, 'options-to-include',
                          new_name='options')
    del argument_table['options-to-remove']


def _rename_remove_option(argument_table, **kwargs):
    utils.rename_argument(argument_table, 'options-to-remove',
                          new_name='options')
    del argument_table['options-to-include']


def _building_command_table(command_table, session, **kwargs):
    # Hooked up to building-command-table.rds
    # We don't need the modify-option-group operation.
    del command_table['modify-option-group']
    # We're going to replace modify-option-group with two commands:
    # add-option-group and remove-option-group
    rds_service = session.get_service('rds')
    modify_operation = rds_service.get_operation('modify-option-group')
    command_table['add-option-to-option-group'] = ServiceOperation(
        parent_name='rds', name='add-option-to-option-group',
        operation_object=modify_operation,
        operation_caller=CLIOperationCaller(session),
        service_object=rds_service)
    command_table['remove-option-from-option-group'] = ServiceOperation(
        parent_name='rds', name='remove-option-from-option-group',
        operation_object=modify_operation,
        operation_caller=CLIOperationCaller(session),
        service_object=rds_service)

########NEW FILE########
__FILENAME__ = removals
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
Remove deprecated commands
--------------------------

This customization removes commands that are either deprecated or not
yet fully supported.

"""
import logging
from functools import partial

LOG = logging.getLogger(__name__)


def register_removals(event_handler):
    cmd_remover = CommandRemover(event_handler)
    cmd_remover.remove(on_event='building-command-table.ses',
                       remove_commands=['delete-verified-email-address',
                                        'list-verified-email-addresses',
                                        'verify-email-address'])
    cmd_remover.remove(on_event='building-command-table.ec2',
                       remove_commands=['import-instance', 'import-volume'])
    cmd_remover.remove(on_event='building-command-table.cloudformation',
                       remove_commands=['estimate-template-cost'])


class CommandRemover(object):
    def __init__(self, events):
        self._events = events

    def remove(self, on_event, remove_commands):
        self._events.register(on_event,
                              self._create_remover(remove_commands))

    def _create_remover(self, commands_to_remove):
        return partial(_remove_commands, commands_to_remove=commands_to_remove)


def _remove_commands(command_table, commands_to_remove, **kwargs):
    # Hooked up to building-command-table.<service>
    for command in commands_to_remove:
        try:
            LOG.debug("Removing operation: %s", command)
            del command_table[command]
        except KeyError:
            LOG.warning("Attempting to delete command that does not exist: %s",
                        command)

########NEW FILE########
__FILENAME__ = route53resourceid
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import logging


logger = logging.getLogger(__name__)


def register_resource_id(cli):
    cli.register('process-cli-arg.route53.*',
                 _check_for_resource_id)


def _check_for_resource_id(param, value, **kwargs):
    if hasattr(param, 'shape_name'):
        if param.shape_name == 'ResourceId':
            orig_value = value
            value = value.split('/')[-1]
            logger.debug('ResourceId %s -> %s', orig_value, value)
            return value

########NEW FILE########
__FILENAME__ = comparator
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import logging
from six import advance_iterator


LOG = logging.getLogger(__name__)


def total_seconds(td):
    """
    timedelta's time_seconds() function for python 2.6 users
    """
    return (td.microseconds + (td.seconds + td.days * 24 *
                               3600) * 10**6) / 10**6


class Comparator(object):
    """
    This class performs all of the comparisons behind the sync operation
    """
    def __init__(self, params=None):
        self.delete = False
        if 'delete' in params:
            self.delete = params['delete']

        self.compare_on_size_only = False
        if 'size_only' in params:
            self.compare_on_size_only = params['size_only']

    def call(self, src_files, dest_files):
        """
        This function preforms the actual comparisons.  The parameters it takes
        are the generated files for both the source and the destination.  The
        key concept in this function is that no matter the type of where the
        files are coming from, they are listed in the same order, least to
        greatest in collation order.  This allows for easy comparisons to
        determine if file needs to be added or deleted.  Comparison keys are
        used to determine if two files are the same and each file has a
        unique comparison key.  If they are the same compare the size and
        last modified times to see if a file needs to be updated.   Ultimately,
        it will yield a sequence of file info objectsthat will be sent to
        the ``S3Handler``.

        :param src_files: The generated FileInfo objects from the source.
        :param dest_files: The genereated FileInfo objects from the dest.

        :returns: Yields the FilInfo objects of the files that need to be
            operated on

        Algorithm:
            Try to take next from both files. If it is empty signal
            corresponding done flag.  If both generated lists are not done
            compare compare_keys.  If equal, compare size and time to see if
            it needs to be updated.  If source compare_key is less than dest
            compare_key, the file needs to be added to the destination.  Take
            the next source file but not not destination file.  If the source
            compare_key is greater than dest compare_key, that destination file
            needs to be deleted from the destination.  Take the next dest file
            but not the source file.  If the source list is empty delete the
            rest of the files in the dest list from the destination.  If the
            dest list is empty add the rest of the file in source list to
            the destionation.
        """
        # :var src_done: True if there are no more files from the source left.
        src_done = False
        # :var dest_done: True if there are no more files form the dest left.
        dest_done = False
        # :var src_take: Take the next source file from the generated files if
        #     true
        src_take = True
        # :var dest_take: Take the next dest file from the generated files if
        #     true
        dest_take = True
        while True:
            try:
                if (not src_done) and src_take:
                    src_file = advance_iterator(src_files)
            except StopIteration:
                src_file = None
                src_done = True
            try:
                if (not dest_done) and dest_take:
                    dest_file = advance_iterator(dest_files)
            except StopIteration:
                dest_file = None
                dest_done = True

            if (not src_done) and (not dest_done):
                src_take = True
                dest_take = True

                compare_keys = self.compare_comp_key(src_file, dest_file)

                if compare_keys == 'equal':
                    same_size = self.compare_size(src_file, dest_file)
                    same_last_modified_time = self.compare_time(src_file, dest_file)

                    if self.compare_on_size_only:
                        should_sync = not same_size
                    else:
                        should_sync = (not same_size) or (not same_last_modified_time)

                    if should_sync:
                        LOG.debug("syncing: %s -> %s, size_changed: %s, "
                                  "last_modified_time_changed: %s",
                                  src_file.src, src_file.dest,
                                  not same_size, not same_last_modified_time)
                        yield src_file
                elif compare_keys == 'less_than':
                    src_take = True
                    dest_take = False
                    LOG.debug("syncing: %s -> %s, file does not exist at destination",
                            src_file.src, src_file.dest)
                    yield src_file

                elif compare_keys == 'greater_than':
                    src_take = False
                    dest_take = True
                    dest_file.operation_name = 'delete'
                    if self.delete:
                        LOG.debug("syncing: (None) -> %s (remove), file does "
                                  "not exist at source (%s) and delete "
                                  "mode enabled",
                                  dest_file.src, dest_file.dest)
                        yield dest_file

            elif (not src_done) and dest_done:
                src_take = True
                LOG.debug("syncing: %s -> %s, file does not exist "
                          "at destination",
                          src_file.src, src_file.dest)
                yield src_file

            elif src_done and (not dest_done):
                dest_take = True
                dest_file.operation_name = 'delete'
                if self.delete:
                    LOG.debug("syncing: (None) -> %s (remove), file does not "
                              "exist at source (%s) and delete mode enabled",
                              dest_file.src, dest_file.dest)
                    yield dest_file
            else:
                break

    def compare_size(self, src_file, dest_file):
        """
        :returns: True if the sizes are the same.
            False otherwise.
        """
        return src_file.size == dest_file.size

    def compare_comp_key(self, src_file, dest_file):
        """
        Determines if the source compare_key is less than, equal to,
        or greater than the destination compare_key
        """

        src_comp_key = src_file.compare_key
        dest_comp_key = dest_file.compare_key
        if (src_comp_key == dest_comp_key):
            return 'equal'

        elif (src_comp_key < dest_comp_key):
            return 'less_than'

        else:
            return 'greater_than'

    def compare_time(self, src_file, dest_file):
        """
        :returns: True if the file does not need updating based on time of
            last modification and type of operation.
            False if the file does need updating based on the time of
            last modification and type of operation.
        """
        src_time = src_file.last_update
        dest_time = dest_file.last_update
        delta = dest_time - src_time
        cmd = src_file.operation_name
        if cmd == "upload" or cmd == "copy":
            if total_seconds(delta) >= 0:
                # Destination is newer than source.
                return True
            else:
                # Destination is older than source, so
                # we have a more recently updated file
                # at the source location.
                return False
        elif cmd == "download":
            if total_seconds(delta) <= 0:
                return True
            else:
                # delta is positive, so the destination
                # is newer than the source.
                return False

########NEW FILE########
__FILENAME__ = constants
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
MULTI_THRESHOLD = 8 * (1024 ** 2)
CHUNKSIZE = 7 * (1024 ** 2)
NUM_THREADS = 10
QUEUE_TIMEOUT_WAIT = 0.2
MAX_PARTS = 950
MAX_SINGLE_UPLOAD_SIZE = 5 * (1024 ** 3)
MAX_UPLOAD_SIZE = 5 * (1024 ** 4)
MAX_QUEUE_SIZE = 1000

########NEW FILE########
__FILENAME__ = description
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.


def add_command_descriptions(cmd_dict):
    """
    This function adds descritpions to the various commands along with
    usage.
    """
    cmd_dict['cp']['description'] = "Copies a local file or S3 object to \
                                     another location locally or in S3."
    cmd_dict['cp']['usage'] = "<LocalPath> <S3Path> or <S3Path> <LocalPath> " \
                              "or <S3Path> <S3Path>"

    cmd_dict['mv']['description'] = "Moves a local file or S3 object to " \
                                    "another location locally or in S3."
    cmd_dict['mv']['usage'] = "<LocalPath> <S3Path> or <S3Path> <LocalPath> " \
                              "or <S3Path> <S3Path>"

    cmd_dict['rm']['description'] = "Deletes an S3 object."
    cmd_dict['rm']['usage'] = "<S3Path>"

    cmd_dict['sync']['description'] = "Syncs directories and S3 prefixes."
    cmd_dict['sync']['usage'] = "<LocalPath> <S3Path> or <S3Path> " \
                                "<LocalPath> or <S3Path> <S3Path>"

    cmd_dict['ls']['description'] = "List S3 objects and common prefixes " \
                                    "under a prefix or all S3 buckets. " \
                                    "Note that the --output argument " \
                                    "is ignored for this command."
    cmd_dict['ls']['usage'] = "<S3Path> or NONE"

    cmd_dict['mb']['description'] = "Creates an S3 bucket."
    cmd_dict['mb']['usage'] = "<S3Path>"

    cmd_dict['rb']['description'] = "Deletes an S3 bucket."
    cmd_dict['rb']['usage'] = "<S3Path>"


def add_param_descriptions(params_dict):
    """
    This function adds descriptions to the various parameters that can be
    used in commands.
    """
    params_dict['dryrun']['documents'] = "Displays the operations that " \
        "would be performed using the specified command without actually" \
        "running them."

    params_dict['quiet']['documents'] = "Does not display the operations " \
        "performed from the specified command."

    params_dict['recursive']['documents'] = "Command is performed on all" \
        "files or objects under the specified directory or prefix."

    params_dict['delete']['documents'] = "Files that exist in the " \
        "destination but not in the source are deleted during sync."

    params_dict['exclude']['documents'] = "Exclude all files or objects" \
        " from the command that matches the specified pattern."

    params_dict['include']['documents'] = "Don't exclude files or objects in " \
        "the command that match the specified pattern"

    params_dict['acl']['documents'] = "Sets the ACl for the object when the " \
        "command is performed.  Only accepts values of ``private``, \
        ``public-read``, ``public-read-write``, ``authenticated-read``, \
        ``bucket-owner-read``, ``bucket-owner-full-control`` and \
        ``log-delivery-write``."

    params_dict['force']['documents'] = "Deletes all objects in the bucket " \
        "including the bucket itself."

    params_dict['no-guess-mime-type']['documents'] = (
        "Do not try to guess the mime type for uploaded files.  By default the "
        "mime type of a file is guessed when it is uploaded.")

    params_dict['content-type']['documents'] = (
        "Specify an explicit content type for this operation.  "
        "This value overrides any guessed mime types.")

    params_dict['cache-control']['documents'] = \
        "Specifies caching behavior along the request/reply chain."

    params_dict['content-disposition']['documents'] = \
        "Specifies presentational information for the object."
    
    params_dict['content-encoding']['documents'] = (
        "Specifies what content encodings have been "
        "applied to the object and thus what decoding mechanisms "
        "must be applied to obtain the media-type referenced "
        "by the Content-Type header field.")
    
    params_dict['content-language']['documents'] = \
        "The language the content is in."

    params_dict['expires']['documents'] = \
        "The date and time at which the object is no longer cacheable."
    
    params_dict['sse']['documents'] = (
        "Enable Server Side Encryption of the object in S3")

    params_dict['storage-class']['documents'] = (
        "The type of storage to use for the object. "
        "Valid choices are: STANDARD | REDUCED_REDUNDANCY. "
        "Defaults to 'STANDARD'")

    params_dict['website-redirect']['documents'] = (
        "If the bucket is configured as a website, redirects requests "
        "for this object to another object in the same bucket or to an "
        "external URL. Amazon S3 stores the value of this header in the "
        "object metadata.")

    params_dict['grants']['documents'] = (
        "Grant specific permissions to individual users or groups.  "
        "You can supply a list of grants of the form::<p/>"
        "  --grants Permission=Grantee_Type=Grantee_ID "
        "[Permission=Grantee_Type=Grantee_ID ...]<p/>"
        "Each value contains the following elements:<p/>"
        "<ul><li><code>Permission</code> - Specifies the granted"
        "permissions, and can be set to read, readacl, writeacl, or full.</li>"
        "<li><code>Grantee_Type</code> - Specifies how the grantee is to "
        "be identified, and can be set to uri, emailaddress, or id.</li>"
        "<li><code>Grantee_ID</code> - Specifies the grantee based on Grantee_Type."
        "</li></ul>"
        "The <code>Grantee_ID</code> value can be one of:"
        "<ul><li><b>uri</b> - The group's URI. For more information, see "
        '<a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/ACLOverview.html#SpecifyingGrantee">'
        "Who Is a Grantee?</a></li>"
        "<li><b>emailaddress</b> - The account's email address.</li>"
        "<li><b>id</b> - The account's canonical ID</li></ul>"
        "</li></ul>"
        "For more information on Amazon S3 access control, see "
        '<a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/UsingAuthAccess.html">Access Control</a>')


########NEW FILE########
__FILENAME__ = dochandler
import os

import awscli
from awscli.clidocs import CLIDocumentEventHandler


class S3DocumentEventHandler(CLIDocumentEventHandler):
    """
    This is the document handler for both the service, s3, and
    the commands. It is the basis for the help command and generating docs.
    """
    def doc_title(self, help_command, **kwargs):
        doc = help_command.doc
        command = help_command.obj
        doc.style.h1(command._name)

    def doc_description(self, help_command, **kwargs):
        doc = help_command.doc
        command = help_command.obj
        doc.style.h2('Description')
        doc.include_doc_string(command.documentation)
        if help_command.obj._name == 's3':
            doc_dir = os.path.join(
                os.path.dirname(os.path.abspath(awscli.__file__)),
                'examples', help_command.event_class.lower())
            # The file is named '_concepts.rst' so that it doesn't
            # collide with any s3 commands, in the rare chance we
            # create a subcommand called "concepts".
            doc_path = os.path.join(doc_dir, '_concepts.rst')
            if os.path.isfile(doc_path):
                help_command.doc.style.h2('Important Concepts')
                fp = open(doc_path)
                for line in fp.readlines():
                    help_command.doc.write(line)

    def doc_synopsis_start(self, help_command, **kwargs):
        if help_command.obj._name != 's3':
            doc = help_command.doc
            command = help_command.obj
            doc.style.h2('Synopsis')
            doc.style.start_codeblock()
            doc.writeln('%s %s' % (command._name, command.usage))

    def doc_synopsis_option(self, arg_name, help_command, **kwargs):
        doc = help_command.doc
        argument = help_command.arg_table[arg_name]
        option_str = argument._name
        if 'nargs' in argument.options:
            if argument.options['nargs'] == '+':
                option_str += " <value> [<value>...]"
            else:
                for i in range(argument.options['nargs']):
                    option_str += " <value>"
        doc.writeln('[--%s]' % option_str)

    def doc_synopsis_end(self, help_command, **kwargs):
        if help_command.obj._name != 's3':
            doc = help_command.doc
            doc.style.end_codeblock()

    def doc_options_start(self, help_command, **kwargs):
        if help_command.obj._name != 's3':
            doc = help_command.doc
            doc.style.h2('Options')
            if len(help_command.arg_table) == 0:
                doc.write('*None*\n')

    def doc_option(self, arg_name, help_command, **kwargs):
        doc = help_command.doc
        argument = help_command.arg_table[arg_name]
        doc.write('``--%s``\n' % argument._name)
        doc.style.indent()
        doc.include_doc_string(argument.documentation)
        doc.style.dedent()
        doc.style.new_paragraph()

    def doc_subitems_start(self, help_command, **kwargs):
        if help_command.command_table:
            doc = help_command.doc
            doc.style.h2('Available Commands')
            doc.style.toctree()

    def doc_subitem(self, command_name, help_command, **kwargs):
        doc = help_command.doc
        doc.style.tocitem(command_name)




########NEW FILE########
__FILENAME__ = executor
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import logging
from six.moves import queue
import sys
import threading

from awscli.customizations.s3.utils import uni_print, \
        IORequest, IOCloseRequest, StablePriorityQueue
from awscli.customizations.s3.tasks import OrderableTask


LOGGER = logging.getLogger(__name__)


class ShutdownThreadRequest(OrderableTask):
    PRIORITY = 11

    def __init__(self, priority_override=None):
        if priority_override is not None:
            self.PRIORITY = priority_override


class Executor(object):
    """
    This class is in charge of all of the threads.  It starts up the threads
    and cleans up the threads when finished.  The two type of threads the
    ``Executor``runs is a worker and a print thread.
    """
    STANDARD_PRIORITY = 11
    IMMEDIATE_PRIORITY= 1

    def __init__(self, num_threads, result_queue,
                 quiet, max_queue_size, write_queue):
        self._max_queue_size = max_queue_size
        self.queue = StablePriorityQueue(maxsize=self._max_queue_size,
                                         max_priority=20)
        self.num_threads = num_threads
        self.result_queue = result_queue
        self.quiet = quiet
        self.threads_list = []
        self.write_queue = write_queue
        self.print_thread = PrintThread(self.result_queue,
                                        self.quiet)
        self.print_thread.daemon = True
        self.io_thread = IOWriterThread(self.write_queue)

    @property
    def num_tasks_failed(self):
        tasks_failed = 0
        if self.print_thread is not None:
            tasks_failed = self.print_thread.num_errors_seen
        return tasks_failed

    def start(self):
        self.io_thread.start()
        # Note that we're *not* adding the IO thread to the threads_list.
        # There's a specific shutdown order we need and we're going to be
        # explicit about it rather than relying on the threads_list order.
        # See .join() for more info.
        self.print_thread.start()
        for i in range(self.num_threads):
            worker = Worker(queue=self.queue)
            worker.setDaemon(True)
            self.threads_list.append(worker)
            worker.start()

    def submit(self, task):
        """
        This is the function used to submit a task to the ``Executor``.
        """
        LOGGER.debug("Submitting task: %s", task)
        self.queue.put(task)

    def initiate_shutdown(self, priority=STANDARD_PRIORITY):
        """Instruct all threads to shutdown.

        This is a graceful shutdown.  It will wait until all
        currently queued tasks have been completed before the threads
        shutdown.  If the task queue is completely full, it may
        take a while for the threads to shutdown.

        This method does not block.  Once ``initiate_shutdown`` has
        been called, you can all ``wait_until_shutdown`` to block
        until the Executor has been shutdown.

        """
        # Implementation detail:  we only queue the worker threads
        # to shutdown.  The print/io threads are shutdown in the
        # ``wait_until_shutdown`` method.
        for i in range(self.num_threads):
            LOGGER.debug(
                "Queueing end sentinel for worker thread (priority: %s)",
                priority)
            self.queue.put(ShutdownThreadRequest(priority))

    def wait_until_shutdown(self):
        """Block until the Executor is fully shutdown.

        This will wait until all worker threads are shutdown, along
        with any additional helper threads used by the executor.

        """
        for thread in self.threads_list:
            LOGGER.debug("Waiting for thread to shutdown: %s", thread)
            while True:
                thread.join(timeout=1)
                if not thread.is_alive():
                    break
            LOGGER.debug("Thread has been shutdown: %s", thread)

        LOGGER.debug("Queueing end sentinel for result thread.")
        self.result_queue.put(ShutdownThreadRequest())
        LOGGER.debug("Queueing end sentinel for IO thread.")
        self.write_queue.put(ShutdownThreadRequest())

        LOGGER.debug("Waiting for result thread to shutdown.")
        self.print_thread.join()
        LOGGER.debug("Waiting for IO thread to shutdown.")
        self.io_thread.join()
        LOGGER.debug("All threads have been shutdown.")


class IOWriterThread(threading.Thread):
    def __init__(self, queue):
        threading.Thread.__init__(self)
        self.queue = queue
        self.fd_descriptor_cache = {}

    def run(self):
        while True:
            task = self.queue.get(True)
            if isinstance(task, ShutdownThreadRequest):
                LOGGER.debug("Shutdown request received in IO thread, "
                             "shutting down.")
                self._cleanup()
                return
            elif isinstance(task, IORequest):
                filename, offset, data = task
                fileobj = self.fd_descriptor_cache.get(filename)
                if fileobj is None:
                    fileobj = open(filename, 'rb+')
                    self.fd_descriptor_cache[filename] = fileobj
                fileobj.seek(offset)
                LOGGER.debug("Writing data to: %s, offset: %s",
                             filename, offset)
                fileobj.write(data)
                fileobj.flush()
            elif isinstance(task, IOCloseRequest):
                LOGGER.debug("IOCloseRequest received for %s, closing file.",
                             task.filename)
                fileobj = self.fd_descriptor_cache.get(task.filename)
                if fileobj is not None:
                    fileobj.close()
                    del self.fd_descriptor_cache[task.filename]

    def _cleanup(self):
        for fileobj in self.fd_descriptor_cache.values():
            fileobj.close()


class Worker(threading.Thread):
    """
    This thread is in charge of performing the tasks provided via
    the main queue ``queue``.
    """
    def __init__(self, queue):
        threading.Thread.__init__(self)
        # This is the queue where work (tasks) are submitted.
        self.queue = queue

    def run(self):
        while True:
            try:
                function = self.queue.get(True)
                if isinstance(function, ShutdownThreadRequest):
                    LOGGER.debug("Shutdown request received in worker thread, "
                                 "shutting down worker thread.")
                    break
                try:
                    LOGGER.debug("Worker thread invoking task: %s", function)
                    function()
                except Exception as e:
                    LOGGER.debug('Error calling task: %s', e, exc_info=True)
            except queue.Empty:
                pass


class PrintThread(threading.Thread):
    """
    This thread controls the printing of results.  When a task is
    completely finished it is permanently write the result to standard
    out. Otherwise, it is a part of a multipart upload/download and
    only shows the most current part upload/download.

    Result Queue
    ------------

    Result queue items are dictionaries that have the following keys:

        * message: An arbitrary string associated with the entry.   This
            can be used to communicate the result of the task.
        * error: Boolean indicating whether or not the task completely
            successfully.
        * total_parts: The total number of parts for multipart transfers (
            deprecated, will be removed in the future).

    """
    def __init__(self, result_queue, quiet):
        threading.Thread.__init__(self)
        self._progress_dict = {}
        self._result_queue = result_queue
        self._quiet = quiet
        self._progress_length = 0
        self._num_parts = 0
        self._file_count = 0
        self._lock = threading.Lock()
        self._needs_newline = False

        self._total_parts = 0
        self._total_files = '...'

        # This is a public attribute that clients can inspect to determine
        # whether or not we saw any results indicating that an error occurred.
        self.num_errors_seen = 0

    def set_total_parts(self, total_parts):
        with self._lock:
            self._total_parts = total_parts

    def set_total_files(self, total_files):
        with self._lock:
            self._total_files = total_files

    def run(self):
        while True:
            try:
                print_task = self._result_queue.get(True)
                if isinstance(print_task, ShutdownThreadRequest):
                    if self._needs_newline:
                        sys.stdout.write('\n')
                    LOGGER.debug("Shutdown request received in print thread, "
                                 "shutting down print thread.")
                    break
                LOGGER.debug("Received print task: %s", print_task)
                try:
                    self._process_print_task(print_task)
                except Exception as e:
                    LOGGER.debug("Error processing print task: %s", e,
                                 exc_info=True)
            except queue.Empty:
                pass

    def _process_print_task(self, print_task):
        print_str = print_task['message']
        if print_task['error']:
            self.num_errors_seen += 1
        final_str = ''
        if 'total_parts' in print_task:
            # Normalize keys so failures and sucess
            # look the same.
            op_list = print_str.split(':')
            print_str = ':'.join(op_list[1:])
            total_part = print_task['total_parts']
            self._num_parts += 1
            if print_str in self._progress_dict:
                self._progress_dict[print_str]['parts'] += 1
            else:
                self._progress_dict[print_str] = {}
                self._progress_dict[print_str]['parts'] = 1
                self._progress_dict[print_str]['total'] = total_part
        else:
            print_components = print_str.split(':')
            final_str += print_str.ljust(self._progress_length, ' ')
            final_str += '\n'
            key = ':'.join(print_components[1:])
            if key in self._progress_dict:
                self._progress_dict.pop(print_str, None)
            else:
                self._num_parts += 1
            self._file_count += 1

        is_done = self._total_files == self._file_count
        if not is_done:
            prog_str = "Completed %s " % self._num_parts
            num_files = self._total_files
            if self._total_files != '...':
                prog_str += "of %s " % self._total_parts
                num_files = self._total_files - self._file_count
            prog_str += "part(s) with %s file(s) remaining" % \
                num_files
            length_prog = len(prog_str)
            prog_str += '\r'
            prog_str = prog_str.ljust(self._progress_length, ' ')
            self._progress_length = length_prog
            final_str += prog_str
        if not self._quiet:
            uni_print(final_str)
            self._needs_newline = not final_str.endswith('\n')
            sys.stdout.flush()

########NEW FILE########
__FILENAME__ = fileformat
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os


class FileFormat(object):
    def format(self, src, dest, parameters):
        """
        This function formats the source and destination
        path to the proper form for a file generator.

        Note that a file is designated as an s3 file if it begins with s3://

        :param src: The path of the source
        :type src: string
        :param dest: The path of the dest
        :type dest: string
        :param parameters: A dictionary that will be formed when the arguments
            of the command line have been parsed.  For this
            function the dictionary should have the key 'dir_op'
            which is a boolean value that is true when
            the operation is being performed on a local directory/
            all objects under a common prefix in s3 or false when
            it is on a single file/object.

        :returns: A dictionary that will be passed to a file generator.
            The dictionary contains the keys src, dest, dir_op, and
            use_src_name. src is a dictionary containing the source path
            and whether its located locally or in s3. dest is a dictionary
            containing the destination path and whether its located
            locally or in s3.
        """
        src_type, src_path = self.identify_type(src)
        dest_type, dest_path = self.identify_type(dest)
        format_table = {'s3': self.s3_format, 'local': self.local_format}
        # :var dir_op: True when the operation being performed is on a
        #     directory/objects under a common prefix or false when it
        #     is a single file
        dir_op = parameters['dir_op']
        src_path = format_table[src_type](src_path, dir_op)[0]
        # :var use_src_name: True when the destination file/object will take on
        #     the name of the source file/object.  False when it
        #     will take on the name the user specified in the
        #     command line.
        dest_path, use_src_name = format_table[dest_type](dest_path, dir_op)
        files = {'src': {'path': src_path, 'type': src_type},
                 'dest': {'path': dest_path, 'type': dest_type},
                 'dir_op': dir_op, 'use_src_name': use_src_name}
        return files

    def local_format(self, path, dir_op):
        """
        This function formats the path of local files and returns whether the
        destination will keep its own name or take the source's name along with
        the editted path.
        Formatting Rules:
            1) If a destination file is taking on a source name, it must end
               with the apporpriate operating system seperator

        General Options:
            1) If the operation is on a directory, the destination file will
               always use the name of the corresponding source file.
            2) If the path of the destination exists and is a directory it
               will always use the name of the source file.
            3) If the destination path ends with the appropriate operating
               system seperator but is not an existing directory, the
               appropriate directories will be made and the file will use the
               source's name.
            4) If the destination path does not end with the appropriate
               operating system seperator and is not an existing directory, the
               appropriate directories will be created and the file name will
               be of the one provided.
        """
        full_path = os.path.abspath(path)
        if (os.path.exists(full_path) and os.path.isdir(full_path)) or dir_op:
            full_path += os.sep
            return full_path, True
        else:
            if path.endswith(os.sep):
                full_path += os.sep
                return full_path, True
            else:
                return full_path, False

    def s3_format(self, path, dir_op):
        """
        This function formats the path of source files and returns whether the
        destination will keep its own name or take the source's name along
        with the edited path.
        Formatting Rules:
            1) If a destination file is taking on a source name, it must end
               with a forward slash.
        General Options:
            1) If the operation is on objects under a common prefix,
               the destination file will always use the name of the
               corresponding source file.
            2) If the path ends with a forward slash, the appropriate prefixes
               will be formed and will use the name of the source.
            3) If the path does not end with a forward slash, the appropriate
               prefix will be formed but use the the name provided as opposed
               to the source name.
        """
        if dir_op:
            if not path.endswith('/'):
                path += '/'
            return path, True
        else:
            if not path.endswith('/'):
                return path, False
            else:
                return path, True

    def identify_type(self, path):
        """
        It identifies whether the path is from local or s3.  Returns the
        adjusted pathname and a string stating whether the file is from local
        or s3.  If from s3 it strips off the s3:// from the beginnning of the
        path
        """
        if path.startswith('s3://'):
            return 's3', path[5:]
        else:
            return 'local', path

########NEW FILE########
__FILENAME__ = filegenerator
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
import sys

import six
from dateutil.parser import parse
from dateutil.tz import tzlocal

from awscli.customizations.s3.fileinfo import FileInfo
from awscli.customizations.s3.utils import find_bucket_key, get_file_stat
from awscli.customizations.s3.utils import BucketLister
from awscli.errorhandler import ClientError


# This class is provided primarily to provide a detailed error message.

class FileDecodingError(Exception):
    """Raised when there was an issue decoding the file."""

    ADVICE = (
        "Please check your locale settings.  The filename was decoded as: %s\n"
        "On posix platforms, check the LC_CTYPE environment variable."
            % (sys.getfilesystemencoding())
    )

    def __init__(self, directory, filename):
        self.directory = directory
        self.file_name = filename
        self.error_message = (
            'There was an error trying to decode the the file "%s" in '
            'directory "%s". \n%s' % (self.file_name,
                                      self.directory.encode('utf-8'),
                                      self.ADVICE)
        )
        super(FileDecodingError, self).__init__(self.error_message)


class FileGenerator(object):
    """
    This is a class the creates a generator to yield files based on information
    returned from the ``FileFormat`` class.  It is universal in the sense that
    it will handle s3 files, local files, local directories, and s3 objects
    under the same common prefix.  The generator yields corresponding
    ``FileInfo`` objects to send to a ``Comparator`` or ``S3Handler``.
    """
    def __init__(self, service, endpoint, operation_name, parameters):
        self._service = service
        self._endpoint = endpoint
        self.operation_name = operation_name

    def call(self, files):
        """
        This is the generalized function to yield the ``FileInfo`` objects.
        ``dir_op`` and ``use_src_name`` flags affect which files are used and
        ensure the proper destination paths and compare keys are formed.
        """
        src = files['src']
        dest = files['dest']
        src_type = src['type']
        dest_type = dest['type']
        function_table = {'s3': self.list_objects, 'local': self.list_files}
        sep_table = {'s3': '/', 'local': os.sep}
        source = src['path']
        file_list = function_table[src_type](source, files['dir_op'])
        for src_path, size, last_update in file_list:
            if files['dir_op']:
                rel_path = src_path[len(src['path']):]
            else:
                rel_path = src_path.split(sep_table[src_type])[-1]
            compare_key = rel_path.replace(sep_table[src_type], '/')
            if files['use_src_name']:
                dest_path = dest['path']
                dest_path += rel_path.replace(sep_table[src_type],
                                              sep_table[dest_type])
            else:
                dest_path = dest['path']
            yield FileInfo(src=src_path, dest=dest_path,
                           compare_key=compare_key, size=size,
                           last_update=last_update, src_type=src_type,
                           service=self._service, endpoint=self._endpoint,
                           dest_type=dest_type,
                           operation_name=self.operation_name)

    def list_files(self, path, dir_op):
        """
        This function yields the appropriate local file or local files
        under a directory depending on if the operation is on a directory.
        For directories a depth first search is implemented in order to
        follow the same sorted pattern as a s3 list objects operation
        outputs.  It yields the file's source path, size, and last
        update
        """
        join, isdir, isfile = os.path.join, os.path.isdir, os.path.isfile
        error, listdir = os.error, os.listdir
        if not dir_op:
            size, last_update = get_file_stat(path)
            yield path, size, last_update
        else:
            # We need to list files in byte order based on the full
            # expanded path of the key: 'test/1/2/3.txt'  However, listdir()
            # will only give us contents a single directory at a time, so we'll
            # get 'test'.  At the same time we don't want to load the entire
            # list of files into memory.  This is handled by first going
            # through the current directory contents and adding the directory
            # separator to any directories.  We can then sort the contents,
            # and ensure byte order.
            names = listdir(path)
            self._check_paths_decoded(path, names)
            for i, name in enumerate(names):
                file_path = join(path, name)
                if isdir(file_path):
                    names[i] = name + os.path.sep
            names.sort()
            for name in names:
                file_path = join(path, name)
                if isdir(file_path):
                    # Anything in a directory will have a prefix of this
                    # current directory and will come before the
                    # remaining contents in this directory.  This means we need
                    # to recurse into this sub directory before yielding the
                    # rest of this directory's contents.
                    for x in self.list_files(file_path, dir_op):
                        yield x
                else:
                    size, last_update = get_file_stat(file_path)
                    yield file_path, size, last_update

    def _check_paths_decoded(self, path, names):
        # We can get a UnicodeDecodeError if we try to listdir(<unicode>) and
        # can't decode the contents with sys.getfilesystemencoding().  In this
        # case listdir() returns the bytestring, which means that
        # join(<unicode>, <str>) could raise a UnicodeDecodeError.  When this
        # happens we raise a FileDecodingError that provides more information
        # into what's going on.
        for name in names:
            if not isinstance(name, six.text_type):
                raise FileDecodingError(path, name)

    def list_objects(self, s3_path, dir_op):
        """
        This function yields the appropriate object or objects under a
        common prefix depending if the operation is on objects under a
        common prefix.  It yields the file's source path, size, and last
        update.
        """
        # Short circuit path: if we are not recursing into the s3
        # bucket and a specific path was given, we can just yield
        # that path and not have to call any operation in s3.
        bucket, prefix = find_bucket_key(s3_path)
        if not dir_op and prefix:
            yield self._list_single_object(s3_path)
        else:
            operation = self._service.get_operation('ListObjects')
            lister = BucketLister(operation, self._endpoint)
            for key in lister.list_objects(bucket=bucket, prefix=prefix):
                source_path, size, last_update = key
                if size == 0 and source_path.endswith('/'):
                    if self.operation_name == 'delete':
                        # This is to filter out manually created folders
                        # in S3.  They have a size zero and would be
                        # undesirably downloaded.  Local directories
                        # are automatically created when they do not
                        # exist locally.  But user should be able to
                        # delete them.
                        yield source_path, size, last_update
                elif not dir_op and s3_path != source_path:
                    pass
                else:
                    yield source_path, size, last_update

    def _list_single_object(self, s3_path):
        # When we know we're dealing with a single object, we can avoid
        # a ListObjects operation (which causes concern for anyone setting
        # IAM policies with the smallest set of permissions needed) and
        # instead use a HeadObject request.
        bucket, key = find_bucket_key(s3_path)
        operation = self._service.get_operation('HeadObject')
        try:
            response = operation.call(
                self._endpoint, bucket=bucket, key=key)[1]
        except ClientError as e:
            # We want to try to give a more helpful error message.
            # This is what the customer is going to see so we want to
            # give as much detail as we have.
            copy_fields = e.__dict__.copy()
            if not e.error_message == 'Unknown':
                raise
            if e.http_status_code == 404:
                # The key does not exist so we'll raise a more specific
                # error message here.
                copy_fields['error_code'] = 'NoSuchKey'
                copy_fields['error_message'] = 'Key "%s" does not exist' % key
            else:
                reason = six.moves.http_client.responses[
                    e.http_status_code]
                copy_fields['error_code'] = reason
                copy_fields['error_message'] = reason
            raise ClientError(**copy_fields)
        file_size = int(response['ContentLength'])
        last_update = parse(response['LastModified'])
        last_update = last_update.astimezone(tzlocal())
        return s3_path, file_size, last_update

########NEW FILE########
__FILENAME__ = fileinfo
import os
import sys
import time
from functools import partial
import errno
import hashlib

from dateutil.parser import parse
from dateutil.tz import tzlocal

from botocore.compat import quote
from awscli.customizations.s3.utils import find_bucket_key, \
        check_etag, check_error, operate, uni_print, \
        guess_content_type, MD5Error


class CreateDirectoryError(Exception):
    pass


def read_file(filename):
    """
    This reads the file into a form that can be sent to S3
    """
    with open(filename, 'rb') as in_file:
        return in_file.read()


def save_file(filename, response_data, last_update):
    """
    This writes to the file upon downloading.  It reads the data in the
    response.  Makes a new directory if needed and then writes the
    data to the file.  It also modifies the last modified time to that
    of the S3 object.
    """
    body = response_data['Body']
    etag = response_data['ETag'][1:-1]
    d = os.path.dirname(filename)
    try:
        if not os.path.exists(d):
            os.makedirs(d)
    except OSError as e:
        if not e.errno == errno.EEXIST:
            raise CreateDirectoryError(
                "Could not create directory %s: %s" % (d, e))
    md5 = hashlib.md5()
    file_chunks = iter(partial(body.read, 1024 * 1024), b'')
    with open(filename, 'wb') as out_file:
        if not _is_multipart_etag(etag):
            for chunk in file_chunks:
                md5.update(chunk)
                out_file.write(chunk)
        else:
            for chunk in file_chunks:
                out_file.write(chunk)
    if not _is_multipart_etag(etag):
        if etag != md5.hexdigest():
            os.remove(filename)
            raise MD5Error(filename)
    last_update_tuple = last_update.timetuple()
    mod_timestamp = time.mktime(last_update_tuple)
    os.utime(filename, (int(mod_timestamp), int(mod_timestamp)))


def _is_multipart_etag(etag):
    return '-' in etag


class TaskInfo(object):
    """
    This class contains important details related to performing a task.  This
    object is usually only used for creating buckets, removing buckets, and
    listing objects/buckets.  This object contains the attributes and
    functions needed to perform the task.  Note that just instantiating one
    of these objects will not be enough to run a listing or bucket command.
    unless ``session`` and ``region`` are specified upon instantiation.

    :param src: the source path
    :type src: string
    :param src_type: if the source file is s3 or local.
    :type src_type: string
    :param operation: the operation being performed.
    :type operation: string
    :param session: ``botocore.session`` object
    :param region: The region for the endpoint

    Note that a local file will always have its absolute path, and a s3 file
    will have its path in the form of bucket/key
    """
    def __init__(self, src, src_type, operation_name, service, endpoint):
        self.src = src
        self.src_type = src_type
        self.operation_name = operation_name
        self.service = service
        self.endpoint = endpoint

    def make_bucket(self):
        """
        This opereation makes a bucket.
        """
        bucket, key = find_bucket_key(self.src)
        bucket_config = {'LocationConstraint': self.endpoint.region_name}
        params = {'endpoint': self.endpoint, 'bucket': bucket}
        if self.endpoint.region_name != 'us-east-1':
            params['create_bucket_configuration'] = bucket_config
        response_data, http = operate(self.service, 'CreateBucket', params)

    def remove_bucket(self):
        """
        This operation removes a bucket.
        """
        bucket, key = find_bucket_key(self.src)
        params = {'endpoint': self.endpoint, 'bucket': bucket}
        response_data, http = operate(self.service, 'DeleteBucket', params)


class FileInfo(TaskInfo):
    """
    This is a child object of the ``TaskInfo`` object.  It can perform more
    operations such as ``upload``, ``download``, ``copy``, ``delete``,
    ``move``.  Similiarly to
    ``TaskInfo`` objects attributes like ``session`` need to be set in order
    to perform operations.

    :param dest: the destination path
    :type dest: string
    :param compare_key: the name of the file relative to the specified
        directory/prefix.  This variable is used when performing synching
        or if the destination file is adopting the source file's name.
    :type compare_key: string
    :param size: The size of the file in bytes.
    :type size: integer
    :param last_update: the local time of last modification.
    :type last_update: datetime object
    :param dest_type: if the destination is s3 or local.
    :param dest_type: string
    :param parameters: a dictionary of important values this is assigned in
        the ``BasicTask`` object.
    """
    def __init__(self, src, dest=None, compare_key=None, size=None,
                 last_update=None, src_type=None, dest_type=None,
                 operation_name=None, service=None, endpoint=None,
                 parameters=None):
        super(FileInfo, self).__init__(src, src_type=src_type,
                                       operation_name=operation_name,
                                       service=service,
                                       endpoint=endpoint)
        self.dest = dest
        self.dest_type = dest_type
        self.compare_key = compare_key
        self.size = size
        self.last_update = last_update
        # Usually inject ``parameters`` from ``BasicTask`` class.
        if parameters is not None:
            self.parameters = parameters
        else:
            self.parameters = {'acl': None,
                               'sse': None}

    def _permission_to_param(self, permission):
        if permission == 'read':
            return 'grant_read'
        if permission == 'full':
            return 'grant_full_control'
        if permission == 'readacl':
            return 'grant_read_acp'
        if permission == 'writeacl':
            return 'grant_write_acp'
        raise ValueError('permission must be one of: '
                         'read|readacl|writeacl|full')

    def _handle_object_params(self, params):
        if self.parameters['acl']:
            params['acl'] = self.parameters['acl'][0]
        if self.parameters['grants']:
            for grant in self.parameters['grants']:
                try:
                    permission, grantee = grant.split('=', 1)
                except ValueError:
                    raise ValueError('grants should be of the form '
                                     'permission=principal')
                params[self._permission_to_param(permission)] = grantee
        if self.parameters['sse']:
            params['server_side_encryption'] = 'AES256'
        if self.parameters['storage_class']:
            params['storage_class'] = self.parameters['storage_class'][0]
        if self.parameters['website_redirect']:
            params['website_redirect_location'] = \
                    self.parameters['website_redirect'][0]
        if self.parameters['guess_mime_type']:
            self._inject_content_type(params, self.src)
        if self.parameters['content_type']:
            params['content_type'] = self.parameters['content_type'][0]
        if self.parameters['cache_control']:
            params['cache_control'] = self.parameters['cache_control'][0]
        if self.parameters['content_disposition']:
            params['content_disposition'] = \
                    self.parameters['content_disposition'][0]
        if self.parameters['content_encoding']:
            params['content_encoding'] = self.parameters['content_encoding'][0]
        if self.parameters['content_language']:
            params['content_language'] = self.parameters['content_language'][0]
        if self.parameters['expires']:
            params['expires'] = self.parameters['expires'][0]

    def upload(self):
        """
        Redirects the file to the multipart upload function if the file is
        large.  If it is small enough, it puts the file as an object in s3.
        """
        with open(self.src, 'rb') as body:
            bucket, key = find_bucket_key(self.dest)
            params = {
                'endpoint': self.endpoint,
                'bucket': bucket,
                'key': key,
                'body': body,
            }
            self._handle_object_params(params)
            response_data, http = operate(self.service, 'PutObject', params)
            etag = response_data['ETag'][1:-1]
            body.seek(0)
            check_etag(etag, body)

    def _inject_content_type(self, params, filename):
        # Add a content type param if we can guess the type.
        guessed_type = guess_content_type(filename)
        if guessed_type is not None:
            params['content_type'] = guessed_type

    def download(self):
        """
        Redirects the file to the multipart download function if the file is
        large.  If it is small enough, it gets the file as an object from s3.
        """
        bucket, key = find_bucket_key(self.src)
        params = {'endpoint': self.endpoint, 'bucket': bucket, 'key': key}
        response_data, http = operate(self.service, 'GetObject', params)
        save_file(self.dest, response_data, self.last_update)

    def copy(self):
        """
        Copies a object in s3 to another location in s3.
        """
        copy_source = self.src
        bucket, key = find_bucket_key(self.dest)
        params = {'endpoint': self.endpoint, 'bucket': bucket,
                  'copy_source': copy_source, 'key': key}
        self._handle_object_params(params)
        response_data, http = operate(self.service, 'CopyObject', params)

    def delete(self):
        """
        Deletes the file from s3 or local.  The src file and type is used
        from the file info object.
        """
        if (self.src_type == 's3'):
            bucket, key = find_bucket_key(self.src)
            params = {'endpoint': self.endpoint, 'bucket': bucket, 'key': key}
            response_data, http = operate(self.service, 'DeleteObject',
                                          params)
        else:
            os.remove(self.src)

    def move(self):
        """
        Implements a move command for s3.
        """
        src = self.src_type
        dest = self.dest_type
        if src == 'local' and dest == 's3':
            self.upload()
        elif src == 's3' and dest == 's3':
            self.copy()
        elif src == 's3' and dest == 'local':
            self.download()
        else:
            raise Exception("Invalid path arguments for mv")
        self.delete()

    def create_multipart_upload(self):
        bucket, key = find_bucket_key(self.dest)
        params = {'endpoint': self.endpoint, 'bucket': bucket, 'key': key}
        self._handle_object_params(params)
        response_data, http = operate(self.service, 'CreateMultipartUpload',
                                      params)
        upload_id = response_data['UploadId']
        return upload_id

########NEW FILE########
__FILENAME__ = filters
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import logging
import fnmatch
import os

from awscli.customizations.s3.utils import split_s3_bucket_key


LOG = logging.getLogger(__name__)


def create_filter(parameters):
    """Given the CLI parameters dict, create a Filter object."""
    # We need to evaluate all the filters based on the source
    # directory.
    if parameters['filters']:
        cli_filters = parameters['filters']
        real_filters = []
        for filter_type, filter_pattern in cli_filters:
            real_filters.append((filter_type.lstrip('-'),
                                 filter_pattern))
        source_location = parameters['src']
        if source_location.startswith('s3://'):
            # This gives us (bucket, keyname) and we want
            # the bucket to be the root dir.
            src_rootdir = _get_s3_root(source_location)
            dst_rootdir = _get_local_root(parameters['dest'],
                                          parameters['dir_op'])
        else:
            src_rootdir = _get_local_root(parameters['src'], parameters['dir_op'])
            dst_rootdir = _get_s3_root(parameters['dest'])

        return Filter(real_filters, src_rootdir, dst_rootdir)
    else:
        return Filter({}, None, None)


def _get_s3_root(source_location):
    return split_s3_bucket_key(source_location)[0]


def _get_local_root(source_location, dir_op):
    if dir_op:
        rootdir = os.path.abspath(source_location)
    else:
        rootdir = os.path.abspath(os.path.dirname(source_location))
    return rootdir


class Filter(object):
    """
    This is a universal exclude/include filter.
    """
    def __init__(self, patterns, rootdir, dst_rootdir):
        """
        :var patterns: A list of patterns. A pattern consits of a list
            whose first member is a string 'exclude' or 'include'.
            The second member is the actual rule.
        :var rootdir: The root directory where the patterns are evaluated.
            This will generally be the directory of the source location.
        :var dst_rootdir: The destination root directory where the patterns are
            evaluated.  This is only useful when the --delete option is
            also specified.

        """
        self._original_patterns = patterns
        self.patterns = self._full_path_patterns(patterns, rootdir)
        self.dst_patterns = self._full_path_patterns(patterns, dst_rootdir)

    def _full_path_patterns(self, original_patterns, rootdir):
        # We need to transform the patterns into patterns that have
        # the root dir prefixed, so things like ``--exclude "*"``
        # will actually be ['exclude', '/path/to/root/*']
        full_patterns = []
        for pattern in original_patterns:
            full_patterns.append(
                (pattern[0], os.path.join(rootdir, pattern[1])))
        return full_patterns

    def call(self, file_infos):
        """
        This function iterates over through the yielded file_info objects.  It
        determines the type of the file and applies pattern matching to
        determine if the rule applies.  While iterating though the patterns the
        file is assigned a boolean flag to determine if a file should be
        yielded on past the filer.  Anything identified by the exclude filter
        has its flag set to false.  Anything identified by the include filter
        has its flag set to True.  All files begin with the flag set to true.
        Rules listed at the end will overwrite flags thrown by rules listed
        before it.
        """
        for file_info in file_infos:
            file_path = file_info.src
            file_status = (file_info, True)
            for pattern, dst_pattern in zip(self.patterns, self.dst_patterns):
                current_file_status = self._match_pattern(pattern, file_info)
                if current_file_status is not None:
                    file_status = current_file_status
                dst_current_file_status = self._match_pattern(dst_pattern, file_info)
                if dst_current_file_status is not None:
                    file_status = dst_current_file_status
            LOG.debug("=%s final filtered status, should_include: %s",
                      file_path, file_status[1])
            if file_status[1]:
                yield file_info

    def _match_pattern(self, pattern, file_info):
        file_status = None
        file_path = file_info.src
        pattern_type = pattern[0]
        if file_info.src_type == 'local':
            path_pattern = pattern[1].replace('/', os.sep)
        else:
            path_pattern = pattern[1].replace(os.sep, '/')
        is_match = fnmatch.fnmatch(file_path, path_pattern)
        if is_match and pattern_type == 'include':
            file_status = (file_info, True)
            LOG.debug("%s matched include filter: %s",
                        file_path, path_pattern)
        elif is_match and pattern_type == 'exclude':
            file_status = (file_info, False)
            LOG.debug("%s matched exclude filter: %s",
                        file_path, path_pattern)
        else:
            LOG.debug("%s did not match %s filter: %s",
                        file_path, pattern_type[2:], path_pattern)
        return file_status

########NEW FILE########
__FILENAME__ = s3
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import argparse
import os
import six
import sys

from dateutil.parser import parse
from dateutil.tz import tzlocal

import awscli
from awscli.arguments import BaseCLIArgument
from awscli.argparser import ServiceArgParser, ArgTableArgParser
from awscli.help import HelpCommand
from awscli.customizations import utils
from awscli.customizations.s3.comparator import Comparator
from awscli.customizations.s3.fileformat import FileFormat
from awscli.customizations.s3.filegenerator import FileGenerator
from awscli.customizations.s3.fileinfo import TaskInfo
from awscli.customizations.s3.filters import create_filter
from awscli.customizations.s3.s3handler import S3Handler
from awscli.customizations.s3.description import add_command_descriptions, \
    add_param_descriptions
from awscli.customizations.s3.utils import find_bucket_key, uni_print
from awscli.customizations.s3.dochandler import S3DocumentEventHandler


class AppendFilter(argparse.Action):
    """
    This class is used as an action when parsing the parameters.
    Specifically it is used for actions corresponding to exclude
    and include filters.  What it does is that it appends a list
    consisting of the name of the parameter and its value onto
    a list containing these [parameter, value] lists.  In this
    case, the name of the parameter will either be --include or
    --exclude and the value will be the rule to apply.  This will
    format all of the rules inputted into the command line
    in a way compatible with the Filter class.  Note that rules that
    appear later in the command line take preferance over rulers that
    appear earlier.
    """
    def __call__(self, parser, namespace, values, option_string=None):
        filter_list = getattr(namespace, self.dest)
        if filter_list:
            filter_list.append([option_string, values[0]])
        else:
            filter_list = [[option_string, values[0]]]
        setattr(namespace, self.dest, filter_list)


def awscli_initialize(cli):
    """
    This function is require to use the plugin.  It calls the functions
    required to add all neccessary commands and parameters to the CLI.
    This function is necessary to install the plugin using a configuration
    file
    """
    cli.register("building-command-table.main", add_s3)
    cli.register("doc-examples.S3.*", add_s3_examples)


def s3_plugin_initialize(event_handlers):
    """
    This is a wrapper to make the plugin built-in to the cli as opposed
    to specifiying it in the configuration file.
    """
    awscli_initialize(event_handlers)


def add_s3(command_table, session, **kwargs):
    """
    This creates a new service object for the s3 plugin.  It sends the
    old s3 commands to the namespace ``s3api``.
    """
    utils.rename_command(command_table, 's3', 's3api')
    command_table['s3'] = S3('s3', session)


def add_s3_examples(help_command, **kwargs):
    """
    This function is used to add examples for each command.  It reads in
    reStructuredTexts in the ``doc/source/examples`` directory
    and injects them into the help docs for each command.  Each command
    should have one of these example docs.
    """
    doc_path = os.path.join(
        os.path.abspath(os.path.dirname(awscli.__file__)), 'examples', 's3')
    file_name = '%s.rst' % help_command.obj._name
    doc_path = os.path.join(doc_path, file_name)
    if os.path.isfile(doc_path):
        help_command.doc.style.h2('Examples')
        fp = open(doc_path)
        for line in fp.readlines():
            help_command.doc.write(line)


class S3HelpCommand(HelpCommand):
    """
    This is a wrapper to handle the interactions between the commmand and the
    documentation pipeline.
    """
    EventHandlerClass = S3DocumentEventHandler
    event_class = 'S3'
    name = 's3'


class S3Service(object):
    """
    This is a small class that represents the service object for s3.  Its
    only purpose is to give the s3 a service and a name.  This is
    currently required for doc generation
    """
    def __init__(self):
        self.service_full_name = 'Amazon Simple Storage Service'


class S3(object):
    """
    The service for the plugin.
    """

    def __init__(self, name, session):
        self._name = name
        self._service_object = S3Service()
        self._session = session
        self.documentation = "This provides higher level S3 commands for " \
                             "the AWS CLI."

    def __call__(self, args, parsed_globals):
        """
        This function instantiates the operations table to be filled with
        commands.  Creates a parser based off of the commands in the
        operations table.  Parses the valid arguments and passes the
        remaining off to a corresponding ``S3SubCommand`` object to be called
        on.
        """
        subcommand_table = self._create_subcommand_table()
        service_parser = self._create_service_parser(subcommand_table)
        parsed_args, remaining = service_parser.parse_known_args(args)
        return subcommand_table[parsed_args.operation](
            remaining, parsed_globals)

    def _create_service_parser(self, subcommand_table):
        """
        Creates the parser required to parse the commands on the
        command line
        """
        return ServiceArgParser(
            operations_table=subcommand_table, service_name=self._name)

    def _create_subcommand_table(self):
        """
        Creates an empty dictionary to be filled with ``S3SubCommand`` objects
        when the event is emmitted.
        """
        subcommand_table = {}
        for cmd in CMD_DICT.keys():
            cmd_specification = CMD_DICT[cmd]
            cmd_class = cmd_specification.get('command_class', S3SubCommand)
            # If a cmd_class is provided, the we'll try to grab the
            # description and usage off of that object, otherwise
            # we'll look in the command dict.
            description, usage = self._get_command_usage(cmd_class)
            subcommand_table[cmd] = cmd_class(
                cmd, self._session, cmd_specification['options'],
                cmd_specification.get('description', description),
                cmd_specification.get('usage', usage))

        self._session.emit('building-operation-table.%s' % self._name,
                           operation_table=subcommand_table,
                           session=self._session)
        subcommand_table['help'] = S3HelpCommand(self._session, self,
                                                command_table=subcommand_table,
                                                arg_table=None)
        return subcommand_table

    def _get_command_usage(self, cmd_class):
        return (getattr(cmd_class, 'DESCRIPTION', None),
                getattr(cmd_class, 'USAGE', None))

    def create_help_command(self):
        """
        This function returns a help command object with a filled command
        table.  This command is necessary for generating html docs.
        """
        subcommand_table = self._create_subcommand_table()
        del subcommand_table['help']
        return S3HelpCommand(self._session, self,
                             command_table=subcommand_table,
                             arg_table=None)


class S3SubCommand(object):
    """
    This is the object corresponding to a S3 subcommand.
    """
    DESCRIPTION = None
    USAGE = None

    def __init__(self, name, session, options, documentation="", usage=""):
        """

        :type name: str
        :param name: The name of the subcommand (``ls``, ``cp``, etc.)

        :type session: ``botocore.session.Session``
        :param session: Session object.

        :type options: dict
        :param options: The options for the ``paths`` argument.

        :type documentation: str
        :param documentation: Documentation for the subcommand.

        :type usage: str
        :param usage: The usage string of the subcommand

        """
        self._name = name
        self._session = session
        self.options = options
        self.documentation = documentation
        self.usage = usage

    def __call__(self, args, parsed_globals):
        """
        The call method first creates a parameter table to be filled with
        the possible parameters for a specified command.  Then a parser is
        created to determine the path(s) used for the command along with
        any extra parameters included in the command line.  The argument
        are parsed and put into a namespace.  All of the parameters in the
        namespace are then stored in a dictionary.  This newly created
        dictionary is passed to a ``CommandParameters`` object that stores
        all of the parameters and does much of the initial error checking for
        the plugin.  The formatted dictionary of parameters in the
        CommandParameters are passed to a CommandArchitecture object and
        sets up the components for the operation and runs the operation.
        """
        param_table = self._create_parameter_table()
        operation_parser = self._create_operation_parser(param_table)
        parsed_args, remaining = operation_parser.parse_known_args(args)
        if remaining:
            raise ValueError(
                "Unknown options: %s" % ', '.join(remaining))
        if 'help' in parsed_args and parsed_args.help == 'help':
            help_object = S3HelpCommand(self._session, self,
                                        command_table=None,
                                        arg_table=param_table)
            help_object(remaining, parsed_globals)
        else:
            self._convert_path_args(parsed_args)
            return self._do_command(parsed_args, parsed_globals)

    def _do_command(self, parsed_args, parsed_globals):
        params = self._build_call_parameters(parsed_args, {})
        cmd_params = CommandParameters(self._session, self._name, params)
        cmd_params.add_region(parsed_globals)
        cmd_params.add_endpoint_url(parsed_globals)
        cmd_params.add_verify_ssl(parsed_globals)
        cmd_params.add_paths(parsed_args.paths)
        cmd_params.check_force(parsed_globals)
        cmd = CommandArchitecture(self._session, self._name,
                                  cmd_params.parameters)
        cmd.create_instructions()
        return cmd.run()

    def _convert_path_args(self, parsed_args):
        if not isinstance(parsed_args.paths, list):
            parsed_args.paths = [parsed_args.paths]
        for i in range(len(parsed_args.paths)):
            path = parsed_args.paths[i]
            if isinstance(path, six.binary_type):
                dec_path = path.decode(sys.getfilesystemencoding())
                enc_path = dec_path.encode('utf-8')
                new_path = enc_path.decode('utf-8')
                parsed_args.paths[i] = new_path

    def create_help_command(self):
        """
        This function returns a help command object with a filled arg
        table.  This command is necessary for generating html docs for
        the specified command.
        """
        arg_table = self._populate_parameter_table()
        return S3HelpCommand(self._session, self,
                             command_table=None,
                             arg_table=arg_table)

    def _create_parameter_table(self):
        """
        This creates an empty parameter table that will be filled with
        S3Parameter objects corresponding to the specified command when
        the event is emitted.
        """
        parameter_table = self._populate_parameter_table()
        self._session.emit('building-parameter-table.s3.%s' % self._name,
                           parameter_table=parameter_table,
                           command=self._name)

        return parameter_table

    def _populate_parameter_table(self):
        parameter_table = {}
        for param in CMD_DICT[self._name]['params']:
            parameter_table[param] = S3Parameter(param,
                                                 PARAMS_DICT[param]['options'],
                                                 PARAMS_DICT[param]['documents'])
        return parameter_table

    def _build_call_parameters(self, args, service_params):
        """
        This takes all of the commands in the name space and puts them
        into a dictionary
        """
        for name, value in vars(args).items():
            service_params[name] = value
        return service_params

    def _create_operation_parser(self, parameter_table):
        """
        This creates the ArgTableArgParser for the command.  It adds
        an extra argument to the parser, paths, which represents a required
        the number of positional argument that must follow the command's name.
        """
        parser = ArgTableArgParser(parameter_table)
        parser.add_argument("paths", **self.options)
        return parser

    def _get_endpoint(self, service, parsed_globals):
        return service.get_endpoint(region_name=parsed_globals.region,
                                    endpoint_url=parsed_globals.endpoint_url,
                                    verify=parsed_globals.verify_ssl)


class ListCommand(S3SubCommand):
    def _do_command(self, parsed_args, parsed_globals):
        path = parsed_args.paths[0]
        if path.startswith('s3://'):
            path = path[5:]
        bucket, key = find_bucket_key(path)
        self.service = self._session.get_service('s3')
        self.endpoint = self._get_endpoint(self.service, parsed_globals)
        if not bucket:
            self._list_all_buckets()
        elif parsed_args.dir_op:
            # Then --recursive was specified.
            self._list_all_objects_recursive(bucket, key)
        else:
            self._list_all_objects(bucket, key)
        return 0

    def _list_all_objects(self, bucket, key):
        operation = self.service.get_operation('ListObjects')
        iterator = operation.paginate(self.endpoint, bucket=bucket,
                                      prefix=key, delimiter='/')
        for _, response_data in iterator:
            self._display_page(response_data)

    def _display_page(self, response_data, use_basename=True):
        common_prefixes = response_data['CommonPrefixes']
        contents = response_data['Contents']
        for common_prefix in common_prefixes:
            prefix_components = common_prefix['Prefix'].split('/')
            prefix = prefix_components[-2]
            pre_string = "PRE".rjust(30, " ")
            print_str = pre_string + ' ' + prefix + '/\n'
            uni_print(print_str)
            sys.stdout.flush()
        for content in contents:
            last_mod_str = self._make_last_mod_str(content['LastModified'])
            size_str = self._make_size_str(content['Size'])
            if use_basename:
                filename_components = content['Key'].split('/')
                filename = filename_components[-1]
            else:
                filename = content['Key']
            print_str = last_mod_str + ' ' + size_str + ' ' + \
                filename + '\n'
            uni_print(print_str)
            sys.stdout.flush()

    def _list_all_buckets(self):
        operation = self.service.get_operation('ListBuckets')
        response_data = operation.call(self.endpoint)[1]
        buckets = response_data['Buckets']
        for bucket in buckets:
            last_mod_str = self._make_last_mod_str(bucket['CreationDate'])
            print_str = last_mod_str + ' ' + bucket['Name'] + '\n'
            uni_print(print_str)
            sys.stdout.flush()

    def _list_all_objects_recursive(self, bucket, key):
        operation = self.service.get_operation('ListObjects')
        iterator = operation.paginate(self.endpoint, bucket=bucket,
                                      prefix=key)
        for _, response_data in iterator:
            self._display_page(response_data, use_basename=False)

    def _make_last_mod_str(self, last_mod):
        """
        This function creates the last modified time string whenever objects
        or buckets are being listed
        """
        last_mod = parse(last_mod)
        last_mod = last_mod.astimezone(tzlocal())
        last_mod_tup = (str(last_mod.year), str(last_mod.month).zfill(2),
                        str(last_mod.day).zfill(2), str(last_mod.hour).zfill(2),
                        str(last_mod.minute).zfill(2),
                        str(last_mod.second).zfill(2))
        last_mod_str = "%s-%s-%s %s:%s:%s" % last_mod_tup
        return last_mod_str.ljust(19, ' ')

    def _make_size_str(self, size):
        """
        This function creates the size string when objects are being listed.
        """
        size_str = str(size)
        return size_str.rjust(10, ' ')


class WebsiteCommand(S3SubCommand):
    DESCRIPTION = 'Set the website configuration for a bucket.'
    USAGE = 's3://bucket [--index-document|--error-document] value'

    def _do_command(self, parsed_args, parsed_globals):
        service = self._session.get_service('s3')
        endpoint = self._get_endpoint(service, parsed_globals)
        operation = service.get_operation('PutBucketWebsite')
        bucket = self._get_bucket_name(parsed_args.paths[0])
        website_configuration = self._build_website_configuration(parsed_args)
        operation.call(endpoint, bucket=bucket,
                       website_configuration=website_configuration)
        return 0

    def _build_website_configuration(self, parsed_args):
        website_config = {}
        if parsed_args.index_document is not None:
            website_config['IndexDocument'] = {'Suffix': parsed_args.index_document}
        if parsed_args.error_document is not None:
            website_config['ErrorDocument'] = {'Key': parsed_args.error_document}
        return website_config

    def _get_bucket_name(self, path):
        # We support either:
        # s3://bucketname
        # bucketname
        #
        # We also strip off the trailing slash if a user
        # accidently appends a slash.
        if path.startswith('s3://'):
            path = path[5:]
        if path.endswith('/'):
            path = path[:-1]
        return path


class S3Parameter(BaseCLIArgument):
    """
    This is a class that is used to add a parameter to the the parser along
    with its respective actions, dest, etc.
    """
    def __init__(self, name, options, documentation=''):
        self._name = name
        self.options = options
        self._documentation = documentation

    @property
    def documentation(self):
        return self._documentation

    def add_to_parser(self, parser):
        parser.add_argument('--' + self._name, **self.options)


class CommandArchitecture(object):
    """
    This class drives the actual command.  A command is performed in two
    steps.  First a list of instructions is generated.  This list of
    instructions identifies which type of components are required based on the
    name of the command and the parameters passed to the command line.  After
    the instructions are generated the second step involves using the
    lsit of instructions to wire together an assortment of generators to
    perform the command.
    """
    def __init__(self, session, cmd, parameters):
        self.session = session
        self.cmd = cmd
        self.parameters = parameters
        self.instructions = []
        self._service = self.session.get_service('s3')
        self._endpoint = self._service.get_endpoint(
            region_name=self.parameters['region'],
            endpoint_url=self.parameters['endpoint_url'],
            verify=self.parameters['verify_ssl'])

    def create_instructions(self):
        """
        This function creates the instructions based on the command name and
        extra parameters.  Note that all commands must have an s3_handler
        instruction in the instructions and must be at the end of the
        instruction list because it sends the request to S3 and does not
        yield anything.
        """
        if self.cmd not in ['mb', 'rb']:
            self.instructions.append('file_generator')
        if self.parameters.get('filters'):
            self.instructions.append('filters')
        if self.cmd == 'sync':
            self.instructions.append('comparator')
        self.instructions.append('s3_handler')

    def run(self):
        """
        This function wires together all of the generators and completes
        the command.  First a dictionary is created that is indexed first by
        the command name.  Then using the instruction, another dictionary
        can be indexed to obtain the objects corresponding to the
        particular instruction for that command.  To begin the wiring,
        either a ``FileFormat`` or ``TaskInfo`` object, depending on the
        command, is put into a list.  Then the function enters a while loop
        that pops off an instruction.  It then determines the object needed
        and calls the call function of the object using the list as the input.
        Depending on the number of objects in the input list and the number
        of components in the list corresponding to the instruction, the call
        method of the component can be called two different ways.  If the
        number of inputs is equal to the number of components a 1:1 mapping of
        inputs to components is used when calling the call function.  If the
        there are more inputs than components, then a 2:1 mapping of inputs to
        components is used where the component call method takes two inputs
        instead of one.  Whatever files are yielded from the call function
        is appended to a list and used as the input for the next repetition
        of the while loop until there are no more instructions.
        """
        src = self.parameters['src']
        dest = self.parameters['dest']
        paths_type = self.parameters['paths_type']
        files = FileFormat().format(src, dest, self.parameters)
        rev_files = FileFormat().format(dest, src, self.parameters)

        cmd_translation = {}
        cmd_translation['locals3'] = {'cp': 'upload', 'sync': 'upload',
                                      'mv': 'move'}
        cmd_translation['s3s3'] = {'cp': 'copy', 'sync': 'copy', 'mv': 'move'}
        cmd_translation['s3local'] = {'cp': 'download', 'sync': 'download',
                                      'mv': 'move'}
        cmd_translation['s3'] = {
            'rm': 'delete',
            'mb': 'make_bucket',
            'rb': 'remove_bucket'
        }
        operation_name = cmd_translation[paths_type][self.cmd]

        file_generator = FileGenerator(self._service, self._endpoint,
                                       operation_name,
                                       self.parameters)
        rev_generator = FileGenerator(self._service, self._endpoint, '',
                                      self.parameters)
        taskinfo = [TaskInfo(src=files['src']['path'],
                             src_type='s3',
                             operation_name=operation_name,
                             service=self._service,
                             endpoint=self._endpoint)]
        s3handler = S3Handler(self.session, self.parameters)

        command_dict = {}
        if self.cmd == 'sync':
            command_dict = {'setup': [files, rev_files],
                                    'file_generator': [file_generator,
                                                    rev_generator],
                                    'filters': [create_filter(self.parameters),
                                                create_filter(self.parameters)],
                                    'comparator': [Comparator(self.parameters)],
                                    's3_handler': [s3handler]}
        elif self.cmd == 'cp':
            command_dict = {'setup': [files],
                                'file_generator': [file_generator],
                                'filters': [create_filter(self.parameters)],
                                's3_handler': [s3handler]}
        elif self.cmd == 'rm':
            command_dict = {'setup': [files],
                                'file_generator': [file_generator],
                                'filters': [create_filter(self.parameters)],
                                's3_handler': [s3handler]}
        elif self.cmd == 'mv':
            command_dict = {'setup': [files],
                                'file_generator': [file_generator],
                                'filters': [create_filter(self.parameters)],
                                's3_handler': [s3handler]}
        elif self.cmd == 'mb':
            command_dict = {'setup': [taskinfo],
                                's3_handler': [s3handler]}
        elif self.cmd == 'rb':
            command_dict = {'setup': [taskinfo],
                                's3_handler': [s3handler]}

        files = command_dict['setup']

        while self.instructions:
            instruction = self.instructions.pop(0)
            file_list = []
            components = command_dict[instruction]
            for i in range(len(components)):
                if len(files) > len(components):
                    file_list.append(components[i].call(*files))
                else:
                    file_list.append(components[i].call(files[i]))
            files = file_list
        # This is kinda quirky, but each call through the instructions
        # will replaces the files attr with the return value of the
        # file_list.  The very last call is a single list of
        # [s3_handler], and the s3_handler returns the number of
        # tasks failed.  This means that files[0] now contains
        # the number of failed tasks.  In terms of the RC, we're
        # keeping it simple and saying that > 0 failed tasks
        # will give a 1 RC.
        rc = 0
        if files[0] > 0:
            rc = 1
        return rc


class CommandParameters(object):
    """
    This class is used to do some initial error based on the
    parameters and arguments passed to the command line.
    """
    def __init__(self, session, cmd, parameters):
        """
        Stores command name and parameters.  Ensures that the ``dir_op`` flag
        is true if a certain command is being used.
        """
        self.session = session
        self.cmd = cmd
        self.parameters = parameters
        if 'dir_op' not in parameters:
            self.parameters['dir_op'] = False
        if self.cmd in ['sync', 'mb', 'rb']:
            self.parameters['dir_op'] = True

    def add_paths(self, paths):
        """
        Reformats the parameters dictionary by including a key and
        value for the source and the destination.  If a destination is
        not used the destination is the same as the source to ensure
        the destination always have some value.
        """
        self.check_path_type(paths)
        self._normalize_s3_trailing_slash(paths)
        src_path = paths[0]
        self.parameters['src'] = src_path
        if len(paths) == 2:
            self.parameters['dest'] = paths[1]
        elif len(paths) == 1:
            self.parameters['dest'] = paths[0]

    def _normalize_s3_trailing_slash(self, paths):
        for i, path in enumerate(paths):
            if path.startswith('s3://'):
                bucket, key = find_bucket_key(path[5:])
                if not key and not path.endswith('/'):
                    # If only a bucket was specified, we need
                    # to normalize the path and ensure it ends
                    # with a '/', s3://bucket -> s3://bucket/
                    path += '/'
                    paths[i] = path

    def _verify_bucket_exists(self, bucket_name):
        session = self.session
        service = session.get_service('s3')
        endpoint = service.get_endpoint(self.parameters['region'])
        operation = service.get_operation('ListObjects')
        # This will raise an exception if the bucket does not exist.
        operation.call(endpoint, bucket=bucket_name, max_keys=0)

    def check_path_type(self, paths):
        """
        This initial check ensures that the path types for the specified
        command is correct.
        """
        template_type = {'s3s3': ['cp', 'sync', 'mv'],
                         's3local': ['cp', 'sync', 'mv'],
                         'locals3': ['cp', 'sync', 'mv'],
                         's3': ['mb', 'rb', 'rm'],
                         'local': [], 'locallocal': []}
        paths_type = ''
        usage = "usage: aws s3 %s %s" % (self.cmd,
                                         CMD_DICT[self.cmd]['usage'])
        for i in range(len(paths)):
            if paths[i].startswith('s3://'):
                paths_type = paths_type + 's3'
            else:
                paths_type = paths_type + 'local'
        if self.cmd in template_type[paths_type]:
            self.parameters['paths_type'] = paths_type
        else:
            raise TypeError("%s\nError: Invalid argument type" % usage)

    def check_src_path(self, paths):
        """
        This checks the source paths to deem if they are valid.  The check
        performed in S3 is first it lists the objects using the source path.
        If there is an error like the bucket does not exist, the error will be
        caught with ``check_error()`` function.  If the operation is on a
        single object in s3, it checks that a list of object was returned and
        that the first object listed is the name of the specified in the
        command line.  If the operation is on objects under a common prefix,
        it will check that there are common prefixes and objects under
        the specified prefix.
        For local files, it first checks that the path exists.  Then it checks
        that the path is a directory if it is a directory operation or that
        the path is a file if the operation is on a single file.
        """
        src_path = paths[0]
        dir_op = self.parameters['dir_op']
        if not src_path.startswith('s3://'):
            src_path = os.path.abspath(src_path)
            if os.path.exists(src_path):
                if os.path.isdir(src_path) and not dir_op:
                    raise Exception("Error: Requires a local file")
                elif os.path.isfile(src_path) and dir_op:
                    raise Exception("Error: Requires a local directory")
                else:
                    pass
            else:
                raise Exception("Error: Local path does not exist")

    def check_force(self, parsed_globals):
        """
        This function recursive deletes objects in a bucket if the force
        parameters was thrown when using the remove bucket command.
        """
        if 'force' in self.parameters:
            if self.parameters['force']:
                bucket = find_bucket_key(self.parameters['src'][5:])[0]
                path = 's3://' + bucket
                try:
                    del_objects = S3SubCommand('rm', self.session, {'nargs': 1})
                    del_objects([path, '--recursive'], parsed_globals)
                except:
                    pass

    def add_region(self, parsed_globals):
        self.parameters['region'] = parsed_globals.region

    def add_endpoint_url(self, parsed_globals):
        """
        Adds endpoint_url to the parameters.
        """
        if 'endpoint_url' in parsed_globals:
            self.parameters['endpoint_url'] = getattr(parsed_globals, 'endpoint_url')
        else:
            self.parameters['endpoint_url'] = None

    def add_verify_ssl(self, parsed_globals):
        self.parameters['verify_ssl'] = parsed_globals.verify_ssl


# This is a dictionary useful for automatically adding the different commands,
# the amount of arguments it takes, and the optional parameters that can appear
# on the same line as the command.  It also contains descriptions and usage
# keys for help command and doc generation.
CMD_DICT = {'cp': {'options': {'nargs': 2},
                   'params': ['dryrun', 'quiet', 'recursive',
                              'include', 'exclude', 'acl',
                              'no-guess-mime-type',
                              'sse', 'storage-class', 'grants',
                              'website-redirect', 'content-type',
                              'cache-control', 'content-disposition',
                              'content-encoding', 'content-language',
                              'expires']},
            'mv': {'options': {'nargs': 2},
                   'params': ['dryrun', 'quiet', 'recursive',
                              'include', 'exclude', 'acl',
                              'sse', 'storage-class', 'grants',
                              'website-redirect', 'content-type',
                              'cache-control', 'content-disposition',
                              'content-encoding', 'content-language',
                              'expires']},
            'rm': {'options': {'nargs': 1},
                   'params': ['dryrun', 'quiet', 'recursive',
                              'include', 'exclude']},
            'sync': {'options': {'nargs': 2},
                     'params': ['dryrun', 'delete', 'exclude',
                                'include', 'quiet', 'acl', 'grants',
                                'no-guess-mime-type',
                                'sse', 'storage-class', 'content-type',
                                'cache-control', 'content-disposition',
                                'content-encoding', 'content-language',
                                'expires', 'size-only']},
            'ls': {'options': {'nargs': '?', 'default': 's3://'},
                   'params': ['recursive'], 'default': 's3://',
                   'command_class': ListCommand},
            'mb': {'options': {'nargs': 1}, 'params': []},
            'rb': {'options': {'nargs': 1}, 'params': ['force']},
            'website': {'options': {'nargs': 1},
                        'params': ['index-document', 'error-document'],
                        'command_class': WebsiteCommand},
            }

add_command_descriptions(CMD_DICT)


# This is a dictionary useful for keeping track of the parameters passed to
# add_argument when the parameter is added to the parser.  The documents
# key is a description of what the parameter does and is used for the help
# command and doc generation.
PARAMS_DICT = {'dryrun': {'options': {'action': 'store_true'}},
               'delete': {'options': {'action': 'store_true'}},
               'quiet': {'options': {'action': 'store_true'}},
               'force': {'options': {'action': 'store_true'}},
               'no-guess-mime-type': {'options': {'action': 'store_false',
                                                  'dest': 'guess_mime_type',
                                                  'default': True}},
               'content-type': {'options': {'nargs': 1}},
               'recursive': {'options': {'action': 'store_true',
                                         'dest': 'dir_op'}},
               'exclude': {'options': {'action': AppendFilter, 'nargs': 1,
                           'dest': 'filters'}},
               'include': {'options': {'action': AppendFilter, 'nargs': 1,
                           'dest': 'filters'}},
               'acl': {'options': {'nargs': 1,
                                   'choices': ['private', 'public-read',
                                               'public-read-write',
                                               'authenticated-read',
                                               'bucket-owner-read',
                                               'bucket-owner-full-control',
                                               'log-delivery-write']}},
               'grants': {'options': {'nargs': '+'}},
               'sse': {'options': {'action': 'store_true'}},
               'storage-class': {'options': {'nargs': 1,
                                             'choices': [
                                                 'STANDARD',
                                                 'REDUCED_REDUNDANCY']}},
               'website-redirect': {'options': {'nargs': 1}},
               'cache-control': {'options': {'nargs': 1}},
               'content-disposition': {'options': {'nargs': 1}},
               'content-encoding': {'options': {'nargs': 1}},
               'content-language': {'options': {'nargs': 1}},
               'expires': {'options': {'nargs': 1}},
               'size-only': {'options': {'action': 'store_true'}, 'documents':
                   ('Makes the size of each key the only criteria used to '
                    'decide whether to sync from source to destination.')},
               'index-document': {'options': {}, 'documents':
                   ('A suffix that is appended to a request that is for a '
                    'directory on the website endpoint (e.g. if the suffix '
                    'is index.html and you make a request to '
                    'samplebucket/images/ the data that is returned will '
                    'be for the object with the key name images/index.html) '
                    'The suffix must not be empty and must not include a '
                    'slash character.')},
               'error-document': {'options': {}, 'documents':
                   'The object key name to use when a 4XX class error occurs.'}

               }
add_param_descriptions(PARAMS_DICT)

########NEW FILE########
__FILENAME__ = s3handler
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import logging
import math
import os
from six.moves import queue

from awscli.customizations.s3.constants import MULTI_THRESHOLD, CHUNKSIZE, \
    NUM_THREADS, MAX_UPLOAD_SIZE, MAX_QUEUE_SIZE
from awscli.customizations.s3.utils import find_chunksize, \
    operate, find_bucket_key, relative_path
from awscli.customizations.s3.executor import Executor
from awscli.customizations.s3 import tasks

LOGGER = logging.getLogger(__name__)


class S3Handler(object):
    """
    This class sets up the process to perform the tasks sent to it.  It
    sources the ``self.executor`` from which threads inside the
    class pull tasks from to complete.
    """
    MAX_IO_QUEUE_SIZE = 20

    def __init__(self, session, params, multi_threshold=MULTI_THRESHOLD,
                 chunksize=CHUNKSIZE):
        self.session = session
        self.result_queue = queue.Queue()
        # The write_queue has potential for optimizations, so the constant
        # for maxsize is scoped to this class (as opposed to constants.py)
        # so we have the ability to change this value later.
        self.write_queue = queue.Queue(maxsize=self.MAX_IO_QUEUE_SIZE)
        self.params = {'dryrun': False, 'quiet': False, 'acl': None,
                       'guess_mime_type': True, 'sse': False,
                       'storage_class': None, 'website_redirect': None,
                       'content_type': None, 'cache_control': None,
                       'content_disposition': None, 'content_encoding': None,
                       'content_language': None, 'expires': None,
                       'grants': None}
        self.params['region'] = params['region']
        for key in self.params.keys():
            if key in params:
                self.params[key] = params[key]
        self.multi_threshold = multi_threshold
        self.chunksize = chunksize
        self.executor = Executor(
            num_threads=NUM_THREADS, result_queue=self.result_queue,
            quiet=self.params['quiet'], max_queue_size=MAX_QUEUE_SIZE,
            write_queue=self.write_queue
        )
        self._multipart_uploads = []
        self._multipart_downloads = []

    def call(self, files):
        """
        This function pulls a ``FileInfo`` or ``TaskInfo`` object from
        a list ``files``.  Each object is then deemed if it will be a
        multipart operation and add the necessary attributes if so.  Each
        object is then wrapped with a ``BasicTask`` object which is
        essentially a thread of execution for a thread to follow.  These
        tasks are then submitted to the main executor.
        """
        try:
            self.executor.start()
            total_files, total_parts = self._enqueue_tasks(files)
            self.executor.print_thread.set_total_files(total_files)
            self.executor.print_thread.set_total_parts(total_parts)
            self.executor.initiate_shutdown()
            self.executor.wait_until_shutdown()
            self._shutdown()
        except Exception as e:
            LOGGER.debug('Exception caught during task execution: %s',
                         str(e), exc_info=True)
            self.result_queue.put({'message': str(e), 'error': True})
            self.executor.initiate_shutdown(
                priority=self.executor.IMMEDIATE_PRIORITY)
            self._shutdown()
            self.executor.wait_until_shutdown()
        except KeyboardInterrupt:
            self.result_queue.put({'message': "Cleaning up. Please wait...",
                                   'error': True})
            self.executor.initiate_shutdown(
                priority=self.executor.IMMEDIATE_PRIORITY)
            self._shutdown()
            self.executor.wait_until_shutdown()
        return self.executor.num_tasks_failed

    def _shutdown(self):
        # And finally we need to make a pass through all the existing
        # multipart uploads and abort any pending multipart uploads.
        self._abort_pending_multipart_uploads()
        self._remove_pending_downloads()

    def _abort_pending_multipart_uploads(self):
        # For the purpose of aborting uploads, we consider any
        # upload context with an upload id.
        for upload, filename in self._multipart_uploads:
            if upload.is_cancelled():
                try:
                    upload.wait_for_upload_id()
                except tasks.UploadCancelledError:
                    pass
                else:
                    # This means that the upload went from STARTED -> CANCELLED.
                    # This could happen if a part thread decided to cancel the
                    # upload.  We need to explicitly abort the upload here.
                    self._cancel_upload(upload.wait_for_upload_id(), filename)
            upload.cancel_upload(self._cancel_upload, args=(filename,))

    def _remove_pending_downloads(self):
        # The downloads case is easier than the uploads case because we don't
        # need to make any service calls.  To properly cleanup we just need
        # to go through the multipart downloads that were in progress but
        # cancelled and remove the local file.
        for context, local_filename in self._multipart_downloads:
            if (context.is_cancelled() or context.is_started()) and \
                    os.path.exists(local_filename):
                # The file is in an inconsistent state (not all the parts
                # were written to the file) so we should remove the
                # local file rather than leave it in a bad state.  We don't
                # want to remove the files if the download has *not* been
                # started because we haven't touched the file yet, so it's
                # better to leave the old version of the file rather than
                # deleting the file entirely.
                os.remove(local_filename)
            context.cancel()

    def _cancel_upload(self, upload_id, filename):
        bucket, key = find_bucket_key(filename.dest)
        params = {
            'bucket': bucket,
            'key': key,
            'upload_id': upload_id,
            'endpoint': filename.endpoint,
        }
        LOGGER.debug("Aborting multipart upload for: %s", key)
        response_data, http = operate(
            filename.service, 'AbortMultipartUpload', params)

    def _enqueue_tasks(self, files):
        total_files = 0
        total_parts = 0
        for filename in files:
            num_uploads = 1
            is_multipart_task = self._is_multipart_task(filename)
            too_large = False
            if hasattr(filename, 'size'):
                too_large = filename.size > MAX_UPLOAD_SIZE
            if too_large and filename.operation_name == 'upload':
                warning = "Warning %s exceeds 5 TB and upload is " \
                            "being skipped" % relative_path(filename.src)
                self.result_queue.put({'message': warning, 'error': True})
            elif is_multipart_task and not self.params['dryrun']:
                # If we're in dryrun mode, then we don't need the
                # real multipart tasks.  We can just use a BasicTask
                # in the else clause below, which will print out the
                # fact that it's transferring a file rather than
                # the specific part tasks required to perform the
                # transfer.
                num_uploads = self._enqueue_multipart_tasks(filename)
            else:
                task = tasks.BasicTask(
                    session=self.session, filename=filename,
                    parameters=self.params,
                    result_queue=self.result_queue)
                self.executor.submit(task)
            total_files += 1
            total_parts += num_uploads
        return total_files, total_parts

    def _is_multipart_task(self, filename):
        # First we need to determine if it's an operation that even
        # qualifies for multipart upload.
        if hasattr(filename, 'size'):
            above_multipart_threshold = filename.size > self.multi_threshold
            if above_multipart_threshold:
                if filename.operation_name in ('upload', 'download',
                                               'move', 'copy'):
                    return True
                else:
                    return False
        else:
            return False

    def _enqueue_multipart_tasks(self, filename):
        num_uploads = 1
        if filename.operation_name == 'upload':
            num_uploads = self._enqueue_multipart_upload_tasks(filename)
        elif filename.operation_name == 'move':
            if filename.src_type == 'local' and filename.dest_type == 's3':
                num_uploads = self._enqueue_multipart_upload_tasks(
                    filename, remove_local_file=True)
            elif filename.src_type == 's3' and filename.dest_type == 'local':
                num_uploads = self._enqueue_range_download_tasks(
                    filename, remove_remote_file=True)
            elif filename.src_type == 's3' and filename.dest_type == 's3':
                num_uploads = self._enqueue_multipart_copy_tasks(
                    filename, remove_remote_file=True)
            else:
                raise ValueError("Unknown transfer type of %s -> %s" %
                                 (filename.src_type, filename.dest_type))
        elif filename.operation_name == 'copy':
            num_uploads = self._enqueue_multipart_copy_tasks(
                filename, remove_remote_file=False)
        elif filename.operation_name == 'download':
            num_uploads = self._enqueue_range_download_tasks(filename)
        return num_uploads

    def _enqueue_range_download_tasks(self, filename, remove_remote_file=False):
        chunksize = find_chunksize(filename.size, self.chunksize)
        num_downloads = int(filename.size / chunksize)
        context = tasks.MultipartDownloadContext(num_downloads)
        create_file_task = tasks.CreateLocalFileTask(context=context,
                                                     filename=filename)
        self.executor.submit(create_file_task)
        for i in range(num_downloads):
            task = tasks.DownloadPartTask(
                part_number=i, chunk_size=chunksize,
                result_queue=self.result_queue, service=filename.service,
                filename=filename, context=context, io_queue=self.write_queue)
            self.executor.submit(task)
        complete_file_task = tasks.CompleteDownloadTask(
            context=context, filename=filename, result_queue=self.result_queue,
            params=self.params, io_queue=self.write_queue)
        self.executor.submit(complete_file_task)
        self._multipart_downloads.append((context, filename.dest))
        if remove_remote_file:
            remove_task = tasks.RemoveRemoteObjectTask(
                filename=filename, context=context)
            self.executor.submit(remove_task)
        return num_downloads

    def _enqueue_multipart_upload_tasks(self, filename,
                                        remove_local_file=False):
        # First we need to create a CreateMultipartUpload task,
        # then create UploadTask objects for each of the parts.
        # And finally enqueue a CompleteMultipartUploadTask.
        chunksize = find_chunksize(filename.size, self.chunksize)
        num_uploads = int(math.ceil(filename.size /
                                    float(chunksize)))
        upload_context = self._enqueue_upload_start_task(
            chunksize, num_uploads, filename)
        self._enqueue_upload_tasks(
            num_uploads, chunksize, upload_context, filename, tasks.UploadPartTask)
        self._enqueue_upload_end_task(filename, upload_context)
        if remove_local_file:
            remove_task = tasks.RemoveFileTask(local_filename=filename.src,
                                               upload_context=upload_context)
            self.executor.submit(remove_task)
        return num_uploads

    def _enqueue_multipart_copy_tasks(self, filename,
                                      remove_remote_file=False):
        chunksize = find_chunksize(filename.size, self.chunksize)
        num_uploads = int(math.ceil(filename.size / float(chunksize)))
        upload_context = self._enqueue_upload_start_task(
            chunksize, num_uploads, filename)
        self._enqueue_upload_tasks(
            num_uploads, chunksize, upload_context, filename, tasks.CopyPartTask)
        self._enqueue_upload_end_task(filename, upload_context)
        if remove_remote_file:
            remove_task = tasks.RemoveRemoteObjectTask(
                filename=filename, context=upload_context)
            self.executor.submit(remove_task)
        return num_uploads

    def _enqueue_upload_start_task(self, chunksize, num_uploads, filename):
        upload_context = tasks.MultipartUploadContext(
            expected_parts=num_uploads)
        create_multipart_upload_task = tasks.CreateMultipartUploadTask(
            session=self.session, filename=filename,
            parameters=self.params,
            result_queue=self.result_queue, upload_context=upload_context)
        self.executor.submit(create_multipart_upload_task)
        return upload_context

    def _enqueue_upload_tasks(self, num_uploads, chunksize, upload_context, filename,
                              task_class):
        for i in range(1, (num_uploads + 1)):
            task = task_class(
                part_number=i, chunk_size=chunksize,
                result_queue=self.result_queue, upload_context=upload_context,
                filename=filename)
            self.executor.submit(task)

    def _enqueue_upload_end_task(self, filename, upload_context):
        complete_multipart_upload_task = tasks.CompleteMultipartUploadTask(
            session=self.session, filename=filename, parameters=self.params,
            result_queue=self.result_queue, upload_context=upload_context)
        self.executor.submit(complete_multipart_upload_task)
        self._multipart_uploads.append((upload_context, filename))

########NEW FILE########
__FILENAME__ = tasks
import logging
import math
import os
import time
import socket
import threading

from botocore.vendored import requests
from botocore.exceptions import IncompleteReadError

from awscli.customizations.s3.utils import find_bucket_key, MD5Error, \
    operate, ReadFileChunk, relative_path, IORequest, IOCloseRequest


LOGGER = logging.getLogger(__name__)


class UploadCancelledError(Exception):
    pass


class DownloadCancelledError(Exception):
    pass


class RetriesExeededError(Exception):
    pass


def print_operation(filename, failed, dryrun=False):
    """
    Helper function used to print out what an operation did and whether
    it failed.
    """
    print_str = filename.operation_name
    if dryrun:
        print_str = '(dryrun) ' + print_str
    if failed:
        print_str += " failed"
    print_str += ": "
    if filename.src_type == "s3":
        print_str = print_str + "s3://" + filename.src
    else:
        print_str += relative_path(filename.src)
    if filename.operation_name not in ["delete", "make_bucket",
                                       "remove_bucket"]:
        if filename.dest_type == "s3":
            print_str += " to s3://" + filename.dest
        else:
            print_str += " to " + relative_path(filename.dest)
    return print_str


class OrderableTask(object):
    PRIORITY = 10


class BasicTask(OrderableTask):
    """
    This class is a wrapper for all ``TaskInfo`` and ``TaskInfo`` objects
    It is practically a thread of execution.  It also injects the necessary
    attributes like ``session`` object in order for the filename to
    perform its designated operation.
    """
    def __init__(self, session, filename, parameters, result_queue):
        self.session = session
        self.service = self.session.get_service('s3')

        self.filename = filename
        self.filename.parameters = parameters

        self.parameters = parameters
        self.result_queue = result_queue

    def __call__(self):
        self._execute_task(attempts=3)

    def _execute_task(self, attempts, last_error=''):
        if attempts == 0:
            # We've run out of retries.
            self._queue_print_message(self.filename, failed=True,
                                      dryrun=self.parameters['dryrun'],
                                      error_message=last_error)
            return
        filename = self.filename
        try:
            if not self.parameters['dryrun']:
                getattr(filename, filename.operation_name)()
        except requests.ConnectionError as e:
            connect_error = str(e)
            LOGGER.debug("%s %s failure: %s",
                         filename.src, filename.operation_name, connect_error)
            self._execute_task(attempts - 1, last_error=str(e))
        except MD5Error as e:
            LOGGER.debug("%s %s failure: Data was corrupted: %s",
                         filename.src, filename.operation_name, e)
            self._execute_task(attempts - 1, last_error=str(e))
        except Exception as e:
            LOGGER.debug(str(e), exc_info=True)
            self._queue_print_message(filename, failed=True,
                                      dryrun=self.parameters['dryrun'],
                                      error_message=str(e))
        else:
            self._queue_print_message(filename, failed=False,
                                      dryrun=self.parameters['dryrun'])

    def _queue_print_message(self, filename, failed, dryrun,
                             error_message=None):
        try:
            if filename.operation_name != 'list_objects':
                message = print_operation(filename, failed,
                                          self.parameters['dryrun'])
                if error_message is not None:
                    message += ' ' + error_message
                result = {'message': message, 'error': failed}
                self.result_queue.put(result)
        except Exception as e:
            LOGGER.debug('%s' % str(e))


class CopyPartTask(OrderableTask):
    def __init__(self, part_number, chunk_size,
                 result_queue, upload_context, filename):
        self._result_queue = result_queue
        self._upload_context = upload_context
        self._part_number = part_number
        self._chunk_size = chunk_size
        self._filename = filename

    def _is_last_part(self, part_number):
        return self._part_number == int(
            math.ceil(self._filename.size / float(self._chunk_size)))

    def _total_parts(self):
        return int(math.ceil(
            self._filename.size / float(self._chunk_size)))

    def __call__(self):
        LOGGER.debug("Uploading part copy %s for filename: %s",
                     self._part_number, self._filename.src)
        total_file_size = self._filename.size
        start_range = (self._part_number - 1) * self._chunk_size
        if self._is_last_part(self._part_number):
            end_range = total_file_size - 1
        else:
            end_range = start_range + self._chunk_size - 1
        range_param = 'bytes=%s-%s' % (start_range, end_range)
        try:
            LOGGER.debug("Waiting for upload id.")
            upload_id = self._upload_context.wait_for_upload_id()
            bucket, key = find_bucket_key(self._filename.dest)
            src_bucket, src_key = find_bucket_key(self._filename.src)
            params = {'endpoint': self._filename.endpoint,
                      'bucket': bucket, 'key': key,
                      'part_number': self._part_number,
                      'upload_id': upload_id,
                      'copy_source': '%s/%s' % (src_bucket, src_key),
                      'copy_source_range': range_param}
            response_data, http = operate(
                self._filename.service, 'UploadPartCopy', params)
            etag = response_data['CopyPartResult']['ETag'][1:-1]
            self._upload_context.announce_finished_part(
                etag=etag, part_number=self._part_number)

            message = print_operation(self._filename, 0)
            result = {'message': message, 'total_parts': self._total_parts(),
                      'error': False}
            self._result_queue.put(result)
        except UploadCancelledError as e:
            # We don't need to do anything in this case.  The task
            # has been cancelled, and the task that cancelled the
            # task has already queued a message.
            LOGGER.debug("Not uploading part copy, task has been cancelled.")
        except Exception as e:
            LOGGER.debug('Error during upload part copy: %s', e,
                         exc_info=True)
            message = print_operation(self._filename, failed=True,
                                      dryrun=False)
            message += '\n' + str(e)
            result = {'message': message, 'error': True}
            self._result_queue.put(result)
            self._upload_context.cancel_upload()
        else:
            LOGGER.debug("Copy part number %s completed for filename: %s",
                         self._part_number, self._filename.src)


class UploadPartTask(OrderableTask):
    """
    This is a task used to upload a part of a multipart upload.
    This task pulls from a ``part_queue`` which represents the
    queue for a specific multipart upload.  This pulling from a
    ``part_queue`` is necessary in order to keep track and
    complete the multipart upload initiated by the ``FileInfo``
    object.
    """
    def __init__(self, part_number, chunk_size,
                 result_queue, upload_context, filename):
        self._result_queue = result_queue
        self._upload_context = upload_context
        self._part_number = part_number
        self._chunk_size = chunk_size
        self._filename = filename

    def _read_part(self):
        actual_filename = self._filename.src
        in_file_part_number = self._part_number - 1
        starting_byte = in_file_part_number * self._chunk_size
        return ReadFileChunk(actual_filename, starting_byte, self._chunk_size)

    def __call__(self):
        LOGGER.debug("Uploading part %s for filename: %s",
                     self._part_number, self._filename.src)
        try:
            LOGGER.debug("Waiting for upload id.")
            upload_id = self._upload_context.wait_for_upload_id()
            bucket, key = find_bucket_key(self._filename.dest)
            total = int(math.ceil(
                self._filename.size/float(self._chunk_size)))
            body = self._read_part()
            params = {'endpoint': self._filename.endpoint,
                      'bucket': bucket, 'key': key,
                      'part_number': self._part_number,
                      'upload_id': upload_id,
                      'body': body}
            try:
                response_data, http = operate(
                    self._filename.service, 'UploadPart', params)
            finally:
                body.close()
            etag = response_data['ETag'][1:-1]
            self._upload_context.announce_finished_part(
                etag=etag, part_number=self._part_number)

            message = print_operation(self._filename, 0)
            result = {'message': message, 'total_parts': total,
                      'error': False}
            self._result_queue.put(result)
        except UploadCancelledError as e:
            # We don't need to do anything in this case.  The task
            # has been cancelled, and the task that cancelled the
            # task has already queued a message.
            LOGGER.debug("Not uploading part, task has been cancelled.")
        except Exception as e:
            LOGGER.debug('Error during part upload: %s', e,
                         exc_info=True)
            message = print_operation(self._filename, failed=True,
                                      dryrun=False)
            message += '\n' + str(e)
            result = {'message': message, 'error': True}
            self._result_queue.put(result)
            self._upload_context.cancel_upload()
        else:
            LOGGER.debug("Part number %s completed for filename: %s",
                         self._part_number, self._filename.src)


class CreateLocalFileTask(OrderableTask):
    def __init__(self, context, filename):
        self._context = context
        self._filename = filename

    def __call__(self):
        dirname = os.path.dirname(self._filename.dest)
        try:
            if not os.path.isdir(dirname):
                try:
                    os.makedirs(dirname)
                except OSError:
                    # It's possible that between the if check and the makedirs
                    # check that another thread has come along and created the
                    # directory.  In this case the directory already exists and we
                    # can move on.
                    pass
            # Always create the file.  Even if it exists, we need to
            # wipe out the existing contents.
            with open(self._filename.dest, 'wb'):
                pass
        except Exception as e:
            self._context.cancel()
        else:
            self._context.announce_file_created()


class CompleteDownloadTask(OrderableTask):
    def __init__(self, context, filename, result_queue, params, io_queue):
        self._context = context
        self._filename = filename
        self._result_queue = result_queue
        self._parameters = params
        self._io_queue = io_queue

    def __call__(self):
        # When the file is downloading, we have a few things we need to do:
        # 1) Fix up the last modified time to match s3.
        # 2) Tell the result_queue we're done.
        # 3) Queue an IO request to the IO thread letting it know we're
        #    done with the file.
        self._context.wait_for_completion()
        last_update_tuple = self._filename.last_update.timetuple()
        mod_timestamp = time.mktime(last_update_tuple)
        os.utime(self._filename.dest, (int(mod_timestamp), int(mod_timestamp)))
        message = print_operation(self._filename, False,
                                  self._parameters['dryrun'])
        print_task = {'message': message, 'error': False}
        self._result_queue.put(print_task)
        self._io_queue.put(IOCloseRequest(self._filename.dest))


class DownloadPartTask(OrderableTask):
    """
    This task downloads and writes a part to a file.  This task pulls
    from a ``part_queue`` which represents the queue for a specific
    multipart download.  This pulling from a ``part_queue`` is necessary
    in order to keep track and complete the multipart download initiated by
    the ``FileInfo`` object.
    """

    # Amount to read from response body at a time.
    ITERATE_CHUNK_SIZE = 1024 * 1024
    READ_TIMEOUT = 60
    TOTAL_ATTEMPTS = 5

    def __init__(self, part_number, chunk_size, result_queue, service,
                 filename, context, io_queue):
        self._part_number = part_number
        self._chunk_size = chunk_size
        self._result_queue = result_queue
        self._filename = filename
        self._service = filename.service
        self._context = context
        self._io_queue = io_queue

    def __call__(self):
        try:
            self._download_part()
        except Exception as e:
            LOGGER.debug(
                'Exception caught downloading byte range: %s',
                e, exc_info=True)
            self._context.cancel()
            raise e

    def _download_part(self):
        total_file_size = self._filename.size
        start_range = self._part_number * self._chunk_size
        if self._part_number == int(total_file_size / self._chunk_size) - 1:
            end_range = ''
        else:
            end_range = start_range + self._chunk_size - 1
        range_param = 'bytes=%s-%s' % (start_range, end_range)
        LOGGER.debug("Downloading bytes range of %s for file %s", range_param,
                     self._filename.dest)
        bucket, key = find_bucket_key(self._filename.src)
        params = {'endpoint': self._filename.endpoint, 'bucket': bucket,
                  'key': key, 'range': range_param}
        for i in range(self.TOTAL_ATTEMPTS):
            try:
                LOGGER.debug("Making GetObject requests with byte range: %s",
                             range_param)
                response_data, http = operate(self._service, 'GetObject',
                                              params)
                LOGGER.debug("Response received from GetObject")
                body = response_data['Body']
                self._queue_writes(body)
                self._context.announce_completed_part(self._part_number)

                message = print_operation(self._filename, 0)
                total_parts = int(self._filename.size / self._chunk_size)
                result = {'message': message, 'error': False,
                          'total_parts': total_parts}
                self._result_queue.put(result)
                LOGGER.debug("Task complete: %s", self)
                return
            except (socket.timeout, socket.error) as e:
                LOGGER.debug("Socket timeout caught, retrying request, "
                             "(attempt %s / %s)", i, self.TOTAL_ATTEMPTS,
                             exc_info=True)
                continue
            except IncompleteReadError as e:
                LOGGER.debug("Incomplete read detected: %s, (attempt %s / %s)",
                             e, i, self.TOTAL_ATTEMPTS)
                continue
        raise RetriesExeededError("Maximum number of attempts exceeded: %s" %
                                  self.TOTAL_ATTEMPTS)

    def _queue_writes(self, body):
        self._context.wait_for_file_created()
        LOGGER.debug("Writing part number %s to file: %s",
                     self._part_number, self._filename.dest)
        iterate_chunk_size = self.ITERATE_CHUNK_SIZE
        body.set_socket_timeout(self.READ_TIMEOUT)
        amount_read = 0
        current = body.read(iterate_chunk_size)
        while current:
            offset = self._part_number * self._chunk_size + amount_read
            LOGGER.debug("Submitting IORequest to write queue.")
            self._io_queue.put(IORequest(self._filename.dest, offset, current))
            LOGGER.debug("Request successfully submitted.")
            amount_read += len(current)
            current = body.read(iterate_chunk_size)
        # Change log message.
        LOGGER.debug("Done queueing writes for part number %s to file: %s",
                     self._part_number, self._filename.dest)


class CreateMultipartUploadTask(BasicTask):
    def __init__(self, session, filename, parameters, result_queue,
                 upload_context):
        super(CreateMultipartUploadTask, self).__init__(
            session, filename, parameters, result_queue)
        self._upload_context = upload_context

    def __call__(self):
        LOGGER.debug("Creating multipart upload for file: %s",
                     self.filename.src)
        try:
            upload_id = self.filename.create_multipart_upload()
            LOGGER.debug("Announcing upload id: %s", upload_id)
            self._upload_context.announce_upload_id(upload_id)
        except Exception as e:
            LOGGER.debug("Error trying to create multipart upload: %s",
                         e, exc_info=True)
            self._upload_context.cancel_upload()
            message = print_operation(self.filename, True,
                                      self.parameters['dryrun'])
            message += '\n' + str(e)
            result = {'message': message, 'error': True}
            self.result_queue.put(result)
            raise e


class RemoveRemoteObjectTask(OrderableTask):
    def __init__(self, filename, context):
        self._context = context
        self._filename = filename

    def __call__(self):
        LOGGER.debug("Waiting for download to finish.")
        self._context.wait_for_completion()
        bucket, key = find_bucket_key(self._filename.src)
        params = {'endpoint': self._filename.endpoint,
                  'bucket': bucket, 'key': key}
        response_data, http = operate(
            self._filename.service, 'DeleteObject', params)


class CompleteMultipartUploadTask(BasicTask):
    def __init__(self, session, filename, parameters, result_queue,
                 upload_context):
        super(CompleteMultipartUploadTask, self).__init__(
            session, filename, parameters, result_queue)
        self._upload_context = upload_context

    def __call__(self):
        LOGGER.debug("Completing multipart upload for file: %s",
                     self.filename.src)
        upload_id = self._upload_context.wait_for_upload_id()
        parts = self._upload_context.wait_for_parts_to_finish()
        LOGGER.debug("Received upload id and parts list.")
        bucket, key = find_bucket_key(self.filename.dest)
        params = {
            'bucket': bucket, 'key': key,
            'endpoint': self.filename.endpoint,
            'upload_id': upload_id,
            'multipart_upload': {'Parts': parts},
        }
        try:
            operate(self.filename.service, 'CompleteMultipartUpload', params)
        except Exception as e:
            LOGGER.debug("Error trying to complete multipart upload: %s",
                         e, exc_info=True)
            message = print_operation(
                self.filename, failed=True,
                dryrun=self.parameters['dryrun'])
            message += '\n' + str(e)
            result = {
                'message': message,
                'error': True
            }
        else:
            LOGGER.debug("Multipart upload completed for: %s",
                         self.filename.src)
            message = print_operation(self.filename, False,
                                      self.parameters['dryrun'])
            result = {'message': message, 'error': False}
            self._upload_context.announce_completed()
        self.result_queue.put(result)


class RemoveFileTask(BasicTask):
    def __init__(self, local_filename, upload_context):
        self._local_filename = local_filename
        self._upload_context = upload_context
        # This 'filename' attr has to be here because other objects
        # introspect tasks objects.  This should eventually be removed
        # but it's needed for now.
        self.filename = None

    def __call__(self):
        LOGGER.debug("Waiting for upload to complete.")
        self._upload_context.wait_for_completion()
        LOGGER.debug("Removing local file: %s", self._local_filename)
        os.remove(self._local_filename)


class MultipartUploadContext(object):
    """Context object for a multipart upload.

    Performing a multipart upload usually consists of three parts:

        * CreateMultipartUpload
        * UploadPart
        * CompleteMultipartUpload

    Each of those three parts are not independent of each other.  In order
    to upload a part, you need to know the upload id (created during the
    CreateMultipartUpload operation).  In order to complete a multipart
    you need the etags from all the parts (created during the UploadPart
    operations).  This context object provides the necessary building blocks
    to allow for the three stages to efficiently communicate with each other.

    This class is thread safe.

    """
    # These are the valid states for this object.
    _UNSTARTED = '_UNSTARTED'
    _STARTED = '_STARTED'
    _CANCELLED = '_CANCELLED'
    _COMPLETED = '_COMPLETED'

    def __init__(self, expected_parts):
        self._upload_id = None
        self._expected_parts = expected_parts
        self._parts = []
        self._lock = threading.Lock()
        self._upload_id_condition = threading.Condition(self._lock)
        self._parts_condition = threading.Condition(self._lock)
        self._upload_complete_condition = threading.Condition(self._lock)
        self._state = self._UNSTARTED

    def announce_upload_id(self, upload_id):
        with self._upload_id_condition:
            self._upload_id = upload_id
            self._state = self._STARTED
            self._upload_id_condition.notifyAll()

    def announce_finished_part(self, etag, part_number):
        with self._parts_condition:
            self._parts.append({'ETag': etag, 'PartNumber': part_number})
            self._parts_condition.notifyAll()

    def wait_for_parts_to_finish(self):
        with self._parts_condition:
            while len(self._parts) < self._expected_parts:
                if self._state == self._CANCELLED:
                    raise UploadCancelledError("Upload has been cancelled.")
                self._parts_condition.wait(timeout=1)
            return list(sorted(self._parts, key=lambda p: p['PartNumber']))

    def wait_for_upload_id(self):
        with self._upload_id_condition:
            while self._upload_id is None:
                if self._state == self._CANCELLED:
                    raise UploadCancelledError("Upload has been cancelled.")
                self._upload_id_condition.wait(timeout=1)
            return self._upload_id

    def wait_for_completion(self):
        with self._upload_complete_condition:
            while not self._state == self._COMPLETED:
                if self._state == self._CANCELLED:
                    raise UploadCancelledError("Upload has been cancelled.")
                self._upload_complete_condition.wait(timeout=1)

    def cancel_upload(self, canceller=None, args=None, kwargs=None):
        """Cancel the upload.

        If the upload is already in progress (via ``self.in_progress()``)
        you can provide a ``canceller`` argument that can be used to cancel
        the multipart upload request (typically this would call something like
        AbortMultipartUpload.  The canceller argument is a function that takes
        a single argument, which is the upload id::

            def my_canceller(upload_id):
                cancel.upload(bucket, key, upload_id)

        The ``canceller`` callable will only be called if the
        task is in progress.  If the task has not been started or is
        complete, then ``canceller`` will not be called.

        Note that ``canceller`` is called while an exclusive lock is held,
        so you cannot make any calls into the MultipartUploadContext object
        in the ``canceller`` object.

        """
        with self._lock:
            if self._state == self._STARTED and canceller is not None:
                if args is None:
                    args = ()
                if kwargs is None:
                    kwargs = {}
                canceller(self._upload_id, *args, **kwargs)
            self._state = self._CANCELLED

    def in_progress(self):
        """Determines whether or not the multipart upload is in process.

        Note that this has a very short gap from the time that a
        CreateMultipartUpload is called to the time the
        MultipartUploadContext object is told about the upload
        where this method will return False even though the multipart
        upload is in fact in progress.  This is solely based on whether
        or not the MultipartUploadContext has been notified about an
        upload id.
        """
        with self._lock:
            return self._state == self._STARTED

    def is_complete(self):
        with self._lock:
            return self._state == self._COMPLETED

    def is_cancelled(self):
        with self._lock:
            return self._state == self._CANCELLED

    def announce_completed(self):
        """Let the context object know that the upload is complete.

        This should be called after a CompleteMultipartUpload operation.

        """
        with self._upload_complete_condition:
            self._state = self._COMPLETED
            self._upload_complete_condition.notifyAll()


class MultipartDownloadContext(object):

    _STATES = {
        'UNSTARTED': 'UNSTARTED',
        'STARTED': 'STARTED',
        'COMPLETED': 'COMPLETED',
        'CANCELLED': 'CANCELLED'
    }

    def __init__(self, num_parts, lock=None):
        self.num_parts = num_parts

        if lock is None:
            lock = threading.Lock()
        self._lock = lock
        self._created_condition = threading.Condition(self._lock)
        self._completed_condition = threading.Condition(self._lock)
        self._state = self._STATES['UNSTARTED']
        self._finished_parts = set()

    def announce_completed_part(self, part_number):
        with self._completed_condition:
            self._finished_parts.add(part_number)
            if len(self._finished_parts) == self.num_parts:
                self._state = self._STATES['COMPLETED']
                self._completed_condition.notifyAll()

    def announce_file_created(self):
        with self._created_condition:
            self._state = self._STATES['STARTED']
            self._created_condition.notifyAll()

    def wait_for_file_created(self):
        with self._created_condition:
            while not self._state == self._STATES['STARTED']:
                if self._state == self._STATES['CANCELLED']:
                    raise DownloadCancelledError(
                        "Download has been cancelled.")
                self._created_condition.wait(timeout=1)

    def wait_for_completion(self):
        with self._completed_condition:
            while not self._state == self._STATES['COMPLETED']:
                if self._state == self._STATES['CANCELLED']:
                    raise DownloadCancelledError(
                        "Download has been cancelled.")
                self._completed_condition.wait(timeout=1)

    def cancel(self):
        with self._lock:
            self._state = self._STATES['CANCELLED']

    def is_cancelled(self):
        with self._lock:
            return self._state == self._STATES['CANCELLED']

    def is_started(self):
        with self._lock:
            return self._state == self._STATES['STARTED']

########NEW FILE########
__FILENAME__ = utils
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from datetime import datetime
import mimetypes
import hashlib
import math
import os
import sys
from collections import namedtuple, deque
from functools import partial

from six import PY3
from six.moves import queue
from dateutil.parser import parse
from dateutil.tz import tzlocal
from botocore.compat import unquote_str

from awscli.customizations.s3.constants import MAX_PARTS
from awscli.customizations.s3.constants import MAX_SINGLE_UPLOAD_SIZE


class MD5Error(Exception):
    """
    Exception for md5's that do not match.
    """
    pass


class StablePriorityQueue(queue.Queue):
    """Priority queue that maintains FIFO order for same priority items.

    This class was written to handle the tasks created in
    awscli.customizations.s3.tasks, but it's possible to use this
    class outside of that context.  In order for this to be the case,
    the following conditions should be met:

        * Objects that are queued should have a PRIORITY attribute.
          This should be an integer value not to exceed the max_priority
          value passed into the ``__init__``.  Objects with lower
          priority numbers are retrieved before objects with higher
          priority numbers.
        * A relatively small max_priority should be chosen.  ``get()``
          calls are O(max_priority).

    Any object that does not have a ``PRIORITY`` attribute or whose
    priority exceeds ``max_priority`` will be queued at the highest
    (least important) priority available.

    """
    def __init__(self, maxsize=0, max_priority=20):
        queue.Queue.__init__(self, maxsize=maxsize)
        self.priorities = [deque([]) for i in range(max_priority + 1)]
        self.default_priority = max_priority

    def _qsize(self):
        size = 0
        for bucket in self.priorities:
            size += len(bucket)
        return size

    def _put(self, item):
        priority = min(getattr(item, 'PRIORITY', self.default_priority),
                        self.default_priority)
        self.priorities[priority].append(item)

    def _get(self):
        for bucket in self.priorities:
            if not bucket:
                continue
            return bucket.popleft()


def find_bucket_key(s3_path):
    """
    This is a helper function that given an s3 path such that the path is of
    the form: bucket/key
    It will return the bucket and the key represented by the s3 path
    """
    s3_components = s3_path.split('/')
    bucket = s3_components[0]
    s3_key = ""
    if len(s3_components) > 1:
        s3_key = '/'.join(s3_components[1:])
    return bucket, s3_key


def split_s3_bucket_key(s3_path):
    """Split s3 path into bucket and key prefix.

    This will also handle the s3:// prefix.

    :return: Tuple of ('bucketname', 'keyname')

    """
    if s3_path.startswith('s3://'):
        s3_path = s3_path[5:]
    return find_bucket_key(s3_path)


def get_file_stat(path):
    """
    This is a helper function that given a local path return the size of
    the file in bytes and time of last modification.
    """
    stats = os.stat(path)
    update_time = datetime.fromtimestamp(stats.st_mtime, tzlocal())
    return stats.st_size, update_time


def check_etag(etag, fileobj):
    """
    This fucntion checks the etag and the md5 checksum to ensure no
    data was corrupted upon transfer.
    """
    get_chunk = partial(fileobj.read, 1024 * 1024)
    m = hashlib.md5()
    for chunk in iter(get_chunk, b''):
        m.update(chunk)
    if '-' not in etag:
        if etag != m.hexdigest():
            raise MD5Error


def check_error(response_data):
    """
    A helper function that prints out the error message recieved in the
    response_data and raises an error when there is an error.
    """
    if response_data:
        if 'Errors' in response_data:
            errors = response_data['Errors']
            for error in errors:
                raise Exception("Error: %s\n" % error['Message'])


def operate(service, cmd, kwargs):
    """
    A helper function that universally calls any command by taking in the
    service, name of the command, and any additional parameters required in
    the call.
    """
    operation = service.get_operation(cmd)
    http_response, response_data = operation.call(**kwargs)
    check_error(response_data)
    return response_data, http_response


def find_chunksize(size, current_chunksize):
    """
    The purpose of this function is determine a chunksize so that
    the number of parts in a multipart upload is not greater than
    the ``MAX_PARTS``.  If the ``chunksize`` is greater than
    ``MAX_SINGLE_UPLOAD_SIZE`` it returns ``MAX_SINGLE_UPLOAD_SIZE``.
    """
    chunksize = current_chunksize
    num_parts = int(math.ceil(size / float(chunksize)))
    while num_parts > MAX_PARTS:
        chunksize *= 2
        num_parts = int(math.ceil(size / float(chunksize)))
    if chunksize > MAX_SINGLE_UPLOAD_SIZE:
        return MAX_SINGLE_UPLOAD_SIZE
    else:
        return chunksize


class MultiCounter(object):
    """
    This class is used as a way to keep track of how many multipart
    operations are in progress.  It also is used to track how many
    part operations are occuring.
    """
    def __init__(self):
        self.count = 0


def uni_print(statement):
    """
    This function is used to properly write unicode to stdout.  It
    ensures that the proper encoding is used if the statement is
    not in a version type of string.  The initial check is to
    allow if ``sys.stdout`` does not use an encoding
    """
    encoding = getattr(sys.stdout, 'encoding', None)
    if encoding is not None and not PY3:
        sys.stdout.write(statement.encode(sys.stdout.encoding))
    else:
        try:
            sys.stdout.write(statement)
        except UnicodeEncodeError:
            # Some file like objects like cStringIO will
            # try to decode as ascii.  Interestingly enough
            # this works with a normal StringIO.
            sys.stdout.write(statement.encode('utf-8'))


def guess_content_type(filename):
    """Given a filename, guess it's content type.

    If the type cannot be guessed, a value of None is returned.
    """
    return mimetypes.guess_type(filename)[0]


def relative_path(filename, start=os.path.curdir):
    """Cross platform relative path of a filename.

    If no relative path can be calculated (i.e different
    drives on Windows), then instead of raising a ValueError,
    the absolute path is returned.

    """
    try:
        dirname, basename = os.path.split(filename)
        relative_dir = os.path.relpath(dirname, start)
        return os.path.join(relative_dir, basename)
    except ValueError:
        return os.path.abspath(filename)


class ReadFileChunk(object):
    def __init__(self, filename, start_byte, size):
        self._filename = filename
        self._start_byte = start_byte
        self._fileobj = open(self._filename, 'rb')
        self._size = self._calculate_file_size(self._fileobj, requested_size=size,
                                               start_byte=start_byte)
        self._fileobj.seek(self._start_byte)
        self._amount_read = 0

    def _calculate_file_size(self, fileobj, requested_size, start_byte):
        actual_file_size = os.fstat(fileobj.fileno()).st_size
        max_chunk_size = actual_file_size - start_byte
        return min(max_chunk_size, requested_size)

    def read(self, amount=None):
        if amount is None:
            remaining = self._size - self._amount_read
            data = self._fileobj.read(remaining)
            self._amount_read += remaining
            return data
        else:
            actual_amount = min(self._size - self._amount_read, amount)
            data = self._fileobj.read(actual_amount)
            self._amount_read += actual_amount
            return data

    def seek(self, where):
        self._fileobj.seek(self._start_byte + where)
        self._amount_read = where

    def close(self):
        self._fileobj.close()

    def tell(self):
        return self._amount_read

    def __len__(self):
        # __len__ is defined because requests will try to determine the length
        # of the stream to set a content length.  In the normal case
        # of the file it will just stat the file, but we need to change that
        # behavior.  By providing a __len__, requests will use that instead
        # of stat'ing the file.
        return self._size

    def __enter__(self):
        return self

    def __exit__(self, *args, **kwargs):
        self._fileobj.close()

    def __iter__(self):
        # This is a workaround for http://bugs.python.org/issue17575
        # Basically httplib will try to iterate over the contents, even
        # if its a file like object.  This wasn't noticed because we've
        # already exhausted the stream so iterating over the file immediately
        # steps, which is what we're simulating here.
        return iter([])


def _date_parser(date_string):
    return parse(date_string).astimezone(tzlocal())


class BucketLister(object):
    """List keys in a bucket."""
    def __init__(self, operation, endpoint, date_parser=_date_parser):
        self._operation = operation
        self._endpoint = endpoint
        self._date_parser = date_parser

    def list_objects(self, bucket, prefix=None):
        kwargs = {'bucket': bucket, 'encoding_type': 'url'}
        if prefix is not None:
            kwargs['prefix'] = prefix
        # This event handler is needed because we use encoding_type url and
        # we're paginating.  The pagination token is the last Key of the
        # Contents list.  However, botocore does not know that the encoding
        # type needs to be urldecoded.
        with ScopedEventHandler(self._operation.session, 'after-call.s3.ListObjects',
                                self._decode_keys):
            pages = self._operation.paginate(self._endpoint, **kwargs)
            for response, page in pages:
                contents = page['Contents']
                for content in contents:
                    source_path = bucket + '/' + content['Key']
                    size = content['Size']
                    last_update = self._date_parser(content['LastModified'])
                    yield source_path, size, last_update

    def _decode_keys(self, parsed, **kwargs):
        for content in parsed['Contents']:
            content['Key'] = unquote_str(content['Key'])


class ScopedEventHandler(object):
    """Register an event callback for the duration of a scope."""

    def __init__(self, session, event_name, handler):
        self._session = session
        self._event_name = event_name
        self._handler = handler

    def __enter__(self):
        self._session.register(self._event_name, self._handler)

    def __exit__(self, exc_type, exc_value, traceback):
        self._session.unregister(self._event_name, self._handler)


IORequest = namedtuple('IORequest', ['filename', 'offset', 'data'])
# Used to signal that IO for the filename is finished, and that
# any associated resources may be cleaned up.
IOCloseRequest = namedtuple('IOCloseRequest', ['filename'])

########NEW FILE########
__FILENAME__ = service
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import botocore.session


class OperationProxy(object):
    """
    A callable proxy for a service operation. This object essentially
    acts like a function where the keyword arguments are passed as
    arguments to the operation. This is used along with the Service
    class below to provide a simple interface to Botocore operations.

    Botocore usually returns two objects from an operation: the HTTP
    response and parsed data. Calling an OperationProxy instance will
    return the parsed data, or raise an OperationException if the
    HTTP response code is not successful (200, 201, etc).
    """
    def __init__(self, service, name, endpoint):
        self.service = service
        self.name = name
        self.endpoint = endpoint

        self._operation = service.get_operation(name)

    def __call__(self, **kwargs):
        res, data = self._operation.call(self.endpoint, **kwargs)

        return data


class Service(object):
    """
    An object to represent a Botocore service for a particular
    region/endpoint and session, which allows simple attribute
    access to operations.

        >>> s3 = Service('s3')
        >>> s3.CreateBucket(bucket='my-test-bucket')
        >>> data = s3.PutObject(bucket='my-test-bucket', key='test.txt',
                                body='Hello, world!')
        >>> print(data['ETag'])
        '828ef3fdfa96f00ad9f27c383fc9ac7f'

    An endpoint can be set in the constructor:

        >>> s3 = Service('s3', 'us-west-1')

    As can a specific endpoint URL:

       >>> s3 = Service('s3', {'endpoint_url': 'http://...'})

    Note that by default, a new session is used. When creating instances
    from a custom command you should reuse the default CLI session, which
    has extra error handling and profile handling enabled.A custom session
    can be used by passing it to the constructor:

        >>> s3 = Service('s3', session=mysession)

    """
    def __init__(self, name, endpoint_args=None, session=None):
        self.name = name
        self.session = session or botocore.session.get_session()

        self._service = self.session.get_service(name)

        if endpoint_args is None:
            endpoint_args = {}
        elif not isinstance(endpoint_args, dict):
            endpoint_args = {'region_name': endpoint_args}

        self.endpoint = self._service.get_endpoint(**endpoint_args)

    def __getattr__(self, name):
        return OperationProxy(self._service, name, self.endpoint)

########NEW FILE########
__FILENAME__ = sessendemail
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
This customization provides a simpler interface for the ``ses send-email``
command.  This simplified form is based on the legacy CLI.  The simple format
will be::

aws ses send-email --subject SUBJECT --from FROM_EMAIL
    --to-addresses addr ... --cc-addresses addr ... 
    --bcc-addresses addr ... --reply-to-addresses addr ...
    --return-path addr --text TEXTBODY --html HTMLBODY

"""

from awscli.customizations import utils
from awscli.arguments import CustomArgument
from awscli.customizations.utils import validate_mutually_exclusive_handler

TO_HELP = ('The email addresses of the primary recipients.  '
           'You can specify multiple recipients as space-separated values')
CC_HELP = ('The email addresses of copy recipients (Cc).  '
           'You can specify multiple recipients as space-separated values')
BCC_HELP = ('The email addresses of blind-carbon-copy recipients (Bcc).  '
            'You can specify multiple recipients as space-separated values')
SUBJECT_HELP = 'The subject of the message'
TEXT_HELP = 'The raw text body of the message'
HTML_HELP = 'The HTML body of the message'


def register_ses_send_email(event_handler):
    event_handler.register('building-argument-table.ses.send-email',
                           _promote_args)
    event_handler.register(
        'operation-args-parsed.ses.send-email',
        validate_mutually_exclusive_handler(
            ['destination'], ['to', 'cc', 'bcc']))
    event_handler.register(
        'operation-args-parsed.ses.send-email',
        validate_mutually_exclusive_handler(
            ['message'], ['text', 'html']))


def _promote_args(argument_table, **kwargs):
    argument_table['message'].required = False
    argument_table['destination'].required = False
    utils.rename_argument(argument_table, 'source',
                          new_name='from')
    argument_table['to'] = AddressesArgument(
        'to', 'ToAddresses', help_text=TO_HELP)
    argument_table['cc'] = AddressesArgument(
        'cc', 'CcAddresses', help_text=CC_HELP)
    argument_table['bcc'] = AddressesArgument(
        'bcc', 'BccAddresses', help_text=BCC_HELP)
    argument_table['subject'] = BodyArgument(
        'subject', 'Subject', help_text=SUBJECT_HELP)
    argument_table['text'] = BodyArgument(
        'text', 'Text', help_text=TEXT_HELP)
    argument_table['html'] = BodyArgument(
        'html', 'Html', help_text=HTML_HELP)


def _build_destination(params, key, value):
    # Build up the Destination data structure
    if 'destination' not in params:
        params['destination'] = {}
    params['destination'][key] = value


def _build_message(params, key, value):
    # Build up the Message data structure
    if 'message' not in params:
        params['message'] = {'Subject': {}, 'Body': {}}
    if key in ('Text', 'Html'):
        params['message']['Body'][key] = {'Data': value}
    elif key == 'Subject':
        params['message']['Subject'] = {'Data': value}


class AddressesArgument(CustomArgument):

    def __init__(self, name, json_key, help_text='', dest=None, default=None,
                 action=None, required=None, choices=None, cli_type_name=None):
        super(AddressesArgument, self).__init__(name=name, help_text=help_text,
                                                required=required, nargs='+')
        self._json_key = json_key
    
    def add_to_params(self, parameters, value):
        if value:
            _build_destination(parameters, self._json_key, value)


class BodyArgument(CustomArgument):

    def __init__(self, name, json_key, help_text='', required=None):
        super(BodyArgument, self).__init__(name=name, help_text=help_text,
                                           required=required)
        self._json_key = json_key
    
    def add_to_params(self, parameters, value):
        if value:
            _build_message(parameters, self._json_key, value)
            

########NEW FILE########
__FILENAME__ = streamingoutputarg
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.arguments import BaseCLIArgument


def add_streaming_output_arg(argument_table, operation, **kwargs):
    # Implementation detail:  hooked up to 'building-argument-table'
    # event.
    stream_param = operation.is_streaming()
    if stream_param:
        argument_table['outfile'] = StreamingOutputArgument(
            response_key=stream_param, operation=operation,
            name='outfile')


class StreamingOutputArgument(BaseCLIArgument):

    BUFFER_SIZE = 32768
    HELP = 'Filename where the content will be saved'

    def __init__(self, response_key, operation, name, buffer_size=None):
        self._name = name
        self.argument_object = operation
        if buffer_size is None:
            buffer_size = self.BUFFER_SIZE
        self._buffer_size = buffer_size
        self._operation = operation
        # This is the key in the response body where we can find the
        # streamed contents.
        self._response_key = response_key
        self._output_file = None
        self._name = name

    @property
    def cli_name(self):
        # Because this is a parameter, not an option, it shouldn't have the
        # '--' prefix. We want to use the self.py_name to indicate that it's an
        # argument.
        return self._name

    @property
    def cli_type_name(self):
        return 'string'

    @property
    def required(self):
        return True

    @property
    def documentation(self):
        return self.HELP

    def add_to_parser(self, parser):
        parser.add_argument(self._name, metavar=self.py_name,
                            help=self.HELP)

    def add_to_params(self, parameters, value):
        self._output_file = value
        service_name = self._operation.service.endpoint_prefix
        operation_name = self._operation.name
        self._operation.session.register('after-call.%s.%s' % (
            service_name, operation_name), self.save_file)

    def save_file(self, http_response, parsed, **kwargs):
        body = parsed[self._response_key]
        buffer_size = self._buffer_size
        with open(self._output_file, 'wb') as fp:
            data = body.read(buffer_size)
            while data:
                fp.write(data)
                data = body.read(buffer_size)
        # We don't want to include the streaming param in
        # the returned response.
        del parsed[self._response_key]

########NEW FILE########
__FILENAME__ = toplevelbool
# language governing permissions and limitations under the License.
"""
Top Level Boolean Parameters
----------------------------

This customization will take a parameter that has
a structure of a single boolean element and allow the argument
to be specified without a value.

Instead of having to say::

    --ebs-optimized '{"Value": true}'
    --ebs-optimized '{"Value": false}'

You can instead say `--ebs-optimized/--no-ebs-optimized`.


"""
import logging
from functools import partial


from awscli.argprocess import detect_shape_structure
from awscli import arguments
from awscli.customizations.utils import validate_mutually_exclusive_handler


LOG = logging.getLogger(__name__)
# This sentinel object is used to distinguish when
# a parameter is not specified vs. specified with no value
# (a value of None).
_NOT_SPECIFIED = object()


def register_bool_params(event_handler):
    event_handler.register('building-argument-table.ec2.*',
                           partial(pull_up_bool,
                                   event_handler=event_handler))


def pull_up_bool(argument_table, event_handler, **kwargs):
    # List of tuples of (positive_bool, negative_bool)
    # This is used to validate that we don't specify
    # an --option and a --no-option.
    boolean_pairs = []
    event_handler.register(
        'operation-args-parsed.ec2.*',
        partial(validate_boolean_mutex_groups,
                boolean_pairs=boolean_pairs))
    for key, value in list(argument_table.items()):
        if hasattr(value, 'argument_object'):
            arg_object = value.argument_object
            if detect_shape_structure(arg_object) == 'structure(scalar)' and \
                    len(arg_object.members) == 1 and \
                    arg_object.members[0].name == 'Value' and \
                    arg_object.members[0].type == 'boolean':
                # Swap out the existing CLIArgument for two args:
                # one that supports --option and --option <some value>
                # and another arg of --no-option.
                new_arg = PositiveBooleanArgument(
                    value.name, arg_object, value.operation_object,
                    value.name)
                argument_table[value.name] = new_arg
                negative_name = 'no-%s' % value.name
                negative_arg = NegativeBooleanParameter(
                    negative_name, new_arg.py_name,
                    arg_object, value.operation_object,
                    action='store_true', dest='no_%s' % new_arg.py_name,
                    group_name=value.name)
                argument_table[negative_name] = negative_arg
                # If we've pulled up a structure(scalar) arg
                # into a pair of top level boolean args, we need
                # to validate that a user only provides the argument
                # once.  They can't say --option/--no-option, nor
                # can they say --option --option Value=false.
                boolean_pairs.append((new_arg, negative_arg))


def validate_boolean_mutex_groups(boolean_pairs, parsed_args, **kwargs):
    # Validate we didn't pass in an --option and a --no-option.
    for positive, negative in boolean_pairs:
        if getattr(parsed_args, positive.py_name) is not _NOT_SPECIFIED and \
                getattr(parsed_args, negative.py_name) is not _NOT_SPECIFIED:
            raise ValueError(
                'Cannot specify both the "%s" option and '
                'the "%s" option.' % (positive.cli_name, negative.cli_name))


class PositiveBooleanArgument(arguments.CLIArgument):
    def __init__(self, name, argument_object, operation_object, group_name):
        super(PositiveBooleanArgument, self).__init__(
            name, argument_object, operation_object)
        self._group_name = group_name

    @property
    def group_name(self):
        return self._group_name

    def add_to_parser(self, parser):
        # We need to support three forms:
        # --option-name
        # --option-name Value=(true|false)
        parser.add_argument(self.cli_name,
                            help=self.documentation,
                            action='store',
                            default=_NOT_SPECIFIED,
                            nargs='?')

    def add_to_params(self, parameters, value):
        if value is _NOT_SPECIFIED:
            return
        elif value is None:
            # Then this means that the user explicitly
            # specified this arg with no value,
            # e.g. --boolean-parameter
            # which means we should add a true value
            # to the parameters dict.
            parameters[self.argument_object.py_name] = {'Value': True}
        else:
            # Otherwise the arg was specified with a value.
            parameters[self.argument_object.py_name] = self._unpack_argument(
                value)


class NegativeBooleanParameter(arguments.BooleanArgument):
    def __init__(self, name, positive_py_name,
                 argument_object, operation_object,
                 action='store_true', dest=None, group_name=None):
        super(NegativeBooleanParameter, self).__init__(
            name, argument_object, operation_object, default=_NOT_SPECIFIED)
        self._group_name = group_name
        self._positive_py_name = positive_py_name

    def add_to_params(self, parameters, value):
        if value is not _NOT_SPECIFIED and value:
            parameters[self._positive_py_name] = {'Value': False}

########NEW FILE########
__FILENAME__ = utils
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
Utility functions to make it easier to work with customizations.

"""
def rename_argument(argument_table, existing_name, new_name):
    current = argument_table[existing_name]
    argument_table[new_name] = current
    current.name = new_name
    del argument_table[existing_name]


def rename_command(command_table, existing_name, new_name):
    current = command_table[existing_name]
    command_table[new_name] = current
    current.name = new_name
    del command_table[existing_name]


def validate_mutually_exclusive_handler(*groups):
    def _handler(parsed_args, **kwargs):
        return validate_mutually_exclusive(parsed_args, *groups)
    return _handler


def validate_mutually_exclusive(parsed_args, *groups):
    """Validate mututally exclusive groups in the parsed args."""
    args_dict = vars(parsed_args)
    all_args = set(arg for group in groups for arg in group)
    if not any(k in all_args for k in args_dict if args_dict[k] is not None):
        # If none of the specified args are in a mutually exclusive group
        # there is nothing left to validate.
        return
    current_group = None
    for key in [k for k in args_dict if args_dict[k] is not None]:
        key_group = _get_group_for_key(key, groups)
        if key_group is None:
            # If they key is not part of a mutex group, we can move on.
            continue
        if current_group is None:
            current_group = key_group
        elif not key_group == current_group:
            raise ValueError('The key "%s" cannot be specified when one '
                             'of the following keys are also specified: '
                             '%s' % (key, ', '.join(current_group)))


def _get_group_for_key(key, groups):
    for group in groups:
        if key in group:
            return group

########NEW FILE########
__FILENAME__ = errorhandler
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

import sys
import logging

LOG = logging.getLogger(__name__)


class BaseOperationError(Exception):
    MSG_TEMPLATE = ("A {error_type} error ({error_code}) occurred "
                    "when calling the {operation_name} operation: "
                    "{error_message}")

    def __init__(self, error_code, error_message, error_type, operation_name,
                 http_status_code):
        msg = self.MSG_TEMPLATE.format(
            error_code=error_code, error_message=error_message,
            error_type=error_type, operation_name=operation_name)
        super(BaseOperationError, self).__init__(msg)
        self.error_code = error_code
        self.error_message = error_message
        self.error_type = error_type
        self.operation_name = operation_name
        self.http_status_code = http_status_code


class ClientError(BaseOperationError):
    pass


class ServerError(BaseOperationError):
    pass


class ErrorHandler(object):
    """
    This class is responsible for handling any HTTP errors that occur
    when a service operation is called.  It is registered for the
    ``after-call`` event and will have the opportunity to inspect
    all operation calls.  If the HTTP response contains an error
    ``status_code`` an appropriate error message will be printed and
    the handler will short-circuit all further processing by exiting
    with an appropriate error code.
    """

    def __call__(self, http_response, parsed, operation, **kwargs):
        LOG.debug('HTTP Response Code: %d', http_response.status_code)
        msg_template = ("A {error_type} error ({error_code}) occurred "
                        "when calling the {operation_name} operation: "
                        "{error_message}")
        error_type = None
        error_class = None
        if http_response.status_code >= 500:
            error_type = 'server'
            error_class = ServerError
        if http_response.status_code >= 400 or http_response.status_code == 301:
            error_type = 'client'
            error_class = ClientError
        if error_class is not None:
            code, message = self._get_error_code_and_message(parsed)
            raise error_class(
                error_code=code, error_message=message,
                error_type=error_type, operation_name=operation.name,
                http_status_code=http_response.status_code)

    def _get_error_code_and_message(self, response):
        code = 'Unknown'
        message = 'Unknown'
        if 'Errors' in response:
            if isinstance(response['Errors'], list):
                error = response['Errors'][-1]
                if 'Code' in error:
                    code = error['Code']
                elif 'Type' in error:
                    code = error['Type']
                if 'Message' in error:
                    message = error['Message']
        return (code, message)

########NEW FILE########
__FILENAME__ = formatter
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.

# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at

#     http://aws.amazon.com/apache2.0/

# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import logging
import sys
import json

from botocore.utils import set_value_from_jmespath

from awscli.table import MultiTable, Styler, ColorizedStyler
from awscli import text
from awscli import compat


LOG = logging.getLogger(__name__)


class Formatter(object):
    def __init__(self, args):
        self._args = args

    def _remove_request_id(self, response_data):
        # We only want to display the ResponseMetadata (which includes
        # the request id) if there is an error in the response.
        # Since all errors have been unified under the Errors key,
        # this should be a reasonable way to filter.
        if 'Errors' not in response_data:
            if 'ResponseMetadata' in response_data:
                if 'RequestId' in response_data['ResponseMetadata']:
                    request_id = response_data['ResponseMetadata']['RequestId']
                    LOG.debug('RequestId: %s', request_id)
                del response_data['ResponseMetadata']

    def _get_default_stream(self):
        if getattr(sys.stdout, 'encoding', None) is None:
            # In python3, sys.stdout.encoding is always set.
            # In python2, if you redirect to stdout, then
            # encoding is not None.  In this case we'll default
            # to utf-8.
            return compat.get_stdout_text_writer()
        else:
            return sys.stdout


class FullyBufferedFormatter(Formatter):
    def __call__(self, operation, response, stream=None):
        if stream is None:
            # Retrieve stdout on invocation instead of at import time
            # so that if anything wraps stdout we'll pick up those changes
            # (specifically colorama on windows wraps stdout).
            stream = self._get_default_stream()
        # I think the interfaces between non-paginated
        # and paginated responses can still be cleaned up.
        if operation.can_paginate and self._args.paginate:
            response_data = response.build_full_result()
        else:
            response_data = response
        try:
            self._remove_request_id(response_data)
            if self._args.query is not None:
                response_data = self._args.query.search(response_data)
            self._format_response(operation, response_data, stream)
        finally:
            # flush is needed to avoid the "close failed in file object
            # destructor" in python2.x (see http://bugs.python.org/issue11380).
            stream.flush()


class JSONFormatter(FullyBufferedFormatter):

    def _format_response(self, operation, response, stream):
        # For operations that have no response body (e.g. s3 put-object)
        # the response will be an empty string.  We don't want to print
        # that out to the user but other "falsey" values like an empty
        # dictionary should be printed.
        if response:
            json.dump(response, stream, indent=4)
            stream.write('\n')


class TableFormatter(FullyBufferedFormatter):
    """Pretty print a table from a given response.

    The table formatter is able to take any generic response
    and generate a pretty printed table.  It does this without
    using the output definition from the model.

    """
    def __init__(self, args, table=None):
        super(TableFormatter, self).__init__(args)
        if args.color == 'auto':
            self.table = MultiTable(initial_section=False,
                                    column_separator='|')
        elif args.color == 'off':
            styler = Styler()
            self.table = MultiTable(initial_section=False,
                                    column_separator='|', styler=styler)
        elif args.color == 'on':
            styler = ColorizedStyler()
            self.table = MultiTable(initial_section=False,
                                    column_separator='|', styler=styler)
        else:
            raise ValueError("Unknown color option: %s" % args.color)

    def _format_response(self, operation, response, stream):
        if self._build_table(operation.name, response):
            try:
                self.table.render(stream)
            except IOError:
                # If they're piping stdout to another process which exits before
                # we're done writing all of our output, we'll get an error about a
                # closed pipe which we can safely ignore.
                pass

    def _build_table(self, title, current, indent_level=0):
        if not current:
            return False
        if title is not None:
            self.table.new_section(title, indent_level=indent_level)
        if isinstance(current, list):
            if isinstance(current[0], dict):
                self._build_sub_table_from_list(current, indent_level, title)
            else:
                for item in current:
                    if self._scalar_type(item):
                        self.table.add_row([item])
                    elif all(self._scalar_type(el) for el in item):
                        self.table.add_row(item)
                    else:
                        self._build_table(title=None, current=item)
        if isinstance(current, dict):
            # Render a single row section with keys as header
            # and the row as the values, unless the value
            # is a list.
            self._build_sub_table_from_dict(current, indent_level)
        return True

    def _build_sub_table_from_dict(self, current, indent_level):
        # Render a single row section with keys as header
        # and the row as the values, unless the value
        # is a list.
        headers, more = self._group_scalar_keys(current)
        if len(headers) == 1:
            # Special casing if a dict has a single scalar key/value pair.
            self.table.add_row([headers[0], current[headers[0]]])
        elif headers:
            self.table.add_row_header(headers)
            self.table.add_row([current[k] for k in headers])
        for remaining in more:
            self._build_table(remaining, current[remaining],
                              indent_level=indent_level + 1)

    def _build_sub_table_from_list(self, current, indent_level, title):
        headers, more = self._group_scalar_keys_from_list(current)
        self.table.add_row_header(headers)
        first = True
        for element in current:
            if not first and more:
                self.table.new_section(title,
                                       indent_level=indent_level)
                self.table.add_row_header(headers)
            first = False
            # Use .get() to account for the fact that sometimes an element
            # may not have all the keys from the header.
            self.table.add_row([element.get(header, '') for header in headers])
            for remaining in more:
                # Some of the non scalar attributes may not necessarily
                # be in every single element of the list, so we need to
                # check this condition before recursing.
                if remaining in element:
                    self._build_table(remaining, element[remaining],
                                    indent_level=indent_level + 1)

    def _scalar_type(self, element):
        return not isinstance(element, (list, dict))

    def _group_scalar_keys_from_list(self, list_of_dicts):
        # We want to make sure we catch all the keys in the list of dicts.
        # Most of the time each list element has the same keys, but sometimes
        # a list element will have keys not defined in other elements.
        headers = set()
        more = set()
        for item in list_of_dicts:
            current_headers, current_more = self._group_scalar_keys(item)
            headers.update(current_headers)
            more.update(current_more)
        headers = list(sorted(headers))
        more = list(sorted(more))
        return headers, more

    def _group_scalar_keys(self, current):
        # Given a dict, separate the keys into those whose values are
        # scalar, and those whose values aren't.  Return two lists,
        # one is the scalar value keys, the second is the remaining keys.
        more = []
        headers = []
        for element in current:
            if self._scalar_type(current[element]):
                headers.append(element)
            else:
                more.append(element)
        headers.sort()
        more.sort()
        return headers, more


class TextFormatter(Formatter):

    def __call__(self, operation, response, stream=None):
        if stream is None:
            stream = self._get_default_stream()
        try:
            if operation.can_paginate and self._args.paginate:
                result_keys = response.result_keys
                for _, page in response:
                    current = {}
                    for result_key in result_keys:
                        data = result_key.search(page)
                        set_value_from_jmespath(
                            current,
                            result_key.expression,
                            data
                        )
                    self._format_response(current, stream)
                if response.resume_token:
                    # Tell the user about the next token so they can continue
                    # if they want.
                    self._format_response(
                        {'NextToken': {'NextToken': response.resume_token}},
                        stream)
            else:
                self._remove_request_id(response)
                self._format_response(response, stream)
        finally:
            # flush is needed to avoid the "close failed in file object
            # destructor" in python2.x (see http://bugs.python.org/issue11380).
            stream.flush()

    def _format_response(self, response, stream):
        if self._args.query is not None:
            expression = self._args.query
            response = expression.search(response)
        text.format_text(response, stream)


def get_formatter(format_type, args):
    if format_type == 'json':
        return JSONFormatter(args)
    elif format_type == 'text':
        return TextFormatter(args)
    elif format_type == 'table':
        return TableFormatter(args)
    raise ValueError("Unknown output type: %s" % format_type)

########NEW FILE########
__FILENAME__ = handlers
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""Builtin CLI extensions.

This is a collection of built in CLI extensions that can be automatically
registered with the event system.

"""
from awscli.argprocess import ParamShorthand
from awscli.argprocess import uri_param
from awscli.errorhandler import ErrorHandler
from awscli.customizations.streamingoutputarg import add_streaming_output_arg
from awscli.customizations.addexamples import add_examples
from awscli.customizations.removals import register_removals
from awscli.customizations.ec2addcount import ec2_add_count
from awscli.customizations.paginate import register_pagination
from awscli.customizations.ec2decryptpassword import ec2_add_priv_launch_key
from awscli.customizations.ec2secgroupsimplify import register_secgroup
from awscli.customizations.preview import register_preview_commands
from awscli.customizations.ec2bundleinstance import register_bundleinstance
from awscli.customizations.s3.s3 import s3_plugin_initialize
from awscli.customizations.ec2runinstances import register_runinstances
from awscli.customizations.rds import register_rds_modify_split
from awscli.customizations.putmetricdata import register_put_metric_data
from awscli.customizations.sessendemail import register_ses_send_email
from awscli.customizations.iamvirtmfa import IAMVMFAWrapper
from awscli.customizations.argrename import register_arg_renames
from awscli.customizations.dryrundocs import register_dryrun_docs
from awscli.customizations.route53resourceid import register_resource_id
from awscli.customizations.configure import register_configure_cmd
from awscli.customizations.cloudtrail import initialize as cloudtrail_init
from awscli.customizations.toplevelbool import register_bool_params
from awscli.customizations.ec2protocolarg import register_protocol_args
from awscli.customizations import datapipeline
from awscli.customizations.globalargs import register_parse_global_args
from awscli.customizations.cloudsearch import initialize as cloudsearch_init


def awscli_initialize(event_handlers):
    event_handlers.register('load-cli-arg', uri_param)
    param_shorthand = ParamShorthand()
    event_handlers.register('process-cli-arg', param_shorthand)
    error_handler = ErrorHandler()
    event_handlers.register('after-call.*.*', error_handler)
    # The following will get fired for every option we are
    # documenting.  It will attempt to add an example_fn on to
    # the parameter object if the parameter supports shorthand
    # syntax.  The documentation event handlers will then use
    # the examplefn to generate the sample shorthand syntax
    # in the docs.  Registering here should ensure that this
    # handler gets called first but it still feels a bit brittle.
    event_handlers.register('doc-option-example.*.*.*',
                            param_shorthand.add_example_fn)
    event_handlers.register('doc-examples.*.*',
                            add_examples)
    event_handlers.register('building-argument-table.s3api.*',
                            add_streaming_output_arg)
    event_handlers.register('building-argument-table.ec2.run-instances',
                            ec2_add_count)
    event_handlers.register('building-argument-table.ec2.get-password-data',
                            ec2_add_priv_launch_key)
    register_parse_global_args(event_handlers)
    register_pagination(event_handlers)
    register_secgroup(event_handlers)
    register_bundleinstance(event_handlers)
    s3_plugin_initialize(event_handlers)
    register_runinstances(event_handlers)
    register_removals(event_handlers)
    register_preview_commands(event_handlers)
    register_rds_modify_split(event_handlers)
    register_put_metric_data(event_handlers)
    register_ses_send_email(event_handlers)
    IAMVMFAWrapper(event_handlers)
    register_arg_renames(event_handlers)
    register_dryrun_docs(event_handlers)
    register_resource_id(event_handlers)
    register_configure_cmd(event_handlers)
    cloudtrail_init(event_handlers)
    register_bool_params(event_handlers)
    register_protocol_args(event_handlers)
    datapipeline.register_customizations(event_handlers)
    cloudsearch_init(event_handlers)

########NEW FILE########
__FILENAME__ = help
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import sys
import logging
import os
import platform
import shlex
from subprocess import Popen, PIPE

from docutils.core import publish_string
from docutils.writers import manpage

import bcdoc.docevents
from bcdoc.restdoc import ReSTDocument
from bcdoc.textwriter import TextWriter

from awscli.clidocs import ProviderDocumentEventHandler
from awscli.clidocs import ServiceDocumentEventHandler
from awscli.clidocs import OperationDocumentEventHandler
from awscli.argprocess import ParamShorthand


LOG = logging.getLogger('awscli.help')


class ExecutableNotFoundError(Exception):
    def __init__(self, executable_name):
        super(ExecutableNotFoundError, self).__init__(
            'Could not find executable named "%s"' % executable_name)


def get_renderer():
    """
    Return the appropriate HelpRenderer implementation for the
    current platform.
    """
    if platform.system() == 'Windows':
        return WindowsHelpRenderer()
    else:
        return PosixHelpRenderer()


class HelpRenderer(object):
    """
    Interface for a help renderer.

    The renderer is responsible for displaying the help content on
    a particular platform.
    """

    def render(self, contents):
        """
        Each implementation of HelpRenderer must implement this
        render method.
        """
        pass


class PosixHelpRenderer(HelpRenderer):
    """
    Render help content on a Posix-like system.  This includes
    Linux and MacOS X.
    """

    PAGER = 'less -R'

    def get_pager_cmdline(self):
        pager = self.PAGER
        if 'MANPAGER' in os.environ:
            pager = os.environ['MANPAGER']
        elif 'PAGER' in os.environ:
            pager = os.environ['PAGER']
        return shlex.split(pager)

    def render(self, contents):
        man_contents = publish_string(contents, writer=manpage.Writer())
        if not self._exists_on_path('groff'):
            raise ExecutableNotFoundError('groff')
        cmdline = ['groff', '-man', '-T', 'ascii']
        LOG.debug("Running command: %s", cmdline)
        p3 = self._popen(cmdline, stdin=PIPE, stdout=PIPE, stderr=PIPE)
        groff_output = p3.communicate(input=man_contents)[0]
        cmdline = self.get_pager_cmdline()
        LOG.debug("Running command: %s", cmdline)
        p4 = self._popen(cmdline, stdin=PIPE)
        p4.communicate(input=groff_output)
        sys.exit(1)

    def _get_rst2man_name(self):
        if self._exists_on_path('rst2man.py'):
            return 'rst2man.py'
        elif self._exists_on_path('rst2man'):
            # Some distros like ubuntu will rename rst2man.py to rst2man
            # if you install their version (i.e. "apt-get install
            # python-docutils").  Though they could technically rename
            # this to anything we'll support it renamed to 'rst2man' by
            # explicitly checking for this case ourself.
            return 'rst2man'
        else:
            # Give them the original name as set from docutils.
            raise ExecutableNotFoundError('rst2man.py')

    def _exists_on_path(self, name):
        # Since we're only dealing with POSIX systems, we can
        # ignore things like PATHEXT.
        return any([os.path.exists(os.path.join(p, name))
                    for p in os.environ.get('PATH', []).split(os.pathsep)])

    def _popen(self, *args, **kwargs):
        return Popen(*args, **kwargs)


class WindowsHelpRenderer(HelpRenderer):
    """
    Render help content on a Windows platform.
    """

    def render(self, contents):
        text_output = publish_string(contents,
                                     writer=TextWriter())
        sys.stdout.write(text_output.decode('utf-8'))
        sys.exit(1)


class RawRenderer(HelpRenderer):
    """
    Render help as the raw ReST document.
    """

    def render(self, contents):
        sys.stdout.write(contents)
        sys.exit(1)


class HelpCommand(object):
    """
    HelpCommand Interface
    ---------------------
    A HelpCommand object acts as the interface between objects in the
    CLI (e.g. Providers, Services, Operations, etc.) and the documentation
    system (bcdoc).

    A HelpCommand object wraps the object from the CLI space and provides
    a consistent interface to critical information needed by the
    documentation pipeline such as the object's name, description, etc.

    The HelpCommand object is passed to the component of the
    documentation pipeline that fires documentation events.  It is
    then passed on to each document event handler that has registered
    for the events.

    All HelpCommand objects contain the following attributes:

        + ``session`` - A ``botocore`` ``Session`` object.
        + ``obj`` - The object that is being documented.
        + ``command_table`` - A dict mapping command names to
              callable objects.
        + ``arg_table`` - A dict mapping argument names to callable objects.
        + ``doc`` - A ``Document`` object that is used to collect the
              generated documentation.

    In addition, please note the `properties` defined below which are
    required to allow the object to be used in the document pipeline.

    Implementations of HelpCommand are provided here for Provider,
    Service and Operation objects.  Other implementations for other
    types of objects might be needed for customization in plugins.
    As long as the implementations conform to this basic interface
    it should be possible to pass them to the documentation system
    and generate interactive and static help files.
    """

    EventHandlerClass = None
    """
    Each subclass should define this class variable to point to the
    EventHandler class used by this HelpCommand.
    """

    def __init__(self, session, obj, command_table, arg_table):
        self.session = session
        self.obj = obj
        if command_table is None:
            command_table = {}
        self.command_table = command_table
        if arg_table is None:
            arg_table = {}
        self.arg_table = arg_table
        self.renderer = get_renderer()
        self.doc = ReSTDocument(target='man')

    @property
    def event_class(self):
        """
        Return the ``event_class`` for this object.

        The ``event_class`` is used by the documentation pipeline
        when generating documentation events.  For the event below::

            doc-title.<event_class>.<name>

        The document pipeline would use this property to determine
        the ``event_class`` value.
        """
        pass

    @property
    def name(self):
        """
        Return the name of the wrapped object.

        This would be called by the document pipeline to determine
        the ``name`` to be inserted into the event, as shown above.
        """
        pass

    def __call__(self, args, parsed_globals):
        # Create an event handler for a Provider Document
        instance = self.EventHandlerClass(self)
        # Now generate all of the events for a Provider document.
        # We pass ourselves along so that we can, in turn, get passed
        # to all event handlers.
        bcdoc.docevents.generate_events(self.session, self)
        self.renderer.render(self.doc.getvalue())
        instance.unregister()


class ProviderHelpCommand(HelpCommand):
    """Implements top level help command.

    This is what is called when ``aws help`` is run.

    """
    EventHandlerClass = ProviderDocumentEventHandler

    def __init__(self, session, command_table, arg_table,
                 description, synopsis, usage):
        HelpCommand.__init__(self, session, session.provider,
                             command_table, arg_table)
        self.description = description
        self.synopsis = synopsis
        self.help_usage = usage

    @property
    def event_class(self):
        return 'Provider'

    @property
    def name(self):
        return self.obj.name


class ServiceHelpCommand(HelpCommand):
    """Implements service level help.

    This is the object invoked whenever a service command
    help is implemented, e.g. ``aws ec2 help``.

    """

    EventHandlerClass = ServiceDocumentEventHandler

    def __init__(self, session, obj, command_table, arg_table, name,
                 event_class):
        super(ServiceHelpCommand, self).__init__(session, obj, command_table,
                                                 arg_table)
        self._name = name
        self._event_class = event_class

    @property
    def event_class(self):
        return self._event_class

    @property
    def name(self):
        return self._name


class OperationHelpCommand(HelpCommand):
    """Implements operation level help.

    This is the object invoked whenever help for a service is requested,
    e.g. ``aws ec2 describe-instances help``.

    """
    EventHandlerClass = OperationDocumentEventHandler

    def __init__(self, session, service, operation, arg_table, name,
                 event_class):
        HelpCommand.__init__(self, session, operation, None, arg_table)
        self.service = service
        self.param_shorthand = ParamShorthand()
        self._name = name
        self._event_class = event_class

    @property
    def event_class(self):
        return self._event_class

    @property
    def name(self):
        return self._name

########NEW FILE########
__FILENAME__ = paramfile
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

import logging
import os

from botocore.vendored import requests
import six

from awscli.compat import compat_open


logger = logging.getLogger(__name__)


class ResourceLoadingError(Exception):
    pass


def get_paramfile(path):
    """
    It is possible to pass parameters to operations by referring
    to files or URI's.  If such a reference is detected, this
    function attempts to retrieve the data from the file or URI
    and returns it.  If there are any errors or if the ``path``
    does not appear to refer to a file or URI, a ``None`` is
    returned.
    """
    data = None
    if isinstance(path, six.string_types):
        for prefix in PrefixMap:
            if path.startswith(prefix):
                data = PrefixMap[prefix](prefix, path)
    return data


def get_file(prefix, path):
    file_path = path[len(prefix):]
    file_path = os.path.expanduser(file_path)
    file_path = os.path.expandvars(file_path)
    if not os.path.isfile(file_path):
        raise ResourceLoadingError("file does not exist: %s" % file_path)
    try:
        with compat_open(file_path, 'r') as f:
            return f.read()
    except (OSError, IOError) as e:
        raise ResourceLoadingError('Unable to load paramfile %s: %s' % (
            path, e))


def get_uri(prefix, uri):
    try:
        r = requests.get(uri)
        if r.status_code == 200:
            return r.text
        else:
            raise ResourceLoadingError(
                "received non 200 status code of %s" % (
                    r.status_code))
    except Exception as e:
        raise ResourceLoadingError('Unable to retrieve %s: %s' % (uri, e))


PrefixMap = {'file://': get_file,
             'http://': get_uri,
             'https://': get_uri}

########NEW FILE########
__FILENAME__ = plugin
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import logging

from botocore.hooks import HierarchicalEmitter

log = logging.getLogger('awscli.plugin')

BUILTIN_PLUGINS = {'__builtin__': 'awscli.handlers'}


def load_plugins(plugin_mapping, event_hooks=None, include_builtins=True):
    """

    :type plugin_mapping: dict
    :param plugin_mapping: A dict of plugin name to import path,
        e.g. ``{"plugingName": "package.modulefoo"}``.

    :type event_hooks: ``EventHooks``
    :param event_hooks: Event hook emitter.  If one if not provided,
        an emitter will be created and returned.  Otherwise, the
        passed in ``event_hooks`` will be used to initialize plugins.

    :type include_builtins: bool
    :param include_builtins: If True, the builtin awscli plugins (specified in
        ``BUILTIN_PLUGINS``) will be included in the list of plugins to load.

    :rtype: HierarchicalEmitter
    :return: An event emitter object.

    """
    if include_builtins:
        plugin_mapping.update(BUILTIN_PLUGINS)
    modules = _import_plugins(plugin_mapping)
    if event_hooks is None:
        event_hooks = HierarchicalEmitter()
    for name, plugin in zip(plugin_mapping.keys(), modules):
        log.debug("Initializing plugin %s: %s", name, plugin)
        plugin.awscli_initialize(event_hooks)
    return event_hooks


def _import_plugins(plugin_names):
    plugins = []
    for name, path in plugin_names.items():
        log.debug("Importing plugin %s: %s", name, path)
        if '.' not in path:
            plugins.append(__import__(path))
        else:
            package, module = path.rsplit('.', 1)
            module = __import__(path, fromlist=[module])
            plugins.append(module)
    return plugins

########NEW FILE########
__FILENAME__ = schema
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

class ParameterRequiredError(ValueError): pass


class SchemaTransformer(object):
    """
    Transforms a custom argument parameter schema into an internal
    model representation so that it can be treated like a normal
    service model. This includes shorthand JSON parsing and
    automatic documentation generation. The format of the schema
    follows JSON Schema, which can be found here:

    http://json-schema.org/

    Only a relevant subset of features is supported here:

    * Types: `object`, `array`, `string`, `number`, `integer`,
             `boolean`
    * Properties: `type`, `description`, `required`, `enum`

    For example::

    {
        "type": "array",
        "items": {
            "type": "object",
            "properties": {
                "arg1": {
                    "type": "string",
                    "required": True,
                    "enum": [
                        "Value1",
                        "Value2",
                        "Value3"
                    ]
                },
                "arg2": {
                    "type": "integer",
                    "description": "The number of calls"
                }
            }
        }
    }

    Assuming the schema is applied to a service named `foo`, with an
    operation named `bar` and that the parameter is called `baz`, you
    could call it with the shorthand JSON like so::

        $ aws foo bar --baz arg1=Value1,arg2=5 arg1=Value2

    """
    # Map schema types to internal representation types
    TYPE_MAP = {
        'array': 'list',
        'object': 'structure',
    }

    # Map schema properties to internal representation properties
    PROPERTY_MAP = {
        # List item description
        'items': 'members',
        # Object properties description
        'properties': 'members',
    }

    # List of known properties to copy or transform without any
    # other special processing.
    SUPPORTED_BASIC_PROPERTIES = [
        'type', 'description', 'required', 'enum'
    ]

    def __init__(self, schema):
        self.schema = schema

    def transform(self):
        """Convert to an internal representation of parameters"""
        return self._process_param(self.schema)

    def _process_param(self, param):
        transformed = {}

        if 'type' not in param:
            raise ParameterRequiredError(
                'The type property is required: {0}'.format(param))

        # Handle basic properties which are just copied and optionally
        # mapped to new values.
        for basic_property in self.SUPPORTED_BASIC_PROPERTIES:
            if basic_property in param:
                value = param[basic_property]

                if basic_property == 'type':
                    value = self.TYPE_MAP.get(value, value)

                mapped = self.PROPERTY_MAP.get(basic_property, basic_property)
                transformed[mapped] = value

        # Handle complex properties
        if 'items' in param:
            mapped = self.PROPERTY_MAP.get('items', 'items')
            transformed[mapped] = self._process_param(param['items'])

        if 'properties' in param:
            mapped = self.PROPERTY_MAP.get('properties', 'properties')
            transformed[mapped] = {}

            for key, value in param['properties'].items():
                transformed[mapped][key] = self._process_param(value)

        return transformed

########NEW FILE########
__FILENAME__ = table
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.

# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at

#     http://aws.amazon.com/apache2.0/

# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
import sys
import struct

import colorama
import six


def determine_terminal_width(default_width=80):
    # If we can't detect the terminal width, the default_width is returned.
    try:
        from termios import TIOCGWINSZ
        from fcntl import ioctl
    except ImportError:
        return default_width
    try:
        height, width = struct.unpack('hhhh', ioctl(sys.stdout,
                                                    TIOCGWINSZ, '\000' * 8))[0:2]
    except Exception:
        return default_width
    else:
        return width


def is_a_tty():
    try:
        return os.isatty(sys.stdout.fileno())
    except Exception:
        return False


def center_text(text, length=80, left_edge='|', right_edge='|',
                text_length=None):
    """Center text with specified edge chars.

    You can pass in the length of the text as an arg, otherwise it is computed
    automatically for you.  This can allow you to center a string not based
    on it's literal length (useful if you're using ANSI codes).
    """
    # postcondition: len(returned_text) == length
    if text_length is None:
        text_length = len(text)
    output = []
    char_start = (length // 2) - (text_length // 2) - 1
    output.append(left_edge + ' ' * char_start + text)
    length_so_far = len(left_edge) + char_start + text_length
    right_side_spaces = length - len(right_edge) - length_so_far
    output.append(' ' * right_side_spaces)
    output.append(right_edge)
    final = ''.join(output)
    return final


def align_left(text, length, left_edge='|', right_edge='|', text_length=None,
               left_padding=2):
    """Left align text."""
    # postcondition: len(returned_text) == length
    if text_length is None:
        text_length = len(text)
    computed_length = (
        text_length + left_padding + len(left_edge) + len(right_edge))
    if length - computed_length >= 0:
        padding = left_padding
    else:
        padding = 0
    output = []
    length_so_far = 0
    output.append(left_edge)
    length_so_far += len(left_edge)
    output.append(' ' * padding)
    length_so_far += padding
    output.append(text)
    length_so_far += text_length
    output.append(' ' * (length - length_so_far - len(right_edge)))
    output.append(right_edge)
    return ''.join(output)


def convert_to_vertical_table(sections):
    # Any section that only has a single row is
    # inverted, so:
    # header1 | header2 | header3
    # val1    | val2    | val2
    #
    # becomes:
    #
    # header1 | val1
    # header2 | val2
    # header3 | val3
    for i, section in enumerate(sections):
        if len(section.rows) == 1 and section.headers:
            headers = section.headers
            new_section = Section()
            new_section.title = section.title
            new_section.indent_level = section.indent_level
            for header, element in zip(headers, section.rows[0]):
                new_section.add_row([header, element])
            sections[i] = new_section


class IndentedStream(object):
    def __init__(self, stream, indent_level, left_indent_char='|',
                 right_indent_char='|'):
        self._stream = stream
        self._indent_level = indent_level
        self._left_indent_char = left_indent_char
        self._right_indent_char = right_indent_char

    def write(self, text):
        self._stream.write(self._left_indent_char * self._indent_level)
        if text.endswith('\n'):
            self._stream.write(text[:-1])
            self._stream.write(self._right_indent_char * self._indent_level)
            self._stream.write('\n')
        else:
            self._stream.write(text)

    def __getattr__(self, attr):
        return getattr(self._stream, attr)


class Styler(object):
    def style_title(self, text):
        return text

    def style_header_column(self, text):
        return text

    def style_row_element(self, text):
        return text

    def style_indentation_char(self, text):
        return text


class ColorizedStyler(Styler):
    def __init__(self):
        # autoreset allows us to not have to sent
        # reset sequences for every string.
        colorama.init(autoreset=True)

    def style_title(self, text):
        # Originally bold + underline
        return text
        #return colorama.Style.BOLD + text + colorama.Style.RESET_ALL

    def style_header_column(self, text):
        # Originally underline
        return text

    def style_row_element(self, text):
        return (colorama.Style.BRIGHT + colorama.Fore.BLUE +
                text + colorama.Style.RESET_ALL)

    def style_indentation_char(self, text):
        return (colorama.Style.DIM + colorama.Fore.YELLOW +
                text + colorama.Style.RESET_ALL)


class MultiTable(object):
    def __init__(self, terminal_width=None, initial_section=True,
                 column_separator='|', terminal=None,
                 styler=None, auto_reformat=True):
        self._auto_reformat = auto_reformat
        if initial_section:
            self._current_section = Section()
            self._sections = [self._current_section]
        else:
            self._current_section = None
            self._sections = []
        if styler is None:
            # Move out to factory.
            if is_a_tty():
                self._styler = ColorizedStyler()
            else:
                self._styler = Styler()
        else:
            self._styler = styler
        self._rendering_index = 0
        self._column_separator = column_separator
        if terminal_width is None:
            self._terminal_width = determine_terminal_width()

    def add_title(self, title):
        self._current_section.add_title(title)

    def add_row_header(self, headers):
        self._current_section.add_header(headers)

    def add_row(self, row_elements):
        self._current_section.add_row(row_elements)

    def new_section(self, title, indent_level=0):
        self._current_section = Section()
        self._sections.append(self._current_section)
        self._current_section.add_title(title)
        self._current_section.indent_level = indent_level

    def render(self, stream):
        max_width = self._calculate_max_width()
        should_convert_table = self._determine_conversion_needed(max_width)
        if should_convert_table:
            convert_to_vertical_table(self._sections)
            max_width = self._calculate_max_width()
        stream.write('-' * max_width + '\n')
        for section in self._sections:
            self._render_section(section, max_width, stream)

    def _determine_conversion_needed(self, max_width):
        # If we don't know the width of the controlling terminal,
        # then we don't try to resize the table.
        if max_width > self._terminal_width:
            return self._auto_reformat

    def _calculate_max_width(self):
        max_width = max(s.total_width(padding=4, with_border=True,
                                      outer_padding=s.indent_level)
                        for s in self._sections)
        return max_width

    def _render_section(self, section, max_width, stream):
        stream = IndentedStream(stream, section.indent_level,
                                self._styler.style_indentation_char('|'),
                                self._styler.style_indentation_char('|'))
        max_width -= (section.indent_level * 2)
        self._render_title(section, max_width, stream)
        self._render_column_titles(section, max_width, stream)
        self._render_rows(section, max_width, stream)

    def _render_title(self, section, max_width, stream):
        # The title consists of:
        # title        :  |   This is the title      |
        # bottom_border:  ----------------------------
        if section.title:
            title = self._styler.style_title(section.title)
            stream.write(center_text(title, max_width, '|', '|',
                                     len(section.title)) + '\n')
            if not section.headers and not section.rows:
                stream.write('+%s+' % ('-' * (max_width - 2)) + '\n')

    def _render_column_titles(self, section, max_width, stream):
        if not section.headers:
            return
        # In order to render the column titles we need to know
        # the width of each of the columns.
        widths = section.calculate_column_widths(padding=4,
                                                 max_width=max_width)
        # TODO: Built a list instead of +=, it's more efficient.
        current = ''
        length_so_far = 0
        # The first cell needs both left and right edges '|  foo  |'
        # while subsequent cells only need right edges '  foo  |'.
        first = True
        for width, header in zip(widths, section.headers):
            stylized_header = self._styler.style_header_column(header)
            if first:
                left_edge = '|'
                first = False
            else:
                left_edge = ''
            current += center_text(text=stylized_header, length=width,
                                   left_edge=left_edge, right_edge='|',
                                   text_length=len(header))
            length_so_far += width
        self._write_line_break(stream, widths)
        stream.write(current + '\n')

    def _write_line_break(self, stream, widths):
        # Write out something like:
        # +-------+---------+---------+
        parts = []
        first = True
        for width in widths:
            if first:
                parts.append('+%s+' % ('-' * (width - 2)))
                first = False
            else:
                parts.append('%s+' % ('-' * (width - 1)))
        parts.append('\n')
        stream.write(''.join(parts))

    def _render_rows(self, section, max_width, stream):
        if not section.rows:
            return
        widths = section.calculate_column_widths(padding=4,
                                                 max_width=max_width)
        if not widths:
            return
        self._write_line_break(stream, widths)
        for row in section.rows:
            # TODO: Built the string in a list then join instead of using +=,
            # it's more efficient.
            current = ''
            length_so_far = 0
            first = True
            for width, element in zip(widths, row):
                if first:
                    left_edge = '|'
                    first = False
                else:
                    left_edge = ''
                stylized = self._styler.style_row_element(element)
                current += align_left(text=stylized, length=width,
                                      left_edge=left_edge,
                                      right_edge=self._column_separator,
                                      text_length=len(element))
                length_so_far += width
            stream.write(current + '\n')
        self._write_line_break(stream, widths)


class Section(object):
    def __init__(self):
        self.title = ''
        self.headers = []
        self.rows = []
        self.indent_level = 0
        self._num_cols = None
        self._max_widths = []

    def __repr__(self):
        return ("Section(title=%s, headers=%s, indent_level=%s, num_rows=%s)" %
                (self.title, self.headers, self.indent_level, len(self.rows)))

    def calculate_column_widths(self, padding=0, max_width=None):
        # postcondition: sum(widths) == max_width
        unscaled_widths = [w + padding for w in self._max_widths]
        if max_width is None:
            return unscaled_widths
        if not unscaled_widths:
            return unscaled_widths
        else:
            # Compute scale factor for max_width.
            scale_factor = max_width / float(sum(unscaled_widths))
            scaled = [int(round(scale_factor * w)) for w in unscaled_widths]
            # Once we've scaled the columns, we may be slightly over/under
            # the amount we need so we have to adjust the columns.
            off_by = sum(scaled) - max_width
            while off_by != 0:
                iter_order = range(len(scaled))
                if off_by < 0:
                    iter_order = reversed(iter_order)
                for i in iter_order:
                    if off_by > 0:
                        scaled[i] -= 1
                        off_by -= 1
                    else:
                        scaled[i] += 1
                        off_by += 1
                    if off_by == 0:
                        break
            return scaled

    def total_width(self, padding=0, with_border=False, outer_padding=0):
        total = 0
        # One char on each side == 2 chars total to the width.
        border_padding = 2
        for w in self.calculate_column_widths():
            total += w + padding
        if with_border:
            total += border_padding
        total += outer_padding + outer_padding
        return max(len(self.title) + border_padding + outer_padding +
                   outer_padding, total)

    def add_title(self, title):
        self.title = title

    def add_header(self, headers):
        self._update_max_widths(headers)
        if self._num_cols is None:
            self._num_cols = len(headers)
        self.headers = self._format_headers(headers)

    def _format_headers(self, headers):
        return headers

    def add_row(self, row):
        if self._num_cols is None:
            self._num_cols = len(row)
        if len(row) != self._num_cols:
            raise ValueError("Row should have %s elements, instead "
                             "it has %s" % (self._num_cols, len(row)))
        row = self._format_row(row)
        self.rows.append(row)
        self._update_max_widths(row)

    def _format_row(self, row):
        return [six.text_type(r) for r in row]

    def _update_max_widths(self, row):
        if not self._max_widths:
            self._max_widths = [len(el) for el in row]
        else:
            for i, el in enumerate(row):
                self._max_widths[i] = max(len(el), self._max_widths[i])

########NEW FILE########
__FILENAME__ = testutils
# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""Test utilities for the AWS CLI.

This module includes various classes/functions that help in writing
CLI unit/integration tests.  This module should not be imported by
any module **except** for test code.  This is included in the CLI
package so that code that is not part of the CLI can still take
advantage of all the testing utilities we provide.

"""
import os
import sys
import copy
import shutil
import time
import json
import random
import logging
import tempfile
import platform
import contextlib
from subprocess import Popen, PIPE

try:
    import mock
except ImportError as e:
    # In the off chance something imports this module
    # that's not suppose to, we should not stop the CLI
    # by raising an ImportError.  Now if anything actually
    # *uses* this module that isn't suppose to, that's s
    # different story.
    mock = None
import six
from botocore.hooks import HierarchicalEmitter
from botocore.session import Session
import botocore.loaders
from botocore.vendored import requests

import awscli.clidriver
from awscli.plugin import load_plugins
from awscli.clidriver import CLIDriver
from awscli import EnvironmentVariables


# The unittest module got a significant overhaul
# in 2.7, so if we're in 2.6 we can use the backported
# version unittest2.
if sys.version_info[:2] == (2, 6):
    import unittest2 as unittest
else:
    import unittest


_LOADER = botocore.loaders.Loader()
INTEG_LOG = logging.getLogger('awscli.tests.integration')
AWS_CMD = None


def create_clidriver():
    driver = awscli.clidriver.create_clidriver()
    driver.session._loader = _LOADER
    return driver


def get_aws_cmd():
    global AWS_CMD
    import awscli
    if AWS_CMD is None:
        # Try <repo>/bin/aws
        repo_root = os.path.dirname(os.path.abspath(awscli.__file__))
        aws_cmd = os.path.join(repo_root, 'bin', 'aws')
        if not os.path.isfile(aws_cmd):
            aws_cmd = _search_path_for_cmd('aws')
            if aws_cmd is None:
                raise ValueError('Could not find "aws" executable.  Either '
                                 'make sure it is on your PATH, or you can '
                                 'explicitly set this value using '
                                 '"set_aws_cmd()"')
        AWS_CMD = aws_cmd
    return AWS_CMD


def _search_path_for_cmd(cmd_name):
    for path in os.environ.get('PATH', '').split(os.pathsep):
        full_cmd_path = os.path.join(path, cmd_name)
        if os.path.isfile(full_cmd_path):
            return full_cmd_path
    return None


def set_aws_cmd(aws_cmd):
    global AWS_CMD
    AWS_CMD = aws_cmd


@contextlib.contextmanager
def temporary_file(mode):
    """This is a cross platform temporary file creation.

    tempfile.NamedTemporary file on windows creates a secure temp file
    that can't be read by other processes and can't be opened a second time.

    For tests, we generally *want* them to be read multiple times.
    The test fixture writes the temp file contents, the test reads the
    temp file.

    """
    temporary_directory = tempfile.mkdtemp()
    basename = 'tmpfile-%s-%s' % (int(time.time()), random.randint(1, 1000))
    full_filename = os.path.join(temporary_directory, basename)
    open(full_filename, 'w').close()
    try:
        with open(full_filename, mode) as f:
            yield f
    finally:
        shutil.rmtree(temporary_directory)


class BaseCLIDriverTest(unittest.TestCase):
    """Base unittest that use clidriver.

    This will load all the default plugins as well so it
    will simulate the behavior the user will see.
    """
    def setUp(self):
        self.environ = {
            'AWS_DATA_PATH': os.environ['AWS_DATA_PATH'],
            'AWS_DEFAULT_REGION': 'us-east-1',
            'AWS_ACCESS_KEY_ID': 'access_key',
            'AWS_SECRET_ACCESS_KEY': 'secret_key',
            'AWS_CONFIG_FILE': '',
        }
        self.environ_patch = mock.patch('os.environ', self.environ)
        self.environ_patch.start()
        emitter = HierarchicalEmitter()
        session = Session(EnvironmentVariables, emitter, loader=_LOADER)
        load_plugins({}, event_hooks=emitter)
        driver = CLIDriver(session=session)
        self.session = session
        self.driver = driver

    def tearDown(self):
        self.environ_patch.stop()


class BaseAWSHelpOutputTest(BaseCLIDriverTest):
    def setUp(self):
        super(BaseAWSHelpOutputTest, self).setUp()
        self.renderer_patch = mock.patch('awscli.help.get_renderer')
        self.renderer_mock = self.renderer_patch.start()
        self.renderer = CapturedRenderer()
        self.renderer_mock.return_value = self.renderer

    def tearDown(self):
        super(BaseAWSHelpOutputTest, self).tearDown()
        self.renderer_patch.stop()

    def assert_contains(self, contains):
        if contains not in self.renderer.rendered_contents:
            self.fail("The expected contents:\n%s\nwere not in the "
                      "actual rendered contents:\n%s" % (
                          contains, self.renderer.rendered_contents))

    def assert_not_contains(self, contents):
        if contents in self.renderer.rendered_contents:
            self.fail("The contents:\n%s\nwere not suppose to be in the "
                      "actual rendered contents:\n%s" % (
                          contents, self.renderer.rendered_contents))

    def assert_text_order(self, *args, **kwargs):
        # First we need to find where the SYNOPSIS section starts.
        starting_from = kwargs.pop('starting_from')
        args = list(args)
        contents = self.renderer.rendered_contents
        self.assertIn(starting_from, contents)
        start_index = contents.find(starting_from)
        arg_indices = [contents.find(arg, start_index) for arg in args]
        previous = arg_indices[0]
        for i, index in enumerate(arg_indices[1:], 1):
            if index == -1:
                self.fail('The string %r was not found in the contents: %s'
                          % (args[index], contents))
            if index < previous:
                self.fail('The string %r came before %r, but was suppose to come '
                          'after it.\n%s' % (args[i], args[i - 1], contents))
            previous = index


class CapturedRenderer(object):
    def __init__(self):
        self.rendered_contents = ''

    def render(self, contents):
        self.rendered_contents = contents.decode('utf-8')


class BaseAWSCommandParamsTest(unittest.TestCase):
    maxDiff = None

    def setUp(self):
        self.last_params = {}
        # awscli/__init__.py injects AWS_DATA_PATH at import time
        # so that we can find cli.json.  This might be fixed in the
        # future, but for now we just grab that value out of the real
        # os.environ so the patched os.environ has this data and
        # the CLI works.
        self.environ = {
            'AWS_DATA_PATH': os.environ['AWS_DATA_PATH'],
            'AWS_DEFAULT_REGION': 'us-east-1',
            'AWS_ACCESS_KEY_ID': 'access_key',
            'AWS_SECRET_ACCESS_KEY': 'secret_key',
        }
        self.environ_patch = mock.patch('os.environ', self.environ)
        self.environ_patch.start()
        self.http_response = requests.models.Response()
        self.http_response.status_code = 200
        self.parsed_response = {}
        self.make_request_patch = mock.patch('botocore.endpoint.Endpoint.make_request')
        self.make_request_is_patched = False
        self.operations_called = []
        self.parsed_responses = None
        self.driver = create_clidriver()

    def tearDown(self):
        # This clears all the previous registrations.
        self.environ_patch.stop()
        if self.make_request_is_patched:
            self.make_request_patch.stop()

    def before_call(self, params, **kwargs):
        self._store_params(params)

    def _store_params(self, params):
        self.last_params = params

    def patch_make_request(self):
        make_request_patch = self.make_request_patch.start()
        if self.parsed_responses is not None:
            make_request_patch.side_effect = lambda *args, **kwargs: \
                (self.http_response, self.parsed_responses.pop(0))
        else:
            make_request_patch.return_value = (self.http_response, self.parsed_response)
        self.make_request_is_patched = True

    def assert_params_for_cmd(self, cmd, params=None, expected_rc=0,
                              stderr_contains=None, ignore_params=None):
        stdout, stderr, rc = self.run_cmd(cmd, expected_rc)
        if stderr_contains is not None:
            self.assertIn(stderr_contains, stderr)
        if params is not None:
            last_params = copy.copy(self.last_params)
            if ignore_params is not None:
                for key in ignore_params:
                    try:
                        del last_params[key]
                    except KeyError:
                        pass
            self.assertDictEqual(params, last_params)
        return stdout, stderr, rc

    def before_parameter_build(self, params, operation, **kwargs):
        self.last_kwargs = params
        self.operations_called.append((operation, params))

    def run_cmd(self, cmd, expected_rc=0):
        logging.debug("Calling cmd: %s", cmd)
        self.patch_make_request()
        self.driver.session.register('before-call', self.before_call)
        self.driver.session.register('before-parameter-build',
                                self.before_parameter_build)
        if not isinstance(cmd, list):
            cmdlist = cmd.split()
        else:
            cmdlist = cmd

        captured_stderr = six.StringIO()
        captured_stdout = six.StringIO()
        with mock.patch('sys.stderr', captured_stderr):
            with mock.patch('sys.stdout', captured_stdout):
                rc = self.driver.main(cmdlist)
        stderr = captured_stderr.getvalue()
        stdout = captured_stdout.getvalue()
        self.assertEqual(
            rc, expected_rc,
            "Unexpected rc (expected: %s, actual: %s) for command: %s" % (
                expected_rc, rc, cmd))
        return stdout, stderr, rc


class FileCreator(object):
    def __init__(self):
        self.rootdir = tempfile.mkdtemp()

    def remove_all(self):
        shutil.rmtree(self.rootdir)

    def create_file(self, filename, contents):
        """Creates a file in a tmpdir

        ``filename`` should be a relative path, e.g. "foo/bar/baz.txt"
        It will be translated into a full path in a tmp dir.

        Returns the full path to the file.
        """
        full_path = os.path.join(self.rootdir, filename)
        if not os.path.isdir(os.path.dirname(full_path)):
            os.makedirs(os.path.dirname(full_path))
        with open(full_path, 'w') as f:
            f.write(contents)
        return full_path

    def append_file(self, filename, contents):
        """Append contents to a file

        ``filename`` should be a relative path, e.g. "foo/bar/baz.txt"
        It will be translated into a full path in a tmp dir.

        Returns the full path to the file.
        """
        full_path = os.path.join(self.rootdir, filename)
        if not os.path.isdir(os.path.dirname(full_path)):
            os.makedirs(os.path.dirname(full_path))
        with open(full_path, 'a') as f:
            f.write(contents)
        return full_path

    def full_path(self, filename):
        """Translate relative path to full path in temp dir.

        f.full_path('foo/bar.txt') -> /tmp/asdfasd/foo/bar.txt
        """
        return os.path.join(self.rootdir, filename)


class ProcessTerminatedError(Exception):
    pass


class Result(object):
    def __init__(self, rc, stdout, stderr, memory_usage=None):
        self.rc = rc
        self.stdout = stdout
        self.stderr = stderr
        INTEG_LOG.debug("rc: %s", rc)
        INTEG_LOG.debug("stdout: %s", stdout)
        INTEG_LOG.debug("stderr: %s", stderr)
        if memory_usage is None:
            memory_usage = []
        self.memory_usage = memory_usage

    @property
    def json(self):
        return json.loads(self.stdout)


def _escape_quotes(command):
    # For windows we have different rules for escaping.
    # First, double quotes must be escaped.
    command = command.replace('"', '\\"')
    # Second, single quotes do nothing, to quote a value we need
    # to use double quotes.
    command = command.replace("'", '"')
    return command


def aws(command, collect_memory=False, env_vars=None,
        wait_for_finish=True):
    """Run an aws command.

    This help function abstracts the differences of running the "aws"
    command on different platforms.

    If collect_memory is ``True`` the the Result object will have a list
    of memory usage taken at 2 second intervals.  The memory usage
    will be in bytes.

    If env_vars is None, this will set the environment variables
    to be used by the aws process.

    If wait_for_finish is False, then the Process object is returned
    to the caller.  It is then the caller's responsibility to ensure
    proper cleanup.  This can be useful if you want to test timeout's
    or how the CLI responds to various signals.

    """
    if platform.system() == 'Windows':
        command = _escape_quotes(command)
    if 'AWS_TEST_COMMAND' in os.environ:
        aws_command = os.environ['AWS_TEST_COMMAND']
    else:
        aws_command = 'python %s' % get_aws_cmd()
    full_command = '%s %s' % (aws_command, command)
    stdout_encoding = _get_stdout_encoding()
    if isinstance(full_command, six.text_type) and not six.PY3:
        full_command = full_command.encode(stdout_encoding)
    INTEG_LOG.debug("Running command: %s", full_command)
    env = os.environ.copy()
    env['AWS_DEFAULT_REGION'] = "us-east-1"
    if env_vars is not None:
        env = env_vars
    process = Popen(full_command, stdout=PIPE, stderr=PIPE, shell=True,
                    env=env)
    if not wait_for_finish:
        return process
    memory = None
    if not collect_memory:
        stdout, stderr = process.communicate()
    else:
        stdout, stderr, memory = _wait_and_collect_mem(process)
    return Result(process.returncode,
                  stdout.decode(stdout_encoding),
                  stderr.decode(stdout_encoding),
                  memory)


def _get_stdout_encoding():
    encoding = getattr(sys.__stdout__, 'encoding', None)
    if encoding is None:
        encoding = 'utf-8'
    return encoding


def _wait_and_collect_mem(process):
    # We only know how to collect memory on mac/linux.
    if platform.system() == 'Darwin':
        get_memory = _get_memory_with_ps
    elif platform.system() == 'Linux':
        get_memory = _get_memory_with_ps
    else:
        raise ValueError(
            "Can't collect memory for process on platform %s." %
            platform.system())
    memory = []
    while process.poll() is None:
        try:
            current = get_memory(process.pid)
        except ProcessTerminatedError:
            # It's possible the process terminated between .poll()
            # and get_memory().
            break
        memory.append(current)
    stdout, stderr = process.communicate()
    return stdout, stderr, memory


def _get_memory_with_ps(pid):
    # It's probably possible to do with proc_pidinfo and ctypes on a Mac,
    # but we'll do it the easy way with parsing ps output.
    command_list = 'ps u -p'.split()
    command_list.append(str(pid))
    p = Popen(command_list, stdout=PIPE)
    stdout = p.communicate()[0]
    if not p.returncode == 0:
        raise ProcessTerminatedError(str(pid))
    else:
        # Get the RSS from output that looks like this:
        # USER       PID  %CPU %MEM      VSZ    RSS   TT  STAT STARTED      TIME COMMAND
        # user     47102   0.0  0.1  2437000   4496 s002  S+    7:04PM   0:00.12 python2.6
        return int(stdout.splitlines()[1].split()[5]) * 1024

########NEW FILE########
__FILENAME__ = text
# Copyright 2012-2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.

# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at

#     http://aws.amazon.com/apache2.0/

# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import six


def format_text(data, stream):
    _format_text(data, stream)


def _format_text(item, stream, identifier=None, scalar_keys=None):
    if isinstance(item, dict):
        _format_dict(scalar_keys, item, identifier, stream)
    elif isinstance(item, list):
        _format_list(item, identifier, stream)
    else:
        # If it's not a list or a dict, we just write the scalar
        # value out directly.
        stream.write(item)
        stream.write('\n')


def _format_list(item, identifier, stream):
    if not item:
        return
    if any(isinstance(el, dict) for el in item):
        all_keys = _all_scalar_keys(item)
        for element in item:
            _format_text(element, stream=stream, identifier=identifier,
                         scalar_keys=all_keys)
    elif any(isinstance(el, list) for el in item):
        scalar_elements, non_scalars = _partition_list(item)
        if scalar_elements:
            _format_scalar_list(scalar_elements, identifier, stream)
        for non_scalar in non_scalars:
            _format_text(non_scalar, stream=stream,
                         identifier=identifier)
    else:
        _format_scalar_list(item, identifier, stream)


def _partition_list(item):
    scalars = []
    non_scalars = []
    for element in item:
        if isinstance(element, (list, dict)):
            non_scalars.append(element)
        else:
            scalars.append(element)
    return scalars, non_scalars


def _format_scalar_list(elements, identifier, stream):
    if identifier is not None:
        for item in elements:
            stream.write('%s\t%s\n' % (identifier.upper(),
                                       item))
    else:
        # For a bare list, just print the contents.
        stream.write('\t'.join([six.text_type(item) for item in elements]))
        stream.write('\n')


def _format_dict(scalar_keys, item, identifier, stream):
    scalars, non_scalars = _partition_dict(item, scalar_keys=scalar_keys)
    if scalars:
        if identifier is not None:
            scalars.insert(0, identifier.upper())
        stream.write('\t'.join(scalars))
        stream.write('\n')
    for new_identifier, non_scalar in non_scalars:
        _format_text(item=non_scalar, stream=stream,
                     identifier=new_identifier)


def _all_scalar_keys(list_of_dicts):
    keys_seen = set()
    for item_dict in list_of_dicts:
        for key, value in item_dict.items():
            if not isinstance(value, (dict, list)):
                keys_seen.add(key)
    return list(sorted(keys_seen))


def _partition_dict(item_dict, scalar_keys):
    # Given a dictionary, partition it into two list based on the
    # values associated with the keys.
    # {'foo': 'scalar', 'bar': 'scalar', 'baz': ['not, 'scalar']}
    # scalar = [('foo', 'scalar'), ('bar', 'scalar')]
    # non_scalar = [('baz', ['not', 'scalar'])]
    scalar = []
    non_scalar = []
    if scalar_keys is None:
        # scalar_keys can have more than just the keys in the item_dict,
        # but if user does not provide scalar_keys, we'll grab the keys
        # from the current item_dict
        for key, value in sorted(item_dict.items()):
            if isinstance(value, (dict, list)):
                non_scalar.append((key, value))
            else:
                scalar.append(six.text_type(value))
    else:
        for key in scalar_keys:
            scalar.append(six.text_type(item_dict.get(key, '')))
        remaining_keys = sorted(set(item_dict.keys()) - set(scalar_keys))
        for remaining_key in remaining_keys:
            non_scalar.append((remaining_key, item_dict[remaining_key]))
    return scalar, non_scalar

########NEW FILE########
__FILENAME__ = utils
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import csv

import six


def split_on_commas(value):
    if not any(char in value for char in ['"', '\\', "'", ']', '[']):
        # No quotes or escaping, just use a simple split.
        return value.split(',')
    elif not any(char in value for char in ['"', "'", '[', ']']):
        # Simple escaping, let the csv module handle it.
        return list(csv.reader(six.StringIO(value), escapechar='\\'))[0]
    else:
        # If there's quotes for the values, we have to handle this
        # ourselves.
        return _split_with_quotes(value)


def _split_with_quotes(value):
    try:
        parts = list(csv.reader(six.StringIO(value), escapechar='\\'))[0]
    except csv.Error:
        raise ValueError("Bad csv value: %s" % value)
    iter_parts = iter(parts)
    new_parts = []
    for part in iter_parts:
        # Find the first quote
        quote_char = _find_quote_char_in_part(part)

        # Find an opening list bracket
        list_start = part.find('=[')

        if list_start >= 0 and value.find(']') != -1 and \
           (quote_char is None or part.find(quote_char) > list_start):
            # This is a list, eat all the items until the end
            new_chunk = _eat_items(value, iter_parts, part, ']')
            list_items = _split_with_quotes(new_chunk[list_start + 2:-1])
            new_chunk = new_chunk[:list_start + 1] + ','.join(list_items)
            new_parts.append(new_chunk)
            continue
        elif quote_char is None:
            new_parts.append(part)
            continue
        elif part.count(quote_char) == 2:
            # Starting and ending quote are in this part.
            # While it's not needed right now, this will
            # break down if we ever need to escape quotes while
            # quoting a value.
            new_parts.append(part.replace(quote_char, ''))
            continue
        # Now that we've found a starting quote char, we
        # need to combine the parts until we encounter an end quote.
        new_chunk = _eat_items(value, iter_parts, part, quote_char, quote_char)
        new_parts.append(new_chunk)
    return new_parts


def _eat_items(value, iter_parts, part, end_char, replace_char=''):
    """
    Eat items from an iterator, optionally replacing characters with
    a blank and stopping when the end_char has been reached.
    """
    current = part
    chunks = [current.replace(replace_char, '')]
    while True:
        try:
            current = six.advance_iterator(iter_parts)
        except StopIteration:
            raise ValueError(value)
        chunks.append(current.replace(replace_char, ''))
        if end_char in current:
            break
    return ','.join(chunks)


def _find_quote_char_in_part(part):
    if '"' not in part and "'" not in part:
        return
    quote_char = None
    double_quote = part.find('"')
    single_quote = part.find("'")
    if double_quote >= 0 and single_quote == -1:
        quote_char = '"'
    elif single_quote >= 0 and double_quote == -1:
        quote_char = "'"
    elif double_quote < single_quote:
        quote_char = '"'
    elif single_quote < double_quote:
        quote_char = "'"
    return quote_char

########NEW FILE########
__FILENAME__ = bootstrapdocs
import json
import os
import subprocess
import sys

RST_GENERATION_SCRIPT = 'htmlgen'
script_path = os.path.join(os.path.dirname(__file__),
                           RST_GENERATION_SCRIPT)
os.environ['PATH'] += ':.'
rc = subprocess.call("python "+ script_path, shell=True, env=os.environ)
if rc != 0:
    sys.stderr.write("Failed to generate documentation!\n")
    sys.exit(2)

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# aws-cli documentation build configuration file, created by
# sphinx-quickstart on Fri Feb  1 12:57:38 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('.'))

import bootstrapdocs

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'AWS CLI'
copyright = u'2013, Amazon Web Services'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.3.'
# The full version, including alpha/beta/rc tags.
release = '1.3.13'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:

# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['examples']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'guzzle_sphinx_theme.GuzzleStyle'
#pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#html_theme = 'pyramid'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
# html_theme_options = {
#     'bodyfont': '"Andale Mono", Courier, monospace;'
#     }

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = ['.']

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
html_sidebars = {
  '**': ['sidebarlogo.html', 'localtoc.html', 'searchbox.html', 'feedback.html']
}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'aws-clidoc'

# Adds an HTML table visitor to apply Bootstrap table classes
html_translator_class = 'guzzle_sphinx_theme.HTMLTranslator'
p = os.path.abspath('.')
p = os.path.join(p, 'guzzle_sphinx_theme')
html_theme_path = [p]
html_theme = 'guzzle_sphinx_theme'
# Register the theme as an extension to generate a sitemap.xml
extensions.append("guzzle_sphinx_theme")

html_theme_options = {
    # Set the name of the project to appear in the nav menu
    "project_nav_name": "AWS CLI",
    # Set your GitHub user and repo to enable GitHub stars links
    "github_user": "aws",
    "github_repo": "aws-cli",
    # Set to true to bind left and right key events to turn the page
    "bind_key_events": False,
    # Specify a base_url used to generate sitemap.xml links. If not
    # specified, then no sitemap will be built.
    "base_url": "http://docs.aws.amazon.com/cli/latest/",
}



# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'aws-cli.tex', u'AWS CLI Documentation',
   u'Amazon Web Services', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).

man_pages = [('reference/index', 'aws', 'The main command', '', 1),
             ('reference/autoscaling/index', 'Auto Scaling',
              'The autoscaling service', '', 1),
             ('reference/cloudformation/index', 'aws-cloudformation',
              'AWS CloudFormation', '', 1),
             ('reference/cloudwatch/index', 'aws-cloudwatch',
              'Amazon CloudWatch', '', 1),
             ('reference/datapipeline/index', 'aws-datapipeline',
              'AWS Data Pipeline', '', 1),
             ('reference/directconnect/index', 'aws-directconnect',
              'AWS Direct Connect', '', 1),
             ('reference/dynamodb/index', 'aws-dynamodb',
              'Amazon DynamoDB', '', 1),
             ('reference/ec2/index', 'aws-ec2',
              'Amazon Elastic Compute Cloud', '', 1),
             ('reference/elasticache/index', 'aws-elasticache',
              'Amazon ElastiCache', '', 1),
             ('reference/elasticbeanstalk/index', 'aws-elasticbeanstalk',
              'AWS Elastic Beanstalk', '', 1),
             ('reference/elastictranscoder/index', 'aws-elastictranscoder',
              'Amazon Elastic Transcoder', '', 1),
             ('reference/elb/index', 'aws-elb',
              'Elastic Load Balancing', '', 1),
             ('reference/emr/index', 'aws-emr',
              'Amazon Elastic MapReduce', '', 1),
             ('reference/iam/index', 'aws-iam',
              'AWS Identity and Access Management', '', 1),
             ('reference/importexport/index', 'aws-importexport',
              'AWS Import/Export', '', 1),
             ('reference/opsworks/index', 'aws-opsworks',
              'AWS OpsWorks', '', 1),
             ('reference/rds/index', 'aws-rds',
              'Amazon Relational Database Service', '', 1),
             ('reference/redshift/index', 'aws-redshift',
              'Amazon Redshift', '', 1),
             ('reference/route53/index', 'aws-route53',
              'Amazon Route 53', '', 1),
             ('reference/s3/index', 'aws-s3',
              'Amazon Simple Storage Service', '', 1),
             ('reference/ses/index', 'aws-ses',
              'Amazon Simple Email Service', '', 1),
             ('reference/sns/index', 'aws-sns',
              'Amazon Simple Notification Service', '', 1),
             ('reference/sqs/index', 'aws-sqs',
              'Amazon Simple Queue Service', '', 1),
             ('reference/storagegateway/index', 'aws-storagegateway',
              'AWS Storage Gateway', '', 1),
             ('reference/sts/index', 'aws-sts',
              'AWS Security Token Service', '', 1),
             ('reference/support/index', 'aws-support',
              'AWS Support', '', 1),
             ('reference/swf/index', 'aws-swf',
              'Amazon Simple Workflow Service', '', 1),
             ]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = []


# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = modify_operation
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
Modify an existing option.

This one changes the behavior of the ``--bucket`` option to create the
specified bucket if it does not already exist.

TODO
In this case we are adding to the existing documentation.  But what if
we wanted to replace it entirely?  Or modify what was already provided?
We don't have a way to handle that right now.

One option would be to try to handle it at the event level so that we
could somehow specify when registering the handler that this handler
should be fired instead of any other registered handlers.

Another option would be to somehow pass the content generated for a
particular event through all handlers registered for that event.  A
handler could then remove previously generated content or modify it
although the modification would be tedious since it would have to
search through the current text and make changes.  This could break
easily when the doc strings in the JSON model change.
"""

def awscli_initialize(cli):
    cli.register('before-call.elastictranscoder.create-pipeline',
                 handler=create_bucket)
    cli.register('doc-option.Operation.create-pipeline.output-bucket',
                 handler=doc_handler)

def create_bucket(session, **kwargs):
    # We need the S3 service object
    s3 = session.get_service('s3')
    # And we need an endpoint.  We arent' specifying a region so
    # we will get the user's default region for this profile.
    endpoint = s3.get_endpoint()
    # And finally, we need the list-objects operation.
    operation = s3.get_operation('list-objects')
    http_response, data = operation.call(endpoint, bucket=opts.output_bucket,
                                         maxkeys=0)
    if http_response.status_code == 404:
        # The bucket does not exist, try to create it.
        operation = s3.get_operation('create-bucket')
        http_response, data = operation.call(endpoint,
                                             bucket=opts.output_bucket)
        if http_response.status_code != 200:
            # TODO - what should we do in the case of errors?
            pass


def doc_handler(arg_name, help_command, **kwargs):
    """
    This handler will get called when we are documenting the
    ``output-bucket`` option of the ``create-pipeline`` command.

    The handler is passed a ``help_command`` object which contains
    state that was passed from the CLI to this handler.  In the
    `help_command``, you will find a ``doc`` attribute which contains
    the actual document object that is being used to collect the
    documentation.

    For now, this ``document`` object will always be a ``ReSTDocument``
    but other formats are possible.  How would this code handle that?
    We could query the format of the document we are passed and try to
    change what we output to match that format but hopefully the doc
    object will have the right abstractions available so we can just
    do things like ``start_note`` and the right thing will happen for
    that type of document.
    """
    doc = help_command.doc
    doc.style.new_paragraph()
    doc.style.start_note()
    doc.writeln('If the bucket does not exist, it will be created')
    doc.style.end_note()

########NEW FILE########
__FILENAME__ = new_command
"""
Add a new top-level command
---------------------------

This example adds a new top-level command called config.  We are going
to assume that the config command will have the following subcommands::

    aws config help
    aws config foo
    aws config bar

TODO
This is still more work than it should have to be.  We need to find ways
to simplify this process and more of the details in awscli rather than in
the plugin.
"""
from awscli.clidriver import BuiltInCommand
from awscli.help import HelpCommand
from awscli.argparser import ServiceArgParser
from awscli.clidocs import CLIDocumentEventHandler


class ConfigDocumentEventHandler(CLIDocumentEventHandler):
    """
    This class implements handlers for document events.
    """

    def doc_title(self, help_command, **kwargs):
        doc = help_command.doc
        doc.style.h1(help_command.name)

    def doc_description(self, help_command, **kwargs):
        doc = help_command.doc
        config = help_command.obj
        doc.style.h2('Description')
        doc.include_doc_string(config.documentation)


class ConfigHelpCommand(HelpCommand):
    """
    A wrapper to handle the interactions between our config command
    and the documentation pipeline.  The two things the HelpCommand
    must do are:

      + Specify a value for EventHandlerClass which is the class
          that will be instantiated by the HelpCommand to register
          and handle document events.
      + Provide implementations of the ``event_class`` and ``name``
        property getters.
    """

    EventHandlerClass = ConfigDocumentEventHandler
    
    @property
    def event_class(self):
        return 'Config'
    
    @property
    def name(self):
        return 'config'

    
class ConfigCommand(BuiltInCommand):

    documentation = "Edit the AWSCLI config file"

    def do_foo(self, remaining_args, parsed_globals):
        print('do foo now')

    def do_bar(self, remaining_args, parsed_globals):
        print('do bar now')

    def __call__(self, args, parsed_globals):
        op_table = self._create_operations_table()
        command_parser = self._create_service_parser(op_table)
        parsed_args, remaining = command_parser.parse_known_args(args)
        return op_table[parsed_args.operation](remaining, parsed_globals)

    def _create_service_parser(self, operation_table):
        # TODO
        # Kind of kludgy that we have to use ServiceArgParser here.
        return ServiceArgParser(
            operations_table=operation_table, service_name=self.name)

    def _create_operations_table(self):
        op_table = {}
        op_table['foo'] = self.do_foo
        op_table['bar'] = self.do_bar
        op_table['help'] = ConfigHelpCommand(self.session, self,
                                             command_table=op_table,
                                             arg_table=None)
        return op_table
                                                     
def add_command(command_table, session, **kwargs):
    """
    This is our event handler.  It will get called as the top-level
    commands are getting built.  We can add our custom commands to
    the ``command_table``.  To do this, we have to add a callable
    to the ``command_table`` that will get called when our command
    is specified on the command line.
    """
    command_table['config'] = ConfigCommand('config', session)


def awscli_initialize(cli):
    cli.register('building-command-table', handler=add_command)

########NEW FILE########
__FILENAME__ = new_option
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""
Add a new top-level option
--------------------------

This example adds a new top-level option called ``--foobar``.
"""
from awscli.clidriver import BuiltInArgument

# TODO
# Not sure that we want people to have to understand and insert
# markup.  It's probably best to remove the markup from the options
# in cli.json and then change the doc code to do the right formatting.
HELP = "<p>This will enable the dreaded foobar mode.</p>"


def add_arg(argument_table, **kwargs):
    """
    This is our event handler.  It will get called as the top-level
    parameters are getting built.  We can add our custom options to
    the ``argument_table``.  The easiest way to do this is to just
    create an instance of the ``BuiltInArgument`` class defined in
    ``clidriver.py``.
    """
    # When we create the new BuiltInArgument, we pass in it's name
    # and, optionally, any additional parameters we want to pass to
    # argparse when this argument is added to the parser.  Since we
    # want this to be a boolean option, we do need to do some extra
    # configuration.
    # Also, if you want to add a help string that gets printed
    # when the user does ``aws help`` you should add that to the
    # dictionary, too.
    argument = BuiltInArgument('foobar', {'action': 'store_true',
                                          'help': HELP})
    argument.add_to_arg_table(argument_table)


def do_foobar(parsed_args, **kwargs):
    # Should this be called with something more than just the
    # parsed_args?  How about a session or a CLIDriver object?
    if parsed_args.foobar:
        print('Enable foobar mode here')


def awscli_initialize(cli):
    cli.register('building-top-level-params', handler=add_arg)
    cli.register('top-level-args-parsed', handler=do_foobar)

########NEW FILE########
__FILENAME__ = test_filegenerator
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.


# Note that all of these functions can be found in the unit tests.
# The only difference is that these tests use botocore's actual session
# variables to communicate with s3 as these are integration tests.  Therefore,
# only tests that use sessions are included as integration tests.

import unittest
import os

import botocore.session
from awscli import EnvironmentVariables
from awscli.customizations.s3.filegenerator import FileGenerator
from awscli.customizations.s3.fileinfo import FileInfo
from tests.unit.customizations.s3 import make_s3_files, s3_cleanup, \
    compare_files


class S3FileGeneratorIntTest(unittest.TestCase):
    def setUp(self):
        self.session = botocore.session.get_session(EnvironmentVariables)
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        self.bucket = make_s3_files(self.session)
        self.file1 = self.bucket + '/' + 'text1.txt'
        self.file2 = self.bucket + '/' + 'another_directory/text2.txt'

    def tearDown(self):
        s3_cleanup(self.bucket, self.session)

    def test_s3_file(self):
        #
        # Generate a single s3 file
        # Note: Size and last update are not tested because s3 generates them.
        #
        input_s3_file = {'src': {'path': self.file1, 'type': 's3'},
                         'dest': {'path': 'text1.txt', 'type': 'local'},
                         'dir_op': False, 'use_src_name': False}
        params = {'region': 'us-east-1'}
        expected_file_size = 15
        result_list = list(
            FileGenerator(self.service, self.endpoint, '', params).call(
                input_s3_file))
        file_info = FileInfo(src=self.file1, dest='text1.txt',
                             compare_key='text1.txt',
                             size=expected_file_size,
                             last_update=result_list[0].last_update,
                             src_type='s3',
                             dest_type='local', operation_name='')

        expected_list = [file_info]
        self.assertEqual(len(result_list), 1)
        compare_files(self, result_list[0], expected_list[0])

    def test_s3_directory(self):
        #
        # Generates s3 files under a common prefix. Also it ensures that
        # zero size files are ignored.
        # Note: Size and last update are not tested because s3 generates them.
        #
        input_s3_file = {'src': {'path': self.bucket+'/', 'type': 's3'},
                         'dest': {'path': '', 'type': 'local'},
                         'dir_op': True, 'use_src_name': True}
        params = {'region': 'us-east-1'}
        result_list = list(
            FileGenerator(self.service, self.endpoint, '', params).call(
                input_s3_file))
        file_info = FileInfo(src=self.file2,
                             dest='another_directory' + os.sep + 'text2.txt',
                             compare_key='another_directory/text2.txt',
                             size=21,
                             last_update=result_list[0].last_update,
                             src_type='s3',
                             dest_type='local', operation_name='')
        file_info2 = FileInfo(src=self.file1,
                              dest='text1.txt',
                              compare_key='text1.txt',
                              size=15,
                              last_update=result_list[1].last_update,
                              src_type='s3',
                              dest_type='local', operation_name='')

        expected_result = [file_info, file_info2]
        self.assertEqual(len(result_list), 2)
        compare_files(self, result_list[0], expected_result[0])
        compare_files(self, result_list[1], expected_result[1])

    def test_s3_delete_directory(self):
        #
        # Generates s3 files under a common prefix. Also it ensures that
        # the directory itself is included because it is a delete command
        # Note: Size and last update are not tested because s3 generates them.
        #
        input_s3_file = {'src': {'path': self.bucket+'/', 'type': 's3'},
                         'dest': {'path': '', 'type': 'local'},
                         'dir_op': True, 'use_src_name': True}
        params = {'region': 'us-east-1'}
        result_list = list(
            FileGenerator(self.service, self.endpoint,
                          'delete', params).call(
                input_s3_file))

        file_info1 = FileInfo(
            src=self.bucket + '/another_directory/',
            dest='another_directory' + os.sep,
            compare_key='another_directory/',
            size=0,
            last_update=result_list[0].last_update,
            src_type='s3',
            dest_type='local', operation_name='delete',
            service=self.service, endpoint=self.endpoint)
        file_info2 = FileInfo(
            src=self.file2,
            dest='another_directory' + os.sep + 'text2.txt',
            compare_key='another_directory/text2.txt',
            size=21,
            last_update=result_list[1].last_update,
            src_type='s3',
            dest_type='local', operation_name='delete',
            service=self.service,
            endpoint=self.endpoint)
        file_info3 = FileInfo(
            src=self.file1,
            dest='text1.txt',
            compare_key='text1.txt',
            size=15,
            last_update=result_list[2].last_update,
            src_type='s3',
            dest_type='local', operation_name='delete',
            service=self.service,
            endpoint=self.endpoint)

        expected_list = [file_info1, file_info2, file_info3]
        self.assertEqual(len(result_list), 3)
        compare_files(self, result_list[0], expected_list[0])
        compare_files(self, result_list[1], expected_list[1])
        compare_files(self, result_list[2], expected_list[2])


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_plugin
# -*- coding: utf-8 -*-
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

# The following tests are performed to ensure that the commands work.
# It does not check every possible parameter that can be thrown as
# those are checked by tests in other classes
import os
import random
import platform
import contextlib
import time
import signal

import botocore.session
import six

from awscli.testutils import unittest, aws, FileCreator
from tests.unit.customizations.s3 import create_bucket as _create_bucket
from awscli.customizations.s3 import constants


@contextlib.contextmanager
def cd(directory):
    original = os.getcwd()
    try:
        os.chdir(directory)
        yield
    finally:
        os.chdir(original)


class BaseS3CLICommand(unittest.TestCase):
    """Base class for aws s3 command.

    This contains convenience functions to make writing these tests easier
    and more streamlined.

    """
    def setUp(self):
        self.files = FileCreator()
        self.session = botocore.session.get_session()
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        self.extra_setup()

    def extra_setup(self):
        # Subclasses can use this to define extra setup steps.
        pass

    def tearDown(self):
        self.files.remove_all()
        self.extra_teardown()

    def extra_teardown(self):
        # Subclasses can use this to define extra teardown steps.
        pass

    def assert_key_contents_equal(self, bucket, key, expected_contents):
        if isinstance(expected_contents, six.BytesIO):
            expected_contents = expected_contents.getvalue().decode('utf-8')
        actual_contents = self.get_key_contents(bucket, key)
        # The contents can be huge so we try to give helpful error messages
        # without necessarily printing the actual contents.
        self.assertEqual(len(actual_contents), len(expected_contents))
        if actual_contents != expected_contents:
            self.fail("Contents for %s/%s do not match (but they "
                      "have the same length)" % (bucket, key))

    def create_bucket(self):
        bucket_name = _create_bucket(self.session)
        self.addCleanup(self.delete_bucket, bucket_name)
        return bucket_name

    def put_object(self, bucket_name, key_name, contents=''):
        operation = self.service.get_operation('PutObject')
        http = operation.call(self.endpoint, bucket=bucket_name,
                              key=key_name, body=contents)[0]
        self.assertEqual(http.status_code, 200)
        self.addCleanup(self.delete_key, bucket_name, key_name)

    def delete_bucket(self, bucket_name):
        self.remove_all_objects(bucket_name)
        operation = self.service.get_operation('DeleteBucket')
        response = operation.call(self.endpoint, bucket=bucket_name)[0]
        self.assertEqual(response.status_code, 204, response.content)

    def remove_all_objects(self, bucket_name):
        operation = self.service.get_operation('ListObjects')
        pages = operation.paginate(self.endpoint, bucket=bucket_name)
        parsed = pages.build_full_result()
        key_names = [obj['Key'] for obj in parsed['Contents']]
        for key_name in key_names:
            self.delete_key(bucket_name, key_name)

    def delete_key(self, bucket_name, key_name):
        operation = self.service.get_operation('DeleteObject')
        response = operation.call(self.endpoint, bucket=bucket_name,
                                  key=key_name)[0]
        self.assertEqual(response.status_code, 204)

    def get_key_contents(self, bucket_name, key_name):
        operation = self.service.get_operation('GetObject')
        http, parsed = operation.call(
            self.endpoint, bucket=bucket_name, key=key_name)
        self.assertEqual(http.status_code, 200)
        return parsed['Body'].read().decode('utf-8')

    def key_exists(self, bucket_name, key_name):
        operation = self.service.get_operation('HeadObject')
        http, parsed = operation.call(
            self.endpoint, bucket=bucket_name, key=key_name)
        return http.status_code == 200

    def list_buckets(self):
        operation = self.service.get_operation('ListBuckets')
        http, parsed = operation.call(self.endpoint)
        self.assertEqual(http.status_code, 200)
        return parsed['Buckets']

    def content_type_for_key(self, bucket_name, key_name):
        operation = self.service.get_operation('HeadObject')
        http, parsed = operation.call(
            self.endpoint, bucket=bucket_name, key=key_name)
        self.assertEqual(http.status_code, 200)
        return parsed['ContentType']

    def assert_no_errors(self, p):
        self.assertEqual(
            p.rc, 0,
            "Non zero rc (%s) received: %s" % (p.rc, p.stdout + p.stderr))
        self.assertNotIn("Error:", p.stdout)
        self.assertNotIn("failed:", p.stdout)
        self.assertNotIn("client error", p.stdout)
        self.assertNotIn("server error", p.stdout)


class TestMoveCommand(BaseS3CLICommand):

    def test_mv_local_to_s3(self):
        bucket_name = self.create_bucket()
        full_path = self.files.create_file('foo.txt', 'this is foo.txt')
        p = aws('s3 mv %s s3://%s/foo.txt' % (full_path,
                                              bucket_name))
        self.assert_no_errors(p)
        # When we move an object, the local file is gone:
        self.assertTrue(not os.path.exists(full_path))
        # And now resides in s3.
        self.assert_key_contents_equal(bucket_name, 'foo.txt', 'this is foo.txt')

    def test_mv_s3_to_local(self):
        bucket_name = self.create_bucket()
        self.put_object(bucket_name, 'foo.txt', 'this is foo.txt')
        full_path = self.files.full_path('foo.txt')
        self.assertTrue(self.key_exists(bucket_name, key_name='foo.txt'))
        p = aws('s3 mv s3://%s/foo.txt %s' % (bucket_name, full_path))
        self.assert_no_errors(p)
        self.assertTrue(os.path.exists(full_path))
        with open(full_path, 'r') as f:
            self.assertEqual(f.read(), 'this is foo.txt')
        # The s3 file should not be there anymore.
        self.assertTrue(not self.key_exists(bucket_name, key_name='foo.txt'))

    def test_mv_s3_to_s3(self):
        from_bucket = self.create_bucket()
        to_bucket = self.create_bucket()
        self.put_object(from_bucket, 'foo.txt', 'this is foo.txt')

        p = aws('s3 mv s3://%s/foo.txt s3://%s/foo.txt' % (from_bucket,
                                                           to_bucket))
        self.assert_no_errors(p)
        contents = self.get_key_contents(to_bucket, 'foo.txt')
        self.assertEqual(contents, 'this is foo.txt')
        # And verify that the object no longer exists in the from_bucket.
        self.assertTrue(not self.key_exists(from_bucket, key_name='foo.txt'))

    def test_mv_s3_to_s3_multipart(self):
        from_bucket = self.create_bucket()
        to_bucket = self.create_bucket()
        file_contents = six.BytesIO(b'abcd' * (1024 * 1024 * 10))
        self.put_object(from_bucket, 'foo.txt', file_contents)

        p = aws('s3 mv s3://%s/foo.txt s3://%s/foo.txt' % (from_bucket,
                                                           to_bucket))
        self.assert_no_errors(p)
        self.assert_key_contents_equal(to_bucket, 'foo.txt', file_contents)
        # And verify that the object no longer exists in the from_bucket.
        self.assertTrue(not self.key_exists(from_bucket, key_name='foo.txt'))

    def test_mv_s3_to_s3_multipart_recursive(self):
        from_bucket = self.create_bucket()
        to_bucket = self.create_bucket()

        large_file_contents = six.BytesIO(b'abcd' * (1024 * 1024 * 10))
        small_file_contents = 'small file contents'
        self.put_object(from_bucket, 'largefile', large_file_contents)
        self.put_object(from_bucket, 'smallfile', small_file_contents)

        p = aws('s3 mv s3://%s/ s3://%s/ --recursive' % (from_bucket,
                                                         to_bucket))
        self.assert_no_errors(p)
        # Nothing's in the from_bucket.
        self.assertTrue(not self.key_exists(from_bucket, key_name='largefile'))
        self.assertTrue(not self.key_exists(from_bucket, key_name='smallfile'))

        # And both files are in the to_bucket.
        self.assertTrue(self.key_exists(to_bucket, key_name='largefile'))
        self.assertTrue(self.key_exists(to_bucket, key_name='smallfile'))

        # And the contents are what we expect.
        self.assert_key_contents_equal(to_bucket, 'smallfile',
                                       small_file_contents)
        self.assert_key_contents_equal(to_bucket, 'largefile',
                                       large_file_contents)

    def test_mv_with_large_file(self):
        bucket_name = self.create_bucket()
        # 40MB will force a multipart upload.
        file_contents = six.BytesIO(b'abcd' * (1024 * 1024 * 10))
        foo_txt = self.files.create_file(
            'foo.txt', file_contents.getvalue().decode('utf-8'))
        p = aws('s3 mv %s s3://%s/foo.txt' % (foo_txt, bucket_name))
        self.assert_no_errors(p)
        # When we move an object, the local file is gone:
        self.assertTrue(not os.path.exists(foo_txt))
        # And now resides in s3.
        self.assert_key_contents_equal(bucket_name, 'foo.txt', file_contents)

        # Now verify we can download this file.
        p = aws('s3 mv s3://%s/foo.txt %s' % (bucket_name, foo_txt))
        self.assert_no_errors(p)
        self.assertTrue(os.path.exists(foo_txt))
        self.assertEqual(os.path.getsize(foo_txt), len(file_contents.getvalue()))

    def test_mv_to_nonexistent_bucket(self):
        full_path = self.files.create_file('foo.txt', 'this is foo.txt')
        p = aws('s3 mv %s s3://bad-noexist-13143242/foo.txt' % (full_path,))
        self.assertEqual(p.rc, 1)


class TestRm(BaseS3CLICommand):
    @unittest.skipIf(platform.system() not in ['Darwin', 'Linux'],
                    'Newline in filename test not valid on windows.')
    # Windows won't let you do this.  You'll get:
    # [Errno 22] invalid mode ('w') or filename: # 'c:\\windows\\temp\\tmp0fv8uu\\foo\r.txt'
    def test_rm_with_newlines(self):
        bucket_name = self.create_bucket()

        # Note the carriage return in the key name.
        foo_txt = self.files.create_file('foo\r.txt', 'this is foo.txt')
        p = aws('s3 cp %s s3://%s/foo\r.txt' % (foo_txt, bucket_name))
        self.assert_no_errors(p)

        # Make sure object is in bucket.
        self.assertTrue(self.key_exists(bucket_name, key_name='foo\r.txt'))

        # Then delete the file.
        p = aws('s3 rm s3://%s/ --recursive' % (bucket_name,))

        # And verify it's gone.
        self.assertFalse(self.key_exists(bucket_name, key_name='foo\r.txt'))


class TestCp(BaseS3CLICommand):

    def test_cp_to_and_from_s3(self):
        # This tests the ability to put a single file in s3
        # move it to a different bucket.
        # and download the file locally
        bucket_name = self.create_bucket()

        # copy file into bucket.
        foo_txt = self.files.create_file('foo.txt', 'this is foo.txt')
        p = aws('s3 cp %s s3://%s/foo.txt' % (foo_txt, bucket_name))
        self.assert_no_errors(p)

        # Make sure object is in bucket.
        self.assertTrue(self.key_exists(bucket_name, key_name='foo.txt'))
        self.assertEqual(
            self.get_key_contents(bucket_name, key_name='foo.txt'),
            'this is foo.txt')

        self.assertEqual(
            self.content_type_for_key(bucket_name, key_name='foo.txt'),
            'text/plain')

        # Make a new name for the file and copy it locally.
        full_path = self.files.full_path('bar.txt')
        p = aws('s3 cp s3://%s/foo.txt %s' % (bucket_name, full_path))
        self.assert_no_errors(p)

        with open(full_path, 'r') as f:
            self.assertEqual(f.read(), 'this is foo.txt')

    def test_cp_without_trailing_slash(self):
        # There's a unit test for this, but we still want to verify this
        # with an integration test.
        bucket_name = self.create_bucket()

        # copy file into bucket.
        foo_txt = self.files.create_file('foo.txt', 'this is foo.txt')
        # Note that the destination has no trailing slash.
        p = aws('s3 cp %s s3://%s' % (foo_txt, bucket_name))
        self.assert_no_errors(p)

        # Make sure object is in bucket.
        self.assertTrue(self.key_exists(bucket_name, key_name='foo.txt'))
        self.assertEqual(
            self.get_key_contents(bucket_name, key_name='foo.txt'),
            'this is foo.txt')

    def test_cp_s3_s3_multipart(self):
        from_bucket = self.create_bucket()
        to_bucket = self.create_bucket()
        file_contents = six.BytesIO(b'abcd' * (1024 * 1024 * 10))
        self.put_object(from_bucket, 'foo.txt', file_contents)

        p = aws('s3 cp s3://%s/foo.txt s3://%s/foo.txt' % (from_bucket, to_bucket))
        self.assert_no_errors(p)
        self.assert_key_contents_equal(to_bucket, 'foo.txt', file_contents)
        self.assertTrue(self.key_exists(from_bucket, key_name='foo.txt'))

    def test_guess_mime_type(self):
        bucket_name = self.create_bucket()
        bar_png = self.files.create_file('bar.jpeg', 'fake png image')
        p = aws('s3 cp %s s3://%s/bar.jpeg' % (bar_png, bucket_name))
        self.assert_no_errors(p)

        # We should have correctly guessed the content type based on the
        # filename extension.
        self.assertEqual(
            self.content_type_for_key(bucket_name, key_name='bar.jpeg'),
            'image/jpeg')

    def test_download_large_file(self):
        # This will force a multipart download.
        bucket_name = self.create_bucket()
        foo_contents = six.BytesIO(b'abcd' * (1024 * 1024 * 10))
        self.put_object(bucket_name, key_name='foo.txt', contents=foo_contents)
        local_foo_txt = self.files.full_path('foo.txt')
        p = aws('s3 cp s3://%s/foo.txt %s' % (bucket_name, local_foo_txt))
        self.assert_no_errors(p)
        self.assertEqual(os.path.getsize(local_foo_txt),
                         len(foo_contents.getvalue()))

    @unittest.skipIf(platform.system() not in ['Darwin', 'Linux'],
                    'SIGINT not supported on Windows.')
    def test_download_ctrl_c_does_not_hang(self):
        bucket_name = self.create_bucket()
        foo_contents = six.BytesIO(b'abcd' * (1024 * 1024 * 20))
        self.put_object(bucket_name, key_name='foo.txt', contents=foo_contents)
        local_foo_txt = self.files.full_path('foo.txt')
        process = aws('s3 cp s3://%s/foo.txt %s' % (bucket_name, local_foo_txt), wait_for_finish=False)
        # Give it some time to start up and enter it's main task loop.
        time.sleep(1)
        # The process has 30 seconds to finish after being sent a Ctrl+C,
        # otherwise the test fails.
        process.send_signal(signal.SIGINT)
        deadline = time.time() + 30
        while time.time() < deadline:
            rc = process.poll()
            if rc is not None:
                break
        else:
            process.kill()
            self.fail("CLI did not exist within 30 seconds of receiving a Ctrl+C")
        # A Ctrl+C should have a non-zero RC.  We either caught the process in
        # its main polling loop (rc=1), or it was successfully terminated by
        # the SIGINT (rc=-2).
        self.assertIn(process.returncode, [1, -2])

    def test_cp_to_nonexistent_bucket(self):
        foo_txt = self.files.create_file('foo.txt', 'this is foo.txt')
        p = aws('s3 cp %s s3://noexist-bucket-foo-bar123/foo.txt' % (foo_txt,))
        self.assertEqual(p.rc, 1)

    def test_cp_empty_file(self):
        bucket_name = self.create_bucket()
        foo_txt = self.files.create_file('foo.txt', contents='')
        p = aws('s3 cp %s s3://%s/' % (foo_txt, bucket_name))
        self.assertEqual(p.rc, 0)
        self.assertNotIn('failed', p.stderr)
        self.assertTrue(self.key_exists(bucket_name, 'foo.txt'))

    def test_download_non_existent_key(self):
        p = aws('s3 cp s3://jasoidfjasdjfasdofijasdf/foo.txt foo.txt')
        self.assertEqual(p.rc, 1)
        expected_err_msg = (
            'A client error (NoSuchKey) occurred when calling the '
            'HeadObject operation: Key "foo.txt" does not exist')
        self.assertIn(expected_err_msg, p.stdout)


class TestSync(BaseS3CLICommand):
    def test_sync_with_plus_chars(self):
        # 1. Create > 1000 files with '+' in the filename.
        # 2. Sync up to s3.
        # 3. Sync up to s3
        # 4. Verify nothing was synced up down from s3 in step 3.
        bucket_name = self.create_bucket()
        filenames = []
        for i in range(2000):
            # Create a file with a space char and a '+' char in the filename.
            filenames.append(self.files.create_file('foo +%06d' % i, contents=''))
        p = aws('s3 sync %s s3://%s/' % (self.files.rootdir, bucket_name))
        self.assert_no_errors(p)
        time.sleep(1)
        p2 = aws('s3 sync %s s3://%s/' % (self.files.rootdir, bucket_name))
        self.assertNotIn('upload:', p2.stdout)
        self.assertEqual('', p2.stdout)

    def test_sync_to_from_s3(self):
        bucket_name = self.create_bucket()
        foo_txt = self.files.create_file('foo.txt', 'foo contents')
        bar_txt = self.files.create_file('bar.txt', 'bar contents')

        # Sync the directory and the bucket.
        p = aws('s3 sync %s s3://%s' % (self.files.rootdir, bucket_name))
        self.assert_no_errors(p)

        # Ensure both files are in the bucket.
        self.assertTrue(self.key_exists(bucket_name, 'foo.txt'))
        self.assertTrue(self.key_exists(bucket_name, 'bar.txt'))

        # Sync back down.  First remote the local files.
        os.remove(foo_txt)
        os.remove(bar_txt)
        p = aws('s3 sync s3://%s %s' % (bucket_name, self.files.rootdir))
        # The files should be back now.
        self.assertTrue(os.path.isfile(foo_txt))
        self.assertTrue(os.path.isfile(bar_txt))
        with open(foo_txt, 'r') as f:
            self.assertEqual(f.read(), 'foo contents')
        with open(bar_txt, 'r') as f:
            self.assertEqual(f.read(), 'bar contents')

    def test_sync_to_nonexistent_bucket(self):
        self.files.create_file('foo.txt', 'foo contents')
        self.files.create_file('bar.txt', 'bar contents')

        # Sync the directory and the bucket.
        p = aws('s3 sync %s s3://noexist-bkt-nme-1412' % (self.files.rootdir,))
        self.assertEqual(p.rc, 1)

    def test_sync_with_empty_files(self):
        self.files.create_file('foo.txt', 'foo contents')
        self.files.create_file('bar.txt', contents='')
        bucket_name = self.create_bucket()
        p = aws('s3 sync %s s3://%s/' % (self.files.rootdir, bucket_name))
        self.assertEqual(p.rc, 0)
        self.assertNotIn('failed', p.stderr)
        self.assertTrue(
            self.key_exists(bucket_name=bucket_name, key_name='bar.txt'))

    def test_sync_with_delete_option_with_same_prefix(self):
        # Test for issue 440 (https://github.com/aws/aws-cli/issues/440)
        # First, we need to create a directory structure that has a dir with
        # the same prefix as some of the files:
        #
        #  test/foo.txt
        #  test-123.txt
        #  test-321.txt
        #  test.txt
        bucket_name = self.create_bucket()
        # create test/foo.txt
        nested_dir = os.path.join(self.files.rootdir, 'test')
        os.mkdir(nested_dir)
        self.files.create_file(os.path.join(nested_dir, 'foo.txt'),
                               contents='foo.txt contents')
        # Then create test-123.txt, test-321.txt, test.txt.
        self.files.create_file('test-123.txt', 'test-123.txt contents')
        self.files.create_file('test-321.txt', 'test-321.txt contents')
        self.files.create_file('test.txt', 'test.txt contents')

        # Now sync this content up to s3.
        # Allow settling time so that we have a different time between
        # source and destination.
        time.sleep(2)
        p = aws('s3 sync %s s3://%s/' % (self.files.rootdir, bucket_name))
        self.assert_no_errors(p)

        # Now here's the issue.  If we try to sync the contents down
        # with the --delete flag we should *not* see any output, the
        # sync operation should determine that nothing is different and
        # therefore do nothing.  We can just use --dryrun to show the issue.
        p = aws('s3 sync s3://%s/ %s --dryrun --delete' % (
            bucket_name, self.files.rootdir))
        self.assert_no_errors(p)
        # These assertion methods will give better error messages than just
        # checking if the output is empty.
        self.assertNotIn('download:', p.stdout)
        self.assertNotIn('delete:', p.stdout)
        self.assertEqual('', p.stdout)


@unittest.skipIf(platform.system() not in ['Darwin', 'Linux'],
                 'Symlink tests only supported on mac/linux')
class TestBadSymlinks(BaseS3CLICommand):
    def test_bad_symlink_stops_sync_process(self):
        bucket_name = self.create_bucket()
        nested_dir = os.path.join(self.files.rootdir, 'realfiles')
        os.mkdir(nested_dir)
        full_path = self.files.create_file(os.path.join(nested_dir, 'foo.txt'),
                                           contents='foo.txt contents')
        symlink_dir = os.path.join(self.files.rootdir, 'symlinkdir')
        os.mkdir(symlink_dir)
        os.symlink(full_path, os.path.join(symlink_dir, 'a-goodsymlink'))
        os.symlink('non-existent-file', os.path.join(symlink_dir, 'b-badsymlink'))
        os.symlink(full_path, os.path.join(symlink_dir, 'c-goodsymlink'))
        p = aws('s3 sync %s s3://%s/' % (symlink_dir, bucket_name))
        self.assertEqual(p.rc, 1, p.stdout)
        self.assertIn('[Errno 2] No such file or directory', p.stdout)


class TestUnicode(BaseS3CLICommand):
    """
    The purpose of these tests are to ensure that the commands can handle
    unicode characters in both keyname and from those generated for both
    uploading and downloading files.
    """

    def test_cp(self):
        bucket_name = self.create_bucket()
        local_example1_txt = self.files.create_file(u'\u00e9xample.txt', 'example1 contents')
        s3_example1_txt = 's3://%s/%s' % (bucket_name,
                                          os.path.basename(local_example1_txt))
        local_example2_txt = self.files.full_path(u'\u00e9xample2.txt')

        p = aws('s3 cp %s %s' % (local_example1_txt, s3_example1_txt))
        self.assert_no_errors(p)

        # Download the file to the second example2.txt filename.
        p = aws('s3 cp %s %s --quiet' % (s3_example1_txt, local_example2_txt))
        self.assert_no_errors(p)
        with open(local_example2_txt, 'rb') as f:
            self.assertEqual(f.read(), b'example1 contents')

    def test_recursive_cp(self):
        bucket_name = self.create_bucket()
        local_example1_txt = self.files.create_file(u'\u00e9xample.txt', 'example1 contents')
        local_example2_txt = self.files.create_file(u'\u00e9xample2.txt', 'example2 contents')
        p = aws('s3 cp %s s3://%s --recursive --quiet' % (
            self.files.rootdir, bucket_name))
        self.assert_no_errors(p)

        os.remove(local_example1_txt)
        os.remove(local_example2_txt)

        p = aws('s3 cp s3://%s %s --recursive --quiet' % (
            bucket_name, self.files.rootdir))
        self.assert_no_errors(p)
        self.assertEqual(open(local_example1_txt).read(), 'example1 contents')
        self.assertEqual(open(local_example2_txt).read(), 'example2 contents')


class TestLs(BaseS3CLICommand):
    """
    This tests using the ``ls`` command.
    """
    def test_ls_bucket(self):
        p = aws('s3 ls')
        self.assert_no_errors(p)

    def test_ls_with_no_env_vars(self):
        # By default, the aws() function injects
        # an AWS_DEFAULT_REGION into the env var of the
        # process.  We're verifying that a region does *not*
        # need to be set anywhere.  If we provide our
        # own environ dict, then the aws() function won't
        # inject a region.
        env = os.environ.copy()
        p = aws('s3 ls', env_vars=env)
        self.assert_no_errors(p)

    def test_ls_bucket_with_s3_prefix(self):
        p = aws('s3 ls s3://')
        self.assert_no_errors(p)

    def test_ls_non_existent_bucket(self):
        p = aws('s3 ls s3://foobara99842u4wbts829381')
        self.assertEqual(p.rc, 255)
        self.assertIn(
            ('A client error (NoSuchBucket) occurred when calling the '
             'ListObjects operation: The specified bucket does not exist'),
            p.stderr)
        # There should be no stdout if we can't find the bucket.
        self.assertEqual(p.stdout, '')

    def test_ls_with_prefix(self):
        bucket_name = self.create_bucket()
        self.put_object(bucket_name, 'foo.txt', 'contents')
        self.put_object(bucket_name, 'foo', 'contents')
        self.put_object(bucket_name, 'bar.txt', 'contents')
        self.put_object(bucket_name, 'subdir/foo.txt', 'contents')
        p = aws('s3 ls s3://%s' % bucket_name)
        self.assertIn('PRE subdir/', p.stdout)
        self.assertIn('8 foo.txt', p.stdout)
        self.assertIn('8 foo', p.stdout)
        self.assertIn('8 bar.txt', p.stdout)

    def test_ls_recursive(self):
        bucket_name = self.create_bucket()
        self.put_object(bucket_name, 'foo.txt', 'contents')
        self.put_object(bucket_name, 'foo', 'contents')
        self.put_object(bucket_name, 'bar.txt', 'contents')
        self.put_object(bucket_name, 'subdir/foo.txt', 'contents')
        p = aws('s3 ls s3://%s --recursive' % bucket_name)
        self.assertIn('8 foo.txt', p.stdout)
        self.assertIn('8 foo', p.stdout)
        self.assertIn('8 bar.txt', p.stdout)
        self.assertIn('8 subdir/foo.txt', p.stdout)

    def test_ls_without_prefix(self):
        # The ls command does not require an s3:// prefix,
        # we're always listing s3 contents.
        bucket_name = self.create_bucket()
        self.put_object(bucket_name, 'foo.txt', 'contents')
        p = aws('s3 ls %s' % bucket_name)
        self.assertEqual(p.rc, 0)
        self.assertIn('foo.txt', p.stdout)


class TestMbRb(BaseS3CLICommand):
    """
    Tests primarily using ``rb`` and ``mb`` command.
    """
    def extra_setup(self):
        self.bucket_name = 'awscli-s3integ-' + str(random.randint(1, 1000))

    def test_mb_rb(self):
        p = aws('s3 mb s3://%s' % self.bucket_name)
        self.assert_no_errors(p)

        response = self.list_buckets()
        self.assertIn(self.bucket_name, [b['Name'] for b in response])

        p = aws('s3 rb s3://%s' % self.bucket_name)
        self.assert_no_errors(p)

    def test_fail_mb_rb(self):
        # Choose a bucket name that already exists.
        p = aws('s3 mb s3://mybucket')
        self.assertIn("BucketAlreadyExists", p.stdout)
        self.assertEqual(p.rc, 1)


class TestDryrun(BaseS3CLICommand):
    """
    This ensures that dryrun works.
    """
    def test_dryrun(self):
        # Make a bucket.
        bucket_name = self.create_bucket()
        foo_txt = self.files.create_file('foo.txt', 'foo contents')

        # Copy file into bucket.
        p = aws('s3 cp %s s3://%s/ --dryrun' % (foo_txt, bucket_name))
        self.assertEqual(p.rc, 0)
        self.assert_no_errors(p)
        self.assertFalse(self.key_exists(bucket_name, 'foo.txt'))

    def test_dryrun_large_files(self):
        bucket_name = self.create_bucket()
        foo_txt = self.files.create_file('foo.txt', 'a' * 1024 * 1024 * 10)

        # Copy file into bucket.
        p = aws('s3 cp %s s3://%s/ --dryrun' % (foo_txt, bucket_name))
        self.assertEqual(p.rc, 0)
        self.assert_no_errors(p)
        self.assertFalse(
            self.key_exists(bucket_name, 'foo.txt'),
            "The key 'foo.txt' exists in S3. It looks like the --dryrun "
            "argument was not obeyed.")

    def test_dryrun_download_large_file(self):
        bucket_name = self.create_bucket()
        full_path = self.files.create_file('largefile', 'a' * 1024 * 1024 * 10)
        with open(full_path, 'rb') as body:
            self.put_object(bucket_name, 'foo.txt', body)

        foo_txt = self.files.full_path('foo.txt')
        p = aws('s3 cp s3://%s/foo.txt %s --dryrun' % (bucket_name, foo_txt))
        self.assertEqual(p.rc, 0)
        self.assert_no_errors(p)
        self.assertFalse(
            os.path.exists(foo_txt),
            "The file 'foo.txt' exists locally. It looks like the --dryrun "
            "argument was not obeyed.")


@unittest.skipIf(platform.system() not in ['Darwin', 'Linux'],
                 'Memory tests only supported on mac/linux')
class TestMemoryUtilization(BaseS3CLICommand):
    # These tests verify the memory utilization and growth are what we expect.
    def extra_setup(self):
        expected_memory_usage = constants.NUM_THREADS * constants.CHUNKSIZE
        # margin for things like python VM overhead, botocore service
        # objects, etc.  1.5 is really generous, perhaps over time this can be
        # lowered.
        runtime_margin = 1.5
        self.max_mem_allowed = runtime_margin * expected_memory_usage

    def assert_max_memory_used(self, process, max_mem_allowed, full_command):
        peak_memory = max(process.memory_usage)
        if peak_memory > self.max_mem_allowed:
            failure_message = (
                'Exceeded max memory allowed (%s MB) for command '
                '"%s": %s MB' % (self.max_mem_allowed / 1024.0 / 1024.0,
                              full_command,
                              peak_memory / 1024.0 / 1024.0))
            self.fail(failure_message)

    def test_transfer_single_large_file(self):
        # 40MB will force a multipart upload.
        bucket_name = self.create_bucket()
        file_contents = 'abcdabcd' * (1024 * 1024 * 10)
        foo_txt = self.files.create_file('foo.txt', file_contents)
        full_command = 's3 mv %s s3://%s/foo.txt' % (foo_txt, bucket_name)
        p = aws(full_command, collect_memory=True)
        self.assert_no_errors(p)
        self.assert_max_memory_used(p, self.max_mem_allowed, full_command)

        # Verify downloading it back down obeys memory utilization.
        download_full_command = 's3 mv s3://%s/foo.txt %s' % (
            bucket_name, foo_txt)
        p = aws(download_full_command, collect_memory=True)
        self.assert_no_errors(p)
        self.assert_max_memory_used(p, self.max_mem_allowed, download_full_command)


class TestWebsiteConfiguration(BaseS3CLICommand):
    def test_create_website_index_configuration(self):
        bucket_name = self.create_bucket()
        # Supply only --index-document argument.
        full_command = 's3 website %s --index-document index.html' % (bucket_name)
        p = aws(full_command)
        self.assertEqual(p.rc, 0)
        self.assert_no_errors(p)
        # Verify we have a bucket website configured.
        operation = self.service.get_operation('GetBucketWebsite')
        parsed = operation.call(
            self.endpoint, bucket=bucket_name)[1]
        self.assertEqual(parsed['IndexDocument']['Suffix'], 'index.html')
        self.assertEqual(parsed['ErrorDocument'], {})
        self.assertEqual(parsed['RoutingRules'], [])
        self.assertEqual(parsed['RedirectAllRequestsTo'], {})

    def test_create_website_index_and_error_configuration(self):
        bucket_name = self.create_bucket()
        # Supply both --index-document and --error-document arguments.
        p = aws('s3 website %s --index-document index.html '
                '--error-document error.html' % bucket_name)
        self.assertEqual(p.rc, 0)
        self.assert_no_errors(p)
        # Verify we have a bucket website configured.
        operation = self.service.get_operation('GetBucketWebsite')
        parsed = operation.call(
            self.endpoint, bucket=bucket_name)[1]
        self.assertEqual(parsed['IndexDocument']['Suffix'], 'index.html')
        self.assertEqual(parsed['ErrorDocument']['Key'], 'error.html')
        self.assertEqual(parsed['RoutingRules'], [])
        self.assertEqual(parsed['RedirectAllRequestsTo'], {})


class TestIncludeExcludeFilters(BaseS3CLICommand):
    def assert_no_files_would_be_uploaded(self, p):
        self.assert_no_errors(p)
        # There should be no output.
        self.assertEqual(p.stdout, '')
        self.assertEqual(p.stderr, '')

    def test_basic_exclude_filter_for_single_file(self):
        full_path = self.files.create_file('foo.txt', 'this is foo.txt')
        # With no exclude we should upload the file.
        p = aws('s3 cp %s s3://random-bucket-name/ --dryrun' % full_path)
        self.assert_no_errors(p)
        self.assertIn('(dryrun) upload:', p.stdout)

        p2 = aws("s3 cp %s s3://random-bucket-name/ --dryrun --exclude '*'"
                 % full_path)
        self.assert_no_files_would_be_uploaded(p2)

    def test_explicitly_exclude_single_file(self):
        full_path = self.files.create_file('foo.txt', 'this is foo.txt')
        p = aws('s3 cp %s s3://random-bucket-name/ --dryrun --exclude foo.txt'
                 % full_path)
        self.assert_no_files_would_be_uploaded(p)

    def test_cwd_doesnt_matter(self):
        full_path = self.files.create_file('foo.txt', 'this is foo.txt')
        with cd(os.path.expanduser('~')):
            p = aws("s3 cp %s s3://random-bucket-name/ --dryrun --exclude '*'"
                    % full_path)
        self.assert_no_files_would_be_uploaded(p)

    def test_recursive_exclude(self):
        # create test/foo.txt
        nested_dir = os.path.join(self.files.rootdir, 'test')
        os.mkdir(nested_dir)
        self.files.create_file(os.path.join(nested_dir, 'foo.txt'),
                               contents='foo.txt contents')
        # Then create test-123.txt, test-321.txt, test.txt.
        self.files.create_file('test-123.txt', 'test-123.txt contents')
        self.files.create_file('test-321.txt', 'test-321.txt contents')
        self.files.create_file('test.txt', 'test.txt contents')
        # An --exclude test* should exclude everything here.
        p = aws("s3 cp %s s3://random-bucket-name/ --dryrun --exclude '*' "
                "--recursive" % self.files.rootdir)
        self.assert_no_files_would_be_uploaded(p)

        # We can include the test directory though.
        p = aws("s3 cp %s s3://random-bucket-name/ --dryrun "
                "--exclude '*' --include 'test/*' --recursive"
                % self.files.rootdir)
        self.assert_no_errors(p)
        self.assertRegexpMatches(p.stdout,
                                 r'\(dryrun\) upload:.*test/foo.txt.*')

    def test_s3_filtering(self):
        # Should behave the same as local file filtering.
        bucket_name = self.create_bucket()
        self.put_object(bucket_name, key_name='foo.txt')
        self.put_object(bucket_name, key_name='bar.txt')
        self.put_object(bucket_name, key_name='baz.jpg')
        p = aws("s3 rm s3://%s/ --dryrun --exclude '*' --recursive"
                % bucket_name)
        self.assert_no_files_would_be_uploaded(p)

        p = aws(
            "s3 rm s3://%s/ --dryrun --exclude '*.jpg' --exclude '*.txt' "
            "--recursive" % bucket_name)
        self.assert_no_files_would_be_uploaded(p)

        p = aws("s3 rm s3://%s/ --dryrun --exclude '*.txt' --recursive"
                % bucket_name)
        self.assert_no_errors(p)
        self.assertRegexpMatches(p.stdout, r'\(dryrun\) delete:.*baz.jpg.*')
        self.assertNotIn(p.stdout, 'bar.txt')
        self.assertNotIn(p.stdout, 'foo.txt')

    def test_exclude_filter_with_delete(self):
        # Test for: https://github.com/aws/aws-cli/issues/778
        bucket_name = self.create_bucket()
        first = self.files.create_file('foo.txt', 'contents')
        second = self.files.create_file('bar.py', 'contents')
        p = aws("s3 sync %s s3://%s/" % (self.files.rootdir, bucket_name))
        self.assert_no_errors(p)
        self.assertTrue(self.key_exists(bucket_name, key_name='bar.py'))
        os.remove(second)
        # We now have the same state as specified in the bug:
        # local           remote
        # -----           ------
        #
        # foo.txt         foo.txt
        #                 bar.py
        #
        # If we now run --exclude '*.py' --delete, then we should *not*
        # delete bar.py and the remote side.
        p = aws("s3 sync %s s3://%s/ --exclude '*.py' --delete" % (
            self.files.rootdir, bucket_name))
        self.assert_no_errors(p)
        self.assertTrue(
            self.key_exists(bucket_name, key_name='bar.py'),
            ("The --delete flag was not applied to the receiving "
             "end, the 'bar.py' file was deleted even though it was excluded."))

    def test_exclude_filter_with_relative_path(self):
        # Same test as test_exclude_filter_with_delete, except we don't
        # use an absolute path on the source dir.
        bucket_name = self.create_bucket()
        first = self.files.create_file('foo.txt', 'contents')
        second = self.files.create_file('bar.py', 'contents')
        p = aws("s3 sync %s s3://%s/" % (self.files.rootdir, bucket_name))
        self.assert_no_errors(p)
        self.assertTrue(self.key_exists(bucket_name, key_name='bar.py'))
        os.remove(second)
        cwd = os.getcwd()
        try:
            os.chdir(self.files.rootdir)
            # Note how we're using "." for the source directory.
            p = aws("s3 sync . s3://%s/ --exclude '*.py' --delete" % bucket_name)
        finally:
            os.chdir(cwd)
        self.assert_no_errors(p)
        self.assertTrue(
            self.key_exists(bucket_name, key_name='bar.py'),
            ("The --delete flag was not applied to the receiving "
             "end, the 'bar.py' file was deleted even though it was excluded."))


class TestFileWithSpaces(BaseS3CLICommand):
    def test_upload_download_file_with_spaces(self):
        bucket_name = self.create_bucket()
        filename = self.files.create_file('with space.txt', 'contents')
        p = aws('s3 cp %s s3://%s/ --recursive' % (self.files.rootdir,
                                                   bucket_name))
        self.assert_no_errors(p)
        os.remove(filename)
        # Now download the file back down locally.
        p = aws('s3 cp s3://%s/ %s --recursive' % (bucket_name,
                                                   self.files.rootdir))
        self.assert_no_errors(p)
        self.assertEqual(os.listdir(self.files.rootdir)[0], 'with space.txt')

    def test_sync_file_with_spaces(self):
        bucket_name = self.create_bucket()
        bucket_name = self.create_bucket()
        filename = self.files.create_file('with space.txt', 'contents')
        p = aws('s3 sync %s s3://%s/' % (self.files.rootdir,
                                         bucket_name))
        self.assert_no_errors(p)
        # Now syncing again should *not* trigger any uploads (i.e we should
        # get nothing on stdout).
        p2 = aws('s3 sync %s s3://%s/' % (self.files.rootdir,
                                          bucket_name))
        self.assertEqual(p2.stdout, '')
        self.assertEqual(p2.stderr, '')
        self.assertEqual(p2.rc, 0)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_s3handler
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.


# Note that all of these functions can be found in the unit tests.
# The only difference is that these tests use botocore's actual session
# variables to communicate with s3 as these are integration tests.  Therefore,
# only tests that use sessions are included as integration tests.

import os
import datetime
import random
from six import StringIO
import sys
from awscli.testutils import unittest

from awscli import EnvironmentVariables
from awscli.customizations.s3.s3handler import S3Handler
from awscli.customizations.s3.fileinfo import FileInfo
import botocore.session
from tests.unit.customizations.s3 import make_loc_files, clean_loc_files, \
    make_s3_files, s3_cleanup, create_bucket, list_contents, list_buckets


class S3HandlerTestDeleteList(unittest.TestCase):
    """
    This tests the ability to delete both files locally and in s3
    """
    def setUp(self):
        self.session = botocore.session.get_session(EnvironmentVariables)
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1'}
        self.s3_handler = S3Handler(self.session, params)
        self.bucket = make_s3_files(self.session)
        self.loc_files = make_loc_files()

    def tearDown(self):
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_loc_delete(self):
        """
        Test delete local file tasks.  The local files are the same
        generated from filegenerator_test.py
        """
        files = [self.loc_files[0], self.loc_files[1]]
        tasks = []
        for filename in files:
            self.assertTrue(os.path.exists(filename))
            tasks.append(FileInfo(
                src=filename, src_type='local',
                dest_type='s3', operation_name='delete',
                size=0,
                service=self.service,
                endpoint=self.endpoint))
        self.s3_handler.call(tasks)
        for filename in files:
            self.assertFalse(os.path.exists(filename))

    def test_s3_delete(self):
        """
        Tests S3 deletes. The files used are the same generated from
        filegenerators_test.py.  This includes the create s3 file.
        """
        keys = [self.bucket + '/another_directory/text2.txt',
                self.bucket + '/text1.txt',
                self.bucket + '/another_directory/']
        tasks = []
        for key in keys:
            tasks.append(FileInfo(
                src=key, src_type='s3',
                dest_type='local', operation_name='delete',
                size=0,
                service=self.service,
                endpoint=self.endpoint,
            ))
        self.assertEqual(len(list_contents(self.bucket, self.session)), 3)
        self.s3_handler.call(tasks)
        self.assertEqual(len(list_contents(self.bucket, self.session)), 0)

    def test_list_objects(self):
        """
        Tests the ability to list objects, common prefixes, and buckets.
        If an error occurs the test fails as this is only a printing
        operation
        """
        prefix_name = self.bucket + '/'
        file_info = FileInfo(
            src=prefix_name,
            operation_name='list_objects',
            size=0,
            service=self.service,
            endpoint=self.endpoint,
        )
        params = {'region': 'us-east-1'}
        s3_handler = S3Handler(self.session, params)
        s3_handler.call([file_info])

        file_info = FileInfo(
            src='', operation_name='list_objects', size=0,
            service=self.service,
            endpoint=self.endpoint,
        )
        params = {'region': 'us-east-1'}
        s3_handler = S3Handler(self.session, params)
        s3_handler.call([file_info])

class S3HandlerTestDeleteList(unittest.TestCase):
    def setUp(self):
        self.session = botocore.session.get_session(EnvironmentVariables)
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1'}
        self.s3_handler = S3Handler(self.session, params)
        self.bucket = make_s3_files(self.session, key1='a+b/foo', key2=None)
        self.loc_files = make_loc_files()

    def tearDown(self):
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session, key1='a+b/foo', key2=None)

    def test_delete_url_encode(self):
        key = self.bucket + '/a+b/foo'
        tasks = [FileInfo(
            src=key, src_type='s3',
            dest_type='local', operation_name='delete', size=0,
            service=self.service, endpoint=self.endpoint,
        )]
        self.assertEqual(len(list_contents(self.bucket, self.session)), 1)
        self.s3_handler.call(tasks)
        self.assertEqual(len(list_contents(self.bucket, self.session)), 0)


class S3HandlerTestUpload(unittest.TestCase):
    """
    This class tests the ability to upload objects into an S3 bucket as
    well as multipart uploads
    """
    def setUp(self):
        self.session = botocore.session.get_session(EnvironmentVariables)
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1', 'acl': ['private']}
        self.s3_handler = S3Handler(self.session, params)
        self.s3_handler_multi = S3Handler(self.session, multi_threshold=10,
                                          chunksize=2,
                                          params=params)
        self.bucket = create_bucket(self.session)
        self.loc_files = make_loc_files()
        self.s3_files = [self.bucket + '/text1.txt',
                         self.bucket + '/another_directory/text2.txt']
        self.output = StringIO()
        self.saved_stdout = sys.stdout
        sys.stdout = self.output

    def tearDown(self):
        self.output.close()
        sys.stdout = self.saved_stdout
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_upload(self):
        # Confirm there are no objects in the bucket.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 0)
        # Create file info objects to perform upload.
        files = [self.loc_files[0], self.loc_files[1]]
        tasks = []
        for i in range(len(files)):
            tasks.append(FileInfo(
                src=self.loc_files[i],
                dest=self.s3_files[i],
                operation_name='upload', size=0,
                service=self.service,
                endpoint=self.endpoint,
            ))
        # Perform the upload.
        self.s3_handler.call(tasks)
        # Confirm the files were uploaded.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 2)

    def test_multi_upload(self):
        files = [self.loc_files[0], self.loc_files[1]]
        tasks = []
        for i in range(len(files)):
            tasks.append(FileInfo(
                src=self.loc_files[i],
                dest=self.s3_files[i], size=15,
                operation_name='upload',
                service=self.service,
                endpoint=self.endpoint,
            ))

        # Note nothing is uploaded because the file is too small
        # a print statement will show up if it fails.
        self.s3_handler_multi.call(tasks)
        print_op = "Error: Your proposed upload is smaller than the minimum"
        self.assertIn(print_op, self.output.getvalue())


class S3HandlerTestUnicodeMove(unittest.TestCase):
    def setUp(self):
        self.session = botocore.session.get_session(EnvironmentVariables)
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1', 'acl': ['private']}
        self.s3_handler = S3Handler(self.session, params)
        self.bucket = make_s3_files(self.session, key1=u'\u2713')
        self.bucket2 = create_bucket(self.session)
        self.s3_files = [self.bucket + '/' + u'\u2713']
        self.s3_files2 = [self.bucket2 + '/' + u'\u2713']

    def tearDown(self):
        s3_cleanup(self.bucket, self.session, key1=u'\u2713')
        s3_cleanup(self.bucket2, self.session, key1=u'\u2713')

    def test_move_unicode(self):
        # Confirm there are no objects in the bucket.
        self.assertEqual(len(list_contents(self.bucket2, self.session)), 0)
        # Create file info objects to perform move.
        tasks = []
        for i in range(len(self.s3_files)):
            tasks.append(FileInfo(
                src=self.s3_files[i], src_type='s3',
                dest=self.s3_files2[i], dest_type='s3',
                operation_name='move', size=0,
                service=self.service, endpoint=self.endpoint
            ))
        # Perform the move.
        self.s3_handler.call(tasks)
        self.assertEqual(len(list_contents(self.bucket2, self.session)), 1)


class S3HandlerTestMove(unittest.TestCase):
    """
    This class tests the ability to move s3 objects.  The move
    operation uses a copy then delete.  Thus, tests the ability
    to copy objects as well as delete
    """
    def setUp(self):
        self.session = botocore.session.get_session(EnvironmentVariables)
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1', 'acl': ['private']}
        self.s3_handler = S3Handler(self.session, params)
        self.bucket = make_s3_files(self.session)
        self.bucket2 = create_bucket(self.session)
        self.s3_files = [self.bucket + '/text1.txt',
                         self.bucket + '/another_directory/text2.txt']
        self.s3_files2 = [self.bucket2 + '/text1.txt',
                          self.bucket2 + '/another_directory/text2.txt']

    def tearDown(self):
        s3_cleanup(self.bucket, self.session)
        s3_cleanup(self.bucket2, self.session)

    def test_move(self):
        # Confirm there are no objects in the bucket.
        self.assertEqual(len(list_contents(self.bucket2, self.session)), 0)
        # Create file info objects to perform move.
        tasks = []
        for i in range(len(self.s3_files)):
            tasks.append(FileInfo(
                src=self.s3_files[i], src_type='s3',
                dest=self.s3_files2[i], dest_type='s3',
                operation_name='move', size=0,
                service=self.service, endpoint=self.endpoint
            ))
        # Perform the move.
        self.s3_handler.call(tasks)
        # Confirm the files were moved.  The origial bucket had three
        # objects. Only two were moved.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 1)
        self.assertEqual(len(list_contents(self.bucket2, self.session)), 2)


class S3HandlerTestDownload(unittest.TestCase):
    """
    This class tests the ability to download s3 objects locally as well
    as using multipart downloads
    """
    def setUp(self):
        self.session = botocore.session.get_session(EnvironmentVariables)
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1'}
        self.s3_handler = S3Handler(self.session, params)
        self.s3_handler_multi = S3Handler(self.session, multi_threshold=10,
                                          chunksize=2, params=params)
        self.bucket = make_s3_files(self.session)
        self.s3_files = [self.bucket + '/text1.txt',
                         self.bucket + '/another_directory/text2.txt']
        directory1 = os.path.abspath('.') + os.sep + 'some_directory' + os.sep
        filename1 = directory1 + "text1.txt"
        directory2 = directory1 + 'another_directory' + os.sep
        filename2 = directory2 + "text2.txt"
        self.loc_files = [filename1, filename2]

    def tearDown(self):
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_download(self):
        # Confirm that the files do not exist.
        for filename in self.loc_files:
            self.assertFalse(os.path.exists(filename))
        # Create file info objects to perform download.
        tasks = []
        time = datetime.datetime.now()
        for i in range(len(self.s3_files)):
            tasks.append(FileInfo(
                src=self.s3_files[i], src_type='s3',
                dest=self.loc_files[i], dest_type='local',
                last_update=time, operation_name='download',
                size=0,
                service=self.service,
                endpoint=self.endpoint
            ))
        # Perform the download.
        self.s3_handler.call(tasks)
        # Confirm that the files now exist.
        for filename in self.loc_files:
            self.assertTrue(os.path.exists(filename))
        # Ensure the contents are as expected.
        with open(self.loc_files[0], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is a test.')
        with open(self.loc_files[1], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is another test.')

    def test_multi_download(self):
        tasks = []
        time = datetime.datetime.now()
        for i in range(len(self.s3_files)):
            tasks.append(FileInfo(
                src=self.s3_files[i], src_type='s3',
                dest=self.loc_files[i], dest_type='local',
                last_update=time, operation_name='download',
                size=15,
                service=self.service,
                endpoint=self.endpoint,
            ))
        # Perform the multipart  download.
        self.s3_handler_multi.call(tasks)
        # Confirm that the files now exist.
        for filename in self.loc_files:
            self.assertTrue(os.path.exists(filename))
        # Ensure the contents are as expected.
        with open(self.loc_files[0], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is a test.')
        with open(self.loc_files[1], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is another test.')


class S3HandlerTestBucket(unittest.TestCase):
    """
    Test the ability to make a bucket then remove it.
    """
    def setUp(self):
        self.session = botocore.session.get_session(EnvironmentVariables)
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        self.params = {'region': 'us-east-1'}
        self.s3_handler = S3Handler(self.session, self.params)
        self.bucket = None

    def tearDown(self):
        s3_cleanup(self.bucket, self.session)

    def test_bucket(self):
        rand1 = random.randrange(5000)
        rand2 = random.randrange(5000)
        self.bucket = str(rand1) + 'mybucket' + str(rand2) + '/'

        file_info = FileInfo(
            src=self.bucket, operation_name='make_bucket', size=0,
            service=self.service,
            endpoint=self.endpoint,
        )
        S3Handler(self.session, self.params).call([file_info])
        buckets_list = []
        for bucket in list_buckets(self.session):
            buckets_list.append(bucket['Name'])
        self.assertIn(self.bucket[:-1], buckets_list)

        file_info = FileInfo(
            src=self.bucket, operation_name='remove_bucket', size=0,
            service=self.service, endpoint=self.endpoint)
        S3Handler(self.session, self.params).call([file_info])
        buckets_list = []
        for bucket in list_buckets(self.session):
            buckets_list.append(bucket['Name'])
        self.assertNotIn(self.bucket[:-1], buckets_list)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_configure
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
import tempfile
import random

from awscli.testutils import unittest, aws


class TestConfigureCommand(unittest.TestCase):

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        self.config_filename = os.path.join(
            self.tempdir, 'config-%s' % random.randint(1, 100000))
        self.env_vars = os.environ.copy()
        self.env_vars['AWS_CONFIG_FILE'] = self.config_filename

    def tearDown(self):
        if os.path.isfile(self.config_filename):
            os.remove(self.config_filename)
        os.rmdir(self.tempdir)

    def set_config_file_contents(self, contents):
        with open(self.config_filename, 'w') as f:
            f.write(contents)

    def get_config_file_contents(self):
        with open(self.config_filename, 'r') as f:
            return f.read()

    def test_list_command(self):
        self.set_config_file_contents(
            '\n'
            '[default]\n'
            'aws_access_key_id=12345\n'
            'aws_secret_access_key=12345\n'
            'region=us-west-2\n'
        )
        self.env_vars.pop('AWS_DEFAULT_REGION', None)
        self.env_vars.pop('AWS_ACCESS_KEY_ID', None)
        self.env_vars.pop('AWS_SECRET_ACCESS_KEY', None)
        p = aws('configure list', env_vars=self.env_vars)
        self.assertRegexpMatches(p.stdout, r'access_key.+config-file')
        self.assertRegexpMatches(p.stdout, r'secret_key.+config-file')
        self.assertRegexpMatches(p.stdout, r'region\s+us-west-2\s+config-file')

    def test_get_command(self):
        self.set_config_file_contents(
            '\n'
            '[default]\n'
            'aws_access_key_id=access_key\n'
            'aws_secret_access_key=secret_key\n'
            'region=us-west-2\n'
        )
        p = aws('configure get aws_access_key_id', env_vars=self.env_vars)
        self.assertEqual(p.stdout.strip(), 'access_key')

    def test_get_command_with_profile_set(self):
        self.set_config_file_contents(
            '\n'
            '[default]\n'
            'aws_access_key_id=default_access_key\n'
            '\n'
            '[profile testing]\n'
            'aws_access_key_id=testing_access_key\n'
        )
        p = aws('configure get aws_access_key_id --profile testing',
                env_vars=self.env_vars)
        self.assertEqual(p.stdout.strip(), 'testing_access_key')

    def test_get_with_fq_name(self):
        # test get configs with fully qualified name.
        self.set_config_file_contents(
            '\n'
            '[default]\n'
            'aws_access_key_id=default_access_key\n'
            '\n'
            '[profile testing]\n'
            'aws_access_key_id=testing_access_key\n'
        )
        p = aws('configure get default.aws_access_key_id --profile testing',
                env_vars=self.env_vars)
        self.assertEqual(p.stdout.strip(), 'default_access_key')

    def test_get_with_fq_profile_name(self):
        self.set_config_file_contents(
            '\n'
            '[default]\n'
            'aws_access_key_id=default_access_key\n'
            '\n'
            '[profile testing]\n'
            'aws_access_key_id=testing_access_key\n'
        )
        p = aws('configure get profile.testing.aws_access_key_id --profile default',
                env_vars=self.env_vars)
        self.assertEqual(p.stdout.strip(), 'testing_access_key')

    def test_get_fq_with_quoted_profile_name(self):
        self.set_config_file_contents(
            '\n'
            '[default]\n'
            'aws_access_key_id=default_access_key\n'
            '\n'
            '[profile "testing"]\n'
            'aws_access_key_id=testing_access_key\n'
        )
        p = aws('configure get profile.testing.aws_access_key_id --profile default',
                env_vars=self.env_vars)
        self.assertEqual(p.stdout.strip(), 'testing_access_key')

    def test_get_fq_for_non_profile_configs(self):
        self.set_config_file_contents(
            '\n'
            '[default]\n'
            'aws_access_key_id=default_access_key\n'
            '\n'
            '[profile testing]\n'
            'aws_access_key_id=testing_access_key\n'
            '[preview]\n'
            'emr=true'
        )
        p = aws('configure get preview.emr --profile default',
                env_vars=self.env_vars)
        self.assertEqual(p.stdout.strip(), 'true')

    def test_set_with_config_file_no_exist(self):
        aws('configure set region us-west-1', env_vars=self.env_vars)
        self.assertEqual(
            '[default]\n'
            'region = us-west-1\n', self.get_config_file_contents())

    def test_set_with_empty_config_file(self):
        with open(self.config_filename, 'w'):
            pass

        aws('configure set region us-west-1', env_vars=self.env_vars)
        self.assertEqual(
            '[default]\n'
            'region = us-west-1\n', self.get_config_file_contents())

    def test_set_with_updating_value(self):
        self.set_config_file_contents(
            '[default]\n'
            'region = us-west-2\n')

        aws('configure set region us-west-1', env_vars=self.env_vars)
        self.assertEqual(
            '[default]\n'
            'region = us-west-1\n', self.get_config_file_contents())

    def test_set_with_profile(self):
        aws('configure set region us-west-1 --profile testing',
            env_vars=self.env_vars)
        self.assertEqual(
            '[profile testing]\n'
            'region = us-west-1\n', self.get_config_file_contents())

    def test_set_with_fq_single_dot(self):
        aws('configure set preview.cloudsearch true', env_vars=self.env_vars)
        self.assertEqual(
            '[preview]\n'
            'cloudsearch = true\n', self.get_config_file_contents())

    def test_set_with_fq_double_dot(self):
        aws('configure set profile.testing.region us-west-2',
            env_vars=self.env_vars)
        self.assertEqual(
            '[profile testing]\n'
            'region = us-west-2\n', self.get_config_file_contents())

    def test_set_with_commented_out_field(self):
        self.set_config_file_contents(
            '#[preview]\n'
            ';cloudsearch = true\n')
        aws('configure set preview.cloudsearch true', env_vars=self.env_vars)
        self.assertEqual(
            '#[preview]\n'
            ';cloudsearch = true\n'
            '[preview]\n'
            'cloudsearch = true\n', self.get_config_file_contents())


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_cli
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import time
import os
import tempfile
import random
import shutil

import botocore.session
from awscli.testutils import unittest, aws


class TestBasicCommandFunctionality(unittest.TestCase):
    """
    These are a set of tests that assert high level features of
    the CLI.  They don't anything exhaustive and is meant as a smoke
    test to verify basic CLI functionality isn't entirely broken.
    """

    def put_object(self, bucket, key, content):
        session = botocore.session.get_session()
        service = session.get_service('s3')
        endpoint = service.get_endpoint('us-east-1')
        http, response = service.get_operation(
            'CreateBucket').call(endpoint, bucket=bucket)
        time.sleep(5)
        self.addCleanup(service.get_operation('DeleteBucket').call,
                        endpoint, bucket=bucket)
        http, response = service.get_operation('PutObject').call(
            endpoint, bucket=bucket, key=key, body=content)
        self.addCleanup(service.get_operation('DeleteObject').call,
                        endpoint, bucket=bucket, key=key)

    def test_ec2_describe_instances(self):
        # Verify we can make a call and get output.
        p = aws('ec2 describe-instances')
        self.assertEqual(p.rc, 0)
        # We don't know what instances a user might have, but we know
        # there should at least be a Reservations key.
        self.assertIn('Reservations', p.json)

    def test_help_output(self):
        p = aws('help')
        self.assertEqual(p.rc, 1)
        self.assertIn('AWS', p.stdout)
        self.assertRegexpMatches(p.stdout, 'The\s+AWS\s+Command\s+Line\s+Interface')

    def test_service_help_output(self):
        p = aws('ec2 help')
        self.assertEqual(p.rc, 1)
        self.assertIn('Amazon EC2', p.stdout)

    def test_operation_help_output(self):
        p = aws('ec2 describe-instances help')
        self.assertEqual(p.rc, 1)
        # XXX: This is a rendering bug that needs to be fixed in bcdoc.  In
        # the RST version there are multiple spaces between certain words.
        # For now we're making the test less strict about formatting, but
        # we eventually should update this test to check exactly for
        # 'The describe-instances operation'.
        self.assertRegexpMatches(p.stdout,
                                 '\s+Describes\s+one\s+or\s+more')

    def test_operation_help_with_required_arg(self):
        p = aws('s3api get-object help')
        self.assertEqual(p.rc, 1, p.stderr)
        self.assertIn('get-object', p.stdout)

    def test_help_with_warning_blocks(self):
        p = aws('elastictranscoder create-pipeline help')
        self.assertEqual(p.rc, 1, p.stderr)
        # Check text that appears in the warning block to ensure
        # the block was actually rendered.
        self.assertRegexpMatches(p.stdout, 'To\s+receive\s+notifications')

    def test_param_shorthand(self):
        p = aws(
            'ec2 describe-instances --filters Name=instance-id,Values=i-123')
        self.assertEqual(p.rc, 0)
        self.assertIn('Reservations', p.json)

    def test_param_json(self):
        p = aws(
            'ec2 describe-instances --filters '
            '\'{"Name": "instance-id", "Values": ["i-123"]}\'')
        self.assertEqual(p.rc, 0, p.stdout + p.stderr)
        self.assertIn('Reservations', p.json)

    def test_param_with_bad_json(self):
        p = aws(
            'ec2 describe-instances --filters '
            '\'{"Name": "bad-filter", "Values": ["i-123"]}\'')
        self.assertEqual(p.rc, 255)
        self.assertIn("The filter 'bad-filter' is invalid", p.stderr,
                      "stdout: %s, stderr: %s" % (p.stdout, p.stderr))

    def test_param_with_file(self):
        d = tempfile.mkdtemp()
        self.addCleanup(os.rmdir, d)
        param_file = os.path.abspath(os.path.join(d, 'params.json'))
        with open(param_file, 'w') as f:
            f.write('[{"Name": "instance-id", "Values": ["i-123"]}]')
        self.addCleanup(os.remove, param_file)
        p = aws('ec2 describe-instances --filters file://%s' % param_file)
        self.assertEqual(p.rc, 0)
        self.assertIn('Reservations', p.json)

    def test_streaming_output_operation(self):
        d = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, d)
        bucket_name = 'clistream' + str(
            int(time.time())) + str(random.randint(1, 100))

        self.put_object(bucket=bucket_name, key='foobar',
                        content='foobar contents')
        p = aws('s3api get-object --bucket %s --key foobar %s' % (
            bucket_name, os.path.join(d, 'foobar')))
        self.assertEqual(p.rc, 0)
        with open(os.path.join(d, 'foobar')) as f:
            contents = f.read()
        self.assertEqual(contents, 'foobar contents')

    def test_no_paginate_arg(self):
        d = tempfile.mkdtemp()
        self.addCleanup(shutil.rmtree, d)
        bucket_name = 'nopaginate' + str(
            int(time.time())) + str(random.randint(1, 100))

        self.put_object(bucket=bucket_name, key='foobar',
                        content='foobar contents')
        p = aws('s3api list-objects --bucket %s --no-paginate' % bucket_name)
        self.assertEqual(p.rc, 0, p.stdout + p.stderr)

    def test_top_level_options_debug(self):
        p = aws('ec2 describe-instances --debug')
        self.assertEqual(p.rc, 0)
        self.assertIn('DEBUG', p.stderr)

    def test_make_requests_to_other_region(self):
        p = aws('ec2 describe-instances --region us-west-2')
        self.assertEqual(p.rc, 0)
        self.assertIn('Reservations', p.json)

    def test_help_usage_top_level(self):
        p = aws('')
        self.assertIn('usage: aws [options] <command> '
                      '<subcommand> [parameters]', p.stderr)
        self.assertIn('aws: error', p.stderr)

    def test_help_usage_service_level(self):
        p = aws('ec2')
        self.assertIn('usage: aws [options] <command> '
                      '<subcommand> [parameters]', p.stderr)
        # python3: aws: error: the following arguments are required: operation
        # python2: aws: error: too few arguments
        # We don't care too much about the specific error message, as long
        # as it says we have a parse error.
        self.assertIn('aws: error', p.stderr)

    def test_help_usage_operation_level(self):
        p = aws('ec2 run-instances')
        self.assertIn('usage: aws [options] <command> '
                      '<subcommand> [parameters]', p.stderr)

    def test_unknown_argument(self):
        p = aws('ec2 describe-instances --filterss')
        self.assertEqual(p.rc, 255)
        self.assertIn('Unknown options: --filterss', p.stderr)

    def test_table_output(self):
        p = aws('ec2 describe-instances --output table --color off')
        # We're not testing the specifics of table output, we just want
        # to make sure the output looks like a table using some heuristics.
        # If this prints JSON instead of a table, for example, this test
        # should fail.
        self.assertEqual(p.rc, 0, p.stderr)
        self.assertIn('-----', p.stdout)
        self.assertIn('+-', p.stdout)
        self.assertIn('DescribeInstances', p.stdout)

    def test_version(self):
        p = aws('--version')
        self.assertEqual(p.rc, 0)
        self.assertTrue(p.stderr.startswith('aws-cli'), p.stderr)

    def test_traceback_printed_when_debug_on(self):
        p = aws('ec2 describe-instances --filters BADKEY=foo --debug')
        self.assertIn('Traceback (most recent call last):', p.stderr, p.stderr)
        # Also should see DEBUG statements:
        self.assertIn('DEBUG', p.stderr, p.stderr)

    def test_leftover_args_in_operation(self):
        p = aws('ec2 describe-instances BADKEY=foo')
        self.assertEqual(p.rc, 255)
        self.assertIn("Unknown option", p.stderr, p.stderr)

    def test_json_param_parsing(self):
        # This is convered by unit tests in botocore, but this is a sanity
        # check that we get a json response from a json service.
        p = aws('swf list-domains --registration-status REGISTERED')
        self.assertEqual(p.rc, 0)
        self.assertIsInstance(p.json, dict)

        p = aws('dynamodb list-tables')
        self.assertEqual(p.rc, 0)
        self.assertIsInstance(p.json, dict)

    def test_pagination_with_text_output(self):
        p = aws('iam list-users --output text')
        self.assertEqual(p.rc, 0)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_terminate_instance_in_autoscaling_group
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest

import awscli.clidriver


class TestTerminateInstanceInAutoscalingGroup(BaseAWSCommandParamsTest):

    PREFIX = 'autoscaling terminate-instance-in-auto-scaling-group'

    def test_true(self):
        cmdline = self.PREFIX
        cmdline += ' --instance-id i-12345678'
        cmdline += ' --should-decrement-desired-capacity'
        params = {'InstanceId': 'i-12345678',
                  'ShouldDecrementDesiredCapacity': 'true'}
        self.assert_params_for_cmd(cmdline, params)

    def test_false(self):
        cmdline = self.PREFIX
        cmdline += ' --instance-id i-12345678'
        cmdline += ' --no-should-decrement-desired-capacity'
        params = {'InstanceId': 'i-12345678',
                  'ShouldDecrementDesiredCapacity': 'false'}
        self.assert_params_for_cmd(cmdline, params)

    def test_last_arg_wins(self):
        cmdline = self.PREFIX
        cmdline += ' --instance-id i-12345678'
        cmdline += ' --should-decrement-desired-capacity'
        cmdline += ' --no-should-decrement-desired-capacity'
        # Since the --no-should-decrement-desired-capacity was
        # was added last, it wins.
        params = {'InstanceId': 'i-12345678',
                  'ShouldDecrementDesiredCapacity': 'false'}
        self.assert_params_for_cmd(cmdline, params)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_create_stack
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import awscli.clidriver


class TestDescribeInstances(BaseAWSCommandParamsTest):

    prefix = 'cloudformation create-stack'

    def test_basic_create_stack(self):
        cmdline = self.prefix
        cmdline += ' --stack-name test-stack --template-url http://foo'
        result = {'StackName': 'test-stack', 'TemplateURL': 'http://foo'}
        self.assert_params_for_cmd(cmdline, result)

    def test_create_stack_string_params(self):
        cmdline = self.prefix
        cmdline += ' --stack-name test-stack --template-url http://foo'
        cmdline += ' --parameters ParameterKey=foo,ParameterValue=bar'
        cmdline += ' ParameterKey=foo2,ParameterValue=bar2'
        result = {'StackName': 'test-stack', 'TemplateURL': 'http://foo',
                  'Parameters.member.1.ParameterKey': 'foo',
                  'Parameters.member.1.ParameterValue': 'bar',
                  'Parameters.member.2.ParameterKey': 'foo2',
                  'Parameters.member.2.ParameterValue': 'bar2'}
        self.assert_params_for_cmd(cmdline, result)

    def test_create_stack_for_csv_params_escaping(self):
        # If a template is specified as a comma delimited list,
        # we need to be able to quote the value or escape the comma.
        cmdline = self.prefix
        cmdline += ' --stack-name test-stack --template-url http://foo'
        cmdline += ' --parameters ParameterKey=foo,ParameterValue=one\,two'
        result = {'StackName': 'test-stack', 'TemplateURL': 'http://foo',
                  'Parameters.member.1.ParameterKey': 'foo',
                  'Parameters.member.1.ParameterValue': 'one,two',
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_create_stack_for_csv_with_quoting(self):
        cmdline = self.prefix
        cmdline += ' --stack-name test-stack --template-url http://foo'
        # Note how we're quoting the value of parameter_value.
        cmdline += ' --parameters ParameterKey=foo,ParameterValue="one,two"'
        result = {'StackName': 'test-stack', 'TemplateURL': 'http://foo',
                  'Parameters.member.1.ParameterKey': 'foo',
                  'Parameters.member.1.ParameterValue': 'one,two',
                  }
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_cloudsearch
#!/usr/bin/env python
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestCloudSearchDefineExpression(BaseAWSCommandParamsTest):

    prefix = 'cloudsearch define-expression'

    def test_flattened(self):
        cmdline = self.prefix
        cmdline += ' --domain-name abc123'
        cmdline += ' --name foo'
        cmdline += ' --expression 10'
        result = {
            'DomainName': 'abc123',
            'Expression.ExpressionName': 'foo',
            'Expression.ExpressionValue': '10'
        }
        self.assert_params_for_cmd(cmdline, result)


class TestCloudSearchDefineIndexField(BaseAWSCommandParamsTest):

    prefix = 'cloudsearch define-index-field'

    def test_flattened(self):
        cmdline = self.prefix
        cmdline += ' --domain-name abc123'
        cmdline += ' --name foo'
        cmdline += ' --type int'
        cmdline += ' --default-value 10'
        cmdline += ' --search-enabled false'
        result = {
            'DomainName': 'abc123',
            'IndexField.IndexFieldName': 'foo',
            'IndexField.IndexFieldType': 'int',
            'IndexField.IntOptions.DefaultValue': '10',
            'IndexField.IntOptions.SearchEnabled': 'false'
        }
        self.assert_params_for_cmd(cmdline, result)

########NEW FILE########
__FILENAME__ = test_put_metric_data
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestPutMetricData(BaseAWSCommandParamsTest):
    maxDiff = None

    prefix = 'cloudwatch put-metric-data '

    expected_output = {
        'MetricData.member.1.MetricName': 'FreeMemoryBytes',
        'MetricData.member.1.Timestamp': '2013-08-22T10:58:12.283000+00:00',
        'MetricData.member.1.Unit': 'Bytes',
        'MetricData.member.1.Value': '9130160128',
        'Namespace': '"Foo/Bar"'
    }

    def test_using_json(self):
        args = ('--namespace "Foo/Bar" '
                '--metric-data [{"MetricName":"FreeMemoryBytes",'
                '"Unit":"Bytes",'
                '"Timestamp":"2013-08-22T10:58:12.283Z",'
                '"Value":9130160128}]')
        cmdline = self.prefix + args
        self.assert_params_for_cmd(cmdline, self.expected_output)

    def test_using_promoted_params(self):
        # This is equivalent to the json version in test_using_json
        # above.
        args = ('--namespace "Foo/Bar" '
                '--metric-name FreeMemoryBytes '
                '--unit Bytes '
                '--timestamp 2013-08-22T10:58:12.283Z '
                '--value 9130160128')
        cmdline = self.prefix + args
        self.assert_params_for_cmd(cmdline, self.expected_output)

########NEW FILE########
__FILENAME__ = test_arg_serialize
#!/usr/bin/env python
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest, unittest
from awscli.testutils import temporary_file

from awscli.customizations.datapipeline import QueryArgBuilder
from dateutil.parser import parse


# We're not interested in testing the def->api
# translation process (that has its own test suite),
# but we need something basic enough that shows us
# that we're serializing arguments properly.
TEST_JSON = """\
{"objects": [
{
  "id" : "S3ToS3Copy",
  "type" : "CopyActivity",
  "schedule" : { "ref" : "CopyPeriod" },
  "input" : { "ref" : "InputData" },
  "output" : { "ref" : "OutputData" }
}
]}"""


class TestPutPipelineDefinition(BaseAWSCommandParamsTest):

    prefix = 'datapipeline put-pipeline-definition'

    def test_put_pipeline_definition_with_json(self):
        with temporary_file('r+') as f:
            f.write(TEST_JSON)
            f.flush()
            cmdline = self.prefix
            cmdline += ' --pipeline-id name'
            cmdline += ' --pipeline-definition file://%s' % f.name
            result = {
                'pipelineId': 'name',
                'pipelineObjects': [
                    {"id": "S3ToS3Copy",
                     "name": "S3ToS3Copy",
                     "fields": [
                       {
                         "key": "input",
                         "refValue": "InputData"
                       },
                       {
                         "key": "output",
                         "refValue": "OutputData"
                       },
                       {
                         "key": "schedule",
                         "refValue": "CopyPeriod"
                       },
                       {
                         "key": "type",
                         "stringValue": "CopyActivity"
                       },
                     ]}]
            }
            self.assert_params_for_cmd(cmdline, result)


class TestErrorMessages(BaseAWSCommandParamsTest):
    prefix = 'datapipeline list-runs'

    def test_unknown_status(self):
        self.assert_params_for_cmd(
            self.prefix + ' --pipeline-id foo --status foo',
            expected_rc=255,
            stderr_contains=('Invalid status: foo, must be one of: waiting, '
                             'pending, cancelled, running, finished, '
                             'failed, waiting_for_runner, '
                             'waiting_on_dependencies'))


class FakeParsedArgs(object):
    def __init__(self, start_interval=None, schedule_interval=None,
                 status=None):
        self.start_interval = start_interval
        self.schedule_interval = schedule_interval
        self.status = status


class TestCLIArgumentSerialize(unittest.TestCase):
    maxDiff = None

    # These tests verify that we go from --cli-args
    # to the proper structure needed for the "Query"
    # argument to describe objects.
    def test_build_query_args_default(self):
        parsed_args = FakeParsedArgs()
        current_time = '2014-02-21T00:00:00'
        start_time = '2014-02-17T00:00:00'
        builder = QueryArgBuilder(current_time=parse(current_time))
        query = builder.build_query(parsed_args)
        self.assertEqual(query, {
            'selectors': [{
                'fieldName': '@actualStartTime',
                 'operator': {
                     'type': 'BETWEEN',
                     'values': [start_time, current_time]
                 }
            }]
        })

    def test_build_args_with_start_interval(self):
        parsed_args = FakeParsedArgs(
            start_interval=['2014-02-01T00:00:00',
                            '2014-02-04T00:00:00',]
        )
        builder = QueryArgBuilder()
        query = builder.build_query(parsed_args)
        self.assertEqual(query, {
            'selectors': [{
                'fieldName': '@actualStartTime',
                 'operator': {
                     'type': 'BETWEEN',
                     'values': ['2014-02-01T00:00:00',
                                '2014-02-04T00:00:00']
                 }
            }]
        })

    def test_build_args_with_end_interval(self):
        parsed_args = FakeParsedArgs(
            schedule_interval=['2014-02-01T00:00:00',
                               '2014-02-04T00:00:00',]
        )
        builder = QueryArgBuilder()
        query = builder.build_query(parsed_args)
        self.assertEqual(query, {
            'selectors': [{
                'fieldName': '@scheduleStartTime',
                 'operator': {
                     'type': 'BETWEEN',
                     'values': ['2014-02-01T00:00:00',
                                '2014-02-04T00:00:00']
                 }
            }]
        })

    def test_build_args_with_single_status(self):
        # --status pending
        parsed_args = FakeParsedArgs(
            status=['pending']
        )
        current_time = '2014-02-21T00:00:00'
        start_time = '2014-02-17T00:00:00'
        builder = QueryArgBuilder(current_time=parse(current_time))
        query = builder.build_query(parsed_args)
        self.assertEqual(query, {
            'selectors': [{
                'fieldName': '@actualStartTime',
                 'operator': {
                     'type': 'BETWEEN',
                     'values': [start_time, current_time]
                 }
            }, {
                'fieldName': '@status',
                 'operator': {
                     'type': 'EQ',
                     'values': ['pending']
                 }
            },
            ]
        })

    def test_build_args_with_csv_status(self):
        # --status pending,waiting_on_dependencies
        parsed_args = FakeParsedArgs(
            status=['pending', 'waiting_on_dependencies']
        )
        current_time = '2014-02-21T00:00:00'
        start_time = '2014-02-17T00:00:00'
        builder = QueryArgBuilder(current_time=parse(current_time))
        query = builder.build_query(parsed_args)
        self.assertEqual(query, {
            'selectors': [{
                'fieldName': '@actualStartTime',
                 'operator': {
                     'type': 'BETWEEN',
                     'values': [start_time, current_time]
                 }
            }, {
                'fieldName': '@status',
                 'operator': {
                     'type': 'EQ',
                     'values': ['pending', 'waiting_on_dependencies']
                 }
            },
            ]
        })

    def test_build_args_with_all_values_set(self):
        # --status pending,waiting_on_dependencies
        # --start-interval pending,waiting_on_dependencies
        # --schedule-schedule pending,waiting_on_dependencies
        parsed_args = FakeParsedArgs(
            start_interval=['2014-02-01T00:00:00',
                            '2014-02-04T00:00:00',],
            schedule_interval=['2014-02-05T00:00:00',
                               '2014-02-09T00:00:00',],
            status=['pending', 'waiting_on_dependencies'],
        )
        builder = QueryArgBuilder()
        query = builder.build_query(parsed_args)
        self.assertEqual(query, {
            'selectors': [{
                'fieldName': '@actualStartTime',
                 'operator': {
                     'type': 'BETWEEN',
                     'values': ['2014-02-01T00:00:00',
                                '2014-02-04T00:00:00',],
                 }
            }, {
                'fieldName': '@scheduleStartTime',
                 'operator': {
                     'type': 'BETWEEN',
                     'values': ['2014-02-05T00:00:00',
                                '2014-02-09T00:00:00']
                 }
            }, {
                'fieldName': '@status',
                 'operator': {
                     'type': 'EQ',
                     'values': ['pending', 'waiting_on_dependencies']
                 }
            },]
        })

########NEW FILE########
__FILENAME__ = test_commands
#!/usr/bin/env python
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import copy
import mock

import six
from awscli.testutils import BaseAWSHelpOutputTest, BaseAWSCommandParamsTest, \
        unittest

from awscli.customizations.datapipeline import convert_described_objects
from awscli.customizations.datapipeline import ListRunsCommand
from awscli.customizations.datapipeline import ListRunsFormatter


API_DESCRIBE_OBJECTS = [
    {"fields": [
         {
             "key": "@componentParent",
             "refValue": "S3Input"
         },
         {
             "key": "@scheduledStartTime",
             "stringValue": "2013-08-19T20:00:00"
         },
         {
             "key": "parent",
             "refValue": "S3Input"
         },
         {
             "key": "@sphere",
             "stringValue": "INSTANCE"
         },
         {
             "key": "type",
             "stringValue": "S3DataNode"
         },
         {
             "key": "@version",
             "stringValue": "1"
         },
         {
             "key": "@status",
             "stringValue": "FINISHED"
         },
         {
             "key": "@actualEndTime",
             "stringValue": "2014-02-19T19:44:44"
         },
         {
             "key": "@actualStartTime",
             "stringValue": "2014-02-19T19:44:43"
         },
         {
             "key": "output",
             "refValue": "@MyCopyActivity_2013-08-19T20:00:00"
         },
         {
             "key": "@scheduledEndTime",
             "stringValue": "2013-08-19T21:00:00"
         }
        ],
        "id": "@S3Input_2013-08-19T20:00:00",
        "name": "@S3Input_2013-08-19T20:00:00"
    },
    {"fields": [
         {
             "key": "@componentParent",
             "refValue": "MyEC2Resource"
         },
         {
             "key": "@resourceId",
             "stringValue": "i-12345"
         },
         {
             "key": "@scheduledStartTime",
             "stringValue": "2013-08-19T23:00:00"
         },
         {
             "key": "parent",
             "refValue": "MyEC2Resource"
         },
         {
             "key": "@sphere",
             "stringValue": "INSTANCE"
         },
         {
             "key": "@attemptCount",
             "stringValue": "1"
         },
         {
             "key": "type",
             "stringValue": "Ec2Resource"
         },
         {
             "key": "@version",
             "stringValue": "1"
         },
         {
             "key": "@status",
             "stringValue": "CREATING"
         },
         {
             "key": "input",
             "refValue": "@MyCopyActivity_2013-08-19T23:00:00"
         },
         {
             "key": "@triesLeft",
             "stringValue": "2"
         },
         {
             "key": "@actualStartTime",
             "stringValue": "2014-02-19T19:59:45"
         },
         {
             "key": "@headAttempt",
             "refValue": "@MyEC2Resource_2013-08-19T23:00:00_Attempt=1"
         },
         {
             "key": "@scheduledEndTime",
             "stringValue": "2013-08-20T00:00:00"
         }
        ],
        "id": "@MyEC2Resource_2013-08-19T23:00:00",
        "name": "@MyEC2Resource_2013-08-19T23:00:00"
    }
]

EMPTY_RUNS = """\
       Name                                                Scheduled Start      Status
       ID                                                  Started              Ended
---------------------------------------------------------------------------------------------------
"""

SINGLE_ROW_RUN = """\
       Name                                                Scheduled Start      Status
       ID                                                  Started              Ended
---------------------------------------------------------------------------------------------------
   1.  parent                                              now                  status
       id                                                  actualStartTime      actualEndTime
"""

class TestConvertObjects(unittest.TestCase):

    def test_convert_described_objects(self):
        converted = convert_described_objects(API_DESCRIBE_OBJECTS)
        self.assertEqual(len(converted), 2)
        # This comes from a "refValue" value.
        self.assertEqual(converted[0]['@componentParent'], 'S3Input')
        # Should also merge in @id and name.
        self.assertEqual(converted[0]['@id'], "@S3Input_2013-08-19T20:00:00")
        self.assertEqual(converted[0]['name'], "@S3Input_2013-08-19T20:00:00")
        # This comes from a "stringValue" value.
        self.assertEqual(converted[0]['@sphere'], "INSTANCE")

    def test_convert_objects_are_sorted(self):
        describe_objects = copy.deepcopy(API_DESCRIBE_OBJECTS)
        # Change the existing @scheduledStartTime from
        # 20:00:00 to 23:59:00
        describe_objects[0]['fields'][1]['stringValue'] = (
            "2013-08-19T23:59:00")
        converted = convert_described_objects(
            describe_objects,
            sort_key_func=lambda x: (x['@scheduledStartTime'], x['name']))
        self.assertEqual(converted[0]['@scheduledStartTime'],
                         '2013-08-19T23:00:00')
        self.assertEqual(converted[1]['@scheduledStartTime'],
                         '2013-08-19T23:59:00')


class FakeParsedArgs(object):
    def __init__(self, **kwargs):
        self.endpoint_url = None
        self.region = None
        self.verify_ssl = None
        self.__dict__.update(kwargs)


class TestCommandsRunProperly(BaseAWSCommandParamsTest):
    def setUp(self):
        super(TestCommandsRunProperly, self).setUp()
        self.query_objects = mock.Mock()
        self.describe_objects = mock.Mock()

    def get_service(self, name):
        if name == 'QueryObjects':
            return self.query_objects
        elif name== 'DescribeObjects':
            return self.describe_objects

    def test_list_runs(self):
        self.driver.session = mock.Mock()
        self.driver.session.emit_first_non_none_response.return_value = None
        self.driver.session.get_service.return_value.get_endpoint.return_value = \
                mock.sentinel.endpoint
        self.driver.session.get_service.return_value.get_operation = self.get_service
        self.query_objects.paginate.return_value.build_full_result.return_value = {
            'ids': ['object-ids']}
        self.describe_objects.call.return_value = (
            None, {'pipelineObjects': [
                {'fields': [], 'id': 'id', 'name': 'name'}]})

        command = ListRunsCommand(self.driver.session, formatter=mock.Mock())
        command(['--pipeline-id', 'my-pipeline-id'],
                parsed_globals=FakeParsedArgs(region='us-east-1'))
        self.assertTrue(self.query_objects.paginate.called)
        self.describe_objects.call.assert_called_with(
            mock.sentinel.endpoint, pipeline_id='my-pipeline-id',
            object_ids=['object-ids'])


class TestHelpOutput(BaseAWSHelpOutputTest):
    def test_list_runs_help_output(self):
        self.driver.main(['datapipeline', 'get-pipeline-definition', 'help'])
        self.assert_contains('pipeline definition')
        # The previous API docs should not be in the output
        self.assert_not_contains('pipelineObjects')


class TestListRunsFormatter(BaseAWSCommandParamsTest):
    def test_no_runs_available(self):
        stream = six.StringIO()
        formatter = ListRunsFormatter(stream)
        objects = []
        formatter.display_objects_to_user(objects)
        self.assertEqual(
            [line.strip() for line in stream.getvalue().splitlines()],
            [line.strip() for line in EMPTY_RUNS.splitlines()])

    def test_single_row(self):
        objects = [
            {'@componentParent': 'parent',
             '@id': 'id',
             '@scheduledStartTime': 'now',
             '@status': 'status',
             '@actualStartTime': 'actualStartTime',
             '@actualEndTime': 'actualEndTime',
            }
        ]
        stream = six.StringIO()
        formatter = ListRunsFormatter(stream)
        formatter.display_objects_to_user(objects)
        # Rather than stream.getvalue() == SINGLE_ROW_RUN
        # we compare equality like this to avoid test failures
        # for differences in leading/trailing whitespace and empty lines.
        self.assertEqual(
            [line.strip() for line in stream.getvalue().splitlines()
             if line.strip()],
            [line.strip() for line in SINGLE_ROW_RUN.splitlines()
             if line.strip()])

########NEW FILE########
__FILENAME__ = test_translator
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest

from botocore.compat import OrderedDict, json

from awscli.customizations.datapipeline import translator


# Thoughout these tests, 'df' refers to the condensed JSON definition format
# that the user provides and API refers to the format expected by the API.

class TestTranslatePipelineDefinitions(unittest.TestCase):
    maxDiff = None

    def load_def(self, json_string):
        return json.loads(json_string, object_pairs_hook=OrderedDict)

    def test_convert_schedule_df_to_api(self):
        definition = self.load_def("""{"objects": [
            {
              "id" : "S3ToS3Copy",
              "type" : "CopyActivity",
              "schedule" : { "ref" : "CopyPeriod" },
              "input" : { "ref" : "InputData" },
              "output" : { "ref" : "OutputData" }
            }
            ]}""")
        actual = translator.definition_to_api(definition)
        api = [{"name": "S3ToS3Copy", "id": "S3ToS3Copy",
                "fields": [
                    {"key": "input", "refValue": "InputData"},
                    {"key": "output", "refValue": "OutputData"},
                    {"key": "schedule", "refValue": "CopyPeriod" },
                    {"key": "type", "stringValue": "CopyActivity" },
                ]}]
        self.assertEqual(actual, api)

    def test_convert_df_to_api_schedule(self):
        definition = self.load_def("""{
              "objects": [
                {
                  "id": "MySchedule",
                  "type": "Schedule",
                  "startDateTime": "2013-08-18T00:00:00",
                  "endDateTime": "2013-08-19T00:00:00",
                  "period": "1 day"
                }
            ]}""")
        actual = translator.definition_to_api(definition)
        api = [{"name": "MySchedule", "id": "MySchedule",
                "fields": [
                    {"key": "endDateTime",
                     "stringValue": "2013-08-19T00:00:00"},
                    {"key": "period", "stringValue": "1 day"},
                    {"key": "startDateTime",
                     "stringValue": "2013-08-18T00:00:00"},
                    {"key": "type", "stringValue": "Schedule"},
                ]}]
        self.assertEqual(actual, api)

    def test_convert_df_to_api_with_name(self):
        definition = self.load_def("""{
              "objects": [
                {
                  "id": "MySchedule",
                  "name": "OVERRIDE-NAME",
                  "type": "Schedule",
                  "startDateTime": "2013-08-18T00:00:00",
                  "endDateTime": "2013-08-19T00:00:00",
                  "period": "1 day"
                }
            ]}""")
        actual = translator.definition_to_api(definition)
        api = [{"name": "OVERRIDE-NAME", "id": "MySchedule",
                "fields": [
                    {"key": "endDateTime",
                     "stringValue": "2013-08-19T00:00:00"},
                    {"key": "period", "stringValue": "1 day"},
                    {"key": "startDateTime",
                     "stringValue": "2013-08-18T00:00:00"},
                    {"key": "type", "stringValue": "Schedule"},
                ]}]
        self.assertEqual(actual, api)

    def test_objects_key_is_missing_raise_error(self):
        definition = self.load_def("""{"not-objects": []}""")
        with self.assertRaises(translator.PipelineDefinitionError):
            translator.definition_to_api(definition)

    def test_missing_id_field(self):
        # Note that the 'id' key is missing.
        definition = self.load_def("""{
              "objects": [
                {
                  "name": "OVERRIDE-NAME",
                  "type": "Schedule",
                  "startDateTime": "2013-08-18T00:00:00",
                  "endDateTime": "2013-08-19T00:00:00",
                  "period": "1 day"
                }
            ]}""")
        with self.assertRaises(translator.PipelineDefinitionError):
            translator.definition_to_api(definition)

    def test_list_value_with_strings(self):
        definition = self.load_def("""{"objects": [
            {
              "id" : "emrActivity",
              "type" : "EmrActivity",
              "name" : "Foo",
              "step" : ["s3://foo1", "s3://foo2", "s3://foo3"]
            }
        ]}""")
        actual = translator.definition_to_api(definition)
        api = [{"name": "Foo", "id": "emrActivity",
                "fields": [
                    {"key": "step", "stringValue": "s3://foo1"},
                    {"key": "step", "stringValue": "s3://foo2"},
                    {"key": "step", "stringValue": "s3://foo3"},
                    {"key": "type", "stringValue": "EmrActivity"},
        ]}]
        self.assertEqual(actual, api)

    def test_value_with_refs(self):
        definition = self.load_def("""{"objects": [
            {
              "id" : "emrActivity",
              "type" : "EmrActivity",
              "name" : "Foo",
              "step" : ["s3://foo1", {"ref": "otherValue"}, "s3://foo3"]
            }
        ]}""")
        actual = translator.definition_to_api(definition)
        api = [{"name": "Foo", "id": "emrActivity",
                "fields": [
                    {"key": "step", "stringValue": "s3://foo1"},
                    {"key": "step", "refValue": "otherValue"},
                    {"key": "step", "stringValue": "s3://foo3"},
                    {"key": "type", "stringValue": "EmrActivity"},
        ]}]
        self.assertEqual(actual, api)

    # These tests check the API -> DF conversion.
    def test_api_to_df(self):
        api = [{"name": "S3ToS3Copy", "id": "S3ToS3Copy",
                "fields": [{"key": "type", "stringValue": "CopyActivity" },
                           {"key": "schedule", "refValue": "CopyPeriod" },
                           {"key": "input", "refValue": "InputData"},
                           {"key": "output", "refValue": "OutputData"}]}]
        definition = translator.api_to_definition(api)
        self.assertEqual(definition, {
            'objects': [{
                'id': 'S3ToS3Copy',
                'name': 'S3ToS3Copy',
                'type': 'CopyActivity',
                'schedule': {'ref': 'CopyPeriod'},
                'input': {'ref': 'InputData'},
                'output': {'ref': 'OutputData'}
            }]
        })

    def test_api_to_df_with_dupe_keys(self):
        # Duplicate keys should be aggregated into a list.
        api = [{"name": "S3ToS3Copy", "id": "S3ToS3Copy",
                "fields": [{"key": "type", "stringValue": "CopyActivity" },
                           {"key": "schedule", "refValue": "CopyPeriod" },
                           {"key": "script", "stringValue": "value1"},
                           {"key": "script", "stringValue": "value2"}]}]
        definition = translator.api_to_definition(api)
        self.assertEqual(definition, {
            'objects': [{
                'id': 'S3ToS3Copy',
                'name': 'S3ToS3Copy',
                'type': 'CopyActivity',
                'schedule': {'ref': 'CopyPeriod'},
                'script': ['value1', 'value2'],
            }]
        })


########NEW FILE########
__FILENAME__ = fake_session
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import hashlib
from operator import itemgetter
from botocore.vendored import requests
from six import text_type
from six import StringIO
from io import BytesIO

from mock import MagicMock, Mock

from awscli.customizations.s3.filegenerator import find_bucket_key


class FakeSession(object):
    """
    This class drives the operations for the unit tests for the plugin by
    providing an emulation of botocore's session module.  It is by no
    means a complete emulation of the session module.  The class also
    keeps track of dictionary that tracks an emulated state of s3.
    This feature allows the unit tests to preform commands on the
    emulated s3 profile and actually affect the emulated s3 profile
    without ever affecting an actual s3 profile or contacting s3.

    :var self.s3: This holds the current state of the emulated s3
        profile.  The variable is ordered such that the top level keys are
        bucket names. Each bucket name is a key to a dictionary of
        s3 objects.  The key's of the s3 objects are keys to another
        dictionary of relevant info about the object like its data,
        last modified time, size, and etag.
    :type self.s3: Dictionary. A sample form of the dictionary with
        a single object and bucket is
        {'mybucket': {'mykey': {'Body': "This is a test.",
                                'Size': 15,
                                'LastModified': '2013-07-15T17:03:43.000Z',
                                'ETag': 'ad35657fshafq4tg46'}}}
    :var self.service:  This is a mock serice to emulate botocore's
        session module

    :param md5_error: If true, some operations will raise an exception
        signaling the md5's do not match
    :param connection_error: If true, some operations will raise an exception
        signalling that there was a connection_error.
    """
    def __init__(self, md5_error=False, connection_error=False):
        self.s3 = {}
        self.service = FakeService(self)
        self.md5_error = md5_error
        self.connection_error = connection_error

    def get_config(self):
        return {'region': 'us-west-2'}

    def get_service(self, service='s3'):
        return self.service

    def emit(self, *args, **kwargs):
        pass

    def emit_first_non_none_response(self, *args, **kwargs):
        pass

    def register(self, name, handler):
        pass

    def unregister(self, name, handler):
        pass


class FakeService(object):
    """
    This class is an emulation of botocore's service module.
    It only includes the functions necessary to mock the service
    module in the unit tests.
    """
    def __init__(self, session):
        self.session = session

    def get_endpoint(self, region_name, endpoint_url=None, verify=None):
        endpoint = Mock()
        endpoint.region_name = region_name
        return endpoint

    def get_operation(self, name):
        return FakeOperation(name, self.session)


class FakeOperation(object):

    def __init__(self, name, session):
        """
        This class preforms the actual commands a session's s3.
        The only operations will raise errors when specified are
        PutObject, UploadPart, and GetObject.
        :param name: The name of the operation being preformed.
        """
        self.name = name
        self.session = session

    def call(self, *args, **kwargs):
        """
        This function preforms the call method of an operation.
        :returns: An emulated response data and emulated http_response
            from the operations preformed on s3.
        """
        op_dict = {'PutObject': self.put_object,
                   'CreateBucket': self.create_bucket,
                   'DeleteObject': self.delete_object,
                   'DeleteBucket': self.delete_bucket,
                   'ListObjects': self.list_objects,
                   'ListBuckets': self.list_buckets,
                   'CreateMultipartUpload': self.create_multi_upload,
                   'UploadPart': self.upload_part,
                   'CompleteMultipartUpload': self.complete_multi_upload,
                   'CopyObject': self.copy_object,
                   'GetObject': self.get_object,
                   'HeadObject': self.head_object,
                   'AbortMultipartUpload': self.abort_multi_upload}
        return op_dict[self.name](kwargs)

    def paginate(self, *args, **kwargs):
        """
        This function is exactly the same as the call() method however
        it yields the emulated data response and emulated http_response.
        """
        op_dict = {'ListObjects': self.list_objects}
        yield op_dict[self.name](kwargs)

    def put_object(self, kwargs):
        """
        This function puts an object into a session's s3.  It calculates
        the md5 of the body sent to the operation object and stores it both
        in s3 and the returned emulated http response object.  If the
        specified bucket does not exist it will send an error message in the
        response data.  If the object's session has been flagged to raise
        an exception, the first object(s) will raise the error or cause
        an error to be raised and flip the session's flag no longer allowing
        any other objects using the session to raise that exception.  The
        LastModified time assigned to the object is uniform and arbitrary.
        """
        bucket = kwargs['bucket']
        response_data = {}
        etag = ''
        if bucket in self.session.s3:
            key = kwargs['key']
            content = {}
            body = ''
            if 'body' in kwargs:
                body = kwargs['body']
                if isinstance(body, StringIO):
                    body = body.getvalue()
                if hasattr(body, 'read'):
                    body = body.read()
                elif not isinstance(body, bytearray):
                    body = body.encode('utf-8')
                content['Body'] = body
                m = hashlib.md5()
                m.update(body)
                etag = m.hexdigest()
            content['Size'] = len(body)
            content['LastModified'] = '2013-07-15T17:03:43.000Z'
            content['ETag'] = '"%s"' % etag
            if 'content_type' in kwargs:
                content['ContentType'] = kwargs['content_type']
            if key in self.session.s3[bucket]:
                self.session.s3[bucket][key].update(content)
            else:
                self.session.s3[bucket][key] = content
        else:
            response_data['Errors'] = [{'Message': 'Bucket does not exist'}]
        if self.session.md5_error:
            etag = "dsffsdg"  # This etag should always raise an exception
            self.session.md5_error = False
        elif self.session.connection_error:
            self.session.connection_error = False
            raise requests.ConnectionError()
        response_data['ETag'] = '"%s"' % etag
        return FakeHttp(), response_data

    def create_bucket(self, kwargs):
        """
        This operation creates a bucket.  It sends an error message if the
        bucket already exists
        """
        bucket = kwargs['bucket']
        response_data = {}
        etag = ''
        if bucket not in self.session.s3:
            self.session.s3[bucket] = {}
        else:
            response_data['Errors'] = [{'Message': 'Bucket already exists'}]
        response_data['ETag'] = '"%s"' % etag
        return FakeHttp(), response_data

    def delete_object(self, kwargs):
        """
        This operation deletes an s3 object.  It sends an error message if
        the sprecified bucket does not exist.
        """
        bucket = kwargs['bucket']
        key = kwargs['key']
        response_data = {}
        etag = ''
        if bucket in self.session.s3:
            if key in self.session.s3[bucket]:
                self.session.s3[bucket].pop(key)
        else:
            response_data['Errors'] = [{'Message': 'Bucket does not exist'}]
        response_data['ETag'] = '"%s"' % etag
        return FakeHttp(), response_data

    def copy_object(self, kwargs):
        """
        This operation copies one s3 object to another location in s3.
        If either the bucket of the source or the destination does not
        exist, an error message will be sent stating that the bucket does
        not exist.
        """
        bucket = kwargs['bucket']
        key = kwargs['key']
        copy_source = kwargs['copy_source']
        src_bucket, src_key = find_bucket_key(copy_source)
        if not isinstance(src_key, text_type) and hasattr(src_key, 'decode'):
            src_key = src_key.decode('utf-8')
        response_data = {}
        etag = ''
        if bucket in self.session.s3 or src_bucket in self.session.s3:
            src = self.session.s3[src_bucket][src_key]
            self.session.s3[bucket][key] = src
        else:
            response_data['Errors'] = [{'Message': 'Bucket does not exist'}]
        response_data['ETag'] = '"%s"' % etag
        return FakeHttp(), response_data

    def get_object(self, kwargs):
        """
        This operation gets an object from s3.  It retrieves the body of
        the object or a part of the body specified by a range variable.
        A MagicMock() class is used to transfer the body allowing it to
        be read by a read() operation.  The etags no matter if it is
        a multipart download or not is invalid as it will have a dash
        in the etags.  So it is never compared during download.  If the
        session's connection_error flag is set, it will raise a
        ConnectionError and reset the flag to False.
        """
        bucket = kwargs['bucket']
        key = kwargs['key']
        response_data = {}
        etag = ''
        if bucket in self.session.s3:
            body = self.session.s3[bucket][key]['Body']
            if 'range' in kwargs:
                str_range = kwargs['range']
                str_range = str_range[6:]
                range_components = str_range.split('-')
                beginning = range_components[0]
                end = range_components[1]
                if end == '':
                    body = body[int(beginning):]
                else:
                    body = body[int(beginning):(int(end) + 1)]
            mock_response = BytesIO(body)
            mock_response.set_socket_timeout = Mock()
            response_data['Body'] = mock_response
            etag = self.session.s3[bucket][key]['ETag']
            response_data['ETag'] = etag + '--'
        else:
            response_data['Errors'] = [{'Message': 'Bucket does not exist'}]
        if self.session.connection_error:
            self.session.connection_error = False
            raise requests.ConnectionError
        return FakeHttp(), response_data

    def delete_bucket(self, kwargs):
        """
        This operation deletes an s3 bucket.  If the bucket does not
        exist or is not empty it will send an error message.
        """
        bucket = kwargs['bucket']
        response_data = {}
        etag = ''
        if bucket in self.session.s3:
            if not self.session.s3[bucket]:
                self.session.s3.pop(bucket)
            else:
                response_data['Errors'] = [{'Message': 'Bucket not empty'}]
        else:
            response_data['Errors'] = [{'Message': 'Bucket does not exist'}]
        response_data['ETag'] = '"%s"' % etag
        return FakeHttp(), response_data

    def list_buckets(self, kwargs):
        """
        This function returns the buckets in the session's s3.
        """
        response_data = {}
        etag = ''
        response_data['Buckets'] = []
        for bucket in self.session.s3.keys():
            bucket_dict = {}
            bucket_dict['Name'] = bucket
            response_data['Buckets'].append(bucket_dict)
        response_data['Contents'] = sorted(response_data['Buckets'],
                                           key=lambda k: k['Name'])
        response_data['ETag'] = '"%s"' % etag
        return FakeHttp(), response_data

    def list_objects(self, kwargs):
        """
        This function returns the objects specified by a bucket and prefix.
        In the response data it includes the size and last modified time.
        """
        bucket = kwargs['bucket']
        prefix = ''
        if 'prefix' in kwargs:
            prefix = kwargs['prefix']
        response_data = {}
        etag = ''
        delimiter = ''
        if 'delimiter' in kwargs:
            delimiter = kwargs['delimiter']
            response_data['CommonPrefixes'] = []
        response_data['Contents'] = []
        objects = self.session.s3[bucket]
        for key in objects.keys():
            if key.startswith(prefix):
                key_dict = {}
                key_dict['Key'] = key
                size = objects[key]['Size']
                key_dict['Size'] = size
                key_dict['LastModified'] = objects[key]['LastModified']
                if key.endswith('/') and size == 0 and delimiter:
                    prefix_dict = {}
                    prefix_dict['Prefix'] = key
                    response_data['CommonPrefixes'].append(prefix_dict)
                response_data['Contents'].append(key_dict)
        response_data['Contents'] = sorted(response_data['Contents'],
                                           key=lambda k: k['Key'])
        response_data['ETag'] = '"%s"' % etag
        return FakeHttp(), response_data

    def head_object(self, kwargs):
        bucket = kwargs['bucket']
        key = kwargs['key']
        response_data = {}
        etag = ''
        if bucket not in self.session.s3 or key not in self.session.s3[bucket]:
            return FakeHttp(404), {}
        key = self.session.s3[bucket][key]
        response_data['ContentLength'] = str(key['Size'])
        response_data['LastModified'] = key['LastModified']
        return FakeHttp(), response_data

    def create_multi_upload(self, kwargs):
        """
        A dummy function that returns an arbitrary upload id necessary
        for a multipart upload.
        """
        bucket = kwargs['bucket']
        if bucket in self.session.s3:
            content = {}
            key = kwargs['key']
            content['ContentType'] = kwargs.get('content_type')
            self.session.s3[bucket][key] = content
        return FakeHttp(), {'UploadId': 'upload_id'}

    def upload_part(self, kwargs):
        """
        This function is exaclty the same as PutObject because
        it is used to check parts are properly read and uoloaded
        by checking the etags
        """
        return self.put_object(kwargs)

    def complete_multi_upload(self, kwargs):
        """
        A function that acts as a mock function for
        CompleteMultipartUpload calls
        """
        return FakeHttp(), {}

    def abort_multi_upload(self, kwargs):
        """
        A function that acts as a mock function for
        CompleteMultipartUpload calls
        """
        return FakeHttp(), {}


class FakeHttp(object):
    """
    This class emulates the http responses from an operation's
    call method.  The http responses are only used to retrieve
    etag's.  So only formatted etag's are included in this class.
    """
    def __init__(self, status_code=200):
        self.status_code = status_code

########NEW FILE########
__FILENAME__ = test_comparator
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import datetime
import unittest

from awscli.customizations.s3.comparator import Comparator
from awscli.customizations.s3.fileinfo import FileInfo


class ComparatorTest(unittest.TestCase):
    def setUp(self):
        self.comparator = Comparator({'delete': True})

    def test_compare_key_equal(self):
        """
        Confirms checking compare key works.
        """
        src_files = []
        dest_files = []
        ref_list = []
        result_list = []
        time = datetime.datetime.now()
        src_file = FileInfo(src='', dest='',
                            compare_key='comparator_test.py', size=10,
                            last_update=time, src_type='local',
                            dest_type='s3', operation_name='upload',
                            service=None, endpoint=None)
        dest_file = FileInfo(src='', dest='',
                             compare_key='comparator_test.py', size=10,
                             last_update=time, src_type='s3',
                             dest_type='local', operation_name='',
                             service=None, endpoint=None)
        src_files.append(src_file)
        dest_files.append(dest_file)
        files = self.comparator.call(iter(src_files), iter(dest_files))
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, ref_list)

    def test_compare_size(self):
        """
        Confirms compare size works.
        """
        src_files = []
        dest_files = []
        ref_list = []
        result_list = []
        time = datetime.datetime.now()
        src_file = FileInfo(src='', dest='',
                            compare_key='comparator_test.py', size=11,
                            last_update=time, src_type='local',
                            dest_type='s3', operation_name='upload',
                            service=None, endpoint=None)
        dest_file = FileInfo(src='', dest='',
                             compare_key='comparator_test.py', size=10,
                             last_update=time, src_type='s3',
                             dest_type='local', operation_name='',
                             service=None, endpoint=None)
        src_files.append(src_file)
        dest_files.append(dest_file)
        files = self.comparator.call(iter(src_files), iter(dest_files))
        ref_list.append(src_file)
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, ref_list)

    def test_compare_lastmod_upload(self):
        """
        Confirms compare time works for uploads.
        """
        src_files = []
        dest_files = []
        ref_list = []
        result_list = []
        time = datetime.datetime.now()
        future_time = time + datetime.timedelta(0, 3)
        src_file = FileInfo(src='', dest='',
                            compare_key='comparator_test.py', size=10,
                            last_update=future_time, src_type='local',
                            dest_type='s3', operation_name='upload',
                            service=None, endpoint=None)
        dest_file = FileInfo(src='', dest='',
                             compare_key='comparator_test.py', size=10,
                             last_update=time, src_type='s3',
                             dest_type='local', operation_name='',
                             service=None, endpoint=None)
        src_files.append(src_file)
        dest_files.append(dest_file)
        files = self.comparator.call(iter(src_files), iter(dest_files))
        ref_list.append(src_file)
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, ref_list)

    def test_compare_lastmod_copy(self):
        """
        Confirms compare time works for copies
        """
        src_files = []
        dest_files = []
        ref_list = []
        result_list = []
        time = datetime.datetime.now()
        future_time = time + datetime.timedelta(0, 3)
        src_file = FileInfo(src='', dest='',
                            compare_key='comparator_test.py', size=10,
                            last_update=future_time, src_type='s3',
                            dest_type='s3', operation_name='copy',
                            service=None, endpoint=None)
        dest_file = FileInfo(src='', dest='',
                             compare_key='comparator_test.py', size=10,
                             last_update=time, src_type='s3',
                             dest_type='s3', operation_name='',
                             service=None, endpoint=None)
        src_files.append(src_file)
        dest_files.append(dest_file)
        files = self.comparator.call(iter(src_files), iter(dest_files))
        ref_list.append(src_file)
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, ref_list)

    def test_compare_lastmod_download(self):
        """
        Confirms compare time works for downloads.
        """
        src_files = []
        dest_files = []
        ref_list = []
        result_list = []
        time = datetime.datetime.now()
        future_time = time + datetime.timedelta(0, 3)
        src_file = FileInfo(src='', dest='',
                            compare_key='comparator_test.py', size=10,
                            last_update=time, src_type='s3',
                            dest_type='local', operation_name='download',
                            service=None, endpoint=None)
        dest_file = FileInfo(src='', dest='',
                             compare_key='comparator_test.py', size=10,
                             last_update=future_time, src_type='local',
                             dest_type='s3', operation_name='',
                             service=None, endpoint=None)
        src_files.append(src_file)
        dest_files.append(dest_file)
        files = self.comparator.call(iter(src_files), iter(dest_files))
        ref_list.append(src_file)
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, ref_list)

        # If the source is newer than the destination do not download.
        src_file = FileInfo(src='', dest='',
                            compare_key='comparator_test.py', size=10,
                            last_update=future_time, src_type='s3',
                            dest_type='local', operation_name='download',
                            service=None, endpoint=None)
        dest_file = FileInfo(src='', dest='',
                             compare_key='comparator_test.py', size=10,
                             last_update=time, src_type='local',
                             dest_type='s3', operation_name='',
                             service=None, endpoint=None)
        src_files = []
        dest_files = []
        src_files.append(src_file)
        dest_files.append(dest_file)
        files = self.comparator.call(iter(src_files), iter(dest_files))
        result_list = []
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, [])

    def test_compare_key_less(self):
        """
        Confirm the appropriate action is taken when the soruce compare key
        is less than the destination compare key.
        """
        src_files = []
        dest_files = []
        ref_list = []
        result_list = []
        time = datetime.datetime.now()
        src_file = FileInfo(src='', dest='',
                            compare_key='bomparator_test.py', size=10,
                            last_update=time, src_type='local',
                            dest_type='s3', operation_name='upload',
                            service=None, endpoint=None)
        dest_file = FileInfo(src='', dest='',
                             compare_key='comparator_test.py', size=10,
                             last_update=time, src_type='s3',
                             dest_type='local', operation_name='',
                             service=None, endpoint=None)
        src_files.append(src_file)
        dest_files.append(dest_file)
        dest_file.operation = 'delete'
        ref_list.append(src_file)
        ref_list.append(dest_file)
        files = self.comparator.call(iter(src_files), iter(dest_files))
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, ref_list)

    def test_compare_key_greater(self):
        """
        Confirm the appropriate action is taken when the soruce compare key
        is greater than the destination compare key.
        """
        src_files = []
        dest_files = []
        ref_list = []
        result_list = []
        time = datetime.datetime.now()
        src_file = FileInfo(src='', dest='',
                            compare_key='domparator_test.py', size=10,
                            last_update=time, src_type='local',
                            dest_type='s3', operation_name='upload',
                            service=None, endpoint=None)
        dest_file = FileInfo(src='', dest='',
                             compare_key='comparator_test.py', size=10,
                             last_update=time, src_type='s3',
                             dest_type='local', operation_name='',
                             service=None, endpoint=None)
        src_files.append(src_file)
        dest_files.append(dest_file)
        src_file.operation = 'upload'
        dest_file.operation = 'delete'
        ref_list.append(dest_file)
        ref_list.append(src_file)
        files = self.comparator.call(iter(src_files), iter(dest_files))
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, ref_list)

    def test_empty_src(self):
        """
        Confirm the appropriate action is taken when there are no more source
        files to take.
        """
        src_files = []
        dest_files = []
        ref_list = []
        result_list = []
        time = datetime.datetime.now()
        dest_file = FileInfo(src='', dest='',
                             compare_key='comparator_test.py', size=10,
                             last_update=time, src_type='s3',
                             dest_type='local', operation_name='',
                             service=None, endpoint=None)
        dest_files.append(dest_file)
        dest_file.operation = 'delete'
        ref_list.append(dest_file)
        files = self.comparator.call(iter(src_files), iter(dest_files))
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, ref_list)

    def test_empty_dest(self):
        """
        Confirm the appropriate action is taken when there are no more dest
        files to take.
        """
        src_files = []
        dest_files = []
        ref_list = []
        result_list = []
        time = datetime.datetime.now()
        src_file = FileInfo(src='', dest='',
                            compare_key='domparator_test.py', size=10,
                            last_update=time, src_type='local',
                            dest_type='s3', operation_name='upload',
                            service=None, endpoint=None)
        src_files.append(src_file)
        ref_list.append(src_file)
        files = self.comparator.call(iter(src_files), iter(dest_files))
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, ref_list)

    def test_empty_src_dest(self):
        """
        Confirm the appropriate action is taken when there are no more
        files to take for both source and destination.
        """
        src_files = []
        dest_files = []
        ref_list = []
        result_list = []
        files = self.comparator.call(iter(src_files), iter(dest_files))
        for filename in files:
            result_list.append(filename)
        self.assertEqual(result_list, ref_list)


class ComparatorSizeOnlyTest(unittest.TestCase):
    def setUp(self):
        self.comparator = Comparator({'delete': True, 'size_only': True})

    def test_compare_size_only_dest_older_than_src(self):
        """
        Confirm that files with the same size but different update times are not
        synced when `size_only` is set.
        """
        time_src = datetime.datetime.now()
        time_dst = time_src + datetime.timedelta(days=1)

        src_file = FileInfo(src='', dest='',
                            compare_key='test.py', size=10,
                            last_update=time_src, src_type='local',
                            dest_type='s3', operation_name='upload',
                            service=None, endpoint=None)

        dst_file = FileInfo(src='', dest='',
                            compare_key='test.py', size=10,
                            last_update=time_dst, src_type='s3',
                            dest_type='local', operation_name='',
                            service=None, endpoint=None)

        files = self.comparator.call(iter([src_file]), iter([dst_file]))
        self.assertEqual(sum(1 for _ in files), 0)

    def test_compare_size_only_src_older_than_dest(self):
        """
        Confirm that files with the same size but different update times are not
        synced when `size_only` is set.
        """
        time_dst = datetime.datetime.now()
        time_src = time_dst + datetime.timedelta(days=1)

        src_file = FileInfo(src='', dest='',
                            compare_key='test.py', size=10,
                            last_update=time_src, src_type='local',
                            dest_type='s3', operation_name='upload',
                            service=None, endpoint=None)

        dst_file = FileInfo(src='', dest='',
                            compare_key='test.py', size=10,
                            last_update=time_dst, src_type='s3',
                            dest_type='local', operation_name='',
                            service=None, endpoint=None)

        files = self.comparator.call(iter([src_file]), iter([dst_file]))
        self.assertEqual(sum(1 for _ in files), 0)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_copy_params
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
import sys
import re
import copy

from awscli.testutils import BaseAWSCommandParamsTest

if sys.version_info[:2] == (2, 6):
    from StringIO import StringIO


# file is gone in python3, so instead IOBase must be used.
# Given this test module is the only place that cares about
# this type check, we do the check directly in this test module.
try:
    file_type = file
except NameError:
    import io
    file_type = io.IOBase


class TestGetObject(BaseAWSCommandParamsTest):

    prefix = 's3 cp '

    def setUp(self):
        super(TestGetObject, self).setUp()
        self.file_path = os.path.join(os.path.dirname(__file__),
                                      'test_copy_params_data')
        self.parsed_response = {'ETag': '"120ea8a25e5d487bf68b5f7096440019"',}

    def assert_params(self, cmdline, result):
        self.assert_params_for_cmd(cmdline, result, expected_rc=0,
                                   ignore_params=['payload'])
        self.assertIsInstance(self.last_params['payload'].getvalue(),
                              file_type)

    def test_simple(self):
        cmdline = self.prefix
        cmdline += self.file_path
        cmdline += ' s3://mybucket/mykey'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {}}
        self.assert_params(cmdline, result)

    def test_sse(self):
        cmdline = self.prefix
        cmdline += self.file_path
        cmdline += ' s3://mybucket/mykey'
        cmdline += ' --sse'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {'x-amz-server-side-encryption': 'AES256'}}
        self.assert_params(cmdline, result)

    def test_storage_class(self):
        cmdline = self.prefix
        cmdline += self.file_path
        cmdline += ' s3://mybucket/mykey'
        cmdline += ' --storage-class REDUCED_REDUNDANCY'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {'x-amz-storage-class': 'REDUCED_REDUNDANCY'}}
        self.assert_params(cmdline, result)

    def test_website_redirect(self):
        cmdline = self.prefix
        cmdline += self.file_path
        cmdline += ' s3://mybucket/mykey'
        cmdline += ' --website-redirect /foobar'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {'x-amz-website-redirect-location': '/foobar'}}
        self.assert_params(cmdline, result)

    def test_acl(self):
        cmdline = self.prefix
        cmdline += self.file_path
        cmdline += ' s3://mybucket/mykey'
        cmdline += ' --acl public-read'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {'x-amz-acl': 'public-read'}}
        self.assert_params(cmdline, result)

    def test_content_params(self):
        cmdline = self.prefix
        cmdline += self.file_path
        cmdline += ' s3://mybucket/mykey'
        cmdline += ' --content-encoding x-gzip'
        cmdline += ' --content-language piglatin'
        cmdline += ' --cache-control max-age=3600,must-revalidate'
        cmdline += ' --content-disposition attachment;filename="fname.ext"'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {'Content-Encoding': 'x-gzip',
                              'Content-Language': 'piglatin',
                              'Content-Disposition': 'attachment;filename="fname.ext"',
                              'Cache-Control': 'max-age=3600,must-revalidate'}}
        self.assert_params(cmdline, result)

    def test_grants(self):
        cmdline = self.prefix
        cmdline += self.file_path
        cmdline += ' s3://mybucket/mykey'
        cmdline += ' --grants read=bob'
        cmdline += ' full=alice'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {'x-amz-grant-full-control': 'alice',
                              'x-amz-grant-read': 'bob'}}
        self.assert_params(cmdline, result)

    def test_grants_bad(self):
        cmdline = self.prefix
        cmdline += self.file_path
        cmdline += ' s3://mybucket/mykey'
        cmdline += ' --grants read:bob'
        self.assert_params_for_cmd(cmdline, expected_rc=1,
                                   ignore_params=['payload'])

    def test_content_type(self):
        cmdline = self.prefix
        cmdline += self.file_path
        cmdline += ' s3://mybucket/mykey'
        cmdline += ' --content-type text/xml'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {'Content-Type': 'text/xml'}}
        self.assert_params(cmdline, result)


if __name__ == "__main__":
    unittest.main()


########NEW FILE########
__FILENAME__ = test_cp_command
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest, FileCreator
import re

import mock
import six


class TestCPCommand(BaseAWSCommandParamsTest):

    prefix = 's3 cp '

    def setUp(self):
        super(TestCPCommand, self).setUp()
        self.files = FileCreator()

    def tearDown(self):
        super(TestCPCommand, self).tearDown()
        self.files.remove_all()

    def test_operations_used_in_upload(self):
        full_path = self.files.create_file('foo.txt', 'mycontent')
        cmdline = '%s %s s3://bucket/key.txt' % (self.prefix, full_path)
        self.parsed_responses = [{'ETag': '"c8afdb36c52cf4727836669019e69222"'}]
        self.run_cmd(cmdline, expected_rc=0)
        # The only operation we should have called is PutObject.
        self.assertEqual(len(self.operations_called), 1, self.operations_called)
        self.assertEqual(self.operations_called[0][0].name, 'PutObject')

    def test_key_name_added_when_only_bucket_provided(self):
        full_path = self.files.create_file('foo.txt', 'mycontent')
        cmdline = '%s %s s3://bucket/' % (self.prefix, full_path)
        self.parsed_responses = [{'ETag': '"c8afdb36c52cf4727836669019e69222"'}]
        self.run_cmd(cmdline, expected_rc=0)
        # The only operation we should have called is PutObject.
        self.assertEqual(len(self.operations_called), 1, self.operations_called)
        self.assertEqual(self.operations_called[0][0].name, 'PutObject')
        self.assertEqual(self.operations_called[0][1]['key'], 'foo.txt')
        self.assertEqual(self.operations_called[0][1]['bucket'], 'bucket')

    def test_trailing_slash_appended(self):
        full_path = self.files.create_file('foo.txt', 'mycontent')
        # Here we're saying s3://bucket instead of s3://bucket/
        # This should still work the same as if we added the trailing slash.
        cmdline = '%s %s s3://bucket' % (self.prefix, full_path)
        self.parsed_responses = [{'ETag': '"c8afdb36c52cf4727836669019e69222"'}]
        self.run_cmd(cmdline, expected_rc=0)
        # The only operation we should have called is PutObject.
        self.assertEqual(len(self.operations_called), 1, self.operations_called)
        self.assertEqual(self.operations_called[0][0].name, 'PutObject')
        self.assertEqual(self.operations_called[0][1]['key'], 'foo.txt')
        self.assertEqual(self.operations_called[0][1]['bucket'], 'bucket')

    def test_operations_used_in_download_file(self):
        self.parsed_responses = [
            {"ContentLength": "100", "LastModified": "00:00:00Z"},
            {'ETag': '"foo-1"', 'Body': six.BytesIO(b'foo')},
        ]
        cmdline = '%s s3://bucket/key.txt %s' % (self.prefix,
                                                 self.files.rootdir)
        self.run_cmd(cmdline, expected_rc=0)
        # The only operations we should have called are HeadObject/GetObject.
        self.assertEqual(len(self.operations_called), 2, self.operations_called)
        self.assertEqual(self.operations_called[0][0].name, 'HeadObject')
        self.assertEqual(self.operations_called[1][0].name, 'GetObject')

    def test_operations_used_in_recursive_download(self):
        self.parsed_responses = [
            {'ETag': '"foo-1"', 'Contents': [], 'CommonPrefixes': []},
        ]
        cmdline = '%s s3://bucket/key.txt %s --recursive' % (
            self.prefix, self.files.rootdir)
        self.run_cmd(cmdline, expected_rc=0)
        # We called ListObjects but had no objects to download, so
        # we only have a single ListObjects operation being called.
        self.assertEqual(len(self.operations_called), 1, self.operations_called)
        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_executor
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
import tempfile
import shutil
import mock
from six.moves import queue

from awscli.testutils import unittest, temporary_file
from awscli.customizations.s3.executor import IOWriterThread
from awscli.customizations.s3.executor import ShutdownThreadRequest
from awscli.customizations.s3.executor import Executor
from awscli.customizations.s3.utils import IORequest, IOCloseRequest


class TestIOWriterThread(unittest.TestCase):

    def setUp(self):
        self.queue = queue.Queue()
        self.io_thread = IOWriterThread(self.queue)
        self.temp_dir = tempfile.mkdtemp()
        self.filename = os.path.join(self.temp_dir, 'foo')
        # Create the file, since IOWriterThread expects
        # files to exist, we need to first creat the file.
        open(self.filename, 'w').close()

    def tearDown(self):
        shutil.rmtree(self.temp_dir)

    def test_handles_io_request(self):
        self.queue.put(IORequest(self.filename, 0, b'foobar'))
        self.queue.put(IOCloseRequest(self.filename))
        self.queue.put(ShutdownThreadRequest())
        self.io_thread.run()
        with open(self.filename, 'rb') as f:
            self.assertEqual(f.read(), b'foobar')

    def test_out_of_order_io_requests(self):
        self.queue.put(IORequest(self.filename, 6, b'morestuff'))
        self.queue.put(IORequest(self.filename, 0, b'foobar'))
        self.queue.put(IOCloseRequest(self.filename))
        self.queue.put(ShutdownThreadRequest())
        self.io_thread.run()
        with open(self.filename, 'rb') as f:
            self.assertEqual(f.read(), b'foobarmorestuff')

    def test_multiple_files_in_queue(self):
        second_file = os.path.join(self.temp_dir, 'bar')
        open(second_file, 'w').close()
        self.queue.put(IORequest(self.filename, 0, b'foobar'))
        self.queue.put(IORequest(second_file, 0, b'otherstuff'))
        self.queue.put(IOCloseRequest(second_file))
        self.queue.put(IOCloseRequest(self.filename))
        self.queue.put(ShutdownThreadRequest())

        self.io_thread.run()
        with open(self.filename, 'rb') as f:
            self.assertEqual(f.read(), b'foobar')
        with open(second_file, 'rb') as f:
            self.assertEqual(f.read(), b'otherstuff')


class TestExecutor(unittest.TestCase):
    def test_shutdown_does_not_hang(self):
        executor = Executor(2, queue.Queue(), False,
                            10, queue.Queue(maxsize=1))
        with temporary_file('rb+') as f:
            executor.start()
            class FloodIOQueueTask(object):
                PRIORITY = 10

                def __call__(self):
                    for i in range(50):
                        executor.write_queue.put(IORequest(f.name, 0, b'foobar'))
            executor.submit(FloodIOQueueTask())
            executor.initiate_shutdown()
            executor.wait_until_shutdown()
            self.assertEqual(open(f.name, 'rb').read(), b'foobar')

########NEW FILE########
__FILENAME__ = test_fileformat
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
import unittest

from awscli.customizations.s3.fileformat import FileFormat


class FileFormatTest(unittest.TestCase):
    def setUp(self):
        self.file_format = FileFormat()

    def test_op_dir(self):
        """
        Format a paths for directory operation.  There are slashes at the
        end of the paths.
        """
        src = '.' + os.sep
        dest = 's3://kyknapp/golfVid/'
        parameters = {'dir_op': True}
        files = self.file_format.format(src, dest, parameters)

        ref_files = {'src': {'path': os.path.abspath(src) + os.sep,
                             'type': 'local'},
                     'dest': {'path': 'kyknapp/golfVid/', 'type': 's3'},
                     'dir_op': True, 'use_src_name': True}
        self.assertEqual(files, ref_files)

    def test_op_dir_noslash(self):
        """
        Format a paths for directory operation.  There are no slashes at the
        end of the paths.
        """
        src = '.'
        dest = 's3://kyknapp/golfVid'
        parameters = {'dir_op': True}
        files = self.file_format.format(src, dest, parameters)

        ref_files = {'src': {'path': os.path.abspath(src) + os.sep,
                             'type': 'local'},
                     'dest': {'path': 'kyknapp/golfVid/', 'type': 's3'},
                     'dir_op': True, 'use_src_name': True}
        self.assertEqual(files, ref_files)

    def test_local_use_src_name(self):
        """
        No directory operation. S3 source name given. Existing local
        destination directory given.
        """
        src = 's3://kyknapp/golfVid/hello.txt'
        dest = '.'
        parameters = {'dir_op': False}
        files = self.file_format.format(src, dest, parameters)

        ref_files = {'src': {'path': 'kyknapp/golfVid/hello.txt',
                             'type': 's3'},
                     'dest': {'path': os.path.abspath(dest) + os.sep,
                              'type': 'local'},
                     'dir_op': False, 'use_src_name': True}
        self.assertEqual(files, ref_files)

    def test_local_noexist_file(self):
        """
        No directory operation. S3 source name given. Nonexisting local
        destination directory given.
        """
        src = 's3://kyknapp/golfVid/hello.txt'
        dest = 'someFile' + os.sep
        parameters = {'dir_op': False}
        files = self.file_format.format(src, dest, parameters)

        ref_files = {'src': {'path': 'kyknapp/golfVid/hello.txt',
                             'type': 's3'},
                     'dest': {'path': os.path.abspath(dest) + os.sep,
                              'type': 'local'},
                     'dir_op': False, 'use_src_name': True}
        self.assertEqual(files, ref_files)

    def test_local_keep_dest_name(self):
        """
        No directory operation. S3 source name given. Local
        destination filename given.
        """
        src = 's3://kyknapp/golfVid/hello.txt'
        dest = 'hello.txt'
        parameters = {'dir_op': False}
        files = self.file_format.format(src, dest, parameters)

        ref_files = {'src': {'path': 'kyknapp/golfVid/hello.txt',
                             'type': 's3'},
                     'dest': {'path': os.path.abspath(dest),
                              'type': 'local'},
                     'dir_op': False, 'use_src_name': False}
        self.assertEqual(files, ref_files)

    def test_s3_use_src_name(self):
        """
        No directory operation. Local source name given. S3
        common prefix given.
        """
        src = 'fileformat_test.py'
        dest = 's3://kyknapp/golfVid/'
        parameters = {'dir_op': False}
        files = self.file_format.format(src, dest, parameters)

        ref_files = {'src': {'path': os.path.abspath(src),
                             'type': 'local'},
                     'dest': {'path': 'kyknapp/golfVid/', 'type': 's3'},
                     'dir_op': False, 'use_src_name': True}
        self.assertEqual(files, ref_files)

    def test_s3_keep_dest_name(self):
        """
        No directory operation. Local source name given. S3
        full key given.
        """
        src = 'fileformat_test.py'
        dest = 's3://kyknapp/golfVid/file.py'
        parameters = {'dir_op': False}
        files = self.file_format.format(src, dest, parameters)

        ref_files = {'src': {'path': os.path.abspath(src),
                             'type': 'local'},
                     'dest': {'path': 'kyknapp/golfVid/file.py', 'type': 's3'},
                     'dir_op': False, 'use_src_name': False}
        self.assertEqual(files, ref_files)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_filegenerator
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
from awscli.testutils import unittest
import tempfile
import shutil

import six
import mock

from awscli.customizations.s3.filegenerator import FileGenerator, \
    FileDecodingError
from awscli.customizations.s3.fileinfo import FileInfo
from awscli.customizations.s3.utils import get_file_stat
import botocore.session
from tests.unit.customizations.s3 import make_loc_files, clean_loc_files, \
    make_s3_files, s3_cleanup, compare_files
from tests.unit.customizations.s3.fake_session import FakeSession


class LocalFileGeneratorTest(unittest.TestCase):
    def setUp(self):
        self.local_file = six.text_type(os.path.abspath('.') + os.sep + 'some_directory' \
            + os.sep + 'text1.txt')
        self.local_dir = six.text_type(os.path.abspath('.') + os.sep + 'some_directory' \
            + os.sep)
        self.session = FakeSession()
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        self.files = make_loc_files()

    def tearDown(self):
        clean_loc_files(self.files)

    def test_local_file(self):
        """
        Generate a single local file.
        """
        input_local_file = {'src': {'path': self.local_file,
                                    'type': 'local'},
                            'dest': {'path': 'bucket/text1.txt',
                                     'type': 's3'},
                            'dir_op': False, 'use_src_name': False}
        params = {'region': 'us-east-1'}
        files = FileGenerator(self.service,
                              self.endpoint, '', params).call(input_local_file)
        result_list = []
        for filename in files:
            result_list.append(filename)
        size, last_update = get_file_stat(self.local_file)
        file_info = FileInfo(src=self.local_file, dest='bucket/text1.txt',
                             compare_key='text1.txt', size=size,
                             last_update=last_update, src_type='local',
                             dest_type='s3', operation_name='',
                             service=None, endpoint=None)
        ref_list = [file_info]
        self.assertEqual(len(result_list), len(ref_list))
        for i in range(len(result_list)):
            compare_files(self, result_list[i], ref_list[i])

    def test_local_directory(self):
        """
        Generate an entire local directory.
        """
        input_local_dir = {'src': {'path': self.local_dir,
                                   'type': 'local'},
                           'dest': {'path': 'bucket/',
                                    'type': 's3'},
                           'dir_op': True, 'use_src_name': True}
        params = {'region': 'us-east-1'}
        files = FileGenerator(self.service, self.endpoint,
                              '', params).call(input_local_dir)
        result_list = []
        for filename in files:
            result_list.append(filename)
        size, last_update = get_file_stat(self.local_file)
        file_info = FileInfo(src=self.local_file, dest='bucket/text1.txt',
                             compare_key='text1.txt', size=size,
                             last_update=last_update, src_type='local',
                             dest_type='s3', operation_name='',
                             service=None, endpoint=None)
        path = self.local_dir + 'another_directory' + os.sep \
            + 'text2.txt'
        size, last_update = get_file_stat(path)
        file_info2 = FileInfo(src=path,
                              dest='bucket/another_directory/text2.txt',
                              compare_key='another_directory/text2.txt',
                              size=size, last_update=last_update,
                              src_type='local',
                              dest_type='s3', operation_name='',
                              service=None, endpoint=None)
        ref_list = [file_info2, file_info]
        self.assertEqual(len(result_list), len(ref_list))
        for i in range(len(result_list)):
            compare_files(self, result_list[i], ref_list[i])


class TestListFilesLocally(unittest.TestCase):
    maxDiff = None

    def setUp(self):
        self.directory = six.text_type(tempfile.mkdtemp())

    def tearDown(self):
        shutil.rmtree(self.directory)

    @mock.patch('os.listdir')
    def test_error_raised_on_decoding_error(self, listdir_mock):
        # On Python3, sys.getdefaultencoding
        file_generator = FileGenerator(None, None, None, None)
        # utf-8 encoding for U+2713.
        listdir_mock.return_value = [b'\xe2\x9c\x93']
        with self.assertRaises(FileDecodingError):
            list(file_generator.list_files(self.directory, dir_op=True))

    def test_list_files_is_in_sorted_order(self):
        p = os.path.join
        open(p(self.directory, 'test-123.txt'), 'w').close()
        open(p(self.directory, 'test-321.txt'), 'w').close()
        open(p(self.directory, 'test123.txt'), 'w').close()
        open(p(self.directory, 'test321.txt'), 'w').close()
        os.mkdir(p(self.directory, 'test'))
        open(p(self.directory, 'test', 'foo.txt'), 'w').close()

        file_generator = FileGenerator(None, None, None, None)
        values = list(el[0] for el in file_generator.list_files(
            self.directory, dir_op=True))
        self.assertEqual(values, list(sorted(values)))

    def test_list_local_files_with_unicode_chars(self):
        p = os.path.join
        open(p(self.directory, u'a'), 'w').close()
        open(p(self.directory, u'a\u0300'), 'w').close()
        open(p(self.directory, u'a\u0300-1'), 'w').close()
        open(p(self.directory, u'a\u03001'), 'w').close()
        open(p(self.directory, u'z'), 'w').close()
        open(p(self.directory, u'\u00e6'), 'w').close()
        os.mkdir(p(self.directory, u'a\u0300a'))
        open(p(self.directory, u'a\u0300a', u'a'), 'w').close()
        open(p(self.directory, u'a\u0300a', u'z'), 'w').close()
        open(p(self.directory, u'a\u0300a', u'\u00e6'), 'w').close()

        file_generator = FileGenerator(None, None, None, None)
        values = list(el[0] for el in file_generator.list_files(
            self.directory, dir_op=True))
        expected_order = [os.path.join(self.directory, el) for el in [
            u"a",
            u"a\u0300",
            u"a\u0300-1",
            u"a\u03001",
            u"a\u0300a%sa" % os.path.sep,
            u"a\u0300a%sz" % os.path.sep,
            u"a\u0300a%s\u00e6" % os.path.sep,
            u"z",
            u"\u00e6"
        ]]
        self.assertEqual(values, expected_order)


class S3FileGeneratorTest(unittest.TestCase):
    def setUp(self):
        self.session = FakeSession()
        self.bucket = make_s3_files(self.session)
        self.file1 = self.bucket + '/' + 'text1.txt'
        self.file2 = self.bucket + '/' + 'another_directory/text2.txt'
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')

    def tearDown(self):
        s3_cleanup(self.bucket, self.session)

    def test_s3_file(self):
        """
        Generate a single s3 file
        Note: Size and last update are not tested because s3 generates them.
        """
        input_s3_file = {'src': {'path': self.file1, 'type': 's3'},
                         'dest': {'path': 'text1.txt', 'type': 'local'},
                         'dir_op': False, 'use_src_name': False}
        params = {'region': 'us-east-1'}
        files = FileGenerator(self.service, self.endpoint, '', params).call(input_s3_file)
        result_list = []
        for filename in files:
            result_list.append(filename)
        file_info = FileInfo(src=self.file1, dest='text1.txt',
                             compare_key='text1.txt',
                             size=result_list[0].size,
                             last_update=result_list[0].last_update,
                             src_type='s3',
                             dest_type='local', operation_name='',
                             service=None, endpoint=None)

        ref_list = [file_info]
        self.assertEqual(len(result_list), len(ref_list))
        for i in range(len(result_list)):
            compare_files(self, result_list[i], ref_list[i])

    def test_s3_directory(self):
        """
        Generates s3 files under a common prefix. Also it ensures that
        zero size files are ignored.
        Note: Size and last update are not tested because s3 generates them.
        """
        input_s3_file = {'src': {'path': self.bucket + '/', 'type': 's3'},
                         'dest': {'path': '', 'type': 'local'},
                         'dir_op': True, 'use_src_name': True}
        params = {'region': 'us-east-1'}
        files = FileGenerator(self.service, self.endpoint, '', params).call(input_s3_file)
        result_list = []
        for filename in files:
            result_list.append(filename)
        file_info = FileInfo(src=self.file2,
                             dest='another_directory' + os.sep + 'text2.txt',
                             compare_key='another_directory/text2.txt',
                             size=result_list[0].size,
                             last_update=result_list[0].last_update,
                             src_type='s3',
                             dest_type='local', operation_name='',
                             service=None, endpoint=None)
        file_info2 = FileInfo(src=self.file1,
                              dest='text1.txt',
                              compare_key='text1.txt',
                              size=result_list[1].size,
                              last_update=result_list[1].last_update,
                              src_type='s3',
                              dest_type='local', operation_name='',
                              service=None, endpoint=None)

        ref_list = [file_info, file_info2]
        self.assertEqual(len(result_list), len(ref_list))
        for i in range(len(result_list)):
            compare_files(self, result_list[i], ref_list[i])

    def test_s3_delete_directory(self):
        """
        Generates s3 files under a common prefix. Also it ensures that
        the directory itself is included because it is a delete command
        Note: Size and last update are not tested because s3 generates them.
        """
        input_s3_file = {'src': {'path': self.bucket + '/', 'type': 's3'},
                         'dest': {'path': '', 'type': 'local'},
                         'dir_op': True, 'use_src_name': True}
        params = {'region': 'us-east-1'}
        files = FileGenerator(self.service, self.endpoint, 'delete', params).call(
            input_s3_file)
        result_list = []
        for filename in files:
            result_list.append(filename)

        file_info1 = FileInfo(src=self.bucket + '/another_directory/',
                              dest='another_directory' + os.sep,
                              compare_key='another_directory/',
                              size=result_list[0].size,
                              last_update=result_list[0].last_update,
                              src_type='s3',
                              dest_type='local', operation_name='delete',
                              service=None, endpoint=None)
        file_info2 = FileInfo(src=self.file2,
                              dest='another_directory' + os.sep + 'text2.txt',
                              compare_key='another_directory/text2.txt',
                              size=result_list[1].size,
                              last_update=result_list[1].last_update,
                              src_type='s3',
                              dest_type='local', operation_name='delete',
                              service=None, endpoint=None)
        file_info3 = FileInfo(src=self.file1,
                              dest='text1.txt',
                              compare_key='text1.txt',
                              size=result_list[2].size,
                              last_update=result_list[2].last_update,
                              src_type='s3',
                              dest_type='local', operation_name='delete',
                              service=None, endpoint=None)

        ref_list = [file_info1, file_info2, file_info3]
        self.assertEqual(len(result_list), len(ref_list))
        for i in range(len(result_list)):
            compare_files(self, result_list[i], ref_list[i])


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_fileinfo
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
import tempfile
import shutil
from datetime import datetime
from hashlib import md5

import six
import mock

from awscli.testutils import unittest
from awscli.customizations.s3 import fileinfo


class TestSaveFile(unittest.TestCase):
    def setUp(self):
        self.tempdir = tempfile.mkdtemp()
        self.filename = os.path.join(self.tempdir, 'dir1', 'dir2', 'foo.txt')
        etag = md5()
        etag.update(b'foobar')
        etag = etag.hexdigest()
        self.response_data = {
            'Body': six.BytesIO(b'foobar'),
            'ETag': '"%s"' % etag,
        }
        self.last_update = datetime.now()

    def tearDown(self):
        shutil.rmtree(self.tempdir)

    def test_save_file(self):
        fileinfo.save_file(self.filename, self.response_data, self.last_update)
        self.assertTrue(os.path.isfile(self.filename))

    def test_save_file_dir_exists(self):
        os.makedirs(os.path.dirname(self.filename))
        # We should still be able to save the file.
        fileinfo.save_file(self.filename, self.response_data, self.last_update)
        self.assertTrue(os.path.isfile(self.filename))

    @mock.patch('os.makedirs')
    def test_makedir_other_exception(self, makedirs):
        # If makedirs() raises any other kind of exception, we should
        # propogate the exception.
        makedirs.side_effect = RuntimeError()
        with self.assertRaises(RuntimeError):
            fileinfo.save_file(self.filename, self.response_data,
                               self.last_update)
        self.assertFalse(os.path.isfile(self.filename))

########NEW FILE########
__FILENAME__ = test_filters
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
from awscli.testutils import unittest
import platform

from awscli.customizations.s3.fileinfo import FileInfo
from awscli.customizations.s3.filters import Filter


def platform_path(filepath):
    # Convert posix platforms to windows platforms.
    if platform.system().lower() == 'windows':
        filepath = filepath.replace('/', os.sep)
        filepath = 'C:' + filepath
    return filepath


class FiltersTest(unittest.TestCase):
    def setUp(self):
        self.local_files = [
            self.file_info('test.txt'),
            self.file_info('test.jpg'),
            self.file_info(os.path.join('directory', 'test.jpg')),
        ]
        self.s3_files = [
            self.file_info('bucket/test.txt', src_type='s3'),
            self.file_info('bucket/test.jpg', src_type='s3'),
            self.file_info('bucket/key/test.jpg', src_type='s3'),
        ]

    def file_info(self, filename, src_type='local'):
        if src_type == 'local':
            filename = os.path.abspath(filename)
            dest_type = 's3'
        else:
            dest_type = 'local'
        return FileInfo(src=filename, dest='',
                        compare_key='', size=10,
                        last_update=0, src_type=src_type,
                        dest_type=dest_type, operation_name='',
                        service=None, endpoint=None)

    def create_filter(self, filters=None, root=None, dst_root=None):
        if root is None:
            root = os.getcwd()
        if filters is None:
            filters = {}
        if dst_root is None:
            dst_root = 'bucket'
        return Filter(filters, root, dst_root)

    def test_no_filter(self):
        exc_inc_filter = self.create_filter()
        matched_files = list(exc_inc_filter.call(self.local_files))
        self.assertEqual(matched_files, self.local_files)

        matched_files2 = list(exc_inc_filter.call(self.s3_files))
        self.assertEqual(matched_files2, self.s3_files)

    def test_include(self):
        patterns = [['include', '*.txt']]
        include_filter = self.create_filter([['include', '*.txt']])
        matched_files = list(include_filter.call(self.local_files))
        self.assertEqual(matched_files, self.local_files)

        matched_files2 = list(include_filter.call(self.s3_files))
        self.assertEqual(matched_files2, self.s3_files)

    def test_exclude(self):
        exclude_filter = self.create_filter([['exclude', '*']])
        matched_files = list(exclude_filter.call(self.local_files))
        self.assertEqual(matched_files, [])

        matched_files = list(exclude_filter.call(self.s3_files))
        self.assertEqual(matched_files, [])

    def test_exclude_with_dst_root(self):
        exclude_filter = self.create_filter([['exclude', '*.txt']],
                                            dst_root='bucket')
        matched_files = list(exclude_filter.call(self.local_files))
        b = os.path.basename
        self.assertNotIn('test.txt', [b(f.src) for f in matched_files])
        # Same filter should match the dst files.
        matched_files = list(exclude_filter.call(self.s3_files))
        self.assertNotIn('test.txt', [b(f.src) for f in matched_files])

    def test_exclude_include(self):
        patterns = [['exclude', '*'], ['include', '*.txt']]
        exclude_include_filter = self.create_filter(patterns)
        matched_files = list(exclude_include_filter.call(self.local_files))
        self.assertEqual(matched_files, [self.local_files[0]])

        matched_files = list(exclude_include_filter.call(self.s3_files))
        self.assertEqual(matched_files, [self.s3_files[0]])

    def test_include_exclude(self):
        patterns = [['include', '*.txt'], ['exclude', '*']]
        exclude_all_filter = self.create_filter(patterns)
        matched_files = list(exclude_all_filter.call(self.local_files))
        self.assertEqual(matched_files, [])

        matched_files = list(exclude_all_filter.call(self.s3_files))
        self.assertEqual(matched_files, [])

    def test_prefix_filtering_consistent(self):
        # The same filter should work for both local and remote files.
        # So if I have a directory with 2 files:
        local_files = [
            self.file_info('test1.txt'),
            self.file_info('nottest1.txt'),
        ]
        # And the same 2 files remote (note that the way FileInfo objects
        # are constructed, we'll have the bucket name but no leading '/'
        # character):
        remote_files = [
            self.file_info('bucket/test1.txt', src_type='s3'),
            self.file_info('bucket/nottest1.txt', src_type='s3'),
        ]
        # If I apply the filter to the local to the local files.
        exclude_filter = self.create_filter([['exclude', 't*']])
        filtered_files = list(exclude_filter.call(local_files))
        self.assertEqual(len(filtered_files), 1)
        self.assertEqual(os.path.basename(filtered_files[0].src),
                         'nottest1.txt')

        # I should get the same result if I apply the same filter to s3
        # objects.
        exclude_filter = self.create_filter([['exclude', 't*']], root='bucket')
        same_filtered_files = list(exclude_filter.call(remote_files))
        self.assertEqual(len(same_filtered_files), 1)
        self.assertEqual(os.path.basename(same_filtered_files[0].src),
                         'nottest1.txt')

    def test_bucket_exclude_with_prefix(self):
        s3_files = [
            self.file_info('bucket/dir1/key1.txt', src_type='s3'),
            self.file_info('bucket/dir1/key2.txt', src_type='s3'),
            self.file_info('bucket/dir1/notkey3.txt', src_type='s3'),
        ]
        filtered_files = list(
            self.create_filter([['exclude', 'dir1/*']],
                               root='bucket').call(s3_files))
        self.assertEqual(filtered_files, [])

        key_files = list(
            self.create_filter([['exclude', 'dir1/key*']],
                               root='bucket').call(s3_files))
        self.assertEqual(len(key_files), 1)
        self.assertEqual(key_files[0].src, 'bucket/dir1/notkey3.txt')

    def test_root_dir(self):
        p = platform_path
        local_files = [self.file_info(p('/foo/bar/baz.txt'), src_type='local')]
        local_filter = self.create_filter([['exclude', 'baz.txt']],
                                          root=p('/foo/bar/'))
        filtered = list(local_filter.call(local_files))
        self.assertEqual(filtered, [])

        # However, if we're at the root of /foo', then this filter won't match.
        local_filter = self.create_filter([['exclude', 'baz.txt']],
                                          root=p('/foo/'))
        filtered = list(local_filter.call(local_files))
        self.assertEqual(len(filtered), 1)
        self.assertEqual(filtered[0].src, p('/foo/bar/baz.txt'))


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_ls_command
#!/usr/bin/env python
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
from dateutil import parser, tz

class TestLSCommand(BaseAWSCommandParamsTest):

    def test_operations_used_in_recursive_list(self):
        time_utc = "2014-01-09T20:45:49.000Z"
        self.parsed_responses = [{"CommonPrefixes": [], "Contents": [
            {"Key": "foo/bar.txt", "Size": 100,
             "LastModified": time_utc}]}]
        stdout, _, _ = self.run_cmd('s3 ls s3://bucket/ --recursive', expected_rc=0)
        call_args = self.operations_called[0][1]
        # We should not be calling the args with any delimiter because we
        # want a recursive listing.
        self.assertEqual(call_args['prefix'], '')
        self.assertEqual(call_args['bucket'], 'bucket')
        self.assertNotIn('delimiter', call_args)
        # Time is stored in UTC timezone, but the actual time displayed
        # is specific to your tzinfo, so shift the timezone to your local's.
        time_local = parser.parse(time_utc).astimezone(tz.tzlocal())
        self.assertEqual(
            stdout, '%s        100 foo/bar.txt\n'%time_local.strftime('%Y-%m-%d %H:%M:%S'))

########NEW FILE########
__FILENAME__ = test_s3
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0e
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import argparse
import os
from six import StringIO
import sys
from awscli.testutils import unittest
import mock

import botocore.session
from mock import Mock, MagicMock, patch

from awscli.customizations.s3.s3 import AppendFilter, \
    awscli_initialize, add_s3, \
    S3, S3SubCommand, S3Parameter, CommandArchitecture, CommandParameters, \
    ListCommand
from tests.unit.customizations.s3 import make_loc_files, clean_loc_files, \
    make_s3_files, s3_cleanup, S3HandlerBaseTest
from tests.unit.customizations.s3.fake_session import FakeSession
from awscli.testutils import BaseAWSHelpOutputTest


class AppendFilterTest(unittest.TestCase):
    def test_call(self):
        parser = argparse.ArgumentParser()

        parser.add_argument('--include', action=AppendFilter, nargs=1,
                            dest='path')
        parser.add_argument('--exclude', action=AppendFilter, nargs=1,
                            dest='path')
        parsed_args = parser.parse_args(['--include', 'a', '--exclude', 'b'])
        self.assertEqual(parsed_args.path, [['--include', 'a'],
                                            ['--exclude', 'b']])


class FakeArgs(object):
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

class AWSInitializeTest(unittest.TestCase):
    """
    This test ensures that all events are correctly registered such that
    all of the commands can be run.
    """
    def setUp(self):
        self.cli = Mock()

    def test_initialize(self):
        awscli_initialize(self.cli)
        reference = []
        reference.append("building-command-table.main")
        reference.append("building-operation-table.s3")
        reference.append("doc-examples.S3.*")
        # TODO: Change this test, it has to be updated everytime we
        # add a new command.
        cmds = ['mv', 'rm', 'ls', 'rb', 'mb', 'cp', 'sync', 'website']
        for cmd in cmds:
            reference.append("building-parameter-table.s3." + cmd)
        for arg in self.cli.register.call_args_list:
            self.assertIn(arg[0][0], reference)


class CreateTablesTest(unittest.TestCase):
    def test_s3(self):
        """
        Ensures that the table for the service was created properly.
        Also ensures the original s3 service is renamed to ``s3api``.
        """
        s3_service = Mock()
        s3_service.name = 's3'
        self.services = {'s3': s3_service}
        add_s3(self.services, True)
        orig_service = self.services.pop('s3api')
        self.assertEqual(orig_service, s3_service)
        for service in self.services.keys():
            self.assertIn(service, ['s3'])


class S3SubCommandTest(unittest.TestCase):
    """
    This checks top make sure that the S3SubCommand properly handles commands
    passed to it.
    """
    def setUp(self):
        self.session = FakeSession()
        module = 'awscli.customizations.s3.s3.CommandArchitecture'
        self.cmd_arc_patch = patch(module)
        self.cmd_arc_mock = self.cmd_arc_patch.start()
        self.cmd_arc_mock.run.return_value = "Passed"

    def tearDown(self):
        self.cmd_arc_patch.stop()

    def test_call_error(self):
        """
        This checks to make sure an improper command throws an
        exception.
        """
        s3_command = S3SubCommand('cp', self.session,  {'nargs': 2})
        with self.assertRaisesRegexp(ValueError, 'Unknown options'):
            s3_command(['foo', 's3://', '--sfdf'], [])


class S3ParameterTest(unittest.TestCase):
    """
    Tests the ability to put parameters along with options into
    a parser using the S3Parameter class
    """
    def test_add_to_parser(self):
        s3_param = S3Parameter('test',
                               {'action': 'store_true', 'dest': 'destination'})
        parser = argparse.ArgumentParser()
        s3_param.add_to_parser(parser)
        parsed_args = parser.parse_args(['--test'])
        self.assertTrue(parsed_args.destination)


class CommandArchitectureTest(S3HandlerBaseTest):
    def setUp(self):
        super(CommandArchitectureTest, self).setUp()
        self.session = FakeSession()
        self.bucket = make_s3_files(self.session)
        self.loc_files = make_loc_files()
        self.output = StringIO()
        self.saved_stdout = sys.stdout
        sys.stdout = self.output

    def tearDown(self):
        self.output.close()
        sys.stdout = self.saved_stdout

        super(CommandArchitectureTest, self).setUp()
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_create_instructions(self):
        """
        This tests to make sure the instructions for any command is generated
        properly.
        """
        cmds = ['cp', 'mv', 'rm', 'sync', 'mb', 'rb']

        instructions = {'cp': ['file_generator', 's3_handler'],
                        'mv': ['file_generator', 's3_handler'],
                        'rm': ['file_generator', 's3_handler'],
                        'sync': ['file_generator', 'comparator', 's3_handler'],
                        'mb': ['s3_handler'],
                        'rb': ['s3_handler']}

        params = {'filters': True, 'region': 'us-east-1', 'endpoint_url': None,
                  'verify_ssl': None}
        for cmd in cmds:
            cmd_arc = CommandArchitecture(self.session, cmd,
                                          {'region': 'us-east-1',
                                           'endpoint_url': None,
                                           'verify_ssl': None})
            cmd_arc.create_instructions()
            self.assertEqual(cmd_arc.instructions, instructions[cmd])

        # Test if there is a filter.
        cmd_arc = CommandArchitecture(self.session, 'cp', params)
        cmd_arc.create_instructions()
        self.assertEqual(cmd_arc.instructions, ['file_generator', 'filters',
                                                's3_handler'])

    def test_run_cp_put(self):
        # This ensures that the architecture sets up correctly for a ``cp`` put
        # command.  It is just just a dry run, but all of the components need
        # to be wired correctly for it to work.
        s3_file = 's3://' + self.bucket + '/' + 'text1.txt'
        local_file = self.loc_files[0]
        rel_local_file = os.path.relpath(local_file)
        filters = [['--include', '*']]
        params = {'dir_op': False, 'dryrun': True, 'quiet': False,
                  'src': local_file, 'dest': s3_file, 'filters': filters,
                  'paths_type': 'locals3', 'region': 'us-east-1',
                  'endpoint_url': None, 'verify_ssl': None}
        cmd_arc = CommandArchitecture(self.session, 'cp', params)
        cmd_arc.create_instructions()
        cmd_arc.run()
        output_str = "(dryrun) upload: %s to %s" % (rel_local_file, s3_file)
        self.assertIn(output_str, self.output.getvalue())

    def test_error_on_same_line_as_status(self):
        s3_file = 's3://' + 'bucket-does-not-exist' + '/' + 'text1.txt'
        local_file = self.loc_files[0]
        rel_local_file = os.path.relpath(local_file)
        filters = [['--include', '*']]
        params = {'dir_op': False, 'dryrun': False, 'quiet': False,
                  'src': local_file, 'dest': s3_file, 'filters': filters,
                  'paths_type': 'locals3', 'region': 'us-east-1',
                  'endpoint_url': None, 'verify_ssl': None}
        cmd_arc = CommandArchitecture(self.session, 'cp', params)
        cmd_arc.create_instructions()
        cmd_arc.run()
        # Also, we need to verify that the error message is on the *same* line
        # as the upload failed line, to make it easier to track.
        output_str = (
            "upload failed: %s to %s Error: Bucket does not exist\n" % (
                rel_local_file, s3_file))
        self.assertIn(output_str, self.output.getvalue())

    def test_run_cp_get(self):
        # This ensures that the architecture sets up correctly for a ``cp`` get
        # command.  It is just just a dry run, but all of the components need
        # to be wired correctly for it to work.
        s3_file = 's3://' + self.bucket + '/' + 'text1.txt'
        local_file = self.loc_files[0]
        rel_local_file = os.path.relpath(local_file)
        filters = [['--include', '*']]
        params = {'dir_op': False, 'dryrun': True, 'quiet': False,
                  'src': s3_file, 'dest': local_file, 'filters': filters,
                  'paths_type': 's3local', 'region': 'us-east-1',
                  'endpoint_url': None, 'verify_ssl': None}
        cmd_arc = CommandArchitecture(self.session, 'cp', params)
        cmd_arc.create_instructions()
        cmd_arc.run()
        output_str = "(dryrun) download: %s to %s" % (s3_file, rel_local_file)
        self.assertIn(output_str, self.output.getvalue())

    def test_run_cp_copy(self):
        # This ensures that the architecture sets up correctly for a ``cp`` copy
        # command.  It is just just a dry run, but all of the components need
        # to be wired correctly for it to work.
        s3_file = 's3://' + self.bucket + '/' + 'text1.txt'
        filters = [['--include', '*']]
        params = {'dir_op': False, 'dryrun': True, 'quiet': False,
                  'src': s3_file, 'dest': s3_file, 'filters': filters,
                  'paths_type': 's3s3', 'region': 'us-east-1',
                  'endpoint_url': None, 'verify_ssl': None}
        cmd_arc = CommandArchitecture(self.session, 'cp', params)
        cmd_arc.create_instructions()
        cmd_arc.run()
        output_str = "(dryrun) copy: %s to %s" % (s3_file, s3_file)
        self.assertIn(output_str, self.output.getvalue())

    def test_run_mv(self):
        # This ensures that the architecture sets up correctly for a ``mv``
        # command.  It is just just a dry run, but all of the components need
        # to be wired correctly for it to work.
        s3_file = 's3://' + self.bucket + '/' + 'text1.txt'
        filters = [['--include', '*']]
        params = {'dir_op': False, 'dryrun': True, 'quiet': False,
                  'src': s3_file, 'dest': s3_file, 'filters': filters,
                  'paths_type': 's3s3', 'region': 'us-east-1',
                  'endpoint_url': None, 'verify_ssl': None}
        cmd_arc = CommandArchitecture(self.session, 'mv', params)
        cmd_arc.create_instructions()
        cmd_arc.run()
        output_str = "(dryrun) move: %s to %s" % (s3_file, s3_file)
        self.assertIn(output_str, self.output.getvalue())

    def test_run_remove(self):
        # This ensures that the architecture sets up correctly for a ``rm``
        # command.  It is just just a dry run, but all of the components need
        # to be wired correctly for it to work.
        s3_file = 's3://' + self.bucket + '/' + 'text1.txt'
        filters = [['--include', '*']]
        params = {'dir_op': False, 'dryrun': True, 'quiet': False,
                  'src': s3_file, 'dest': s3_file, 'filters': filters,
                  'paths_type': 's3', 'region': 'us-east-1',
                  'endpoint_url': None, 'verify_ssl': None}
        cmd_arc = CommandArchitecture(self.session, 'rm', params)
        cmd_arc.create_instructions()
        cmd_arc.run()
        output_str = "(dryrun) delete: %s" % s3_file
        self.assertIn(output_str, self.output.getvalue())

    def test_run_sync(self):
        # This ensures that the architecture sets up correctly for a ``sync``
        # command.  It is just just a dry run, but all of the components need
        # to be wired correctly for it to work.
        s3_file = 's3://' + self.bucket + '/' + 'text1.txt'
        local_file = self.loc_files[0]
        s3_prefix = 's3://' + self.bucket + '/'
        local_dir = self.loc_files[3]
        rel_local_file = os.path.relpath(local_file)
        filters = [['--include', '*']]
        params = {'dir_op': True, 'dryrun': True, 'quiet': False,
                  'src': local_dir, 'dest': s3_prefix, 'filters': filters,
                  'paths_type': 'locals3', 'region': 'us-east-1',
                  'endpoint_url': None, 'verify_ssl': None}
        cmd_arc = CommandArchitecture(self.session, 'sync', params)
        cmd_arc.create_instructions()
        cmd_arc.run()
        output_str = "(dryrun) upload: %s to %s" % (rel_local_file, s3_file)
        self.assertIn(output_str, self.output.getvalue())

    def test_run_mb(self):
        # This ensures that the architecture sets up correctly for a ``rb``
        # command.  It is just just a dry run, but all of the components need
        # to be wired correctly for it to work.
        s3_prefix = 's3://' + self.bucket + '/'
        params = {'dir_op': True, 'dryrun': True, 'quiet': False,
                  'src': s3_prefix, 'dest': s3_prefix, 'paths_type': 's3',
                  'region': 'us-east-1', 'endpoint_url': None,
                  'verify_ssl': None}
        cmd_arc = CommandArchitecture(self.session, 'mb', params)
        cmd_arc.create_instructions()
        cmd_arc.run()
        output_str = "(dryrun) make_bucket: %s" % s3_prefix
        self.assertIn(output_str, self.output.getvalue())

    def test_run_rb(self):
        # This ensures that the architecture sets up correctly for a ``rb``
        # command.  It is just just a dry run, but all of the components need
        # to be wired correctly for it to work.
        s3_prefix = 's3://' + self.bucket + '/'
        params = {'dir_op': True, 'dryrun': True, 'quiet': False,
                  'src': s3_prefix, 'dest': s3_prefix, 'paths_type': 's3',
                  'region': 'us-east-1', 'endpoint_url': None,
                  'verify_ssl': None}
        cmd_arc = CommandArchitecture(self.session, 'rb', params)
        cmd_arc.create_instructions()
        rc = cmd_arc.run()
        output_str = "(dryrun) remove_bucket: %s" % s3_prefix
        self.assertIn(output_str, self.output.getvalue())
        self.assertEqual(rc, 0)

    def test_run_rb_nonzero_rc(self):
        # This ensures that the architecture sets up correctly for a ``rb``
        # command.  It is just just a dry run, but all of the components need
        # to be wired correctly for it to work.
        s3_prefix = 's3://' + self.bucket + '/'
        params = {'dir_op': True, 'dryrun': False, 'quiet': False,
                  'src': s3_prefix, 'dest': s3_prefix, 'paths_type': 's3',
                  'region': 'us-east-1', 'endpoint_url': None,
                  'verify_ssl': None}
        cmd_arc = CommandArchitecture(self.session, 'rb', params)
        cmd_arc.create_instructions()
        rc = cmd_arc.run()
        output_str = "remove_bucket failed: %s" % s3_prefix
        self.assertIn(output_str, self.output.getvalue())
        self.assertEqual(rc, 1)


class CommandParametersTest(unittest.TestCase):
    def setUp(self):
        self.environ = {}
        self.environ_patch = patch('os.environ', self.environ)
        self.environ_patch.start()
        self.session = FakeSession()
        self.mock = MagicMock()
        self.mock.get_config = MagicMock(return_value={'region': None})
        self.loc_files = make_loc_files()
        self.bucket = make_s3_files(self.session)

    def tearDown(self):
        self.environ_patch.stop()
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_check_path_type_pass(self):
        # This tests the class's ability to determine whether the correct
        # path types have been passed for a particular command.  It test every
        # possible combination that is correct for every command.
        cmds = {'cp': ['locals3', 's3s3', 's3local'],
                'mv': ['locals3', 's3s3', 's3local'],
                'rm': ['s3'], 'mb': ['s3'], 'rb': ['s3'],
                'sync': ['locals3', 's3s3', 's3local']}
        s3_file = 's3://' + self.bucket + '/' + 'text1.txt'
        local_file = self.loc_files[0]

        combos = {'s3s3': [s3_file, s3_file],
                  's3local': [s3_file, local_file],
                  'locals3': [local_file, s3_file],
                  's3': [s3_file],
                  'local': [local_file],
                  'locallocal': [local_file, local_file]}

        for cmd in cmds.keys():
            cmd_param = CommandParameters(self.session, cmd, {})
            cmd_param.add_region(mock.Mock())
            correct_paths = cmds[cmd]
            for path_args in correct_paths:
                cmd_param.check_path_type(combos[path_args])

    def test_check_path_type_fail(self):
        # This tests the class's ability to determine whether the correct
        # path types have been passed for a particular command. It test every
        # possible combination that is incorrect for every command.
        cmds = {'cp': ['local', 'locallocal', 's3'],
                'mv': ['local', 'locallocal', 's3'],
                'rm': ['local', 'locallocal', 's3s3', 'locals3', 's3local'],
                'ls': ['local', 'locallocal', 's3s3', 'locals3', 's3local'],
                'sync': ['local', 'locallocal', 's3'],
                'mb': ['local', 'locallocal', 's3s3', 'locals3', 's3local'],
                'rb': ['local', 'locallocal', 's3s3', 'locals3', 's3local']}
        s3_file = 's3://' + self.bucket + '/' + 'text1.txt'
        local_file = self.loc_files[0]

        combos = {'s3s3': [s3_file, s3_file],
                  's3local': [s3_file, local_file],
                  'locals3': [local_file, s3_file],
                  's3': [s3_file],
                  'local': [local_file],
                  'locallocal': [local_file, local_file]}

        for cmd in cmds.keys():
            cmd_param = CommandParameters(self.session, cmd, {})
            cmd_param.add_region(mock.Mock())
            wrong_paths = cmds[cmd]
            for path_args in wrong_paths:
                with self.assertRaises(TypeError):
                    cmd_param.check_path_type(combos[path_args])

    def test_check_src_path_pass(self):
        # This tests to see if all of the checks on the source path works.  It
        # does so by testing if s3 objects and and prefixes exist as well as
        # local files and directories.  All of these should not throw an
        # exception.
        s3_file = 's3://' + self.bucket + '/' + 'text1.txt'
        local_file = self.loc_files[0]
        s3_prefix = 's3://' + self.bucket
        local_dir = self.loc_files[3]

        # :var files: a list of tuples where the first element is a single
        #     element list of file paths. The second element is a boolean
        #     representing if the operation is a directory operation.
        files = [([s3_file], False), ([local_file], False),
                 ([s3_prefix], True), ([local_dir], True)]

        parameters = {}
        for filename in files:
            parameters['dir_op'] = filename[1]
            cmd_parameter = CommandParameters(self.session, 'put', parameters)
            cmd_parameter.add_region(mock.Mock())
            cmd_parameter.check_src_path(filename[0])

    def test_check_force(self):
        # This checks to make sure that the force parameter is run. If
        # successful. The delete command will fail as the bucket is empty
        # and be caught by the exception.
        cmd_params = CommandParameters(self.session, 'rb', {'force': True})
        cmd_params.parameters['src'] = 's3://mybucket'
        cmd_params.check_force(None)


class HelpDocTest(BaseAWSHelpOutputTest):
    def setUp(self):
        super(HelpDocTest, self).setUp()
        self.session = botocore.session.get_session()

    def tearDown(self):
        super(HelpDocTest, self).tearDown()

    def test_s3_help(self):
        # This tests the help command for the s3 service. This
        # checks to make sure the appropriate descriptions are
        # added including the tutorial.
        s3 = S3('s3', self.session)
        parser = argparse.ArgumentParser()
        parser.add_argument('--paginate', action='store_true')
        parsed_global = parser.parse_args(['--paginate'])
        help_command = s3.create_help_command()
        help_command([], parsed_global)
        self.assert_contains("This provides higher level S3 commands")
        self.assert_contains("Every command takes one or two positional")
        self.assert_contains("* rb")

    def test_s3command_help(self):
        # This tests the help command for an s3 command. This
        # checks to make sure the command prints appropriate
        # parts.  Note the examples are not included because
        # the event was not registered.
        s3command = S3SubCommand('cp', self.session, {'nargs': 2})
        parser = argparse.ArgumentParser()
        parser.add_argument('--paginate', action='store_true')
        parsed_global = parser.parse_args(['--paginate'])
        help_command = s3command.create_help_command()
        help_command([], parsed_global)
        self.assert_contains("cp")
        self.assert_contains("[--acl <value>]")
        self.assert_contains("Displays the operations that would be")

    def test_help(self):
        # This ensures that the file appropriately redirects to help object
        # if help is the only argument left to be parsed.  There should not
        # have any contents in the docs.
        s3_command = S3SubCommand('sync', self.session, {'nargs': 2})
        s3_command(['help'], [])
        self.assert_contains('sync')
        self.assert_contains("Synopsis")


class TestLSCommand(unittest.TestCase):
    def setUp(self):
        self.session = mock.Mock()
        self.session.get_service.return_value.get_operation.return_value\
                .call.return_value = (None, {'Buckets': []})
        self.session.get_service.return_value.get_operation.return_value\
                .paginate.return_value = [
                    (None, {'Contents': [], 'CommonPrefixes': []})]

    def test_ls_command_with_no_args(self):
        options = {'default': 's3://', 'nargs': '?'}
        ls_command = ListCommand('ls', self.session, options)
        parsed_args = FakeArgs(region=None, endpoint_url=None, verify_ssl=None)
        ls_command([], parsed_args)
        # We should only be a single call.
        self.session.get_service.return_value.get_operation.assert_called_with(
            'ListBuckets')
        call = self.session.get_service.return_value.get_operation\
                .return_value.call
        self.assertEqual(call.call_count, 1)
        self.assertEqual(call.call_args[1], {})
        # Verify get_endpoint
        get_endpoint = self.session.get_service.return_value.get_endpoint
        args = get_endpoint.call_args
        self.assertEqual(args, mock.call(region_name=None, endpoint_url=None,
                                         verify=None))

    def test_ls_with_verify_argument(self):
        options = {'default': 's3://', 'nargs': '?'}
        ls_command = ListCommand('ls', self.session, options)
        parsed_args = FakeArgs(region='us-west-2', endpoint_url=None, verify_ssl=False)
        ls_command([], parsed_args)
        # Verify get_endpoint
        get_endpoint = self.session.get_service.return_value.get_endpoint
        args = get_endpoint.call_args
        self.assertEqual(args, mock.call(region_name='us-west-2', endpoint_url=None,
                                         verify=False))

    def test_ls_command_for_bucket(self):
        options = {'default': 's3://', 'nargs': '?'}
        ls_command = ListCommand('ls', self.session, options)
        ls_command(['s3://mybucket/'], mock.Mock())
        call = self.session.get_service.return_value.get_operation\
                .return_value.call
        paginate = self.session.get_service.return_value.get_operation\
                .return_value.paginate
        # We should make no operation calls.
        self.assertEqual(call.call_count, 0)
        # And only a single pagination call to ListObjects.
        self.session.get_service.return_value.get_operation.assert_called_with(
            'ListObjects')
        self.assertEqual(
            paginate.call_args[1], {'bucket': u'mybucket',
                                    'delimiter': '/', 'prefix': u''})


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_s3handler
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import datetime
import os
import random
import sys
from awscli.testutils import unittest

from awscli import EnvironmentVariables
from awscli.customizations.s3.s3handler import S3Handler
from awscli.customizations.s3.fileinfo import FileInfo
from tests.unit.customizations.s3.fake_session import FakeSession
from tests.unit.customizations.s3 import make_loc_files, clean_loc_files, \
    make_s3_files, s3_cleanup, create_bucket, list_contents, list_buckets, \
    S3HandlerBaseTest


class S3HandlerTestDeleteList(S3HandlerBaseTest):
    """
    This tests the ability to delete both files locally and in s3.
    """
    def setUp(self):
        super(S3HandlerTestDeleteList, self).setUp()
        self.session = FakeSession()
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1'}
        self.s3_handler = S3Handler(self.session, params)
        self.bucket = make_s3_files(self.session)
        self.loc_files = make_loc_files()

    def tearDown(self):
        super(S3HandlerTestDeleteList, self).tearDown()
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_loc_delete(self):
        """
        Test delete local file tasks.  The local files are the same
        generated from filegenerator_test.py.
        """
        files = [self.loc_files[0], self.loc_files[1]]
        tasks = []
        for filename in files:
            self.assertTrue(os.path.exists(filename))
            tasks.append(FileInfo(
                src=filename, src_type='local',
                dest_type='s3', operation_name='delete', size=0,
                service=self.service, endpoint=self.endpoint))
        self.s3_handler.call(tasks)
        for filename in files:
            self.assertFalse(os.path.exists(filename))

    def test_s3_delete(self):
        """
        Tests S3 deletes. The files used are the same generated from
        filegenerators_test.py.  This includes the create s3 file.
        """
        keys = [self.bucket + '/another_directory/text2.txt',
                self.bucket + '/text1.txt',
                self.bucket + '/another_directory/']
        tasks = []
        for key in keys:
            tasks.append(FileInfo(
                src=key, src_type='s3',
                dest_type='local', operation_name='delete',
                size=0,
                service=self.service,
                endpoint=self.endpoint))
        self.assertEqual(len(list_contents(self.bucket, self.session)), 3)
        self.s3_handler.call(tasks)
        self.assertEqual(len(list_contents(self.bucket, self.session)), 0)

    def test_list_objects(self):
        """
        Tests the ability to list objects, common prefixes, and buckets.
        If an error occurs the test fails as this is only a printing
        operation
        """
        prefix_name = self.bucket + '/'
        file_info = FileInfo(
            src=prefix_name, operation_name='list_objects', size=0,
            service=self.service, endpoint=self.endpoint)
        params = {'region': 'us-east-1'}
        s3_handler = S3Handler(self.session, params)
        s3_handler.call([file_info])
        file_info = FileInfo(
            src='', operation_name='list_objects', size=0,
            service=self.service, endpoint=self.endpoint)
        s3_handler = S3Handler(self.session, params)
        s3_handler.call([file_info])


class S3HandlerTestURLEncodeDeletes(S3HandlerBaseTest):
    def setUp(self):
        super(S3HandlerTestURLEncodeDeletes, self).setUp()
        self.session = FakeSession()
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1'}
        self.s3_handler = S3Handler(self.session, params)
        self.bucket = make_s3_files(self.session, key1='a+b/foo', key2=None)

    def tearDown(self):
        super(S3HandlerTestURLEncodeDeletes, self).tearDown()
        s3_cleanup(self.bucket, self.session)

    def test_s3_delete_url_encode(self):
        """
        Tests S3 deletes. The files used are the same generated from
        filegenerators_test.py.  This includes the create s3 file.
        """
        key = self.bucket + '/a+b/foo'
        tasks = [FileInfo(
            src=key, src_type='s3', dest_type='local',
            operation_name='delete', size=0,
            service=self.service, endpoint=self.endpoint)]
        self.assertEqual(len(list_contents(self.bucket, self.session)), 1)
        self.s3_handler.call(tasks)
        self.assertEqual(len(list_contents(self.bucket, self.session)), 0)


class S3HandlerTestUpload(S3HandlerBaseTest):
    """
    This class tests the ability to upload objects into an S3 bucket as
    well as multipart uploads
    """
    def setUp(self):
        super(S3HandlerTestUpload, self).setUp()
        self.session = FakeSession()
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1', 'acl': ['private']}
        self.s3_handler = S3Handler(self.session, params)
        self.s3_handler_multi = S3Handler(self.session, multi_threshold=10,
                                          chunksize=2,
                                          params=params)
        self.bucket = create_bucket(self.session)
        self.loc_files = make_loc_files()
        self.s3_files = [self.bucket + '/text1.txt',
                         self.bucket + '/another_directory/text2.txt']

    def tearDown(self):
        super(S3HandlerTestUpload, self).tearDown()
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_upload(self):
        # Confirm there are no objects in the bucket.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 0)
        # Create file info objects to perform upload.
        files = [self.loc_files[0], self.loc_files[1]]
        tasks = []
        for i in range(len(files)):
            tasks.append(FileInfo(
                src=self.loc_files[i],
                dest=self.s3_files[i],
                operation_name='upload', size=0,
                service=self.service, endpoint=self.endpoint))
        # Perform the upload.
        self.s3_handler.call(tasks)
        # Confirm the files were uploaded.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 2)
        # Verify the guessed content type.
        self.assertEqual(
            self.session.s3[self.bucket][
                'another_directory/text2.txt']['ContentType'],
            'text/plain')

    def test_upload_fail(self):
        """
        One of the uploads will fail to upload in this test as
        the second s3 destination's bucket does not exist.
        """
        self.assertEqual(len(list_contents(self.bucket, self.session)), 0)
        fail_s3_files = [self.bucket + '/text1.txt',
                         self.bucket[:-1] + '/another_directory/text2.txt']
        files = [self.loc_files[0], self.loc_files[1]]
        tasks = []
        for i in range(len(files)):
            tasks.append(FileInfo(
                src=self.loc_files[i],
                dest=fail_s3_files[i],
                compare_key=None,
                src_type='local',
                dest_type='s3',
                operation_name='upload', size=0,
                last_update=None,
                service=self.service,
                endpoint=self.endpoint))
        self.s3_handler.call(tasks)
        # Confirm only one of the files was uploaded.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 1)

    def test_multi_upload(self):
        """
        This test only checks that the multipart upload process works.
        It confirms that the parts are properly formatted but does not
        perform any tests past checking the parts are uploaded correctly.
        """
        files = [self.loc_files[0], self.loc_files[1]]
        tasks = []
        for i in range(len(files)):
            tasks.append(FileInfo(
                src=self.loc_files[i],
                dest=self.s3_files[i], size=15,
                operation_name='upload',
                service=self.service,
                endpoint=self.endpoint))
        self.s3_handler_multi.call(tasks)
        self.assertEqual(
            self.session.s3[self.bucket][
                'another_directory/text2.txt']['ContentType'],
            'text/plain')


class S3HandlerExceptionSingleTaskTest(S3HandlerBaseTest):
    """
    This tests the ability to handle connection and md5 exceptions.
    The command used in this general test is a put command.
    """
    def setUp(self):
        super(S3HandlerExceptionSingleTaskTest, self).setUp()
        self.session = FakeSession(True, True)
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1'}
        self.s3_handler = S3Handler(self.session, params)
        self.bucket = create_bucket(self.session)
        self.loc_files = make_loc_files()
        self.s3_files = [self.bucket + '/text1.txt',
                         self.bucket + '/another_directory/text2.txt']

    def tearDown(self):
        super(S3HandlerExceptionSingleTaskTest, self).tearDown()
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_upload(self):
        # Confirm there are no objects in the bucket.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 0)
        # Create file info objects to perform upload.
        files = [self.loc_files[0], self.loc_files[1]]
        tasks = []
        for i in range(len(files)):
            tasks.append(FileInfo(src=self.loc_files[i],
                                  dest=self.s3_files[i],
                                  operation_name='upload', size=0,
                                  service=self.service,
                                  endpoint=self.endpoint))
        # Perform the upload.
        self.s3_handler.call(tasks)
        # Confirm despite the exceptions, the files were uploaded.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 2)


class S3HandlerExceptionMultiTaskTest(S3HandlerBaseTest):
    """
    This tests the ability to handle multipart upload exceptions.
    This includes a standard error stemming from an operation on
    a nonexisting bucket, connection error, and md5 error.
    """
    def setUp(self):
        super(S3HandlerExceptionMultiTaskTest, self).setUp()
        self.session = FakeSession(True, True)
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1'}
        self.s3_handler_multi = S3Handler(self.session, params,
                                          multi_threshold=10, chunksize=2)
        self.bucket = create_bucket(self.session)
        self.loc_files = make_loc_files()
        self.s3_files = [self.bucket + '/text1.txt',
                         self.bucket + '/another_directory/text2.txt']

    def tearDown(self):
        super(S3HandlerExceptionMultiTaskTest, self).tearDown()
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_multi_upload(self):
        files = [self.loc_files[0], self.loc_files[1]]
        fail_s3_files = [self.bucket + '/text1.txt',
                         self.bucket[:-1] + '/another_directory/text2.txt']
        tasks = []
        for i in range(len(files)):
            tasks.append(FileInfo(
                src=self.loc_files[i],
                dest=fail_s3_files[i], size=15,
                operation_name='upload',
                service=self.service,
                endpoint=self.endpoint))
        self.s3_handler_multi.call(tasks)


class S3HandlerTestMvLocalS3(S3HandlerBaseTest):
    """
    This class tests the ability to move s3 objects.  The move
    operation uses a upload then delete.
    """
    def setUp(self):
        super(S3HandlerTestMvLocalS3, self).setUp()
        self.session = FakeSession()
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1', 'acl': ['private'], 'quiet': True}
        self.s3_handler = S3Handler(self.session, params)
        self.bucket = create_bucket(self.session)
        self.loc_files = make_loc_files()
        self.s3_files = [self.bucket + '/text1.txt',
                         self.bucket + '/another_directory/text2.txt']

    def tearDown(self):
        super(S3HandlerTestMvLocalS3, self).tearDown()
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_move_unicode(self):
        self.bucket2 = make_s3_files(self.session, key1=u'\u2713')
        tasks = [FileInfo(
            src=self.bucket2 + '/' + u'\u2713',
            src_type='s3',
            dest=self.bucket + '/' + u'\u2713',
            dest_type='s3', operation_name='move',
            size=0,
            service=self.service,
            endpoint=self.endpoint,
        )]
        self.s3_handler.call(tasks)
        self.assertEqual(len(list_contents(self.bucket, self.session)), 1)

    def test_move(self):
        # Create file info objects to perform move.
        files = [self.loc_files[0], self.loc_files[1]]
        tasks = []
        for i in range(len(files)):
            tasks.append(FileInfo(
                src=self.loc_files[i], src_type='local',
                dest=self.s3_files[i], dest_type='s3',
                operation_name='move', size=0,
                service=self.service,
                endpoint=self.endpoint))
        # Perform the move.
        self.s3_handler.call(tasks)
        # Confirm the files were uploaded.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 2)
        # Confirm local files do not exist.
        for filename in files:
            self.assertFalse(os.path.exists(filename))


class S3HandlerTestMvS3S3(S3HandlerBaseTest):
    """
    This class tests the ability to move s3 objects.  The move
    operation uses a copy then delete.
    """
    def setUp(self):
        super(S3HandlerTestMvS3S3, self).setUp()
        self.session = FakeSession()
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1', 'acl': ['private']}
        self.s3_handler = S3Handler(self.session, params)
        self.bucket = make_s3_files(self.session)
        self.bucket2 = create_bucket(self.session)
        self.s3_files = [self.bucket + '/text1.txt',
                         self.bucket + '/another_directory/text2.txt']
        self.s3_files2 = [self.bucket2 + '/text1.txt',
                          self.bucket2 + '/another_directory/text2.txt']

    def tearDown(self):
        super(S3HandlerTestMvS3S3, self).tearDown()
        s3_cleanup(self.bucket, self.session)
        s3_cleanup(self.bucket2, self.session)

    def test_move(self):
        # Confirm there are no objects in the bucket.
        self.assertEqual(len(list_contents(self.bucket2, self.session)), 0)
        # Create file info objects to perform move.
        tasks = []
        for i in range(len(self.s3_files)):
            tasks.append(FileInfo(
                src=self.s3_files[i], src_type='s3',
                dest=self.s3_files2[i], dest_type='s3',
                operation_name='move', size=0,
                service=self.service,
                endpoint=self.endpoint))
        # Perform the move.
        self.s3_handler.call(tasks)
        # Confirm the files were moved.  The origial bucket had three
        # objects. Only two were moved.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 1)
        self.assertEqual(len(list_contents(self.bucket2, self.session)), 2)


class S3HandlerTestMvS3Local(S3HandlerBaseTest):
    """
    This class tests the ability to move s3 objects.  The move
    operation uses a download then delete.
    """
    def setUp(self):
        super(S3HandlerTestMvS3Local, self).setUp()
        self.session = FakeSession()
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1'}
        self.s3_handler = S3Handler(self.session, params)
        self.bucket = make_s3_files(self.session)
        self.s3_files = [self.bucket + '/text1.txt',
                         self.bucket + '/another_directory/text2.txt']
        directory1 = os.path.abspath('.') + os.sep + 'some_directory' + os.sep
        filename1 = directory1 + "text1.txt"
        directory2 = directory1 + 'another_directory' + os.sep
        filename2 = directory2 + "text2.txt"
        self.loc_files = [filename1, filename2]

    def tearDown(self):
        super(S3HandlerTestMvS3Local, self).tearDown()
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_move(self):
        # Create file info objects to perform move.
        tasks = []
        time = datetime.datetime.now()
        for i in range(len(self.s3_files)):
            tasks.append(FileInfo(
                src=self.s3_files[i], src_type='s3',
                dest=self.loc_files[i], dest_type='local',
                last_update=time, operation_name='move',
                size=0,
                service=self.service,
                endpoint=self.endpoint))
        # Perform the move.
        self.s3_handler.call(tasks)
        # Confirm that the files now exist.
        for filename in self.loc_files:
            self.assertTrue(os.path.exists(filename))
        # Ensure the contents are as expected.
        with open(self.loc_files[0], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is a test.')
        with open(self.loc_files[1], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is another test.')
        # Ensure the objects are no longer in the bucket.
        self.assertEqual(len(list_contents(self.bucket, self.session)), 1)


class S3HandlerTestDownload(S3HandlerBaseTest):
    """
    This class tests the ability to download s3 objects locally as well
    as using multipart downloads
    """
    def setUp(self):
        super(S3HandlerTestDownload, self).setUp()
        self.session = FakeSession()
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        params = {'region': 'us-east-1'}
        self.s3_handler = S3Handler(self.session, params)
        self.s3_handler_multi = S3Handler(self.session, params,
                                          multi_threshold=10, chunksize=2)
        self.bucket = make_s3_files(self.session)
        self.s3_files = [self.bucket + '/text1.txt',
                         self.bucket + '/another_directory/text2.txt']
        directory1 = os.path.abspath('.') + os.sep + 'some_directory' + os.sep
        filename1 = directory1 + "text1.txt"
        directory2 = directory1 + 'another_directory' + os.sep
        filename2 = directory2 + "text2.txt"
        self.loc_files = [filename1, filename2]

        self.fail_session = FakeSession(connection_error=True)
        self.fail_session.s3 = self.session.s3
        self.s3_handler_multi_except = S3Handler(self.fail_session, params,
                                                 multi_threshold=10,
                                                 chunksize=2)

    def tearDown(self):
        super(S3HandlerTestDownload, self).tearDown()
        clean_loc_files(self.loc_files)
        s3_cleanup(self.bucket, self.session)

    def test_download(self):
        # Confirm that the files do not exist.
        for filename in self.loc_files:
            self.assertFalse(os.path.exists(filename))
        # Create file info objects to perform download.
        tasks = []
        time = datetime.datetime.now()
        for i in range(len(self.s3_files)):
            tasks.append(FileInfo(
                src=self.s3_files[i], src_type='s3',
                dest=self.loc_files[i], dest_type='local',
                last_update=time, operation_name='download',
                size=0,
                service=self.service,
                endpoint=self.endpoint))
        # Perform the download.
        self.s3_handler.call(tasks)
        # Confirm that the files now exist.
        for filename in self.loc_files:
            self.assertTrue(os.path.exists(filename))
        # Ensure the contents are as expected.
        with open(self.loc_files[0], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is a test.')
        with open(self.loc_files[1], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is another test.')

    def test_multi_download(self):
        tasks = []
        time = datetime.datetime.now()
        for i in range(len(self.s3_files)):
            tasks.append(FileInfo(
                src=self.s3_files[i], src_type='s3',
                dest=self.loc_files[i], dest_type='local',
                last_update=time, operation_name='download',
                size=15,
                service=self.service,
                endpoint=self.endpoint,
            ))
        # Perform the multipart  download.
        self.s3_handler_multi.call(tasks)
        # Confirm that the files now exist.
        for filename in self.loc_files:
            self.assertTrue(os.path.exists(filename))
        # Ensure the contents are as expected.
        with open(self.loc_files[0], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is a test.')
        with open(self.loc_files[1], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is another test.')

    def test_multi_download_fail(self):
        """
        This test ensures that a multipart download can handle a
        standard error exception stemming from an operation
        being performed on a nonexistant bucket.  The existing file
        should be downloaded properly but the other will not.
        """
        tasks = []
        wrong_s3_files = [self.bucket + '/text1.txt',
                          self.bucket[:-1] + '/another_directory/text2.txt']
        time = datetime.datetime.now()
        for i in range(len(self.s3_files)):
            tasks.append(FileInfo(
                src=wrong_s3_files[i], src_type='s3',
                dest=self.loc_files[i], dest_type='local',
                last_update=time, operation_name='download',
                size=15,
                service=self.service,
                endpoint=self.endpoint
            ))
        # Perform the multipart  download.
        self.s3_handler_multi.call(tasks)
        # Confirm that the files now exist.
        self.assertTrue(os.path.exists(self.loc_files[0]))
        # The second file should not exist.
        self.assertFalse(os.path.exists(self.loc_files[1]))
        # Ensure that contents are as expected.
        with open(self.loc_files[0], 'rb') as filename:
            self.assertEqual(filename.read(), b'This is a test.')


class S3HandlerTestBucket(S3HandlerBaseTest):
    """
    Test the ability to make a bucket then remove it.
    """
    def setUp(self):
        super(S3HandlerTestBucket, self).setUp()
        self.session = FakeSession()
        self.service = self.session.get_service('s3')
        self.endpoint = self.service.get_endpoint('us-east-1')
        self.params = {'region': 'us-east-1'}
        self.bucket = None

    def tearDown(self):
        super(S3HandlerTestBucket, self).tearDown()
        s3_cleanup(self.bucket, self.session)

    def test_bucket(self):
        rand1 = random.randrange(5000)
        rand2 = random.randrange(5000)
        self.bucket = str(rand1) + 'mybucket' + str(rand2) + '/'
        orig_number_buckets = len(list_buckets(self.session))

        file_info = FileInfo(
            src=self.bucket,
            operation_name='make_bucket',
            size=0,
            service=self.service,
            endpoint=self.endpoint)
        S3Handler(self.session, self.params).call([file_info])
        number_buckets = len(list_buckets(self.session))
        self.assertEqual(orig_number_buckets + 1, number_buckets)

        file_info = FileInfo(
            src=self.bucket,
            operation_name='remove_bucket',
            size=0,
            service=self.service,
            endpoint=self.endpoint)
        S3Handler(self.session, self.params).call([file_info])
        number_buckets = len(list_buckets(self.session))
        self.assertEqual(orig_number_buckets, number_buckets)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_tasks
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest
import random
import threading
import mock
import socket

from botocore.exceptions import IncompleteReadError

from awscli.customizations.s3.tasks import CreateLocalFileTask
from awscli.customizations.s3.tasks import CompleteDownloadTask
from awscli.customizations.s3.tasks import DownloadPartTask
from awscli.customizations.s3.tasks import MultipartUploadContext
from awscli.customizations.s3.tasks import UploadCancelledError
from awscli.customizations.s3.tasks import print_operation
from awscli.customizations.s3.tasks import RetriesExeededError
from awscli.customizations.s3.executor import ShutdownThreadRequest
from awscli.customizations.s3.utils import StablePriorityQueue


class TestMultipartUploadContext(unittest.TestCase):

    def setUp(self):
        self.context = MultipartUploadContext(expected_parts=1)
        self.calls = []
        self.threads = []
        self.call_lock = threading.Lock()
        self.caught_exception = None

    def tearDown(self):
        self.join_threads()

    def join_threads(self):
        for thread in self.threads:
            thread.join()

    def upload_part(self, part_number):
        # This simulates what a thread would do if it wanted to upload
        # a part.  First it would wait for the upload id.
        try:
            upload_id = self.context.wait_for_upload_id()
        except Exception as e:
            self.caught_exception = e
            return
        with self.call_lock:
            self.calls.append(('upload_part', part_number, upload_id))
        # Then it would call UploadPart here.
        # Then it would announce that it's finished with a part.
        self.context.announce_finished_part(etag='etag%s' % part_number,
                                            part_number=part_number)

    def complete_upload(self):
        try:
            upload_id = self.context.wait_for_upload_id()
            parts = self.context.wait_for_parts_to_finish()
        except Exception as e:
            self.caught_exception = e
            return
        with self.call_lock:
            self.calls.append(('complete_upload', upload_id, parts))
            self.context.announce_completed()

    def wait_for_upload_complete(self):
        try:
            self.context.wait_for_completion()
        except Exception as e:
            self.caught_exception = e
            return
        with self.call_lock:
            self.calls.append(('arbitrary_post_complete_operation',))

    def create_upload(self, upload_id):
        with self.call_lock:
            self.calls.append(('create_multipart_upload', 'my_upload_id'))
        self.context.announce_upload_id(upload_id)

    def start_thread(self, thread):
        thread.start()
        self.threads.append(thread)

    def test_normal_non_threaded(self):
        # The context object is pretty straightforward.
        # This shows the non threaded usage of this object.
        context = MultipartUploadContext(expected_parts=3)
        # First you can announce an upload id.
        context.announce_upload_id('my_upload_id')
        # Then a thread that was waiting on the id would be notified.
        self.assertEqual(context.wait_for_upload_id(), 'my_upload_id')
        # Then thread would chug away at the parts.
        context.announce_finished_part(etag='etag1', part_number=1)
        context.announce_finished_part(etag='etag2', part_number=2)
        context.announce_finished_part(etag='etag3', part_number=3)
        # Then a thread that was waiting for all the parts to finish
        # would be notified.
        self.assertEqual(context.wait_for_parts_to_finish(), [
            {'ETag': 'etag1', 'PartNumber': 1},
            {'ETag': 'etag2', 'PartNumber': 2},
            {'ETag': 'etag3', 'PartNumber': 3}])
        context.announce_completed()
        # This will return right away since we've already announced completion.
        self.assertIsNone(context.wait_for_completion())

    def test_basic_threaded_parts(self):
        # Now while test_normal_non_threaded showed the conceptual idea,
        # the real strength of MultipartUploadContext is that it works
        # when there are threads and when these threads operate out of
        # sequence.
        # For example, let's say a thread comes along that wants
        # to upload a part.  It needs to wait until the upload id
        # is announced.
        upload_part_thread = threading.Thread(target=self.upload_part,
                                              args=(1,))
        # Once this thread starts it will immediately block.
        self.start_thread(upload_part_thread)

        # Also, let's start the thread that will do the complete
        # multipart upload.  It will also block because it needs all
        # the parts so it's blocked up the upload_part_thread.  It also
        # needs the upload_id so it's blocked on that as well.
        complete_upload_thread = threading.Thread(target=self.complete_upload)
        self.start_thread(complete_upload_thread)

        # We'll also have some other arbitrary thread that's just waiting for
        # the whole upload to be complete.  This is not the same as
        # complete_upload_thread, as that thread is used to complete the
        # upload.  This thread wants to know when *that* process is all done.
        arbitrary_waiting_thread = threading.Thread(target=self.wait_for_upload_complete)
        self.start_thread(arbitrary_waiting_thread)

        # Then finally the CreateMultipartUpload completes and we
        # announce the upload id.
        self.create_upload('my_upload_id')
        # The upload_part thread can now proceed as well as the complete
        # multipart upload thread.
        self.join_threads()

        self.assertIsNone(self.caught_exception)
        # We can verify that the invariants still hold.
        self.assertEqual(len(self.calls), 4)
        # First there should be three calls, create, upload, complete.
        self.assertEqual(self.calls[0][0], 'create_multipart_upload')
        self.assertEqual(self.calls[1][0], 'upload_part')
        self.assertEqual(self.calls[2][0], 'complete_upload')
        # Then anything that was waiting for the operation to complete should
        # be called afterwards.
        self.assertEqual(self.calls[3][0], 'arbitrary_post_complete_operation')

        # Verify the correct args were used.
        self.assertEqual(self.calls[0][1], 'my_upload_id')
        self.assertEqual(self.calls[1][1:], (1, 'my_upload_id'))
        self.assertEqual(
            self.calls[2][1:],
            ('my_upload_id', [{'ETag': 'etag1', 'PartNumber': 1}]))

    def test_randomized_stress_test(self):
        # Now given that we've verified the functionality from
        # the two tests above, we randomize the threading to ensure
        # that the order doesn't actually matter.  The invariant that
        # the CreateMultipartUpload is called first, then UploadPart
        # operations are called with the appropriate upload_id, then
        # CompleteMultipartUpload with the appropriate upload_id and
        # parts list should hold true regardless of how the threads
        # are ordered.

        # I've run this with much larger values, but 100 is a good
        # tradeoff with coverage vs. execution time.
        for i in range(100):
            expected_parts = random.randint(2, 50)
            self.context = MultipartUploadContext(expected_parts=expected_parts)
            self.threads = []
            self.calls = []
            all_threads = [
                threading.Thread(target=self.complete_upload),
                threading.Thread(target=self.create_upload,
                                args=('my_upload_id',)),
                threading.Thread(target=self.wait_for_upload_complete),
            ]
            for i in range(1, expected_parts + 1):
                all_threads.append(
                    threading.Thread(target=self.upload_part, args=(i,))
                )
            random.shuffle(all_threads)
            for thread in all_threads:
                self.start_thread(thread)
            self.join_threads()
            self.assertEqual(self.calls[0][0], 'create_multipart_upload')
            self.assertEqual(self.calls[-1][0],
                             'arbitrary_post_complete_operation')
            self.assertEqual(self.calls[-2][0], 'complete_upload')
            parts = set()
            for call in self.calls[1:-2]:
                self.assertEqual(call[0], 'upload_part')
                self.assertEqual(call[2], 'my_upload_id')
                parts.add(call[1])
            self.assertEqual(len(parts), expected_parts)

    def test_can_cancel_tasks(self):
        # Let's say that we want have a thread waiting for the upload id.
        upload_part_thread = threading.Thread(target=self.upload_part,
                                            args=(1,))
        self.start_thread(upload_part_thread)
        # But for whatever reason we aren't able to call CreateMultipartUpload.
        # We'd like to let the other thread know that it should abort.
        self.context.cancel_upload()
        # The start thread should be finished.
        self.join_threads()
        # No s3 calls should have been made.
        self.assertEqual(self.calls, [])
        # And any thread that tries to wait for data will get an exception.
        with self.assertRaises(UploadCancelledError):
            self.context.wait_for_upload_id()
        with self.assertRaises(UploadCancelledError):
            self.context.wait_for_parts_to_finish()

    def test_cancel_threads_waiting_for_completion(self):
        # So we have a thread waiting for the entire upload to complete.
        arbitrary_waiting_thread = threading.Thread(target=self.wait_for_upload_complete)
        self.start_thread(arbitrary_waiting_thread)

        # And as it's waiting, something happens and we cancel the upload.
        self.context.cancel_upload()

        # The thread should exit.
        self.join_threads()

        # And we should have seen an exception being raised.
        self.assertIsInstance(self.caught_exception, UploadCancelledError)


class TestPrintOperation(unittest.TestCase):
    def test_print_operation(self):
        filename = mock.Mock()
        filename.operation_name = 'upload'
        filename.src = r'e:\foo'
        filename.src_type = 'local'
        filename.dest = r's3://foo'
        filename.dest_type = 's3'
        message = print_operation(filename, failed=False)
        self.assertIn(r'e:\foo', message)


class TestDownloadPartTask(unittest.TestCase):
    def setUp(self):
        self.result_queue = mock.Mock()
        self.io_queue = mock.Mock()
        self.service = mock.Mock()
        self.filename = mock.Mock()
        self.filename.size = 10 * 1024 * 1024
        self.filename.src = 'bucket/key'
        self.filename.dest = 'local/file'
        self.filename.service = self.service
        self.filename.operation_name = 'download'
        self.context = mock.Mock()
        self.open = mock.MagicMock()

    def test_socket_timeout_is_retried(self):
        self.service.get_operation.return_value.call.side_effect = socket.error
        task = DownloadPartTask(0, 1024 * 1024, self.result_queue,
                                self.service, self.filename, self.context,
                                self.io_queue)
        # The mock is configured to keep raising a socket.error
        # so we should cancel the download.
        with self.assertRaises(RetriesExeededError):
            task()
        self.context.cancel.assert_called_with()
        # And we retried the request multiple times.
        self.assertEqual(DownloadPartTask.TOTAL_ATTEMPTS,
                         self.service.get_operation.call_count)

    def test_download_succeeds(self):
        body = mock.Mock()
        body.read.return_value = b''
        self.service.get_operation.return_value.call.side_effect = [
            socket.error, (mock.Mock(), {'Body': body})]
        task = DownloadPartTask(0, 1024 * 1024, self.result_queue,
                                self.service, self.filename, self.context,
                                self.io_queue)
        task()
        self.assertEqual(self.result_queue.put.call_count, 1)
        # And we tried twice, the first one failed, the second one
        # succeeded.
        self.assertEqual(self.service.get_operation.call_count, 2)

    def test_download_queues_io_properly(self):
        body = mock.Mock()
        body.read.side_effect = [b'foobar', b'morefoobar', b'']
        self.service.get_operation.return_value.call.side_effect = [
            (mock.Mock(), {'Body': body}),
        ]
        task = DownloadPartTask(0, 1024 * 1024, self.result_queue,
                                self.service, self.filename, self.context,
                                self.io_queue)
        task()
        call_args_list = self.io_queue.put.call_args_list
        self.assertEqual(len(call_args_list), 2)
        self.assertEqual(call_args_list[0],
                         mock.call(('local/file', 0, b'foobar')))
        self.assertEqual(call_args_list[1],
                         mock.call(('local/file', 6, b'morefoobar')))

    def test_incomplete_read_is_retried(self):
        self.service.get_operation.return_value.call.side_effect = \
                IncompleteReadError(actual_bytes=1, expected_bytes=2)
        task = DownloadPartTask(0, 1024 * 1024, self.result_queue,
                                self.service, self.filename,
                                self.context, self.io_queue)
        with self.assertRaises(RetriesExeededError):
            task()
        self.context.cancel.assert_called_with()
        self.assertEqual(DownloadPartTask.TOTAL_ATTEMPTS,
                         self.service.get_operation.call_count)


class TestTaskOrdering(unittest.TestCase):
    def setUp(self):
        self.q = StablePriorityQueue(maxsize=10, max_priority=20)

    def create_task(self):
        # We don't actually care about the arguments, we just want to test
        # the ordering of the tasks.
        return CreateLocalFileTask(None, None)

    def complete_task(self):
        return CompleteDownloadTask(None, None, None, None, None)

    def download_task(self):
        return DownloadPartTask(None, None, None, None, mock.Mock(), None, None)

    def shutdown_task(self, priority=None):
        return ShutdownThreadRequest(priority)

    def test_order_unchanged_in_same_priority(self):
        create = self.create_task()
        download = self.download_task()
        complete = self.complete_task()

        self.q.put(create)
        self.q.put(download)
        self.q.put(complete)

        self.assertIs(self.q.get(), create)
        self.assertIs(self.q.get(), download)
        self.assertIs(self.q.get(), complete)

    def test_multiple_tasks(self):
        create = self.create_task()
        download = self.download_task()
        complete = self.complete_task()

        create2 = self.create_task()
        download2 = self.download_task()
        complete2 = self.complete_task()

        self.q.put(create)
        self.q.put(download)
        self.q.put(complete)

        self.q.put(create2)
        self.q.put(download2)
        self.q.put(complete2)

        self.assertIs(self.q.get(), create)
        self.assertIs(self.q.get(), download)
        self.assertIs(self.q.get(), complete)

        self.assertIs(self.q.get(), create2)
        self.assertIs(self.q.get(), download2)
        self.assertIs(self.q.get(), complete2)

    def test_shutdown_tasks_are_last(self):
        create = self.create_task()
        download = self.download_task()
        complete = self.complete_task()
        shutdown = self.shutdown_task(priority=11)

        self.q.put(create)
        self.q.put(download)
        self.q.put(complete)
        self.q.put(shutdown)

        self.assertIs(self.q.get(), create)
        self.assertIs(self.q.get(), download)
        self.assertIs(self.q.get(), complete)
        self.assertIs(self.q.get(), shutdown)

########NEW FILE########
__FILENAME__ = test_utils
from awscli.testutils import unittest
import os
import tempfile
import shutil
import ntpath

import mock

from botocore.hooks import HierarchicalEmitter
from awscli.customizations.s3.utils import find_bucket_key, find_chunksize
from awscli.customizations.s3.utils import ReadFileChunk
from awscli.customizations.s3.utils import relative_path
from awscli.customizations.s3.utils import StablePriorityQueue
from awscli.customizations.s3.utils import BucketLister
from awscli.customizations.s3.utils import ScopedEventHandler
from awscli.customizations.s3.constants import MAX_SINGLE_UPLOAD_SIZE


class FindBucketKey(unittest.TestCase):
    """
    This test ensures the find_bucket_key function works when
    unicode is used.
    """
    def test_unicode(self):
        s3_path = '\u1234' + u'/' + '\u5678'
        bucket, key = find_bucket_key(s3_path)
        self.assertEqual(bucket, '\u1234')
        self.assertEqual(key, '\u5678')


class FindChunksizeTest(unittest.TestCase):
    """
    This test ensures that the ``find_chunksize`` function works
    as expected.
    """
    def test_small_chunk(self):
        """
        This test ensures if the ``chunksize`` is appropriate to begin with,
        it does not change.
        """
        chunksize = 7 * (1024 ** 2)
        size = 8 * (1024 ** 2)
        self.assertEqual(find_chunksize(size, chunksize), chunksize)

    def test_large_chunk(self):
        """
        This test ensures if the ``chunksize`` adapts to an appropriate
        size because the original ``chunksize`` is too small.
        """
        chunksize = 7 * (1024 ** 2)
        size = 8 * (1024 ** 3)
        self.assertEqual(find_chunksize(size, chunksize), chunksize * 2)

    def test_super_chunk(self):
        """
        This tests to ensure that the ``chunksize can never be larger than
        the ``MAX_SINGLE_UPLOAD_SIZE``
        """
        chunksize = MAX_SINGLE_UPLOAD_SIZE + 1
        size = MAX_SINGLE_UPLOAD_SIZE * 2
        self.assertEqual(find_chunksize(size, chunksize),
                         MAX_SINGLE_UPLOAD_SIZE)


class TestReadFileChunk(unittest.TestCase):
    def setUp(self):
        self.tempdir = tempfile.mkdtemp()

    def tearDown(self):
        shutil.rmtree(self.tempdir)

    def test_read_entire_chunk(self):
        filename = os.path.join(self.tempdir, 'foo')
        f = open(filename, 'wb')
        f.write(b'onetwothreefourfivesixseveneightnineten')
        f.flush()
        chunk = ReadFileChunk(filename, start_byte=0, size=3)
        self.assertEqual(chunk.read(), b'one')
        self.assertEqual(chunk.read(), b'')

    def test_read_with_amount_size(self):
        filename = os.path.join(self.tempdir, 'foo')
        f = open(filename, 'wb')
        f.write(b'onetwothreefourfivesixseveneightnineten')
        f.flush()
        chunk = ReadFileChunk(filename, start_byte=11, size=4)
        self.assertEqual(chunk.read(1), b'f')
        self.assertEqual(chunk.read(1), b'o')
        self.assertEqual(chunk.read(1), b'u')
        self.assertEqual(chunk.read(1), b'r')
        self.assertEqual(chunk.read(1), b'')

    def test_reset_stream_emulation(self):
        filename = os.path.join(self.tempdir, 'foo')
        f = open(filename, 'wb')
        f.write(b'onetwothreefourfivesixseveneightnineten')
        f.flush()
        chunk = ReadFileChunk(filename, start_byte=11, size=4)
        self.assertEqual(chunk.read(), b'four')
        chunk.seek(0)
        self.assertEqual(chunk.read(), b'four')

    def test_read_past_end_of_file(self):
        filename = os.path.join(self.tempdir, 'foo')
        f = open(filename, 'wb')
        f.write(b'onetwothreefourfivesixseveneightnineten')
        f.flush()
        chunk = ReadFileChunk(filename, start_byte=36, size=100000)
        self.assertEqual(chunk.read(), b'ten')
        self.assertEqual(chunk.read(), b'')
        self.assertEqual(len(chunk), 3)

    def test_tell_and_seek(self):
        filename = os.path.join(self.tempdir, 'foo')
        f = open(filename, 'wb')
        f.write(b'onetwothreefourfivesixseveneightnineten')
        f.flush()
        chunk = ReadFileChunk(filename, start_byte=36, size=100000)
        self.assertEqual(chunk.tell(), 0)
        self.assertEqual(chunk.read(), b'ten')
        self.assertEqual(chunk.tell(), 3)
        chunk.seek(0)
        self.assertEqual(chunk.tell(), 0)


class TestRelativePath(unittest.TestCase):
    def test_relpath_normal(self):
        self.assertEqual(relative_path('/tmp/foo/bar', '/tmp/foo'),
                         '.' + os.sep + 'bar')

    # We need to patch out relpath with the ntpath version so
    # we can simulate testing drives on windows.
    @mock.patch('os.path.relpath', ntpath.relpath)
    def test_relpath_with_error(self):
        # Just want to check we don't get an exception raised,
        # which is what was happening previously.
        self.assertIn(r'foo\bar', relative_path(r'c:\foo\bar'))


class TestStablePriorityQueue(unittest.TestCase):
    def test_fifo_order_of_same_priorities(self):
        a = mock.Mock()
        a.PRIORITY = 5
        b = mock.Mock()
        b.PRIORITY = 5
        c = mock.Mock()
        c.PRIORITY = 1

        q = StablePriorityQueue(maxsize=10, max_priority=20)
        q.put(a)
        q.put(b)
        q.put(c)

        # First we should get c because it's the lowest priority.
        # We're using assertIs because we want the *exact* object.
        self.assertIs(q.get(), c)
        # Then a and b are the same priority, but we should get
        # a first because it was inserted first.
        self.assertIs(q.get(), a)
        self.assertIs(q.get(), b)

    def test_queue_length(self):
        a = mock.Mock()
        a.PRIORITY = 5

        q = StablePriorityQueue(maxsize=10, max_priority=20)
        self.assertEqual(q.qsize(), 0)

        q.put(a)
        self.assertEqual(q.qsize(), 1)

        q.get()
        self.assertEqual(q.qsize(), 0)

    def test_insert_max_priority_capped(self):
        q = StablePriorityQueue(maxsize=10, max_priority=20)
        a = mock.Mock()
        a.PRIORITY = 100
        q.put(a)

        self.assertIs(q.get(), a)

    def test_priority_attr_is_missing(self):
        # If priority attr is missing, we should add it
        # to the lowest priority.
        q = StablePriorityQueue(maxsize=10, max_priority=20)
        a = object()
        b = mock.Mock()
        b.PRIORITY = 5

        q.put(a)
        q.put(b)

        self.assertIs(q.get(), b)
        self.assertIs(q.get(), a)


class TestBucketList(unittest.TestCase):
    def setUp(self):
        self.operation = mock.Mock()
        self.emitter = HierarchicalEmitter()
        self.operation.session.register = self.emitter.register
        self.operation.session.unregister = self.emitter.unregister
        self.endpoint = mock.sentinel.endpoint
        self.date_parser = mock.Mock()
        self.date_parser.return_value = mock.sentinel.now
        self.responses = []

    def fake_paginate(self, *args, **kwargs):
        for response in self.responses:
            self.emitter.emit('after-call.s3.ListObjects', parsed=response[1])
        return self.responses

    def test_list_objects(self):
        now = mock.sentinel.now
        self.operation.paginate = self.fake_paginate
        self.responses = [
            (None, {'Contents': [
                {'LastModified': '2014-02-27T04:20:38.000Z',
                 'Key': 'a', 'Size': 1},
                {'LastModified': '2014-02-27T04:20:38.000Z',
                 'Key': 'b', 'Size': 2},]}),
            (None, {'Contents': [
                {'LastModified': '2014-02-27T04:20:38.000Z',
                 'Key': 'c', 'Size': 3},
            ]}),
        ]
        lister = BucketLister(self.operation, self.endpoint, self.date_parser)
        objects = list(lister.list_objects(bucket='foo'))
        self.assertEqual(objects, [('foo/a', 1, now), ('foo/b', 2, now),
                                   ('foo/c', 3, now)])

    def test_urlencoded_keys(self):
        # In order to workaround control chars being in key names,
        # we force the urlencoding of the key names and we decode
        # them before yielding them.  For example, note the %0D
        # in bar.txt:
        now = mock.sentinel.now
        self.operation.paginate = self.fake_paginate
        self.responses = [
            (None, {'Contents': [
                {'LastModified': '2014-02-27T04:20:38.000Z',
                 'Key': 'bar%0D.txt', 'Size': 1}]}),
        ]
        lister = BucketLister(self.operation, self.endpoint, self.date_parser)
        objects = list(lister.list_objects(bucket='foo'))
        # And note how it's been converted to '\r'.
        self.assertEqual(objects, [('foo/bar\r.txt', 1, now)])

    def test_urlencoded_with_unicode_keys(self):
        now = mock.sentinel.now
        self.operation.paginate = self.fake_paginate
        self.responses = [
            (None, {'Contents': [
                {'LastModified': '2014-02-27T04:20:38.000Z',
                 'Key': '%E2%9C%93', 'Size': 1}]}),
        ]
        lister = BucketLister(self.operation, self.endpoint, self.date_parser)
        objects = list(lister.list_objects(bucket='foo'))
        # And note how it's been converted to '\r'.
        self.assertEqual(objects, [(u'foo/\u2713', 1, now)])


class TestScopedEventHandler(unittest.TestCase):
    def test_scoped_session_handler(self):
        session = mock.Mock()
        scoped = ScopedEventHandler(session, 'eventname', 'handler')
        with scoped:
            session.register.assert_called_with('eventname', 'handler')
        session.unregister.assert_called_with('eventname', 'handler')


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_website_command
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import re


class TestWebsiteCommand(BaseAWSCommandParamsTest):

    prefix = 's3 website '

    def test_index_document(self):
        cmdline = self.prefix + 's3://mybucket --index-document index.html'
        result = {'uri_params': {'Bucket': 'mybucket'},
                  'headers': {},}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])
        self.assertEqual(
            self.last_kwargs,
            {'website_configuration': {'IndexDocument': {'Suffix': 'index.html'}},
             'bucket': u'mybucket'})

    def test_error_document(self):
        cmdline = self.prefix + 's3://mybucket --error-document mykey'
        result = {'uri_params': {'Bucket': 'mybucket'},
                  'headers': {},}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])
        self.assertEqual(
            self.last_kwargs,
            {'website_configuration': {'ErrorDocument': {'Key': 'mykey'}},
             'bucket': u'mybucket'})


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_cloudtrail
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.

import botocore.session
import json
import os
import six

from awscli.customizations.cloudtrail import CloudTrailSubscribe
from awscli.customizations.service import Service
from mock import Mock, patch
from awscli.testutils import BaseAWSCommandParamsTest
from tests.unit.test_clidriver import FakeSession
from awscli.testutils import unittest


class TestCloudTrail(unittest.TestCase):
    def setUp(self):
        self.session = FakeSession({'config_file': 'myconfigfile'})
        self.subscribe = CloudTrailSubscribe(self.session)

        self.subscribe.iam = Mock()
        self.subscribe.iam.GetUser = Mock(
            return_value={'User': {'Arn': '::::123:456'}})

        self.subscribe.s3 = Mock()
        self.subscribe.s3.endpoint = Mock()
        self.subscribe.s3.endpoint.region_name = 'us-east-1'
        policy_template = six.BytesIO(six.b(u'{"Statement": []}'))
        self.subscribe.s3.GetObject = Mock(
            return_value={'Body': policy_template})
        self.subscribe.s3.ListBuckets = Mock(
            return_value={'Buckets': [{'Name': 'test2'}]})

        self.subscribe.sns = Mock()
        self.subscribe.sns.endpoint = Mock()
        self.subscribe.sns.endpoint.region_name = 'us-east-1'
        self.subscribe.sns.ListTopics = Mock(
            return_value={'Topics': [{'TopicArn': ':test2'}]})
        self.subscribe.sns.CreateTopic = Mock(
            return_value={'TopicArn': 'foo'})
        self.subscribe.sns.GetTopicAttributes = Mock(
            return_value={'Attributes': {'Policy': '{"Statement": []}'}})

    def test_s3_create(self):
        iam = self.subscribe.iam
        s3 = self.subscribe.s3

        self.subscribe.setup_new_bucket('test', 'logs')

        iam.GetUser.assert_called()

        s3.GetObject.assert_called()
        s3.ListBuckets.assert_called()
        s3.CreateBucket.assert_called()
        s3.PutBucketPolicy.assert_called()

        s3.DeleteBucket.assert_not_called()

    def test_s3_create_already_exists(self):
        with self.assertRaises(Exception):
            self.subscribe.setup_new_bucket('test2', 'logs')

    def test_s3_create_set_policy_fail(self):
        s3 = self.subscribe.s3
        orig = s3.PutBucketPolicy
        s3.PutBucketPolicy = Mock(side_effect=Exception('Error!'))

        with self.assertRaises(Exception):
            self.subscribe.setup_new_bucket('test', 'logs')

        s3.CreateBucket.assert_called()
        s3.PutBucketPolicy.assert_called()
        s3.DeleteBucket.assert_called()

        s3.PutBucketPolicy = orig

    def test_get_policy_fail(self):
        orig = self.subscribe.s3.GetObject
        self.subscribe.s3.GetObject = Mock(side_effect=Exception('Error!'))

        with self.assertRaises(Exception):
            self.subscribe.setup_new_bucket('test', 'logs')

        self.subscribe.s3.GetObject = orig

    def test_sns_create(self):
        s3 = self.subscribe.s3
        sns = self.subscribe.sns

        self.subscribe.setup_new_topic('test')

        s3.GetObject.assert_called()
        sns.ListTopics.assert_called()
        sns.CreateTopic.assert_called()
        sns.SetTopicAttributes.assert_called()

        sns.DeleteTopic.assert_not_called()

    def test_sns_create_already_exists(self):
        with self.assertRaises(Exception):
            self.subscribe.setup_new_topic('test2')

    def test_cloudtrail_new_call_format(self):
        self.subscribe.cloudtrail = Mock()
        self.subscribe.cloudtrail.CreateTrail = Mock(return_value={})
        self.subscribe.cloudtrail.DescribeTrail = Mock(return_value={})

        self.subscribe.upsert_cloudtrail_config('test', 'bucket', 'prefix',
                                                'topic', True)

        self.subscribe.cloudtrail.CreateTrail.assert_called_with(
            name='test',
            s3_bucket_name='bucket',
            s3_key_prefix='prefix',
            sns_topic_name='topic',
            include_global_service_events=True
        )

    def test_sns_policy_merge(self):
        left = '''
{
   "Version":"2008-10-17",
   "Id":"us-east-1/698519295917/test__default_policy_ID",
   "Statement":[
      {
         "Effect":"Allow",
         "Sid":"us-east-1/698519295917/test__default_statement_ID",
         "Principal":{
            "AWS":"*"
         },
         "Action":[
            "SNS:GetTopicAttributes",
            "SNS:SetTopicAttributes",
            "SNS:AddPermission",
            "SNS:RemovePermission",
            "SNS:DeleteTopic",
            "SNS:Subscribe",
            "SNS:ListSubscriptionsByTopic",
            "SNS:Publish",
            "SNS:Receive"
         ],
         "Resource":"arn:aws:sns:us-east-1:698519295917:test",
         "Condition":{
            "StringLike":{
               "AWS:SourceArn":"arn:aws:*:*:698519295917:*"
            }
         }
      }
   ]
}'''
        right = '''
{
   "Version":"2008-10-17",
   "Id":"us-east-1/698519295917/test_foo",
   "Statement":[
      {
         "Effect":"Allow",
         "Sid":"us-east-1/698519295917/test_foo_ID",
         "Principal":{
            "AWS":"*"
         },
         "Action":[
            "SNS:GetTopicAttributes",
            "SNS:SetTopicAttributes",
            "SNS:AddPermission",
            "SNS:RemovePermission",
            "SNS:DeleteTopic",
            "SNS:Subscribe",
            "SNS:ListSubscriptionsByTopic",
            "SNS:Publish",
            "SNS:Receive"
         ],
         "Resource":"arn:aws:sns:us-east-1:698519295917:test",
         "Condition":{
            "StringLike":{
               "AWS:SourceArn":"arn:aws:*:*:698519295917:*"
            }
         }
      }
   ]
}'''
        expected = '''
{
   "Version":"2008-10-17",
   "Id":"us-east-1/698519295917/test__default_policy_ID",
   "Statement":[
      {
         "Effect":"Allow",
         "Sid":"us-east-1/698519295917/test__default_statement_ID",
         "Principal":{
            "AWS":"*"
         },
         "Action":[
            "SNS:GetTopicAttributes",
            "SNS:SetTopicAttributes",
            "SNS:AddPermission",
            "SNS:RemovePermission",
            "SNS:DeleteTopic",
            "SNS:Subscribe",
            "SNS:ListSubscriptionsByTopic",
            "SNS:Publish",
            "SNS:Receive"
         ],
         "Resource":"arn:aws:sns:us-east-1:698519295917:test",
         "Condition":{
            "StringLike":{
               "AWS:SourceArn":"arn:aws:*:*:698519295917:*"
            }
         }
      },
      {
         "Effect":"Allow",
         "Sid":"us-east-1/698519295917/test_foo_ID",
         "Principal":{
            "AWS":"*"
         },
         "Action":[
            "SNS:GetTopicAttributes",
            "SNS:SetTopicAttributes",
            "SNS:AddPermission",
            "SNS:RemovePermission",
            "SNS:DeleteTopic",
            "SNS:Subscribe",
            "SNS:ListSubscriptionsByTopic",
            "SNS:Publish",
            "SNS:Receive"
         ],
         "Resource":"arn:aws:sns:us-east-1:698519295917:test",
         "Condition":{
            "StringLike":{
               "AWS:SourceArn":"arn:aws:*:*:698519295917:*"
            }
         }
      }
   ]
}'''

        merged = self.subscribe.merge_sns_policy(left, right)

        self.assertEqual(json.loads(expected), json.loads(merged))


class TestCloudTrailSessions(BaseAWSCommandParamsTest):
    def test_sessions(self):
        """
        Make sure that the session passed to our custom command
        is the same session used when making service calls.
        """
        # Get a new session we will use to test
        driver = self.driver

        def _mock_call(subscribe, *args, **kwargs):
            # Store the subscribe command for assertions
            # This works because the first argument to an
            # instance method is always the instance itself.
            self.subscribe = subscribe

        with patch.object(CloudTrailSubscribe, '_call', _mock_call):
            driver.main('cloudtrail create-subscription --name test --s3-use-bucket test'.split())

            # Test the session that is used in dependent services
            subscribe = self.subscribe
            self.assertEqual(driver.session, subscribe.iam.session)
            self.assertEqual(driver.session, subscribe.s3.session)
            self.assertEqual(driver.session, subscribe.sns.session)
            self.assertEqual(driver.session, subscribe.cloudtrail.session)

########NEW FILE########
__FILENAME__ = test_cloudwatch
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest

import mock

from awscli.customizations import putmetricdata


class TestPutMetricArgument(unittest.TestCase):
    def test_build_metric_name_arg(self):
        arg = putmetricdata.PutMetricArgument('metric-name',
                                              help_text='metric-name')
        parameters = {}
        arg.add_to_params(parameters, 'MyMetricName')
        self.assertEqual(parameters['metric_data'][0]['MetricName'],
                         'MyMetricName')

    def test_build_unit_arg(self):
        arg = putmetricdata.PutMetricArgument('unit',
                                              help_text='unit')
        parameters = {}
        arg.add_to_params(parameters, 'Percent')
        self.assertEqual(parameters['metric_data'][0]['Unit'],
                         'Percent')

    def test_value_arg(self):
        arg = putmetricdata.PutMetricArgument('value',
                                              help_text='value')
        parameters = {}
        arg.add_to_params(parameters, '123.1')
        self.assertEqual(parameters['metric_data'][0]['Value'],
                         '123.1')

    def test_timestamp_arg(self):
        arg = putmetricdata.PutMetricArgument('timestamp',
                                              help_text='timestamp')
        parameters = {}
        arg.add_to_params(parameters, '2013-09-01')
        self.assertEqual(parameters['metric_data'][0]['Timestamp'],
                         '2013-09-01')

    def test_dimensions_arg(self):
        arg = putmetricdata.PutMetricArgument('dimensions',
                                              help_text='dimensions')
        parameters = {}
        arg.add_to_params(parameters, 'User=someuser,Stack=test')
        self.assertEqual(parameters['metric_data'][0]['Dimensions'],
                         [{"Name": "User", "Value": "someuser"},
                          {"Name": "Stack", "Value": "test"}])

    def test_statistics_arg(self):
        arg = putmetricdata.PutMetricArgument('statistic-values',
                                              help_text='statistic-values')
        parameters = {}
        arg.add_to_params(parameters,
                          'Sum=250,Minimum=30,Maximum=70,SampleCount=5')
        self.assertEqual(parameters['metric_data'][0]['StatisticValues'],
                         {'Maximum': '70', 'Minimum': '30',
                          'SampleCount': '5', 'Sum': '250'})

    def test_parse_empty_value(self):
        arg = putmetricdata.PutMetricArgument('dimensions',
                                              help_text='dimensions')
        parameters = {}
        arg.add_to_params(parameters, None)
        self.assertEqual(parameters, {})

########NEW FILE########
__FILENAME__ = test_commands
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest
import mock

from awscli.customizations.commands import BasicHelp, BasicCommand


class TestCommandLoader(unittest.TestCase):

    def test_basic_help_with_contents(self):
        cmd_object = mock.Mock()
        mock_module = mock.Mock()
        mock_module.__file__ = '/some/root'
        cmd_object.DESCRIPTION = BasicCommand.FROM_FILE(
            'foo', 'bar', 'baz.txt', root_module=mock_module)
        help_command = BasicHelp(mock.Mock(), cmd_object, {}, {})
        with mock.patch('awscli.customizations.commands._open') as mock_open:
            mock_open.return_value.__enter__.return_value.read.return_value = \
                    'fake description'
            self.assertEqual(help_command.description, 'fake description')

########NEW FILE########
__FILENAME__ = test_configure
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import sys
import os
import tempfile
import shutil
from awscli.testutils import unittest

import mock
from six import StringIO
from botocore.exceptions import ProfileNotFound

from awscli.customizations import configure



class PrecannedPrompter(object):
    def __init__(self, value):
        self._value = value

    def get_value(self, current_value, logical_name, prompt_text=''):
        return self._value


class EchoPrompter(object):
    def get_value(self, current_value, logical_name, prompt_text=''):
        return current_value


class KeyValuePrompter(object):
    def __init__(self, mapping):
        self.mapping = mapping

    def get_value(self, current_value, config_name, prompt_text=''):
        return self.mapping.get(prompt_text)


class FakeSession(object):
    def __init__(self, all_variables, profile_does_not_exist=False,
                 config_file_vars=None, environment_vars=None,
                 credentials=None):
        self.variables = all_variables
        self.profile_does_not_exist = profile_does_not_exist
        self.config = {}
        if config_file_vars is None:
            config_file_vars = {}
        self.config_file_vars = config_file_vars
        if environment_vars is None:
            environment_vars = {}
        self.environment_vars = environment_vars
        self._credentials = credentials
        self.profile = None

    def get_credentials(self):
        return self._credentials

    def get_scoped_config(self):
        if self.profile_does_not_exist:
            raise ProfileNotFound(profile='foo')
        return self.config

    def get_config_variable(self, name, methods=None):
        if self.profile_does_not_exist and not name == 'config_file':
            raise ProfileNotFound(profile='foo')
        if methods is not None:
            if 'env' in methods:
                return self.environment_vars.get(name)
            elif 'config' in methods:
                return self.config_file_vars.get(name)
        else:
            return self.variables.get(name)

    def emit(self, event_name, **kwargs):
        pass

    def emit_first_non_none_response(self, *args, **kwargs):
        pass


class TestConfigureCommand(unittest.TestCase):
    def setUp(self):
        self.writer = mock.Mock()
        self.global_args = mock.Mock()
        self.global_args.profile = None
        self.precanned = PrecannedPrompter(value='new_value')
        self.session = FakeSession({'config_file': 'myconfigfile'})
        self.configure = configure.ConfigureCommand(self.session,
                                                    prompter=self.precanned,
                                                    config_writer=self.writer)

    def test_configure_command_sends_values_to_writer(self):
        self.configure(args=[], parsed_globals=self.global_args)
        self.writer.update_config.assert_called_with(
            {'aws_access_key_id': 'new_value',
             'aws_secret_access_key': 'new_value',
             'region': 'new_value',
             'output': 'new_value'}, 'myconfigfile')

    def test_same_values_are_not_changed(self):
        # If the user enters the same value as the current value, we don't need
        # to write anything to the config.
        self.configure = configure.ConfigureCommand(self.session,
                                                    prompter=EchoPrompter(),
                                                    config_writer=self.writer)
        self.configure(args=[], parsed_globals=self.global_args)
        self.assertFalse(self.writer.update_config.called)

    def test_none_values_are_not_changed(self):
        # If a user hits enter, this will result in a None value which means
        # don't change the existing values.  In this case, we don't need
        # to write anything out to the config.
        user_presses_enter = None
        precanned = PrecannedPrompter(value=user_presses_enter)
        self.configure = configure.ConfigureCommand(self.session,
                                                    prompter=precanned,
                                                    config_writer=self.writer)
        self.configure(args=[], parsed_globals=self.global_args)
        self.assertFalse(self.writer.update_config.called)

    def test_create_configure_cmd_session_only(self):
        self.configure = configure.ConfigureCommand(self.session)
        self.assertIsInstance(self.configure, configure.ConfigureCommand)

    def test_some_values_changed(self):
        # Test the case where the user only wants to change a single_value.
        responses = {
            "AWS Access Key ID": None,
            "AWS Secert Access Key": None,
            "Default region name": None,
            "Default output format": "NEW OUTPUT FORMAT",
        }
        prompter = KeyValuePrompter(responses)
        self.configure = configure.ConfigureCommand(self.session, prompter=prompter,
                                                    config_writer=self.writer)
        self.configure(args=[], parsed_globals=self.global_args)

        # We only need to write out the default output format.
        self.writer.update_config.assert_called_with(
            {'output': 'NEW OUTPUT FORMAT'}, 'myconfigfile')

    def test_section_name_can_be_changed_for_profiles(self):
        # If the user specifies "--profile myname" we need to write
        # this out to the [profile myname] section.
        self.global_args.profile = 'myname'
        self.configure(args=[], parsed_globals=self.global_args)
        # Note the __section__ key name.
        self.writer.update_config.assert_called_with(
            {'__section__': 'profile myname',
             'aws_access_key_id': 'new_value',
             'aws_secret_access_key': 'new_value',
             'region': 'new_value',
             'output': 'new_value'}, 'myconfigfile')

    def test_session_says_profile_does_not_exist(self):
        # Whenever you try to get a config value from botocore,
        # it will raise an exception complaining about ProfileNotFound.
        # We should handle this case, and write out a new profile section
        # in the config file.
        session = FakeSession({'config_file': 'myconfigfile'},
                              profile_does_not_exist=True)
        self.configure = configure.ConfigureCommand(session,
                                                    prompter=self.precanned,
                                                    config_writer=self.writer)
        self.global_args.profile = 'profile-does-not-exist'
        self.configure(args=[], parsed_globals=self.global_args)
        self.writer.update_config.assert_called_with(
            {'__section__': 'profile profile-does-not-exist',
             'aws_access_key_id': 'new_value',
             'aws_secret_access_key': 'new_value',
             'region': 'new_value',
             'output': 'new_value'}, 'myconfigfile')


class TestInteractivePrompter(unittest.TestCase):
    @mock.patch('awscli.customizations.configure.raw_input')
    def test_access_key_is_masked(self, mock_raw_input):
        mock_raw_input.return_value = 'foo'
        prompter = configure.InteractivePrompter()
        response = prompter.get_value(
            current_value='myaccesskey', config_name='aws_access_key_id',
            prompt_text='Access key')
        # First we should return the value from raw_input.
        self.assertEqual(response, 'foo')
        # We should also not display the entire access key.
        prompt_text = mock_raw_input.call_args[0][0]
        self.assertNotIn('myaccesskey', prompt_text)
        self.assertRegexpMatches(prompt_text, r'\[\*\*\*\*.*\]')

    @mock.patch('awscli.customizations.configure.raw_input')
    def test_access_key_not_masked_when_none(self, mock_raw_input):
        mock_raw_input.return_value = 'foo'
        prompter = configure.InteractivePrompter()
        response = prompter.get_value(
            current_value=None, config_name='aws_access_key_id',
            prompt_text='Access key')
        # First we should return the value from raw_input.
        self.assertEqual(response, 'foo')
        prompt_text = mock_raw_input.call_args[0][0]
        self.assertIn('[None]', prompt_text)

    @mock.patch('awscli.customizations.configure.raw_input')
    def test_secret_key_is_masked(self, mock_raw_input):
        prompter = configure.InteractivePrompter()
        prompter.get_value(
            current_value='mysupersecretkey',
            config_name='aws_secret_access_key',
            prompt_text='Secret Key')
        # We should also not display the entire secret key.
        prompt_text = mock_raw_input.call_args[0][0]
        self.assertNotIn('mysupersecretkey', prompt_text)
        self.assertRegexpMatches(prompt_text, r'\[\*\*\*\*.*\]')

    @mock.patch('awscli.customizations.configure.raw_input')
    def test_non_secret_keys_are_not_masked(self, mock_raw_input):
        prompter = configure.InteractivePrompter()
        prompter.get_value(
            current_value='mycurrentvalue', config_name='not_a_secret_key',
            prompt_text='Enter value')
        # We should also not display the entire secret key.
        prompt_text = mock_raw_input.call_args[0][0]
        self.assertIn('mycurrentvalue', prompt_text)
        self.assertRegexpMatches(prompt_text, r'\[mycurrentvalue\]')


class TestConfigFileWriter(unittest.TestCase):
    def setUp(self):
        self.dirname = tempfile.mkdtemp()
        self.config_filename = os.path.join(self.dirname, 'config')
        self.writer = configure.ConfigFileWriter()

    def tearDown(self):
        shutil.rmtree(self.dirname)

    def assert_update_config(self, original_config_contents, updated_data,
                             updated_config_contents):
        # Given the original_config, when it's updated with update_data,
        # it should produce updated_config_contents.
        with open(self.config_filename, 'w') as f:
            f.write(original_config_contents)
        self.writer.update_config(updated_data, self.config_filename)
        with open(self.config_filename, 'r') as f:
            new_contents = f.read()
        if new_contents != updated_config_contents:
            self.fail("Config file contents do not match.\n"
                      "Expected contents:\n"
                      "%s\n\n"
                      "Actual Contents:\n"
                      "%s\n" % (updated_config_contents, new_contents))

    def test_update_single_existing_value(self):
        original = '[default]\nfoo = 1\nbar = 1'
        updated = '[default]\nfoo = newvalue\nbar = 1'
        self.assert_update_config(
            original, {'foo': 'newvalue'}, updated)

    def test_update_single_existing_value_no_spaces(self):
        original = '[default]\nfoo=1\nbar=1'
        updated = '[default]\nfoo = newvalue\nbar=1'
        self.assert_update_config(
            original, {'foo': 'newvalue'}, updated)

    def test_update_single_new_values(self):
        expected = '[default]\nfoo = 1\nbar = 2\nbaz = newvalue\n'
        self.assert_update_config(
            '[default]\nfoo = 1\nbar = 2',
            {'baz': 'newvalue'}, expected)

    def test_handles_no_spaces(self):
        expected = '[default]\nfoo=1\nbar=2\nbaz = newvalue\n'
        self.assert_update_config(
            '[default]\nfoo=1\nbar=2',
            {'baz': 'newvalue'}, expected)

    def test_insert_values_in_middle_section(self):
        original_contents = (
            '[a]\n'
            'foo = bar\n'
            'baz = bar\n'
            '\n'
            '[b]\n'
            '\n'
            'foo = bar\n'
            '[c]\n'
            'foo = bar\n'
            'baz = bar\n'
        )
        expected_contents = (
            '[a]\n'
            'foo = bar\n'
            'baz = bar\n'
            '\n'
            '[b]\n'
            '\n'
            'foo = newvalue\n'
            '[c]\n'
            'foo = bar\n'
            'baz = bar\n'
        )
        self.assert_update_config(
            original_contents,
            {'foo': 'newvalue', '__section__': 'b'},
            expected_contents)

    def test_insert_new_value_in_middle_section(self):
        original_contents = (
            '[a]\n'
            'foo = bar\n'
            '\n'
            '[b]\n'
            '\n'
            'foo = bar\n'
            '\n'
            '[c]\n'
            'foo = bar\n'
        )
        expected_contents = (
            '[a]\n'
            'foo = bar\n'
            '\n'
            '[b]\n'
            '\n'
            'foo = bar\n'
            'newvalue = newvalue\n'
            '\n'
            '[c]\n'
            'foo = bar\n'
        )
        self.assert_update_config(
            original_contents,
            {'newvalue': 'newvalue', '__section__': 'b'},
            expected_contents)

    def test_new_config_file(self):
        self.assert_update_config(
            '\n',
            {'foo': 'value'},
            '\n[default]\nfoo = value\n')

    def test_section_does_not_exist(self):
        original_contents = (
            '[notdefault]\n'
            'foo = bar\n'
            'baz = bar\n'
            '\n'
            '\n'
            '\n'
            '[other "section"]\n'
            '\n'
            'foo = bar\n'
        )
        appended_contents = (
            '[default]\n'
            'foo = value\n'
        )
        self.assert_update_config(
            original_contents,
            {'foo': 'value'},
            original_contents + appended_contents)

    def test_config_file_does_not_exist(self):
        self.writer.update_config({'foo': 'value'}, self.config_filename)
        with open(self.config_filename, 'r') as f:
            new_contents = f.read()
        self.assertEqual(new_contents, '[default]\nfoo = value\n')

    @unittest.skipIf(sys.platform.lower().startswith('win'),
                     "Test not valid on windows.")
    def test_permissions_on_new_file(self):
        self.writer.update_config({'foo': 'value'}, self.config_filename)
        with open(self.config_filename, 'r') as f:
            f.read()
        self.assertEqual(os.stat(self.config_filename).st_mode & 0xFFF, 0o600)

    def test_update_config_with_comments(self):
        original = (
            '[default]\n'
            '#foo = 1\n'
            'bar = 1\n'
        )
        self.assert_update_config(
            original, {'foo': 'newvalue'},
            '[default]\n'
            '#foo = 1\n'
            'bar = 1\n'
            'foo = newvalue\n'
        )

    def test_spaces_around_key_names(self):
        original = (
            '[default]\n'
            'foo = 1\n'
            'bar = 1\n'
        )
        self.assert_update_config(
            original, {'foo': 'newvalue'},
            '[default]\n'
            'foo = newvalue\n'
            'bar = 1\n'
        )

    def test_unquoted_profile_name(self):
        original = (
            '[profile foobar]\n'
            'foo = 1\n'
            'bar = 1\n'
        )
        self.assert_update_config(
            original, {'foo': 'newvalue', '__section__': 'profile foobar'},
            '[profile foobar]\n'
            'foo = newvalue\n'
            'bar = 1\n'
        )

    def test_double_quoted_profile_name(self):
        original = (
            '[profile "foobar"]\n'
            'foo = 1\n'
            'bar = 1\n'
        )
        self.assert_update_config(
            original, {'foo': 'newvalue', '__section__': 'profile foobar'},
            '[profile "foobar"]\n'
            'foo = newvalue\n'
            'bar = 1\n'
        )

    def test_profile_with_multiple_spaces(self):
        original = (
            '[profile "two  spaces"]\n'
            'foo = 1\n'
            'bar = 1\n'
        )
        self.assert_update_config(
            original, {'foo': 'newvalue', '__section__': 'profile two  spaces'},
            '[profile "two  spaces"]\n'
            'foo = newvalue\n'
            'bar = 1\n'
        )


class TestConfigureListCommand(unittest.TestCase):

    def test_configure_list_command_nothing_set(self):
        # Test the case where the user only wants to change a single_value.
        session = FakeSession(all_variables={'config_file': '/config/location'})
        stream = StringIO()
        self.configure_list = configure.ConfigureListCommand(session, stream)
        self.configure_list(args=[], parsed_globals=None)
        rendered = stream.getvalue()
        self.assertRegexpMatches(rendered, 'profile\s+<not set>')
        self.assertRegexpMatches(rendered, 'access_key\s+<not set>')
        self.assertRegexpMatches(rendered, 'secret_key\s+<not set>')
        self.assertRegexpMatches(rendered, 'region\s+<not set>')

    def test_configure_from_env(self):
        env_vars = {
            'profile': 'myprofilename'
        }
        session = FakeSession(
            all_variables={'config_file': '/config/location'},
            environment_vars=env_vars)
        session.session_var_map = {'profile': (None, "PROFILE_ENV_VAR")}
        stream = StringIO()
        self.configure_list = configure.ConfigureListCommand(session, stream)
        self.configure_list(args=[], parsed_globals=None)
        rendered = stream.getvalue()
        self.assertRegexpMatches(
            rendered, 'profile\s+myprofilename\s+env\s+PROFILE_ENV_VAR')

    def test_configure_from_config_file(self):
        config_file_vars = {
            'region': 'us-west-2'
        }
        session = FakeSession(
            all_variables={'config_file': '/config/location'},
            config_file_vars=config_file_vars)
        session.session_var_map = {'region': ('region', "AWS_REGION")}
        stream = StringIO()
        self.configure_list = configure.ConfigureListCommand(session, stream)
        self.configure_list(args=[], parsed_globals=None)
        rendered = stream.getvalue()
        self.assertRegexpMatches(
            rendered, 'region\s+us-west-2\s+config-file\s+/config/location')

    def test_configure_from_multiple_sources(self):
        # Here the profile is from an env var, the
        # region is from the config file, and the credentials
        # are from an iam-role.
        env_vars = {
            'profile': 'myprofilename'
        }
        config_file_vars = {
            'region': 'us-west-2'
        }
        credentials = mock.Mock()
        credentials.access_key = 'access_key'
        credentials.secret_key = 'secret_key'
        credentials.method = 'iam-role'
        session = FakeSession(
            all_variables={'config_file': '/config/location'},
            environment_vars=env_vars,
            config_file_vars=config_file_vars,
            credentials=credentials)
        session.session_var_map = {
            'region': ('region', 'AWS_REGION'),
            'profile': ('profile', 'AWS_DEFAULT_PROFILE')}
        stream = StringIO()
        self.configure_list = configure.ConfigureListCommand(session, stream)
        self.configure_list(args=[], parsed_globals=None)
        rendered = stream.getvalue()
        # The profile came from an env var.
        self.assertRegexpMatches(
            rendered, 'profile\s+myprofilename\s+env\s+AWS_DEFAULT_PROFILE')
        # The region came from the config file.
        self.assertRegexpMatches(
            rendered, 'region\s+us-west-2\s+config-file\s+/config/location')
        # The credentials came from an IAM role.  Note how we're
        # also checking that the access_key/secret_key are masked
        # with '*' chars except for the last 4 chars.
        self.assertRegexpMatches(
            rendered, r'access_key\s+\*+_key\s+iam-role')
        self.assertRegexpMatches(
            rendered, r'secret_key\s+\*+_key\s+iam-role')


class TestConfigureGetSetCommand(unittest.TestCase):

    def test_configure_get_command(self):
        session = FakeSession({})
        session.config['region'] = 'us-west-2'
        stream = StringIO()
        config_get = configure.ConfigureGetCommand(session, stream)
        config_get(args=['region'], parsed_globals=None)
        rendered = stream.getvalue()
        self.assertEqual(rendered.strip(), 'us-west-2')

    def test_configure_get_command_no_exist(self):
        no_vars_defined = {}
        session = FakeSession(no_vars_defined)
        stream = StringIO()
        config_get = configure.ConfigureGetCommand(session, stream)
        rc = config_get(args=['region'], parsed_globals=None)
        rendered = stream.getvalue()
        # If a config value does not exist, we don't print any output.
        self.assertEqual(rendered, '')
        # And we exit with an rc of 1.
        self.assertEqual(rc, 1)

    def test_dotted_get(self):
        session = FakeSession({})
        session.full_config = {'preview': {'emr': 'true'}}
        stream = StringIO()
        config_get = configure.ConfigureGetCommand(session, stream)
        config_get(args=['preview.emr'], parsed_globals=None)
        rendered = stream.getvalue()
        self.assertEqual(rendered.strip(), 'true')

    def test_get_from_profile(self):
        session = FakeSession({})
        session.config = {'aws_access_key_id': 'access_key'}
        session.profile = None
        stream = StringIO()
        config_get = configure.ConfigureGetCommand(session, stream)
        config_get(args=['profile.testing.aws_access_key_id'],
                   parsed_globals=None)
        rendered = stream.getvalue()
        self.assertEqual(rendered.strip(), 'access_key')
        self.assertEqual(session.profile, 'testing')


class TestConfigureSetCommand(unittest.TestCase):
    def setUp(self):
        self.session = FakeSession({'config_file': 'myconfigfile'})
        self.session.profile = None
        self.config_writer = mock.Mock()

    def test_configure_set_command(self):
        set_command = configure.ConfigureSetCommand(self.session, self.config_writer)
        set_command(args=['region', 'us-west-2'], parsed_globals=None)
        self.config_writer.update_config.assert_called_with(
            {'__section__': 'default', 'region': 'us-west-2'}, 'myconfigfile')

    def test_configure_set_command_dotted(self):
        set_command = configure.ConfigureSetCommand(self.session, self.config_writer)
        set_command(args=['preview.emr', 'true'], parsed_globals=None)
        self.config_writer.update_config.assert_called_with(
            {'__section__': 'preview', 'emr': 'true'}, 'myconfigfile')

    def test_configure_set_with_profile(self):
        self.session.profile = 'testing'
        set_command = configure.ConfigureSetCommand(self.session, self.config_writer)
        set_command(args=['region', 'us-west-2'], parsed_globals=None)
        self.config_writer.update_config.assert_called_with(
            {'__section__': 'profile testing', 'region': 'us-west-2'}, 'myconfigfile')

########NEW FILE########
__FILENAME__ = test_flatten
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest

import mock

from awscli.arguments import CLIArgument
from awscli.customizations import utils
from awscli.customizations.flatten import FlattenedArgument, FlattenArguments

from botocore.operation import Operation
from botocore.parameters import Parameter


def _hydrate(params, container, cli_type, key, value):
    """
    An example to hydrate a complex structure with custom value logic. In this
    case we create a nested structure and divide the value by 100.
    """
    params['bag'] = {
        'ArgumentBaz': {
            'SomeValueAbc': value / 100.0
        }
    }


FLATTEN_CONFIG = {
    'command-name': {
        'original-argument': {
            "keep": False,
            "flatten": {
                "ArgumentFoo": {
                    "name": "foo"
                },
                "ArgumentBar": {
                    "name": "bar",
                    "help_text": "Some help text",
                    "required": True,
                    "hydrate_value": lambda x: x.upper()
                }
            }
        },
        'another-original-argument': {
            "keep": True,
            "flatten": {
                "ArgumentBaz.SomeValue": {
                    "name": "baz",
                    "hydrate": _hydrate
                }
            }
        }
    }
}

class TestFlattenedArgument(unittest.TestCase):
    def test_basic_argument(self):
        kwargs = {
            'container': mock.Mock(),
            'prop': 'ArgumentFoo'
        }
        kwargs['container'].py_name = 'bag'
        kwargs.update(FLATTEN_CONFIG['command-name']['original-argument']
                                    ['flatten']['ArgumentFoo'])
        arg = FlattenedArgument(**kwargs)

        self.assertEqual('foo', arg.name)
        self.assertEqual('', arg.documentation)
        self.assertEqual(False, arg.required)

        params = {}
        arg.add_to_params(params, 'value')
        self.assertEqual('value', params['bag']['ArgumentFoo'])

    def test_hydrate_value_argument(self):
        kwargs = {
            'container': mock.Mock(),
            'prop': 'ArgumentBar'
        }
        kwargs['container'].py_name = 'bag'
        kwargs['container'].cli_type_name = 'list'
        kwargs.update(FLATTEN_CONFIG['command-name']['original-argument']
                                    ['flatten']['ArgumentBar'])
        arg = FlattenedArgument(**kwargs)

        self.assertEqual('bar', arg.name)
        self.assertEqual('Some help text', arg.documentation)
        self.assertEqual(True, arg.required)

        params = {}
        arg.add_to_params(params, 'value')
        self.assertEqual('VALUE', params['bag'][0]['ArgumentBar'])

    def test_hydrate_function_argument(self):
        kwargs = {
            'container': mock.Mock(),
            'prop': 'ArgumentBaz:SomeValue'
        }
        kwargs['container'].py_name = 'bag'
        kwargs.update(FLATTEN_CONFIG['command-name']
                                    ['another-original-argument']
                                    ['flatten']['ArgumentBaz.SomeValue'])
        arg = FlattenedArgument(**kwargs)

        self.assertEqual('baz', arg.name)
        self.assertEqual('', arg.documentation)
        self.assertEqual(False, arg.required)

        params = {}
        arg.add_to_params(params, 1020)
        self.assertEqual(10.2, params['bag']['ArgumentBaz']['SomeValueAbc'])


class TestFlattenCommands(unittest.TestCase):
    def test_flatten_register(self):
        cli = mock.Mock()

        flatten = FlattenArguments('my-service', FLATTEN_CONFIG)
        flatten.register(cli)

        cli.register.assert_called_with(\
            'building-argument-table.my-service.command-name',
            flatten.flatten_args)

    def test_flatten_modify_args(self):
        # Mock operation, arguments, and members for a service
        operation = mock.Mock(spec=Operation)
        operation.cli_name = 'command-name'

        argument_object1 = mock.Mock(spec=Parameter)

        member_foo = mock.Mock()
        member_foo.name = 'ArgumentFoo'
        member_foo.documentation = 'Original docs'
        member_foo.required = False

        member_bar = mock.Mock()
        member_bar.name = 'ArgumentBar'
        member_bar.documentation = 'More docs'
        member_bar.required = False

        argument_object1.members = [member_foo, member_bar]

        argument_object2 = mock.Mock(spec=Parameter)

        member_baz = mock.Mock()
        member_baz.name = 'ArgumentBaz'
        member_baz.documentation = ''
        member_baz.required = False

        member_some_value = mock.Mock()
        member_some_value.name = 'SomeValue'
        member_some_value.documenation = ''
        member_some_value.require = False

        member_baz.members = [member_some_value]

        argument_object2.members = [member_baz]

        cli_argument1 = mock.Mock(spec=CLIArgument)
        cli_argument1.argument_object = argument_object1

        cli_argument2 = mock.Mock(spec=CLIArgument)
        cli_argument2.argument_object = argument_object2

        argument_table = {
            'original-argument': cli_argument1,
            'another-original-argument': cli_argument2
        }

        # Create the flattened argument table
        cli = mock.Mock()
        flatten = FlattenArguments('my-service', FLATTEN_CONFIG)
        flatten.flatten_args(operation, argument_table)

        # Make sure new arguments and any with keep=True are there
        self.assertIn('foo', argument_table)
        self.assertIn('bar', argument_table)
        self.assertNotIn('original-argument', argument_table)
        self.assertIn('baz', argument_table)
        self.assertIn('another-original-argument', argument_table)

        # Make sure the new arguments are the class we expect
        self.assertIsInstance(argument_table['foo'], FlattenedArgument)
        self.assertIsInstance(argument_table['bar'], FlattenedArgument)
        self.assertIsInstance(argument_table['baz'], FlattenedArgument)
        self.assertNotIsInstance(argument_table['another-original-argument'],
                                 FlattenedArgument)

        # Make sure original required trait can be overridden
        self.assertEqual(False, argument_table['foo'].required)
        self.assertEqual(True, argument_table['bar'].required)

        # Make sure docs can be overriden and get the defaults
        self.assertEqual('Original docs', argument_table['foo'].documentation)
        self.assertEqual('Some help text', argument_table['bar'].documentation)

########NEW FILE########
__FILENAME__ = test_globalargs
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest
import six
import mock

from awscli.customizations import globalargs


class FakeParsedArgs(object):
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)

    def __getattr__(self, arg):
        return None


class TestGlobalArgsCustomization(unittest.TestCase):

    def test_parse_query(self):
        parsed_args = FakeParsedArgs(query='foo.bar')
        globalargs.resolve_types(parsed_args)
        # Assert that it looks like a jmespath parsed expression.
        self.assertFalse(isinstance(parsed_args.query, six.string_types))
        self.assertTrue(hasattr(parsed_args.query, 'search'))

    def test_parse_query_error_message(self):
        # Invalid jmespath expression.
        parsed_args = FakeParsedArgs(query='foo.bar.')
        with self.assertRaises(ValueError):
            globalargs.resolve_types(parsed_args)
            globalargs.resolve_types('query')

    def test_parse_verify_ssl_default_value(self):
        with mock.patch('os.environ', {}):
            parsed_args = FakeParsedArgs(verify_ssl=True)
            globalargs.resolve_types(parsed_args)
            # None, so that botocore can apply it's default logic.
            self.assertIsNone(parsed_args.verify_ssl)

    def test_parse_verify_ssl_verify_turned_off(self):
        with mock.patch('os.environ', {}):
            parsed_args = FakeParsedArgs(verify_ssl=False)
            globalargs.resolve_types(parsed_args)
            self.assertFalse(parsed_args.verify_ssl)


    def test_os_environ_overrides_cert_bundle(self):
        environ = {
            'AWS_CA_BUNDLE': '/path/to/bundle.pem',
        }
        with mock.patch('os.environ', environ):
            parsed_args = FakeParsedArgs(verify_ssl=True)
            globalargs.resolve_types(parsed_args)
            self.assertEqual(parsed_args.verify_ssl, '/path/to/bundle.pem')

########NEW FILE########
__FILENAME__ = test_paginate
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest

import mock

from awscli.customizations import paginate


class TestPaginateBase(unittest.TestCase):

    def setUp(self):
        self.operation = mock.Mock()
        self.operation.can_paginate = True
        self.foo_param = mock.Mock()
        self.foo_param.cli_name = 'foo'
        self.foo_param.name = 'Foo'
        self.foo_param.type = 'string'
        self.bar_param = mock.Mock()
        self.bar_param.cli_name = 'bar'
        self.bar_param.type = 'string'
        self.bar_param.name = 'Bar'
        self.params = [self.foo_param, self.bar_param]
        self.operation.pagination = {
            'input_token': 'Foo',
            'limit_key': 'Bar',
        }
        self.operation.params = self.params


class TestArgumentTableModifications(TestPaginateBase):

    def test_customize_arg_table(self):
        argument_table = {
            'foo': mock.Mock(),
            'bar': mock.Mock(),
        }
        paginate.unify_paging_params(argument_table, self.operation,
                                     'building-argument-table.foo.bar')
        # We should mark the built in input_token as 'hidden'.
        self.assertTrue(argument_table['foo']._UNDOCUMENTED)
        # Also need to hide the limit key.
        self.assertTrue(argument_table['bar']._UNDOCUMENTED)
        # We also need to inject startin-token and max-items.
        self.assertIn('starting-token', argument_table)
        self.assertIn('max-items', argument_table)
        # And these should be PageArguments.
        self.assertIsInstance(argument_table['starting-token'],
                              paginate.PageArgument)
        self.assertIsInstance(argument_table['max-items'],
                              paginate.PageArgument)

    def test_operation_with_no_paginate(self):
        # Operations that don't paginate are left alone.
        self.operation.can_paginate = False
        argument_table = {
            'foo': 'FakeArgObject',
            'bar': 'FakeArgObject',
        }
        starting_table = argument_table.copy()
        paginate.unify_paging_params(argument_table, self.operation,
                                     'building-argument-table.foo.bar')
        self.assertEqual(starting_table, argument_table)


class TestStringLimitKey(TestPaginateBase):

    def setUp(self):
        super(TestStringLimitKey, self).setUp()
        self.bar_param.type = 'string'

    def test_integer_limit_key(self):
        argument_table = {
            'foo': mock.Mock(),
            'bar': mock.Mock(),
        }
        paginate.unify_paging_params(argument_table, self.operation,
                                     'building-argument-table.foo.bar')
        # Max items should be the same type as bar, which may not be an int
        self.assertEqual('string', argument_table['max-items'].cli_type_name)


class TestIntegerLimitKey(TestPaginateBase):

    def setUp(self):
        super(TestIntegerLimitKey, self).setUp()
        self.bar_param.type = 'integer'

    def test_integer_limit_key(self):
        argument_table = {
            'foo': mock.Mock(),
            'bar': mock.Mock(),
        }
        paginate.unify_paging_params(argument_table, self.operation,
                                     'building-argument-table.foo.bar')
        # Max items should be the same type as bar, which may not be an int
        self.assertEqual('integer', argument_table['max-items'].cli_type_name)


class TestBadLimitKey(TestPaginateBase):

    def setUp(self):
        super(TestBadLimitKey, self).setUp()
        self.bar_param.type = 'bad'

    def test_integer_limit_key(self):
        argument_table = {
            'foo': mock.Mock(),
            'bar': mock.Mock(),
        }
        with self.assertRaises(TypeError):
            paginate.unify_paging_params(argument_table, self.operation,
                                         'building-argument-table.foo.bar')


class TestShouldEnablePagination(TestPaginateBase):
    def setUp(self):
        super(TestShouldEnablePagination, self).setUp()
        self.parsed_globals = mock.Mock()
        self.parsed_args = mock.Mock()

    def test_should_not_enable_pagination(self):
        # Here the user has specified a manual pagination argument,
        # so we should turn pagination off.
        # From setUp(), the limit_key is 'Bar'
        input_tokens = ['foo', 'bar']
        self.parsed_globals.paginate = True
        # Corresponds to --bar 10
        self.parsed_args.foo = None
        self.parsed_args.bar = 10
        paginate.check_should_enable_pagination(
            input_tokens, self.parsed_args, self.parsed_globals)
        # We should have turned paginate off because the
        # user specified --bar 10
        self.assertFalse(self.parsed_globals.paginate)

    def test_should_enable_pagination_with_no_args(self):
        input_tokens = ['foo', 'bar']
        self.parsed_globals.paginate = True
        # Corresponds to not specifying --foo nor --bar
        self.parsed_args.foo = None
        self.parsed_args.bar = None
        paginate.check_should_enable_pagination(
            input_tokens, self.parsed_args, self.parsed_globals)
        # We should have turned paginate off because the
        # user specified --bar 10
        self.assertTrue(self.parsed_globals.paginate)

    def test_default_to_pagination_on_when_ambiguous(self):
        input_tokens = ['foo', 'max-items']
        self.parsed_globals.paginate = True
        # Here the user specifies --max-items 10 This is ambiguous because the
        # input_token also contains 'max-items'.  Should we assume they want
        # pagination turned off or should we assume that this is the normalized
        # --max-items?
        # Will we default to assuming they meant the normalized
        # --max-items.
        self.parsed_args.foo = None
        self.parsed_args.max_items = 10
        paginate.check_should_enable_pagination(
            input_tokens, self.parsed_args, self.parsed_globals)
        self.assertTrue(self.parsed_globals.paginate,
                        "Pagination was not enabled.")

########NEW FILE########
__FILENAME__ = test_preview
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import mock
import six

from awscli.customizations import preview
from awscli.testutils import BaseAWSCommandParamsTest


class TestPreviewMode(BaseAWSCommandParamsTest):

    def setUp(self):
        super(TestPreviewMode, self).setUp()
        self.stderr = six.StringIO()
        self.stderr_patch = mock.patch('sys.stderr', self.stderr)
        self.stderr_patch.start()
        self.full_config = {'profiles': {}}
        # Implementation detail, but we want to patch out the
        # session config, as that's the only way to control
        # preview services.
        self.driver.session._config=  self.full_config

    def tearDown(self):
        super(TestPreviewMode, self).tearDown()
        self.stderr_patch.stop()

    def test_invoke_preview_mode_service(self):
        # By default cloudfront is a preview service.
        # We check this to make sure we fail loudly if we
        # ever mark cloudfront as not being a preview service
        # by default.
        self.assertIn('cloudfront', preview.PREVIEW_SERVICES)
        rc = self.driver.main('cloudfront help'.split())
        self.assertEqual(rc, 1)
        self.assertIn(preview.PREVIEW_SERVICES['cloudfront'],
                      self.stderr.getvalue())

    @mock.patch('awscli.help.get_renderer')
    def test_preview_service_off(self, get_renderer):
        self.full_config['preview'] = {'cloudfront': 'true'}
        renderer = mock.Mock()
        get_renderer.return_value = renderer
        self.driver.main('cloudfront help'.split())
        # In this case, the normal help processing should have occurred
        # and we check that we rendered the contents.
        self.assertTrue(renderer.render.called)

    def test_preview_service_not_true(self):
        # If it's not "true" then we still make it a preview service.
        self.full_config['preview'] = {'cloudfront': 'false'}
        rc = self.driver.main('cloudfront help'.split())
        self.assertEqual(rc, 1)
        self.assertIn(preview.PREVIEW_SERVICES['cloudfront'],
                      self.stderr.getvalue())

    @mock.patch('awscli.help.get_renderer')
    def test_preview_mode_not_in_provider_help(self, renderer):
        self.driver.main(['help'])
        contents = renderer.return_value.render.call_args
        # The preview services should not be in the help output.
        for service in preview.PREVIEW_SERVICES:
            self.assertNotIn(service, contents)

########NEW FILE########
__FILENAME__ = test_utils
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest
from awscli.testutils import BaseAWSHelpOutputTest

import mock

from awscli.customizations import utils


class FakeParsedArgs(object):
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)


class TestCommandTableRenames(BaseAWSHelpOutputTest):

    def test_rename_command_table(self):
        handler = lambda command_table, **kwargs: utils.rename_command(
            command_table, 'ec2', 'fooec2')
        # Verify that we can rename a top level command.
        self.session.register('building-command-table.main', handler)
        self.driver.main(['fooec2', 'help'])
        self.assert_contains('fooec2')

        # We can also see subcommands help as well.
        self.driver.main(['fooec2', 'run-instances', 'help'])
        self.assert_contains('run-instances')


class TestValidateMututuallyExclusiveGroups(unittest.TestCase):
    def test_two_single_groups(self):
        # The most basic example of mutually exclusive args.
        # If foo is specified, but bar is not, then we're fine.
        parsed = FakeParsedArgs(foo='one', bar=None)
        utils.validate_mutually_exclusive(parsed, ['foo'], ['bar'])
        # If bar is specified and foo is not, then we're fine.
        parsed = FakeParsedArgs(foo=None, bar='one')
        utils.validate_mutually_exclusive(parsed, ['foo'], ['bar'])
        # But if we specify both foo and bar then we get an error.
        parsed = FakeParsedArgs(foo='one', bar='two')
        with self.assertRaises(ValueError):
            utils.validate_mutually_exclusive(parsed, ['foo'], ['bar'])


    def test_multiple_groups(self):
        groups = (['one', 'two', 'three'], ['foo', 'bar', 'baz'],
                  ['qux', 'bad', 'morebad'])
        # This is fine.
        parsed = FakeParsedArgs(foo='foo', bar='bar', baz='baz')
        utils.validate_mutually_exclusive(parsed, *groups)
        # But this is bad.
        parsed = FakeParsedArgs(foo='one', bar=None, qux='three')
        with self.assertRaises(ValueError):
            utils.validate_mutually_exclusive(parsed, *groups)

########NEW FILE########
__FILENAME__ = test_query_objects
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestDataPipelineQueryObjects(BaseAWSCommandParamsTest):
    maxDiff = None

    prefix = 'datapipeline query-objects '

    def test_renamed_object_query_arg(self):
        # --query is renamed to --objects-query so we don't
        # conflict with the global --query argument.
        args = ('--pipeline-id foo '
                '--sphere INSTANCE '
                '--objects-query {"selectors":[{"fieldName":"@status",'
                '"operator":{"type":"EQ","values":["RUNNING"]}}]}')
        cmdline = self.prefix + args
        expected = {
            'pipelineId': 'foo',
            'query': {'selectors': [{'fieldName': '@status',
                                     'operator': {'type': 'EQ',
                                                  'values': ['RUNNING']}}]},
            'sphere': 'INSTANCE'
        }
        self.assert_params_for_cmd(cmdline, expected)

########NEW FILE########
__FILENAME__ = test_examples
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""Test help output for the AWS CLI.

The purpose of these docs is to test that the generated output looks how
we expect.

It's intended to be as end to end as possible, but instead of looking
at the man output, we look one step before at the generated rst output
(it's easier to verify).

"""
from awscli.testutils import BaseAWSHelpOutputTest


# Mapping of command names to subcommands that have examples in their help
# output.  This isn't mean to be an exhaustive list, but should help catch
# things like command table renames, virtual commands, etc.
COMMAND_EXAMPLES = {
    'cloudwatch': ['put-metric-data'],
    's3': ['cp', 'ls', 'mb', 'mv', 'rb', 'rm', 'sync'],
    's3api': ['get-object', 'put-object'],
    'ec2': ['run-instances', 'start-instances', 'stop-instances'],
    'swf': ['deprecate-domain', 'describe-domain'],
    'sqs': ['create-queue', 'get-queue-attributes'],
}


class _ExampleTests(BaseAWSHelpOutputTest):
    def noop_test(self):
        pass


def test_examples():
    for command, subcommands in COMMAND_EXAMPLES.items():
        for subcommand in subcommands:
            yield verify_has_examples, command, subcommand


def verify_has_examples(command, subcommand):
    t = _ExampleTests(methodName='noop_test')
    t.setUp()
    try:
        t.driver.main([command, subcommand, 'help'])
        t.assert_contains('========\nExamples\n========')
    finally:
        t.tearDown()

########NEW FILE########
__FILENAME__ = test_help_output
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
"""Test help output for the AWS CLI.

The purpose of these docs is to test that the generated output looks how
we expect.

It's intended to be as end to end as possible, but instead of looking
at the man output, we look one step before at the generated rst output
(it's easier to verify).

"""
from awscli.testutils import BaseAWSHelpOutputTest

import six
import mock


class TestHelpOutput(BaseAWSHelpOutputTest):
    def test_output(self):
        self.driver.main(['help'])
        self.assert_contains('***\naws\n***')
        self.assert_contains(
            'The AWS Command Line Interface is a unified tool '
            'to manage your AWS services.')
        # Verify we see the docs for top level params, so pick
        # a few representative types of params.
        self.assert_contains('``--endpoint-url``')
        # Boolean type
        self.assert_contains('``--no-paginate``')
        # Arg with choices
        self.assert_contains('``--color``')
        self.assert_contains('* on')
        self.assert_contains('* off')
        self.assert_contains('* auto')
        # Then we should see the services.
        self.assert_contains('* ec2')
        self.assert_contains('* s3api')
        self.assert_contains('* sts')

    def test_service_help_output(self):
        self.driver.main(['ec2', 'help'])
        # We should see the section title for the service.
        self.assert_contains('***\nec2\n***')
        # With a description header.
        self.assert_contains('===========\nDescription\n===========')
        # And we should see the operations listed.
        self.assert_contains('* monitor-instances')
        self.assert_contains('* run-instances')
        self.assert_contains('* describe-instances')

    def test_operation_help_output(self):
        self.driver.main(['ec2', 'run-instances', 'help'])
        # Should see the title with the operation name
        self.assert_contains('*************\nrun-instances\n*************')
        # Should contain part of the help text from the model.
        self.assert_contains('Launches the specified number of instances')
        self.assert_contains('``--count`` (string)')

    def test_arguments_with_example_json_syntax(self):
        self.driver.main(['ec2', 'run-instances', 'help'])
        self.assert_contains('``--iam-instance-profile``')
        self.assert_contains('JSON Syntax')
        self.assert_contains('"Arn": "string"')
        self.assert_contains('"Name": "string"')

    def test_arguments_with_example_shorthand_syntax(self):
        self.driver.main(['ec2', 'run-instances', 'help'])
        self.assert_contains('``--iam-instance-profile``')
        self.assert_contains('Shorthand Syntax')
        self.assert_contains('--iam-instance-profile Arn=value,Name=value')

    def test_required_args_come_before_optional_args(self):
        self.driver.main(['ec2', 'run-instances', 'help'])
        # We're asserting that the args in the synopsis section appear
        # in this order.  They don't have to be in this exact order, but
        # each item in the list has to come before the previous arg.
        self.assert_text_order(
            '--image-id <value>',
            '[--key-name <value>]',
            '[--security-groups <value>]', starting_from='Synopsis')

    def test_service_operation_order(self):
        self.driver.main(['ec2', 'help'])
        self.assert_text_order(
            'activate-license',
            'allocate-address',
            'assign-private-ip-addresses', starting_from='Available Commands')

    def test_top_level_args_order(self):
        self.driver.main(['help'])
        self.assert_text_order(
            'autoscaling\n', 'cloudformation\n', 'elb\n', 'swf\n',
            starting_from='Available Services')

    def test_examples_in_operation_help(self):
        self.driver.main(['ec2', 'run-instances', 'help'])
        self.assert_contains('========\nExamples\n========')

    def test_add_help_for_dryrun(self):
        self.driver.main(['ec2', 'run-instances', 'help'])
        self.assert_contains('DryRunOperation')
        self.assert_contains('UnauthorizedOperation')

    def test_elb_help_output(self):
        self.driver.main(['elb', 'help'])
        # We should *not* have any invalid links like
        # .. _`:
        self.assert_not_contains('.. _`:')


class TestRemoveDeprecatedCommands(BaseAWSHelpOutputTest):
    def assert_command_does_not_exist(self, service, command):
        # Basically try to get the help output for the removed
        # command verify that we get a SystemExit exception
        # and that we get something in stderr that says that
        # we made an invalid choice (because the operation is removed).
        stderr = six.StringIO()
        with mock.patch('sys.stderr', stderr):
            with self.assertRaises(SystemExit):
                self.driver.main([service, command, 'help'])
        # We should see an error message complaining about
        # an invalid choice because the operation has been removed.
        self.assertIn('argument operation: Invalid choice', stderr.getvalue())

    def test_ses_deprecated_commands(self):
        self.driver.main(['ses', 'help'])
        self.assert_not_contains('list-verified-email-addresses')
        self.assert_not_contains('delete-verified-email-address')
        self.assert_not_contains('verify-email-address')

        self.assert_command_does_not_exist(
            'ses', 'list-verified-email-addresses')
        self.assert_command_does_not_exist(
            'ses', 'delete-verified-email-address')
        self.assert_command_does_not_exist(
            'ses', 'verify-email-address')

    def test_ec2_import_export(self):
        self.driver.main(['ec2', 'help'])
        self.assert_not_contains('import-instance')
        self.assert_not_contains('import-volume')
        self.assert_command_does_not_exist(
            'ec2', 'import-instance')
        self.assert_command_does_not_exist(
            'ec2', 'import-volume')

    def test_cloudformation(self):
        self.driver.main(['cloudformation', 'help'])
        self.assert_not_contains('estimate-template-cost')
        self.assert_command_does_not_exist(
            'cloudformation', 'estimate-template-cost')

    def test_boolean_param_documented(self):
        self.driver.main(['autoscaling',
                          'terminate-instance-in-auto-scaling-group', 'help'])
        self.assert_contains(
            ('``--should-decrement-desired-capacity`` | '
             '``--no-should-decrement-desired-capacity`` (boolean)'))

    def test_streaming_output_arg(self):
        self.driver.main(['s3api', 'get-object', 'help'])
        self.assert_not_contains('``--outfile``')
        self.assert_contains('``outfile`` (string)')

    def test_rds_add_arg_help_has_correct_command_name(self):
        self.driver.main(['rds', 'add-option-to-option-group', 'help'])
        self.assert_contains('add-option-to-option-group')

    def test_rds_remove_arg_help_has_correct_command_name(self):
        self.driver.main(['rds', 'remove-option-from-option-group', 'help'])
        self.assert_contains('remove-option-from-option-group')

    def test_modify_operation_not_in_help(self):
        self.driver.main(['rds', 'help'])
        # This was split into add/remove commands.  The modify
        # command should not be available.
        self.assert_not_contains('modify-option-group')


class TestPagingParamDocs(BaseAWSHelpOutputTest):
    def test_starting_token_injected(self):
        self.driver.main(['s3api', 'list-objects', 'help'])
        self.assert_contains('``--starting-token``')

    def test_max_items_injected(self):
        self.driver.main(['s3api', 'list-objects', 'help'])
        self.assert_contains('``--max-items``')

    def test_builtin_paging_params_removed(self):
        self.driver.main(['s3api', 'list-objects', 'help'])
        self.assert_not_contains('``--next-token``')
        self.assert_not_contains('``--max-keys``')


class TestMergeBooleanGroupArgs(BaseAWSHelpOutputTest):
    def test_merge_bool_args(self):
        # Boolean args need to be group together so rather than
        # --foo foo docs
        # --no-foo foo docs again
        #
        # We instead have:
        # --foo | --no-foo foo docs
        self.driver.main(['ec2', 'run-instances', 'help'])
        self.assert_contains('``--dry-run`` | ``--no-dry-run``')

    def test_top_level_bools(self):
        # structure(scalar) of a single value of Value whose value is
        # a boolean is pulled into a top level arg.
        self.driver.main(['ec2', 'modify-instance-attribute', 'help'])
        self.assert_contains('``--ebs-optimized`` | ``--no-ebs-optimized``')

    def test_top_level_bool_has_no_example(self):
        # Normally a structure(bool) param would have an example
        # of {"Value": true|false}", but when we pull the arg up into
        # a top level bool, we should not generate an example.
        self.driver.main(['ec2', 'modify-instance-attribute', 'help'])
        self.assert_not_contains('"Value": true|false')


class TestStructureScalarHasNoExamples(BaseAWSHelpOutputTest):
    def test_no_examples_for_structure_single_scalar(self):
        self.driver.main(['ec2', 'modify-instance-attribute', 'help'])
        self.assert_not_contains('"Value": "string"')
        self.assert_not_contains('Value=string')

    def test_example_for_single_structure_not_named_value(self):
        # Verify that if a structure does match our special case
        # (single element named "Value"), then we still document
        # the example syntax.
        self.driver.main(['s3api', 'restore-object', 'help'])
        self.assert_contains('Days=value')
        # Also should see the JSON syntax in the help output.
        self.assert_contains('"Days": integer')


class TestJSONListScalarDocs(BaseAWSHelpOutputTest):
    def test_space_separated_list_docs(self):
        # A list of scalar type can be specified as JSON:
        #      JSON Syntax:
        #
        #       ["string", ...]
        # But at the same time you can always replace that with
        # a space separated list.  Therefore we want to document
        # the space separated list version and not the JSON list
        # version.
        self.driver.main(['ec2', 'terminate-instances', 'help'])
        self.assert_not_contains('["string", ...]')
        self.assert_contains('"string" "string"')


class TestParamRename(BaseAWSHelpOutputTest):
    def test_create_image_renames(self):
        # We're just cherry picking this particular operation to verify
        # that the rename arg customizations are working.
        self.driver.main(['ec2', 'create-image', 'help'])
        self.assert_not_contains('no-no-reboot')
        self.assert_contains('--reboot')

class TestCustomCommandDocsFromFile(BaseAWSHelpOutputTest):
    def test_description_from_rst_file(self):
        # The description for the configure command
        # is in _description.rst.  We're verifying that we
        # can read those contents properly.
        self.driver.main(['configure', 'help'])
        # These are a few options that are documented in the help output.
        self.assert_contains('metadata_service_timeout')
        self.assert_contains('metadata_service_num_attempts')
        self.assert_contains('aws_access_key_id')

class TestEnumDocsArentDuplicated(BaseAWSHelpOutputTest):
    def test_enum_docs_arent_duplicated(self):
        # Test for: https://github.com/aws/aws-cli/issues/609
        # What's happening is if you have a list param that has
        # an enum, we document it as:
        # a|b|c|d   a|b|c|d
        # Except we show all of the possible enum params twice.
        # Each enum param should only occur once.  The ideal documentation
        # should be:
        #
        # string1 string2
        #
        # Where each value is one of:
        #     value1
        #     value2
        self.driver.main(['cloudformation', 'list-stacks', 'help'])
        # "CREATE_IN_PROGRESS" is a enum value, and should only
        # appear once in the help output.
        contents = self.renderer.rendered_contents
        self.assertTrue(contents.count("CREATE_IN_PROGRESS") == 1,
                        ("Enum param was only suppose to be appear once in "
                         "rendered doc output."))


class TestParametersCanBeHidden(BaseAWSHelpOutputTest):
    def mark_as_undocumented(self, argument_table, **kwargs):
        argument_table['starting-sequence-number']._UNDOCUMENTED = True

    def test_hidden_params_are_not_documented(self):
        # We're going to demonstrate hiding a parameter.
        # --device
        self.driver.session.register('building-argument-table',
                                     self.mark_as_undocumented)
        self.driver.main(['kinesis', 'get-shard-iterator', 'help'])
        self.assert_not_contains('--starting-sequence-number')

########NEW FILE########
__FILENAME__ = test_associate_address
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestAssociateAddress(BaseAWSCommandParamsTest):

    prefix = 'ec2 associate-address'

    def test_basic(self):
        cmdline = self.prefix
        cmdline += ' --instance-id i-12345678'
        cmdline += ' --public-ip 192.168.0.0'
        result = {'InstanceId': 'i-12345678', 'PublicIp': '192.168.0.0'}
        self.assert_params_for_cmd(cmdline, result)

    def test_vpc_basic(self):
        cmdline = self.prefix
        cmdline += ' --instance-id i-12345678'
        cmdline += ' --public-ip 192.168.0.0'
        cmdline += ' --allocation-id eipalloc-12345678'
        cmdline += ' --allow-reassociation'
        result = {'InstanceId': 'i-12345678',
                  'PublicIp': '192.168.0.0',
                  'AllowReassociation': 'true',
                  'AllocationId': 'eipalloc-12345678'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_attach_internet_gateway
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestAttachInternetGateway(BaseAWSCommandParamsTest):

    prefix = 'ec2 attach-internet-gateway'

    def test_both_params(self):
        cmdline = self.prefix
        cmdline += ' --vpc-id vpc-12345678'
        cmdline += ' --internet-gateway-id igw-12345678'
        result = {'VpcId': 'vpc-12345678',
                  'InternetGatewayId': 'igw-12345678'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_bundle_instance
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import awscli.customizations.ec2bundleinstance

import datetime
import base64

import six
from six.moves import cStringIO
import mock

class TestBundleInstance(BaseAWSCommandParamsTest):

    prefix = 'ec2 bundle-instance'

    Base64Policy = ('eyJleHBpcmF0aW9uIjogIjIwMTMtMDgtMTBUMDA6MDA6MDAuM'
                    'DAwMDAwWiIsImNvbmRpdGlvbnMiOiBbeyJidWNrZXQiOiAibXl'
                    'idWNrZXQifSx7ImFjbCI6ICJlYzItYnVuZGxlLXJlYWQifSxbI'
                    'nN0YXJ0cy13aXRoIiwgIiRrZXkiLCAiZm9vYmFyIl1dfQ==')

    PolicySignature = 'ynxybUMv9YuGbPl7HZ8AFJW/2t0='
    
    def setUp(self):
        super(TestBundleInstance, self).setUp()
        # This mocks out datetime.datetime.utcnow() so that it always
        # returns the same datetime object.  This is because this value
        # is embedded into the policy file that is generated and we
        # don't what the policy or its signature to change each time
        # we run the test.
        self.datetime_patcher = mock.patch.object(
            awscli.customizations.ec2bundleinstance.datetime, 'datetime',  
            mock.Mock(wraps=datetime.datetime)
        )
        mocked_datetime = self.datetime_patcher.start()
        mocked_datetime.utcnow.return_value = datetime.datetime(2013, 8, 9)


    def tearDown(self):
        super(TestBundleInstance, self).tearDown()
        self.datetime_patcher.stop
        
    def test_no_policy_provided(self):
        args = ' --instance-id i-12345678 --owner-akid AKIAIOSFODNN7EXAMPLE'
        args += ' --owner-sak wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'
        args += ' --bucket mybucket --prefix foobar'
        args_list = (self.prefix + args).split()
        result =  {'InstanceId': 'i-12345678',
                   'Storage.S3.Bucket': 'mybucket',
                   'Storage.S3.Prefix': 'foobar',
                   'Storage.S3.AWSAccessKeyId': 'AKIAIOSFODNN7EXAMPLE',
                   'Storage.S3.UploadPolicy': self.Base64Policy,
                   'Storage.S3.UploadPolicySignature': self.PolicySignature}
        self.assert_params_for_cmd(args_list, result)
        

    def test_policy_provided(self):
        policy = '{"notarealpolicy":true}'
        base64policy = base64.encodestring(six.b(policy)).strip().decode('utf-8')
        policy_signature = 'a5SmoLOxoM0MHpOdC25nE7KIafg='
        args = ' --instance-id i-12345678 --owner-akid AKIAIOSFODNN7EXAMPLE'
        args += ' --owner-sak wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY'
        args += ' --bucket mybucket --prefix foobar --policy %s' % policy
        args_list = (self.prefix + args).split()
        result =  {'InstanceId': 'i-12345678',
                   'Storage.S3.Bucket': 'mybucket',
                   'Storage.S3.Prefix': 'foobar',
                   'Storage.S3.AWSAccessKeyId': 'AKIAIOSFODNN7EXAMPLE',
                   'Storage.S3.UploadPolicy': base64policy,
                   'Storage.S3.UploadPolicySignature': policy_signature}
        self.assert_params_for_cmd(args_list, result)

    def test_both(self):
        captured = cStringIO()
        json = """{"S3":{"Bucket":"foobar","Prefix":"fiebaz"}}"""
        args = ' --instance-id i-12345678 --owner-aki blah --owner-sak blah --storage %s' % json
        args_list = (self.prefix + args).split()
        with mock.patch('sys.stderr', captured):
            self.assert_params_for_cmd(args_list, {}, expected_rc=255)

        
if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_create_image
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestDescribeInstances(BaseAWSCommandParamsTest):

    prefix = 'ec2 create-image'

    def test_renamed_reboot_arg(self):
        cmdline = self.prefix
        cmdline += ' --instance-id i-12345678 --description foo --name bar'
        # --reboot is a customized renamed arg.  Verifying it still
        # gets mapped as 'NoReboot': 'false'.
        cmdline += ' --reboot'
        result = {'InstanceId': 'i-12345678', 'Description': 'foo',
                  'Name': 'bar', 'NoReboot': 'false'}
        self.assert_params_for_cmd(cmdline, result)

########NEW FILE########
__FILENAME__ = test_create_network_acl_entry
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestCreateNetworkACLEntry(BaseAWSCommandParamsTest):

    prefix = 'ec2 create-network-acl-entry'

    def test_tcp(self):
        cmdline = self.prefix
        cmdline += ' --network-acl-id acl-12345678'
        cmdline += ' --rule-number 100'
        cmdline += ' --protocol tcp'
        cmdline += ' --rule-action allow'
        cmdline += ' --ingress'
        cmdline += ' --port-range From=22,To=22'
        cmdline += ' --cidr-block 0.0.0.0/0'
        result = {'NetworkAclId': 'acl-12345678',
                  'RuleNumber': '100',
                  'Protocol': '6',
                  'RuleAction': 'allow',
                  'Egress': 'false',
                  'CidrBlock': '0.0.0.0/0',
                  'PortRange.From': '22',
                  'PortRange.To': '22'
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_udp(self):
        cmdline = self.prefix
        cmdline += ' --network-acl-id acl-12345678'
        cmdline += ' --rule-number 100'
        cmdline += ' --protocol udp'
        cmdline += ' --rule-action allow'
        cmdline += ' --ingress'
        cmdline += ' --port-range From=22,To=22'
        cmdline += ' --cidr-block 0.0.0.0/0'
        result = {'NetworkAclId': 'acl-12345678',
                  'RuleNumber': '100',
                  'Protocol': '17',
                  'RuleAction': 'allow',
                  'Egress': 'false',
                  'CidrBlock': '0.0.0.0/0',
                  'PortRange.From': '22',
                  'PortRange.To': '22'
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_icmp(self):
        cmdline = self.prefix
        cmdline += ' --network-acl-id acl-12345678'
        cmdline += ' --rule-number 100'
        cmdline += ' --protocol icmp'
        cmdline += ' --rule-action allow'
        cmdline += ' --ingress'
        cmdline += ' --port-range From=22,To=22'
        cmdline += ' --cidr-block 0.0.0.0/0'
        result = {'NetworkAclId': 'acl-12345678',
                  'RuleNumber': '100',
                  'Protocol': '1',
                  'RuleAction': 'allow',
                  'Egress': 'false',
                  'CidrBlock': '0.0.0.0/0',
                  'PortRange.From': '22',
                  'PortRange.To': '22'
                  }
        self.assert_params_for_cmd(cmdline, result)
        
    def test_all(self):
        cmdline = self.prefix
        cmdline += ' --network-acl-id acl-12345678'
        cmdline += ' --rule-number 100'
        cmdline += ' --protocol all'
        cmdline += ' --rule-action allow'
        cmdline += ' --ingress'
        cmdline += ' --port-range From=22,To=22'
        cmdline += ' --cidr-block 0.0.0.0/0'
        result = {'NetworkAclId': 'acl-12345678',
                  'RuleNumber': '100',
                  'Protocol': '-1',
                  'RuleAction': 'allow',
                  'Egress': 'false',
                  'CidrBlock': '0.0.0.0/0',
                  'PortRange.From': '22',
                  'PortRange.To': '22'
                  }
        self.assert_params_for_cmd(cmdline, result)
        
    def test_number(self):
        cmdline = self.prefix
        cmdline += ' --network-acl-id acl-12345678'
        cmdline += ' --rule-number 100'
        cmdline += ' --protocol 99'
        cmdline += ' --rule-action allow'
        cmdline += ' --ingress'
        cmdline += ' --port-range From=22,To=22'
        cmdline += ' --cidr-block 0.0.0.0/0'
        result = {'NetworkAclId': 'acl-12345678',
                  'RuleNumber': '100',
                  'Protocol': '99',
                  'RuleAction': 'allow',
                  'Egress': 'false',
                  'CidrBlock': '0.0.0.0/0',
                  'PortRange.From': '22',
                  'PortRange.To': '22'
                  }
        self.assert_params_for_cmd(cmdline, result)
        

########NEW FILE########
__FILENAME__ = test_create_tags
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import sys

import six
from awscli.testutils import unittest
from awscli.testutils import BaseAWSCommandParamsTest


class TestCreateTags(BaseAWSCommandParamsTest):

    prefix = 'ec2 create-tags'

    def test_create_tag_normal(self):
        cmdline = self.prefix
        cmdline += ' --resources i-12345678 --tags Key=Name,Value=bar'
        result = {'ResourceId.1': 'i-12345678', 'Tag.1.Key': 'Name',
                  'Tag.1.Value': 'bar'}
        self.assert_params_for_cmd(cmdline, result)

    @unittest.skipIf(
        six.PY3, 'Unicode cmd line test only is relevant to python2.')
    def test_create_tag_unicode(self):
        cmdline = self.prefix
        cmdline += u' --resources i-12345678 --tags Key=Name,Value=\u6211'
        encoding = getattr(sys.stdin, 'encoding', 'utf-8')
        if encoding is None:
            encoding = 'utf-8'
        cmdline = cmdline.encode(encoding)
        result = {'ResourceId.1': 'i-12345678', 'Tag.1.Key': 'Name',
                  'Tag.1.Value': u'\u6211'}
        self.assert_params_for_cmd(cmdline, result)

########NEW FILE########
__FILENAME__ = test_describe_instances
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestDescribeInstances(BaseAWSCommandParamsTest):

    prefix = 'ec2 describe-instances'

    def test_no_params(self):
        cmdline = self.prefix
        result = {}
        self.assert_params_for_cmd(cmdline, result)

    def test_instance_id(self):
        args = ' --instance-ids i-12345678'
        cmdline = self.prefix + args
        result = {'InstanceId.1': 'i-12345678'}
        self.assert_params_for_cmd(cmdline, result)

    def test_instance_ids(self):
        args = ' --instance-ids i-12345678 i-87654321'
        cmdline = self.prefix + args
        result = {'InstanceId.1': 'i-12345678', 'InstanceId.2': 'i-87654321'}
        self.assert_params_for_cmd(cmdline, result)

    def test_instance_ids_alternate(self):
        # Not required, but will still work if you use JSON.
        args = ' --instance-ids ["i-12345678","i-87654321"]'
        cmdline = self.prefix + args
        result = {'InstanceId.1': 'i-12345678', 'InstanceId.2': 'i-87654321'}
        self.assert_params_for_cmd(cmdline, result)

    def test_filter_json(self):
        args = """ --filters {"Name":"group-name","Values":["foobar"]}"""
        cmdline = self.prefix + args
        result = {'Filter.1.Value.1': 'foobar', 'Filter.1.Name': 'group-name'}
        self.assert_params_for_cmd(cmdline, result)

    def test_filter_simple(self):
        args = """ --filters Name=group-name,Values=foobar"""
        cmdline = self.prefix + args
        result = {'Filter.1.Value.1': 'foobar', 'Filter.1.Name': 'group-name'}
        self.assert_params_for_cmd(cmdline, result)

    def test_filter_values(self):
        args = """ --filters Name=group-name,Values=foobar,fiebaz"""
        cmdline = self.prefix + args
        result = {'Filter.1.Value.2': 'fiebaz',
                  'Filter.1.Value.1': 'foobar',
                  'Filter.1.Name': 'group-name'}
        self.assert_params_for_cmd(cmdline, result)

    def test_multiple_filters(self):
        args = (' --filters Name=group-name,Values=foobar '
                'Name=instance-id,Values=i-12345')
        cmdline = self.prefix + args
        result = {
            'Filter.1.Name': 'group-name',
            'Filter.1.Value.1': 'foobar',
            'Filter.2.Name': 'instance-id',
            'Filter.2.Value.1': 'i-12345',
        }
        self.assert_params_for_cmd(cmdline, result)

    def test_multiple_filters_alternate(self):
        cmdlist = 'ec2 describe-instances'.split()
        cmdlist.extend(['--filters',
                        'Name = group-name, Values= foobar',
                        'Name=instance-id,Values=i-12345'])
        result = {
            'Filter.1.Name': 'group-name',
            'Filter.1.Value.1': 'foobar',
            'Filter.2.Name': 'instance-id',
            'Filter.2.Value.1': 'i-12345',
        }
        self.assert_params_for_cmd(cmdlist, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_describe_instance_attribute
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import unittest


class TestDescribeInstanceAttribute(BaseAWSCommandParamsTest):

    prefix = 'ec2 describe-instance-attribute'

    def test_both_params(self):
        cmdline = self.prefix
        cmdline += ' --instance-id i-12345678'
        cmdline += ' --attribute blockDeviceMapping'
        result = {'InstanceId': 'i-12345678',
                  'Attribute': 'blockDeviceMapping'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_get_password_data
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import os


PASSWORD_DATA = ("GWDnuoj/7pbMQkg125E8oGMUVCI+r98sGbFFl8SX+dEYxMZzz+byYwwjvyg8i"
                 "SGKaLuLTIWatWopVu5cMWDKH65U4YFL2g3LqyajBrCFnuSE1piTeS/rPQpoSv"
                 "BN5FGj9HWqNrglWAJgh9OZNSGgpEojBenL/0rwSpDWL7f/f52M5doYA6q+v0y"
                 "gEoi1Wq6hcmrBfyA4seW1RlKgnUru5Y9oc1hFHi53E3b1EkjGqCsCemVUwumB"
                 "j8uwCLJRaMcqrCxK1smtAsiSqk0Jk9jpN2vcQgnMPypEdmEEXyWHwq55fjy6c"
                 "h+sqYcwumIL5QcFW2JQ5+XBEoFhC66gOsAXow==")


class TestGetPasswordData(BaseAWSCommandParamsTest):

    prefix = 'ec2 get-password-data'

    def setUp(self):
        super(TestGetPasswordData, self).setUp()
        self.parsed_response = {'InstanceId': 'i-12345678',
                                'Timestamp': '2013-07-27T18:29:23.000Z',
                                'PasswordData': PASSWORD_DATA}

    def test_no_priv_launch_key(self):
        args = ' --instance-id i-12345678'
        cmdline = self.prefix + args
        result = {'InstanceId': 'i-12345678'}
        output = self.assert_params_for_cmd(cmdline, result, expected_rc=0)[0]
        self.assertIn('"InstanceId": "i-12345678"', output)
        self.assertIn('"Timestamp": "2013-07-27T18:29:23.000Z"', output)
        self.assertIn('"PasswordData": "%s"' % PASSWORD_DATA, output)

    def test_nonexistent_priv_launch_key(self):
        args = ' --instance-id i-12345678 --priv-launch-key foo.pem'
        cmdline = self.prefix + args
        result = {}
        error_msg = self.assert_params_for_cmd(
            cmdline, result, expected_rc=255)[1]
        self.assertIn('priv-launch-key should be a path to '
                      'the local SSH private key file used '
                      'to launch the instance.\n', error_msg)

    def test_priv_launch_key(self):
        key_path = os.path.join(os.path.dirname(__file__),
                                'testcli.pem')
        args = ' --instance-id i-12345678 --priv-launch-key %s' % key_path
        cmdline = self.prefix + args
        result = {'InstanceId': 'i-12345678'}
        output = self.assert_params_for_cmd(cmdline, result, expected_rc=0)[0]
        self.assertIn('"InstanceId": "i-12345678"', output)
        self.assertIn('"Timestamp": "2013-07-27T18:29:23.000Z"', output)
        self.assertIn('"PasswordData": "=mG8.r$o-s"', output)

########NEW FILE########
__FILENAME__ = test_modify_image_attribute
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestModifyInstanceAttribute(BaseAWSCommandParamsTest):

    prefix = 'ec2 modify-image-attribute'

    def test_one(self):
        cmdline = self.prefix
        cmdline += ' --image-id ami-d00dbeef'
        cmdline += ' --operation-type add'
        cmdline += ' --user-ids 0123456789012'
        result = {'ImageId': 'ami-d00dbeef',
                  'OperationType': 'add',
                  'UserId.1': '0123456789012'}
        self.assert_params_for_cmd(cmdline, result)

    def test_two(self):
        cmdline = self.prefix
        cmdline += ' --image-id ami-d00dbeef'
        cmdline += (' --launch-permission {"Add":[{"UserId":"123456789012"}],'
                    '"Remove":[{"Group":"all"}]}')
        result = {'ImageId': 'ami-d00dbeef',
                  'LaunchPermission.Add.1.UserId': '123456789012',
                  'LaunchPermission.Remove.1.Group': 'all',
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_assert_error_in_bad_json_path(self):
        cmdline = self.prefix
        cmdline += ' --image-id ami-d00dbeef'
        cmdline += ' --launch-permission THISISNOTJSON'
        # The arg name should be in the error message.
        self.assert_params_for_cmd(cmdline, {}, expected_rc=255,
                                   stderr_contains='launch-permission')


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_modify_instance_attribute
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestModifyInstanceAttribute(BaseAWSCommandParamsTest):

    prefix = 'ec2 modify-instance-attribute '

    def setUp(self):
        super(TestModifyInstanceAttribute, self).setUp()
        self.expected_result = {
            'InstanceId': 'i-1234',
            'InstanceInitiatedShutdownBehavior.Value': 'terminate',
        }

    def test_json_version(self):
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        cmdline += '--instance-initiated-shutdown-behavior {"Value":"terminate"}'
        self.assert_params_for_cmd(cmdline, self.expected_result)

    def test_shorthand_version(self):
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        cmdline += '--instance-initiated-shutdown-behavior Value=terminate'
        self.assert_params_for_cmd(cmdline, self.expected_result)

    def test_value_not_needed(self):
        # For structs of a single param value, you can skip the keep name,
        # so instead of Value=terminate, you can just say terminate.
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        cmdline += '--instance-initiated-shutdown-behavior terminate'
        self.assert_params_for_cmd(cmdline, self.expected_result)

    def test_boolean_value_in_top_level_true(self):
        # Just like everything else in argparse, the last value provided
        # for a destination has precedence.
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        cmdline += '--ebs-optimized Value=true'
        result = {'InstanceId': 'i-1234',
                  'EbsOptimized.Value': 'true',
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_boolean_value_is_top_level_false(self):
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        cmdline += '--ebs-optimized Value=false'
        result = {'InstanceId': 'i-1234',
                  'EbsOptimized.Value': 'false',
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_boolean_value_in_top_level_true_json(self):
        # Just like everything else in argparse, the last value provided
        # for a destination has precedence.
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        cmdline += '--ebs-optimized {"Value":true}'
        result = {'InstanceId': 'i-1234',
                  'EbsOptimized.Value': 'true',
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_boolean_value_is_top_level_false_json(self):
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        cmdline += '--ebs-optimized {"Value":false}'
        result = {'InstanceId': 'i-1234',
                  'EbsOptimized.Value': 'false',
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_boolean_param_top_level_true_no_value(self):
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        cmdline += '--ebs-optimized'
        result = {'InstanceId': 'i-1234',
                  'EbsOptimized.Value': 'true',
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_boolean_param_top_level_false_no_value(self):
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        cmdline += '--no-ebs-optimized'
        result = {'InstanceId': 'i-1234',
                  'EbsOptimized.Value': 'false',
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_mix_value_non_value_boolean_param(self):
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        # Can't mix non-value + value version of the arg.
        cmdline += '--no-ebs-optimized '
        cmdline += '--ebs-optimized Value=true'
        self.assert_params_for_cmd(cmdline, expected_rc=255,
                                   stderr_contains='Cannot specify both')

    def test_mix_non_value_bools_not_allowed(self):
        cmdline = self.prefix
        cmdline += '--instance-id i-1234 '
        # Can't mix non-value + value version of the arg.
        cmdline += '--no-ebs-optimized '
        cmdline += '--ebs-optimized '
        self.assert_params_for_cmd(cmdline, expected_rc=255,
                                   stderr_contains='Cannot specify both')


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_replace_network_acl_entry
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestReplaceNetworkACLEntry(BaseAWSCommandParamsTest):

    prefix = 'ec2 replace-network-acl-entry'

    def test_tcp(self):
        cmdline = self.prefix
        cmdline += ' --network-acl-id acl-12345678'
        cmdline += ' --rule-number 100'
        cmdline += ' --protocol tcp'
        cmdline += ' --rule-action allow'
        cmdline += ' --ingress'
        cmdline += ' --port-range From=22,To=22'
        cmdline += ' --cidr-block 0.0.0.0/0'
        result = {'NetworkAclId': 'acl-12345678',
                  'RuleNumber': '100',
                  'Protocol': '6',
                  'RuleAction': 'allow',
                  'Egress': 'false',
                  'CidrBlock': '0.0.0.0/0',
                  'PortRange.From': '22',
                  'PortRange.To': '22'
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_udp(self):
        cmdline = self.prefix
        cmdline += ' --network-acl-id acl-12345678'
        cmdline += ' --rule-number 100'
        cmdline += ' --protocol udp'
        cmdline += ' --rule-action allow'
        cmdline += ' --ingress'
        cmdline += ' --port-range From=22,To=22'
        cmdline += ' --cidr-block 0.0.0.0/0'
        result = {'NetworkAclId': 'acl-12345678',
                  'RuleNumber': '100',
                  'Protocol': '17',
                  'RuleAction': 'allow',
                  'Egress': 'false',
                  'CidrBlock': '0.0.0.0/0',
                  'PortRange.From': '22',
                  'PortRange.To': '22'
                  }
        self.assert_params_for_cmd(cmdline, result)

    def test_icmp(self):
        cmdline = self.prefix
        cmdline += ' --network-acl-id acl-12345678'
        cmdline += ' --rule-number 100'
        cmdline += ' --protocol icmp'
        cmdline += ' --rule-action allow'
        cmdline += ' --ingress'
        cmdline += ' --port-range From=22,To=22'
        cmdline += ' --cidr-block 0.0.0.0/0'
        result = {'NetworkAclId': 'acl-12345678',
                  'RuleNumber': '100',
                  'Protocol': '1',
                  'RuleAction': 'allow',
                  'Egress': 'false',
                  'CidrBlock': '0.0.0.0/0',
                  'PortRange.From': '22',
                  'PortRange.To': '22'
                  }
        self.assert_params_for_cmd(cmdline, result)
        
    def test_all(self):
        cmdline = self.prefix
        cmdline += ' --network-acl-id acl-12345678'
        cmdline += ' --rule-number 100'
        cmdline += ' --protocol all'
        cmdline += ' --rule-action allow'
        cmdline += ' --ingress'
        cmdline += ' --port-range From=22,To=22'
        cmdline += ' --cidr-block 0.0.0.0/0'
        result = {'NetworkAclId': 'acl-12345678',
                  'RuleNumber': '100',
                  'Protocol': '-1',
                  'RuleAction': 'allow',
                  'Egress': 'false',
                  'CidrBlock': '0.0.0.0/0',
                  'PortRange.From': '22',
                  'PortRange.To': '22'
                  }
        self.assert_params_for_cmd(cmdline, result)
        
    def test_number(self):
        cmdline = self.prefix
        cmdline += ' --network-acl-id acl-12345678'
        cmdline += ' --rule-number 100'
        cmdline += ' --protocol 99'
        cmdline += ' --rule-action allow'
        cmdline += ' --ingress'
        cmdline += ' --port-range From=22,To=22'
        cmdline += ' --cidr-block 0.0.0.0/0'
        result = {'NetworkAclId': 'acl-12345678',
                  'RuleNumber': '100',
                  'Protocol': '99',
                  'RuleAction': 'allow',
                  'Egress': 'false',
                  'CidrBlock': '0.0.0.0/0',
                  'PortRange.From': '22',
                  'PortRange.To': '22'
                  }
        self.assert_params_for_cmd(cmdline, result)
        

########NEW FILE########
__FILENAME__ = test_run_instances
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.compat import compat_open

from awscli.testutils import temporary_file
from awscli.testutils import BaseAWSCommandParamsTest


class TestDescribeInstances(BaseAWSCommandParamsTest):

    prefix = 'ec2 run-instances'

    def test_no_count(self):
        args = ' --image-id ami-foobar'
        args_list = (self.prefix + args).split()
        result = {
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_count_scalar(self):
        args = ' --image-id ami-foobar --count 2'
        args_list = (self.prefix + args).split()
        result = {
            'ImageId': 'ami-foobar',
            'MaxCount': '2',
            'MinCount': '2'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_user_data(self):
        data = u'\u0039'
        with temporary_file('r+') as tmp:
            with compat_open(tmp.name, 'w') as f:
                f.write(data)
                f.flush()
                args = (
                    self.prefix +
                    ' --image-id foo --user-data file://%s' % f.name)
                result = {'ImageId': 'foo',
                          'MaxCount': '1',
                          'MinCount': '1',
                          # base64 encoded content of utf-8 encoding of data.
                          'UserData': 'OQ=='}
            self.assert_params_for_cmd(args, result)

    def test_count_range(self):
        args = ' --image-id ami-foobar --count 5:10'
        args_list = (self.prefix + args).split()
        result = {
            'ImageId': 'ami-foobar',
            'MaxCount': '10',
            'MinCount': '5'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_block_device_mapping(self):
        args = ' --image-id ami-foobar --count 1'
        args_list = (self.prefix + args).split()
        # We're switching to list form because we need to test
        # when there's leading spaces.  This is the CLI equivalent
        # of --block-dev-mapping ' [{"device_name" ...'
        # (note the space between ``'`` and ``[``)
        args_list.append('--block-device-mapping')
        args_list.append(
            ' [{"DeviceName":"/dev/sda1","Ebs":{"VolumeSize":20}}]')
        result = {
            'BlockDeviceMapping.1.DeviceName': '/dev/sda1',
            'BlockDeviceMapping.1.Ebs.VolumeSize': '20',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_secondary_ip_address(self):
        args = ' --image-id ami-foobar --count 1 '
        args += '--secondary-private-ip-addresses 10.0.2.106'
        args_list = (self.prefix + args).split()
        result = {
            'NetworkInterface.1.DeviceIndex': '0',
            'NetworkInterface.1.PrivateIpAddresses.1.Primary': 'false',
            'NetworkInterface.1.PrivateIpAddresses.1.PrivateIpAddress': '10.0.2.106',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_secondary_ip_addresses(self):
        args = ' --image-id ami-foobar --count 1 '
        args += '--secondary-private-ip-addresses 10.0.2.106 10.0.2.107'
        args_list = (self.prefix + args).split()
        result = {
            'NetworkInterface.1.DeviceIndex': '0',
            'NetworkInterface.1.PrivateIpAddresses.1.Primary': 'false',
            'NetworkInterface.1.PrivateIpAddresses.1.PrivateIpAddress': '10.0.2.106',
            'NetworkInterface.1.PrivateIpAddresses.2.Primary': 'false',
            'NetworkInterface.1.PrivateIpAddresses.2.PrivateIpAddress': '10.0.2.107',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)
        
    def test_secondary_ip_address_count(self):
        args = ' --image-id ami-foobar --count 1 '
        args += '--secondary-private-ip-address-count 4'
        args_list = (self.prefix + args).split()
        result = {
            'NetworkInterface.1.DeviceIndex': '0',
            'NetworkInterface.1.SecondaryPrivateIpAddressCount': '4',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_associate_public_ip_address(self):
        args = ' --image-id ami-foobar --count 1 --subnet-id subnet-12345678 '
        args += '--associate-public-ip-address'
        args_list = (self.prefix + args).split()
        result = {
            'NetworkInterface.1.DeviceIndex': '0',
            'NetworkInterface.1.AssociatePublicIpAddress': 'true',
            'NetworkInterface.1.SubnetId': 'subnet-12345678',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_associate_public_ip_address_switch_order(self):
        args = ' --image-id ami-foobar --count 1 '
        args += '--associate-public-ip-address --subnet-id subnet-12345678'
        args_list = (self.prefix + args).split()
        result = {
            'NetworkInterface.1.DeviceIndex': '0',
            'NetworkInterface.1.AssociatePublicIpAddress': 'true',
            'NetworkInterface.1.SubnetId': 'subnet-12345678',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_no_associate_public_ip_address(self):
        args = ' --image-id ami-foobar --count 1  --subnet-id subnet-12345678 '
        args += '--no-associate-public-ip-address'
        args_list = (self.prefix + args).split()
        result = {
            'NetworkInterface.1.DeviceIndex': '0',
            'NetworkInterface.1.AssociatePublicIpAddress': 'false',
            'NetworkInterface.1.SubnetId': 'subnet-12345678',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)
        
    def test_subnet_alone(self):
        args = ' --image-id ami-foobar --count 1 --subnet-id subnet-12345678'
        args_list = (self.prefix + args).split()
        result = {
            'SubnetId': 'subnet-12345678',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_associate_public_ip_address_and_group_id(self):
        args = ' --image-id ami-foobar --count 1 '
        args += '--security-group-id sg-12345678 '
        args += '--associate-public-ip-address --subnet-id subnet-12345678'
        args_list = (self.prefix + args).split()
        result = {
            'NetworkInterface.1.DeviceIndex': '0',
            'NetworkInterface.1.AssociatePublicIpAddress': 'true',
            'NetworkInterface.1.SubnetId': 'subnet-12345678',
            'NetworkInterface.1.SecurityGroupId.1': 'sg-12345678',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_group_id_alone(self):
        args = ' --image-id ami-foobar --count 1 '
        args += '--security-group-id sg-12345678'
        args_list = (self.prefix + args).split()
        result = {
            'SecurityGroupId.1': 'sg-12345678',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_associate_public_ip_address_and_private_ip_address(self):
        args = ' --image-id ami-foobar --count 1 '
        args += '--private-ip-address 10.0.0.200 '
        args += '--associate-public-ip-address --subnet-id subnet-12345678'
        args_list = (self.prefix + args).split()
        result = {
            'NetworkInterface.1.DeviceIndex': '0',
            'NetworkInterface.1.AssociatePublicIpAddress': 'true',
            'NetworkInterface.1.SubnetId': 'subnet-12345678',
            'NetworkInterface.1.PrivateIpAddresses.1.PrivateIpAddress': '10.0.0.200',
            'NetworkInterface.1.PrivateIpAddresses.1.Primary': 'true',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)

    def test_private_ip_address_alone(self):
        args = ' --image-id ami-foobar --count 1 '
        args += '--private-ip-address 10.0.0.200'
        args_list = (self.prefix + args).split()
        result = {
            'PrivateIpAddress': '10.0.0.200',
            'ImageId': 'ami-foobar',
            'MaxCount': '1',
            'MinCount': '1'
        }
        self.assert_params_for_cmd(args_list, result)


########NEW FILE########
__FILENAME__ = test_security_group_operations
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest

from six.moves import cStringIO

class TestAuthorizeSecurityGroupIngress(BaseAWSCommandParamsTest):

    prefix = 'ec2 authorize-security-group-ingress'

    def test_simple_cidr(self):
        args = ' --group-name foobar --protocol tcp --port 22-25 --cidr 0.0.0.0/0'
        args_list = (self.prefix + args).split()
        result =  {'GroupName': 'foobar',
                   'IpPermissions.1.FromPort': '22',
                   'IpPermissions.1.ToPort': '25',
                   'IpPermissions.1.IpProtocol': 'tcp',
                   'IpPermissions.1.IpRanges.1.CidrIp': '0.0.0.0/0'}
        self.assert_params_for_cmd(args_list, result)

    def test_all_port(self):
        args = ' --group-name foobar --protocol tcp --port all --cidr 0.0.0.0/0'
        args_list = (self.prefix + args).split()
        result =  {'GroupName': 'foobar',
                   'IpPermissions.1.FromPort': '-1',
                   'IpPermissions.1.ToPort': '-1',
                   'IpPermissions.1.IpProtocol': 'tcp',
                   'IpPermissions.1.IpRanges.1.CidrIp': '0.0.0.0/0'}
        self.assert_params_for_cmd(args_list, result)

    def test_all_protocol(self):
        args = ' --group-name foobar --protocol all --port all --cidr 0.0.0.0/0'
        args_list = (self.prefix + args).split()
        result =  {'GroupName': 'foobar',
                   'IpPermissions.1.FromPort': '-1',
                   'IpPermissions.1.ToPort': '-1',
                   'IpPermissions.1.IpProtocol': '-1',
                   'IpPermissions.1.IpRanges.1.CidrIp': '0.0.0.0/0'}
        self.assert_params_for_cmd(args_list, result)

    def test_numeric_protocol(self):
        args = ' --group-name foobar --protocol 200 --cidr 0.0.0.0/0'
        args_list = (self.prefix + args).split()
        result =  {'GroupName': 'foobar',
                   'IpPermissions.1.IpProtocol': '200',
                   'IpPermissions.1.IpRanges.1.CidrIp': '0.0.0.0/0'}
        self.assert_params_for_cmd(args_list, result)

    def test_negative_one_protocol(self):
        args = ' --group-name foobar --protocol -1 --cidr 0.0.0.0/0'
        args_list = (self.prefix + args).split()
        result =  {'GroupName': 'foobar',
                   'IpPermissions.1.IpProtocol': '-1',
                   'IpPermissions.1.IpRanges.1.CidrIp': '0.0.0.0/0'}
        self.assert_params_for_cmd(args_list, result)

    def test_classic_group(self):
        args = ' --group-name foobar --protocol udp --source-group fiebaz --group-owner 11111111'
        args_list = (self.prefix + args).split()
        result = {'GroupName': 'foobar',
                  'IpPermissions.1.Groups.1.GroupName': 'fiebaz',
                  'IpPermissions.1.IpProtocol': 'udp',
                  'IpPermissions.1.Groups.1.UserId': '11111111'}
        self.assert_params_for_cmd(args_list, result)

    def test_vpc_group(self):
        args = ' --group-name foobar --protocol icmp --source-group sg-12345678'
        args_list = (self.prefix + args).split()
        result = {'GroupName': 'foobar',
                  'IpPermissions.1.Groups.1.GroupId': 'sg-12345678',
                  'IpPermissions.1.IpProtocol': 'icmp'}
        self.assert_params_for_cmd(args_list, result)

    def test_ip_permissions(self):
        json = """[{"FromPort":8000,"ToPort":9000,"IpProtocol":"tcp","IpRanges":[{"CidrIp":"192.168.100.0/24"}]}]"""
        args = ' --group-name foobar --ip-permissions %s' % json
        args_list = (self.prefix + args).split()
        result = {'GroupName': 'foobar',
                  'IpPermissions.1.FromPort': '8000',
                  'IpPermissions.1.ToPort': '9000',
                  'IpPermissions.1.IpProtocol': 'tcp',
                  'IpPermissions.1.IpRanges.1.CidrIp': '192.168.100.0/24'}
        self.assert_params_for_cmd(args_list, result)

    def test_ip_permissions_with_group_id(self):
        json = """[{"FromPort":8000,"ToPort":9000,"IpProtocol":"tcp","IpRanges":[{"CidrIp":"192.168.100.0/24"}]}]"""
        args = ' --group-id sg-12345678 --ip-permissions %s' % json
        args_list = (self.prefix + args).split()
        result = {'GroupId': 'sg-12345678',
                  'IpPermissions.1.FromPort': '8000',
                  'IpPermissions.1.ToPort': '9000',
                  'IpPermissions.1.IpProtocol': 'tcp',
                  'IpPermissions.1.IpRanges.1.CidrIp': '192.168.100.0/24'}
        self.assert_params_for_cmd(args_list, result)

    def test_both(self):
        captured = cStringIO()
        json = """[{"FromPort":8000,"ToPort":9000,"IpProtocol":"tcp","IpRanges":[{"CidrIp":"192.168.100.0/24"}]}]"""
        args = ' --group-name foobar --port 100 --ip-permissions %s' % json
        args_list = (self.prefix + args).split()
        self.assert_params_for_cmd(args_list, {}, expected_rc=255)


class TestAuthorizeSecurityGroupEgress(BaseAWSCommandParamsTest):

    prefix = 'ec2 authorize-security-group-egress'


class TestRevokeSecurityGroupIngress(BaseAWSCommandParamsTest):

    prefix = 'ec2 revoke-security-group-ingress'


class TestRevokeSecurityGroupEgress(BaseAWSCommandParamsTest):

    prefix = 'ec2 revoke-security-group-egress'


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_create_cache_cluster
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestCreateCacheCluster(BaseAWSCommandParamsTest):
    maxDiff = None

    prefix = 'elasticache create-cache-cluster '

    def test_create_cache_cluster(self):
        args = ('--cache-cluster-id cachecluster-us-east-1c '
                '--num-cache-nodes 1 '
                '--cache-node-type cache.m1.small '
                '--engine memcached '
                '--engine-version 1.4.5 '
                '--cache-security-group-names group1 group2 '
                '--preferred-availability-zone us-east-1c '
                '--auto-minor-version-upgrade '
                '--preferred-maintenance-window fri:08:00-fri:09:00')
        cmdline = self.prefix + args
        result = {'AutoMinorVersionUpgrade': 'true',
                  'CacheClusterId': 'cachecluster-us-east-1c',
                  'CacheNodeType': 'cache.m1.small',
                  'CacheSecurityGroupNames.member.1': 'group1',
                  'CacheSecurityGroupNames.member.2': 'group2',
                  'Engine': 'memcached',
                  'EngineVersion': '1.4.5',
                  'NumCacheNodes': '1',
                  'PreferredAvailabilityZone': 'us-east-1c',
                  'PreferredMaintenanceWindow': 'fri:08:00-fri:09:00'}
        self.assert_params_for_cmd(cmdline, result)

    def test_create_cache_cluster_no_auto_minor_upgrade(self):
        args = ('--cache-cluster-id cachecluster-us-east-1c '
                '--num-cache-nodes 1 '
                '--cache-node-type cache.m1.small '
                '--engine memcached '
                '--engine-version 1.4.5 '
                '--cache-security-group-names group1 group2 '
                '--preferred-availability-zone us-east-1c '
                '--no-auto-minor-version-upgrade '
                '--preferred-maintenance-window fri:08:00-fri:09:00')
        cmdline = self.prefix + args
        result = {'AutoMinorVersionUpgrade': 'false',
                  'CacheClusterId': 'cachecluster-us-east-1c',
                  'CacheNodeType': 'cache.m1.small',
                  'CacheSecurityGroupNames.member.1': 'group1',
                  'CacheSecurityGroupNames.member.2': 'group2',
                  'Engine': 'memcached',
                  'EngineVersion': '1.4.5',
                  'NumCacheNodes': '1',
                  'PreferredAvailabilityZone': 'us-east-1c',
                  'PreferredMaintenanceWindow': 'fri:08:00-fri:09:00'}
        self.assert_params_for_cmd(cmdline, result)

    def test_minor_upgrade_arg_not_specified(self):
        args = ('--cache-cluster-id cachecluster-us-east-1c '
                '--num-cache-nodes 1 '
                '--cache-node-type cache.m1.small '
                '--engine memcached '
                '--engine-version 1.4.5 '
                '--cache-security-group-names group1 group2 '
                '--preferred-availability-zone us-east-1c '
                '--preferred-maintenance-window fri:08:00-fri:09:00')
        cmdline = self.prefix + args
        # Note how if neither '--auto-minor-version-upgrade' nor
        # '--no-auto-minor-version-upgrade' is specified, then
        # AutoMinorVersionUpgrade is not in the result dict.
        result = {'CacheClusterId': 'cachecluster-us-east-1c',
                  'CacheNodeType': 'cache.m1.small',
                  'CacheSecurityGroupNames.member.1': 'group1',
                  'CacheSecurityGroupNames.member.2': 'group2',
                  'Engine': 'memcached',
                  'EngineVersion': '1.4.5',
                  'NumCacheNodes': '1',
                  'PreferredAvailabilityZone': 'us-east-1c',
                  'PreferredMaintenanceWindow': 'fri:08:00-fri:09:00'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_create_application
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest, unittest
import sys
import six


class TestUpdateConfigurationTemplate(BaseAWSCommandParamsTest):

    prefix = 'elasticbeanstalk create-application'

    def test_ascii(self):
        cmdline = self.prefix
        cmdline += ' --application-name FooBar'
        result = {'ApplicationName': 'FooBar',}
        self.assert_params_for_cmd(cmdline, result)

    @unittest.skipIf(
        six.PY3, 'Unicode cmd line test only is relevant to python2.')
    def test_py2_bytestring_unicode(self):
        # In Python2, sys.argv is a list of bytestrings that are encoded
        # in whatever encoding the terminal uses.  We have an extra step
        # where we need to decode the bytestring into unicode.  In
        # python3, sys.argv is a list of unicode objects so this test
        # doesn't make sense for python3.
        cmdline = self.prefix
        app_name = u'\u2713'
        cmdline += u' --application-name %s' % app_name
        encoding = getattr(sys.stdin, 'encoding')
        if encoding is None:
            encoding = 'utf-8'
        cmdline = cmdline.encode(encoding)
        result = {'ApplicationName': u'\u2713',}
        self.assert_params_for_cmd(cmdline, result)

########NEW FILE########
__FILENAME__ = test_update_configuration_template
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import os


class TestUpdateConfigurationTemplate(BaseAWSCommandParamsTest):

    prefix = 'elasticbeanstalk update-configuration-template'

    def test_file(self):
        data_path = os.path.join(os.path.dirname(__file__),
                                 'new_keypair_config.json')
        cmdline = self.prefix
        cmdline += ' --application-name FooBar'
        cmdline += ' --template-name x86_64_m1_medium_config'
        cmdline += ' --option-settings file://%s' % data_path
        cmdline += ' --description This_is_a_test'
        result = {'ApplicationName': 'FooBar',
                  'TemplateName': 'x86_64_m1_medium_config',
                  'Description': 'This_is_a_test',
                  'OptionSettings.member.1.Namespace': 'aws:autoscaling:launchconfiguration',
                  'OptionSettings.member.1.OptionName': 'EC2KeyName',
                  'OptionSettings.member.1.Value': 'webapps',
                  'OptionSettings.member.2.Namespace': 'aws:elasticbeanstalk:container:tomcat:jvmoptions',
                  'OptionSettings.member.2.OptionName': 'Xms',
                  'OptionSettings.member.2.Value': '1256m'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_configure_health_check
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
from awscli.testutils import BaseAWSCommandParamsTest


class TestConfigureHealthCheck(BaseAWSCommandParamsTest):

    prefix = 'elb configure-health-check'

    def test_shorthand_basic(self):
        cmdline = self.prefix
        cmdline += ' --load-balancer-name my-lb'
        cmdline += (' --health-check Target=HTTP:80/weather/us/wa/seattle,'
                    'Interval=300,Timeout=60,UnhealthyThreshold=5,'
                    'HealthyThreshold=9')
        result = {'HealthCheck.HealthyThreshold': '9',
                  'HealthCheck.Interval': '300',
                  'HealthCheck.Target': 'HTTP:80/weather/us/wa/seattle',
                  'HealthCheck.Timeout': '60',
                  'HealthCheck.UnhealthyThreshold': '5',
                  'LoadBalancerName': 'my-lb'}
        self.assert_params_for_cmd(cmdline, result)

    def test_json(self):
        cmdline = self.prefix
        cmdline += ' --load-balancer-name my-lb '
        cmdline += ('--health-check {"Target":"HTTP:80/weather/us/wa/seattle'
                    '?a=b","Interval":300,"Timeout":60,'
                    '"UnhealthyThreshold":5,"HealthyThreshold":9}')
        result = {'HealthCheck.HealthyThreshold': '9',
                  'HealthCheck.Interval': '300',
                  'HealthCheck.Target': 'HTTP:80/weather/us/wa/seattle?a=b',
                  'HealthCheck.Timeout': '60',
                  'HealthCheck.UnhealthyThreshold': '5',
                  'LoadBalancerName': 'my-lb'}
        self.assert_params_for_cmd(cmdline, result)

    def test_shorthand_with_multiple_equals_for_value(self):
        cmdline = self.prefix
        cmdline += ' --load-balancer-name my-lb'
        cmdline += (' --health-check Target="HTTP:80/weather/us/wa/seattle?a=b"'
                    ',Interval=300,Timeout=60,UnhealthyThreshold=5,'
                    'HealthyThreshold=9')
        result = {'HealthCheck.HealthyThreshold': '9',
                  'HealthCheck.Interval': '300',
                  'HealthCheck.Target': 'HTTP:80/weather/us/wa/seattle?a=b',
                  'HealthCheck.Timeout': '60',
                  'HealthCheck.UnhealthyThreshold': '5',
                  'LoadBalancerName': 'my-lb'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_register_instances_with_load_balancer
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import os


TWO_INSTANCE_EXPECTED = {
    'LoadBalancerName': 'my-lb',
    'Instances.member.1.InstanceId': 'i-12345678',
    'Instances.member.2.InstanceId': 'i-87654321'
}


class TestRegisterInstancesWithLoadBalancer(BaseAWSCommandParamsTest):

    prefix = 'elb register-instances-with-load-balancer'

    def test_one_instance(self):
        cmdline = self.prefix
        cmdline += ' --load-balancer-name my-lb'
        cmdline += ' --instances {"InstanceId":"i-12345678"}'
        result = {'LoadBalancerName': 'my-lb',
                  'Instances.member.1.InstanceId': 'i-12345678'}
        self.assert_params_for_cmd(cmdline, result)

    def test_shorthand(self):
        cmdline = self.prefix
        cmdline += ' --load-balancer-name my-lb'
        cmdline += ' --instances i-12345678'
        result = {'LoadBalancerName': 'my-lb',
                  'Instances.member.1.InstanceId': 'i-12345678'}
        self.assert_params_for_cmd(cmdline, result)

    def test_two_instance(self):
        cmdline = self.prefix
        cmdline += ' --load-balancer-name my-lb'
        cmdline += ' --instances {"InstanceId":"i-12345678"}'
        cmdline += ' {"InstanceId":"i-87654321"}'
        self.assert_params_for_cmd(cmdline, TWO_INSTANCE_EXPECTED)

    def test_two_instance_as_json(self):
        cmdline = self.prefix
        cmdline += ' --load-balancer-name my-lb'
        cmdline += ' --instances [{"InstanceId":"i-12345678"},'
        cmdline += '{"InstanceId":"i-87654321"}]'
        self.assert_params_for_cmd(cmdline, TWO_INSTANCE_EXPECTED)

    def test_two_instance_from_file(self):
        data_path = os.path.join(os.path.dirname(__file__),
                                 'test.json')
        cmdline = self.prefix
        cmdline += ' --load-balancer-name my-lb'
        cmdline += ' --instances file://%s' % data_path
        self.assert_params_for_cmd(cmdline, TWO_INSTANCE_EXPECTED)

    def test_json_file_with_spaces(self):
        data_path = os.path.join(os.path.dirname(__file__),
                                 'test_with_spaces.json')
        cmdline = self.prefix
        cmdline += ' --load-balancer-name my-lb'
        cmdline += ' --instances file://%s' % data_path
        self.assert_params_for_cmd(cmdline, TWO_INSTANCE_EXPECTED)

    def test_two_instance_shorthand(self):
        cmdline = self.prefix
        cmdline += ' --load-balancer-name my-lb'
        cmdline += ' --instances i-12345678 i-87654321'
        self.assert_params_for_cmd(cmdline, TWO_INSTANCE_EXPECTED)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_create_virtual_mfa_device
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import os
import re

from six.moves import cStringIO
import mock


class TestCreateVirtualMFADevice(BaseAWSCommandParamsTest):

    prefix = 'iam create-virtual-mfa-device'

    def setUp(self):
        super(TestCreateVirtualMFADevice, self).setUp()
        self.parsed_response = {
            "VirtualMFADevice": {
                "Base32StringSeed": "VFpYTVc2V1lIUFlFRFczSVhLUlpRUTJRVFdUSFRNRDNTQ0c3TkZDUVdQWDVETlNWM0IyUENaQVpWTEpQTlBOTA==",
                "SerialNumber": "arn:aws:iam::419278470775:mfa/fiebaz",
                "QRCodePNG": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD6CAIAAAAHjs1qAAAFiElEQVR42u3bQW7jMBAEwPz/07vHvS0QeLpnZFVf7cgyWRTgJvPzR+Q1+TEEgrsI7iK4i+AugrsI7iK4i+AugrsI7iK4C+4iuIvgLoK7CO4iuIvgLoK7CO4iBe4/rfz/c391k7lLffK5v/r6tZu8Ofu444477rjjjjvuuOOOO+6xwcoZ/WTl5D53cNXlZqG2VPpXxh133HHHHXfccccdd9xxD09/rU7ZylZdVnvo5BY/7rjjjjvuuOOOO+64447713H/RFIOZW0944477rjjjjvuuOOOO+64417ZVM8ZPbLHXiOLO+6444477rjjjjvuuOP+fO5bC2lwMeQKoiN/ew0l7rjjjjvuuOOOO+644457uCLYGmivHuxeBmcfd6/ijrtXccfdq7jj7lXcH5han3Bkeedana9SgTvuuOOOO+6444477ri/hXttc7umMLc2tm5ycI4GnzJfWETijjvuuOOOO+6444477s9oZnKDlZvvGp2tnf/BNx8/NYA77rjjjjvuuOOOO+64v5V7rZkZHKzalXOwcg3J1qorFGK444477rjjjjvuuOOO+1u5fwLrSG9T6zEGxyr3FWqPwkeemcEdd9xxxx133HHHHXfcj3Kv4chNUo17rU7ZmtDjaxJ33HHHHXfccccdd9xxfw33rWYm12McOY/wBQupZgN33HHHHXfccccdd9xxx/2DwTpy0Pzm2sh9oyOgc+sZd9xxxx133HHHHXfcccc9liOVSE3SkYIot2KPFES444477rjjjjvuuOOO+/u4DzYzgxoGpyH35q3R2Nrb758awB133HHHHXfccccdd9zfyj23NmpX3jomkBuc3N7+4DIr+MYdd9xxxx133HHHHXfc38q91gkcUZi7jcGd/9xIHnkG4Y477rjjjjvuuOOOO+6430gOR20hbVUTW4cIrg0O7rjjjjvuuOOOO+644/4a7keKi8G+6At273Nv3vpc3HHHHXfccccdd9xxxx332LjXyocanUfgqJ0pqNU4uOOOO+6444477rjjjjvuS13E1l53jc5Ndjf144477rjjjjvuuOOOO+64x77Skf352mZ+7VhEbo62DiBE6jvccccdd9xxxx133HHHHffs9A8WNbnprz0atv7rYPDrP/IQAe6444477rjjjjvuuON+lPvguK//ci+TrS3RwZ7q5nMEd9xxxx133HHHHXfccce9VSDkippcY5A7nrBVaxyp2nDHHXfccccdd9xxxx133FuTVFs5tUttlVq5pZJ7cCy0arjjjjvuuOOOO+6444477tmd8K0SIDdnNXZHirj+YwV33HHHHXfccccdd9xxfw33mu+bHUiObK4By7E7jgF33HHHHXfccccdd9xxfw33Whcx+Kt/a1Zyn5tbZltkcccdd9xxxx133HHHHXfcb4DOnRp4YgeSQ/l9jRDuuOOOO+6444477rjjjntM4WCBMGhlcHByt1GrcbZOSeCOO+6444477rjjjjvuuD8htUMENcFbN1mrcXIfhDvuuOOOO+6444477rjjvvSrP3cbg3NWG7pcMTV4OuPtRSTuuOOOO+6444477rjjfoV7rXvZWjlbVr6+5CkUNbjjjjvuuOOOO+6444477qsacn+7VeMcaYRyVdsjDxHgjjvuuOOOO+6444477rinNNR6myfexlbXpJnBHXfccccdd9xxxx133J/PvbAFPT5nN4upLQy444477rjjjjvuuOOOO+4t7rWx27pybY/9iLMjvQ3uuOOOO+6444477rjjjvvSfB/Zr86t58H53qpxBpdoYZxxxx133HHHHXfccccd99dwF3lKcBfcRXAXwV0EdxHcRXAXwV0EdxHcRXAXwV1wF8FdBHcR3EVwF8FdBHcR3EVwF8Fd5F/+AgASajf850wfAAAAAElFTkSuQmCC",
            }
        }

    def getpath(self, filename):
        return os.path.join(os.path.abspath(os.path.dirname(__file__)),
                            filename)

    def remove_file_if_exists(self, filename):
        if os.path.isfile(filename):
            os.remove(filename)

    def test_base32(self):
        outfile = self.getpath('fiebaz.b32')
        self.addCleanup(self.remove_file_if_exists, outfile)
        cmdline = self.prefix
        cmdline += ' --virtual-mfa-device-name fiebaz'
        cmdline += ' --outfile %s --bootstrap-method Base32StringSeed' % outfile
        result = {"VirtualMFADeviceName": 'fiebaz'}
        self.assert_params_for_cmd(cmdline, result)
        self.assertTrue(os.path.exists(outfile))

    def test_qrcode(self):
        outfile = self.getpath('fiebaz.png')
        self.addCleanup(self.remove_file_if_exists, outfile)
        cmdline = self.prefix
        cmdline += ' --virtual-mfa-device-name fiebaz'
        cmdline += ' --outfile %s --bootstrap-method QRCodePNG' % outfile
        result = {"VirtualMFADeviceName": 'fiebaz'}
        self.assert_params_for_cmd(cmdline, result)
        self.assertTrue(os.path.exists(outfile))

    def test_bad_filename(self):
        captured = cStringIO()
        outfile = '/some/bad/filename.png'
        self.addCleanup(self.remove_file_if_exists, outfile)
        cmdline = self.prefix
        cmdline += ' --virtual-mfa-device-name fiebaz'
        cmdline += ' --outfile %s --bootstrap-method QRCodePNG' % outfile
        result = {}
        self.assert_params_for_cmd(cmdline, result, expected_rc=255)

########NEW FILE########
__FILENAME__ = test_create_instance
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import awscli.clidriver


class TestCreateInstance(BaseAWSCommandParamsTest):

    prefix = 'opsworks create-instance'

    def test_simple(self):
        cmdline = self.prefix
        cmdline += ' --stack-id f623987f-6303-4bba-a38e-63073e85c726'
        cmdline += ' --layer-ids cb27894d-35f3-4435-b422-6641a785fa4a'
        cmdline += ' --instance-type c1.medium'
        cmdline += ' --hostname aws-client-instance'
        result = {'StackId': 'f623987f-6303-4bba-a38e-63073e85c726',
                  'Hostname': 'aws-client-instance',
                  'LayerIds': ['cb27894d-35f3-4435-b422-6641a785fa4a'],
                  'InstanceType': 'c1.medium'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_create_layer
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import unittest
from awscli.testutils import BaseAWSCommandParamsTest
import os
import awscli.clidriver


class TestCreateLayer(BaseAWSCommandParamsTest):

    prefix = 'opsworks create-layer'

    def test_attributes_file(self):
        data_path = os.path.join(os.path.dirname(__file__),
                                 'create_layer_attributes.json')
        cmdline = self.prefix
        cmdline += ' --stack-id 35959772-cd1e-4082-8346-79096d4179f2'
        cmdline += ' --type rails-app'
        cmdline += ' --name Rails_App_Server'
        cmdline += ' --enable-auto-healing'
        cmdline += ' --attributes file://%s' % data_path
        cmdline += ' --shortname foo'
        result = {'StackId': '35959772-cd1e-4082-8346-79096d4179f2',
                  'Type': 'rails-app',
                  'Name': 'Rails_App_Server',
                  'EnableAutoHealing': True,
                  'Shortname': 'foo',
                  'Attributes': {"MysqlRootPasswordUbiquitous": None,
                                 "RubygemsVersion": "1.8.24",
                                 "RailsStack": "apache_passenger",
                                 "HaproxyHealthCheckMethod": None,
                                 "RubyVersion": "1.9.3",
                                 "BundlerVersion": "1.2.3",
                                 "HaproxyStatsPassword": None,
                                 "PassengerVersion": "3.0.17",
                                 "MemcachedMemory": None,
                                 "EnableHaproxyStats": None,
                                 "ManageBundler": "true",
                                 "NodejsVersion": None,
                                 "HaproxyHealthCheckUrl": None,
                                 "MysqlRootPassword": None,
                                 "GangliaPassword": None,
                                 "GangliaUser": None,
                                 "HaproxyStatsUrl": None,
                                 "GangliaUrl": None,
                                 "HaproxyStatsUser": None}
                  }
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_create_stack
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import os
import awscli.clidriver


class TestCreateStack(BaseAWSCommandParamsTest):

    prefix = 'opsworks create-stack'

    def test_attributes_file(self):
        cmdline = self.prefix
        cmdline += ' --service-role-arn arn-blahblahblah'
        cmdline += ' --name FooStack'
        cmdline += ' --stack-region us-west-2'
        cmdline += ' --default-instance-profile-arn arn-foofoofoo'
        result = {'ServiceRoleArn': 'arn-blahblahblah',
                  'Name': 'FooStack',
                  'Region': 'us-west-2',
                  'DefaultInstanceProfileArn': 'arn-foofoofoo'
                  }
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_describe_layers
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import awscli.clidriver


class TestDescribeLayers(BaseAWSCommandParamsTest):

    prefix = 'opsworks describe-layers'

    def test_both_params(self):
        cmdline = self.prefix
        cmdline += ' --stack-id 35959772-cd1e-4082-8346-79096d4179f2'
        result = {'StackId': '35959772-cd1e-4082-8346-79096d4179f2'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_json_output
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import json


class TestGetPasswordData(BaseAWSCommandParamsTest):

    prefix = 'iam add-user-to-group '

    def test_empty_response_prints_nothing(self):
        # This is the default response, but we want to be explicit
        # that we're returning an empty dict.
        self.parsed_response = {}
        args = ' --group-name foo --user-name bar'
        cmdline = self.prefix + args
        result = {'GroupName': 'foo', 'UserName': 'bar'}
        stdout = self.assert_params_for_cmd(cmdline, result, expected_rc=0)[0]
        # We should have printed nothing because the parsed response
        # is an empty dict: {}.
        self.assertEqual(stdout, '')


class TestListUsers(BaseAWSCommandParamsTest):

    def setUp(self):
        super(TestListUsers, self).setUp()
        self.parsed_response = {
            'Users': [
                {
                    "UserName": "testuser-50",
                    "Path": "/",
                    "CreateDate": "2013-02-12T19:08:52Z",
                    "UserId": "EXAMPLEUSERID",
                    "Arn": "arn:aws:iam::12345:user/testuser1"
                },
                {
                    "UserName": "testuser-51",
                    "Path": "/",
                    "CreateDate": "2012-10-14T23:53:39Z",
                    "UserId": "EXAMPLEUSERID",
                    "Arn": "arn:aws:iam::123456:user/testuser2"
                },
            ]
        }

    def test_json_response(self):
        output = self.run_cmd('iam list-users', expected_rc=0)[0]
        parsed_output = json.loads(output)
        self.assertIn('Users', parsed_output)
        self.assertEqual(len(parsed_output['Users']), 2)
        self.assertEqual(sorted(parsed_output['Users'][0].keys()),
                         ['Arn', 'CreateDate', 'Path', 'UserId', 'UserName'])

    def test_jmespath_json_response(self):
        jmespath_query = 'Users[*].UserName'
        output = self.run_cmd('iam list-users --query %s' % jmespath_query,
                              expected_rc=0)[0]
        parsed_output = json.loads(output)
        self.assertEqual(parsed_output, ['testuser-50', 'testuser-51'])

    def test_unknown_output_type_from_env_var(self):
        # argparse already handles the case with a bad --output
        # specified on the CLI, we need to verify that a bad
        # output format from the env var still gives an error.
        self.environ['AWS_DEFAULT_OUTPUT'] = 'bad-output-type'
        self.run_cmd('iam list-users', expected_rc=255)

########NEW FILE########
__FILENAME__ = test_table_formatter
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import unittest
import six

from awscli.formatter import TableFormatter
from awscli.table import MultiTable, Styler

SIMPLE_LIST = {
    "QueueUrls": [
        "https://us-west-2.queue.amazonaws.com/1/queue1",
        "https://us-west-2.queue.amazonaws.com/1/queue2",
        "https://us-west-2.queue.amazonaws.com/1/queue3",
        "https://us-west-2.queue.amazonaws.com/1/queue4"
    ]
}



SIMPLE_LIST_TABLE = """\
------------------------------------------------------
|                    OperationName                   |
+----------------------------------------------------+
||                     QueueUrls                    ||
|+--------------------------------------------------+|
||  https://us-west-2.queue.amazonaws.com/1/queue1  ||
||  https://us-west-2.queue.amazonaws.com/1/queue2  ||
||  https://us-west-2.queue.amazonaws.com/1/queue3  ||
||  https://us-west-2.queue.amazonaws.com/1/queue4  ||
|+--------------------------------------------------+|
"""

SIMPLE_DICT = {"Attributes":
  {"a": "0",
   "b": "345600",
   "c": "0",
   "d": "65536",
   "e": "1351044153",
   "f": "0"}
}


SIMPLE_DICT_TABLE = """\
----------------------------------------------------
|                   OperationName                  |
+--------------------------------------------------+
||                   Attributes                   ||
|+---+---------+----+--------+--------------+-----+|
|| a |    b    | c  |   d    |      e       |  f  ||
|+---+---------+----+--------+--------------+-----+|
||  0|  345600 |  0 |  65536 |  1351044153  |  0  ||
|+---+---------+----+--------+--------------+-----+|
"""


LIST_OF_DICTS = {
    "OrderableDBInstanceOptions": [
        {
            "AvailabilityZones": [
                {
                    "Name": "us-east-1a",
                    "ProvisionedIopsCapable": False
                },
                {
                    "Name": "us-east-1d",
                    "ProvisionedIopsCapable": True
                }
            ],
            "DBInstanceClass": "db.m1.large",
            "Engine": "mysql",
            "EngineVersion": "5.1.45",
            "LicenseModel": "general-public-license",
            "MultiAZCapable": True,
            "ReadReplicaCapable": True,
            "Vpc": False
        },
        {
            "AvailabilityZones": [
                {
                    "Name": "us-west-2a",
                    "ProvisionedIopsCapable": True
                },
                {
                    "Name": "us-west-2b",
                    "ProvisionedIopsCapable": True
                }
            ],
            "DBInstanceClass": "db.m1.xlarge",
            "Engine": "mysql",
            "EngineVersion": "5.1.57",
            "LicenseModel": "general-public-license",
            "MultiAZCapable": True,
            "ReadReplicaCapable": True,
            "Vpc": False
        },
        {
            "AvailabilityZones": [
                {
                    "Name": "us-west-2a",
                    "ProvisionedIopsCapable": True
                },
            ],
            "DBInstanceClass": "db.m1.xlarge",
            "Engine": "mysql",
            "EngineVersion": "5.1.57",
            "LicenseModel": "general-public-license",
            "MultiAZCapable": True,
            "ReadReplicaCapable": True,
            "Vpc": True
        }
    ]
}

LIST_OF_DICTS_TABLE = """\
-----------------------------------------------------------------------------------------------------------------------------
|                                                       OperationName                                                       |
+---------------------------------------------------------------------------------------------------------------------------+
||                                               OrderableDBInstanceOptions                                                ||
|+-----------------+---------+----------------+-------------------------+-----------------+----------------------+---------+|
|| DBInstanceClass | Engine  | EngineVersion  |      LicenseModel       | MultiAZCapable  | ReadReplicaCapable   |   Vpc   ||
|+-----------------+---------+----------------+-------------------------+-----------------+----------------------+---------+|
||  db.m1.large    |  mysql  |  5.1.45        |  general-public-license |  True           |  True                |  False  ||
|+-----------------+---------+----------------+-------------------------+-----------------+----------------------+---------+|
|||                                                   AvailabilityZones                                                   |||
||+----------------------------------------+------------------------------------------------------------------------------+||
|||                  Name                  |                           ProvisionedIopsCapable                             |||
||+----------------------------------------+------------------------------------------------------------------------------+||
|||  us-east-1a                            |  False                                                                       |||
|||  us-east-1d                            |  True                                                                        |||
||+----------------------------------------+------------------------------------------------------------------------------+||
||                                               OrderableDBInstanceOptions                                                ||
|+-----------------+---------+----------------+-------------------------+-----------------+----------------------+---------+|
|| DBInstanceClass | Engine  | EngineVersion  |      LicenseModel       | MultiAZCapable  | ReadReplicaCapable   |   Vpc   ||
|+-----------------+---------+----------------+-------------------------+-----------------+----------------------+---------+|
||  db.m1.xlarge   |  mysql  |  5.1.57        |  general-public-license |  True           |  True                |  False  ||
|+-----------------+---------+----------------+-------------------------+-----------------+----------------------+---------+|
|||                                                   AvailabilityZones                                                   |||
||+----------------------------------------+------------------------------------------------------------------------------+||
|||                  Name                  |                           ProvisionedIopsCapable                             |||
||+----------------------------------------+------------------------------------------------------------------------------+||
|||  us-west-2a                            |  True                                                                        |||
|||  us-west-2b                            |  True                                                                        |||
||+----------------------------------------+------------------------------------------------------------------------------+||
||                                               OrderableDBInstanceOptions                                                ||
|+-----------------+---------+----------------+--------------------------+-----------------+----------------------+--------+|
|| DBInstanceClass | Engine  | EngineVersion  |      LicenseModel        | MultiAZCapable  | ReadReplicaCapable   |  Vpc   ||
|+-----------------+---------+----------------+--------------------------+-----------------+----------------------+--------+|
||  db.m1.xlarge   |  mysql  |  5.1.57        |  general-public-license  |  True           |  True                |  True  ||
|+-----------------+---------+----------------+--------------------------+-----------------+----------------------+--------+|
|||                                                   AvailabilityZones                                                   |||
||+----------------------------------------+------------------------------------------------------------------------------+||
|||                  Name                  |                           ProvisionedIopsCapable                             |||
||+----------------------------------------+------------------------------------------------------------------------------+||
|||  us-west-2a                            |  True                                                                        |||
||+----------------------------------------+------------------------------------------------------------------------------+||
"""


# First record has "Tags" scalar, second record does not.
INNER_LIST = {
    "Snapshots": [
        {
            "Description": "TestVolume1",
            "Tags": [{"Value": "TestVolume", "Key": "Name"}],
            "VolumeId": "vol-12345",
            "State": "completed",
            "VolumeSize": 8,
            "Progress": "100%",
            "StartTime": "2012-05-23T21:46:41.000Z",
            "SnapshotId": "snap-1234567",
            "OwnerId": "12345"
        },
        {
            "Description": "Created by CreateImage(i-1234) for ami-1234 from vol-1234",
            "VolumeId": "vol-e543b98b",
            "State": "completed",
            "VolumeSize": 8,
            "Progress": "100%",
            "StartTime": "2012-05-25T00:07:20.000Z",
            "SnapshotId": "snap-23456",
            "OwnerId": "12345"
        }
    ]
}

INNER_LIST_TABLE = """\
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------
|                                                                               OperationName                                                                               |
+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
||                                                                                Snapshots                                                                                ||
|+-------------------+--------------+----------------+---------------------+--------------------------------------+-----------------+-----------------+--------------------+|
||    Description    |   OwnerId    |   Progress     |     SnapshotId      |              StartTime               |      State      |    VolumeId     |    VolumeSize      ||
|+-------------------+--------------+----------------+---------------------+--------------------------------------+-----------------+-----------------+--------------------+|
||  TestVolume1      |  12345       |  100%          |  snap-1234567       |  2012-05-23T21:46:41.000Z            |  completed      |  vol-12345      |  8                 ||
|+-------------------+--------------+----------------+---------------------+--------------------------------------+-----------------+-----------------+--------------------+|
|||                                                                                 Tags                                                                                  |||
||+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+||
|||                            Key                            |                                                   Value                                                   |||
||+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+||
|||  Name                                                     |  TestVolume                                                                                               |||
||+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+||
||                                                                                Snapshots                                                                                ||
|+------------------------------------------------------------+----------+-----------+-------------+---------------------------+------------+---------------+--------------+|
||                         Description                        | OwnerId  | Progress  | SnapshotId  |         StartTime         |   State    |   VolumeId    | VolumeSize   ||
|+------------------------------------------------------------+----------+-----------+-------------+---------------------------+------------+---------------+--------------+|
||  Created by CreateImage(i-1234) for ami-1234 from vol-1234 |  12345   |  100%     |  snap-23456 |  2012-05-25T00:07:20.000Z |  completed |  vol-e543b98b |  8           ||
|+------------------------------------------------------------+----------+-----------+-------------+---------------------------+------------+---------------+--------------+|
"""

LIST_WITH_MISSING_KEYS = {
    "Snapshots": [
        {
            "Description": "TestVolume1",
            "Tags": "foo",
            "VolumeId": "vol-12345",
            "State": "completed",
            "VolumeSize": 8,
            "Progress": "100%",
            "StartTime": "2012-05-23T21:46:41.000Z",
            "SnapshotId": "snap-1234567",
            "OwnerId": "12345"
        },
        {
            "Description": "description",
            "VolumeId": "vol-e543b98b",
            "State": "completed",
            "VolumeSize": 8,
            "Progress": "100%",
            "StartTime": "2012-05-25T00:07:20.000Z",
            "SnapshotId": "snap-23456",
            "OwnerId": "12345"
        }
    ]
}

LIST_WITH_MISSING_KEYS_TABLE = """\
-----------------------------------------------------------------------------------------------------------------------------------------
|                                                             OperationName                                                             |
+---------------------------------------------------------------------------------------------------------------------------------------+
||                                                              Snapshots                                                              ||
|+-------------+----------+-----------+---------------+---------------------------+------------+-------+----------------+--------------+|
|| Description | OwnerId  | Progress  |  SnapshotId   |         StartTime         |   State    | Tags  |   VolumeId     | VolumeSize   ||
|+-------------+----------+-----------+---------------+---------------------------+------------+-------+----------------+--------------+|
||  TestVolume1|  12345   |  100%     |  snap-1234567 |  2012-05-23T21:46:41.000Z |  completed |  foo  |  vol-12345     |  8           ||
||  description|  12345   |  100%     |  snap-23456   |  2012-05-25T00:07:20.000Z |  completed |       |  vol-e543b98b  |  8           ||
|+-------------+----------+-----------+---------------+---------------------------+------------+-------+----------------+--------------+|
"""

KEYS_NOT_FROM_FIRST_ROW = {
    "Snapshots": [
        {
            "Description": "TestVolume1",
            "Tags": "foo",
            "VolumeId": "vol-12345",
            "State": "completed",
            "VolumeSize": 8,
            "Progress": "100%",
            "StartTime": "start_time",
            # Missing EndTime.
            "SnapshotId": "snap-1234567",
            "OwnerId": "12345"
        },
        {
            "Description": "description",
            "State": "completed",
            "VolumeSize": 8,
            "Progress": "100%",
            # Missing StartTime
            "EndTime": "end_time",
            "SnapshotId": "snap-23456",
            "OwnerId": "12345"
        }
    ]
}

KEYS_NOT_FROM_FIRST_ROW_TABLE = """\
------------------------------------------------------------------------------------------------------------------------------------
|                                                           OperationName                                                          |
+----------------------------------------------------------------------------------------------------------------------------------+
||                                                            Snapshots                                                           ||
|+-------------+-----------+----------+-----------+---------------+-------------+------------+-------+-------------+--------------+|
|| Description |  EndTime  | OwnerId  | Progress  |  SnapshotId   |  StartTime  |   State    | Tags  |  VolumeId   | VolumeSize   ||
|+-------------+-----------+----------+-----------+---------------+-------------+------------+-------+-------------+--------------+|
||  TestVolume1|           |  12345   |  100%     |  snap-1234567 |  start_time |  completed |  foo  |  vol-12345  |  8           ||
||  description|  end_time |  12345   |  100%     |  snap-23456   |             |  completed |       |             |  8           ||
|+-------------+-----------+----------+-----------+---------------+-------------+------------+-------+-------------+--------------+|
"""

JMESPATH_FILTERED_RESPONSE = [
    [
        [
            "i-12345",
            "ami-12345",
            "ebs",
            "t1.micro",
            "running",
            "disabled",
            "util"
        ]
    ],
    [
        [
            "i-56789",
            "ami-56789",
            "ebs",
            "c1.medium",
            "running",
            "disabled",
            "myname"
        ]
    ],
]
JMESPATH_FILTERED_RESPONSE_TABLE = """\
-------------------------------------------------------------------------------
|                                OperationName                                |
+---------+------------+------+------------+----------+------------+----------+
|  i-12345|  ami-12345 |  ebs |  t1.micro  |  running |  disabled  |  util    |
|  i-56789|  ami-56789 |  ebs |  c1.medium |  running |  disabled  |  myname  |
+---------+------------+------+------------+----------+------------+----------+
"""


JMESPATH_FILTERED_RESPONSE_DICT = [
    [
        {
            "InstanceId": "i-12345",
            "RootDeviceType": "ebs",
            "InstanceType": "t1.micro",
            "ImageId": "ami-12345"
        }
    ],
    [
        {
            "InstanceId": "i-56789",
            "RootDeviceType": "ebs",
            "InstanceType": "c1.medium",
            "ImageId": "ami-56789"
        }
    ],
]


JMESPATH_FILTERED_RESPONSE_DICT_TABLE = """\
---------------------------------------------------------------
|                        OperationName                        |
+-----------+-------------+----------------+------------------+
|  ImageId  | InstanceId  | InstanceType   | RootDeviceType   |
+-----------+-------------+----------------+------------------+
|  ami-12345|  i-12345    |  t1.micro      |  ebs             |
|  ami-56789|  i-56789    |  c1.medium     |  ebs             |
+-----------+-------------+----------------+------------------+
"""


class Object(object):
    def __init__(self, **kwargs):
        self.__dict__.update(kwargs)
        self.query = None


class TestTableFormatter(unittest.TestCase):
    maxDiff = None

    def setUp(self):
        styler = Styler()
        self.table = MultiTable(initial_section=False,
                                column_separator='|', styler=styler,
                                auto_reformat=False)
        self.formatter = TableFormatter(Object(color='off'))
        self.formatter.table = self.table
        self.stream = six.StringIO()

    def assert_data_renders_to(self, data, table):
        self.formatter(Object(name='OperationName', can_paginate=False),
                              data, self.stream)
        rendered = self.stream.getvalue()
        if rendered != table:
            error_message = ['Expected table rendering does not match '
                             'the actual table rendering:']
            error_message.append('Expected:')
            error_message.append(table)
            error_message.append('Actual:')
            error_message.append(rendered)
            self.fail('\n'.join(error_message))

    def test_list_table(self):
        self.assert_data_renders_to(data=SIMPLE_LIST, table=SIMPLE_LIST_TABLE)

    def test_dict_table(self):
        self.assert_data_renders_to(data=SIMPLE_DICT, table=SIMPLE_DICT_TABLE)

    def test_list_of_dicts(self):
        self.assert_data_renders_to(data=LIST_OF_DICTS,
                                    table=LIST_OF_DICTS_TABLE)

    def test_inner_table(self):
        self.assert_data_renders_to(data=INNER_LIST,
                                    table=INNER_LIST_TABLE)

    def test_empty_table(self):
        self.assert_data_renders_to(data={},
                                    table='')

    def test_missing_keys(self):
        self.assert_data_renders_to(data=LIST_WITH_MISSING_KEYS,
                                    table=LIST_WITH_MISSING_KEYS_TABLE)

    def test_new_keys_after_first_row(self):
        self.assert_data_renders_to(data=KEYS_NOT_FROM_FIRST_ROW,
                                    table=KEYS_NOT_FROM_FIRST_ROW_TABLE)

    def test_jmespath_filtered_response(self):
        self.assert_data_renders_to(data=JMESPATH_FILTERED_RESPONSE,
                                    table=JMESPATH_FILTERED_RESPONSE_TABLE)

    def test_jmespath_filtered_dict_response(self):
        self.assert_data_renders_to(data=JMESPATH_FILTERED_RESPONSE_DICT,
                                    table=JMESPATH_FILTERED_RESPONSE_DICT_TABLE)

########NEW FILE########
__FILENAME__ = test_text_output
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
from awscli.testutils import unittest
import json
import os
import sys
import re
import locale

from six.moves import cStringIO
import six
import mock

from awscli.formatter import Formatter


class TestListUsers(BaseAWSCommandParamsTest):

    def setUp(self):
        super(TestListUsers, self).setUp()
        self.first_parsed_response = {
            'Users': [
                {
                    "UserName": "testuser-50",
                    "Path": "/",
                    "CreateDate": "2013-02-12T19:08:52Z",
                    "UserId": "EXAMPLEUSERID",
                    "Arn": "arn:aws:iam::12345:user/testuser1"
                },
            ],
            'Groups': []
        }
        self.second_parsed_response = {
            'Users': [
                {
                    "UserName": "testuser-51",
                    "Path": "/",
                    "CreateDate": "2012-10-14T23:53:39Z",
                    "UserId": "EXAMPLEUSERID",
                    "Arn": "arn:aws:iam::123456:user/testuser2"
                },
            ],
            'Groups': []
        }

    def patch_make_request(self):
        make_request_patch = self.make_request_patch.start()
        make_request_patch.side_effect = [
            (self.http_response, self.first_parsed_response),
            (self.http_response, self.second_parsed_response),
        ]
        self.make_request_is_patched = True

    def test_text_response(self):
        output = self.run_cmd('iam list-users --output text', expected_rc=0)[0]
        self.assertEqual(
            output,
            ('USERS\tarn:aws:iam::12345:user/testuser1\t2013-02-12T19:08:52Z\t'
             '/\tEXAMPLEUSERID\ttestuser-50\n'))

        # Test something with a jmespath expression.
        output = self.run_cmd(
            'rds describe-engine-default-parameters ' \
            '--db-parameter-group-family mysql5.1 --output text',
            expected_rc=0)[0]
        self.assertEqual(
            output,
            'ENGINEDEFAULTS\tNone\n')


class CustomFormatter(Formatter):
    def __call__(self, operation, response, stream=None):
        self.stream = self._get_default_stream()


class TestDefaultStream(BaseAWSCommandParamsTest):
    @unittest.skipIf(six.PY3, "Text writer only vaild on py3.")
    def test_default_stream_with_table_output(self):
        formatter = CustomFormatter(None)
        stream = cStringIO()
        with mock.patch('sys.stdout', stream):
            formatter(None, None)
            formatter.stream.write(u'\u00e9')
        # Ensure the unicode data is written as UTF-8 by default.
        self.assertEqual(
            formatter.stream.getvalue(),
            u'\u00e9'.encode(locale.getpreferredencoding()))

########NEW FILE########
__FILENAME__ = test_describe_db_log_files
#!/usr/bin/env python
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestDescribeDBLogFiles(BaseAWSCommandParamsTest):
    maxDiff = None
    prefix = 'rds describe-db-log-files '

    def test_add_option(self):
        args = ('--file-last-written 10 '
                '--db-instance-identifier foo')
        cmdline = self.prefix + args
        result = {'DBInstanceIdentifier': 'foo',
                  'FileLastWritten': '10'}
        self.assert_params_for_cmd(cmdline, result)

########NEW FILE########
__FILENAME__ = test_modify_option_group
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestAddOptionGroup(BaseAWSCommandParamsTest):
    maxDiff = None
    # This tests the customization where modify-option-group
    # was split into two commands add-option-group and
    # remove-option-group.  This class is testing add-option-gruop

    prefix = 'rds add-option-to-option-group '

    def test_add_option(self):
        args = ('--option-group-name myoptiongroup2 '
                '--options {"OptionName":"TDE"}')
        cmdline = self.prefix + args
        result = {'OptionsToInclude.member.1.OptionName': 'TDE',
                  'OptionGroupName': 'myoptiongroup2'}
        self.assert_params_for_cmd(cmdline, result)

    def test_option_to_remove_is_not_allowed(self):
        args = ('--option-group-name myoptiongroup2 '
                '--options-to-remove foo')
        cmdline = self.prefix + args
        self.assert_params_for_cmd(
            cmdline, {}, 255,
            stderr_contains='Unknown options: --options-to-remove')


class TestRemoveOptionGroup(BaseAWSCommandParamsTest):

    prefix = 'rds remove-option-from-option-group '

    def test_remove_options(self):
        args = ('--option-group-name myoptiongroup2 '
                '--options TDE')
        cmdline = self.prefix + args
        result = {'OptionsToRemove.member.1': 'TDE',
                  'OptionGroupName': 'myoptiongroup2'}
        self.assert_params_for_cmd(cmdline, result)

    def test_option_to_add_is_not_allowed(self):
        args = ('--option-group-name myoptiongroup2 '
                '--options-to-include {"OptionName":"TDE"}')
        cmdline = self.prefix + args
        result = {'OptionsToInclude.member.1.OptionName': 'TDE',
                  'OptionGroupName': 'myoptiongroup2'}
        self.assert_params_for_cmd(
            cmdline, {}, 255,
            stderr_contains='Unknown options: --options-to-include')


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_resource_id
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


CHANGEBATCH_JSON = ('{"Comment":"string","Changes":['
                    '{"Action":"CREATE","ResourceRecordSet":{'
                    '"Name":"test-foo.bar.com",'
                    '"Type":"CNAME",'
                    '"TTL":300,'
                    '"ResourceRecords":['
                    '{"Value":"foo-bar-com.us-west-1.elb.amazonaws.com"}'
                    ']}}]}')

CHANGEBATCH_XML = ('<ChangeResourceRecordSetsRequest '
                   'xmlns="https://route53.amazonaws.com/doc/2013-04-01/">'
                   '<ChangeBatch><Comment>string</Comment>'
                   '<Changes><Change><Action>CREATE</Action>'
                   '<ResourceRecordSet>'
                   '<Name>test-foo.bar.com</Name>'
                   '<Type>CNAME</Type><TTL>300</TTL>'
                   '<ResourceRecords><ResourceRecord>'
                   '<Value>foo-bar-com.us-west-1.elb.amazonaws.com</Value>'
                   '</ResourceRecord></ResourceRecords>'
                   '</ResourceRecordSet></Change></Changes>'
                   '</ChangeBatch></ChangeResourceRecordSetsRequest>')

class TestGetHostedZone(BaseAWSCommandParamsTest):

    prefix = 'route53 get-hosted-zone'

    def setUp(self):
        super(TestGetHostedZone, self).setUp()

    def test_full_resource_id(self):
        args = ' --id /hostedzone/ZD3IYMVP1KDDM'
        cmdline = self.prefix + args
        result = {'uri_params': {'Id': 'ZD3IYMVP1KDDM'},
                  'headers': {}}
        self.assert_params_for_cmd(cmdline, result, expected_rc=0,
                                   ignore_params=['payload'])[0]

    def test_short_resource_id(self):
        args = ' --id ZD3IYMVP1KDDM'
        cmdline = self.prefix + args
        result = {'uri_params': {'Id': 'ZD3IYMVP1KDDM'},
                  'headers': {}}
        self.assert_params_for_cmd(cmdline, result, expected_rc=0,
                                   ignore_params=['payload'])[0]


class TestChangeResourceRecord(BaseAWSCommandParamsTest):

    prefix = 'route53 change-resource-record-sets'

    def setUp(self):
        super(TestChangeResourceRecord, self).setUp()

    def test_full_resource_id(self):
        args = ' --hosted-zone-id /change/ZD3IYMVP1KDDM'
        args += ' --change-batch %s' % CHANGEBATCH_JSON
        cmdline = self.prefix + args
        result = {'uri_params': {'HostedZoneId': 'ZD3IYMVP1KDDM'},
                  'headers': {}}
        self.assert_params_for_cmd(cmdline, result, expected_rc=0,
                                   ignore_params=['payload'])[0]
        self.assertEqual(self.last_params['payload'].getvalue(),
                         CHANGEBATCH_XML)


class TestGetChange(BaseAWSCommandParamsTest):

    prefix = 'route53 get-change'

    def setUp(self):
        super(TestGetChange, self).setUp()

    def test_full_resource_id(self):
        args = ' --id /change/ZD3IYMVP1KDDM'
        cmdline = self.prefix + args
        result = {'uri_params': {'Id': 'ZD3IYMVP1KDDM'},
                  'headers': {}}
        self.assert_params_for_cmd(cmdline, result, expected_rc=0,
                                   ignore_params=['payload'])[0]

    def test_short_resource_id(self):
        args = ' --id ZD3IYMVP1KDDM'
        cmdline = self.prefix + args
        result = {'uri_params': {'Id': 'ZD3IYMVP1KDDM'},
                  'headers': {}}
        self.assert_params_for_cmd(cmdline, result, expected_rc=0,
                                   ignore_params=['payload'])[0]


class TestMaxItems(BaseAWSCommandParamsTest):

    prefix = 'route53 list-resource-record-sets'

    def test_full_resource_id(self):
        args = ' --hosted-zone-id /hostedzone/ABCD --max-items 1'
        cmdline = self.prefix + args
        result = {
            'uri_params': {
                'HostedZoneId': 'ABCD',
            },
            'headers': {}
        }
        self.assert_params_for_cmd(cmdline, result, expected_rc=0,
                                   ignore_params=['payload'])[0]

########NEW FILE########
__FILENAME__ = test_get_object
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import os
import re

import six

import awscli.clidriver


class TestGetObject(BaseAWSCommandParamsTest):

    prefix = 's3api get-object'

    def setUp(self):
        super(TestGetObject, self).setUp()
        self.parsed_response = {'Body': six.StringIO()}

    def remove_file_if_exists(self, filename):
        if os.path.isfile(filename):
            os.remove(filename)

    def test_simple(self):
        cmdline = self.prefix
        cmdline += ' --bucket mybucket'
        cmdline += ' --key mykey'
        cmdline += ' outfile'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {},}
        self.addCleanup(self.remove_file_if_exists, 'outfile')
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])

    def test_range(self):
        cmdline = self.prefix
        cmdline += ' --bucket mybucket'
        cmdline += ' --key mykey'
        cmdline += ' --range bytes=0-499'
        cmdline += ' outfile'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {'Range': 'bytes=0-499'},}
        self.addCleanup(self.remove_file_if_exists, 'outfile')
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])

    def test_response_headers(self):
        cmdline = self.prefix
        cmdline += ' --bucket mybucket'
        cmdline += ' --key mykey'
        cmdline += ' --response-cache-control No-cache'
        cmdline += ' --response-content-encoding x-gzip'
        cmdline += ' outfile'
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey',
                                 'ResponseCacheControl': 'No-cache',
                                 'ResponseContentEncoding': 'x-gzip'},
                  'headers': {},}
        self.addCleanup(self.remove_file_if_exists, 'outfile')
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_list_objects
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestListObjects(BaseAWSCommandParamsTest):

    prefix = 's3api list-objects'

    def setUp(self):
        super(TestListObjects, self).setUp()
        self.parsed_response = {'Contents': []}

    def test_simple(self):
        cmdline = self.prefix
        cmdline += ' --bucket mybucket'
        result = {'uri_params': {'Bucket': 'mybucket'},
                  'headers': {},}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])

    def test_max_items(self):
        cmdline = self.prefix
        cmdline += ' --bucket mybucket'
        # The max-items is a customization and therefore won't
        # show up in the result params.
        cmdline += ' --max-items 100'
        result = {'uri_params': {'Bucket': 'mybucket'},
                  'headers': {},}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])

    def test_starting_token(self):
        # We don't need to test this in depth because botocore
        # tests this.  We just want to make sure this is hooked up
        # properly.
        cmdline = self.prefix
        cmdline += ' --bucket mybucket'
        cmdline += ' --starting-token foo___2'
        result = {'uri_params': {'Bucket': 'mybucket', 'Marker': 'foo'},
                  'headers': {},}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])

    def test_no_paginate(self):
        cmdline = self.prefix
        cmdline += ' --bucket mybucket --no-paginate'
        result = {'uri_params': {'Bucket': 'mybucket'},
                  'headers': {},}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])

    def test_max_keys_can_be_specified(self):
        cmdline = self.prefix
        # --max-keys is a hidden argument and not documented,
        # but for back-compat reasons if a user specifies this,
        # we will automatically see this and turn auto-pagination off.
        cmdline += ' --bucket mybucket --max-keys 1'
        result = {'uri_params': {'Bucket': 'mybucket', 'MaxKeys': 1},
                  'headers': {},}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])
        self.assertEqual(len(self.operations_called), 1)
        self.assertEqual(len(self.operations_called), 1)
        self.assertEqual(self.operations_called[0][0].name, 'ListObjects')

########NEW FILE########
__FILENAME__ = test_put_bucket_tagging
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import re
import copy

from awscli.testutils import BaseAWSCommandParamsTest
import six


# file is gone in python3, so instead IOBase must be used.
# Given this test module is the only place that cares about
# this type check, we do the check directly in this test module.
try:
    file_type = file
except NameError:
    import io
    file_type = io.IOBase


TAGSET = """{"TagSet":[{"Key":"key1","Value":"value1"},{"Key":"key2","Value":"value2"}]}"""


class TestPutBucketTagging(BaseAWSCommandParamsTest):

    prefix = 's3api put-bucket-tagging'

    def setUp(self):
        super(TestPutBucketTagging, self).setUp()
        self.payload = None

    def test_simple(self):
        cmdline = self.prefix
        cmdline += ' --bucket mybucket'
        cmdline += ' --tagging %s' % TAGSET
        result = {'uri_params': {'Bucket': 'mybucket'},
                  'headers': {'Content-MD5': '5s++BGwLE2moBAK9duxpFw=='}}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_put_object
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import os
import re
import copy

from awscli.testutils import BaseAWSCommandParamsTest
import six

import awscli.clidriver

# file is gone in python3, so instead IOBase must be used.
# Given this test module is the only place that cares about
# this type check, we do the check directly in this test module.
try:
    file_type = file
except NameError:
    import io
    file_type = io.IOBase


class TestGetObject(BaseAWSCommandParamsTest):

    prefix = 's3api put-object'

    def setUp(self):
        super(TestGetObject, self).setUp()
        self.file_path = os.path.join(os.path.dirname(__file__),
                                      'test_put_object_data')

    def test_simple(self):
        cmdline = self.prefix
        cmdline += ' --bucket mybucket'
        cmdline += ' --key mykey'
        cmdline += ' --body %s' % self.file_path
        result = {'uri_params': {'Bucket': 'mybucket',
                                 'Key': 'mykey'},
                  'headers': {}}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])
        self.assertIsInstance(self.last_params['payload'].getvalue(), file_type)

    def test_headers(self):
        cmdline = self.prefix
        cmdline += ' --bucket mybucket'
        cmdline += ' --key mykey'
        cmdline += ' --body %s' % self.file_path
        cmdline += ' --acl public-read'
        cmdline += ' --content-encoding x-gzip'
        cmdline += ' --content-type text/plain'
        result = {'uri_params': {'Bucket': 'mybucket', 'Key': 'mykey'},
                  'headers': {'x-amz-acl': 'public-read',
                              'Content-Encoding': 'x-gzip',
                              'Content-Type': 'text/plain'}}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])
        payload = self.last_params['payload'].getvalue()
        self.assertEqual(payload.name, self.file_path)

    def test_website_redirect(self):
        cmdline = self.prefix
        cmdline += ' --bucket mybucket'
        cmdline += ' --key mykey'
        cmdline += ' --acl public-read'
        cmdline += ' --website-redirect-location http://www.example.com/'
        result = {
            'uri_params': {'Bucket': 'mybucket', 'Key': 'mykey'},
            'headers': {
                'x-amz-acl': 'public-read',
                'x-amz-website-redirect-location': 'http://www.example.com/',
            }}
        self.assert_params_for_cmd(cmdline, result, ignore_params=['payload'])


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_send_email
#!/usr/bin/env python
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest

from six.moves import cStringIO
import mock


class TestSendEmail(BaseAWSCommandParamsTest):

    prefix = 'ses send-email'

    def test_plain_text(self):
        args = (' --subject This_is_a_test --from foo@bar.com'
                ' --to fie@baz.com --text This_is_the_message')
        args_list = (self.prefix + args).split()
        result = {'Source': 'foo@bar.com',
                  'Destination.ToAddresses.member.1': 'fie@baz.com',
                  'Message.Subject.Data': 'This_is_a_test',
                  'Message.Body.Text.Data': 'This_is_the_message'}

        self.assert_params_for_cmd(args_list, result)

    def test_plain_text_multiple_to(self):
        args = (' --subject This_is_a_test --from foo@bar.com'
                ' --to fie1@baz.com fie2@baz.com --text This_is_the_message')
        args_list = (self.prefix + args).split()
        result = {'Source': 'foo@bar.com',
                  'Destination.ToAddresses.member.1': 'fie1@baz.com',
                  'Destination.ToAddresses.member.2': 'fie2@baz.com',
                  'Message.Subject.Data': 'This_is_a_test',
                  'Message.Body.Text.Data': 'This_is_the_message'}

        self.assert_params_for_cmd(args_list, result)

    def test_plain_text_multiple_cc(self):
        args = (' --subject This_is_a_test --from foo@bar.com'
                ' --to fie1@baz.com fie2@baz.com --text This_is_the_message'
                ' --cc fie3@baz.com fie4@baz.com')
        args_list = (self.prefix + args).split()
        result = {'Source': 'foo@bar.com',
                  'Destination.ToAddresses.member.1': 'fie1@baz.com',
                  'Destination.ToAddresses.member.2': 'fie2@baz.com',
                  'Destination.CcAddresses.member.1': 'fie3@baz.com',
                  'Destination.CcAddresses.member.2': 'fie4@baz.com',
                  'Message.Subject.Data': 'This_is_a_test',
                  'Message.Body.Text.Data': 'This_is_the_message'}

        self.assert_params_for_cmd(args_list, result)

    def test_plain_text_multiple_bcc(self):
        args = (' --subject This_is_a_test --from foo@bar.com'
                ' --to fie1@baz.com fie2@baz.com --text This_is_the_message'
                ' --cc fie3@baz.com fie4@baz.com'
                ' --bcc fie5@baz.com fie6@baz.com')
        args_list = (self.prefix + args).split()
        result = {'Source': 'foo@bar.com',
                  'Destination.ToAddresses.member.1': 'fie1@baz.com',
                  'Destination.ToAddresses.member.2': 'fie2@baz.com',
                  'Destination.CcAddresses.member.1': 'fie3@baz.com',
                  'Destination.CcAddresses.member.2': 'fie4@baz.com',
                  'Destination.BccAddresses.member.1': 'fie5@baz.com',
                  'Destination.BccAddresses.member.2': 'fie6@baz.com',
                  'Message.Subject.Data': 'This_is_a_test',
                  'Message.Body.Text.Data': 'This_is_the_message'}

        self.assert_params_for_cmd(args_list, result)

    def test_html_text(self):
        args = (' --subject This_is_a_test --from foo@bar.com'
                ' --to fie@baz.com --html This_is_the_html_message')
        args_list = (self.prefix + args).split()
        result = {'Source': 'foo@bar.com',
                  'Destination.ToAddresses.member.1': 'fie@baz.com',
                  'Message.Subject.Data': 'This_is_a_test',
                  'Message.Body.Html.Data': 'This_is_the_html_message'}

        self.assert_params_for_cmd(args_list, result)

    def test_html_both(self):
        args = (' --subject This_is_a_test --from foo@bar.com'
                ' --to fie@baz.com --html This_is_the_html_message'
                ' --text This_is_the_text_message')
        args_list = (self.prefix + args).split()
        result = {'Source': 'foo@bar.com',
                  'Destination.ToAddresses.member.1': 'fie@baz.com',
                  'Message.Subject.Data': 'This_is_a_test',
                  'Message.Body.Text.Data': 'This_is_the_text_message',
                  'Message.Body.Html.Data': 'This_is_the_html_message'}

        self.assert_params_for_cmd(args_list, result)

    def test_using_json(self):
        args = (' --message {"Subject":{"Data":"This_is_a_test"},"Body":{"Text":{"Data":"This_is_the_message"}}}'
                ' --from foo@bar.com'
                ' --destination {"ToAddresses":["fie@baz.com"]}')
        args_list = (self.prefix + args).split()
        result = {'Source': 'foo@bar.com',
                  'Destination.ToAddresses.member.1': 'fie@baz.com',
                  'Message.Subject.Data': 'This_is_a_test',
                  'Message.Body.Text.Data': 'This_is_the_message'}

        self.assert_params_for_cmd(args_list, result)

    def test_both_destination_and_to(self):
        captured = cStringIO()
        args = (' --message {"Subject":{"Data":"This_is_a_test"},"Body":{"Text":{"Data":"This_is_the_message"}}}'
                ' --from foo@bar.com'
                ' --destination {"ToAddresses":["fie@baz.com"]}'
                ' --to fie2@baz.com')
        args_list = (self.prefix + args).split()
        self.assert_params_for_cmd(args_list, {}, expected_rc=255)

    def test_both_message_and_text(self):
        captured = cStringIO()
        args = (' --message {"Subject":{"Data":"This_is_a_test"},"Body":{"Text":{"Data":"This_is_the_message"}}}'
                ' --from foo@bar.com'
                ' --destination {"ToAddresses":["fie@baz.com"]}'
                ' --text This_is_another_body')
        args_list = (self.prefix + args).split()
        self.assert_params_for_cmd(args_list, {}, expected_rc=255)

########NEW FILE########
__FILENAME__ = test_create_platform_application
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest


class TestCreatePlatformApplication(BaseAWSCommandParamsTest):

    prefix = 'sns create-platform-application'

    def test_gcm_shorthand(self):
        cmdline = self.prefix
        cmdline += ' --name gcmpushapp'
        cmdline += ' --platform GCM'
        cmdline += ' --attributes '
        cmdline += 'PlatformCredential=foo,'
        cmdline += 'PlatformPrincipal=bar'
        result = {'Name': 'gcmpushapp',
                  'Platform': 'GCM',
                  'Attributes.entry.1.key': 'PlatformCredential',
                  'Attributes.entry.1.value': 'foo',
                  'Attributes.entry.2.key': 'PlatformPrincipal',
                  'Attributes.entry.2.value': 'bar'}
        self.assert_params_for_cmd(cmdline, result)

    def test_gcm_json(self):
        cmdline = self.prefix
        cmdline += ' --name gcmpushapp'
        cmdline += ' --platform GCM'
        cmdline += ' --attributes '
        cmdline += ('{"PlatformCredential":"AIzaSyClE2lcV2zEKTLYYo645zfk2jhQPFeyxDo",'
                    '"PlatformPrincipal":"There+is+no+principal+for+GCM"}')
        result = {'Name': 'gcmpushapp',
                  'Platform': 'GCM',
                  'Attributes.entry.1.key': 'PlatformCredential',
                  'Attributes.entry.1.value': 'AIzaSyClE2lcV2zEKTLYYo645zfk2jhQPFeyxDo',
                  'Attributes.entry.2.key': 'PlatformPrincipal',
                  'Attributes.entry.2.value': 'There+is+no+principal+for+GCM'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_add_permission
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import awscli.clidriver


class TestAddPermission(BaseAWSCommandParamsTest):

    prefix = 'sqs add-permission'
    queue_url = 'https://queue.amazonaws.com/4444/testcli'

    def test_all_param(self):
        cmdline = self.prefix
        cmdline += ' --queue-url %s' % self.queue_url
        cmdline += ' --aws-account-ids 888888888888'
        cmdline += ' --actions SendMessage'
        cmdline += ' --label FooBarLabel'
        result = {'QueueUrl': self.queue_url,
                  'ActionName.1': 'SendMessage',
                  'AWSAccountId.1': '888888888888',
                  'Label': 'FooBarLabel'}
        self.assert_params_for_cmd(cmdline, result)

    def test_multiple_accounts(self):
        cmdline = self.prefix
        cmdline += ' --queue-url %s' % self.queue_url
        cmdline += ' --aws-account-ids 888888888888 999999999999'
        cmdline += ' --actions SendMessage'
        cmdline += ' --label FooBarLabel'
        result = {'QueueUrl': self.queue_url,
                  'ActionName.1': 'SendMessage',
                  'AWSAccountId.1': '888888888888',
                  'AWSAccountId.2': '999999999999',
                  'Label': 'FooBarLabel'}
        self.assert_params_for_cmd(cmdline, result)

    def test_multiple_actions(self):
        cmdline = self.prefix
        cmdline += ' --queue-url %s' % self.queue_url
        cmdline += ' --aws-account-ids 888888888888'
        cmdline += ' --actions SendMessage ReceiveMessage'
        cmdline += ' --label FooBarLabel'
        result = {'QueueUrl': self.queue_url,
                  'ActionName.1': 'SendMessage',
                  'ActionName.2': 'ReceiveMessage',
                  'AWSAccountId.1': '888888888888',
                  'Label': 'FooBarLabel'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_change_message_visibility
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import awscli.clidriver


class TestChangeMessageVisibility(BaseAWSCommandParamsTest):

    prefix = 'sqs change-message-visibility'
    queue_url = 'https://queue.amazonaws.com/4444/testcli'
    receipt_handle = 'abcedfghijklmnopqrstuvwxyz'

    def test_all_params(self):
        cmdline = self.prefix
        cmdline += ' --queue-url %s' % self.queue_url
        cmdline += ' --receipt-handle %s' % self.receipt_handle
        cmdline += ' --visibility-timeout 30'
        result = {'QueueUrl': self.queue_url,
                  'ReceiptHandle': self.receipt_handle,
                  'VisibilityTimeout': '30'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_create_queue
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import awscli.clidriver


class TestCreateQueue(BaseAWSCommandParamsTest):

    prefix = 'sqs create-queue'
    queue_name = 'foobar'

    def test_simple(self):
        cmdline = self.prefix
        cmdline += ' --queue-name %s' % self.queue_name
        result = {'QueueName': self.queue_name}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_get_queue_attributes
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import awscli.clidriver


class TestGetQueueAttributes(BaseAWSCommandParamsTest):

    prefix = 'sqs get-queue-attributes'
    queue_url = 'https://queue.amazonaws.com/4444/testcli'

    def test_no_attr(self):
        cmdline = self.prefix + ' --queue-url %s' % self.queue_url
        result = {'QueueUrl': self.queue_url}
        self.assert_params_for_cmd(cmdline, result)

    def test_all(self):
        cmdline = self.prefix + ' --queue-url %s' % self.queue_url
        cmdline += ' --attribute-names All'
        result = {'QueueUrl': self.queue_url,
                  'AttributeName.1': 'All'}
        self.assert_params_for_cmd(cmdline, result)

    def test_one(self):
        cmdline = self.prefix + ' --queue-url %s' % self.queue_url
        cmdline += ' --attribute-names VisibilityTimeout'
        result = {'QueueUrl': self.queue_url,
                  'AttributeName.1': 'VisibilityTimeout'}
        self.assert_params_for_cmd(cmdline, result)

    def test_two(self):
        cmdline = self.prefix + ' --queue-url %s' % self.queue_url
        cmdline += ' --attribute-names VisibilityTimeout QueueArn'
        result = {'QueueUrl': self.queue_url,
                  'AttributeName.1': 'VisibilityTimeout',
                  'AttributeName.2': 'QueueArn'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_list_queues
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import awscli.clidriver


class TestListQueues(BaseAWSCommandParamsTest):

    prefix = 'sqs list-queues'

    def test_no_param(self):
        cmdline = self.prefix
        result = {}
        self.assert_params_for_cmd(cmdline, result)

    def test_prefix(self):
        cmdline = self.prefix + ' --queue-name-prefix test'
        result = {'QueueNamePrefix': 'test'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_set_queue_attributes
#!/usr/bin/env python
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import BaseAWSCommandParamsTest
import awscli.clidriver


class TestSetQueueAttributes(BaseAWSCommandParamsTest):

    prefix = 'sqs set-queue-attributes'
    queue_url = 'https://queue.amazonaws.com/4444/testcli'

    def test_one(self):
        cmdline = self.prefix + ' --queue-url %s' % self.queue_url
        cmdline += ' --attributes {"VisibilityTimeout":"15"}'
        result = {'QueueUrl': self.queue_url,
                  'Attribute.1.Name': 'VisibilityTimeout',
                  'Attribute.1.Value': '15'}
        self.assert_params_for_cmd(cmdline, result)

    def test_shorthand(self):
        cmdline = self.prefix + ' --queue-url %s' % self.queue_url
        cmdline += ' --attributes VisibilityTimeout=15'
        result = {'QueueUrl': self.queue_url,
                  'Attribute.1.Name': 'VisibilityTimeout',
                  'Attribute.1.Value': '15'}
        self.assert_params_for_cmd(cmdline, result)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = test_argprocess
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import json
from awscli.testutils import unittest
from awscli.testutils import BaseCLIDriverTest
from awscli.testutils import temporary_file

import mock

from awscli.clidriver import CLIArgument
from awscli.help import OperationHelpCommand
from awscli.argprocess import detect_shape_structure
from awscli.argprocess import unpack_cli_arg
from awscli.argprocess import ParamShorthand
from awscli.argprocess import ParamError
from awscli.argprocess import ParamUnknownKeyError
from awscli.argprocess import uri_param
from awscli.arguments import CustomArgument


MAPHELP = """--attributes key_name=string,key_name2=string
Where valid key names are:
  Policy"""


# These tests use real service types so that we can
# verify the real shapes of services.
class BaseArgProcessTest(BaseCLIDriverTest):

    def get_param_object(self, dotted_name):
        service_name, operation_name, param_name = dotted_name.split('.')
        service = self.session.get_service(service_name)
        operation = service.get_operation(operation_name)
        for p in operation.params:
            if p.name == param_name:
                return p
        else:
            raise ValueError("Unknown param: %s" % param_name)


class TestURIParams(BaseArgProcessTest):
    def test_uri_param(self):
        p = self.get_param_object('ec2.DescribeInstances.Filters')
        with temporary_file('r+') as f:
            json_argument = json.dumps([{"Name": "instance-id", "Values": ["i-1234"]}])
            f.write(json_argument)
            f.flush()
            result = uri_param(p, 'file://%s' % f.name)
        self.assertEqual(result, json_argument)

    def test_uri_param_no_paramfile_false(self):
        p = self.get_param_object('ec2.DescribeInstances.Filters')
        p.no_paramfile = False
        with temporary_file('r+') as f:
            json_argument = json.dumps([{"Name": "instance-id", "Values": ["i-1234"]}])
            f.write(json_argument)
            f.flush()
            result = uri_param(p, 'file://%s' % f.name)
        self.assertEqual(result, json_argument)

    def test_uri_param_no_paramfile_true(self):
        p = self.get_param_object('ec2.DescribeInstances.Filters')
        p.no_paramfile = True
        with temporary_file('r+') as f:
            json_argument = json.dumps([{"Name": "instance-id", "Values": ["i-1234"]}])
            f.write(json_argument)
            f.flush()
            result = uri_param(p, 'file://%s' % f.name)
        self.assertEqual(result, None)

class TestArgShapeDetection(BaseArgProcessTest):

    def assert_shape_type(self, spec, expected_type):
        p = self.get_param_object(spec)
        actual_structure = detect_shape_structure(p)
        self.assertEqual(actual_structure, expected_type)

    def test_detect_scalar(self):
        self.assert_shape_type('iam.AddRoleToInstanceProfile.RoleName',
                               'scalar')

    def test_detect_list_of_strings(self):
        self.assert_shape_type('sns.AddPermission.AWSAccountId', 'list-scalar')

    def test_detect_structure_of_scalars(self):
        self.assert_shape_type(
            'elasticbeanstalk.CreateConfigurationTemplate.SourceConfiguration',
            'structure(scalars)')

    def test_list_structure_scalars(self):
        self.assert_shape_type(
            'elb.RegisterInstancesWithLoadBalancer.Instances',
            'list-structure(scalar)')

    def test_list_structure_scalars_2(self):
        self.assert_shape_type(
            'elb.CreateLoadBalancer.Listeners',
            'list-structure(scalars)')

    def test_list_structure_of_list_and_strings(self):
        self.assert_shape_type(
            'ec2.DescribeInstances.Filters', 'list-structure(list-scalar, scalar)')

    def test_map_scalar(self):
        self.assert_shape_type(
            'sqs.SetQueueAttributes.Attributes', 'map-scalar')


class TestParamShorthand(BaseArgProcessTest):
    def setUp(self):
        super(TestParamShorthand, self).setUp()
        self.simplify = ParamShorthand()

    def test_simplify_structure_scalars(self):
        p = self.get_param_object(
            'elasticbeanstalk.CreateConfigurationTemplate.SourceConfiguration')
        value = 'ApplicationName=foo,TemplateName=bar'
        json_value = '{"ApplicationName": "foo", "TemplateName": "bar"}'
        returned = self.simplify(p, value)
        json_version = unpack_cli_arg(p, json_value)
        self.assertEqual(returned, json_version)

    def test_parse_boolean_shorthand(self):
        bool_param = mock.Mock()
        bool_param.type = 'boolean'
        self.assertTrue(unpack_cli_arg(bool_param, True))
        self.assertTrue(unpack_cli_arg(bool_param, 'True'))
        self.assertTrue(unpack_cli_arg(bool_param, 'true'))

        self.assertFalse(unpack_cli_arg(bool_param, False))
        self.assertFalse(unpack_cli_arg(bool_param, 'False'))
        self.assertFalse(unpack_cli_arg(bool_param, 'false'))

    def test_simplify_map_scalar(self):
        p = self.get_param_object('sqs.SetQueueAttributes.Attributes')
        returned = self.simplify(p, 'VisibilityTimeout=15')
        json_version = unpack_cli_arg(p, '{"VisibilityTimeout": "15"}')
        self.assertEqual(returned, {'VisibilityTimeout': '15'})
        self.assertEqual(returned, json_version)

    def test_list_structure_scalars(self):
        p = self.get_param_object(
            'elb.RegisterInstancesWithLoadBalancer.Instances')
        # Because this is a list type param, we'll use nargs
        # with argparse which means the value will be presented
        # to us as a list.
        returned = self.simplify(p, ['instance-1',  'instance-2'])
        self.assertEqual(returned, [{'InstanceId': 'instance-1'},
                                    {'InstanceId': 'instance-2'}])

    def test_list_structure_list_scalar(self):
        p = self.get_param_object('ec2.DescribeInstances.Filters')
        expected = [{"Name": "instance-id", "Values": ["i-1", "i-2"]},
                    {"Name": "architecture", "Values": ["i386"]}]
        returned = self.simplify(
            p, ["Name=instance-id,Values=i-1,i-2",
                "Name=architecture,Values=i386"])
        self.assertEqual(returned, expected)

        # With spaces around the comma.
        returned2 = self.simplify(
            p, ["Name=instance-id, Values=i-1,i-2",
                "Name=architecture, Values=i386"])
        self.assertEqual(returned2, expected)

        # Strip off leading/trailing spaces.
        returned3 = self.simplify(
            p, ["Name = instance-id, Values = i-1,i-2",
                "Name = architecture, Values = i386"])
        self.assertEqual(returned3, expected)

    def test_list_structure_list_scalar_2(self):
        p = self.get_param_object('emr.ModifyInstanceGroups.InstanceGroups')
        expected = [
            {"InstanceGroupId": "foo",
             "InstanceCount": 4},
            {"InstanceGroupId": "bar",
             "InstanceCount": 1}
        ]

        simplified = self.simplify(p, [
            "InstanceGroupId=foo,InstanceCount=4",
            "InstanceGroupId=bar,InstanceCount=1"
        ])

        self.assertEqual(simplified, expected)

    def test_list_structure_list_scalar_3(self):
        arg = CustomArgument('foo', schema={
            'type': 'array',
            'items': {
                'type': 'object',
                'properties': {
                    'Name': {
                        'type': 'string'
                    },
                    'Args': {
                        'type': 'array',
                        'items': {
                            'type': 'string'
                        }
                    }
                }
            }
        })
        arg.create_argument_object()
        p = arg.argument_object

        expected = [
            {"Name": "foo",
             "Args": ["a", "k1=v1", "b"]},
            {"Name": "bar",
             "Args": ["baz"]}
        ]

        simplified = self.simplify(p, [
            "Name=foo,Args=[a,k1=v1,b]",
            "Name=bar,Args=baz"
        ])

        self.assertEqual(simplified, expected)

    def test_list_structure_list_multiple_scalar(self):
        p = self.get_param_object('elastictranscoder.CreateJob.Playlists')
        returned = self.simplify(
            p, ['Name=foo,Format=hslv3,OutputKeys=iphone1,iphone2'])
        self.assertEqual(returned, [{'OutputKeys': ['iphone1', 'iphone2'],
                                     'Name': 'foo', 'Format': 'hslv3'}])

    def test_list_structure_scalars_2(self):
        p = self.get_param_object('elb.CreateLoadBalancer.Listeners')
        expected = [
            {"Protocol": "protocol1",
             "LoadBalancerPort": 1,
             "InstanceProtocol": "instance_protocol1",
             "InstancePort": 2,
             "SSLCertificateId": "ssl_certificate_id1"},
            {"Protocol": "protocol2",
             "LoadBalancerPort": 3,
             "InstanceProtocol": "instance_protocol2",
             "InstancePort": 4,
             "SSLCertificateId": "ssl_certificate_id2"},
        ]
        returned = unpack_cli_arg(
            p, ['{"Protocol": "protocol1", "LoadBalancerPort": 1, '
                '"InstanceProtocol": "instance_protocol1", '
                '"InstancePort": 2, "SSLCertificateId": '
                '"ssl_certificate_id1"}',
                '{"Protocol": "protocol2", "LoadBalancerPort": 3, '
                '"InstanceProtocol": "instance_protocol2", '
                '"InstancePort": 4, "SSLCertificateId": '
                '"ssl_certificate_id2"}',
            ])
        self.maxDiff = None
        self.assertEqual(returned, expected)
        simplified = self.simplify(p, [
            'Protocol=protocol1,LoadBalancerPort=1,'
            'InstanceProtocol=instance_protocol1,'
            'InstancePort=2,SSLCertificateId=ssl_certificate_id1',
            'Protocol=protocol2,LoadBalancerPort=3,'
            'InstanceProtocol=instance_protocol2,'
            'InstancePort=4,SSLCertificateId=ssl_certificate_id2'
        ])
        self.assertEqual(simplified, expected)

    def test_keyval_with_long_values(self):
        p = self.get_param_object(
            'dynamodb.UpdateTable.ProvisionedThroughput')
        value = 'WriteCapacityUnits=10,ReadCapacityUnits=10'
        returned = self.simplify(p, value)
        self.assertEqual(returned, {'WriteCapacityUnits': 10,
                                    'ReadCapacityUnits': 10})

    def test_error_messages_for_structure_scalar(self):
        p = self.get_param_object(
            'elasticbeanstalk.CreateConfigurationTemplate.SourceConfiguration')
        value = 'ApplicationName:foo,TemplateName=bar'
        error_msg = "Error parsing parameter '--source-configuration'.*should be"
        with self.assertRaisesRegexp(ParamError, error_msg):
            self.simplify(p, value)

    def test_mispelled_param_name(self):
        p = self.get_param_object(
            'elasticbeanstalk.CreateConfigurationTemplate.SourceConfiguration')
        error_msg = 'valid choices.*ApplicationName'
        with self.assertRaisesRegexp(ParamUnknownKeyError, error_msg):
            # Typo in 'ApplicationName'
            self.simplify(p, 'ApplicationNames=foo, TemplateName=bar')

    def test_improper_separator(self):
        # If the user uses ':' instead of '=', we should give a good
        # error message.
        p = self.get_param_object(
            'elasticbeanstalk.CreateConfigurationTemplate.SourceConfiguration')
        value = 'ApplicationName:foo,TemplateName:bar'
        error_msg = "Error parsing parameter '--source-configuration'.*should be"
        with self.assertRaisesRegexp(ParamError, error_msg):
            self.simplify(p, value)

    def test_improper_separator_for_filters_param(self):
        p = self.get_param_object('ec2.DescribeInstances.Filters')
        error_msg = "Error parsing parameter '--filters'.*should be"
        with self.assertRaisesRegexp(ParamError, error_msg):
            self.simplify(p, ["Name:tag:Name,Values:foo"])

    def test_unknown_key_for_filters_param(self):
        p = self.get_param_object('ec2.DescribeInstances.Filters')
        with self.assertRaisesRegexp(ParamUnknownKeyError,
                                     'valid choices.*Name'):
            self.simplify(p, ["Names=instance-id,Values=foo,bar"])

    def test_csv_syntax_escaped(self):
        p = self.get_param_object('cloudformation.CreateStack.Parameters')
        returned = self.simplify(
            p, ["ParameterKey=key,ParameterValue=foo\,bar"])
        expected = [{"ParameterKey": "key",
                     "ParameterValue": "foo,bar"}]
        self.assertEqual(returned, expected)

    def test_csv_syntax_double_quoted(self):
        p = self.get_param_object('cloudformation.CreateStack.Parameters')
        returned = self.simplify(
            p, ['ParameterKey=key,ParameterValue="foo,bar"'])
        expected = [{"ParameterKey": "key",
                     "ParameterValue": "foo,bar"}]
        self.assertEqual(returned, expected)

    def test_csv_syntax_single_quoted(self):
        p = self.get_param_object('cloudformation.CreateStack.Parameters')
        returned = self.simplify(
            p, ["ParameterKey=key,ParameterValue='foo,bar'"])
        expected = [{"ParameterKey": "key",
                     "ParameterValue": "foo,bar"}]
        self.assertEqual(returned, expected)

    def test_csv_syntax_errors(self):
        p = self.get_param_object('cloudformation.CreateStack.Parameters')
        error_msg = "Error parsing parameter '--parameters'.*should be"
        with self.assertRaisesRegexp(ParamError, error_msg):
            self.simplify(p, ['ParameterKey=key,ParameterValue="foo,bar'])
        with self.assertRaisesRegexp(ParamError, error_msg):
            self.simplify(p, ['ParameterKey=key,ParameterValue=foo,bar"'])
        with self.assertRaisesRegexp(ParamError, error_msg):
            self.simplify(p, ['ParameterKey=key,ParameterValue=""foo,bar"'])
        with self.assertRaisesRegexp(ParamError, error_msg):
            self.simplify(p, ['ParameterKey=key,ParameterValue="foo,bar\''])


class TestDocGen(BaseArgProcessTest):
    # These aren't very extensive doc tests, as we want to stay somewhat
    # flexible and allow the docs to slightly change without breaking these
    # tests.
    def setUp(self):
        super(TestDocGen, self).setUp()
        self.simplify = ParamShorthand()

    def test_gen_map_type_docs(self):
        p = self.get_param_object('sqs.SetQueueAttributes.Attributes')
        argument = CLIArgument(p.cli_name, p, p.operation)
        help_command = OperationHelpCommand(
            self.session, p.operation, None, {p.cli_name: argument},
            name='set-queue-attributes', event_class='sqs')
        help_command.param_shorthand.add_example_fn(p.cli_name, help_command)
        self.assertTrue(p.example_fn)
        doc_string = p.example_fn(p)
        self.assertIn(MAPHELP, doc_string)

    def test_gen_list_scalar_docs(self):
        p = self.get_param_object(
            'elb.RegisterInstancesWithLoadBalancer.Instances')
        argument = CLIArgument(p.cli_name, p, p.operation)
        help_command = OperationHelpCommand(
            self.session, p.operation, None,
            {p.cli_name: argument},
            name='register-instances-with-load-balancer',
            event_class='elb')
        help_command.param_shorthand.add_example_fn(p.cli_name, help_command)
        self.assertTrue(p.example_fn)
        doc_string = p.example_fn(p)
        self.assertEqual(doc_string,
                         '--instances InstanceId1 InstanceId2 InstanceId3')

    def test_gen_list_structure_of_scalars_docs(self):
        p = self.get_param_object('elb.CreateLoadBalancer.Listeners')
        argument = CLIArgument(p.cli_name, p, p.operation)
        help_command = OperationHelpCommand(
            self.session, p.operation, None, {p.cli_name: argument},
            name='create-load-balancer', event_class='elb')
        help_command.param_shorthand.add_example_fn(p.cli_name, help_command)
        self.assertTrue(p.example_fn)
        doc_string = p.example_fn(p)
        self.assertIn('Key value pairs, with multiple values separated by a space.', doc_string)
        self.assertIn('Protocol=string', doc_string)
        self.assertIn('LoadBalancerPort=integer', doc_string)
        self.assertIn('InstanceProtocol=string', doc_string)
        self.assertIn('InstancePort=integer', doc_string)
        self.assertIn('SSLCertificateId=string', doc_string)

    def test_gen_list_structure_multiple_scalar_docs(self):
        p = self.get_param_object('elastictranscoder.CreateJob.Playlists')
        argument = CLIArgument(p.cli_name, p, p.operation)
        help_command = OperationHelpCommand(
            self.session, p.operation, None, {p.cli_name: argument},
            name='create-job', event_class='elastictranscoder')
        help_command.param_shorthand.add_example_fn(p.cli_name, help_command)
        self.assertTrue(p.example_fn)
        doc_string = p.example_fn(p)
        s = 'Key value pairs, where values are separated by commas.\n--playlists Name=string1,Format=string1,OutputKeys=string1,string2'
        self.assertEqual(doc_string, s)


class TestUnpackJSONParams(BaseArgProcessTest):
    def setUp(self):
        super(TestUnpackJSONParams, self).setUp()
        self.simplify = ParamShorthand()

    def test_json_with_spaces(self):
        p = self.get_param_object('ec2.RunInstances.BlockDeviceMappings')
        # If a user specifies the json with spaces, it will show up as
        # a multi element list.  For example:
        # --block-device-mappings [{ "DeviceName":"/dev/sdf",
        # "VirtualName":"ephemeral0"}, {"DeviceName":"/dev/sdg",
        # "VirtualName":"ephemeral1" }]
        #
        # Will show up as:
        block_device_mapping = [
            '[{', 'DeviceName:/dev/sdf,', 'VirtualName:ephemeral0},',
            '{DeviceName:/dev/sdg,', 'VirtualName:ephemeral1', '}]']
        # The shell has removed the double quotes so this is invalid
        # JSON, but we should still raise a better exception.
        with self.assertRaises(ParamError) as e:
            unpack_cli_arg(p, block_device_mapping)
        # Parameter name should be in error message.
        self.assertIn('--block-device-mappings', str(e.exception))
        # The actual JSON itself should be in the error message.
        # Becaues this is a list, only the first element in the JSON
        # will show.  This will at least let customers know what
        # we tried to parse.
        self.assertIn('[{', str(e.exception))


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_arguments
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest
from awscli import arguments

class DemoArgument(arguments.CustomArgument):
    pass


class TestArgumentClasses(unittest.TestCase):
    def test_can_set_required(self):
        arg = DemoArgument('test-arg')
        self.assertFalse(arg.required)
        arg.required = True
        self.assertTrue(arg.required)

########NEW FILE########
__FILENAME__ = test_clidriver
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest
from awscli.testutils import BaseAWSCommandParamsTest
import logging

import mock
import six
from botocore.vendored.requests import models
from botocore.exceptions import NoCredentialsError

import awscli
from awscli.clidriver import CLIDriver
from awscli.clidriver import create_clidriver
from awscli.clidriver import CustomArgument
from awscli.clidriver import CLIOperationCaller
from awscli.customizations.commands import BasicCommand
from awscli import formatter
from botocore.hooks import HierarchicalEmitter
from botocore.provider import Provider


GET_DATA = {
    'cli': {
        'description': 'description',
        'synopsis': 'usage: foo',
        'options': {
            "debug": {
                "action": "store_true",
                "help": "Turn on debug logging"
            },
            "output": {
                "choices": [
                    "json",
                    "text",
                    "table"
                ],
                "metavar": "output_format"
            },
            "query": {
                "help": "<p>A JMESPath query to use in filtering the response data.</p>"
            },
            "profile": {
                "help": "Use a specific profile from your credential file",
                "metavar": "profile_name"
            },
            "region": {
                "choices": "{provider}/_regions",
                "metavar": "region_name"
            },
            "endpoint-url": {
                "help": "Override service's default URL with the given URL",
                "metavar": "endpoint_url"
            },
            "no-verify-ssl": {
                "action": "store_false",
                "dest": "verify_ssl",
                "help": "Override default behavior of verifying SSL certificates"
            },
            "no-paginate": {
                "action": "store_false",
                "help": "Disable automatic pagination",
                "dest": "paginate"
            },
        }
    },
    'aws/_services': {'s3':{}},
    'aws/_regions': {},
}

GET_VARIABLE = {
    'provider': 'aws',
    'output': 'json',
}


class FakeSession(object):
    def __init__(self, emitter=None):
        self.operation = None
        if emitter is None:
            emitter = HierarchicalEmitter()
        self.emitter = emitter
        self.provider = Provider(self, 'aws')
        self.profile = None
        self.stream_logger_args = None
        self.credentials = 'fakecredentials'

    def register(self, event_name, handler):
        self.emitter.register(event_name, handler)

    def emit(self, event_name, **kwargs):
        return self.emitter.emit(event_name, **kwargs)

    def emit_first_non_none_response(self, event_name, **kwargs):
        responses = self.emitter.emit(event_name, **kwargs)
        for _, response in responses:
            if response is not None:
                return response

    def get_available_services(self):
        return ['s3']

    def get_data(self, name):
        return GET_DATA[name]

    def get_config_variable(self, name):
        return GET_VARIABLE[name]

    def get_service(self, name):
        # Get service returns a service object,
        # so we'll just return a Mock object with
        # enough of the "right stuff".
        service = mock.Mock()
        operation = mock.Mock()
        param = mock.Mock()
        param.type = 'string'
        param.py_name = 'bucket'
        param.cli_name = '--bucket'
        param.name = 'bucket'
        operation.params = [param]
        operation.cli_name = 'list-objects'
        operation.name = 'ListObjects'
        operation.is_streaming.return_value = False
        operation.paginate.return_value.build_full_result.return_value = {
            'foo': 'paginate'}
        operation.call.return_value = (mock.Mock(), {'foo': 'bar'})
        self.operation = operation
        service.operations = [operation]
        service.name = 's3'
        service.cli_name = 's3'
        service.endpoint_prefix = 's3'
        service.get_operation.return_value = operation
        operation.service = service
        operation.service.session = self
        return service

    def get_service_data(self, service_name):
        return {'operations': {'ListObjects': {'input': {
            'members': dict.fromkeys(
                ['Bucket', 'Delimiter', 'Marker', 'MaxKeys', 'Prefix']),
        }}}}

    def user_agent(self):
        return 'user_agent'

    def set_stream_logger(self, *args, **kwargs):
        self.stream_logger_args = (args, kwargs)

    def get_credentials(self):
        return self.credentials


class FakeCommand(BasicCommand):
    def _run_main(self, args, parsed_globals):
        # We just return success. If this code is reached, it means that
        # all the logic in the __call__ method has sucessfully been run.
        # We subclass it here because the default implementation raises
        # an exception and we don't want that behavior.
        return 0


class FakeCommandVerify(FakeCommand):
    def _run_main(self, args, parsed_globals):
        # Verify passed arguments exist and then return success.
        # This will fail if the expected structure is missing, e.g.
        # if a string is passed in args instead of the expected
        # structure from a custom schema.
        assert args.bar[0]['Name'] == 'test'
        return 0


class TestCliDriver(unittest.TestCase):
    def setUp(self):
        self.session = FakeSession()

    def test_session_can_be_passed_in(self):
        driver = CLIDriver(session=self.session)
        self.assertEqual(driver.session, self.session)

    def test_paginate_rc(self):
        driver = CLIDriver(session=self.session)
        rc = driver.main('s3 list-objects --bucket foo'.split())
        self.assertEqual(rc, 0)

    def test_no_profile(self):
        driver = CLIDriver(session=self.session)
        driver.main('s3 list-objects --bucket foo'.split())
        self.assertEqual(driver.session.profile, None)

    def test_profile(self):
        driver = CLIDriver(session=self.session)
        driver.main('s3 list-objects --bucket foo --profile foo'.split())
        self.assertEqual(driver.session.profile, 'foo')

    def test_error_logger(self):
        driver = CLIDriver(session=self.session)
        driver.main('s3 list-objects --bucket foo --profile foo'.split())
        expected = {'log_level': logging.ERROR, 'logger_name': 'awscli'}
        self.assertEqual(driver.session.stream_logger_args[1], expected)


class TestCliDriverHooks(unittest.TestCase):
    # These tests verify the proper hooks are emitted in clidriver.
    def setUp(self):
        self.session = FakeSession()
        self.emitter = mock.Mock()
        self.emitter.emit.return_value = []
        self.stdout = six.StringIO()
        self.stderr = six.StringIO()
        self.stdout_patch = mock.patch('sys.stdout', self.stdout)
        self.stdout_patch.start()
        self.stderr_patch = mock.patch('sys.stderr', self.stderr)
        self.stderr_patch.start()

    def tearDown(self):
        self.stdout_patch.stop()
        self.stderr_patch.stop()

    def assert_events_fired_in_order(self, events):
        args = self.emitter.emit.call_args_list
        actual_events = [arg[0][0] for arg in args]
        self.assertEqual(actual_events, events)

    def serialize_param(self, param, value, **kwargs):
        if param.py_name == 'bucket':
            return value + '-altered!'

    def test_expected_events_are_emitted_in_order(self):
        self.emitter.emit.return_value = []
        self.session.emitter = self.emitter
        driver = CLIDriver(session=self.session)
        driver.main('s3 list-objects --bucket foo'.split())
        self.assert_events_fired_in_order([
            # Events fired while parser is being created.
            'building-command-table.main',
            'building-top-level-params',
            'top-level-args-parsed',
            'building-command-table.s3',
            'building-argument-table.s3.list-objects',
            'operation-args-parsed.s3.list-objects',
            'load-cli-arg.s3.list-objects.bucket',
            'process-cli-arg.s3.list-objects',
        ])

    def test_create_help_command(self):
        # When we generate the HTML docs, we don't actually run
        # commands, we just call the create_help_command methods.
        # We want to make sure that in this case, the corresponding
        # building-command-table events are fired.
        # The test above will prove that is true when running a command.
        # This test proves it is true when generating the HTML docs.
        self.emitter.emit.return_value = []
        self.session.emitter = self.emitter
        driver = CLIDriver(session=self.session)
        main_hc = driver.create_help_command()
        command = main_hc.command_table['s3']
        command.create_help_command()
        self.assert_events_fired_in_order([
            # Events fired while parser is being created.
            'building-command-table.main',
            'building-top-level-params',
            'building-command-table.s3',
        ])

    def test_cli_driver_changes_args(self):
        emitter = HierarchicalEmitter()
        emitter.register('process-cli-arg.s3.list-objects', self.serialize_param)
        self.session.emitter = emitter
        driver = CLIDriver(session=self.session)
        driver.main('s3 list-objects --bucket foo'.split())
        self.assertIn(mock.call.paginate(mock.ANY, bucket='foo-altered!'),
                      self.session.operation.method_calls)

    def test_unknown_params_raises_error(self):
        driver = CLIDriver(session=self.session)
        rc = driver.main('s3 list-objects --bucket foo --unknown-arg foo'.split())
        self.assertEqual(rc, 255)
        self.assertIn('Unknown options', self.stderr.getvalue())

    def test_unknown_command_suggests_help(self):
        driver = CLIDriver(session=self.session)
        # We're catching SystemExit here because this is raised from the bowels
        # of argparser so short of patching the ArgumentParser's exit() method,
        # we can just catch SystemExit.
        with self.assertRaises(SystemExit):
            # Note the typo in 'list-objects'
            driver.main('s3 list-objecst --bucket foo --unknown-arg foo'.split())
        # Tell the user what went wrong.
        self.assertIn("Invalid choice: 'list-objecst'", self.stderr.getvalue())
        # Offer the user a suggestion.
        self.assertIn("maybe you meant:\n\n  * list-objects", self.stderr.getvalue())


class TestSearchPath(unittest.TestCase):
    def tearDown(self):
        six.moves.reload_module(awscli)

    @mock.patch('os.pathsep', ';')
    @mock.patch('os.environ', {'AWS_DATA_PATH': 'c:\\foo;c:\\bar'})
    def test_windows_style_search_path(self):
        driver = CLIDriver()
        # Because the os.environ patching happens at import time,
        # we have to force a reimport of the module to test our changes.
        six.moves.reload_module(awscli)
        # Our two overrides should be the last two elements in the search path.
        search_path = driver.session.loader.get_search_paths()[:-2]
        self.assertEqual(search_path, ['c:\\foo', 'c:\\bar'])


class TestAWSCommand(BaseAWSCommandParamsTest):
    # These tests will simulate running actual aws commands
    # but with the http part mocked out.
    def setUp(self):
        super(TestAWSCommand, self).setUp()
        self.stderr = six.StringIO()
        self.stderr_patch = mock.patch('sys.stderr', self.stderr)
        self.stderr_patch.start()

    def tearDown(self):
        super(TestAWSCommand, self).tearDown()
        self.stderr_patch.stop()

    def record_get_endpoint_args(self, *args, **kwargs):
        self.get_endpoint_args = (args, kwargs)
        self.real_get_endpoint(*args, **kwargs)

    def inject_new_param(self, argument_table, **kwargs):
        argument = CustomArgument('unknown-arg', {})
        argument.add_to_arg_table(argument_table)

    def inject_new_param_no_paramfile(self, argument_table, **kwargs):
        argument = CustomArgument('unknown-arg', no_paramfile=True)
        argument.add_to_arg_table(argument_table)

    def inject_command(self, command_table, session, **kwargs):
        command = FakeCommand(session)
        command.NAME = 'foo'
        command.ARG_TABLE = [
            {'name': 'bar', 'action': 'store'}
        ]
        command_table['foo'] = command

    def inject_command_schema(self, command_table, session, **kwargs):
        command = FakeCommandVerify(session)
        command.NAME = 'foo'

        # Build a schema using all the types we are interested in
        schema = {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "Name": {
                        "type": "string",
                        "required": True
                    },
                    "Count": {
                        "type": "integer"
                    }
                }
            }
        }

        command.ARG_TABLE = [
            {'name': 'bar', 'schema': schema}
        ]

        command_table['foo'] = command

    def test_aws_with_endpoint_url(self):
        with mock.patch('botocore.service.Service.get_endpoint') as endpoint:
            http_response = models.Response()
            http_response.status_code = 200
            endpoint.return_value.make_request.return_value = (
                http_response, {})
            self.assert_params_for_cmd(
                'ec2 describe-instances --endpoint-url https://foobar.com/',
                expected_rc=0)
        endpoint.assert_called_with(region_name=None,
                                    verify=None,
                                    endpoint_url='https://foobar.com/')

    def test_aws_with_region(self):
        with mock.patch('botocore.service.Service.get_endpoint') as endpoint:
            http_response = models.Response()
            http_response.status_code = 200
            endpoint.return_value.make_request.return_value = (
                http_response, {})
            self.assert_params_for_cmd(
                'ec2 describe-instances --region us-east-1',
                expected_rc=0)
        endpoint.assert_called_with(region_name='us-east-1',
                                    verify=None,
                                    endpoint_url=None)

    def test_aws_with_verify_false(self):
        with mock.patch('botocore.service.Service.get_endpoint') as endpoint:
            http_response = models.Response()
            http_response.status_code = 200
            endpoint.return_value.make_request.return_value = (
                http_response, {})
            self.assert_params_for_cmd(
                'ec2 describe-instances --region us-east-1 --no-verify-ssl',
                expected_rc=0)
        # Because we used --no-verify-ssl, get_endpoint should be
        # called with verify=False
        endpoint.assert_called_with(region_name='us-east-1',
                                    verify=False,
                                    endpoint_url=None)

    def test_aws_with_cacert_env_var(self):
        with mock.patch('botocore.endpoint.QueryEndpoint.__init__') as endpoint:
            http_response = models.Response()
            http_response.status_code = 200
            endpoint.return_value = None
            endpoint.make_request.return_value = (
                http_response, {})
            self.environ['AWS_CA_BUNDLE'] = '/path/cacert.pem'
            self.assert_params_for_cmd(
                'ec2 describe-instances --region us-east-1',
                expected_rc=0)
        call_args = endpoint.call_args
        self.assertEqual(call_args[1]['verify'], '/path/cacert.pem')

    def test_default_to_verifying_ssl(self):
        with mock.patch('botocore.endpoint.QueryEndpoint.__init__') as endpoint:
            http_response = models.Response()
            http_response.status_code = 200
            endpoint.return_value = None
            endpoint.make_request.return_value = (
                http_response, {})
            self.assert_params_for_cmd(
                'ec2 describe-instances --region us-east-1',
                expected_rc=0)
        call_args = endpoint.call_args
        self.assertEqual(call_args[1]['verify'], True)

    def test_s3_with_region_and_endpoint_url(self):
        with mock.patch('botocore.service.Service.get_endpoint') as endpoint:
            http_response = models.Response()
            http_response.status_code = 200
            endpoint.return_value.make_request.return_value = (
                http_response, {'CommonPrefixes': [], 'Contents': []})
            self.assert_params_for_cmd(
                's3 ls s3://test --region us-east-1 --endpoint-url https://foobar.com/',
                expected_rc=0)
        endpoint.assert_called_with(region_name='us-east-1',
                                    endpoint_url='https://foobar.com/',
                                    verify=None)

    def test_s3_with_no_verify_ssl(self):
        with mock.patch('botocore.service.Service.get_endpoint') as endpoint:
            http_response = models.Response()
            http_response.status_code = 200
            endpoint.return_value.make_request.return_value = (
                http_response, {'CommonPrefixes': [], 'Contents': []})
            self.assert_params_for_cmd(
                's3 ls s3://test --no-verify-ssl',
                expected_rc=0)
        endpoint.assert_called_with(region_name=None,
                                    endpoint_url=None,
                                    verify=False)

    def test_event_emission_for_top_level_params(self):
        driver = create_clidriver()
        # --unknown-foo is an known arg, so we expect a 255 rc.
        rc = driver.main('ec2 describe-instances --unknown-arg foo'.split())
        self.assertEqual(rc, 255)
        self.assertIn('Unknown options: --unknown-arg', self.stderr.getvalue())

        # The argument table is memoized in the CLIDriver object. So
        # when we call main() above, it will get created and cached
        # and the argument table won't get created again (and therefore
        # the building-top-level-params event will not get generated again).
        # So, for this test we need to create a new driver object.
        driver = create_clidriver()
        driver.session.register(
            'building-top-level-params', self.inject_new_param)
        driver.session.register(
            'top-level-args-parsed',
            lambda parsed_args, **kwargs: args_seen.append(parsed_args))

        args_seen = []

        # Now we should get an rc of 0 as the arg is expected
        # (though nothing actually does anything with the arg).
        self.patch_make_request()
        rc = driver.main('ec2 describe-instances --unknown-arg foo'.split())
        self.assertEqual(rc, 0)
        self.assertEqual(len(args_seen), 1)
        self.assertEqual(args_seen[0].unknown_arg, 'foo')

    def test_custom_arg_paramfile(self):
        with mock.patch('awscli.handlers.uri_param',
                        return_value=None) as uri_param_mock:
            driver = create_clidriver()
            driver.session.register(
                'building-argument-table', self.inject_new_param)

            self.patch_make_request()
            rc = driver.main(
                'ec2 describe-instances --unknown-arg file:///foo'.split())

            self.assertEqual(rc, 0)

            # Make sure uri_param was called
            uri_param_mock.assert_called()
            # Make sure it was called with our passed-in URI
            self.assertEqual('file:///foo',
                             uri_param_mock.call_args_list[-1][1]['value'])

    def test_custom_command_paramfile(self):
        with mock.patch('awscli.handlers.uri_param',
                        return_value=None) as uri_param_mock:
            driver = create_clidriver()
            driver.session.register(
                'building-command-table', self.inject_command)

            self.patch_make_request()
            rc = driver.main(
                'ec2 foo --bar file:///foo'.split())

            self.assertEqual(rc, 0)

            uri_param_mock.assert_called()

    def test_custom_arg_no_paramfile(self):
        driver = create_clidriver()
        driver.session.register(
            'building-argument-table', self.inject_new_param_no_paramfile)

        self.patch_make_request()
        rc = driver.main(
            'ec2 describe-instances --unknown-arg file:///foo'.split())

        self.assertEqual(rc, 0)

    def test_custom_command_schema(self):
        driver = create_clidriver()
        driver.session.register(
            'building-command-table', self.inject_command_schema)

        self.patch_make_request()

        # Test single shorthand item
        rc = driver.main(
            'ec2 foo --bar Name=test,Count=4'.split())

        self.assertEqual(rc, 0)

        # Test shorthand list of items with optional values
        rc = driver.main(
            'ec2 foo --bar Name=test,Count=4 Name=another'.split())

        self.assertEqual(rc, 0)

        # Test missing require shorthand item
        rc = driver.main(
            'ec2 foo --bar Count=4'.split())

        self.assertEqual(rc, 255)

        # Test extra unknown shorthand item
        rc = driver.main(
            'ec2 foo --bar Name=test,Unknown='.split())

        self.assertEqual(rc, 255)

        # Test long form JSON
        rc = driver.main(
            'ec2 foo --bar {"Name":"test","Count":4}'.split())

        self.assertEqual(rc, 0)

        # Test malformed long form JSON
        rc = driver.main(
            'ec2 foo --bar {"Name":"test",Count:4}'.split())

        self.assertEqual(rc, 255)

    def test_empty_params_gracefully_handled(self):
        # Simulates the equivalent in bash: --identifies ""
        cmd = 'ses get-identity-dkim-attributes --identities'.split()
        cmd.append('')
        self.assert_params_for_cmd(cmd,expected_rc=0)

    def test_file_param_does_not_exist(self):
        driver = create_clidriver()
        rc = driver.main('ec2 describe-instances '
                         '--filters file://does/not/exist.json'.split())
        self.assertEqual(rc, 255)
        self.assertIn("Error parsing parameter '--filters': "
                      "file does not exist: does/not/exist.json",
                      self.stderr.getvalue())

    def test_aws_configure_in_error_message_no_credentials(self):
        driver = create_clidriver()
        def raise_exception(*args, **kwargs):
            raise NoCredentialsError()
        driver.session.register(
            'building-command-table',
            lambda command_table, **kwargs: \
                command_table.__setitem__('ec2', raise_exception))
        with mock.patch('sys.stderr') as f:
            driver.main('ec2 describe-instances'.split())
        self.assertEqual(
            f.write.call_args_list[0][0][0],
            'Unable to locate credentials. '
            'You can configure credentials by running "aws configure".')


class TestHTTPParamFileDoesNotExist(BaseAWSCommandParamsTest):

    def setUp(self):
        super(TestHTTPParamFileDoesNotExist, self).setUp()
        self.stderr = six.StringIO()
        self.stderr_patch = mock.patch('sys.stderr', self.stderr)
        self.stderr_patch.start()

    def tearDown(self):
        super(TestHTTPParamFileDoesNotExist, self).tearDown()
        self.stderr_patch.stop()

    def test_http_file_param_does_not_exist(self):
        error_msg = ("Error parsing parameter '--filters': "
                     "Unable to retrieve http://does/not/exist.json: "
                     "received non 200 status code of 404")
        with mock.patch('botocore.vendored.requests.get') as get:
            get.return_value.status_code = 404
            self.assert_params_for_cmd(
                'ec2 describe-instances --filters http://does/not/exist.json',
                expected_rc=255, stderr_contains=error_msg)


class TestCLIOperationCaller(BaseAWSCommandParamsTest):
    def setUp(self):
        super(TestCLIOperationCaller, self).setUp()
        self.session = mock.Mock()

    def test_invoke_with_no_credentials(self):
        # This is what happens you have no credentials.
        # get_credentials() return None.
        self.session.get_credentials.return_value = None
        caller = CLIOperationCaller(self.session)
        with self.assertRaises(NoCredentialsError):
            caller.invoke(None, None, None)


class TestVerifyArgument(BaseAWSCommandParamsTest):
    def setUp(self):
        super(TestVerifyArgument, self).setUp()
        self.driver.session.register('top-level-args-parsed', self.record_args)
        self.recorded_args = None

    def record_args(self, parsed_args, **kwargs):
        self.recorded_args = parsed_args

    def test_no_verify_argument(self):
        self.assert_params_for_cmd('s3api list-buckets --no-verify-ssl'.split())
        self.assertFalse(self.recorded_args.verify_ssl)

    def test_verify_argument_is_none_by_default(self):
        self.assert_params_for_cmd('s3api list-buckets'.split())
        self.assertIsNone(self.recorded_args.verify_ssl)


class TestFormatter(BaseAWSCommandParamsTest):
    def test_bad_output(self):
        with self.assertRaises(ValueError):
            formatter.get_formatter('bad-type', None)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_completer
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import create_clidriver
import os
import pprint
import logging
import difflib

import mock

from awscli.completer import Completer

LOG = logging.getLogger(__name__)

GLOBALOPTS = ['--debug', '--endpoint-url', '--no-verify-ssl',
              '--no-paginate', '--output', '--profile',
              '--region', '--version', '--color', '--query']

COMPLETIONS = [
    ('aws ', -1, set(['autoscaling', 'cloudformation', 'cloudsearch',
                      'cloudtrail', 'cloudwatch', 'configure', 'datapipeline',
                      'directconnect', 'dynamodb', 'ec2',
                      'elasticache', 'elasticbeanstalk', 'elastictranscoder',
                      'elb', 'iam', 'importexport', 'kinesis',
                      'opsworks', 'rds', 'redshift', 'route53', 's3', 's3api',
                      'ses', 'sns', 'sqs', 'storagegateway', 'sts',
                      'support', 'swf'])),
    ('aws cloud', -1, set(['cloudformation', 'cloudsearch',
                           'cloudtrail', 'cloudwatch'])),
    ('aws cloudf', -1, set(['cloudformation'])),
    ('aws cloudfr', -1, set([])),
    ('aws foobar', -1, set([])),
    ('aws  --', -1, set(GLOBALOPTS)),
    ('aws  --re', -1, set(['--region'])),
    ('aws sts ', -1, set(['assume-role', 'assume-role-with-saml',
                          'get-federation-token',
                          'decode-authorization-message',
                          'assume-role-with-web-identity',
                          'get-session-token'])),
    ('aws sts --debug --de', -1, set([])),
    ('aws sts de', -1, set(['decode-authorization-message'])),
    ('aws sts --', -1, set(GLOBALOPTS)),
    ('aws sts decode-authorization-message', -1, set([])),
    ('aws sts decode-authorization-message --encoded-message --re', -1,
     set(['--region'])),
    ('aws sts decode-authorization-message --encoded-message --enco', -1,
     set([])),
    ('aws ec2 --region ', -1, set(['us-east-1', 'ap-northeast-1',
                                   'sa-east-1', 'ap-southeast-1',
                                   'ap-southeast-2', 'us-west-2',
                                   'us-west-1', 'eu-west-1',
                                   'us-gov-west-1', 'fips-us-gov-west-1',
                                   'cn-north-1'])),
    ('aws ec2 --debug describe-instances --instance-ids ', -1,
     set([])),
    ('aws ec2 --debug describe-instances --instance-ids i-12345678 - ', -1,
     set(['--filters', '--dry-run', '--no-dry-run', '--endpoint-url',
          '--no-verify-ssl', '--no-paginate', '--output', '--profile',
          '--starting-token', '--max-items',
          '--region', '--version', '--color', '--query'])),
    ('aws s3', -1, set(['cp', 'mv', 'rm', 'mb', 'rb', 'ls', 'sync', 'website'])),
    ('aws s3 m', -1, set(['mv', 'mb'])),
    ('aws s3 cp -', -1, set(['--no-guess-mime-type', '--dryrun',
                             '--recursive', '--website-redirect',
                             '--quiet', '--acl', '--storage-class',
                             '--sse', '--exclude', '--include',
                             '--cache-control', '--content-type',
                             '--content-disposition',
                             '--content-encoding', '--content-language',
                             '--expires', '--grants'] + GLOBALOPTS)),
    ('aws s3 cp --quiet -', -1, set(['--no-guess-mime-type', '--dryrun',
                                     '--recursive', '--content-type',
                                     '--content-disposition', '--cache-control',
                                     '--content-encoding', '--content-language',
                                     '--expires', '--website-redirect', '--acl',
                                     '--storage-class', '--sse',
                                     '--exclude', '--include',
                                     '--grants'] + GLOBALOPTS)),
    ('aws emr ', -1, set([])),
    ]


def check_completer(cmdline, results, expected_results):
    if not results == expected_results:
        # Borrowed from assertDictEqual, though this doesn't
        # handle the case when unicode literals are used in one
        # dict but not in the other (and we want to consider them
        # as being equal).
        pretty_d1 = pprint.pformat(results, width=1).splitlines()
        pretty_d2 = pprint.pformat(expected_results, width=1).splitlines()
        diff = ('\n' + '\n'.join(difflib.ndiff(pretty_d1, pretty_d2)))
        raise AssertionError("Results are not equal:\n%s" % diff)
    assert results == expected_results


def test_completions():
    environ = {
        'AWS_DATA_PATH': os.environ['AWS_DATA_PATH'],
        'AWS_DEFAULT_REGION': 'us-east-1',
        'AWS_ACCESS_KEY_ID': 'access_key',
        'AWS_SECRET_ACCESS_KEY': 'secret_key',
        'AWS_CONFIG_FILE': '',
    }
    with mock.patch('os.environ', environ):
        completer = Completer()
        completer.clidriver = create_clidriver()
        for cmdline, point, expected_results in COMPLETIONS:
            if point == -1:
                point = len(cmdline)
            results = set(completer.complete(cmdline, point))
            yield check_completer, cmdline, results, expected_results

########NEW FILE########
__FILENAME__ = test_errorhandler
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest

import mock
from awscli import errorhandler


class TestErrorHandler(unittest.TestCase):

    def create_http_response(self, **kwargs):
        response = mock.Mock()
        for key, value in kwargs.items():
            setattr(response, key, value)
        return response

    def test_error_handler_client_side(self):
        response = {
            'CommonPrefixes': [],
            'Contents': [],
            'Errors': [{'Code': 'AccessDenied',
                        'HostId': 'foohost',
                        'Message': 'Access Denied',
                        'RequestId': 'requestid'}],
            'ResponseMetadata': {}}
        handler = errorhandler.ErrorHandler()
        http_response = self.create_http_response(status_code=403)
        # We're manually using the try/except form because
        # we want to catch the exception and assert that it has specific
        # attributes on it.
        operation = mock.Mock()
        operation.name = 'OperationName'
        try:
            handler(http_response, response, operation)
        except errorhandler.ClientError as e:
            # First, the operation name should be in the error message.
            self.assertIn('OperationName', str(e))
            # We should state that this is a ClientError.
            self.assertIn('client error', str(e))
            # And these values should be available on the exception
            # so clients can access this information programmatically.
            self.assertEqual(e.error_code, 'AccessDenied')
            self.assertEqual(e.error_message, 'Access Denied')
            self.assertEqual(e.operation_name, 'OperationName')
        except Exception as e:
            self.fail("Unexpected error raised: %s" % e)
        else:
            self.fail("Expected errorhandler.ClientError to be raised "
                      "but no exception was raised.")

    def test_no_exception_raised_on_200(self):
        response = {
            'CommonPrefixes': [],
            'Contents': [],
        }
        handler = errorhandler.ErrorHandler()
        http_response = self.create_http_response(status_code=200)
        # We're manually using the try/except form because
        # we want to catch the exception and assert that it has specific
        # attributes on it.
        operation = mock.Mock()
        operation.name = 'OperationName'
        try:
            self.assertIsNone(handler(http_response, response, operation))
        except errorhandler.BaseOperationError as e:
            self.fail("Unexpected error raised: %s" % e)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_help
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest
import sys
import os

import mock

from awscli.help import PosixHelpRenderer, ExecutableNotFoundError


class FakePosixHelpRenderer(PosixHelpRenderer):
    def __init__(self):
        self.exists_on_path = {}
        self.popen_calls = []

    def _exists_on_path(self, name):
        return self.exists_on_path.get(name)

    def _popen(self, *args, **kwargs):
        self.popen_calls.append((args, kwargs))
        return mock.Mock()


class TestHelpPager(unittest.TestCase):

    def setUp(self):
        self.environ = {}
        self.environ_patch = mock.patch('os.environ', self.environ)
        self.environ_patch.start()
        self.renderer = PosixHelpRenderer()

    def tearDown(self):
        self.environ_patch.stop()

    def test_no_env_vars(self):
        self.assertEqual(self.renderer.get_pager_cmdline(),
                         self.renderer.PAGER.split())

    def test_manpager(self):
        pager_cmd = 'foobar'
        os.environ['MANPAGER'] = pager_cmd
        self.assertEqual(self.renderer.get_pager_cmdline(),
                         pager_cmd.split())

    def test_pager(self):
        pager_cmd = 'fiebaz'
        os.environ['PAGER'] = pager_cmd
        self.assertEqual(self.renderer.get_pager_cmdline(),
                         pager_cmd.split())

    def test_both(self):
        os.environ['MANPAGER'] = 'foobar'
        os.environ['PAGER'] = 'fiebaz'
        self.assertEqual(self.renderer.get_pager_cmdline(),
                         'foobar'.split())

    def test_manpager_with_args(self):
        pager_cmd = 'less -X'
        os.environ['MANPAGER'] = pager_cmd
        self.assertEqual(self.renderer.get_pager_cmdline(),
                         pager_cmd.split())

    def test_pager_with_args(self):
        pager_cmd = 'less -X --clearscreen'
        os.environ['PAGER'] = pager_cmd
        self.assertEqual(self.renderer.get_pager_cmdline(),
                         pager_cmd.split())

    @unittest.skipIf(sys.platform.startswith('win'), "requires posix system")
    @mock.patch('sys.exit', mock.Mock())
    def test_no_groff_exists(self):
        renderer = FakePosixHelpRenderer()
        # Simulate neither rst2man.py nor rst2man existing on the path.
        renderer.exists_on_path['groff'] = False
        with self.assertRaisesRegexp(ExecutableNotFoundError,
                                     'Could not find executable named "groff"'):
            renderer.render('foo')

    def test_shlex_split_for_pager_var(self):
        pager_cmd = '/bin/sh -c "col -bx | vim -c \'set ft=man\' -"'
        os.environ['PAGER'] = pager_cmd
        self.assertEqual(self.renderer.get_pager_cmdline(),
                         ['/bin/sh', '-c', "col -bx | vim -c 'set ft=man' -"])

########NEW FILE########
__FILENAME__ = test_plugin
# Copyright 2012-2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
import sys
from awscli.testutils import unittest

import mock

from awscli import plugin
from botocore import hooks


class FakeModule(object):
    def __init__(self):
        self.called = False
        self.context = None
        self.events_seen = []

    def awscli_initialize(self, context):
        self.called = True
        self.context = context
        self.context.register(
            'before_operation',
            (lambda **kwargs: self.events_seen.append(kwargs)))


class TestPlugins(unittest.TestCase):

    def setUp(self):
        self.fake_module = FakeModule()
        sys.modules['__fake_plugin__'] = self.fake_module

    def tearDown(self):
        del sys.modules['__fake_plugin__']

    def test_plugin_register(self):
        emitter = plugin.load_plugins({'fake_plugin': '__fake_plugin__'})
        self.assertTrue(self.fake_module.called)
        self.assertTrue(isinstance(emitter, hooks.HierarchicalEmitter))
        self.assertTrue(isinstance(self.fake_module.context,
                                   hooks.HierarchicalEmitter))

    def test_event_hooks_can_be_passed_in(self):
        hooks = plugin.HierarchicalEmitter()
        emitter = plugin.load_plugins({'fake_plugin': '__fake_plugin__'},
                                      event_hooks=hooks)
        emitter.emit('before_operation')
        self.assertEqual(len(self.fake_module.events_seen), 1)


class TestPluginCanBePackage(unittest.TestCase):
    def setUp(self):
        self.fake_module = FakeModule()
        self.fake_package = mock.Mock()
        sys.modules['__fake_plugin__'] = self.fake_package
        sys.modules['__fake_plugin__.__fake__'] = self.fake_package
        sys.modules['__fake_plugin__.__fake__.bar'] = self.fake_module

    def tearDown(self):
        del sys.modules['__fake_plugin__.__fake__']

    def test_plugin_register(self):
        plugin.load_plugins(
            {'fake_plugin': '__fake_plugin__.__fake__.bar'})
        self.assertTrue(self.fake_module.called)


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_schema
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest

from awscli.schema import ParameterRequiredError, SchemaTransformer

"""
Note: this schema is currently not supported by the ParamShorthand
parser due to its complexity, but is tested here to ensure the
robustness of the transformer.
"""
INPUT_SCHEMA = {
    "type": "array",
    "items": {
        "type": "object",
        "properties": {
            "Name": {
                "type": "string",
                "description": "The name of the step. ",
            },
            "Jar": {
                "type": "string",
                "description": "A path to a JAR file run during the step.",
            },
            "Args": {
                "type": "array",
                "description":
                    "A list of command line arguments to pass to the step.",
                "items": {
                        "type": "string"
                    }
            },
            "MainClass": {
                "type": "string",
                "description":
                    "The name of the main class in the specified "
                    "Java file. If not specified, the JAR file should "
                    "specify a Main-Class in its manifest file."
            },
            "Properties": {
                "type": "array",
                "description":
                    "A list of Java properties that are set when the step "
                    "runs. You can use these properties to pass key value "
                    "pairs to your main function.",
                "items": {
                    "type": "object",
                    "properties": {
                        "Key":{
                            "type": "string",
                            "description":
                                "The unique identifier of a key value pair."
                        },
                        "Value": {
                            "type": "string",
                            "description":
                                "The value part of the identified key."
                        }
                    }
                }
            }
        }
    }
}

EXPECTED_OUTPUT = {
    "type": "list",
    "members": {
        "type": "structure",
        "members": {
            "Name": {
                "type": "string",
                "description": "The name of the step. ",
            },
            "Jar": {
                "type": "string",
                "description": "A path to a JAR file run during the step.",
            },
            "Args": {
                "type": "list",
                "description":
                    "A list of command line arguments to pass to the step.",
                "members": {
                    "type": "string"
                }
            },
            "MainClass": {
                "type": "string",
                "description":
                    "The name of the main class in the specified "
                    "Java file. If not specified, the JAR file should "
                    "specify a Main-Class in its manifest file."
            },
            "Properties": {
                "type": "list",
                "description":
                    "A list of Java properties that are set when the step "
                    "runs. You can use these properties to pass key value "
                    "pairs to your main function.",
                "members": {
                    "type": "structure",
                    "members": {
                        "Key":{
                            "type": "string",
                            "description":
                                "The unique identifier of a key value pair."
                        },
                        "Value": {
                            "type": "string",
                            "description":
                                "The value part of the identified key."
                        }
                    }
                }
            }
        }
    }
}

MISSING_TYPE = {
    "type": "object",
    "properties": {
        "Foo": {
            "description": "I am a foo"
        }
    }
}

class TestSchemaTransformer(unittest.TestCase):
    def test_schema(self):
        transformer = SchemaTransformer(INPUT_SCHEMA)
        output = transformer.transform()

        self.assertEqual(output, EXPECTED_OUTPUT)

    def test_missing_type(self):
        transformer = SchemaTransformer(MISSING_TYPE)

        with self.assertRaises(ParameterRequiredError):
            transformer.transform()

########NEW FILE########
__FILENAME__ = test_table
# Copyright (c) 2013 Amazon.com, Inc. or its affiliates.  All Rights Reserved
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the
# "Software"), to deal in the Software without restriction, including
# without limitation the rights to use, copy, modify, merge, publish, dis-
# tribute, sublicense, and/or sell copies of the Software, and to permit
# persons to whom the Software is furnished to do so, subject to the fol-
# lowing conditions:
#
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-
# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT
# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
# IN THE SOFTWARE.
#
import unittest

from awscli.table import Section, MultiTable, convert_to_vertical_table


class TestSection(unittest.TestCase):
    def setUp(self):
        self.section = Section()

    def test_add_row_tracks_max_widths(self):
        self.section.add_row(['one', 'two', 'three'])
        self.assertEqual(self.section.calculate_column_widths(), [3, 3, 5])
        self.section.add_row(['1234567', '1234567', '1234567'])
        self.assertEqual(self.section.calculate_column_widths(), [7, 7, 7])
        self.section.add_row(['a', 'a', 'a'])
        self.assertEqual(self.section.calculate_column_widths(), [7, 7, 7])
        self.section.add_row(['123456789', '1', '1'])
        self.assertEqual(self.section.calculate_column_widths(), [9, 7, 7])
        self.assertEqual(self.section.calculate_column_widths(1), [10, 8, 8])
        self.assertEqual(self.section.total_width(), 23)

    def test_add_row_also_tracks_header(self):
        self.section.add_header(['123456789', '12345', '1234567'])
        self.section.add_row(['a', 'a', 'a'])
        self.section.add_row(['aa', 'aa', 'aa'])
        self.section.add_row(['aaa', 'aaa', 'aaa'])
        self.assertEqual(self.section.calculate_column_widths(), [9, 5, 7])
        self.assertEqual(self.section.total_width(), 21)
        self.section.add_row(['aaa', '123456789', 'aaa'])
        self.assertEqual(self.section.calculate_column_widths(), [9, 9, 7])
        self.assertEqual(self.section.total_width(padding=3, with_border=True),
                         36)

    def test_max_width_with_scaling_perfect_scaling(self):
        self.section.add_row(['one', 'two', 'three'])
        self.section.add_row(['1234567', '1234567', '1234567'])
        # Perfect scaling, exactly double.
        widths = self.section.calculate_column_widths(max_width=42)
        self.assertEqual(widths, [14, 14, 14])

    def test_max_width_scaling_one_unit_short(self):
        self.section.add_row(['one', 'two', 'three'])
        self.section.add_row(['1234567', '1234567', '1234567'])
        widths = self.section.calculate_column_widths(max_width=41)
        self.assertEqual(widths, [13, 14, 14])

    def test_max_width_scaling_is_negative(self):
        self.section.add_row(['12345', '12345'])
        widths = self.section.calculate_column_widths(max_width=17)
        self.assertEqual(widths, [8, 9])

    def test_allow_sections_to_be_padded(self):
        self.section.add_row(['one', 'two', 'three'])
        self.section.add_row(['1234567', '1234567', '1234567'])
        self.assertEqual(self.section.total_width(padding=2), 27)
        self.assertEqual(
            self.section.total_width(padding=2, outer_padding=1), 29)

    def test_title_accounts_for_outer_padding(self):
        self.section.add_row(['a', 'b', 'c'])
        self.section.add_title('123456789')
        self.assertEqual(
            self.section.total_width(padding=2, outer_padding=3), 17)

    def test_unicode_text_row(self):
        self.section.add_row([1])
        self.section.add_row(['check'])
        self.section.add_row([u'\u2713'])
        self.assertEqual(
            self.section.rows,
            [[u'1'], [u'check'], [u'\u2713']])


class TestMultiTable(unittest.TestCase):
    def setUp(self):
        self.table = MultiTable()

    def test_max_width_calculation(self):
        self.table.add_title('foo')
        self.table.add_row_header(['one', 'two', 'three'])
        self.table.add_row(['one', 'two', 'three'])
        self.table.new_section('bar')
        self.table.add_row_header(['one', 'two'])
        self.table.add_row(['12345', '1234567'])


class TestVerticalTableConversion(unittest.TestCase):
    def setUp(self):
        self.table = MultiTable()

    def test_convert_section_to_vertical(self):
        self.table.add_title('foo')
        self.table.add_row_header(['key1', 'key2', 'key3'])
        self.table.add_row(['val1', 'val2', 'val3'])
        convert_to_vertical_table(self.table._sections)
        # To convert to a vertical table, there should be no headers:
        section = self.table._sections[0]
        self.assertEqual(section.headers, [])
        # Then we should create a two column row with key val pairs.
        self.assertEqual(
            section.rows,
            [['key1', 'val1'], ['key2', 'val2'], ['key3', 'val3']])


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_text
# Copyright (c) 2013 Amazon.com, Inc. or its affiliates.  All Rights Reserved
#
# Permission is hereby granted, free of charge, to any person obtaining a
# copy of this software and associated documentation files (the
# "Software"), to deal in the Software without restriction, including
# without limitation the rights to use, copy, modify, merge, publish, dis-
# tribute, sublicense, and/or sell copies of the Software, and to permit
# persons to whom the Software is furnished to do so, subject to the fol-
# lowing conditions:
#
# The above copyright notice and this permission notice shall be included
# in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL-
# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT
# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
# IN THE SOFTWARE.
#
import unittest
import sys

import six
import mock

from awscli import text


class TestSection(unittest.TestCase):
    def format_text(self, data, stream=None):
        if stream is None:
            stream = six.StringIO()
        text.format_text(data, stream=stream)
        return stream.getvalue()

    def assert_text_renders_to(self, data, expected_rendering):
        rendered = self.format_text(data)
        self.assertEqual(rendered, expected_rendering)

    def test_dict_format(self):
        self.assert_text_renders_to(dict(a=1, b=2, c=3), "1\t2\t3\n")

    def test_list_format(self):
        self.assert_text_renders_to([1, 2, 3], "1\t2\t3\n")

    def test_list_of_dicts(self):
        self.assert_text_renders_to(
            {'foo': [dict(a=1, b=2, c=3), dict(a=4, b=5, c=6)]},
            'FOO\t1\t2\t3\n'
            'FOO\t4\t5\t6\n')

    def test_multiple_list_of_dicts(self):
        self.assert_text_renders_to(
            {'foo': [dict(a=1, b=2, c=3), dict(a=4, b=5, c=6)],
             'zoo': [dict(a=7, b=8, c=9), dict(a=0, b=1, c=2)]},
            'FOO\t1\t2\t3\n'
            'FOO\t4\t5\t6\n'
            'ZOO\t7\t8\t9\n'
            'ZOO\t0\t1\t2\n'
        )

    def test_different_keys_in_sublists(self):
        self.assert_text_renders_to(
            #                                missing "b"        adds "d"
            {'foo': [dict(a=1, b=2, c=3), dict(a=4, c=5), dict(a=6, d=7)]},
            'FOO\t1\t2\t3\t\n'
            'FOO\t4\t\t5\t\n'
            'FOO\t6\t\t\t7\n'
        )

    def test_different_keys_in_nested_sublists(self):
        self.assert_text_renders_to({'bar':[
            {'foo': [dict(a=1, b=2, c=3), dict(a=4, c=5)]},
            {'foo': [dict(b=6, d=7), dict(b=8, c=9)]},
        ]},
            'FOO\t1\t2\t3\n'
            'FOO\t4\t\t5\n'
            'FOO\t6\t\t7\n'
            'FOO\t8\t9\t\n'
        )

    def test_different_keys_in_deeply_nested_sublists(self):
        self.assert_text_renders_to({'bar':[
            {'foo': [[[dict(a=1, b=2, c=3), dict(a=4, c=5)]]]},
            {'foo': [[[dict(b=6, d=7), dict(b=8, c=9)]]]},
        ]},
            'FOO\t1\t2\t3\n'
            'FOO\t4\t\t5\n'
            'FOO\t6\t\t7\n'
            'FOO\t8\t9\t\n'
        )

    def test_scalars_and_complex_types(self):
        self.assert_text_renders_to(
            {'foo': [dict(a=1, b=dict(y='y', z='z'), c=3),
                     dict(a=4, b=dict(y='y', z='z'), c=6)]},
            'FOO\t1\t3\n'
            'B\ty\tz\n'
            'FOO\t4\t6\n'
            'B\ty\tz\n')

    def test_nested_list_of_lists(self):
        self.assert_text_renders_to(
            [['1', '2', '3'], ['4', '5', '6']],
            '1\t2\t3\n'
            '4\t5\t6\n'
        )

    def test_deeply_nested_lists(self):
        self.assert_text_renders_to(
            [
                [['1', '2', '3'], ['4', '5', '6']],
                [['7', '8', '9'], ['0', '1', '2']],
            ],
            '1\t2\t3\n'
            '4\t5\t6\n'
            '7\t8\t9\n'
            '0\t1\t2\n'
        )

    def test_unicode_text(self):
        self.assert_text_renders_to([['1', '2', u'\u2713']],
                                     u'1\t2\t\u2713\n')

    def test_single_scalar_value(self):
        self.assert_text_renders_to('foobarbaz', 'foobarbaz\n')

    def test_empty_list(self):
        self.assert_text_renders_to([], '')

    def test_empty_inner_list(self):
        self.assert_text_renders_to([[]], '')

    def test_deeploy_nested_empty_list(self):
        self.assert_text_renders_to([[[[]]]], '')

    def test_deeploy_nested_single_scalar(self):
        self.assert_text_renders_to([[[['a']]]], 'a\n')

    def test_empty_list_mock_calls(self):
        # We also need this test as well as test_empty_list
        # because we want to ensure that write() is never called with
        # a list object.
        fake_stream = mock.Mock()
        self.format_text(data=[], stream=fake_stream)
        # We should not call .write() at all for an empty list.
        self.assertFalse(fake_stream.write.called)

    def test_list_of_strings_in_dict(self):
        self.assert_text_renders_to(
            {'KeyName': ['a', 'b', 'c']},
             'KEYNAME\ta\n'
             'KEYNAME\tb\n'
             'KEYNAME\tc\n')

    def test_inconsistent_sublists(self):
        self.assert_text_renders_to(
            [
                [['1', '2'], ['3', '4', '5', '6']],
                [['7', '8', '9'], ['0']]
            ],
            '1\t2\n'
            '3\t4\t5\t6\n'
            '7\t8\t9\n'
            '0\n'
        )

    def test_lists_mixed_with_scalars(self):
        self.assert_text_renders_to(
            [
                ['a', 'b', ['c', 'd']],
                ['e', 'f', ['g', 'h']]
            ],
            'a\tb\n'
            'c\td\n'
            'e\tf\n'
            'g\th\n'
        )

    def test_deeply_nested_with_scalars(self):
        self.assert_text_renders_to(
            [
                ['a', 'b', ['c', 'd', ['e', 'f', ['g', 'h']]]],
                ['i', 'j', ['k', 'l', ['m', 'n', ['o', 'p']]]],
            ],
            'a\tb\n'
            'c\td\n'
            'e\tf\n'
            'g\th\n'
            'i\tj\n'
            'k\tl\n'
            'm\tn\n'
            'o\tp\n'
        )

    def test_deeply_nested_with_identifier(self):
        self.assert_text_renders_to(
            {'foo': [
                ['a', 'b', ['c', 'd']],
                ['e', 'f', ['g', 'h']]
            ]},
            'FOO\ta\n'
            'FOO\tb\n'
            'FOO\tc\n'
            'FOO\td\n'
            'FOO\te\n'
            'FOO\tf\n'
            'FOO\tg\n'
            'FOO\th\n'
        )


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_utils
# Copyright 2013 Amazon.com, Inc. or its affiliates. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License"). You
# may not use this file except in compliance with the License. A copy of
# the License is located at
#
#     http://aws.amazon.com/apache2.0/
#
# or in the "license" file accompanying this file. This file is
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF
# ANY KIND, either express or implied. See the License for the specific
# language governing permissions and limitations under the License.
from awscli.testutils import unittest

from awscli.utils import split_on_commas


class TestCSVSplit(unittest.TestCase):

    def test_normal_csv_split(self):
        self.assertEqual(split_on_commas('foo,bar,baz'),
                         ['foo', 'bar', 'baz'])

    def test_quote_split(self):
        self.assertEqual(split_on_commas('foo,"bar",baz'),
                         ['foo', 'bar', 'baz'])

    def test_inner_quote_split(self):
        self.assertEqual(split_on_commas('foo,bar="1,2,3",baz'),
                         ['foo', 'bar=1,2,3', 'baz'])

    def test_single_quote(self):
        self.assertEqual(split_on_commas("foo,bar='1,2,3',baz"),
                         ['foo', 'bar=1,2,3', 'baz'])

    def test_mixing_double_single_quotes(self):
        self.assertEqual(split_on_commas("""foo,bar="1,'2',3",baz"""),
                         ['foo', "bar=1,'2',3", 'baz'])

    def test_mixing_double_single_quotes_before_first_comma(self):
        self.assertEqual(split_on_commas("""foo,bar="1','2',3",baz"""),
                         ['foo', "bar=1','2',3", 'baz'])

    def test_inner_quote_split_with_equals(self):
        self.assertEqual(split_on_commas('foo,bar="Foo:80/bar?a=b",baz'),
                         ['foo', 'bar=Foo:80/bar?a=b', 'baz'])

    def test_single_quoted_inner_value_with_no_commas(self):
        self.assertEqual(split_on_commas("foo,bar='BAR',baz"),
                         ['foo', 'bar=BAR', 'baz'])

    def test_escape_quotes(self):
        self.assertEqual(split_on_commas('foo,bar=1\,2\,3,baz'),
                         ['foo', 'bar=1,2,3', 'baz'])

    def test_no_commas(self):
        self.assertEqual(split_on_commas('foo'), ['foo'])

    def test_trailing_commas(self):
        self.assertEqual(split_on_commas('foo,'), ['foo', ''])

    def test_escape_backslash(self):
        self.assertEqual(split_on_commas('foo,bar\\\\,baz\\\\,qux'),
                         ['foo', 'bar\\', 'baz\\', 'qux'])

    def test_square_brackets(self):
        self.assertEqual(split_on_commas('foo,bar=["a=b",\'2\',c=d],baz'),
                         ['foo', 'bar=a=b,2,c=d', 'baz'])

    def test_quoted_square_brackets(self):
        self.assertEqual(split_on_commas('foo,bar="[blah]",c=d],baz'),
                         ['foo', 'bar=[blah]', 'c=d]', 'baz'])

    def test_missing_bracket(self):
        self.assertEqual(split_on_commas('foo,bar=[a,baz'),
                         ['foo', 'bar=[a', 'baz'])

    def test_missing_bracket2(self):
        self.assertEqual(split_on_commas('foo,bar=a],baz'),
                         ['foo', 'bar=a]', 'baz'])

    def test_bracket_in_middle(self):
        self.assertEqual(split_on_commas('foo,bar=a[b][c],baz'),
                         ['foo', 'bar=a[b][c]', 'baz'])

########NEW FILE########
