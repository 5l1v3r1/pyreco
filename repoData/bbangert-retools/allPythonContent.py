__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# retools documentation build configuration file, created by
# sphinx-quickstart on Sun Jul 10 12:01:31 2011.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.doctest', 'sphinx.ext.viewcode']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'retools'
copyright = u'2011-2012, Ben Bangert'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.3'
# The full version, including alpha/beta/rc tags.
release = '0.3'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'retoolsdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'retools.tex', u'retools Documentation',
   u'Ben Bangert', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'retools', u'retools Documentation',
     [u'Ben Bangert'], 1)
]


# -- Options for Epub output ---------------------------------------------------

# Bibliographic Dublin Core info.
epub_title = u'retools'
epub_author = u'Ben Bangert'
epub_publisher = u'Ben Bangert'
epub_copyright = u'2011-2012, Ben Bangert'

# The language of the text. It defaults to the language option
# or en if the language is not set.
#epub_language = ''

# The scheme of the identifier. Typical schemes are ISBN or URL.
#epub_scheme = ''

# The unique identifier of the text. This can be a ISBN number
# or the project homepage.
#epub_identifier = ''

# A unique identification for the text.
#epub_uid = ''

# HTML files that should be inserted before the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_pre_files = []

# HTML files shat should be inserted after the pages created by sphinx.
# The format is a list of tuples containing the path and title.
#epub_post_files = []

# A list of files that should not be packed into the epub file.
#epub_exclude_files = []

# The depth of the table of contents in toc.ncx.
#epub_tocdepth = 3

# Allow duplicate toc entries.
#epub_tocdup = True

########NEW FILE########
__FILENAME__ = cache
"""Caching

Cache regions are used to simplify common expirations and group function
caches.

To indicate functions should use cache regions, apply the decorator::

    from retools.cache import cache_region

    @cache_region('short_term')
    def myfunction(arg1):
        return arg1

To configure the cache regions, setup the :class:`~retools.cache.CacheRegion`
object::

    from retools.cache import CacheRegion

    CacheRegion.add_region("short_term", expires=60)

"""
import cPickle
import time
from datetime import date

from retools import global_connection
from retools.exc import CacheConfigurationError
from retools.lock import Lock
from retools.lock import LockTimeout
from retools.util import func_namespace
from retools.util import has_self_arg

from functools import wraps

class _NoneMarker(object):
    pass
NoneMarker = _NoneMarker()


class CacheKey(object):
    """Cache Key object

    Generator of cache keys for a variety of purposes once
    provided with a region, namespace, and key (args).

    """
    def __init__(self, region, namespace, key, today=None):
        """Setup a CacheKey object

        The CacheKey object creates the key-names used to store and
        retrieve values from Redis.

        :param region: Name of the region
        :type region: string
        :param namespace: Namespace to use
        :type namespace: string
        :param key: Key of the cached data, to differentiate various
                    arguments to the same callable

        """
        if not today:
            today = str(date.today())
        self.lock_key = 'retools:lock:%s:%s:%s' % (region, namespace, key)
        self.redis_key = 'retools:%s:%s:%s' % (region, namespace, key)
        self.redis_hit_key = 'retools:hits:%s:%s:%s:%s' % (
            today, region, namespace, key)
        self.redis_miss_key = 'retools:misses:%s:%s:%s:%s' % (
            today, region, namespace, key)
        self.redis_keyset = 'retools:%s:%s:keys' % (region, namespace)


class CacheRegion(object):
    """CacheRegion manager and configuration object

    For organization sake, the CacheRegion object is used to configure
    the available cache regions, query regions for currently cached
    keys, and set batches of keys by region for immediate expiration.

    Caching can be turned off globally by setting enabled to False::

        CacheRegion.enabled = False

    Statistics should also be turned on or off globally::

        CacheRegion.statistics = False

    However, if only some namespaces should have statistics recorded,
    then this should be used directly.

    """
    regions = {}
    enabled = True
    statistics = True

    @classmethod
    def add_region(cls, name, expires, redis_expiration=60 * 60 * 24 * 7):
        """Add a cache region to the current configuration

        :param name: The name of the cache region
        :type name: string
        :param expires: The expiration in seconds.
        :type expires: integer
        :param redis_expiration: How long the Redis key expiration is
                                 set for. Defaults to 1 week.
        :type redis_expiration: integer

        """
        cls.regions[name] = dict(expires=expires,
                                 redis_expiration=redis_expiration)

    @classmethod
    def _add_tracking(cls, pipeline, region, namespace, key):
        """Add's basic set members for tracking

        This is added to a Redis pipeline for a single round-trip to
        Redis.

        """
        pipeline.sadd('retools:regions', region)
        pipeline.sadd('retools:%s:namespaces' % region, namespace)
        pipeline.sadd('retools:%s:%s:keys' % (region, namespace), key)

    @classmethod
    def invalidate(cls, region):
        """Invalidate an entire region

        .. note::

            This does not actually *clear* the region of data, but
            just sets the value to expire on next access.

        :param region: Region name
        :type region: string

        """
        redis = global_connection.redis
        namespaces = redis.smembers('retools:%s:namespaces' % region)
        if not namespaces:
            return None

        # Locate the longest expiration of a region, so we can set
        # the created value far enough back to force a refresh
        longest_expire = max(
              [x['expires'] for x in CacheRegion.regions.values()])
        new_created = time.time() - longest_expire - 3600

        for ns in namespaces:
            cache_keyset_key = 'retools:%s:%s:keys' % (region, ns)
            keys = set(['']) | redis.smembers(cache_keyset_key)
            for key in keys:
                cache_key = 'retools:%s:%s:%s' % (region, ns, key)
                if not redis.exists(cache_key):
                    redis.srem(cache_keyset_key, key)
                else:
                    redis.hset(cache_key, 'created', new_created)

    @classmethod
    def load(cls, region, namespace, key, regenerate=True, callable=None,
             statistics=None):
        """Load a value from Redis, and possibly recreate it

        This method is used to load a value from Redis, and usually
        regenerates the value using the callable when provided.

        If ``regenerate`` is ``False`` and a ``callable`` is not passed
        in, then :obj:`~retools.cache.NoneMarker` will be returned.

        :param region: Region name
        :type region: string
        :param namespace: Namespace for the value
        :type namespace: string
        :param key: Key for this value under the namespace
        :type key: string
        :param regenerate: If False, then existing keys will always be
                           returned regardless of cache expiration. In the
                           event that there is no existing key and no
                           callable was provided, then a NoneMarker will
                           be returned.
        :type regenerate: bool
        :param callable: A callable to use when the cached value needs to be
                         created
        :param statistics: Whether or not hit/miss statistics should be
                           updated
        :type statistics: bool

        """
        if statistics is None:
            statistics = cls.statistics
        redis = global_connection.redis
        now = time.time()
        region_settings = cls.regions[region]
        expires = region_settings['expires']
        redis_expiration = region_settings['redis_expiration']

        keys = CacheKey(region=region, namespace=namespace, key=key)

        # Create a transaction to update our hit counter for today and
        # retrieve the current value.
        if statistics:
            p = redis.pipeline(transaction=True)
            p.hgetall(keys.redis_key)
            p.get(keys.redis_hit_key)
            p.incr(keys.redis_hit_key)
            results = p.execute()
            result, existing_hits = results[0], results[1]
            if existing_hits is None:
                existing_hits = 0
            else:
                existing_hits = int(existing_hits)
        else:
            result = redis.hgetall(keys.redis_key)

        expired = True
        if result and now - float(result['created']) < expires:
            expired = False

        if (result and not regenerate) or not expired:
            # We have a result and were told not to regenerate so
            # we always return it immediately regardless of expiration,
            # or its not expired
            return cPickle.loads(result['value'])

        if not result and not regenerate:
            # No existing value, but we were told not to regenerate it and
            # there's no callable, so we return a NoneMarker
            return NoneMarker

        # Don't wait for the lock if we have an old value
        if result and 'value' in result:
            timeout = 0
        else:
            timeout = 60 * 60

        try:
            with Lock(keys.lock_key, expires=expires, timeout=timeout):
                # Did someone else already create it?
                result = redis.hgetall(keys.redis_key)
                now = time.time()
                if result and 'value' in result and \
                   now - float(result['created']) < expires:
                    return cPickle.loads(result['value'])

                value = callable()

                p = redis.pipeline(transaction=True)
                p.hmset(keys.redis_key, {'created': now,
                                    'value': cPickle.dumps(value)})
                p.expire(keys.redis_key, redis_expiration)
                cls._add_tracking(p, region, namespace, key)
                if statistics:
                    p.getset(keys.redis_hit_key, 0)
                    new_hits = int(p.execute()[0])
                else:
                    p.execute()
        except LockTimeout:
            if result:
                return cPickle.loads(result['value'])
            else:
                # log some sort of error?
                return NoneMarker

        # Nothing else to do if not recording stats
        if not statistics:
            return value

        misses = new_hits - existing_hits
        if misses:
            p = redis.pipeline(transaction=True)
            p.incr(keys.redis_hit_key, amount=existing_hits)
            p.incr(keys.redis_miss_key, amount=misses)
            p.execute()
        else:
            redis.incr(keys.redis_hit_key, amount=existing_hits)
        return value


def invalidate_region(region):
    """Invalidate all the namespace's in a given region

    .. note::

        This does not actually *clear* the region of data, but
        just sets the value to expire on next access.

    :param region: Region name
    :type region: string

    """
    CacheRegion.invalidate(region)


def invalidate_callable(callable, *args):
    """Invalidate the cache for a callable

    :param callable: The callable that was cached
    :type callable: callable object
    :param \*args: Arguments the function was called with that
                   should be invalidated. If the args is just the
                   differentiator for the function, or not present, then all
                   values for the function will be invalidated.

    Example::

        @cache_region('short_term', 'small_engine')
        def local_search(search_term):
            # do search and return it

        @cache_region('long_term')
        def lookup_folks():
            # look them up and return them

        # To clear local_search for search_term = 'fred'
        invalidate_function(local_search, 'fred')

        # To clear all cached variations of the local_search function
        invalidate_function(local_search)

        # To clear out lookup_folks
        invalidate_function(lookup_folks)

    """
    redis = global_connection.redis
    region = callable._region
    namespace = callable._namespace

    # Get the expiration for this region
    new_created = time.time() - CacheRegion.regions[region]['expires'] - 3600

    if args:
        try:
            cache_key = " ".join(map(str, args))
        except UnicodeEncodeError:
            cache_key = " ".join(map(unicode, args))
        redis.hset('retools:%s:%s:%s' % (region, namespace, cache_key),
                   'created', new_created)
    else:
        cache_keyset_key = 'retools:%s:%s:keys' % (region, namespace)
        keys = set(['']) | redis.smembers(cache_keyset_key)
        p = redis.pipeline(transaction=True)
        for key in keys:
            p.hset('retools:%s:%s:%s' % (region, namespace, key), 'created',
                   new_created)
        p.execute()
    return None
invalidate_function = invalidate_callable


def cache_region(region, *deco_args, **kwargs):
    """Decorate a function such that its return result is cached,
    using a "region" to indicate the cache arguments.

    :param region: Name of the region to cache to
    :type region: string
    :param \*deco_args: Optional ``str()``-compatible arguments which will
      uniquely identify the key used by this decorated function, in addition
      to the positional arguments passed to the function itself at call time.
      This is recommended as it is needed to distinguish between any two
      functions or methods that have the same name (regardless of parent
      class or not).
    :type deco_args: list

    .. note::

        The function being decorated must only be called with
        positional arguments, and the arguments must support
        being stringified with ``str()``.  The concatenation
        of the ``str()`` version of each argument, combined
        with that of the ``*args`` sent to the decorator,
        forms the unique cache key.

    Example::

        from retools.cache import cache_region

        @cache_region('short_term', 'load_things')
        def load(search_term, limit, offset):
            '''Load from a database given a search term, limit, offset.'''
            return database.query(search_term)[offset:offset + limit]

    The decorator can also be used with object methods.  The ``self``
    argument is not part of the cache key.  This is based on the
    actual string name ``self`` being in the first argument
    position::

        class MyThing(object):
            @cache_region('short_term', 'load_things')
            def load(self, search_term, limit, offset):
                '''Load from a database given a search term, limit, offset.'''
                return database.query(search_term)[offset:offset + limit]

    Classmethods work as well - use ``cls`` as the name of the class argument,
    and place the decorator around the function underneath ``@classmethod``::

        class MyThing(object):
            @classmethod
            @cache_region('short_term', 'load_things')
            def load(cls, search_term, limit, offset):
                '''Load from a database given a search term, limit, offset.'''
                return database.query(search_term)[offset:offset + limit]

    .. note::

        When a method on a class is decorated, the ``self`` or ``cls``
        argument in the first position is
        not included in the "key" used for caching.

    """
    def decorate(func):
        namespace = func_namespace(func, deco_args)
        skip_self = has_self_arg(func)
        regenerate = kwargs.get('regenerate', True)

        @wraps(func)
        def cached(*args):
            if region not in CacheRegion.regions:
                raise CacheConfigurationError(
                    'Cache region not configured: %s' % region)
            if not CacheRegion.enabled:
                return func(*args)

            if skip_self:
                try:
                    cache_key = " ".join(map(str, args[1:]))
                except UnicodeEncodeError:
                    cache_key = " ".join(map(unicode, args[1:]))
            else:
                try:
                    cache_key = " ".join(map(str, args))
                except UnicodeEncodeError:
                    cache_key = " ".join(map(unicode, args))

            def go():
                return func(*args)
            return CacheRegion.load(region, namespace, cache_key,
                                    regenerate=regenerate, callable=go)
        cached._region = region
        cached._namespace = namespace
        return cached
    return decorate

########NEW FILE########
__FILENAME__ = event
"""Queue Events

Queue events allow for custimization of what occurs when a worker runs, and
adds extension points for job execution.

Job Events
==========

job_prerun
----------

Runs in the child process immediately before the job is performed.

If a :obj:`job_prerun` function raises :exc:`~retools.exc.AbortJob`then the
job will be aborted gracefully and the :obj:`job_failure` will not be called.

Signal handler will recieve the job function as the sender with the keyword
argument ``job``, which is a :class:`~retools.queue.Job` instance.


job_postrun
-----------

Runs in the child process after the job is performed.

These will be skipped if the job segfaults or raises an exception.

Signal handler will recieve the job function as the sender with the keyword
arguments ``job`` and ``result``, which is the :class:`~retools.queue.Job`
instance and the result of the function.


job_wrapper
-----------

Runs in the child process and wraps the job execution

Objects configured for this signal must be context managers, and can be
ensured they will have the opportunity to before and after the job. Commonly
used for locking and other events which require ensuring cleanup of a resource
after the job is called regardless of the outcome.

Signal handler will be called with the job function, the
:class:`~retools.queue.Job` instance, and the keyword arguments for the job.


job_failure
-----------

Runs in the child process when a job throws an exception

Signal handler will be called with the job function, the
:class:`~retools.queue.Job` instance, and the exception object. The signal
handler **should not raise an exception**.

"""

########NEW FILE########
__FILENAME__ = exc
"""retools exceptions"""


class RetoolsException(BaseException):
    """retools package base exception"""


class ConfigurationError(RetoolsException):
    """Raised for general configuration errors"""


class CacheConfigurationError(RetoolsException):
    """Raised when there's a cache configuration error"""


class QueueError(RetoolsException):
    """Raised when there's an error in the queue code"""


class AbortJob(RetoolsException):
    """Raised to abort execution of a job"""

########NEW FILE########
__FILENAME__ = jobs
import json

def simplemath(arg1=0, arg2=2):
    return arg1 + arg2


# Return the result after it runs
def return_result(job=None, result=None):
    pl = job.redis.pipeline()
    result = json.dumps({'data': result})
    pl.lpush('retools:result:%s' % job.job_id, result)
    pl.expire('retools:result:%s', 3600)
    pl.execute()

# If it fails, return a 'ERROR'
def return_failure(job=None, exc=None):
    pl = job.redis.pipeline()
    exc = json.dumps({'data': 'ERROR: %s' % exc})
    pl.lpush('retools:result:%s' % job.job_id, exc)
    pl.expire('retools:result:%s', 3600)
    pl.execute()


def add_events(qm):
    qm.subscriber('job_postrun', 'retools.jobs:simplemath',
                  handler='retools.jobs:return_result')
    qm.subscriber('job_failure', 'retools.jobs:simplemath',
                  handler='retools.jobs:return_failure')

def wait_for_result(qm, job, **kwargs):
    job_id = qm.enqueue(job, **kwargs)
    result = qm.redis.blpop('retools:result:%s' % job_id)[1]
    return json.loads(result)['data']

########NEW FILE########
__FILENAME__ = limiter
"""Generic Limiter to ensure N parallel operations

.. note::

    The limiter functionality is new.
    Please report any issues found on `the retools Github issue
    tracker <https://github.com/bbangert/retools/issues>`_.

The limiter is useful when you want to make sure that only N operations for a given process happen at the same time,
i.e.: concurrent requests to the same domain.

The limiter works by acquiring and releasing limits.

Creating a limiter::

    from retools.limiter import Limiter

    def do_something():
        limiter = Limiter(limit=10, prefix='my-operation')  # using default redis connection

        for i in range(100):
            if limiter.acquire_limit('operation-%d' % i):
                execute_my_operation()
                limiter.release_limit('operation-%d' % i)  # since we are releasing it synchronously
                                                           # all the 100 operations will be performed with
                                                           # one of them locked at a time

Specifying a default expiration in seconds::

    def do_something():
        limiter = Limiter(limit=10, expiration_in_seconds=45)  # using default redis connection

Specifying a redis connection::

    def do_something():
        limiter = Limiter(limit=10, redis=my_redis_connection)

Every time you try to acquire a limit, the expired limits you previously acquired get removed from the set.

This way if your process dies in the mid of its operation, the keys will eventually expire.
"""

import time

import redis

from retools import global_connection
from retools.util import flip_pairs


class Limiter(object):
    '''Configures and limits operations'''
    def __init__(self, limit, redis=None, prefix='retools_limiter', expiration_in_seconds=10):
        """Initializes a Limiter.

        :param limit: An integer that describes the limit on the number of items
        :param redis: A Redis instance. Defaults to the redis instance
                      on the global_connection.
        :param prefix: The default limit set name. Defaults to 'retools_limiter'.
        :param expiration_in_seconds: The number in seconds that keys should be locked if not
                            explicitly released.
        """

        self.limit = limit
        self.redis = redis or global_connection.redis
        self.prefix = prefix
        self.expiration_in_seconds = expiration_in_seconds

    def acquire_limit(self, key, expiration_in_seconds=None, retry=True):
        """Tries to acquire a limit for a given key. Returns True if the limit can be acquired.

        :param key: A string with the key to acquire the limit for.
                    This key should be used when releasing.
        :param expiration_in_seconds: The number in seconds that this key should be locked if not
                            explicitly released. If this is not passed, the default is used.
        :param key: Internal parameter that specifies if the operation should be retried.
                        Defaults to True.
        """

        limit_available = self.redis.zcard(self.prefix) < self.limit

        if limit_available:
            self.__lock_limit(key, expiration_in_seconds)
            return True

        if retry:
            self.redis.zremrangebyscore(self.prefix, '-inf', time.time())
            return self.acquire_limit(key, expiration_in_seconds, retry=False)

        return False

    def release_limit(self, key):
        """Releases a limit for a given key.

        :param key: A string with the key to release the limit on.
        """

        self.redis.zrem(self.prefix, key)

    def __lock_limit(self, key, expiration_in_seconds=None):
        expiration = expiration_in_seconds or self.expiration_in_seconds
        self.__zadd(self.prefix, key, time.time() + expiration)

    def __zadd(self, set_name, *args, **kwargs):
        """
        Custom ZADD interface that adapts to match the argument order of the currently
        used backend.  Using this method makes it transparent whether you use a Redis
        or a StrictRedis connection.

        Found this code at https://github.com/ui/rq-scheduler/pull/17.
        """
        conn = self.redis

        # If we're dealing with StrictRedis, flip each pair of imaginary
        # (name, score) tuples in the args list
        if conn.__class__ is redis.StrictRedis:  # StrictPipeline is a subclass of StrictRedis, too
            args = tuple(flip_pairs(args))

        return conn.zadd(set_name, *args)

########NEW FILE########
__FILENAME__ = lock
"""A Redis backed distributed global lock

This code uses the formula here:
https://github.com/jeffomatic/redis-exp-lock-js

It provides several improvements over the original version based on:
http://chris-lamb.co.uk/2010/06/07/distributing-locking-python-and-redis/

It provides a few improvements over the one present in the Python redis
library, for example since it utilizes the Lua functionality, it no longer
requires every client to have synchronized time.

"""
# Copyright 2010,2011 Chris Lamb <lamby@debian.org>

import time
import uuid

from retools import global_connection


acquire_lua = """
local result = redis.call('SETNX', KEYS[1], ARGV[1])
if result == 1 then
    redis.call('EXPIRE', KEYS[1], ARGV[2])
end
return result"""


release_lua = """
if redis.call('GET', KEYS[1]) == ARGV[1] then
    return redis.call('DEL', KEYS[1])
end
return 0
"""


class Lock(object):
    def __init__(self, key, expires=60, timeout=10, redis=None):
        """Distributed locking using Redis Lua scripting for CAS operations.

        Usage::

            with Lock('my_lock'):
                print "Critical section"

        :param  expires:    We consider any existing lock older than
                            ``expires`` seconds to be invalid in order to
                            detect crashed clients. This value must be higher
                            than it takes the critical section to execute.
        :param  timeout:    If another client has already obtained the lock,
                            sleep for a maximum of ``timeout`` seconds before
                            giving up. A value of 0 means we never wait.
        :param  redis:      The redis instance to use if the default global
                            redis connection is not desired.

        """
        self.key = key
        self.timeout = timeout
        self.expires = expires
        if not redis:
            redis = global_connection.redis
        self.redis = redis
        self._acquire_lua = redis.register_script(acquire_lua)
        self._release_lua = redis.register_script(release_lua)
        self.lock_key = None

    def __enter__(self):
        self.acquire()

    def __exit__(self, exc_type, exc_value, traceback):
        self.release()

    def acquire(self):
        """Acquire the lock

        :returns: Whether the lock was acquired or not
        :rtype: bool

        """
        self.lock_key = uuid.uuid4().hex
        timeout = self.timeout
        while timeout >= 0:
            if self._acquire_lua(keys=[self.key],
                                 args=[self.lock_key, self.expires]):
                return
            timeout -= 1
            if timeout >= 0:
                time.sleep(1)
        raise LockTimeout("Timeout while waiting for lock")

    def release(self):
        """Release the lock

        This only releases the lock if it matches the UUID we think it
        should have, to prevent deleting someone else's lock if we
        lagged.

        """
        if self.lock_key:
            self._release_lua(keys=[self.key], args=[self.lock_key])
        self.lock_key = None


class LockTimeout(BaseException):
    """Raised in the event a timeout occurs while waiting for a lock"""

########NEW FILE########
__FILENAME__ = queue
"""Queue worker and manager

.. note::

    The queueing functionality is new, and has gone through some preliminary
    testing. Please report any issues found on `the retools Github issue
    tracker <https://github.com/bbangert/retools/issues>`_.

Any function that takes keyword arguments can be a ``job`` that a worker runs.
The :class:`~retools.queue.QueueManager` handles configuration and enqueing jobs
to be run.

Declaring jobs::

    def default_job():
        # do some basic thing

    def important(somearg=None):
        # do an important thing


    def my_event_handler(sender, **kwargs):
        # do something

    def save_error(sender, **kwargs):
        # record error


Running Jobs::

    from retools.queue import QueueManager

    qm = QueueManager()
    qm.subscriber('job_failure', handler='mypackage.jobs:save_error')
    qm.subscriber('job_postrun', 'mypackage.jobs:important',
                  handler='mypackage.jobs:my_event_handler')
    qm.enqueue('mypackage.jobs:important', somearg='fred')


.. note::

    The events for a job are registered with the :class:`QueueManager` and are
    encoded in the job's JSON blob. Updating events for a job will therefore
    only take effect for new jobs queued, and not existing ones on the queue.

.. _queue_events:

Events
======

The retools queue has events available for additional functionality without
having to subclass or directly extend retools. These functions will be run by
the worker when the job is handled.

Available events to register for:

* **job_prerun**: Runs immediately before the job is run.
* **job_wrapper**: Wraps the execution of the job, these should be context
  managers.
* **job_postrun**: Runs after the job completes *successfully*, this will not
  be run if the job throws an exception.
* **job_failure**: Runs when a job throws an exception.

Event Function Signatures
-------------------------

Event functions have different call semantics, the following is a list of how
the event functions will be called:

* **job_prerun**: (job=job_instance)
* **job_wrapper**: (job_function, job_instance, **job_keyword_arguments)
* **job_postrun**: (job=job_instance, result=job_function_result)
* **job_failure**: (job=job_instance, exc=job_exception)

Attributes of interest on the job instance are documented in the
:meth:`Job.__init__` method.

.. _queue_worker:

Running the Worker
==================

After installing ``retools``, a ``retools-worker`` command will be available
that can spawn a worker. Queues to watch can be listed in order for priority
queueing, in which case the worker will try each queue in order looking for jobs
to process.

Example invokation:

.. code-block:: bash

    $ retools-worker high,medium,main

"""
import os
import signal
import socket
import subprocess
import sys
import time
import uuid
from datetime import datetime
from optparse import OptionParser

import pkg_resources

try:
    import json
except ImportError:  # pragma: nocover
    import simplejson as json

from setproctitle import setproctitle

from retools import global_connection
from retools.exc import ConfigurationError
from retools.util import with_nested_contexts


class QueueManager(object):
    """Configures and enqueues jobs"""
    def __init__(self, redis=None, default_queue_name='main',
                 serializer=json.dumps, deserializer=json.loads):
        """Initialize a QueueManager

        :param redis: A Redis instance. Defaults to the redis instance
                      on the global_connection.
        :param default_queue_name: The default queue name. Defaults to 'main'.
        :param serializer: A callable to serialize json data, defaults
                            to json.dumps().
        :param deserializer: A callable to deserialize json data, defaults
                              to json.loads().

        """
        self.default_queue_name = default_queue_name
        self.redis = redis or global_connection.redis
        self.names = {}  # cache name lookups
        self.job_config = {}
        self.job_events = {}
        self.global_events = {}
        self.serializer = serializer
        self.deserializer = deserializer

    def set_queue_for_job(self, job_name, queue_name):
        """Set the queue that a given job name will go to

        :param job_name: The pkg_resource name of the job function. I.e.
                         retools.jobs:my_function
        :param queue_name: Name of the queue on Redis job payloads should go
                           to

        """
        self.job_config[job_name] = queue_name

    def get_job(self, job_id, queue_name=None, full_job=True):
        if queue_name is None:
            queue_name = self.default_queue_name

        full_queue_name = 'retools:queue:' + queue_name
        current_len = self.redis.llen(full_queue_name)

        # that's O(n), we should do better
        for i in range(current_len):
            # the list can change while doing this
            # so we need to catch any index error
            job = self.redis.lindex(full_queue_name, i)
            job_data = self.deserializer(job)

            if job_data['job_id'] == job_id:
                if not full_job:
                    return job_data['job_id']

                return Job(full_queue_name, job, self.redis,
                           serializer=self.serializer,
                           deserializer=self.deserializer)

        raise IndexError(job_id)

    def get_jobs(self, queue_name=None, full_job=True):
        if queue_name is None:
            queue_name = self.default_queue_name

        full_queue_name = 'retools:queue:' + queue_name
        current_len = self.redis.llen(full_queue_name)

        for i in range(current_len):
            # the list can change while doing this
            # so we need to catch any index error
            job = self.redis.lindex(full_queue_name, i)
            if not full_job:
                job_dict = self.deserializer(job)
                yield job_dict['job_id']

            yield Job(full_queue_name, job, self.redis,
                      serializer=self.serializer,
                      deserializer=self.deserializer)

    def subscriber(self, event, job=None, handler=None):
        """Set events for a specific job or for all jobs

        :param event: The name of the event to subscribe to.
        :param job: Optional, a specific job to bind to.
        :param handler: The location of the handler to call.

        """
        if job:
            job_events = self.job_events.setdefault(job, {})
            job_events.setdefault(event, []).append(handler)
        else:
            self.global_events.setdefault(event, []).append(handler)

    def enqueue(self, job, **kwargs):
        """Enqueue a job

        :param job: The pkg_resouce name of the function. I.e.
                    retools.jobs:my_function
        :param kwargs: Keyword arguments the job should be called with.
                       These arguments must be serializeable by JSON.
        :returns: The job id that was queued.

        """
        if job not in self.names:
            job_func = pkg_resources.EntryPoint.parse('x=%s' % job).load(False)
            self.names[job] = job_func

        queue_name = kwargs.pop('queue_name', None)
        if not queue_name:
            queue_name = self.job_config.get('job', self.default_queue_name)

        metadata = kwargs.pop('metadata', None)
        if metadata is None:
            metadata = {}

        full_queue_name = 'retools:queue:' + queue_name
        job_id = uuid.uuid4().hex
        events = self.global_events.copy()
        if job in self.job_events:
            for k, v in self.job_events[job].items():
                events.setdefault(k, []).extend(v)

        job_dct = {
            'job_id': job_id,
            'job': job,
            'kwargs': kwargs,
            'events': events,
            'metadata': metadata,
            'state': {}
        }
        pipeline = self.redis.pipeline()
        pipeline.rpush(full_queue_name, self.serializer(job_dct))
        pipeline.sadd('retools:queues', queue_name)
        pipeline.execute()
        return job_id


class Job(object):
    def __init__(self, queue_name, job_payload, redis,
                 serializer=json.dumps, deserializer=json.loads):
        """Create a job instance given a JSON job payload

        :param job_payload: A JSON string representing a job.
        :param queue_name: The queue this job was pulled off of.
        :param redis: The redis instance used to pull this job.

        A :class:`Job` instance is created when the Worker pulls a
        job payload off the queue. The ``current_job`` global is set
        upon creation to indicate the current job being processed.

        Attributes of interest for event functions:

        * **job_id**: The Job's ID
        * **job_name**: The Job's name (it's package + function name)
        * **queue_name**: The queue this job came from
        * **kwargs**: The keyword arguments the job is called with
        * **state**: The state dict, this can be used by events to retain
          additional arguments. I.e. for a retry extension, retry information
          can be stored in the ``state`` dict.
        * **func**: A reference to the job function
        * **redis**: A :class:`redis.Redis` instance.
        * **serializer**: A callable to serialize json data, defaults
          to :func:`json.dumps`.
        * **deserializer**: A callable to deserialize json data, defaults
          to :func:`json.loads`.
        """
        global current_job
        current_job = self

        self.deserializer = deserializer
        self.serializer = serializer
        self.payload = payload = deserializer(job_payload)
        self.job_id = payload['job_id']
        self.job_name = payload['job']
        self.queue_name = queue_name
        self.kwargs = payload['kwargs']
        self.state = payload['state']
        self.metadata = payload.get('metadata', {})
        self.events = {}
        self.redis = redis
        self.func = None
        self.events = self.load_events(event_dict=payload['events'])

    def __repr__(self):
        """Display representation of self"""
        res = '<%s object at %s: ' % (self.__class__.__name__, hex(id(self)))
        res += 'Events: %s, ' % self.events
        res += 'State: %s, ' % self.state
        res += 'Job ID: %s, ' % self.job_id
        res += 'Job Name: %s, ' % self.job_name
        res += 'Queue: %s' % self.queue_name
        res += '>'
        return res

    @staticmethod
    def load_events(event_dict):
        """Load all the events given the references

        :param event_dict: A dictionary of events keyed by event name
                           to a list of handlers for the event.

        """
        events = {}
        for k, v in event_dict.items():
            funcs = []
            for name in v:
                mod_name, func_name = name.split(':')
                try:
                    mod = sys.modules[mod_name]
                except KeyError:
                    __import__(mod_name)
                    mod = sys.modules[mod_name]
                funcs.append(getattr(mod, func_name))
            events[k] = funcs
        return events

    def perform(self):
        """Runs the job calling all the job signals as appropriate"""
        self.run_event('job_prerun')
        try:
            if 'job_wrapper' in self.events:
                result = with_nested_contexts(self.events['job_wrapper'],
                                              self.func, [self], self.kwargs)
            else:
                result = self.func(**self.kwargs)
            self.run_event('job_postrun', result=result)
            return True
        except Exception, exc:
            self.run_event('job_failure', exc=exc)
            return False

    def to_dict(self):
        return {
            'job_id': self.job_id,
            'job': self.job_name,
            'kwargs': self.kwargs,
            'events': self.payload['events'],
            'state': self.state,
            'metadata': self.metadata}

    def to_json(self):
        return self.serializer(self.to_dict())

    def enqueue(self):
        """Queue this job in Redis"""
        full_queue_name = self.queue_name
        queue_name = full_queue_name.lstrip('retools:queue:')
        pipeline = self.redis.pipeline()
        pipeline.rpush(full_queue_name, self.to_json())
        pipeline.sadd('retools:queues', queue_name)
        pipeline.execute()
        return self.job_id

    def run_event(self, event, **kwargs):
        """Run all registered events for this job"""
        for event_func in self.events.get(event, []):
            event_func(job=self, **kwargs)


class Worker(object):
    """A Worker works on jobs"""
    def __init__(self, queues, redis=None):
        """Create a worker

        :param queues: List of queues to process
        :type queues: list
        :param redis: Redis instance to use, defaults to the global_connection.

        In the event that there is only a single queue in the list
        Redis list blocking will be used for lower latency job
        processing

        """
        self.redis = redis or global_connection.redis
        if not queues:
            raise ConfigurationError(
                  "No queues were configured for this worker")
        self.queues = ['retools:queue:%s' % x for x in queues]
        self.paused = self.shutdown = False
        self.job = None
        self.child_id = None
        self.jobs = {}  # job function import cache

    @classmethod
    def get_workers(cls, redis=None):
        redis = redis or global_connection.redis
        for worker_id in redis.smembers('retools:workers'):
            yield cls.from_id(worker_id)

    @classmethod
    def get_worker_ids(cls, redis=None):
        redis = redis or global_connection.redis
        return redis.smembers('retools:workers')

    @classmethod
    def from_id(cls, worker_id, redis=None):
        redis = redis or global_connection.redis
        if not redis.sismember("retools:workers", worker_id):
            raise IndexError(worker_id)
        queues = redis.get("retools:worker:%s:queues" % worker_id)
        queues = queues.split(',')
        return Worker(queues, redis)

    @property
    def worker_id(self):
        """Returns this workers id based on hostname, pid, queues"""
        return '%s:%s:%s' % (socket.gethostname(), os.getpid(),
              self.queue_names)

    @property
    def queue_names(self):
        names = [x.lstrip('retools:queue:') for x in self.queues]
        return ','.join(names)

    def work(self, interval=5, blocking=False):
        """Work on jobs

        This is the main method of the Worker, and will register itself
        with Redis as a Worker, wait for jobs, then process them.

        :param interval: Time in seconds between polling.
        :type interval: int
        :param blocking: Whether or not blocking pop should be used. If the
                         blocking pop is used, then the worker will block for
                         ``interval`` seconds at a time waiting for a new
                         job. This affects how often the worker can respond to
                         signals.
        :type blocking: bool

        """
        self.set_proc_title('Starting')
        self.startup()

        try:
            while 1:
                if self.shutdown:
                    break

                # Set this first since reserve may block for awhile
                self.set_proc_title("Waiting for %s" % self.queue_names)

                if not self.paused and self.reserve(interval, blocking):
                    self.working_on()
                    self.child_id = os.fork()
                    if self.child_id:
                        self.set_proc_title("Forked %s at %s" % (
                            self.child_id, datetime.now()))
                        try:
                            os.wait()
                        except OSError:
                            # got killed
                            pass
                    else:
                        self.set_proc_title("Processing %s since %s" % (
                            self.job.queue_name, datetime.now()))
                        self.perform()
                        sys.exit()
                    self.done_working()
                    self.child_id = None
                    self.job = None
                else:
                    if self.paused:
                        self.set_proc_title("Paused")
                    elif not blocking:
                        self.set_proc_title(
                              "Waiting for %s" % self.queue_names)
                        time.sleep(interval)
        finally:
            self.unregister_worker()

    def reserve(self, interval, blocking):
        """Attempts to pull a job off the queue(s)"""
        queue_name = None
        if blocking:
            result = self.redis.blpop(self.queues, timeout=interval)
            if result:
                queue_name, job_payload = result
        else:
            for queue in self.queues:
                job_payload = self.redis.lpop(queue)
                if job_payload:
                    queue_name = queue
                    break
        if not queue_name:
            return False

        self.job = job = Job(queue_name=queue_name, job_payload=job_payload,
                             redis=self.redis, serializer=self.serializer,
                             deserializer=self.deserializer)
        try:
            job.func = self.jobs[job.job_name]
        except KeyError:
            mod_name, func_name = job.job_name.split(':')
            __import__(mod_name)
            mod = sys.modules[mod_name]
            job.func = self.jobs[job.job_name] = getattr(mod, func_name)
        return True

    def set_proc_title(self, title):
        """Sets the active process title, retains the retools prefic"""
        setproctitle('retools: ' + title)

    def register_worker(self):
        """Register this worker with Redis"""
        pipeline = self.redis.pipeline()
        pipeline.sadd("retools:workers", self.worker_id)
        pipeline.set("retools:worker:%s:started" % self.worker_id, time.time())
        pipeline.set("retools:worker:%s:queues" % self.worker_id,
                     self.queue_names)
        pipeline.execute()

    def unregister_worker(self, worker_id=None):
        """Unregister this worker with Redis"""
        worker_id = worker_id or self.worker_id
        pipeline = self.redis.pipeline()
        pipeline.srem("retools:workers", worker_id)
        pipeline.delete("retools:worker:%s" % worker_id)
        pipeline.delete("retools:worker:%s:started" % worker_id)
        pipeline.delete("retools:worker:%s:queues" % worker_id)
        pipeline.execute()

    def startup(self):
        """Runs basic startup tasks"""
        self.register_signal_handlers()
        self.prune_dead_workers()
        self.register_worker()

    def trigger_shutdown(self, *args):
        """Graceful shutdown of the worker"""
        self.shutdown = True

    def immediate_shutdown(self, *args):
        """Immediately shutdown the worker, kill child process if needed"""
        self.shutdown = True
        self.kill_child()

    def kill_child(self, *args):
        """Kill the child process immediately"""
        if self.child_id:
            os.kill(self.child_id, signal.SIGTERM)

    def pause_processing(self, *args):
        """Cease pulling jobs off the queue for processing"""
        self.paused = True

    def resume_processing(self, *args):
        """Resume pulling jobs for processing off the queue"""
        self.paused = False

    def prune_dead_workers(self):
        """Prune dead workers from Redis"""
        all_workers = self.redis.smembers("retools:workers")
        known_workers = self.worker_pids()
        hostname = socket.gethostname()
        for worker in all_workers:
            host, pid, queues = worker.split(':')
            if host != hostname or pid in known_workers:
                continue
            self.unregister_worker(worker)

    def register_signal_handlers(self):
        """Setup all the signal handlers"""
        signal.signal(signal.SIGTERM, self.immediate_shutdown)
        signal.signal(signal.SIGINT, self.immediate_shutdown)
        signal.signal(signal.SIGQUIT, self.trigger_shutdown)
        signal.signal(signal.SIGUSR1, self.kill_child)
        signal.signal(signal.SIGUSR2, self.pause_processing)
        signal.signal(signal.SIGCONT, self.resume_processing)

    def working_on(self):
        """Indicate with Redis what we're working on"""
        data = {
            'queue': self.job.queue_name,
            'run_at': time.time(),
            'payload': self.job.payload
        }
        self.redis.set("retools:worker:%s" % self.worker_id,
                       self.serializer(data))

    def done_working(self):
        """Called when we're done working on a job"""
        self.redis.delete("retools:worker:%s" % self.worker_id)

    def worker_pids(self):
        """Returns a list of all the worker processes"""
        ps = subprocess.Popen("ps -U 0 -A | grep 'retools:'", shell=True,
                              stdout=subprocess.PIPE)
        data = ps.stdout.read()
        ps.stdout.close()
        ps.wait()
        return [x.split()[0] for x in data.split('\n') if x]

    def perform(self):
        """Run the job and call the appropriate signal handlers"""
        self.job.perform()


def run_worker():
    usage = "usage: %prog queues"
    parser = OptionParser(usage=usage)
    parser.add_option("--interval", dest="interval", type="int", default=5,
                      help="Polling interval")
    parser.add_option("-b", dest="blocking", action="store_true",
                      default=False,
                      help="Whether to use blocking queue semantics")
    (options, args) = parser.parse_args()

    if len(args) < 1:
        sys.exit("Error: Failed to provide queues or packages_to_scan args")

    worker = Worker(queues=args[0].split(','))
    worker.work(interval=options.interval, blocking=options.blocking)
    sys.exit()

########NEW FILE########
__FILENAME__ = jobs
def echo_default(default='hello'):  # pragma: nocover
    return default


def echo_back():  # pragma: nocover
    return 'howdy all'

########NEW FILE########
__FILENAME__ = test_cache
# coding: utf-8
import unittest
import time
import cPickle
from contextlib import nested

import redis
import redis.client

from nose.tools import raises
from nose.tools import eq_
from mock import Mock
from mock import patch


class TestCacheKey(unittest.TestCase):
    def _makeOne(self):
        from retools.cache import CacheKey
        return CacheKey

    def test_key_config(self):
        CK = self._makeOne()('home', 'my_func', '1 2 3', today='2004-02-02')
        eq_(CK.redis_hit_key, 'retools:hits:2004-02-02:home:my_func:1 2 3')


class TestCacheRegion(unittest.TestCase):
    def _makeOne(self):
        from retools.cache import CacheRegion
        CacheRegion.enabled = True
        return CacheRegion

    def _marker(self):
        from retools.cache import NoneMarker
        return NoneMarker

    def test_add_region(self):
        CR = self._makeOne()
        CR.add_region('short_term', 60)
        eq_(CR.regions['short_term']['expires'], 60)

    def test_generate_value(self):
        mock_redis = Mock(spec=redis.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        results = ['0', (None, '0')]

        def side_effect(*args, **kwargs):
            return results.pop()

        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.side_effect = side_effect
        mock_redis.hgetall.return_value = {}
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)

            def a_func():
                return "This is a value: %s" % time.time()
            value = CR.load('short_term', 'my_func', '1 2 3', callable=a_func)
            assert 'This is a value' in value
            exec_calls = [x for x in mock_pipeline.method_calls \
                  if x[0] == 'execute']
            eq_(len(mock_pipeline.method_calls), 11)
            eq_(len(exec_calls), 2)

    def test_existing_value_no_regen(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        results = ['0', ({'created': '111',
                          'value': "S'This is a value: 1311702429.28'\n."},
                   '0')]

        def side_effect(*args, **kwargs):
            return results.pop()

        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.side_effect = side_effect
        mock_redis.hgetall.return_value = {}
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)
            value = CR.load('short_term', 'my_func', '1 2 3', regenerate=False)
            assert 'This is a value' in value
            exec_calls = [x for x in mock_pipeline.method_calls \
                  if x[0] == 'execute']
            eq_(len(mock_pipeline.method_calls), 4)
            eq_(len(exec_calls), 1)

    def test_value_created_after_check_but_expired(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        results = ['0', (None, '0')]

        def side_effect(*args, **kwargs):
            return results.pop()

        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.side_effect = side_effect
        mock_redis.hgetall.return_value = {'created': '1',
              'value': "S'This is a value: 1311702429.28'\n."}
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)

            def a_func():
                return "This is a value: %s" % time.time()

            value = CR.load('short_term', 'my_func', '1 2 3', callable=a_func)
            assert 'This is a value' in value
            exec_calls = [x for x in mock_pipeline.method_calls \
                  if x[0] == 'execute']
            eq_(len(mock_pipeline.method_calls), 11)
            eq_(len(exec_calls), 2)

    def test_value_expired_and_no_lock(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        results = ['0', ({'created': '111',
                          'value': "S'This is a value: 1311702429.28'\n."},
                   '0')]

        def side_effect(*args, **kwargs):
            return results.pop()

        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.side_effect = side_effect
        mock_redis.hgetall.return_value = {}
        mock_redis.exists.return_value = False
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)

            def a_func():
                return "This is a value: %s" % time.time()

            value = CR.load('short_term', 'my_func', '1 2 3', callable=a_func)
            assert 'This is a value' in value
            exec_calls = [x for x in mock_pipeline.method_calls \
                  if x[0] == 'execute']
            eq_(len(mock_pipeline.method_calls), 11)
            eq_(len(exec_calls), 2)

    def test_generate_value_no_stats(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        results = ['0', (None, '0')]

        def side_effect(*args, **kwargs):
            return results.pop()

        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.side_effect = side_effect
        mock_redis.hgetall.return_value = {}
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)

            now = time.time()

            def a_func():
                return "This is a value: %s" % now
            value = CR.load('short_term', 'my_func', '1 2 3', callable=a_func,
                            statistics=False)
            assert 'This is a value' in value
            assert str(now) in value
            exec_calls = [x for x in mock_pipeline.method_calls \
                if x[0] == 'execute']
            eq_(len(mock_pipeline.method_calls), 6)
            eq_(len(exec_calls), 1)

    def test_generate_value_other_creator(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        now = time.time()
        results = ['0', (None, None)]

        def side_effect(*args, **kwargs):
            return results.pop()

        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.side_effect = side_effect
        mock_redis.hgetall.return_value = {'created': now,
              'value': cPickle.dumps("This is a NEW value")}
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)

            def a_func():  # pragma: nocover
                return "This is a value: %s" % time.time()
            value = CR.load('short_term', 'my_func', '1 2 3', callable=a_func)
            assert 'This is a NEW value' in value
            exec_calls = [x for x in mock_pipeline.method_calls \
                if x[0] == 'execute']
            eq_(len(exec_calls), 1)

    def test_existing_value(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        now = time.time()
        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.return_value = ({'created': now,
              'value': cPickle.dumps("This is a value")}, '0')
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)

            called = []

            def a_func():  # pragma: nocover
                called.append(1)
                return "This is a value: %s" % time.time()
            value = CR.load('short_term', 'my_func', '1 2 3', callable=a_func)
            assert 'This is a value' in value
            eq_(called, [])
            exec_calls = [x for x in mock_pipeline.method_calls \
                  if x[0] == 'execute']
            eq_(len(exec_calls), 1)

    def test_new_value_and_misses(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        results = [None, ['30'], (None, '0')]

        def side_effect(*args, **kwargs):
            return results.pop()

        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.side_effect = side_effect
        mock_redis.hgetall.return_value = {}
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)

            called = []

            def a_func():  # pragma: nocover
                called.append(1)
                return "This is a value: %s" % time.time()
            value = CR.load('short_term', 'my_func', '1 2 3', callable=a_func)
            assert 'This is a value' in value
            exec_calls = [x for x in mock_pipeline.method_calls \
                  if x[0] == 'execute']
            eq_(len(exec_calls), 3)

            # Check that we increment the miss counter by 30
            last_incr_call = filter(lambda x: x[0] == 'incr',
                                    mock_pipeline.method_calls)[-1]
            eq_(last_incr_call[2], {'amount': 30})

    def test_return_marker(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.return_value = (None, '0')
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)

            value = CR.load('short_term', 'my_func', '1 2 3', regenerate=False)
            eq_(value, self._marker())
            exec_calls = [x for x in mock_pipeline.method_calls \
                  if x[0] == 'execute']
            eq_(len(exec_calls), 1)


class TestInvalidateRegion(unittest.TestCase):
    def _makeOne(self):
        from retools.cache import invalidate_region
        return invalidate_region

    def _makeCR(self):
        from retools.cache import CacheRegion
        CacheRegion.regions = {}
        return CacheRegion

    def test_invalidate_region_empty(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_redis.smembers.return_value = set([])

        invalidate_region = self._makeOne()
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeCR()
            CR.add_region('short_term', expires=600)

            invalidate_region('short_term')
            eq_(len(mock_redis.method_calls), 1)

    def test_invalidate_small_region(self):
        mock_redis = Mock(spec=redis.client.Redis)
        results = [set(['keyspace']), set(['a_func'])]

        def side_effect(*args):
            return results.pop()

        mock_redis.smembers.side_effect = side_effect

        invalidate_region = self._makeOne()
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeCR()
            CR.add_region('short_term', expires=600)

            invalidate_region('short_term')
            calls = mock_redis.method_calls
            eq_(calls[0][1], ('retools:short_term:namespaces',))
            eq_(len(calls), 6)

    def test_remove_nonexistent_key(self):
        mock_redis = Mock(spec=redis.client.Redis)
        results = [set(['keyspace']), set(['a_func'])]

        def side_effect(*args):
            return results.pop()

        mock_redis.smembers.side_effect = side_effect
        mock_redis.exists.return_value = False

        invalidate_region = self._makeOne()
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeCR()
            CR.add_region('short_term', expires=600)

            invalidate_region('short_term')
            calls = mock_redis.method_calls
            eq_(calls[0][1], ('retools:short_term:namespaces',))
            eq_(len(calls), 6)


class TestInvalidFunction(unittest.TestCase):
    def _makeOne(self):
        from retools.cache import invalidate_function
        return invalidate_function

    def _makeCR(self):
        from retools.cache import CacheRegion
        CacheRegion.regions = {}
        return CacheRegion

    def test_invalidate_function_without_args(self):

        def my_func():  # pragma: nocover
            return "Hello"
        my_func._region = 'short_term'
        my_func._namespace = 'retools:a_key'

        mock_redis = Mock(spec=redis.client.Redis)
        mock_redis.smembers.return_value = set(['1'])

        mock_pipeline = Mock(spec=redis.client.Pipeline)
        mock_redis.pipeline.return_value = mock_pipeline

        invalidate_function = self._makeOne()
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeCR()
            CR.add_region('short_term', expires=600)

            invalidate_function(my_func)
            calls = mock_redis.method_calls
            eq_(calls[0][1], ('retools:short_term:retools:a_key:keys',))
            eq_(len(calls), 2)

    def test_invalidate_function_with_args(self):

        def my_func(name):  # pragma: nocover
            return "Hello %s" % name
        my_func._region = 'short_term'
        my_func._namespace = 'retools:a_key decarg'

        mock_redis = Mock(spec=redis.client.Redis)
        mock_redis.smembers.return_value = set(['1'])

        invalidate_function = self._makeOne()
        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeCR()
            CR.add_region('short_term', expires=600)

            invalidate_function(my_func, 'fred')
            calls = mock_redis.method_calls
            eq_(calls[0][1][0], 'retools:short_term:retools:a_key decarg:fred')
            eq_(calls[0][0], 'hset')
            eq_(len(calls), 1)

            # And a unicode key
            mock_redis.reset_mock()
            invalidate_function(my_func,
                  u"\u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac")
            calls = mock_redis.method_calls
            eq_(calls[0][1][0],
                  u'retools:short_term:retools:a_key' \
                  u' decarg:\u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac')
            eq_(calls[0][0], 'hset')
            eq_(len(calls), 1)


class TestCacheDecorator(unittest.TestCase):
    def _makeOne(self):
        from retools.cache import CacheRegion
        CacheRegion.enabled = True
        CacheRegion.regions = {}
        return CacheRegion

    def _decorateFunc(self, func, *args):
        from retools.cache import cache_region
        return cache_region(*args)(func)

    def test_no_region(self):
        from retools.exc import CacheConfigurationError

        @raises(CacheConfigurationError)
        def test_it():
            CR = self._makeOne()
            CR.add_region('short_term', 60)

            def dummy_func():  # pragma: nocover
                return "This is a value: %s" % time.time()
            decorated = self._decorateFunc(dummy_func, 'long_term')
            decorated()
        test_it()

    def test_generate(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        results = ['0', (None, '0')]

        def side_effect(*args, **kwargs):
            return results.pop()
        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.side_effect = side_effect
        mock_redis.hgetall.return_value = {}

        def dummy_func():
            return "This is a value: %s" % time.time()

        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)
            decorated = self._decorateFunc(dummy_func, 'short_term')
            value = decorated()
            assert 'This is a value' in value
            exec_calls = [x for x in mock_pipeline.method_calls \
                if x[0] == 'execute']
            eq_(len(exec_calls), 2)

    def test_cache_disabled(self):
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        mock_redis.pipeline.return_value = mock_pipeline

        def dummy_func():
            return "This is a value: %s" % time.time()

        with patch('retools.global_connection._redis', mock_redis):
            CR = self._makeOne()
            CR.add_region('short_term', 60)
            CR.enabled = False
            decorated = self._decorateFunc(dummy_func, 'short_term')
            value = decorated()
            assert 'This is a value' in value
            exec_calls = [x for x in mock_pipeline.method_calls \
                  if x[0] == 'execute']
            eq_(len(exec_calls), 0)

    def test_unicode_keys(self):
        keys = [
            # arabic (egyptian)
            u"\u0644\u064a\u0647\u0645\u0627\u0628\u062a\u0643\u0644\u0645" \
            u"\u0648\u0634\u0639\u0631\u0628\u064a\u061f",
            # Chinese (simplified)
            u"\u4ed6\u4eec\u4e3a\u4ec0\u4e48\u4e0d\u8bf4\u4e2d\u6587",
            # Chinese (traditional)
            u"\u4ed6\u5011\u7232\u4ec0\u9ebd\u4e0d\u8aaa\u4e2d\u6587",
            # czech
            u"\u0050\u0072\u006f\u010d\u0070\u0072\u006f\u0073\u0074\u011b" \
            u"\u006e\u0065\u006d\u006c\u0075\u0076\u00ed\u010d\u0065\u0073" \
            u"\u006b\u0079",
            # hebrew
            u"\u05dc\u05de\u05d4\u05d4\u05dd\u05e4\u05e9\u05d5\u05d8\u05dc" \
            u"\u05d0\u05de\u05d3\u05d1\u05e8\u05d9\u05dd\u05e2\u05d1\u05e8" \
            u"\u05d9\u05ea",
            # Hindi (Devanagari)
            u"\u092f\u0939\u0932\u094b\u0917\u0939\u093f\u0928\u094d\u0926" \
            u"\u0940\u0915\u094d\u092f\u094b\u0902\u0928\u0939\u0940\u0902" \
            u"\u092c\u094b\u0932\u0938\u0915\u0924\u0947\u0939\u0948\u0902",
            # Japanese (kanji and hiragana)
            u"\u306a\u305c\u307f\u3093\u306a\u65e5\u672c\u8a9e\u3092\u8a71" \
            u"\u3057\u3066\u304f\u308c\u306a\u3044\u306e\u304b",
            # Russian (Cyrillic)
            u"\u043f\u043e\u0447\u0435\u043c\u0443\u0436\u0435\u043e\u043d" \
            u"\u0438\u043d\u0435\u0433\u043e\u0432\u043e\u0440\u044f\u0442" \
            u"\u043f\u043e\u0440\u0443\u0441\u0441\u043a\u0438",
            # Spanish
            u"\u0050\u006f\u0072\u0071\u0075\u00e9\u006e\u006f\u0070\u0075" \
            u"\u0065\u0064\u0065\u006e\u0073\u0069\u006d\u0070\u006c\u0065" \
            u"\u006d\u0065\u006e\u0074\u0065\u0068\u0061\u0062\u006c\u0061" \
            u"\u0072\u0065\u006e\u0045\u0073\u0070\u0061\u00f1\u006f\u006c",
            # Vietnamese
            u"\u0054\u1ea1\u0069\u0073\u0061\u006f\u0068\u1ecd\u006b\u0068" \
            u"\u00f4\u006e\u0067\u0074\u0068\u1ec3\u0063\u0068\u1ec9\u006e" \
            u"\u00f3\u0069\u0074\u0069\u1ebf\u006e\u0067\u0056\u0069\u1ec7" \
            u"\u0074",
            # Japanese
            u"\u0033\u5e74\u0042\u7d44\u91d1\u516b\u5148\u751f",
            # Japanese
            u"\u5b89\u5ba4\u5948\u7f8e\u6075\u002d\u0077\u0069\u0074\u0068" \
            u"\u002d\u0053\u0055\u0050\u0045\u0052\u002d\u004d\u004f\u004e" \
            u"\u004b\u0045\u0059\u0053",
            # Japanese
            u"\u0048\u0065\u006c\u006c\u006f\u002d\u0041\u006e\u006f\u0074" \
            u"\u0068\u0065\u0072\u002d\u0057\u0061\u0079\u002d\u305d\u308c" \
            u"\u305e\u308c\u306e\u5834\u6240",
            # Japanese
            u"\u3072\u3068\u3064\u5c4b\u6839\u306e\u4e0b\u0032",
            # Japanese
            u"\u004d\u0061\u006a\u0069\u3067\u004b\u006f\u0069\u3059\u308b" \
            u"\u0035\u79d2\u524d",
            # Japanese
            u"\u30d1\u30d5\u30a3\u30fc\u0064\u0065\u30eb\u30f3\u30d0",
            # Japanese
            u"\u305d\u306e\u30b9\u30d4\u30fc\u30c9\u3067",
            # greek
            u"\u03b5\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac",
            # Maltese (Malti)
            u"\u0062\u006f\u006e\u0121\u0075\u0073\u0061\u0127\u0127\u0061",
            # Russian (Cyrillic)
            u"\u043f\u043e\u0447\u0435\u043c\u0443\u0436\u0435\u043e\u043d" \
            u"\u0438\u043d\u0435\u0433\u043e\u0432\u043e\u0440\u044f\u0442" \
            u"\u043f\u043e\u0440\u0443\u0441\u0441\u043a\u0438"
        ]
        mock_redis = Mock(spec=redis.client.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        results = ['0', (None, '0')]

        def side_effect(*args, **kwargs):
            return results.pop()
        mock_redis.pipeline.return_value = mock_pipeline
        mock_pipeline.execute.side_effect = side_effect
        mock_redis.hgetall.return_value = {}

        def dummy_func(arg):
            return "This is a value: %s" % time.time()

        for key in keys:
            with patch('retools.global_connection._redis', mock_redis):
                CR = self._makeOne()
                CR.add_region('short_term', 60)
                decorated = self._decorateFunc(dummy_func, 'short_term')
                value = decorated(key)
                assert 'This is a value' in value
                exec_calls = [x for x in mock_pipeline.method_calls \
                      if x[0] == 'execute']
                eq_(len(exec_calls), 2)
            mock_pipeline.reset_mock()
            results.extend(['0', (None, '0')])

        for key in keys:
            with patch('retools.global_connection._redis', mock_redis):
                CR = self._makeOne()
                CR.add_region('short_term', 60)

                class DummyClass(object):
                    def dummy_func(self, arg):
                        return "This is a value: %s" % time.time()
                    dummy_func = self._decorateFunc(dummy_func, 'short_term')
                cl_inst = DummyClass()
                value = cl_inst.dummy_func(key)
                assert 'This is a value' in value
                exec_calls = [x for x in mock_pipeline.method_calls \
                      if x[0] == 'execute']
                eq_(len(exec_calls), 2)
            mock_pipeline.reset_mock()
            results.extend(['0', (None, '0')])

########NEW FILE########
__FILENAME__ = test_limiter
# coding: utf-8

import unittest
import time

import redis
from nose.tools import eq_
from mock import Mock
from mock import patch

from retools.limiter import Limiter
from retools import global_connection


class TestLimiterWithMockRedis(unittest.TestCase):
    def test_can_create_limiter_without_prefix_and_without_connection(self):
        limiter = Limiter(limit=10)

        eq_(limiter.redis, global_connection.redis)
        eq_(limiter.limit, 10)
        eq_(limiter.prefix, 'retools_limiter')

    def test_can_create_limiter_without_prefix(self):
        mock_redis = Mock(spec=redis.Redis)

        limiter = Limiter(limit=10, redis=mock_redis)

        eq_(limiter.redis, mock_redis)
        eq_(limiter.prefix, 'retools_limiter')

    def test_can_create_limiter_with_prefix(self):
        mock_redis = Mock(spec=redis.Redis)

        limiter = Limiter(limit=10, redis=mock_redis, prefix='something')

        eq_(limiter.redis, mock_redis)
        eq_(limiter.prefix, 'something')

    def test_can_create_limiter_with_expiration(self):
        mock_redis = Mock(spec=redis.Redis)

        limiter = Limiter(limit=10, redis=mock_redis, expiration_in_seconds=20)

        eq_(limiter.expiration_in_seconds, 20)

    def test_has_limit(self):
        mock_time = Mock()
        mock_time.return_value = 40.5

        mock_redis = Mock(spec=redis.Redis)
        mock_redis.zcard.return_value = 0

        limiter = Limiter(limit=10, redis=mock_redis, expiration_in_seconds=20)

        with patch('time.time', mock_time):
            has_limit = limiter.acquire_limit(key='test1')

        eq_(has_limit, True)

        mock_redis.zadd.assert_called_once_with('retools_limiter', 'test1', 60.5)

    def test_acquire_limit_after_removing_items(self):
        mock_time = Mock()
        mock_time.return_value = 40.5

        mock_redis = Mock(spec=redis.Redis)
        mock_redis.zcard.side_effect = [10, 8]

        limiter = Limiter(limit=10, redis=mock_redis, expiration_in_seconds=20)

        with patch('time.time', mock_time):
            has_limit = limiter.acquire_limit(key='test1')

        eq_(has_limit, True)

        mock_redis.zadd.assert_called_once_with('retools_limiter', 'test1', 60.5)
        mock_redis.zremrangebyscore.assert_called_once_with('retools_limiter', '-inf', 40.5)

    def test_acquire_limit_fails_even_after_removing_items(self):
        mock_time = Mock()
        mock_time.return_value = 40.5

        mock_redis = Mock(spec=redis.Redis)
        mock_redis.zcard.side_effect = [10, 10]

        limiter = Limiter(limit=10, redis=mock_redis, expiration_in_seconds=20)

        with patch('time.time', mock_time):
            has_limit = limiter.acquire_limit(key='test1')

        eq_(has_limit, False)

        eq_(mock_redis.zadd.called, False)
        mock_redis.zremrangebyscore.assert_called_once_with('retools_limiter', '-inf', 40.5)

    def test_release_limit(self):
        mock_redis = Mock(spec=redis.Redis)

        limiter = Limiter(limit=10, redis=mock_redis, expiration_in_seconds=20)

        limiter.release_limit(key='test1')

        mock_redis.zrem.assert_called_once_with('retools_limiter', 'test1')


class TestLimiterWithActualRedis(unittest.TestCase):
    def test_has_limit(self):
        limiter = Limiter(prefix='test-%.6f' % time.time(), limit=2, expiration_in_seconds=400)

        has_limit = limiter.acquire_limit(key='test1')
        eq_(has_limit, True)

        has_limit = limiter.acquire_limit(key='test2')
        eq_(has_limit, True)

        has_limit = limiter.acquire_limit(key='test3')
        eq_(has_limit, False)

    def test_has_limit_after_removing_items(self):
        limiter = Limiter(prefix='test-%.6f' % time.time(), limit=2, expiration_in_seconds=400)

        has_limit = limiter.acquire_limit(key='test1')
        eq_(has_limit, True)

        has_limit = limiter.acquire_limit(key='test2', expiration_in_seconds=-1)
        eq_(has_limit, True)

        has_limit = limiter.acquire_limit(key='test3')
        eq_(has_limit, True)

    def test_has_limit_after_releasing_items(self):
        limiter = Limiter(prefix='test-%.6f' % time.time(), limit=2, expiration_in_seconds=400)

        has_limit = limiter.acquire_limit(key='test1')
        eq_(has_limit, True)

        has_limit = limiter.acquire_limit(key='test2')
        eq_(has_limit, True)

        limiter.release_limit(key='test2')

        has_limit = limiter.acquire_limit(key='test3')
        eq_(has_limit, True)

class TestLimiterWithStrictRedis(unittest.TestCase):
    def setUp(self):
        self.redis = redis.StrictRedis()

    def test_has_limit(self):
        limiter = Limiter(prefix='test-%.6f' % time.time(), limit=2, expiration_in_seconds=400, redis=self.redis)

        has_limit = limiter.acquire_limit(key='test1')
        eq_(has_limit, True)

        has_limit = limiter.acquire_limit(key='test2')
        eq_(has_limit, True)

        has_limit = limiter.acquire_limit(key='test3')
        eq_(has_limit, False)

    def test_has_limit_after_removing_items(self):
        limiter = Limiter(prefix='test-%.6f' % time.time(), limit=2, expiration_in_seconds=400, redis=self.redis)

        has_limit = limiter.acquire_limit(key='test1')
        eq_(has_limit, True)

        has_limit = limiter.acquire_limit(key='test2', expiration_in_seconds=-1)
        eq_(has_limit, True)

        has_limit = limiter.acquire_limit(key='test3')
        eq_(has_limit, True)

    def test_has_limit_after_releasing_items(self):
        limiter = Limiter(prefix='test-%.6f' % time.time(), limit=2, expiration_in_seconds=400, redis=self.redis)

        has_limit = limiter.acquire_limit(key='test1')
        eq_(has_limit, True)

        has_limit = limiter.acquire_limit(key='test2')
        eq_(has_limit, True)

        limiter.release_limit(key='test2')

        has_limit = limiter.acquire_limit(key='test3')
        eq_(has_limit, True)

########NEW FILE########
__FILENAME__ = test_lock
import unittest
import time
import threading
import uuid

import redis
from nose.tools import raises
from nose.tools import eq_

from retools import global_connection


class TestLock(unittest.TestCase):
    def _makeOne(self):
        from retools.lock import Lock
        return Lock

    def _lockException(self):
        from retools.lock import LockTimeout
        return LockTimeout

    def setUp(self):
        self.key = uuid.uuid4()

    def tearDown(self):
        global_connection.redis.delete(self.key)

    def test_lock_runs(self):
        Lock = self._makeOne()
        x = 0
        with Lock(self.key):
            x += 1

    def test_lock_fail(self):
        Lock = self._makeOne()

        bv = threading.Event()
        ev = threading.Event()

        def get_lock():
            with Lock(self.key):
                bv.set()
                ev.wait()
        t = threading.Thread(target=get_lock)
        t.start()
        ac = []

        @raises(self._lockException())
        def test_it():
            with Lock(self.key, timeout=0):
                ac.append(10)  # pragma: nocover
        bv.wait()
        test_it()
        eq_(ac, [])
        ev.set()
        t.join()
        with Lock(self.key, timeout=0):
            ac.append(10)
        eq_(ac, [10])

    def test_lock_retry(self):
        Lock = self._makeOne()
        bv = threading.Event()
        ev = threading.Event()

        def get_lock():
            with Lock(self.key):
                bv.set()
                ev.wait()
        t = threading.Thread(target=get_lock)
        t.start()
        ac = []

        bv.wait()

        @raises(self._lockException())
        def test_it():
            with Lock(self.key, timeout=1):
                ac.append(10)  # pragma: nocover
        test_it()
        ev.set()
        t.join()

########NEW FILE########
__FILENAME__ = test_queue
# coding: utf-8
import unittest
import time

import redis
import redis.client
import json
from decimal import Decimal

from nose.tools import raises
from nose.tools import eq_
from mock import Mock
from mock import patch


class TestQueue(unittest.TestCase):
    def _makeQM(self, **kwargs):
        from retools.queue import QueueManager
        return QueueManager(**kwargs)


class TestJob(TestQueue):
    def test_enqueue_job(self):
        mock_redis = Mock(spec=redis.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        mock_redis.pipeline.return_value = mock_pipeline
        qm = self._makeQM(redis=mock_redis)
        job_id = qm.enqueue('retools.tests.jobs:echo_default',
                            default='hi there')
        meth, args, kw = mock_pipeline.method_calls[0]
        eq_('rpush', meth)
        eq_(kw, {})
        queue_name, job_body = args
        job_data = json.loads(job_body)
        eq_(job_data['job_id'], job_id)
        eq_(job_data['kwargs'], {"default": "hi there"})

    def test_enqueue_job_by_name(self):
        mock_redis = Mock(spec=redis.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        mock_redis.pipeline.return_value = mock_pipeline
        qm = self._makeQM(redis=mock_redis)

        job_id = qm.enqueue('retools.tests.jobs:echo_default',
                            default='hi there')
        meth, args, kw = mock_pipeline.method_calls[0]
        eq_('rpush', meth)
        eq_(kw, {})
        queue_name, job_body = args
        job_data = json.loads(job_body)
        eq_(job_data['job_id'], job_id)
        eq_(job_data['kwargs'], {"default": "hi there"})
        mock_redis.llen = Mock(return_value=1)

        created = time.time()

        # trying get_jobs/get_job
        job = json.dumps({'job_id': job_id,
                          'job': 'retools.tests.jobs:echo_default',
                          'kwargs': {},
                          'state': '',
                          'events': {},
                          'metadata': {'created': created}
                         })

        mock_redis.lindex = Mock(return_value=job)

        jobs = list(qm.get_jobs())
        self.assertEqual(len(jobs), 1)
        my_job = qm.get_job(job_id)
        self.assertEqual(my_job.job_name, 'retools.tests.jobs:echo_default')
        self.assertEqual(my_job.metadata['created'], created)

        # testing the Worker class methods
        from retools.queue import Worker
        mock_redis = Mock(spec=redis.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        mock_redis.pipeline.return_value = mock_pipeline
        mock_redis.smembers = Mock(return_value=[])

        workers = list(Worker.get_workers(redis=mock_redis))
        self.assertEqual(len(workers), 0)

        worker = Worker(queues=['main'])
        mock_redis.smembers = Mock(return_value=[worker.worker_id])
        worker.register_worker()
        try:
            workers = list(Worker.get_workers(redis=mock_redis))
            self.assertEqual(len(workers), 1, workers)
            ids = Worker.get_worker_ids(redis=mock_redis)
            self.assertEqual(ids, [worker.worker_id])
        finally:
            worker.unregister_worker()

    def test_custom_serializer(self):
        mock_redis = Mock(spec=redis.Redis)
        mock_pipeline = Mock(spec=redis.client.Pipeline)
        mock_redis.pipeline.return_value = mock_pipeline

        def serialize(data):
            import simplejson
            return simplejson.dumps(data, use_decimal=True)


        def deserialize(data):
            import simplejson
            return simplejson.loads(data, use_decimal=True)

        qm = self._makeQM(redis=mock_redis, serializer=serialize,
                          deserializer=deserialize)

        job_id = qm.enqueue('retools.tests.jobs:echo_default',
                            decimal_value=Decimal('1.2'))
        meth, args, kw = mock_pipeline.method_calls[0]
        eq_('rpush', meth)
        eq_(kw, {})
        queue_name, job_body = args
        job_data = deserialize(job_body)
        eq_(job_data['job_id'], job_id)
        eq_(job_data['kwargs'], {'decimal_value': Decimal('1.2')})



########NEW FILE########
__FILENAME__ = test_util
import unittest

from contextlib import contextmanager

from nose.tools import eq_


class TestNamespaceFunc(unittest.TestCase):
    def _makeKey(self, func, deco_args):
        from retools.util import func_namespace
        return func_namespace(func, deco_args)

    def test_func_name(self):
        def a_func(): pass
        eq_('retools.tests.test_util.a_func.', self._makeKey(a_func, []))

    def test_class_method_name(self):
        # This simulates the class method by checking for 'cls' arg
        eq_('retools.tests.test_util.DummyClass.',
            self._makeKey(DummyClass.class_method, []))


class TestContextManager(unittest.TestCase):
    def _call_with_contexts(self, ctx_managers, func, kwargs):
        from retools.util import with_nested_contexts
        return with_nested_contexts(ctx_managers, func, [], kwargs)

    def test_nest_call(self):
        def a_func(**kwargs):
            kwargs['list'].append('here')
            return kwargs['list']

        @contextmanager
        def ctx_a(func, *args, **kwargs):
            eq_(func, a_func)
            kwargs['list'].append(0)
            yield
            kwargs['list'].append(1)

        @contextmanager
        def ctx_b(func, *args, **kwargs):
            eq_(func, a_func)
            kwargs['list'].append(2)
            yield
            kwargs['list'].append(3)

        lst = []
        kwargs = dict(list=lst)
        result = self._call_with_contexts([ctx_a, ctx_b], a_func, kwargs)
        eq_([0, 2, 'here', 3, 1], result)


class DummyClass(object):  # pragma: nocover
    def class_method(cls):
        return arg


class TestChunks(unittest.TestCase):
    def test_can_get_chunks(self):
        from retools.util import chunks
        items = [1, 2, 3, 4]

        eq_(list(chunks(items, 2)), [(1, 2), (3, 4)])


class TestFlipPairs(unittest.TestCase):
    def test_can_flip_pairs(self):
        from retools.util import flip_pairs
        items = [1, 2, 3, 4]

        eq_(list(flip_pairs(items)), [2, 1, 4, 3])


#def flip_pairs(l):
    #for x, y in chunks(l, 2):
        #yield y
        #yield x

########NEW FILE########
__FILENAME__ = util
"""Utility functions"""
import inspect
from itertools import izip


def func_namespace(func, deco_args):
    """Generates a unique namespace for a function"""
    kls = None
    if hasattr(func, 'im_func'):
        kls = func.im_class
        func = func.im_func

    deco_key = " ".join(map(str, deco_args))
    if kls:
        return '%s.%s.%s' % (kls.__module__, kls.__name__, deco_key)
    else:
        return '%s.%s.%s' % (func.__module__, func.__name__, deco_key)


def has_self_arg(func):
    """Return True if the given function has a 'self' argument."""
    return inspect.getargspec(func)[0] and \
          inspect.getargspec(func)[0][0] in ('self', 'cls')


def with_nested_contexts(context_managers, func, args, kwargs):
    """Nested context manager calling

    Given a function, and keyword arguments to call it with, it will
    be wrapped in a with statment using every context manager in the
    context_managers list for nested with calling.

    Every context_manager will get the function reference, and keyword
    arguments.

    Example::

        with ContextA(func, *args, **kwargs):
            with ContextB(func, *args, **kwargs):
                return func(**kwargs)

        # is equivilant to
        ctx_managers = [ContextA, ContextB]
        return with_nested_contexts(ctx_managers, func, kwargs)

    """
    if not context_managers:
        return func(**kwargs)
    else:
        ctx_manager = context_managers[0]
        with ctx_manager(func, *args, **kwargs):
            return with_nested_contexts(context_managers[1:],
                  func, args, kwargs)


def chunks(iterable, n):
    args = [iter(iterable)] * n
    return izip(*args)


def flip_pairs(l):
    for x, y in chunks(l, 2):
        yield y
        yield x

########NEW FILE########
