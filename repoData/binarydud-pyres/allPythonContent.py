__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# PyRes documentation build configuration file, created by
# sphinx-quickstart on Wed Jan  6 16:25:18 2010.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.append(os.path.abspath(os.path.dirname(__file__+'/../../../')))

# -- General configuration -----------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.todo', 'sphinx.ext.coverage']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'pyres'
copyright = u'2012, Matt George'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.3'
# The full version, including alpha/beta/rc tags.
release = '1.3'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of documents that shouldn't be included in the build.
#unused_docs = []

# List of directories, relative to source directory, that shouldn't be searched
# for source files.
exclude_trees = []

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  Major themes that come with
# Sphinx are currently 'default' and 'sphinxdoc'.
sys.path.append(os.path.abspath('_theme'))
html_theme_path = ['_theme']
html_theme = 'flask'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_use_modindex = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# If nonempty, this is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = ''

# Output file base name for HTML help builder.
htmlhelp_basename = 'pyresdoc'


# -- Options for LaTeX output --------------------------------------------------

# The paper size ('letter' or 'a4').
#latex_paper_size = 'letter'

# The font size ('10pt', '11pt' or '12pt').
#latex_font_size = '10pt'

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'pyres.tex', u'pyres Documentation',
   u'Matt George', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# Additional stuff for the LaTeX preamble.
#latex_preamble = ''

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_use_modindex = True


########NEW FILE########
__FILENAME__ = compat
import sys
import types

try:
    import cPickle as pickle
except ImportError: # pragma: no cover
    import pickle

# True if we are running on Python 3.
PY3 = sys.version_info[0] == 3

if PY3: # pragma: no cover
    string_types = str,
    integer_types = int,
    class_types = type,
    text_type = str
    binary_type = bytes
    long = int
    import subprocess as commands

else:
    string_types = basestring,
    integer_types = (int, long)
    class_types = (type, types.ClassType)
    text_type = unicode
    binary_type = str
    long = long
    import commands



########NEW FILE########
__FILENAME__ = exceptions
class NoQueueError(Exception):
    pass

class JobError(RuntimeError):
    pass

class TimeoutError(JobError):
    pass

class CrashError(JobError):
    pass
########NEW FILE########
__FILENAME__ = extensions
import os
import datetime
import time
import signal

try:
    import multiprocessing
except:
    import sys
    sys.exit("multiprocessing was not available")

from pyres import ResQ

from pyres.exceptions import NoQueueError
from pyres.worker import Worker

class JuniorWorker(Worker):
    def work(self, interval=5):
        self.startup()
        while True:
            if self._shutdown:
                break
            job = self.reserve()
            if job:
                print "got: %s" % job

                self.child = os.fork()

                if self.child:
                    print 'Forked %s at %s' % (self.child,
                                               datetime.datetime.now())
                    os.waitpid(self.child, 0)
                else:
                    print 'Processing %s since %s' % (job._queue,
                                                      datetime.datetime.now())
                    self.process(job)
                    os._exit(0)
                self.child = None
            else:
                break

        self.unregister_worker()

class Manager(object):
    def __init__(self, queues, host, max_children=10):
        self.queues = queues
        self._host = host
        self.max_children = max_children
        self._shutdown = False
        self.children = []
        self.resq = ResQ(host)
        self.validate_queues()
        self.reports = {}

    def __str__(self):
        hostname = os.uname()[1]
        pid = os.getpid()
        return 'Manager:%s:%s:%s' % (hostname, pid, ','.join(self.queues))

    def validate_queues(self):
        if not self.queues:
            raise NoQueueError("Please give each worker at least one queue.")

    def check_rising(self, queue, size):
        if queue in self.reports:
            new_time = time.time()
            old_size = self.reports[queue][0]
            old_time = self.reports[queue][1]
            if new_time > old_time + 5 and size > old_size + 20:
                return True
        else:
            self.reports[queue] = (size, time.time())
            return False

    def work(self):
        self.startup()
        while True:
            if self._shutdown:
                break
            #check to see if stuff is still going
            for queue in self.queues:
                #check queue size
                size = self.resq.size(queue)

                if self.check_rising(queue, size):
                    if len(self.children) < self.max_children:
                        self.start_child(queue)

    def startup(self):
        self.register_manager()
        self.register_signals()

    def register_manager(self):
        self.resq.redis.sadd('managers', str(self))

    def unregister_manager(self):
        self.resq.redis.srem('managers', str(self))

    def register_signals(self):
        signal.signal(signal.SIGTERM, self.shutdown_all)
        signal.signal(signal.SIGINT, self.shutdown_all)
        signal.signal(signal.SIGQUIT, self.schedule_shutdown)
        signal.signal(signal.SIGUSR1, self.kill_children)

    def shutdown_all(self, signum, frame):
        self.schedule_shutdown(signum, frame)
        self.kill_children(signum, frame)

    def schedule_shutdown(self, signum, frame):
        self._shutdown = True

    def kill_children(self):
        for child in self.children:
            child.terminate()

    def start_child(self, queue):
        p = multiprocessing.Process(target=JuniorWorker.run, args=([queue],
                                                                   self._host))
        self.children.append(p)
        p.start()
        return True

    @classmethod
    def run(cls, queues=(), host="localhost:6379"):
        manager = cls(queues, host)
        manager.work()

########NEW FILE########
__FILENAME__ = base
import sys
import traceback

class BaseBackend(object):
    """Provides a base class that custom backends can subclass. Also provides basic
    traceback and message parsing.

    The ``__init__`` takes these keyword arguments:

        ``exp`` -- The exception generated by your failure.

        ``queue`` -- The queue in which the ``Job`` was enqueued when it failed.

        ``payload`` -- The payload that was passed to the ``Job``.

        ``worker`` -- The worker that was processing the ``Job`` when it failed.

    """
    def __init__(self, exp, queue, payload, worker=None):
        excc = sys.exc_info()[0]

        self._exception = excc
        try:
            self._traceback = traceback.format_exc()
        except AttributeError:
            self._traceback = None

        self._worker = worker
        self._queue = queue
        self._payload = payload


    def _parse_traceback(self, trace):
        """Return the given traceback string formatted for a notification."""
        if not trace:
            return []

        return trace.split('\n')

    def _parse_message(self, exc):
        """Return a message for a notification from the given exception."""
        return '%s: %s' % (exc.__class__.__name__, str(exc))


########NEW FILE########
__FILENAME__ = mail
import smtplib

from textwrap import dedent
from email.mime.text import MIMEText

from base import BaseBackend

class MailBackend(BaseBackend):
    """Extends ``BaseBackend`` to provide support for emailing failures.
    Intended to be used with the MultipleBackend:

    from pyres import failure

    from pyres.failure.mail import MailBackend
    from pyres.failure.multiple import MultipleBackend
    from pyres.failure.redis import RedisBackend

    class EmailFailure(MailBackend):
        subject = 'Pyres Failure on {queue}'
        from_user = 'My Email User <mailuser@mydomain.tld>'
        recipients = ['Me <me@mydomain.tld>']

        smtp_host = 'mail.mydomain.tld'
        smtp_port = 25
        smtp_tls = True

        smtp_user = 'mailuser'
        smtp_password = 'm41lp455w0rd'

    failure.backend = MultipleBackend
    failure.backend.classes = [RedisBackend, EmailFailure]


    Additional notes:
        - The following tokens are available in subject: queue, worker, exception

        - Override the create_message method to provide an alternate body. It
        should return one of the message types from email.mime.*
    """
    subject = 'Pyres Failure on {queue}'

    recipients = []
    from_user = None
    smtp_host = None
    smtp_port = 25

    smtp_tls = False

    smtp_user = None
    smtp_password = None

    def save(self, resq=None):
        if not self.recipients or not self.smtp_host or not self.from_user:
            return

        message = self.create_message()
        subject = self.format_subject()

        message['Subject'] = subject
        message['From'] = self.from_user
        message['To'] = ", ".join(self.recipients)

        self.send_message(message)

    def format_subject(self):
        return self.subject.format(queue=self._queue,
                                   worker=self._worker,
                                   exception=self._exception)

    def create_message(self):
        """Returns a message body to send in this email. Should be from email.mime.*"""

        body = dedent("""\
        Received exception {exception} on {queue} from worker {worker}:

        {traceback}

        Payload:
        {payload}

        """).format(exception=self._exception,
                   traceback=self._traceback,
                   queue=self._queue,
                   payload=self._payload,
                   worker=self._worker)

        return MIMEText(body)

    def send_message(self, message):
        smtp = smtplib.SMTP(self.smtp_host, self.smtp_port)

        try:
            smtp.ehlo()

            if self.smtp_tls:
                smtp.starttls()

            if self.smtp_user:
                smtp.login(self.smtp_user, self.smtp_password)

            smtp.sendmail(self.from_user, self.recipients, message.as_string())
        finally:
            smtp.close()

########NEW FILE########
__FILENAME__ = multiple
from pyres.failure.base import BaseBackend
from pyres.failure.redis import RedisBackend

class MultipleBackend(BaseBackend):
    """Extends ``BaseBackend`` to provide support for delegating calls to multiple
    backends. Queries are delegated to the first backend in the list. Defaults to
    only the RedisBackend.

    To use:

    from pyres import failure

    from pyres.failure.base import BaseBackend
    from pyres.failure.multiple import MultipleBackend
    from pyres.failure.redis import RedisBackend

    class CustomBackend(BaseBackend):
        def save(self, resq):
            print('Custom backend')

    failure.backend = MultipleBackend
    failure.backend.classes = [RedisBackend, CustomBackend]
    """
    classes = []

    def __init__(self, *args):
        if not self.classes:
            self.classes = [RedisBackend]

        self.backends = [klass(*args) for klass in self.classes]
        BaseBackend.__init__(self, *args)

    @classmethod
    def count(cls, resq):
        first = MultipleBackend.classes[0]
        return first.count(resq)

    @classmethod
    def all(cls, resq, start=0, count=1):
        first = MultipleBackend.classes[0]
        return first.all(resq, start, count)

    @classmethod
    def clear(cls, resq):
        first = MultipleBackend.classes[0]
        return first.clear(resq)

    def save(self, resq=None):
        map(lambda x: x.save(resq), self.backends)

########NEW FILE########
__FILENAME__ = redis
import datetime, time
from base64 import b64encode

from .base import BaseBackend
from pyres import ResQ

class RedisBackend(BaseBackend):
    """Extends the ``BaseBackend`` to provide a Redis backend for failed jobs."""

    def save(self, resq=None):
        """Saves the failed Job into a "failed" Redis queue preserving all its original enqueud info."""
        if not resq:
            resq = ResQ()
        data = {
            'failed_at' : datetime.datetime.now().strftime('%Y/%m/%d %H:%M:%S'),
            'payload'   : self._payload,
            'exception' : self._exception.__class__.__name__,
            'error'     : self._parse_message(self._exception),
            'backtrace' : self._parse_traceback(self._traceback),
            'queue'     : self._queue
        }
        if self._worker:
            data['worker'] = self._worker
        data = ResQ.encode(data)
        resq.redis.rpush('resque:failed', data)

    @classmethod
    def count(cls, resq):
        return int(resq.redis.llen('resque:failed'))

    @classmethod
    def all(cls, resq, start=0, count=1):
        items = resq.redis.lrange('resque:failed', start, count) or []

        ret_list = []
        for i in items:
            failure = ResQ.decode(i)
            failure['redis_value'] = b64encode(i)
            ret_list.append(failure)
        return ret_list

    @classmethod
    def clear(cls, resq):
        return resq.redis.delete('resque:failed')


########NEW FILE########
__FILENAME__ = horde
import sys
try:
    import multiprocessing
except:
    sys.exit("multiprocessing was not available")

import time, os, signal
import datetime
import logging
import logging.handlers
from pyres import ResQ, Stat, get_logging_handler, special_log_file
from pyres.exceptions import NoQueueError
try:
    from collections import OrderedDict
except ImportError:
    from ordereddict import OrderedDict
from pyres.job import Job
from pyres.compat import string_types
import pyres.json_parser as json
try:
    from setproctitle import setproctitle
except:
    def setproctitle(name):
        pass

def setup_logging(procname, namespace='', log_level=logging.INFO, log_file=None):

    logger = multiprocessing.get_logger()
    #logger = multiprocessing.log_to_stderr()
    logger.setLevel(log_level)
    handler = get_logging_handler(log_file, procname, namespace)
    logger.addHandler(handler)
    return logger

class Minion(multiprocessing.Process):
    def __init__(self, queues, server, password, log_level=logging.INFO, log_path=None, interval=5, concat_logs=False,
                 max_jobs=0):
        multiprocessing.Process.__init__(self, name='Minion')

        #format = '%(asctime)s %(levelname)s %(filename)s-%(lineno)d: %(message)s'
        #logHandler = logging.StreamHandler()
        #logHandler.setFormatter(logging.Formatter(format))
        #self.logger = multiprocessing.get_logger()
        #self.logger.addHandler(logHandler)
        #self.logger.setLevel(logging.DEBUG)

        self.queues = queues
        self._shutdown = False
        self.hostname = os.uname()[1]
        self.server = server
        self.password = password
        self.interval = interval

        self.log_level = log_level
        self.log_path = log_path
        self.log_file = None
        self.concat_logs = concat_logs
        self.max_jobs = max_jobs

    def prune_dead_workers(self):
        pass

    def schedule_shutdown(self, signum, frame):
        self._shutdown = True

    def register_signal_handlers(self):
        signal.signal(signal.SIGTERM, self.schedule_shutdown)
        signal.signal(signal.SIGINT, self.schedule_shutdown)
        signal.signal(signal.SIGQUIT, self.schedule_shutdown)

    def register_minion(self):
        self.resq.redis.sadd('resque:minions',str(self))
        self.started = datetime.datetime.now()

    def startup(self):
        self.register_signal_handlers()
        self.prune_dead_workers()
        self.register_minion()

    def __str__(self):
        return '%s:%s:%s' % (self.hostname, self.pid, ','.join(self.queues))

    def reserve(self):
        self.logger.debug('checking queues: %s' % self.queues)
        job = Job.reserve(self.queues, self.resq, self.__str__())
        if job:
            self.logger.info('Found job on %s' % job._queue)
            return job

    def process(self, job):
        if not job:
            return
        try:
            self.working_on(job)
            job.perform()
        except Exception as e:
            exceptionType, exceptionValue, exceptionTraceback = sys.exc_info()
            self.logger.error("%s failed: %s" % (job, e))
            job.fail(exceptionTraceback)
            self.failed()
        else:
            self.logger.debug("Hells yeah")
            self.logger.info('completed job: %s' % job)
        finally:
            self.done_working()

    def working_on(self, job):
        setproctitle('pyres_minion:%s: working on job: %s' % (os.getppid(), job._payload))
        self.logger.debug('marking as working on')
        data = {
            'queue': job._queue,
            'run_at': int(time.mktime(datetime.datetime.now().timetuple())),
            'payload': job._payload
        }
        data = json.dumps(data)
        self.resq.redis["resque:minion:%s" % str(self)] = data
        self.logger.debug("minion:%s" % str(self))
        #self.logger.debug(self.resq.redis["resque:minion:%s" % str(self)])

    def failed(self):
        Stat("failed", self.resq).incr()

    def processed(self):
        total_processed = Stat("processed", self.resq)
        total_processed.incr()

    def done_working(self):
        self.logger.debug('done working')
        self.processed()
        self.resq.redis.delete("resque:minion:%s" % str(self))

    def unregister_minion(self):
        self.resq.redis.srem('resque:minions',str(self))
        self.started = None

    def work(self, interval=5):

        self.startup()
        cur_job = 0
        while True:
            setproctitle('pyres_minion:%s: waiting for job on: %s' % (os.getppid(),self.queues))
            self.logger.info('waiting on job')
            if self._shutdown:
                self.logger.info('shutdown scheduled')
                break
            self.logger.debug('max_jobs: %d cur_jobs: %d' % (self.max_jobs, cur_job))
            if (self.max_jobs > 0 and self.max_jobs < cur_job):
                self.logger.debug('max_jobs reached on %s: %d' % (self.pid, cur_job))
                self.logger.debug('minion sleeping for: %d secs' % interval)
                time.sleep(interval)
                cur_job = 0
            job = self.reserve()
            if job:
                self.process(job)
                cur_job = cur_job + 1
            else:
                cur_job = 0
                self.logger.debug('minion sleeping for: %d secs' % interval)
                time.sleep(interval)
        self.unregister_minion()

    def clear_logger(self):
        for handler in self.logger.handlers:
            self.logger.removeHandler(handler)

    def run(self):
        setproctitle('pyres_minion:%s: Starting' % (os.getppid(),))
        if self.log_path:
            if special_log_file(self.log_path):
                self.log_file = self.log_path
            elif self.concat_logs:
                self.log_file = os.path.join(self.log_path, 'minion.log')
            else:
                self.log_file = os.path.join(self.log_path, 'minion-%s.log' % self.pid)
        namespace = 'minion:%s' % self.pid
        self.logger = setup_logging('minion', namespace, self.log_level, self.log_file)
        #self.clear_logger()
        if isinstance(self.server,string_types):
            self.resq = ResQ(server=self.server, password=self.password)
        elif isinstance(self.server, ResQ):
            self.resq = self.server
        else:
            raise Exception("Bad server argument")


        self.work(self.interval)
        #while True:
        #    job = self.q.get()
        #    print 'pid: %s is running %s ' % (self.pid,job)


class Khan(object):
    _command_map = {
        'ADD': 'add_minion',
        'REMOVE': '_remove_minion',
        'SHUTDOWN': '_schedule_shutdown'
    }
    def __init__(self, pool_size=5, queues=[], server='localhost:6379', password=None, logging_level=logging.INFO,
            log_file=None, minions_interval=5, concat_minions_logs=False, max_jobs=0):
        #super(Khan,self).__init__(queues=queues,server=server,password=password)
        self._shutdown = False
        self.pool_size = int(pool_size)
        self.queues = queues
        self.server = server
        self.password = password
        self.pid = os.getpid()
        self.validate_queues()
        self._workers = OrderedDict()
        self.server = server
        self.password = password
        self.logging_level = logging_level
        self.log_file = log_file
        self.minions_interval = minions_interval
        self.concat_minions_logs = concat_minions_logs
        self.max_jobs = max_jobs

        #self._workers = list()

    def setup_resq(self):
        if hasattr(self,'logger'):
            self.logger.info('Connecting to redis server - %s' % self.server)
        if isinstance(self.server,string_types):
            self.resq = ResQ(server=self.server, password=self.password)
        elif isinstance(self.server, ResQ):
            self.resq = self.server
        else:
            raise Exception("Bad server argument")

    def validate_queues(self):
        "Checks if a worker is given atleast one queue to work on."
        if not self.queues:
            raise NoQueueError("Please give each worker at least one queue.")

    def startup(self):
        self.register_signal_handlers()


    def register_signal_handlers(self):
        signal.signal(signal.SIGTERM, self.schedule_shutdown)
        signal.signal(signal.SIGINT, self.schedule_shutdown)
        signal.signal(signal.SIGQUIT, self.schedule_shutdown)
        signal.signal(signal.SIGUSR1, self.kill_child)
        signal.signal(signal.SIGUSR2, self.add_child)
        if hasattr(signal, 'SIGINFO'):
            signal.signal(signal.SIGINFO, self.current_state)

    def current_state(self):
        tmap = {}
        main_thread = None
        import traceback
        from cStringIO import StringIO
        # get a map of threads by their ID so we can print their names
        # during the traceback dump
        for t in threading.enumerate():
            if getattr(t, "ident", None):
                tmap[t.ident] = t
            else:
                main_thread = t

        out = StringIO()
        sep = "=" * 49 + "\n"
        for tid, frame in sys._current_frames().iteritems():
            thread = tmap.get(tid, main_thread)
            if not thread:
                # skip old junk (left-overs from a fork)
                continue
            out.write("%s\n" % (thread.getName(), ))
            out.write(sep)
            traceback.print_stack(frame, file=out)
            out.write(sep)
            out.write("LOCAL VARIABLES\n")
            out.write(sep)
            pprint(frame.f_locals, stream=out)
            out.write("\n\n")
        self.logger.info(out.getvalue())

    def _schedule_shutdown(self):
        self.schedule_shutdown(None, None)

    def schedule_shutdown(self, signum, frame):
        self.logger.info('Khan Shutdown scheduled')
        self._shutdown = True

    def kill_child(self, signum, frame):
        self._remove_minion()

    def add_child(self, signum, frame):
        self.add_minion()

    def register_khan(self):
        if not hasattr(self, 'resq'):
            self.setup_resq()
        self.resq.redis.sadd('resque:khans',str(self))
        self.started = datetime.datetime.now()

    def _check_commands(self):
        if not self._shutdown:
            self.logger.debug('Checking commands')
            command = self.resq.redis.lpop('resque:khan:%s' % str(self))
            self.logger.debug('COMMAND FOUND: %s ' % command)
            if command:
                self.process_command(command)
                self._check_commands()

    def process_command(self, command):
        self.logger.info('Processing Command')
        #available commands, shutdown, add 1, remove 1
        command_item = self._command_map.get(command, None)
        if command_item:
            fn = getattr(self, command_item)
            if fn:
                fn()

    def add_minion(self):
        self._add_minion()
        self.resq.redis.srem('resque:khans',str(self))
        self.pool_size += 1
        self.resq.redis.sadd('resque:khans',str(self))

    def _add_minion(self):
        if hasattr(self,'logger'):
            self.logger.info('Adding minion')
        if self.log_file:
            if special_log_file(self.log_file):
                log_path = self.log_file
            else:
                log_path = os.path.dirname(self.log_file)
        else:
            log_path = None
        m = Minion(self.queues, self.server, self.password, interval=self.minions_interval,
                   log_level=self.logging_level, log_path=log_path, concat_logs=self.concat_minions_logs,
                   max_jobs=self.max_jobs)
        m.start()
        self._workers[m.pid] = m
        if hasattr(self,'logger'):
            self.logger.info('minion added at: %s' % m.pid)
        return m

    def _shutdown_minions(self):
        """
        send the SIGNINT signal to each worker in the pool.
        """
        setproctitle('pyres_manager: Waiting on children to shutdown.')
        for minion in self._workers.values():
            minion.terminate()
            minion.join()

    def _remove_minion(self, pid=None):
        #if pid:
        #    m = self._workers.pop(pid)
        pid, m = self._workers.popitem(False)
        m.terminate()
        self.resq.redis.srem('resque:khans',str(self))
        self.pool_size -= 1
        self.resq.redis.sadd('resque:khans',str(self))
        return m

    def unregister_khan(self):
        if hasattr(self,'logger'):
            self.logger.debug('unregistering khan')
        self.resq.redis.srem('resque:khans',str(self))
        self.started = None

    def setup_minions(self):
        for i in range(self.pool_size):
            self._add_minion()

    def _setup_logging(self):
        self.logger = setup_logging('khan', 'khan', self.logging_level, self.log_file)

    def work(self, interval=2):
        setproctitle('pyres_manager: Starting')
        self.startup()
        self.setup_minions()
        self._setup_logging()
        self.logger.info('Running as pid: %s' % self.pid)
        self.logger.info('Added %s child processes' % self.pool_size)
        self.logger.info('Setting up pyres connection')
        self.setup_resq()
        self.register_khan()
        setproctitle('pyres_manager: running %s' % self.queues)
        while True:
            self._check_commands()
            if self._shutdown:
                #send signals to each child
                self._shutdown_minions()
                break
            #get job
            else:
                self.logger.debug('manager sleeping for: %d secs' % interval)
                time.sleep(interval)
        self.unregister_khan()

    def __str__(self):
        hostname = os.uname()[1]
        return '%s:%s:%s' % (hostname, self.pid, self.pool_size)

    @classmethod
    def run(cls, pool_size=5, queues=[], server='localhost:6379', password=None, interval=2,
            logging_level=logging.INFO, log_file=None, minions_interval=5, concat_minions_logs=False, max_jobs=0):
        worker = cls(pool_size=pool_size, queues=queues, server=server, password=password, logging_level=logging_level,
                     log_file=log_file, minions_interval=minions_interval, concat_minions_logs=concat_minions_logs,
                     max_jobs=max_jobs)
        worker.work(interval=interval)

#if __name__ == "__main__":
#    k = Khan()
#    k.run()

if __name__ == "__main__":
    from optparse import OptionParser
    parser = OptionParser(usage="%prog [options] queue list")
    parser.add_option("-s", dest="server", default="localhost:6379")
    (options,args) = parser.parse_args()
    if len(args) < 1:
        parser.print_help()
        parser.error("Please give the horde at least one queue.")
    Khan.run(pool_size=2, queues=args, server=options.server)
    #khan.run()
    #Worker.run(queues, options.server)

########NEW FILE########
__FILENAME__ = job
import logging
import time
from datetime import timedelta
from pyres import ResQ, safe_str_to_class
from pyres import failure
from pyres.failure.redis import RedisBackend
from pyres.compat import string_types

class Job(object):
    """Every job on the ResQ is an instance of the *Job* class.

    The ``__init__`` takes these keyword arguments:

        ``queue`` -- A string defining the queue to which this Job will be
                     added.

        ``payload`` -- A dictionary which contains the string name of a class
                       which extends this Job and a list of args which will be
                       passed to that class.

        ``resq`` -- An instance of the ResQ class.

        ``worker`` -- The name of a specific worker if you'd like this Job to be
                      done by that worker. Default is "None".

    """

    safe_str_to_class = staticmethod(safe_str_to_class)

    def __init__(self, queue, payload, resq, worker=None):
        self._queue = queue
        self._payload = payload
        self.resq = resq
        self._worker = worker

        self.enqueue_timestamp = self._payload.get("enqueue_timestamp")

        # Set the default back end, jobs can override when we import them
        # inside perform().
        failure.backend = RedisBackend

    def __str__(self):
        return "(Job{%s} | %s | %s)" % (
            self._queue, self._payload['class'], repr(self._payload['args']))

    def perform(self):
        """This method converts payload into args and calls the ``perform``
        method on the payload class.

        Before calling ``perform``, a ``before_perform`` class method
        is called, if it exists.  It takes a dictionary as an argument;
        currently the only things stored on the dictionary are the
        args passed into ``perform`` and a timestamp of when the job
        was enqueued.

        Similarly, an ``after_perform`` class method is called after
        ``perform`` is finished.  The metadata dictionary contains the
        same data, plus a timestamp of when the job was performed, a
        ``failed`` boolean value, and if it did fail, a ``retried``
        boolean value.  This method is called after retry, and is
        called regardless of whether an exception is ultimately thrown
        by the perform method.


        """
        payload_class_str = self._payload["class"]
        payload_class = self.safe_str_to_class(payload_class_str)
        payload_class.resq = self.resq
        args = self._payload.get("args")

        metadata = dict(args=args)
        if self.enqueue_timestamp:
            metadata["enqueue_timestamp"] = self.enqueue_timestamp

        before_perform = getattr(payload_class, "before_perform", None)

        metadata["failed"] = False
        metadata["perform_timestamp"] = time.time()
        check_after = True
        try:
            if before_perform:
                payload_class.before_perform(metadata)
            return payload_class.perform(*args)
        except Exception as e:
            check_after = False
            metadata["failed"] = True
            metadata["exception"] = e
            if not self.retry(payload_class, args):
                metadata["retried"] = False
                raise
            else:
                metadata["retried"] = True
                logging.exception("Retry scheduled after error in %s", self._payload)
        finally:
            after_perform = getattr(payload_class, "after_perform", None)
            if after_perform and check_after:
                payload_class.after_perform(metadata)
            delattr(payload_class,'resq')

    def fail(self, exception):
        """This method provides a way to fail a job and will use whatever
        failure backend you've provided. The default is the ``RedisBackend``.

        """
        fail = failure.create(exception, self._queue, self._payload,
                              self._worker)
        fail.save(self.resq)
        return fail

    def retry(self, payload_class, args):
        """This method provides a way to retry a job after a failure.
        If the jobclass defined by the payload containes a ``retry_every`` attribute then pyres
        will attempt to retry the job until successful or until timeout defined by ``retry_timeout`` on the payload class.

        """
        retry_every = getattr(payload_class, 'retry_every', None)
        retry_timeout = getattr(payload_class, 'retry_timeout', 0)

        if retry_every:
            now = ResQ._current_time()
            first_attempt = self._payload.get("first_attempt", now)
            retry_until = first_attempt + timedelta(seconds=retry_timeout)
            retry_at = now + timedelta(seconds=retry_every)
            if retry_at < retry_until:
                self.resq.enqueue_at(retry_at, payload_class, *args,
                        **{'first_attempt':first_attempt})
                return True
        return False

    @classmethod
    def reserve(cls, queues, res, worker=None, timeout=10):
        """Reserve a job on one of the queues. This marks this job so
        that other workers will not pick it up.

        """
        if isinstance(queues, string_types):
            queues = [queues]
        queue, payload = res.pop(queues, timeout=timeout)
        if payload:
            return cls(queue, payload, res, worker)

########NEW FILE########
__FILENAME__ = json_parser
from datetime import datetime
from pyres.compat import string_types

try:
    #import simplejson as json
    import json
except ImportError:
    import simplejson as json

DATE_FORMAT = '%Y-%m-%dT%H:%M:%S'
DATE_PREFIX = '@D:'

class CustomJSONEncoder(json.JSONEncoder):

    def default(self, o):
        if isinstance(o, datetime):
            return o.strftime(DATE_PREFIX + DATE_FORMAT)
        return json.JSONEncoder.default(self, o)


class CustomJSONDecoder(json.JSONDecoder):

    def decode(self, json_string):
        decoded = json.loads(json_string)
        return self.convert(decoded)

    def convert(self, value):
        if isinstance(value, string_types) and value.startswith(DATE_PREFIX):
            try:
                return datetime.strptime(value[len(DATE_PREFIX):], DATE_FORMAT)
            except ValueError:
                return value
        elif isinstance(value, dict):
            for k, v in value.items():
                new = self.convert(v)
                if new != v:
                    value[k] = new
        elif isinstance(value, list):
            for k, v in enumerate(value):
                new = self.convert(v)
                if new != v:
                    value[k] = new
        return value


def dumps(values):
    return json.dumps(values, cls=CustomJSONEncoder)


def loads(string):
    return json.loads(string, cls=CustomJSONDecoder)

########NEW FILE########
__FILENAME__ = scheduler
import signal
import time
import logging

from pyres import ResQ, __version__
from pyres.compat import string_types

logger = logging.getLogger(__name__)

class Scheduler(object):

    def __init__(self, server="localhost:6379", password=None):
        """
        >>> from pyres.scheduler import Scheduler
        >>> scheduler = Scheduler('localhost:6379')
        """
        self._shutdown = False
        if isinstance(server, string_types):
            self.resq = ResQ(server=server, password=password)
        elif isinstance(server, ResQ):
            self.resq = server
        else:
            raise Exception("Bad server argument")

    def register_signal_handlers(self):
        logger.info('registering signals')
        signal.signal(signal.SIGTERM, self.schedule_shutdown)
        signal.signal(signal.SIGINT, self.schedule_shutdown)
        signal.signal(signal.SIGQUIT, self.schedule_shutdown)

    def schedule_shutdown(self, signal, frame):
        logger.info('shutting down started')
        self._shutdown = True

    def __call__(self):
        _setproctitle("Starting")
        logger.info('starting up')
        self.register_signal_handlers()
        #self.load_schedule()
        logger.info('looking for delayed items')
        while True:
            if self._shutdown:
                break
            self.handle_delayed_items()
            _setproctitle("Waiting")
            logger.debug('sleeping')
            time.sleep(5)
        logger.info('shutting down complete')

    def next_timestamp(self):
        while True:
            timestamp = self.resq.next_delayed_timestamp()
            if timestamp:
                yield timestamp
            else:
                break


    def next_item(self, timestamp):
        while True:
            item = self.resq.next_item_for_timestamp(timestamp)
            if item:
                yield item
            else:
                break

    def handle_delayed_items(self):
        for timestamp in self.next_timestamp():
            _setproctitle('Handling timestamp %s' % timestamp)
            logger.debug('handling timestamp: %s' % timestamp)
            for item in self.next_item(timestamp):
                logger.debug('queueing item %s' % item)
                klass = item['class']
                queue = item['queue']
                args = item['args']
                kwargs = {}
                if 'first_attempt' in item:
                    kwargs['first_attempt'] = item['first_attempt']
                self.resq.enqueue_from_string(klass, queue, *args, **kwargs)


    @classmethod
    def run(cls, server, password=None):
        sched = cls(server=server, password=password)
        sched()


try:
    from setproctitle import setproctitle
except ImportError:
    def setproctitle(name):
        pass

def _setproctitle(msg):
    setproctitle("pyres_scheduler-%s: %s" % (__version__, msg))

########NEW FILE########
__FILENAME__ = scripts
import logging

from optparse import OptionParser

from pyres.horde import Khan
from pyres import setup_logging, setup_pidfile
from pyres.scheduler import Scheduler
from pyres.worker import Worker


def pyres_manager():
    usage = "usage: %prog [options] arg1"
    parser = OptionParser(usage=usage)
    #parser.add_option("-q", dest="queue_list")
    parser.add_option("--host", dest="host", default="localhost")
    parser.add_option("--port", dest="port",type="int", default=6379)
    parser.add_option("--password", dest="password", default=None)
    parser.add_option("-i", '--interval', dest='manager_interval', default=None, help='the default time interval to sleep between runs - manager')
    parser.add_option("--minions_interval", dest='minions_interval', default=None, help='the default time interval to sleep between runs - minions')
    parser.add_option('-l', '--log-level', dest='log_level', default='info', help='log level.  Valid values are "debug", "info", "warning", "error", "critical", in decreasing order of verbosity. Defaults to "info" if parameter not specified.')
    parser.add_option("--pool", type="int", dest="pool_size", default=1, help="Number of minions to spawn under the manager.")
    parser.add_option("-j", "--process_max_jobs", dest="max_jobs", type=int, default=0, help='how many jobs should be processed on worker run.')
    parser.add_option('-f', dest='logfile', help='If present, a logfile will be used.  "stderr", "stdout", and "syslog" are all special values.')
    parser.add_option('-p', dest='pidfile', help='If present, a pidfile will be used.')
    parser.add_option("--concat_minions_logs", action="store_true", dest="concat_minions_logs", help='Concat all minions logs on same file.')
    (options,args) = parser.parse_args()

    if len(args) != 1:
        parser.print_help()
        parser.error("Argument must be a comma seperated list of queues")

    log_level = getattr(logging, options.log_level.upper(), 'INFO')
    #logging.basicConfig(level=log_level, format="%(asctime)s: %(levelname)s: %(message)s")
    concat_minions_logs = options.concat_minions_logs
    setup_pidfile(options.pidfile)

    manager_interval = options.manager_interval
    if manager_interval is not None:
        manager_interval = float(manager_interval)

    minions_interval = options.minions_interval
    if minions_interval is not None:
        minions_interval = float(minions_interval)

    queues = args[0].split(',')
    server = '%s:%s' % (options.host,options.port)
    password = options.password
    Khan.run(pool_size=options.pool_size, queues=queues, server=server, password=password, interval=manager_interval,
            logging_level=log_level, log_file=options.logfile, minions_interval=minions_interval,
            concat_minions_logs=concat_minions_logs, max_jobs=options.max_jobs)


def pyres_scheduler():
    usage = "usage: %prog [options] arg1"
    parser = OptionParser(usage=usage)
    #parser.add_option("-q", dest="queue_list")
    parser.add_option("--host", dest="host", default="localhost")
    parser.add_option("--port", dest="port",type="int", default=6379)
    parser.add_option("--password", dest="password", default=None)
    parser.add_option('-l', '--log-level', dest='log_level', default='info', help='log level.  Valid values are "debug", "info", "warning", "error", "critical", in decreasing order of verbosity. Defaults to "info" if parameter not specified.')
    parser.add_option('-f', dest='logfile', help='If present, a logfile will be used.  "stderr", "stdout", and "syslog" are all special values.')
    parser.add_option('-p', dest='pidfile', help='If present, a pidfile will be used.')
    (options,args) = parser.parse_args()
    log_level = getattr(logging, options.log_level.upper(),'INFO')
    #logging.basicConfig(level=log_level, format="%(module)s: %(asctime)s: %(levelname)s: %(message)s")
    setup_logging(procname="pyres_scheduler", log_level=log_level, filename=options.logfile)
    setup_pidfile(options.pidfile)
    server = '%s:%s' % (options.host, options.port)
    password = options.password
    Scheduler.run(server, password)


def pyres_worker():
    usage = "usage: %prog [options] arg1"
    parser = OptionParser(usage=usage)

    parser.add_option("--host", dest="host", default="localhost")
    parser.add_option("--port", dest="port",type="int", default=6379)
    parser.add_option("--password", dest="password", default=None)
    parser.add_option("-i", '--interval', dest='interval', default=None, help='the default time interval to sleep between runs')
    parser.add_option('-l', '--log-level', dest='log_level', default='info', help='log level.  Valid values are "debug", "info", "warning", "error", "critical", in decreasing order of verbosity. Defaults to "info" if parameter not specified.')
    parser.add_option('-f', dest='logfile', help='If present, a logfile will be used.  "stderr", "stdout", and "syslog" are all special values.')
    parser.add_option('-p', dest='pidfile', help='If present, a pidfile will be used.')
    parser.add_option("-t", '--timeout', dest='timeout', default=None, help='the timeout in seconds for this worker')
    (options,args) = parser.parse_args()

    if len(args) != 1:
        parser.print_help()
        parser.error("Argument must be a comma seperated list of queues")

    log_level = getattr(logging, options.log_level.upper(), 'INFO')
    setup_logging(procname="pyres_worker", log_level=log_level, filename=options.logfile)
    setup_pidfile(options.pidfile)

    interval = options.interval
    if interval is not None:
        interval = int(interval)

    timeout = options.timeout and int(options.timeout)

    queues = args[0].split(',')
    server = '%s:%s' % (options.host,options.port)
    password = options.password
    Worker.run(queues, server, password, interval, timeout=timeout)

########NEW FILE########
__FILENAME__ = worker
import logging
import signal
import datetime, time
import os, sys
from pyres import json_parser as json
from pyres.compat import commands
import random

from pyres.exceptions import NoQueueError, JobError, TimeoutError, CrashError
from pyres.job import Job
from pyres import ResQ, Stat, __version__
from pyres.compat import string_types


logger = logging.getLogger(__name__)

class Worker(object):
    """Defines a worker. The ``pyres_worker`` script instantiates this Worker
    class and passes a comma-separated list of queues to listen on.::

       >>> from pyres.worker import Worker
       >>> Worker.run([queue1, queue2], server="localhost:6379/0")

    """

    job_class = Job

    def __init__(self, queues=(), server="localhost:6379", password=None, timeout=None):
        self.queues = queues
        self.validate_queues()
        self._shutdown = False
        self.child = None
        self.pid = os.getpid()
        self.hostname = os.uname()[1]
        self.timeout = timeout

        if isinstance(server, string_types):
            self.resq = ResQ(server=server, password=password)
        elif isinstance(server, ResQ):
            self.resq = server
        else:
            raise Exception("Bad server argument")

    def validate_queues(self):
        """Checks if a worker is given at least one queue to work on."""
        if not self.queues:
            raise NoQueueError("Please give each worker at least one queue.")

    def register_worker(self):
        self.resq.redis.sadd('resque:workers', str(self))
        #self.resq._redis.add("worker:#{self}:started", Time.now.to_s)
        self.started = datetime.datetime.now()

    def _set_started(self, dt):
        if dt:
            key = int(time.mktime(dt.timetuple()))
            self.resq.redis.set("resque:worker:%s:started" % self, key)
        else:
            self.resq.redis.delete("resque:worker:%s:started" % self)

    def _get_started(self):
        datestring = self.resq.redis.get("resque:worker:%s:started" % self)
        #ds = None
        #if datestring:
        #    ds = datetime.datetime.strptime(datestring, '%Y-%m-%d %H:%M:%S')
        return datestring

    started = property(_get_started, _set_started)

    def unregister_worker(self):
        self.resq.redis.srem('resque:workers', str(self))
        self.started = None
        Stat("processed:%s" % self, self.resq).clear()
        Stat("failed:%s" % self, self.resq).clear()

    def prune_dead_workers(self):
        all_workers = Worker.all(self.resq)
        known_workers = Worker.worker_pids()
        for worker in all_workers:
            host, pid, queues = worker.id.split(':')
            if host != self.hostname:
                continue
            if pid in known_workers:
                continue
            logger.warning("pruning dead worker: %s" % worker)
            worker.unregister_worker()

    def startup(self):
        self.register_signal_handlers()
        self.prune_dead_workers()
        self.register_worker()

    def register_signal_handlers(self):
        signal.signal(signal.SIGTERM, self.shutdown_all)
        signal.signal(signal.SIGINT, self.shutdown_all)
        signal.signal(signal.SIGQUIT, self.schedule_shutdown)
        signal.signal(signal.SIGUSR1, self.kill_child)

    def shutdown_all(self, signum, frame):
        self.schedule_shutdown(signum, frame)
        self.kill_child(signum, frame)

    def schedule_shutdown(self, signum, frame):
        self._shutdown = True

    def kill_child(self, signum, frame):
        if self.child:
            logger.info("Killing child at %s" % self.child)
            os.kill(self.child, signal.SIGKILL)

    def __str__(self):
        if getattr(self,'id', None):
            return self.id
        return '%s:%s:%s' % (self.hostname, self.pid, ','.join(self.queues))

    def _setproctitle(self, msg):
        setproctitle("pyres_worker-%s [%s]: %s" % (__version__,
                                                   ','.join(self.queues),
                                                   msg))

    def work(self, interval=5):
        """Invoked by ``run`` method. ``work`` listens on a list of queues and sleeps
        for ``interval`` time.

        ``interval`` -- Number of seconds the worker will wait until processing the next job. Default is "5".

        Whenever a worker finds a job on the queue it first calls ``reserve`` on
        that job to make sure another worker won't run it, then *forks* itself to
        work on that job.

        """
        self._setproctitle("Starting")
        logger.info("starting")
        self.startup()

        while True:
            if self._shutdown:
                logger.info('shutdown scheduled')
                break

            self.register_worker()

            job = self.reserve(interval)

            if job:
                self.fork_worker(job)
            else:
                if interval == 0:
                    break
                #procline @paused ? "Paused" : "Waiting for #{@queues.join(',')}"
                self._setproctitle("Waiting")
                #time.sleep(interval)
        self.unregister_worker()

    def fork_worker(self, job):
        """Invoked by ``work`` method. ``fork_worker`` does the actual forking to create the child
        process that will process the job. It's also responsible for monitoring the child process
        and handling hangs and crashes.

        Finally, the ``process`` method actually processes the job by eventually calling the Job
        instance's ``perform`` method.

        """
        logger.debug('picked up job')
        logger.debug('job details: %s' % job)
        self.before_fork(job)
        self.child = os.fork()
        if self.child:
            self._setproctitle("Forked %s at %s" %
                               (self.child,
                                datetime.datetime.now()))
            logger.info('Forked %s at %s' % (self.child,
                                              datetime.datetime.now()))

            try:
                start = datetime.datetime.now()

                # waits for the result or times out
                while True:
                    pid, status = os.waitpid(self.child, os.WNOHANG)
                    if pid != 0:
                        if os.WIFEXITED(status) and os.WEXITSTATUS(status) == 0:
                            break
                        if os.WIFSTOPPED(status):
                            logger.warning("Process stopped by signal %d" % os.WSTOPSIG(status))
                        else:
                            if os.WIFSIGNALED(status):
                                raise CrashError("Unexpected exit by signal %d" % os.WTERMSIG(status))
                            raise CrashError("Unexpected exit status %d" % os.WEXITSTATUS(status))

                    time.sleep(0.5)

                    now = datetime.datetime.now()
                    if self.timeout and ((now - start).seconds > self.timeout):
                        os.kill(self.child, signal.SIGKILL)
                        os.waitpid(-1, os.WNOHANG)
                        raise TimeoutError("Timed out after %d seconds" % self.timeout)

            except OSError as ose:
                import errno

                if ose.errno != errno.EINTR:
                    raise ose
            except JobError:
                self._handle_job_exception(job)
            finally:
                # If the child process' job called os._exit manually we need to
                # finish the clean up here.
                if self.job():
                    self.done_working(job)

            logger.debug('done waiting')
        else:
            self._setproctitle("Processing %s since %s" %
                               (job,
                                datetime.datetime.now()))
            logger.info('Processing %s since %s' %
                         (job, datetime.datetime.now()))
            self.after_fork(job)

            # re-seed the Python PRNG after forking, otherwise
            # all job process will share the same sequence of
            # random numbers
            random.seed()

            self.process(job)
            os._exit(0)
        self.child = None

    def before_fork(self, job):
        """
        hook for making changes immediately before forking to process
        a job
        """
        pass

    def after_fork(self, job):
        """
        hook for making changes immediately after forking to process a
        job
        """
        pass

    def before_process(self, job):
        return job

    def process(self, job=None):
        if not job:
            job = self.reserve()

        job_failed = False
        try:
            try:
                self.working_on(job)
                job = self.before_process(job)
                return job.perform()
            except Exception:
                job_failed = True
                self._handle_job_exception(job)
            except SystemExit as e:
                if e.code != 0:
                    job_failed = True
                    self._handle_job_exception(job)

            if not job_failed:
                logger.debug('completed job')
                logger.debug('job details: %s' % job)
        finally:
            self.done_working(job)

    def _handle_job_exception(self, job):
        exceptionType, exceptionValue, exceptionTraceback = sys.exc_info()
        logger.exception("%s failed: %s" % (job, exceptionValue))
        job.fail(exceptionTraceback)
        self.failed()

    def reserve(self, timeout=10):
        logger.debug('checking queues %s' % self.queues)
        job = self.job_class.reserve(self.queues, self.resq, self.__str__(), timeout=timeout)
        if job:
            logger.info('Found job on %s: %s' % (job._queue, job))
            return job

    def working_on(self, job):
        logger.debug('marking as working on')
        data = {
            'queue': job._queue,
            'run_at': str(int(time.mktime(datetime.datetime.now().timetuple()))),
            'payload': job._payload
        }
        data = json.dumps(data)
        self.resq.redis["resque:worker:%s" % str(self)] = data
        logger.debug("worker:%s" % str(self))
        logger.debug(self.resq.redis["resque:worker:%s" % str(self)])

    def done_working(self, job):
        logger.debug('done working on %s', job)
        self.processed()
        self.resq.redis.delete("resque:worker:%s" % str(self))

    def processed(self):
        total_processed = Stat("processed", self.resq)
        worker_processed = Stat("processed:%s" % str(self), self.resq)
        total_processed.incr()
        worker_processed.incr()

    def get_processed(self):
        return Stat("processed:%s" % str(self), self.resq).get()

    def failed(self):
        Stat("failed", self.resq).incr()
        Stat("failed:%s" % self, self.resq).incr()

    def get_failed(self):
        return Stat("failed:%s" % self, self.resq).get()

    def job(self):
        data = self.resq.redis.get("resque:worker:%s" % self)
        if data:
            return ResQ.decode(data)
        return {}


    def processing(self):
        return self.job()

    def state(self):
        if self.resq.redis.exists('resque:worker:%s' % self):
            return 'working'
        return 'idle'

    @classmethod
    def worker_pids(cls):
        """Returns an array of all pids (as strings) of the workers on
        this machine.  Used when pruning dead workers."""
        cmd = "ps -A -o pid,command | grep pyres_worker | grep -v grep"
        output = commands.getoutput(cmd)
        if output:
            return map(lambda l: l.strip().split(' ')[0], output.split("\n"))
        else:
            return []

    @classmethod
    def run(cls, queues, server="localhost:6379", password=None, interval=None, timeout=None):
        worker = cls(queues=queues, server=server, password=password, timeout=timeout)
        if interval is not None:
            worker.work(interval)
        else:
            worker.work()

    @classmethod
    def all(cls, host="localhost:6379"):
        if isinstance(host,string_types):
            resq = ResQ(host)
        elif isinstance(host, ResQ):
            resq = host

        return [Worker.find(w,resq) for w in resq.workers() or []]

    @classmethod
    def working(cls, host):
        if isinstance(host, string_types):
            resq = ResQ(host)
        elif isinstance(host, ResQ):
            resq = host
        total = []
        for key in Worker.all(host):
            total.append('resque:worker:%s' % key)
        names = []
        for key in total:
            value = resq.redis.get(key)
            if value:
                w = Worker.find(key[14:], resq) #resque:worker:
                names.append(w)
        return names

    @classmethod
    def find(cls, worker_id, resq):
        if Worker.exists(worker_id, resq):
            queues = worker_id.split(':')[-1].split(',')
            worker = cls(queues,resq)
            worker.id = worker_id
            return worker
        else:
            return None

    @classmethod
    def exists(cls, worker_id, resq):
        return resq.redis.sismember('resque:workers', worker_id)


try:
    from setproctitle import setproctitle
except ImportError:
    def setproctitle(name):
        pass


if __name__ == "__main__":
    from optparse import OptionParser
    parser = OptionParser()
    parser.add_option("-q", dest="queue_list")
    parser.add_option("-s", dest="server", default="localhost:6379")
    (options,args) = parser.parse_args()
    if not options.queue_list:
        parser.print_help()
        parser.error("Please give each worker at least one queue.")
    queues = options.queue_list.split(',')
    Worker.run(queues, options.server)

########NEW FILE########
__FILENAME__ = test_failure
from tests import PyResTests, Basic
from pyres import failure
from pyres.job import Job

class FailureTests(PyResTests):
    def setUp(self):
        PyResTests.setUp(self)
        self.queue_name = 'basic'
        self.job_class = Basic

    def test_count(self):
        self.resq.enqueue(self.job_class,"test1")
        job = Job.reserve(self.queue_name,self.resq)
        job.fail("problem")
        assert failure.count(self.resq) == 1
        assert self.redis.llen('resque:failed') == 1

    def test_create(self):
        self.resq.enqueue(self.job_class,"test1")
        job = Job.reserve(self.queue_name,self.resq)
        e = Exception('test')
        fail = failure.create(e, self.queue_name, job._payload)
        assert isinstance(fail._payload, dict)
        fail.save(self.resq)
        assert failure.count(self.resq) == 1
        assert self.redis.llen('resque:failed') == 1

    def test_all(self):
        self.resq.enqueue(self.job_class,"test1")
        job = Job.reserve(self.queue_name,self.resq)
        e = Exception('problem')
        job.fail(e)
        assert len(failure.all(self.resq, 0, 20)) == 1

    def test_clear(self):
        self.resq.enqueue(self.job_class,"test1")
        job = Job.reserve(self.queue_name,self.resq)
        e = Exception('problem')
        job.fail(e)
        assert self.redis.llen('resque:failed') == 1
        failure.clear(self.resq)
        assert self.redis.llen('resque:failed') == 0

    def test_requeue(self):
        self.resq.enqueue(self.job_class,"test1")
        job = Job.reserve(self.queue_name,self.resq)
        e = Exception('problem')
        fail_object = job.fail(e)
        assert self.resq.size(self.queue_name) == 0
        failure.requeue(self.resq, fail_object)
        assert self.resq.size(self.queue_name) == 1
        job = Job.reserve(self.queue_name,self.resq)
        assert job._queue == self.queue_name
        mod_with_class = '{module}.{klass}'.format(
            module=self.job_class.__module__,
            klass=self.job_class.__name__)
        self.assertEqual(job._payload, {'class':mod_with_class,'args':['test1'],'enqueue_timestamp': job.enqueue_timestamp})

########NEW FILE########
__FILENAME__ = test_failure_multi
from tests import Basic
from tests.test_failure import FailureTests

from pyres import failure
from pyres.failure.base import BaseBackend
from pyres.failure.multiple import MultipleBackend
from pyres.failure.redis import RedisBackend

# Inner class for the failure backend
class TestBackend(BaseBackend):
    def save(self, resq):
        resq.redis.set('testbackend:called', 1)

failure.backend = MultipleBackend
failure.backend.classes = [RedisBackend, TestBackend]

class BasicMultiBackend(Basic):
    queue = 'basicmultibackend'

class MultipleFailureTests(FailureTests):
    def setUp(self):
        FailureTests.setUp(self)
        self.job_class = BasicMultiBackend
        self.queue_name = 'basicmultibackend'

########NEW FILE########
__FILENAME__ = test_horde
from tests import PyResTests, Basic, TestProcess
from pyres import horde
import os

class KhanTests(PyResTests):
    def test_khan_init(self):
        from pyres.exceptions import NoQueueError
        self.assertRaises(NoQueueError, horde.Khan, 2, [])
        self.assertRaises(ValueError, horde.Khan, 'test', ['test'])
    
    def test_register_khan(self):
        khan = horde.Khan(pool_size=1, queues=['basic'])
        khan.register_khan()
        name = "%s:%s:1" % (os.uname()[1],os.getpid())
        assert self.redis.sismember('resque:khans',name)

    def test_unregister_khan(self):
        khan = horde.Khan(pool_size=1, queues=['basic'])
        khan.register_khan()
        name = "%s:%s:1" % (os.uname()[1],os.getpid())
        assert self.redis.sismember('resque:khans',name)
        assert self.redis.scard('resque:khans') == 1
        khan.unregister_khan()
        assert not self.redis.sismember('resque:khans', name)
        assert self.redis.scard('resque:khans') == 0

    def test_setup_minions(self):
        khan = horde.Khan(pool_size=1, queues=['basic'])
        khan.setup_minions()
        assert len(khan._workers) == 1
        khan._shutdown_minions()

    def test_setup_resq(self):
        khan = horde.Khan(pool_size=1, queues=['basic'])
        assert not hasattr(khan, 'resq')
        khan.setup_resq()
        assert hasattr(khan, 'resq')

    def test_add_minion(self):
        khan = horde.Khan(pool_size=1, queues=['basic'])
        khan.setup_minions()
        khan.register_khan()
        name = "%s:%s:1" % (os.uname()[1],os.getpid())
        assert self.redis.sismember('resque:khans',name)
        khan.add_minion()
        assert len(khan._workers) == 2
        assert not self.redis.sismember('resque:khans',name)
        name = '%s:%s:2' % (os.uname()[1], os.getpid())
        assert khan.pool_size == 2
        assert self.redis.sismember('resque:khans',name)
        khan._shutdown_minions()

    def test_remove_minion(self):
        khan = horde.Khan(pool_size=1, queues=['basic'])
        khan.setup_minions()
        khan.register_khan()
        assert khan.pool_size == 1
        khan._remove_minion()
        assert khan.pool_size == 0

########NEW FILE########
__FILENAME__ = test_jobs
from datetime import datetime
from tests import PyResTests, Basic, TestProcess, ReturnAllArgsJob
from pyres.job import Job
class JobTests(PyResTests):
    def test_reserve(self):
        self.resq.enqueue(Basic,"test1")
        job = Job.reserve('basic', self.resq)
        assert job._queue == 'basic'
        assert job._payload
        self.assertEqual(job._payload, {'class':'tests.Basic','args':['test1'],'enqueue_timestamp':job.enqueue_timestamp})
    
    def test_perform(self):
        self.resq.enqueue(Basic,"test1")
        job = Job.reserve('basic',self.resq)
        self.resq.enqueue(TestProcess)
        job2 = Job.reserve('high', self.resq)
        assert job.perform() == "name:test1"
        assert job2.perform()
    
    def test_fail(self):
        self.resq.enqueue(Basic,"test1")
        job = Job.reserve('basic',self.resq)
        assert self.redis.llen('resque:failed') == 0
        job.fail("problem")
        assert self.redis.llen('resque:failed') == 1

    def test_date_arg_type(self):
        dt = datetime.now().replace(microsecond=0)
        self.resq.enqueue(ReturnAllArgsJob, dt)
        job = Job.reserve('basic',self.resq)
        result = job.perform()
        assert result[0] == dt

########NEW FILE########
__FILENAME__ = test_json
from datetime import datetime
from tests import PyResTests
import pyres.json_parser as json

class JSONTests(PyResTests):
    def test_encode_decode_date(self):
        dt = datetime(1972, 1, 22);
        encoded = json.dumps({'dt': dt})
        decoded = json.loads(encoded)
        assert decoded['dt'] == dt

    def test_dates_in_lists(self):
        dates = [datetime.now() for i in range(50)]
        decoded = json.loads(json.dumps(dates))
        for value in dates:
            assert isinstance(value, datetime)

    def test_dates_in_dict(self):
        dates = dict((i, datetime.now()) for i in range(50))
        decoded = json.loads(json.dumps(dates))
        for i, value in dates.items():
            assert isinstance(i, int)
            assert isinstance(value, datetime)


########NEW FILE########
__FILENAME__ = test_resq
from tests import PyResTests, Basic, TestProcess
from pyres import ResQ
from pyres.worker import Worker
from pyres.job import Job
import os
class ResQTests(PyResTests):
    def test_enqueue(self):
        self.resq.enqueue(Basic,"test1")
        self.resq.enqueue(Basic,"test2", "moretest2args")
        ResQ._enqueue(Basic, "test3")
        assert self.redis.llen("resque:queue:basic") == 3
        assert self.redis.sismember('resque:queues','basic')

    def test_push(self):
        self.resq.push('pushq','content-newqueue')
        self.resq.push('pushq','content2-newqueue')
        assert self.redis.llen('resque:queue:pushq') == 2
        assert self.redis.lindex('resque:queue:pushq', 0).decode() == ResQ.encode('content-newqueue')
        assert self.redis.lindex('resque:queue:pushq', 1).decode() == ResQ.encode('content2-newqueue')

    def test_pop(self):
        self.resq.push('pushq','content-newqueue')
        self.resq.push('pushq','content2-newqueue')
        assert self.redis.llen('resque:queue:pushq') == 2
        assert self.resq.pop('pushq') == ('pushq', 'content-newqueue')
        assert self.redis.llen('resque:queue:pushq') == 1
        assert self.resq.pop(['pushq']) == ('pushq', 'content2-newqueue')
        assert self.redis.llen('resque:queue:pushq') == 0

    def test_pop_two_queues(self):
        self.resq.push('pushq1', 'content-q1-1')
        self.resq.push('pushq1', 'content-q1-2')
        self.resq.push('pushq2', 'content-q2-1')
        assert self.redis.llen('resque:queue:pushq1') == 2
        assert self.redis.llen('resque:queue:pushq2') == 1
        assert self.resq.pop(['pushq1', 'pushq2']) == ('pushq1', 'content-q1-1')
        assert self.redis.llen('resque:queue:pushq1') == 1
        assert self.redis.llen('resque:queue:pushq2') == 1
        assert self.resq.pop(['pushq2', 'pushq1']) == ('pushq2', 'content-q2-1')
        assert self.redis.llen('resque:queue:pushq1') == 1
        assert self.redis.llen('resque:queue:pushq2') == 0
        assert self.resq.pop(['pushq2', 'pushq1']) == ('pushq1', 'content-q1-2')
        assert self.redis.llen('resque:queue:pushq1') == 0
        assert self.redis.llen('resque:queue:pushq2') == 0
        assert self.resq.pop(['pushq1', 'pushq2'], timeout=1) == (None, None)

    def test_peek(self):
        self.resq.enqueue(Basic,"test1")
        self.resq.enqueue(Basic,"test2")
        assert len(self.resq.peek('basic',0,20)) == 2

    def test_size(self):
        self.resq.enqueue(Basic,"test1")
        self.resq.enqueue(Basic,"test2")
        assert self.resq.size('basic') == 2
        assert self.resq.size('noq') == 0

    def test_redis_property(self):
        from redis import Redis
        rq = ResQ(server="localhost:6379")
        red = Redis()
        #rq2 = ResQ(server=red)
        self.assertRaises(Exception, rq.redis,[Basic])

    def test_info(self):
        self.resq.enqueue(Basic,"test1")
        self.resq.enqueue(TestProcess)
        info = self.resq.info()
        assert info['queues'] == 2
        assert info['servers'] == ['localhost:6379']
        assert info['workers'] == 0
        worker = Worker(['basic'])
        worker.register_worker()
        info = self.resq.info()
        assert info['workers'] == 1

    def test_workers(self):
        worker = Worker(['basic'])
        worker.register_worker()
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        assert len(self.resq.workers()) == 1
        #assert Worker.find(name, self.resq) in self.resq.workers()

    def test_enqueue_from_string(self):
        self.resq.enqueue_from_string('tests.Basic','basic','test1')
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        assert self.redis.llen("resque:queue:basic") == 1
        job = Job.reserve('basic', self.resq)
        worker = Worker(['basic'])
        worker.process(job)
        assert not self.redis.get('resque:worker:%s' % worker)
        assert not self.redis.get("resque:stat:failed")
        assert not self.redis.get("resque:stat:failed:%s" % name)

    def test_remove_queue(self):
        self.resq.enqueue_from_string('tests.Basic','basic','test1')
        assert 'basic' in self.resq._watched_queues
        assert self.redis.sismember('resque:queues','basic')
        assert self.redis.llen('resque:queue:basic') == 1
        self.resq.remove_queue('basic')
        assert 'basic' not in self.resq._watched_queues
        assert not self.redis.sismember('resque:queues','basic')
        assert not self.redis.exists('resque:queue:basic')

    def test_keys(self):
        self.resq.enqueue_from_string('tests.Basic','basic','test1')
        assert 'queue:basic' in self.resq.keys()
        assert 'queues' in self.resq.keys()

    def test_queues(self):
        assert self.resq.queues() == []
        self.resq.enqueue_from_string('tests.Basic','basic','test1')
        assert len(self.resq.queues()) == 1
        self.resq.enqueue_from_string('tests.Basic','basic','test1')
        assert len(self.resq.queues()) == 1
        self.resq.enqueue_from_string('tests.Basic','basic2','test1')
        assert len(self.resq.queues()) == 2
        assert 'test' not in self.resq.queues()
        assert 'basic' in self.resq.queues()

    def test_close(self):
        self.resq.close()

########NEW FILE########
__FILENAME__ = test_schedule
from tests import PyResTests, Basic, TestProcess, ErrorObject
from pyres import ResQ
from pyres.job import Job
from pyres.scheduler import Scheduler
import os
import datetime
import time
class ScheduleTests(PyResTests):
    def test_enqueue_at(self):
        d = datetime.datetime.now() + datetime.timedelta(days=1)
        d2 = d + datetime.timedelta(days=1)
        key = int(time.mktime(d.timetuple()))
        key2 = int(time.mktime(d2.timetuple()))
        self.resq.enqueue_at(d, Basic,"test1")
        self.resq.enqueue_at(d, Basic,"test2")
        assert self.redis.llen("resque:delayed:%s" % key) == 2
        assert len(self.redis.zrange('resque:delayed_queue_schedule',0,20)) == 1
        self.resq.enqueue_at(d2, Basic,"test1")
        assert self.redis.llen("resque:delayed:%s" % key2) == 1
        assert len(self.redis.zrange('resque:delayed_queue_schedule',0,20)) == 2
    
    def test_delayed_queue_schedule_size(self):
        d = datetime.datetime.now() + datetime.timedelta(days=1)
        d2 = d + datetime.timedelta(days=1)
        d3 = d
        key = int(time.mktime(d.timetuple()))
        key2 = int(time.mktime(d2.timetuple()))
        self.resq.enqueue_at(d, Basic,"test1")
        self.resq.enqueue_at(d2, Basic,"test1")
        self.resq.enqueue_at(d3, Basic,"test1")
        assert self.resq.delayed_queue_schedule_size() == 3
    
    def test_delayed_timestamp_size(self):
        d = datetime.datetime.now() + datetime.timedelta(days=1)
        d2 = d + datetime.timedelta(days=1)
        key = int(time.mktime(d.timetuple()))
        key2 = int(time.mktime(d2.timetuple()))
        self.resq.enqueue_at(d, Basic,"test1")
        assert self.resq.delayed_timestamp_size(key) == 1
        self.resq.enqueue_at(d, Basic,"test1")
        assert self.resq.delayed_timestamp_size(key) == 2
    
    def test_next_delayed_timestamp(self):
        d = datetime.datetime.now() + datetime.timedelta(days=-1)
        d2 = d + datetime.timedelta(days=-2)
        key = int(time.mktime(d.timetuple()))
        key2 = int(time.mktime(d2.timetuple()))
        self.resq.enqueue_at(d, Basic,"test1")
        self.resq.enqueue_at(d2, Basic,"test1")
        item = self.resq.next_delayed_timestamp()
        assert  int(item) == key2
    
    def test_next_item_for_timestamp(self):
        d = datetime.datetime.now() + datetime.timedelta(days=-1)
        d2 = d + datetime.timedelta(days=-2)
        #key = int(time.mktime(d.timetuple()))
        #key2 = int(time.mktime(d2.timetuple()))
        self.resq.enqueue_at(d, Basic,"test1")
        self.resq.enqueue_at(d2, Basic,"test1")
        timestamp = self.resq.next_delayed_timestamp()
        item = self.resq.next_item_for_timestamp(timestamp)
        assert isinstance(item, dict)
        assert self.redis.zcard('resque:delayed_queue_schedule') == 1
    
    def test_scheduler_init(self):
        scheduler = Scheduler(self.resq)
        assert not scheduler._shutdown
        scheduler = Scheduler('localhost:6379')
        assert not scheduler._shutdown
        self.assertRaises(Exception, Scheduler, Basic)
    
    def test_schedule_shutdown(self):
        scheduler = Scheduler(self.resq)
        scheduler.schedule_shutdown(19,'')
        assert scheduler._shutdown
        

########NEW FILE########
__FILENAME__ = test_stats
from tests import PyResTests
from pyres import Stat
class StatTests(PyResTests):
    def test_incr(self):
        stat_obj = Stat('test_stat', self.resq)
        stat_obj.incr()
        assert self.redis.get('resque:stat:test_stat') == b'1'
        stat_obj.incr()
        assert self.redis.get('resque:stat:test_stat') == b'2'
        stat_obj.incr(2)
        assert self.redis.get('resque:stat:test_stat') == b'4'
    
    def test_decr(self):
        stat_obj = Stat('test_stat', self.resq)
        stat_obj.incr()
        stat_obj.incr()
        assert self.redis.get('resque:stat:test_stat') == b'2'
        stat_obj.decr()
        assert self.redis.get('resque:stat:test_stat') == b'1'
        stat_obj.incr()
        stat_obj.decr(2)
        assert self.redis.get('resque:stat:test_stat') == b'0'
    
    def test_get(self):
        stat_obj = Stat('test_stat', self.resq)
        stat_obj.incr()
        stat_obj.incr()
        assert stat_obj.get() == 2
    
    def test_clear(self):
        stat_obj = Stat('test_stat', self.resq)
        stat_obj.incr()
        stat_obj.incr()
        assert self.redis.exists('resque:stat:test_stat')
        stat_obj.clear()
        assert not self.redis.exists('resque:stat:test_stat')

########NEW FILE########
__FILENAME__ = test_worker
from tests import PyResTests, Basic, TestProcess, ErrorObject, RetryOnExceptionJob, TimeoutJob, CrashJob, PrematureExitJob, PrematureHardExitJob
from pyres import ResQ
from pyres.job import Job
from pyres.scheduler import Scheduler
from pyres.worker import Worker
import os
import time
import datetime


class WorkerTests(PyResTests):
    def test_worker_init(self):
        from pyres.exceptions import NoQueueError
        self.assertRaises(NoQueueError, Worker,[])
        self.assertRaises(Exception, Worker,['test'],TestProcess())

    def test_startup(self):
        worker = Worker(['basic'])
        worker.startup()
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        assert self.redis.sismember('resque:workers',name)
        import signal
        assert signal.getsignal(signal.SIGTERM) == worker.shutdown_all
        assert signal.getsignal(signal.SIGINT) == worker.shutdown_all
        assert signal.getsignal(signal.SIGQUIT) == worker.schedule_shutdown
        assert signal.getsignal(signal.SIGUSR1) == worker.kill_child

    def test_register(self):
        worker = Worker(['basic'])
        worker.register_worker()
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        assert self.redis.sismember('resque:workers',name)

    def test_unregister(self):
        worker = Worker(['basic'])
        worker.register_worker()
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        assert self.redis.sismember('resque:workers',name)
        worker.unregister_worker()
        assert name not in self.redis.smembers('resque:workers')

    def test_working_on(self):
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        self.resq.enqueue(Basic,"test1")
        job = Job.reserve('basic', self.resq)
        worker = Worker(['basic'])
        worker.working_on(job)
        assert self.redis.exists("resque:worker:%s" % name)

    def test_processed(self):
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        worker = Worker(['basic'])
        worker.processed()
        assert self.redis.exists("resque:stat:processed")
        assert self.redis.exists("resque:stat:processed:%s" % name)
        assert self.redis.get("resque:stat:processed").decode() == str(1)
        assert self.redis.get("resque:stat:processed:%s" % name).decode() == str(1)
        assert worker.get_processed() == 1
        worker.processed()
        assert self.redis.get("resque:stat:processed").decode() == str(2)
        assert self.redis.get("resque:stat:processed:%s" % name).decode() == str(2)
        assert worker.get_processed() == 2

    def test_failed(self):
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        worker = Worker(['basic'])
        worker.failed()
        assert self.redis.exists("resque:stat:failed")
        assert self.redis.exists("resque:stat:failed:%s" % name)
        assert self.redis.get("resque:stat:failed").decode() == str(1)
        assert self.redis.get("resque:stat:failed:%s" % name).decode() == str(1)
        assert worker.get_failed() == 1
        worker.failed()
        assert self.redis.get("resque:stat:failed").decode() == str(2)
        assert self.redis.get("resque:stat:failed:%s" % name).decode() == str(2)
        assert worker.get_failed() == 2

    def test_process(self):
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        self.resq.enqueue(Basic,"test1")
        job = Job.reserve('basic', self.resq)
        worker = Worker(['basic'])
        worker.process(job)
        assert not self.redis.get('resque:worker:%s' % worker)
        assert not self.redis.get("resque:stat:failed")
        assert not self.redis.get("resque:stat:failed:%s" % name)
        self.resq.enqueue(Basic,"test1")
        worker.process()
        assert not self.redis.get('resque:worker:%s' % worker)
        assert not self.redis.get("resque:stat:failed")
        assert not self.redis.get("resque:stat:failed:%s" % name)


    def test_signals(self):
        worker = Worker(['basic'])
        worker.startup()
        import inspect, signal
        frame = inspect.currentframe()
        worker.schedule_shutdown(frame, signal.SIGQUIT)
        assert worker._shutdown
        del worker
        worker = Worker(['high'])
        #self.resq.enqueue(TestSleep)
        #worker.work()
        #assert worker.child
        assert not worker.kill_child(frame, signal.SIGUSR1)

    def test_job_failure(self):
        self.resq.enqueue(ErrorObject)
        worker = Worker(['basic'])
        worker.process()
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        assert not self.redis.get('resque:worker:%s' % worker)
        assert self.redis.get("resque:stat:failed").decode() == str(1)
        assert self.redis.get("resque:stat:failed:%s" % name).decode() == str(1)

    def test_get_job(self):
        worker = Worker(['basic'])
        self.resq.enqueue(Basic,"test1")
        job = Job.reserve('basic', self.resq)
        worker.working_on(job)
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        assert worker.job() == ResQ.decode(self.redis.get('resque:worker:%s' % name))
        assert worker.processing() == ResQ.decode(self.redis.get('resque:worker:%s' % name))
        worker.done_working(job)
        w2 = Worker(['basic'])
        print(w2.job())
        assert w2.job() == {}

    def test_working(self):
        worker = Worker(['basic'])
        self.resq.enqueue_from_string('tests.Basic','basic','test1')
        worker.register_worker()
        job = Job.reserve('basic', self.resq)
        worker.working_on(job)
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        workers = Worker.working(self.resq)
        assert len(workers) == 1
        assert str(worker) == str(workers[0])
        assert worker != workers[0]

    def test_started(self):
        import datetime
        worker = Worker(['basic'])
        dt = datetime.datetime.now()
        worker.started = dt
        name = "%s:%s:%s" % (os.uname()[1],os.getpid(),'basic')
        assert self.redis.get('resque:worker:%s:started' % name).decode() == str(int(time.mktime(dt.timetuple())))
        assert worker.started.decode() == str(int(time.mktime(dt.timetuple())))
        worker.started = None
        assert not self.redis.exists('resque:worker:%s:started' % name)

    def test_state(self):
        worker = Worker(['basic'])
        assert worker.state() == 'idle'
        self.resq.enqueue_from_string('tests.Basic','basic','test1')
        worker.register_worker()
        job = Job.reserve('basic', self.resq)
        worker.working_on(job)
        assert worker.state() == 'working'
        worker.done_working(job)
        assert worker.state() == 'idle'

    def test_prune_dead_workers(self):
        worker = Worker(['basic']) # we haven't registered this worker, so the assertion below holds
        assert self.redis.scard('resque:workers') == 0
        self.redis.sadd('resque:workers',"%s:%s:%s" % (os.uname()[1],'1','basic'))
        self.redis.sadd('resque:workers',"%s:%s:%s" % (os.uname()[1],'2','basic'))
        self.redis.sadd('resque:workers',"%s:%s:%s" % (os.uname()[1],'3','basic'))
        assert self.redis.scard('resque:workers') == 3
        worker.prune_dead_workers()
        assert self.redis.scard('resque:workers') == 0
        self.redis.sadd('resque:workers',"%s:%s:%s" % ('host-that-does-not-exist','1','basic'))
        self.redis.sadd('resque:workers',"%s:%s:%s" % ('host-that-does-not-exist','2','basic'))
        self.redis.sadd('resque:workers',"%s:%s:%s" % ('host-that-does-not-exist','3','basic'))
        worker.prune_dead_workers()
        # the assertion below should hold, because the workers we registered above are on a
        # different host, and thus should not be pruned by this process
        assert self.redis.scard('resque:workers') == 3

    def test_retry_on_exception(self):
        now = datetime.datetime.now()
        self.set_current_time(now)
        worker = Worker(['basic'])
        scheduler = Scheduler()

        # queue up a job that will fail for 30 seconds
        self.resq.enqueue(RetryOnExceptionJob,
                now + datetime.timedelta(seconds=30))
        worker.process()
        assert worker.get_failed() == 0

        # check it retries the first time
        self.set_current_time(now + datetime.timedelta(seconds=5))
        scheduler.handle_delayed_items()
        assert None == worker.process()
        assert worker.get_failed() == 0

        # check it runs fine when it's stopped crashing
        self.set_current_time(now + datetime.timedelta(seconds=60))
        scheduler.handle_delayed_items()
        assert True == worker.process()
        assert worker.get_failed() == 0

    def test_kills_stale_workers_after_timeout(self):
        timeout = 1

        worker = Worker(['basic'], timeout=timeout)
        self.resq.enqueue(TimeoutJob, timeout + 1)

        assert worker.get_failed() == 0
        worker.fork_worker(worker.reserve())
        assert worker.get_failed() == 1

    def test_detect_crashed_workers_as_failures(self):
        worker = Worker(['basic'])
        self.resq.enqueue(CrashJob)

        assert worker.job() == {}
        assert worker.get_failed() == 0

        worker.fork_worker(worker.reserve())

        assert worker.job() == {}
        assert worker.get_failed() == 1

    def test_detect_non_0_sys_exit_as_failure(self):
        worker = Worker(['basic'])
        self.resq.enqueue(PrematureExitJob, 9)

        assert worker.job() == {}
        assert worker.get_failed() == 0

        worker.fork_worker(worker.reserve())

        assert worker.job() == {}
        assert worker.get_failed() == 1

    def test_detect_code_0_sys_exit_as_success(self):
        worker = Worker(['basic'])
        self.resq.enqueue(PrematureExitJob, 0)

        assert worker.job() == {}
        assert worker.get_failed() == 0

        worker.fork_worker(worker.reserve())

        assert worker.job() == {}
        assert worker.get_failed() == 0

    def test_detect_non_0_os_exit_as_failure(self):
        worker = Worker(['basic'])
        self.resq.enqueue(PrematureHardExitJob, 9)

        assert worker.job() == {}
        assert worker.get_failed() == 0

        worker.fork_worker(worker.reserve())

        assert worker.job() == {}
        assert worker.get_failed() == 1

    def test_detect_code_0_os_exit_as_success(self):
        worker = Worker(['basic'])
        self.resq.enqueue(PrematureHardExitJob, 0)

        assert worker.job() == {}
        assert worker.get_failed() == 0

        worker.fork_worker(worker.reserve())

        assert worker.job() == {}
        assert worker.get_failed() == 0

    def test_retries_give_up_eventually(self):
        now = datetime.datetime.now()
        self.set_current_time(now)
        worker = Worker(['basic'])
        scheduler = Scheduler()

        # queue up a job that will fail for 60 seconds
        self.resq.enqueue(RetryOnExceptionJob,
                now + datetime.timedelta(seconds=60))
        worker.process()
        assert worker.get_failed() == 0

        # check it retries the first time
        self.set_current_time(now + datetime.timedelta(seconds=5))
        scheduler.handle_delayed_items()
        assert None == worker.process()
        assert worker.get_failed() == 0

        # check it fails when we've been trying too long
        self.set_current_time(now + datetime.timedelta(seconds=20))
        scheduler.handle_delayed_items()
        assert None == worker.process()
        assert worker.get_failed() == 1

    def test_worker_pids(self):
        # spawn worker processes and get pids
        pids = []
        pids.append(self.spawn_worker(['basic']))
        pids.append(self.spawn_worker(['basic']))
        time.sleep(1)
        worker_pids = Worker.worker_pids()

        # send kill signal to workers and wait for them to exit
        import signal
        for pid in pids:
            os.kill(pid, signal.SIGQUIT)
            os.waitpid(pid, 0)

        # ensure worker_pids() returned the correct pids
        for pid in pids:
            assert str(pid) in worker_pids

        # ensure the workers are no longer returned by worker_pids()
        worker_pids = Worker.worker_pids()
        for pid in pids:
            assert str(pid) not in worker_pids

    def spawn_worker(self, queues):
        pid = os.fork()
        if not pid:
            Worker.run(queues, interval=1)
            os._exit(0)
        else:
            return pid

    def set_current_time(self, time):
        ResQ._current_time = staticmethod(lambda: time)

########NEW FILE########
