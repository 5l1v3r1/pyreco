__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# pynsq documentation build configuration file, created by
# sphinx-quickstart on Sun Jun 16 16:18:30 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('..'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'pynsq'
copyright = u'2013, Matt Reiferson and Jehiah Czebotar'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.6'
# The full version, including alpha/beta/rc tags.
release = '0.6.2'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
#keep_warnings = False


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'pynsqdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'pynsq.tex', u'pynsq Documentation',
   u'Matt Reiferson and Jehiah Czebotar', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'pynsq', u'pynsq Documentation',
     [u'Matt Reiferson and Jehiah Czebotar'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'pynsq', u'pynsq Documentation',
   u'Matt Reiferson and Jehiah Czebotar', 'pynsq', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

# If true, do not generate a @detailmenu in the "Top" node's menu.
#texinfo_no_detailmenu = False

########NEW FILE########
__FILENAME__ = all_identify_options
#!/usr/bin/env python
import nsq
import ssl
import tornado.options


def handler(msg):
    print msg.body
    return True


if __name__ == '__main__':
    tornado.options.parse_command_line()
    reader = nsq.Reader(topic='test', channel='test', nsqd_tcp_addresses=['127.0.0.1:4150'],
                        message_handler=handler, heartbeat_interval=10, tls_v1=True,
                        tls_options={'cert_reqs': ssl.CERT_NONE}, snappy=True,
                        output_buffer_size=4096, output_buffer_timeout=100,
                        user_agent='test')
    nsq.run()

########NEW FILE########
__FILENAME__ = nsq_to_nsq
# nsq_to_nsq.py
# Written by Ryder Moody and Jehiah Czebotar.
# Slower than the golang nsq_to_nsq included with nsqd, but useful as a
# starting point for a message transforming client written in python.

import tornado.options
from nsq import Reader, run
from nsq import Writer, Error
import functools
import logging
from host_pool import HostPool

class NSQProxy:
    def __init__(self, topic, nsqds):
        self.topic = topic
        self.writer_pool = HostPool([Writer([nsqd]) for nsqd in nsqds])

    def relay(self, nsq_message):
        nsq_message.enable_async()
        writer = self.writer_pool.get()
        callback = functools.partial(self._on_message_response, nsq_message=nsq_message, writer=writer)
        writer.pub(self.topic, nsq_message.body, callback)

    def _on_message_response(self, conn, data, nsq_message, writer):
        if isinstance(data, Error):
            logging.warning("requeuing message: %s", nsq_message.body)
            self.writer_pool.failed(writer)
            nsq_message.requeue()
        else:
            self.writer_pool.success(writer)
            nsq_message.finish()

if __name__ == "__main__":
    tornado.options.define('destination_topic', type=str)
    tornado.options.define('topic', type=str)
    tornado.options.define('nsqd_tcp_address', type=str, multiple=True)
    tornado.options.define('destination_nsqd_tcp_address', type=str, multiple=True)
    tornado.options.define('lookupd_http_address', type=str, multiple=True)
    tornado.options.define('channel', type=str)
    tornado.options.define('max_in_flight', type=int, default=500)

    tornado.options.parse_command_line()

    assert tornado.options.options.topic
    assert tornado.options.options.destination_nsqd_tcp_address
    assert tornado.options.options.channel

    destination_topic = str(tornado.options.options.destination_topic or tornado.options.options.topic)
    lookupd_http_addresses = map(lambda addr: 'http://' + addr, tornado.options.options.lookupd_http_address)

    proxy = NSQProxy(destination_topic, tornado.options.options.destination_nsqd_tcp_address)

    Reader(
        topic=tornado.options.options.topic,
        channel=tornado.options.options.channel,
        message_handler=proxy.relay,
        max_in_flight=tornado.options.options.max_in_flight,
        lookupd_http_addresses=lookupd_http_addresses,
        nsqd_tcp_addresses=tornado.options.options.nsqd_tcp_address,
    )
    run()

########NEW FILE########
__FILENAME__ = async
import time
import socket
import struct
import logging

from version import __version__

try:
    import ssl
except ImportError:
    ssl = None  # pyflakes.ignore

try:
    from snappy_socket import SnappySocket
except ImportError:
    SnappySocket = None  # pyflakes.ignore

try:
    import simplejson as json
except ImportError:
    import json  # pyflakes.ignore

import tornado.iostream
import tornado.ioloop
import tornado.simple_httpclient

import nsq
from evented_mixin import EventedMixin


class AsyncConn(EventedMixin):
    """
    Low level object representing a TCP connection to nsqd.

    When a message on this connection is requeued and the requeue delay has not been specified,
    it calculates the delay automatically by an increasing multiple of ``requeue_delay``.

    Generates the following events that can be listened to with :meth:`nsq.AsyncConn.on`:

     * ``connect``
     * ``close``
     * ``error``
     * ``identify``
     * ``identify_response``
     * ``heartbeat``
     * ``ready``
     * ``message``
     * ``response``
     * ``backoff``
     * ``resume``

    :param host: the host to connect to

    :param port: the post to connect to

    :param timeout: the timeout for read/write operations (in seconds)

    :param heartbeat_interval: the amount of time in seconds to negotiate with the connected
        producers to send heartbeats (requires nsqd 0.2.19+)

    :param requeue_delay: the base multiple used when calculating requeue delay
        (multiplied by # of attempts)

    :param tls_v1: enable TLS v1 encryption (requires nsqd 0.2.22+)

    :param tls_options: dictionary of options to pass to `ssl.wrap_socket()
        <http://docs.python.org/2/library/ssl.html#ssl.wrap_socket>`_ as ``**kwargs``

    :param snappy: enable Snappy stream compression (requires nsqd 0.2.23+)

    :param output_buffer_size: size of the buffer (in bytes) used by nsqd for buffering writes
        to this connection

    :param output_buffer_timeout: timeout (in ms) used by nsqd before flushing buffered writes
        (set to 0 to disable).  **Warning**: configuring clients with an extremely low (``< 25ms``)
        ``output_buffer_timeout`` has a significant effect on ``nsqd`` CPU usage (particularly
        with ``> 50`` clients connected).

    :param sample_rate: take only a sample of the messages being sent to the client. Not setting
        this or setting it to 0 will ensure you get all the messages destined for the client.
        Sample rate can be greater than 0 or less than 100 and the client will receive that
        percentage of the message traffic. (requires nsqd 0.2.25+)

    :param user_agent: a string identifying the agent for this client in the spirit of
        HTTP (default: ``<client_library_name>/<version>``) (requires nsqd 0.2.25+)
    """
    def __init__(self, host, port, timeout=1.0, heartbeat_interval=30, requeue_delay=90,
                 tls_v1=False, tls_options=None, snappy=False, user_agent=None,
                 output_buffer_size=16 * 1024, output_buffer_timeout=250, sample_rate=0,
                 io_loop=None):
        assert isinstance(host, (str, unicode))
        assert isinstance(port, int)
        assert isinstance(timeout, float)
        assert isinstance(tls_options, (dict, None.__class__))
        assert isinstance(heartbeat_interval, int) and heartbeat_interval >= 1
        assert isinstance(requeue_delay, int) and requeue_delay >= 0
        assert isinstance(output_buffer_size, int) and output_buffer_size >= 0
        assert isinstance(output_buffer_timeout, int) and output_buffer_timeout >= 0
        assert isinstance(sample_rate, int) and sample_rate >= 0 and sample_rate < 100
        assert tls_v1 and ssl or not tls_v1, \
            'tls_v1 requires Python 2.6+ or Python 2.5 w/ pip install ssl'

        self.state = 'INIT'
        self.host = host
        self.port = port
        self.timeout = timeout
        self.last_recv_timestamp = time.time()
        self.last_msg_timestamp = time.time()
        self.in_flight = 0
        self.rdy = 0
        self.rdy_timeout = None
        # for backwards compatibility when interacting with older nsqd
        # (pre 0.2.20), default this to their hard-coded max
        self.max_rdy_count = 2500
        self.tls_v1 = tls_v1
        self.tls_options = tls_options
        self.snappy = snappy
        self.hostname = socket.gethostname()
        self.short_hostname = self.hostname.split('.')[0]
        self.heartbeat_interval = heartbeat_interval * 1000
        self.requeue_delay = requeue_delay
        self.io_loop = io_loop
        if not self.io_loop:
            self.io_loop = tornado.ioloop.IOLoop.instance()

        self.output_buffer_size = output_buffer_size
        self.output_buffer_timeout = output_buffer_timeout
        self.sample_rate = sample_rate
        self.user_agent = user_agent

        if self.user_agent is None:
            self.user_agent = 'pynsq/%s' % __version__

        super(AsyncConn, self).__init__()

    @property
    def id(self):
        return str(self)

    def __str__(self):
        return self.host + ':' + str(self.port)

    def connect(self):
        if self.state not in ['INIT', 'DISCONNECTED']:
            return

        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.socket.settimeout(self.timeout)
        self.socket.setblocking(0)

        self.stream = tornado.iostream.IOStream(self.socket, io_loop=self.io_loop)
        self.stream.set_close_callback(self._socket_close)

        self.state = 'CONNECTING'
        self.on('connect', self._on_connect)
        self.on('data', self._on_data)

        self.stream.connect((self.host, self.port), self._connect_callback)

    def _connect_callback(self):
        self.state = 'CONNECTED'
        self.stream.write(nsq.MAGIC_V2)
        self._start_read()
        self.trigger('connect', conn=self)

    def _start_read(self):
        self.stream.read_bytes(4, self._read_size)

    def _socket_close(self):
        self.state = 'DISCONNECTED'
        self.trigger('close', conn=self)

    def close(self):
        self.stream.close()

    def _read_size(self, data):
        try:
            size = struct.unpack('>l', data)[0]
            self.stream.read_bytes(size, self._read_body)
        except Exception:
            self.close()
            self.trigger('error', conn=self,
                         error=nsq.IntegrityError('failed to unpack size'))

    def _read_body(self, data):
        try:
            self.trigger('data', conn=self, data=data)
        except Exception:
            logging.exception('uncaught exception in data event')
        self.io_loop.add_callback(self._start_read)

    def send(self, data):
        self.stream.write(data)

    def upgrade_to_tls(self, options=None):
        assert ssl, 'tls_v1 requires Python 2.6+ or Python 2.5 w/ pip install ssl'

        # in order to upgrade to TLS we need to *replace* the IOStream...
        #
        # first remove the event handler for the currently open socket
        # so that when we add the socket to the new SSLIOStream below,
        # it can re-add the appropriate event handlers.
        self.io_loop.remove_handler(self.socket.fileno())

        opts = {
            'cert_reqs': ssl.CERT_REQUIRED,
            'ca_certs': tornado.simple_httpclient._DEFAULT_CA_CERTS
        }
        opts.update(options or {})
        self.socket = ssl.wrap_socket(self.socket, ssl_version=ssl.PROTOCOL_TLSv1,
                                      do_handshake_on_connect=False, **opts)

        self.stream = tornado.iostream.SSLIOStream(self.socket, io_loop=self.io_loop)
        self.stream.set_close_callback(self._socket_close)

        # now that the IOStream has been swapped we can kickstart
        # the SSL handshake
        self.stream._do_ssl_handshake()

    def upgrade_to_snappy(self):
        assert SnappySocket, 'snappy requires the python-snappy package'

        # in order to upgrade to Snappy we need to use whatever IOStream
        # is currently in place (normal or SSL)...
        #
        # first read any compressed bytes the existing IOStream might have
        # already buffered and use that to bootstrap the SnappySocket, then
        # monkey patch the existing IOStream by replacing its socket
        # with a wrapper that will automagically handle compression.
        existing_data = self.stream._consume(self.stream._read_buffer_size)
        self.socket = SnappySocket(self.socket)
        self.socket.bootstrap(existing_data)
        self.stream.socket = self.socket

    def send_rdy(self, value):
        try:
            self.send(nsq.ready(value))
        except Exception, e:
            self.close()
            self.trigger('error', conn=self,
                         error=nsq.SendError('failed to send RDY %d' % value, e))
            return False
        self.last_rdy = value
        self.rdy = value
        return True

    def _on_connect(self, **kwargs):
        identify_data = {
            'short_id': self.short_hostname,
            'long_id': self.hostname,
            'heartbeat_interval': self.heartbeat_interval,
            'feature_negotiation': True,
            'tls_v1': self.tls_v1,
            'snappy': self.snappy,
            'output_buffer_timeout': self.output_buffer_timeout,
            'output_buffer_size': self.output_buffer_size,
            'sample_rate': self.sample_rate,
            'user_agent': self.user_agent
        }
        self.trigger('identify', conn=self, data=identify_data)
        self.on('response', self._on_identify_response)
        try:
            self.send(nsq.identify(identify_data))
        except Exception, e:
            self.close()
            self.trigger('error', conn=self,
                         error=nsq.SendError('failed to bootstrap connection', e))

    def _on_identify_response(self, data, **kwargs):
        self.off('response', self._on_identify_response)

        if data == 'OK':
            return self.trigger('ready', conn=self)

        try:
            data = json.loads(data)
        except ValueError:
            self.close()
            err = 'failed to parse IDENTIFY response JSON from nsqd - %r' % data
            self.trigger('error', conn=self, error=nsq.IntegrityError(err))
            return

        self.trigger('identify_response', conn=self, data=data)

        self._features_to_enable = []
        if self.tls_v1 and data.get('tls_v1'):
            self._features_to_enable.append('tls_v1')
        if self.snappy and data.get('snappy'):
            self._features_to_enable.append('snappy')

        self.on('response', self._on_response_continue)
        self._on_response_continue(conn=self, data=None)

    def _on_response_continue(self, data, **kwargs):
        if self._features_to_enable:
            feature = self._features_to_enable.pop(0)
            if feature == 'tls_v1':
                self.upgrade_to_tls(self.tls_options)
            elif feature == 'snappy':
                self.upgrade_to_snappy()
            return

        self.off('response', self._on_response_continue)
        self.trigger('ready', conn=self)

    def _on_data(self, data, **kwargs):
        self.last_recv_timestamp = time.time()
        frame, data = nsq.unpack_response(data)
        if frame == nsq.FRAME_TYPE_MESSAGE:
            self.last_msg_timestamp = time.time()
            self.rdy = max(self.rdy - 1, 0)
            self.in_flight += 1

            message = nsq.decode_message(data)
            message.on('finish', self._on_message_finish)
            message.on('requeue', self._on_message_requeue)
            message.on('touch', self._on_message_touch)

            self.trigger('message', conn=self, message=message)
        elif frame == nsq.FRAME_TYPE_RESPONSE and data == '_heartbeat_':
            self.send(nsq.nop())
            self.trigger('heartbeat', conn=self)
        elif frame == nsq.FRAME_TYPE_RESPONSE:
            self.trigger('response', conn=self, data=data)
        elif frame == nsq.FRAME_TYPE_ERROR:
            self.trigger('error', conn=self, error=nsq.Error(data))

    def _on_message_requeue(self, message, backoff=True, time_ms=-1, **kwargs):
        self.in_flight -= 1
        try:
            time_ms = self.requeue_delay * message.attempts * 1000 if time_ms < 0 else time_ms
            self.send(nsq.requeue(message.id, time_ms))
        except Exception, e:
            self.close()
            self.trigger('error', conn=self, error=nsq.SendError(
                'failed to send REQ %s @ %d' % (message.id, time_ms), e))

        if backoff:
            self.trigger('backoff', conn=self)

    def _on_message_finish(self, message, **kwargs):
        self.in_flight -= 1
        try:
            self.send(nsq.finish(message.id))
        except Exception, e:
            self.close()
            self.trigger('error', conn=self,
                         error=nsq.SendError('failed to send FIN %s' % message.id, e))

        self.trigger('resume', conn=self)

    def _on_message_touch(self, message, **kwargs):
        try:
            self.send(nsq.touch(message.id))
        except Exception, e:
            self.close()
            self.trigger('error', conn=self,
                         error=nsq.SendError('failed to send TOUCH %s' % message.id, e))

########NEW FILE########
__FILENAME__ = backoff_timer
from decimal import Decimal


def _Decimal(v):
    if not isinstance(v, Decimal):
        return Decimal(str(v))
    return v


class BackoffTimer(object):
    """
    This is a timer that is smart about backing off exponentially when there are problems
    """
    def __init__(self, min_interval, max_interval, ratio=.25, short_length=10, long_length=250):
        assert isinstance(min_interval, (int, float, Decimal))
        assert isinstance(max_interval, (int, float, Decimal))

        self.min_interval = _Decimal(min_interval)
        self.max_interval = _Decimal(max_interval)

        self.max_short_timer = (self.max_interval - self.min_interval) * _Decimal(ratio)
        self.max_long_timer = (self.max_interval - self.min_interval) * (1 - _Decimal(ratio))
        self.short_unit = self.max_short_timer / _Decimal(short_length)
        self.long_unit = self.max_long_timer / _Decimal(long_length)

        self.short_interval = Decimal(0)
        self.long_interval = Decimal(0)

    def success(self):
        """Update the timer to reflect a successfull call"""
        self.short_interval -= self.short_unit
        self.long_interval -= self.long_unit
        self.short_interval = max(self.short_interval, Decimal(0))
        self.long_interval = max(self.long_interval, Decimal(0))

    def failure(self):
        """Update the timer to reflect a failed call"""
        self.short_interval += self.short_unit
        self.long_interval += self.long_unit
        self.short_interval = min(self.short_interval, self.max_short_timer)
        self.long_interval = min(self.long_interval, self.max_long_timer)

    def get_interval(self):
        return float(self.min_interval + self.short_interval + self.long_interval)

########NEW FILE########
__FILENAME__ = client
import time
import logging

import tornado.ioloop


class Client(object):
    def __init__(self, io_loop=None, **kwargs):
        self.io_loop = io_loop
        if not self.io_loop:
            self.io_loop = tornado.ioloop.IOLoop.instance()

        tornado.ioloop.PeriodicCallback(self._check_last_recv_timestamps,
                                        60 * 1000,
                                        io_loop=self.io_loop).start()

    def _on_connection_identify(self, conn, data, **kwargs):
        logging.info('[%s:%s] IDENTIFY sent %r' % (conn.id, self.name, data))

    def _on_connection_identify_response(self, conn, data, **kwargs):
        logging.info('[%s:%s] IDENTIFY received %r' % (conn.id, self.name, data))
        if conn.tls_v1 and not data.get('tls_v1'):
            logging.warning('[%s:%s] tls_v1 requested but disabled, could not negotiate feature',
                            conn.id, self.name)
        if conn.snappy and not data.get('snappy'):
            logging.warning('[%s:%s] snappy requested but disabled, could not negotiate feature',
                            conn.id, self.name)

    def _on_connection_error(self, conn, error, **kwargs):
        logging.error('[%s:%s] ERROR: %r', conn.id, self.name, error)

    def _check_last_recv_timestamps(self):
        now = time.time()

        def is_stale(conn):
            timestamp = conn.last_recv_timestamp
            return (now - timestamp) > ((conn.heartbeat_interval * 2) / 1000.0)

        # first get the list of stale connections, then close
        # (`conn.close()` may modify the list of connections while we're iterating)
        stale_connections = [conn for conn in self.conns.values() if is_stale(conn)]
        for conn in stale_connections:
            timestamp = conn.last_recv_timestamp
            # this connection hasnt received data for more than
            # the configured heartbeat interval, close it
            logging.warning('[%s:%s] connection is stale (%.02fs), closing',
                            conn.id, self.name, (now - timestamp))
            conn.close()

    def _on_heartbeat(self, conn):
        logging.info('[%s:%s] received heartbeat' % (conn.id, self.name))
        self.heartbeat(conn)

    def heartbeat(self, conn):
        """
        Called whenever a heartbeat has been received

        This is useful to subclass and override to perform an action based on liveness (for
        monitoring, etc.)

        :param conn: the :class:`nsq.AsyncConn` over which the heartbeat was received
        """
        pass

########NEW FILE########
__FILENAME__ = evented_mixin
from collections import defaultdict


class DuplicateListenerError(Exception):
    pass


class InvalidListenerError(Exception):
    pass


class EventedMixin(object):
    """
    Provides methods to trigger and listen for arbitrary events named as strings.
    """
    def __init__(self, *args, **kwargs):
        self.__listeners = defaultdict(list)

    def on(self, name, callback):
        """
        Listen for the named event with the specified callback.

        :param name: the name of the event
        :type name: string

        :param callback: the callback to execute when the event is triggered
        :type callback: callable
        """
        assert callable(callback), 'callback is not callable'
        if callback in self.__listeners[name]:
            raise DuplicateListenerError
        self.__listeners[name].append(callback)

    def off(self, name, callback):
        """
        Stop listening for the named event via the specified callback.

        :param name: the name of the event
        :type name: string

        :param callback: the callback that was originally used
        :type callback: callable
        """
        if callback not in self.__listeners[name]:
            raise InvalidListenerError
        self.__listeners[name].remove(callback)

    def trigger(self, name, *args, **kwargs):
        """
        Execute the callbacks for the listeners on the specified event with the
        supplied arguments.

        All extra arguments are passed through to each callback.

        :param name: the name of the event
        :type name: string
        """
        for ev in self.__listeners[name]:
            ev(*args, **kwargs)

########NEW FILE########
__FILENAME__ = legacy_reader
import warnings
from reader import Reader


class LegacyReader(object):
    """
    In ``v0.5.0`` we dropped support for "tasks" in the :class:`nsq.Reader` API in
    favor of a single message handler.

    ``LegacyReader`` is a backwards compatible API for clients interacting with ``v0.5.0+`` that
    want to continue to use "tasks".

    Usage::

        from nsq import LegacyReader as Reader
    """
    def __init__(self, *args, **kwargs):
        warnings.warn('LegacyReader is a deprecated wrapper and will be removed in a future' +
                      ' release.  Use (multiple) Reader(s) each with their own' +
                      ' message handler.', DeprecationWarning)

        old_params = {}

        old_kwargs = ('all_tasks', 'topic', 'channel'
                      'nsqd_tcp_addresses', 'lookupd_http_addresses'
                      'max_tries', 'max_in_flight', 'requeue_delay', 'lookupd_poll_interval',
                      'low_rdy_idle_timeout', 'heartbeat_interval', 'max_backoff_duration')

        if args:
            keys = old_kwargs[:len(args)]
            old_params = dict(zip(keys, args))

        old_params.update(kwargs)

        all_tasks = old_params['all_tasks']
        topic = old_params['topic']
        channel = old_params['channel']

        del old_params['all_tasks']
        del old_params['topic']
        del old_params['channel']

        assert isinstance(all_tasks, dict)
        for key, method in all_tasks.items():
            assert callable(method), 'key %s must have a callable value' % key

        self.readers = []
        for task, method in all_tasks.items():
            if len(all_tasks) > 1:
                task_channel = channel + '.' + task
            else:
                task_channel = channel

            r = Reader(topic=topic, channel=task_channel, message_handler=method, **old_params)
            self.readers.append(r)

########NEW FILE########
__FILENAME__ = message
from evented_mixin import EventedMixin


class Message(EventedMixin):
    """
    A class representing a message received from ``nsqd``.

    If you want to perform asynchronous message processing use the
    :meth:`nsq.Message.enable_async` method, pass the message around,
    and respond using the appropriate instance method.

    Generates the following events that can be listened to with :meth:`nsq.Message.on`:

     * ``finish``
     * ``requeue``
     * ``touch``

    NOTE: A calling a message's :meth:`nsq.Message.finish()` and :meth:`nsq.Message.requeue()`
    methods positively and negatively impact the backoff state, respectively.  However,
    sending the ``backoff=False`` keyword argument to :meth:`nsq.Message.requeue()` is
    considered neutral and will not impact backoff state.

    :param id: the ID of the message
    :type id: string

    :param body: the raw message body
    :type body: string

    :param timestamp: the timestamp the message was produced
    :type timestamp: int

    :param attempts: the number of times this message was attempted
    :type attempts: int

    :ivar id: the ID of the message (from the parameter).
    :type id: string

    :ivar body: the raw message body (from the parameter).
    :type body: string

    :ivar timestamp: the timestamp the message was produced
                     (from the parameter).
    :type timestamp: int

    :ivar attempts: the number of times this message was attempted
                    (from the parameter).
    :type attempts: int
    """
    def __init__(self, id, body, timestamp, attempts):
        self._async_enabled = False
        self._has_responded = False
        self.id = id
        self.body = body
        self.timestamp = timestamp
        self.attempts = attempts

        super(Message, self).__init__()

    def enable_async(self):
        """
        Enables asynchronous processing for this message.

        :class:`nsq.Reader` will not automatically respond to the message
        upon return of ``message_handler``.
        """
        self._async_enabled = True

    def is_async(self):
        """
        Returns whether or not asynchronous processing has been enabled.
        """
        return self._async_enabled

    def has_responded(self):
        """
        Returns whether or not this message has been responded to.
        """
        return self._has_responded

    def finish(self):
        """
        Respond to ``nsqd`` that you've processed this message successfully (or would like
        to silently discard it).
        """
        assert not self._has_responded
        self._has_responded = True
        self.trigger('finish', message=self)

    def requeue(self, **kwargs):
        """
        Respond to ``nsqd`` that you've failed to process this message successfully (and would
        like it to be requeued).

        :param backoff: whether or not :class:`nsq.Reader` should apply backoff handling
        :type backoff: bool

        :param delay: the amount of time (in seconds) that this message should be delayed
            if -1 it will be calculated based on # of attempts
        :type delay: int
        """
        assert not self._has_responded
        self._has_responded = True
        self.trigger('requeue', message=self, **kwargs)

    def touch(self):
        """
        Respond to ``nsqd`` that you need more time to process the message.
        """
        assert not self._has_responded
        self.trigger('touch', message=self)

########NEW FILE########
__FILENAME__ = nsq
import struct
import re

try:
    import simplejson as json
except ImportError:
    import json  # pyflakes.ignore

from message import Message


MAGIC_V2 = '  V2'
NL = '\n'

FRAME_TYPE_RESPONSE = 0
FRAME_TYPE_ERROR = 1
FRAME_TYPE_MESSAGE = 2


class Error(Exception):
    pass


class SendError(Error):
    def __init__(self, msg, error=None):
        self.msg = msg
        self.error = error

    def __str__(self):
        return 'SendError: %s (%s)' % (self.msg, self.error)


class ConnectionClosedError(Error):
    pass


class IntegrityError(Error):
    pass


def unpack_response(data):
    frame = struct.unpack('>l', data[:4])[0]
    return frame, data[4:]


def decode_message(data):
    timestamp = struct.unpack('>q', data[:8])[0]
    attempts = struct.unpack('>h', data[8:10])[0]
    id = data[10:26]
    body = data[26:]
    return Message(id, body, timestamp, attempts)


def _command(cmd, body, *params):
    body_data = ''
    params_data = ''
    if body:
        assert isinstance(body, str), 'body must be a string'
        body_data = struct.pack('>l', len(body)) + body
    if len(params):
        params = [p.encode('utf-8') if isinstance(p, unicode) else p for p in params]
        params_data = ' ' + ' '.join(params)
    return '%s%s%s%s' % (cmd, params_data, NL, body_data)


def subscribe(topic, channel):
    assert valid_topic_name(topic)
    assert valid_channel_name(channel)
    return _command('SUB', None, topic, channel)


def identify(data):
    return _command('IDENTIFY', json.dumps(data))


def ready(count):
    assert isinstance(count, int), 'ready count must be an integer'
    assert count >= 0, 'ready count cannot be negative'
    return _command('RDY', None, str(count))


def finish(id):
    return _command('FIN', None, id)


def requeue(id, time_ms=0):
    assert isinstance(time_ms, int), 'requeue time_ms must be an integer'
    return _command('REQ', None, id, str(time_ms))


def touch(id):
    return _command('TOUCH', None, id)


def nop():
    return _command('NOP', None)


def pub(topic, data):
    return _command('PUB', data, topic)


def mpub(topic, data):
    assert isinstance(data, (set, list))
    body = struct.pack('>l', len(data))
    for m in data:
        body += struct.pack('>l', len(m)) + m
    return _command('MPUB', body, topic)


def valid_topic_name(topic):
    if not 0 < len(topic) < 65:
        return False
    if re.match(r'^[\.a-zA-Z0-9_-]+$', topic):
        return True
    return False


def valid_channel_name(channel):
    if not 0 < len(channel) < 65:
        return False
    if re.match(r'^[\.a-zA-Z0-9_-]+(#ephemeral)?$', channel):
        return True
    return False

########NEW FILE########
__FILENAME__ = reader
import logging
import time
import functools
import urllib
import random

try:
    import simplejson as json
except ImportError:
    import json  # pyflakes.ignore

import tornado.ioloop
import tornado.httpclient

from backoff_timer import BackoffTimer
from client import Client
import nsq
import async


class Reader(Client):
    """
    Reader provides high-level functionality for building robust NSQ consumers in Python
    on top of the async module.

    Reader receives messages over the specified ``topic/channel`` and calls ``message_handler``
    for each message (up to ``max_tries``).

    Multiple readers can be instantiated in a single process (to consume from multiple
    topics/channels at once).

    Supports various hooks to modify behavior when heartbeats are received, to temporarily
    disable the reader, and pre-process/validate messages.

    When supplied a list of ``nsqlookupd`` addresses, it will periodically poll those
    addresses to discover new producers of the specified ``topic``.

    It maintains a sufficient RDY count based on the # of producers and your configured
    ``max_in_flight``.

    Handlers should be defined as shown in the examples below. The handler receives a
    :class:`nsq.Message` object that has instance methods :meth:`nsq.Message.finish`,
    :meth:`nsq.Message.requeue`, and :meth:`nsq.Message.touch` to respond to ``nsqd``.

    When messages are not responded to explicitly, it is responsible for sending
    ``FIN`` or ``REQ`` commands based on return value of  ``message_handler``. When
    re-queueing, it will backoff from processing additional messages for an increasing
    delay (calculated exponentially based on consecutive failures up to ``max_backoff_duration``).

    Synchronous example::

        import nsq

        def handler(message):
            print message
            return True

        r = nsq.Reader(message_handler=handler,
                lookupd_http_addresses=['http://127.0.0.1:4161'],
                topic='nsq_reader', channel='asdf', lookupd_poll_interval=15)
        nsq.run()

    Asynchronous example::

        import nsq

        buf = []

        def process_message(message):
            global buf
            message.enable_async()
            # cache the message for later processing
            buf.append(message)
            if len(buf) >= 3:
                for msg in buf:
                    print msg
                    msg.finish()
                buf = []
            else:
                print 'deferring processing'

        r = nsq.Reader(message_handler=process_message,
                lookupd_http_addresses=['http://127.0.0.1:4161'],
                topic='nsq_reader', channel='async', max_in_flight=9)
        nsq.run()

    :param message_handler: the callable that will be executed for each message received

    :param topic: specifies the desired NSQ topic

    :param channel: specifies the desired NSQ channel

    :param name: a string that is used for logging messages (defaults to 'topic:channel')

    :param nsqd_tcp_addresses: a sequence of string addresses of the nsqd instances this reader
        should connect to

    :param lookupd_http_addresses: a sequence of string addresses of the nsqlookupd instances this
        reader should query for producers of the specified topic

    :param max_tries: the maximum number of attempts the reader will make to process a message after
        which messages will be automatically discarded

    :param max_in_flight: the maximum number of messages this reader will pipeline for processing.
        this value will be divided evenly amongst the configured/discovered nsqd producers

    :param lookupd_poll_interval: the amount of time in seconds between querying all of the supplied
        nsqlookupd instances.  a random amount of time based on thie value will be initially
        introduced in order to add jitter when multiple readers are running

    :param lookupd_poll_jitter: The maximum fractional amount of jitter to add to the
        lookupd pool loop. This helps evenly distribute requests even if multiple consumers
        restart at the same time.

    :param low_rdy_idle_timeout: the amount of time in seconds to wait for a message from a producer
        when in a state where RDY counts are re-distributed (ie. max_in_flight < num_producers)

    :param max_backoff_duration: the maximum time we will allow a backoff state to last in seconds

    :param \*\*kwargs: passed to :class:`nsq.AsyncConn` initialization
    """
    def __init__(self, topic, channel, message_handler=None, name=None,
                 nsqd_tcp_addresses=None, lookupd_http_addresses=None,
                 max_tries=5, max_in_flight=1, lookupd_poll_interval=60,
                 low_rdy_idle_timeout=10, max_backoff_duration=128, lookupd_poll_jitter=0.3,
                 **kwargs):
        super(Reader, self).__init__(**kwargs)

        assert isinstance(topic, (str, unicode)) and len(topic) > 0
        assert isinstance(channel, (str, unicode)) and len(channel) > 0
        assert isinstance(max_in_flight, int) and max_in_flight > 0
        assert isinstance(max_backoff_duration, (int, float)) and max_backoff_duration > 0
        assert isinstance(name, (str, unicode, None.__class__))
        assert isinstance(lookupd_poll_interval, int)
        assert isinstance(lookupd_poll_jitter, float)
        assert lookupd_poll_jitter >= 0 and lookupd_poll_jitter <= 1

        if nsqd_tcp_addresses:
            if not isinstance(nsqd_tcp_addresses, (list, set, tuple)):
                assert isinstance(nsqd_tcp_addresses, (str, unicode))
                nsqd_tcp_addresses = [nsqd_tcp_addresses]
        else:
            nsqd_tcp_addresses = []

        if lookupd_http_addresses:
            if not isinstance(lookupd_http_addresses, (list, set, tuple)):
                assert isinstance(lookupd_http_addresses, (str, unicode))
                lookupd_http_addresses = [lookupd_http_addresses]
            random.shuffle(lookupd_http_addresses)
        else:
            lookupd_http_addresses = []

        assert nsqd_tcp_addresses or lookupd_http_addresses

        self.name = name or (topic + ':' + channel)
        self.message_handler = None
        if message_handler:
            self.set_message_handler(message_handler)
        self.topic = topic
        self.channel = channel
        self.nsqd_tcp_addresses = nsqd_tcp_addresses
        self.lookupd_http_addresses = lookupd_http_addresses
        self.lookupd_query_index = 0
        self.max_tries = max_tries
        self.max_in_flight = max_in_flight
        self.low_rdy_idle_timeout = low_rdy_idle_timeout
        self.total_rdy = 0
        self.need_rdy_redistributed = False
        self.lookupd_poll_interval = lookupd_poll_interval
        self.lookupd_poll_jitter = lookupd_poll_jitter
        self.random_rdy_ts = time.time()
        self.conn_kwargs = kwargs

        self.backoff_timer = BackoffTimer(0, max_backoff_duration)
        self.backoff_timeout = None
        self.backoff_block = False

        self.conns = {}
        self.connection_attempts = {}
        self.http_client = tornado.httpclient.AsyncHTTPClient(io_loop=self.io_loop)

        # will execute when run() is called (for all Reader instances)
        self.io_loop.add_callback(self._run)

    def _run(self):
        assert self.message_handler, "you must specify the Reader's message_handler"

        logging.info('[%s] starting reader for %s/%s...', self.name, self.topic, self.channel)

        for addr in self.nsqd_tcp_addresses:
            address, port = addr.split(':')
            self.connect_to_nsqd(address, int(port))

        tornado.ioloop.PeriodicCallback(self._redistribute_rdy_state,
                                        5 * 1000,
                                        io_loop=self.io_loop).start()

        if not self.lookupd_http_addresses:
            return
        # trigger the first lookup query manually
        self.query_lookupd()

        periodic = tornado.ioloop.PeriodicCallback(self.query_lookupd,
                                                   self.lookupd_poll_interval * 1000,
                                                   io_loop=self.io_loop)

        # randomize the time we start this poll loop so that all
        # consumers don't query at exactly the same time
        delay = random.random() * self.lookupd_poll_interval * self.lookupd_poll_jitter
        self.io_loop.add_timeout(time.time() + delay, periodic.start)

    def set_message_handler(self, message_handler):
        """
        Assigns the callback method to be executed for each message received

        :param message_handler: a callable that takes a single argument
        """
        assert callable(message_handler), 'message_handler must be callable'
        self.message_handler = message_handler

    def _connection_max_in_flight(self):
        return max(1, self.max_in_flight / max(1, len(self.conns)))

    def is_starved(self):
        """
        Used to identify when buffered messages should be processed and responded to.

        When max_in_flight > 1 and you're batching messages together to perform work
        is isn't possible to just compare the len of your list of buffered messages against
        your configured max_in_flight (because max_in_flight may not be evenly divisible
        by the number of producers you're connected to, ie. you might never get that many
        messages... it's a *max*).

        Example::

            def message_handler(self, nsq_msg, reader):
                # buffer messages
                if reader.is_starved():
                    # perform work

            reader = nsq.Reader(...)
            reader.set_message_handler(functools.partial(message_handler, reader=reader))
            nsq.run()
        """
        for conn in self.conns.itervalues():
            if conn.in_flight > 0 and conn.in_flight >= (conn.last_rdy * 0.85):
                return True
        return False

    def _on_message(self, conn, message, **kwargs):
        try:
            self._handle_message(conn, message)
        except Exception:
            logging.exception('[%s:%s] failed to handle_message() %r', conn.id, self.name, message)

    def _handle_message(self, conn, message):
        self.total_rdy = max(self.total_rdy - 1, 0)

        rdy_conn = conn
        if len(self.conns) > self.max_in_flight:
            # if all connections aren't getting RDY
            # occsionally randomize which connection gets RDY
            time_since_random_rdy = time.time() - self.random_rdy_ts
            if time_since_random_rdy > 30:
                self.random_rdy_ts = time.time()
                conns_with_no_rdy = [c for c in self.conns.itervalues() if not c.rdy]
                rdy_conn = random.choice(conns_with_no_rdy)
                if rdy_conn is not conn:
                    logging.info('[%s:%s] redistributing RDY to %s',
                                 conn.id, self.name, rdy_conn.id)

        self._maybe_update_rdy(rdy_conn)

        success = False
        try:
            pre_processed_message = self.preprocess_message(message)
            if not self.validate_message(pre_processed_message):
                return message.finish()
            if message.attempts > self.max_tries:
                self.giving_up(message)
                return message.finish()
            success = self.process_message(message)
        except Exception:
            logging.exception('[%s:%s] uncaught exception while handling message %s body:%r',
                              conn.id, self.name, message.id, message.body)
            if not message.has_responded():
                return message.requeue()

        if not message.is_async() and not message.has_responded():
            assert success is not None, 'ambiguous return value for synchronous mode'
            if success:
                return message.finish()
            return message.requeue()

    def _maybe_update_rdy(self, conn):
        if self.backoff_timer.get_interval():
            return

        if conn.rdy <= 1 or conn.rdy < int(conn.last_rdy * 0.25):
            self._send_rdy(conn, self._connection_max_in_flight())

    def _finish_backoff_block(self):
        self.backoff_timeout = None
        self.backoff_block = False

        # test the waters after finishing a backoff round
        # if we have no connections, this will happen when a new connection gets RDY 1
        if self.conns:
            conn = random.choice(self.conns.values())
            logging.info('[%s:%s] testing backoff state with RDY 1', conn.id, self.name)
            self._send_rdy(conn, 1)

            # for tests
            return conn

    def _on_backoff_resume(self, success, **kwargs):
        start_backoff_interval = self.backoff_timer.get_interval()
        if success:
            self.backoff_timer.success()
        elif not self.backoff_block:
            self.backoff_timer.failure()
        self._enter_continue_or_exit_backoff(start_backoff_interval)

    def _enter_continue_or_exit_backoff(self, start_backoff_interval):
        # Take care of backoff in the appropriate cases.  When this
        # happens, we set a failure on the backoff timer and set the RDY count to zero.
        # Once the backoff time has expired, we allow *one* of the connections let
        # a single message through to test the water.  This will continue until we
        # reach no backoff in which case we go back to the normal RDY count.

        current_backoff_interval = self.backoff_timer.get_interval()

        # do nothing
        if self.backoff_block:
            return

        # we're out of backoff completely, return to full blast for all conns
        if start_backoff_interval and not current_backoff_interval:
            rdy = self._connection_max_in_flight()
            logging.info('[%s] backoff complete, resuming normal operation (%d connections)',
                         self.name, len(self.conns))
            for c in self.conns.values():
                self._send_rdy(c, rdy)
            return

        # enter or continue a backoff iteration
        if current_backoff_interval:
            self._start_backoff_block()

    def _start_backoff_block(self):
        self.backoff_block = True
        backoff_interval = self.backoff_timer.get_interval()

        logging.info('[%s] backing off for %0.2f seconds (%d connections)',
                     self.name, backoff_interval, len(self.conns))
        for c in self.conns.values():
            self._send_rdy(c, 0)

        self.backoff_timeout = self.io_loop.add_timeout(time.time() + backoff_interval,
                                                        self._finish_backoff_block)

    def _rdy_retry(self, conn, value):
        conn.rdy_timeout = None
        self._send_rdy(conn, value)

    def _send_rdy(self, conn, value):
        if conn.rdy_timeout:
            self.io_loop.remove_timeout(conn.rdy_timeout)
            conn.rdy_timeout = None

        if value and self.disabled():
            logging.info('[%s:%s] disabled, delaying RDY state change', conn.id, self.name)
            rdy_retry_callback = functools.partial(self._rdy_retry, conn, value)
            conn.rdy_timeout = self.io_loop.add_timeout(time.time() + 15, rdy_retry_callback)
            return

        if value > conn.max_rdy_count:
            value = conn.max_rdy_count

        if (self.total_rdy + value) > self.max_in_flight:
            if not conn.rdy:
                # if we're going from RDY 0 to non-0 and we couldn't because
                # of the configured max in flight, try again
                rdy_retry_callback = functools.partial(self._rdy_retry, conn, value)
                conn.rdy_timeout = self.io_loop.add_timeout(time.time() + 5, rdy_retry_callback)
            return

        if conn.send_rdy(value):
            self.total_rdy = max(self.total_rdy - conn.rdy + value, 0)

    def connect_to_nsqd(self, host, port):
        """
        Adds a connection to ``nsqd`` at the specified address.

        :param host: the address to connect to
        :param port: the port to connect to
        """
        assert isinstance(host, (str, unicode))
        assert isinstance(port, int)

        conn = async.AsyncConn(host, port, **self.conn_kwargs)
        conn.on('identify', self._on_connection_identify)
        conn.on('identify_response', self._on_connection_identify_response)
        conn.on('error', self._on_connection_error)
        conn.on('close', self._on_connection_close)
        conn.on('ready', self._on_connection_ready)
        conn.on('message', self._on_message)
        conn.on('heartbeat', self._on_heartbeat)
        conn.on('backoff', functools.partial(self._on_backoff_resume, success=False))
        conn.on('resume', functools.partial(self._on_backoff_resume, success=True))

        if conn.id in self.conns:
            return

        # only attempt to re-connect once every 10s per destination
        # this throttles reconnects to failed endpoints
        now = time.time()
        last_connect_attempt = self.connection_attempts.get(conn.id)
        if last_connect_attempt and last_connect_attempt > now - 10:
            return
        self.connection_attempts[conn.id] = now

        logging.info('[%s:%s] connecting to nsqd', conn.id, self.name)
        conn.connect()

        return conn

    def _on_connection_ready(self, conn, **kwargs):
        conn.send(nsq.subscribe(self.topic, self.channel))
        # re-check to make sure another connection didn't beat this one done
        if conn.id in self.conns:
            logging.warning(
                '[%s:%s] connected to NSQ but anothermatching connection already exists',
                conn.id, self.name)
            conn.close()
            return

        if conn.max_rdy_count < self.max_in_flight:
            logging.warning(
                '[%s:%s] max RDY count %d < reader max in flight %d, truncation possible',
                conn.id, self.name, conn.max_rdy_count, self.max_in_flight)

        self.conns[conn.id] = conn
        # we send an initial RDY of 1 up to our configured max_in_flight
        # this resolves two cases:
        #    1. `max_in_flight >= num_conns` ensuring that no connections are ever
        #       *initially* starved since redistribute won't apply
        #    2. `max_in_flight < num_conns` ensuring that we never exceed max_in_flight
        #       and rely on the fact that redistribute will handle balancing RDY across conns
        if not self.backoff_timer.get_interval() or len(self.conns) == 1:
            # only send RDY 1 if we're not in backoff (some other conn
            # should be testing the waters)
            # (but always send it if we're the first)
            self._send_rdy(conn, 1)

    def _on_connection_close(self, conn, **kwargs):
        if conn.id in self.conns:
            del self.conns[conn.id]

        self.total_rdy = max(self.total_rdy - conn.rdy, 0)

        logging.warning('[%s:%s] connection closed', conn.id, self.name)

        if (conn.rdy_timeout or conn.rdy) and \
                (len(self.conns) == self.max_in_flight or self.backoff_timer.get_interval()):
            # we're toggling out of (normal) redistribution cases and this conn
            # had a RDY count...
            #
            # trigger RDY redistribution to make sure this RDY is moved
            # to a new connection
            self.need_rdy_redistributed = True

        if conn.rdy_timeout:
            self.io_loop.remove_timeout(conn.rdy_timeout)
            conn.rdy_timeout = None

        if not self.lookupd_http_addresses:
            # automatically reconnect to nsqd addresses when not using lookupd
            logging.info('[%s:%s] attempting to reconnect in 15s', conn.id, self.name)
            reconnect_callback = functools.partial(self.connect_to_nsqd,
                                                   host=conn.host, port=conn.port)
            self.io_loop.add_timeout(time.time() + 15, reconnect_callback)

    def query_lookupd(self):
        """
        Trigger a query of the configured ``nsq_lookupd_http_addresses``.
        """
        endpoint = self.lookupd_http_addresses[self.lookupd_query_index]
        self.lookupd_query_index = (self.lookupd_query_index + 1) % len(self.lookupd_http_addresses)
        lookupd_url = endpoint + '/lookup?topic=' + urllib.quote(self.topic)
        req = tornado.httpclient.HTTPRequest(lookupd_url, method='GET',
                                             connect_timeout=1, request_timeout=2)
        callback = functools.partial(self._finish_query_lookupd, lookupd_url=lookupd_url)
        self.http_client.fetch(req, callback=callback)

    def _finish_query_lookupd(self, response, lookupd_url):
        if response.error:
            logging.warning('[%s] lookupd %s query error: %s',
                            self.name, lookupd_url, response.error)
            return

        try:
            lookup_data = json.loads(response.body)
        except ValueError:
            logging.warning('[%s] lookupd %s failed to parse JSON: %r',
                            self.name, lookupd_url, response.body)
            return

        if lookup_data['status_code'] != 200:
            logging.warning('[%s] lookupd %s responded with %d',
                            self.name, lookupd_url, lookup_data['status_code'])
            return

        for producer in lookup_data['data']['producers']:
            # TODO: this can be dropped for 1.0
            address = producer.get('broadcast_address', producer.get('address'))
            assert address
            self.connect_to_nsqd(address, producer['tcp_port'])

    def _redistribute_rdy_state(self):
        # We redistribute RDY counts in a few cases:
        #
        # 1. our # of connections exceeds our configured max_in_flight
        # 2. we're in backoff mode (but not in a current backoff block)
        # 3. something out-of-band has set the need_rdy_redistributed flag (connection closed
        #    that was about to get RDY during backoff)
        #
        # At a high level, we're trying to mitigate stalls related to low-volume
        # producers when we're unable (by configuration or backoff) to provide a RDY count
        # of (at least) 1 to all of our connections.

        if self.disabled() or self.backoff_block:
            return

        if len(self.conns) > self.max_in_flight:
            self.need_rdy_redistributed = True
            logging.debug('redistributing RDY state (%d conns > %d max_in_flight)',
                          len(self.conns), self.max_in_flight)

        backoff_interval = self.backoff_timer.get_interval()
        if backoff_interval and len(self.conns) > 1:
            self.need_rdy_redistributed = True
            logging.debug('redistributing RDY state (%d backoff interval and %d conns > 1)',
                          backoff_interval, len(self.conns))

        if self.need_rdy_redistributed:
            self.need_rdy_redistributed = False

            # first set RDY 0 to all connections that have not received a message within
            # a configurable timeframe (low_rdy_idle_timeout).
            for conn_id, conn in self.conns.iteritems():
                last_message_duration = time.time() - conn.last_msg_timestamp
                logging.debug('[%s:%s] rdy: %d (last message received %.02fs)',
                              conn.id, self.name, conn.rdy, last_message_duration)
                if conn.rdy > 0 and last_message_duration > self.low_rdy_idle_timeout:
                    logging.info('[%s:%s] idle connection, giving up RDY count', conn.id, self.name)
                    self._send_rdy(conn, 0)

            if backoff_interval:
                max_in_flight = 1 - self.total_rdy
            else:
                max_in_flight = self.max_in_flight - self.total_rdy

            # randomly walk the list of possible connections and send RDY 1 (up to our
            # calculate "max_in_flight").  We only need to send RDY 1 because in both
            # cases described above your per connection RDY count would never be higher.
            #
            # We also don't attempt to avoid the connections who previously might have had RDY 1
            # because it would be overly complicated and not actually worth it (ie. given enough
            # redistribution rounds it doesn't matter).
            possible_conns = self.conns.values()
            while possible_conns and max_in_flight:
                max_in_flight -= 1
                conn = possible_conns.pop(random.randrange(len(possible_conns)))
                logging.info('[%s:%s] redistributing RDY', conn.id, self.name)
                self._send_rdy(conn, 1)

            # for tests
            return conn

    #
    # subclass overwriteable
    #

    def process_message(self, message):
        """
        Called when a message is received in order to execute the configured ``message_handler``

        This is useful to subclass and override if you want to change how your
        message handlers are called.

        :param message: the :class:`nsq.Message` received
        """
        return self.message_handler(message)

    def giving_up(self, message):
        """
        Called when a message has been received where ``msg.attempts > max_tries``

        This is useful to subclass and override to perform a task (such as writing to disk, etc.)

        :param message: the :class:`nsq.Message` received
        """
        logging.warning('[%s] giving up on message %s after max tries %d %r',
                        self.name, message.id, self.max_tries, message.body)

    def disabled(self):
        """
        Called as part of RDY handling to identify whether this Reader has been disabled

        This is useful to subclass and override to examine a file on disk or a key in cache
        to identify if this reader should pause execution (during a deploy, etc.).
        """
        return False

    def validate_message(self, message):
        return True

    def preprocess_message(self, message):
        return message

########NEW FILE########
__FILENAME__ = snappy_socket
import snappy
import socket
import errno


class SnappySocket(object):
    def __init__(self, socket):
        self._decompressor = snappy.StreamDecompressor()
        self._compressor = snappy.StreamCompressor()
        self._socket = socket
        self._bootstrapped = None

    def __getattr__(self, name):
        return getattr(self._socket, name)

    def bootstrap(self, data):
        if data:
            self._bootstrapped = self._decompressor.decompress(data)

    def recv(self, size):
        return self._recv(size, self._socket.recv)

    def read(self, size):
        return self._recv(size, self._socket.read)

    def _recv(self, size, method):
        if self._bootstrapped:
            data = self._bootstrapped
            self._bootstrapped = None
            return data
        chunk = method(size)
        if chunk:
            uncompressed = self._decompressor.decompress(chunk)
        if not uncompressed:
            raise socket.error(errno.EWOULDBLOCK)
        return uncompressed

    def send(self, data):
        chunk = self._compressor.add_chunk(data, compress=True)
        self._socket.send(chunk)

########NEW FILE########
__FILENAME__ = sync
import socket
import struct

import nsq


class SyncConn(object):
    def __init__(self, timeout=1.0):
        self.buffer = ''
        self.timeout = timeout

    def connect(self, host, port):
        assert isinstance(host, (str, unicode))
        assert isinstance(port, int)
        self.s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        self.s.settimeout(self.timeout)
        self.s.connect((host, port))
        self.s.send(nsq.MAGIC_V2)

    def _readn(self, size):
        while True:
            if len(self.buffer) >= size:
                break
            packet = self.s.recv(4096)
            if not packet:
                raise Exception('failed to read %d' % size)
            self.buffer += packet
        data = self.buffer[:size]
        self.buffer = self.buffer[size:]
        return data

    def read_response(self):
        size = struct.unpack('>l', self._readn(4))[0]
        return self._readn(size)

    def send(self, data):
        self.s.send(data)

########NEW FILE########
__FILENAME__ = version
# also update in setup.py
__version__ = '0.6.2'

########NEW FILE########
__FILENAME__ = writer
# -*- coding: utf-8 -*-
import logging
import time
import functools
import random

from client import Client
import nsq
import async


class Writer(Client):
    """
    A high-level producer class built on top of the `Tornado IOLoop <http://tornadoweb.org>`_
    supporting async publishing (``PUB`` & ``MPUB``) of messages to ``nsqd`` over the TCP protocol.

    Example publishing a message repeatedly using a Tornado IOLoop periodic callback::

        import nsq
        import tornado.ioloop
        import time

        def pub_message():
            writer.pub('test', time.strftime('%H:%M:%S'), finish_pub)

        def finish_pub(conn, data):
            print data

        writer = nsq.Writer(['127.0.0.1:4150'])
        tornado.ioloop.PeriodicCallback(pub_message, 1000).start()
        nsq.run()

    Example publshing a message from a Tornado HTTP request handler::

        import functools
        import tornado.httpserver
        import tornado.ioloop
        import tornado.options
        import tornado.web
        from nsq import Writer, Error
        from tornado.options import define, options

        class MainHandler(tornado.web.RequestHandler):
            @property
            def nsq(self):
                return self.application.nsq

            def get(self):
                topic = 'log'
                msg = 'Hello world'
                msg_cn = 'Hello 世界'

                self.nsq.pub(topic, msg) # pub
                self.nsq.mpub(topic, [msg, msg_cn]) # mpub

                # customize callback
                callback = functools.partial(self.finish_pub, topic=topic, msg=msg)
                self.nsq.pub(topic, msg, callback=callback)

                self.write(msg)

            def finish_pub(self, conn, data, topic, msg):
                if isinstance(data, Error):
                    # try to re-pub message again if pub failed
                    self.nsq.pub(topic, msg)

        class Application(tornado.web.Application):
            def __init__(self, handlers, **settings):
                self.nsq = Writer(['127.0.0.1:4150'])
                super(Application, self).__init__(handlers, **settings)

    :param nsqd_tcp_addresses: a sequence with elements of the form 'address:port' corresponding
        to the ``nsqd`` instances this writer should publish to

    :param name: a string that is used for logging messages (defaults to first nsqd address)

    :param \*\*kwargs: passed to :class:`nsq.AsyncConn` initialization
    """
    def __init__(self, nsqd_tcp_addresses, name=None, **kwargs):
        super(Writer, self).__init__(**kwargs)

        if not isinstance(nsqd_tcp_addresses, (list, set, tuple)):
            assert isinstance(nsqd_tcp_addresses, (str, unicode))
            nsqd_tcp_addresses = [nsqd_tcp_addresses]
        assert nsqd_tcp_addresses

        self.name = name or nsqd_tcp_addresses[0]
        self.nsqd_tcp_addresses = nsqd_tcp_addresses
        self.conns = {}
        self.conn_kwargs = kwargs

        self.io_loop.add_callback(self._run)

    def _run(self):
        logging.info('starting writer...')
        self.connect()

    def pub(self, topic, msg, callback=None):
        self._pub('pub', topic, msg, callback)

    def mpub(self, topic, msg, callback=None):
        if isinstance(msg, (str, unicode)):
            msg = [msg]
        assert isinstance(msg, (list, set, tuple))

        self._pub('mpub', topic, msg, callback)

    def _pub(self, command, topic, msg, callback):
        if not callback:
            callback = functools.partial(self._finish_pub, command=command,
                                         topic=topic, msg=msg)

        if not self.conns:
            callback(None, nsq.SendError('no connections'))
            return

        conn = random.choice(self.conns.values())
        conn.callback_queue.append(callback)
        cmd = getattr(nsq, command)
        try:
            conn.send(cmd(topic, msg))
        except Exception:
            logging.exception('[%s] failed to send %s' % (conn.id, command))
            conn.close()

    def _on_connection_response(self, conn, data, **kwargs):
        if conn.callback_queue:
            callback = conn.callback_queue.pop(0)
            callback(conn, data)

    def connect(self):
        for addr in self.nsqd_tcp_addresses:
            host, port = addr.split(':')
            self.connect_to_nsqd(host, int(port))

    def connect_to_nsqd(self, host, port):
        assert isinstance(host, (str, unicode))
        assert isinstance(port, int)

        conn = async.AsyncConn(host, port, **self.conn_kwargs)
        conn.on('identify', self._on_connection_identify)
        conn.on('identify_response', self._on_connection_identify_response)
        conn.on('error', self._on_connection_response)
        conn.on('response', self._on_connection_response)
        conn.on('close', self._on_connection_close)
        conn.on('ready', self._on_connection_ready)
        conn.on('heartbeat', self.heartbeat)

        if conn.id in self.conns:
            return

        logging.info('[%s] connecting to nsqd', conn.id)
        conn.connect()
        conn.callback_queue = []

    def _on_connection_ready(self, conn, **kwargs):
        # re-check to make sure another connection didn't beat this one
        if conn.id in self.conns:
            logging.warning(
                '[%s] connected but another matching connection already exists', conn.id)
            conn.close()
            return
        self.conns[conn.id] = conn

    def _on_connection_close(self, conn, **kwargs):
        if conn.id in self.conns:
            del self.conns[conn.id]

        for callback in conn.callback_queue:
            try:
                callback(conn, nsq.ConnectionClosedError())
            except Exception:
                logging.exception('[%s] uncaught exception in callback', conn.id)

        logging.warning('[%s] connection closed', conn.id)
        logging.info('[%s] attempting to reconnect in 15s', conn.id)
        reconnect_callback = functools.partial(self.connect_to_nsqd,
                                               host=conn.host, port=conn.port)
        self.io_loop.add_timeout(time.time() + 15, reconnect_callback)

    def _finish_pub(self, conn, data, command, topic, msg):
        if isinstance(data, nsq.Error):
            logging.error('[%s] failed to %s (%s, %s), data is %s',
                          conn.id, command, topic, msg, data)

########NEW FILE########
__FILENAME__ = mock_socket
"""
Mock socket module, copied (and edited) from:
http://svn.python.org/projects/python/branches/pep-0384/Lib/test/mock_socket.py
"""


class MockSocket:
    def __init__(self):
        self.output = []
        self.lines = []
        self.conn = None
        self.timeout = None

    def connect(self, addr):
        pass

    def queue_recv(self, line):
        self.lines.append(line)

    def recv(self, bufsize, flags=None):
        return self.lines.pop(0)

    def settimeout(self, timeout=None):
        if timeout is None:
            self.timeout = 60
        else:
            self.timeout = timeout

    def gettimeout(self):
        return self.timeout

    def send(self, data, flags=None):
        self.output.append(data)
        return len(data)

    def close(self):
        pass

    def setblocking(self, val):
        pass


def socket(family=None, type=None, proto=None):
    return MockSocket()


# Constants
AF_INET = None
SOCK_STREAM = None
SOL_SOCKET = None
SO_REUSEADDR = None

########NEW FILE########
__FILENAME__ = test_async
from __future__ import with_statement
import os
import sys

import struct
from mock import patch, create_autospec, MagicMock
from tornado.iostream import IOStream

# shunt '..' into sys.path since we are in a 'tests' subdirectory
base_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
if base_dir not in sys.path:
    sys.path.insert(0, base_dir)

import nsq


# The functions below are meant to be used as specs for mocks of callbacks. Yay dynamic typing
def f(*args, **kwargs):
    pass


def _get_test_conn(io_loop=None):
    conn = nsq.async.AsyncConn('test', 4150, io_loop=io_loop)
    # now set the stream attribute, which is ordinarily set in conn.connect()
    conn.stream = create_autospec(IOStream)
    return conn


@patch('nsq.async.socket', autospec=True)
@patch('nsq.async.tornado.iostream.IOStream', autospec=True)
def test_connect(mock_iostream, mock_socket):
    conn = _get_test_conn()
    conn.connect()
    conn.stream.set_close_callback.assert_called_once_with(conn._socket_close)
    conn.stream.connect.assert_called_once_with((conn.host, conn.port), conn._connect_callback)
    assert conn.state == 'CONNECTING'

    # now ensure that we will bail if we were already in a connecting or connected state
    conn = _get_test_conn()
    conn.state = 'CONNECTING'
    conn.connect()
    assert not conn.stream.set_close_callback.called
    assert not conn.stream.connect.called

    conn = _get_test_conn()
    conn.state = 'CONNECTED'
    conn.connect()
    assert not conn.stream.set_close_callback.called
    assert not conn.stream.connect.called


def test_connect_callback():
    conn = _get_test_conn()
    on_connect = create_autospec(f)
    conn.on('connect', on_connect)
    # simulate having called `conn.connect` by setting state to `connecting`
    conn.state = 'CONNECTING'
    with patch.object(conn, '_start_read', autospec=True) as mock_start_read:
        conn._connect_callback()
        assert conn.state == 'CONNECTED'
        conn.stream.write.assert_called_once_with(nsq.nsq.MAGIC_V2)
        mock_start_read.assert_called_once_with()
        on_connect.assert_called_once_with(conn=conn)


def test_start_read():
    conn = _get_test_conn()
    conn._start_read()
    conn.stream.read_bytes.assert_called_once_with(4, conn._read_size)


def test_read_size():
    conn = _get_test_conn()
    body_size = 6
    body_size_packed = struct.pack('>l', body_size)
    conn._read_size(body_size_packed)
    conn.stream.read_bytes.assert_called_once_with(body_size, conn._read_body)

    # now test that we get the right behavior when we get malformed data
    # for this, we'll want to stick on mock on conn.close
    conn.stream.read_bytes.reset_mock()
    with patch.object(conn, 'close', autospec=True) as mock_close:
        conn._read_size('asdfasdf')
        mock_close.assert_called_once_with()
        assert not conn.stream.read_bytes.called


def test_read_body():
    mock_io_loop = MagicMock()

    conn = _get_test_conn(io_loop=mock_io_loop)
    on_data = create_autospec(f)
    conn.on('data', on_data)

    mock_ioloop_addcb = create_autospec(f)
    mock_io_loop.add_callback = mock_ioloop_addcb
    data = 'NSQ'
    conn._read_body(data)
    on_data.assert_called_once_with(conn=conn, data=data)
    mock_ioloop_addcb.assert_called_once_with(conn._start_read)

    # now test functionality when the data_callback fails
    on_data.reset_mock()
    mock_ioloop_addcb.reset_mock()
    on_data.return_value = Exception("Boom.")
    conn._read_body(data)
    # verify that we still added callback for the next start_read
    mock_ioloop_addcb.assert_called_once_with(conn._start_read)

########NEW FILE########
__FILENAME__ = test_backoff
from __future__ import with_statement
import os
import sys
import random
import time

from mock import patch, create_autospec
from tornado.ioloop import IOLoop

# shunt '..' into sys.path since we are in a 'tests' subdirectory
base_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
if base_dir not in sys.path:
    sys.path.insert(0, base_dir)

import nsq


_conn_port = 4150


def _message_handler(msg):
    msg.enable_async()


def _get_reader(io_loop=None):
    return nsq.Reader("test", "test",
                      message_handler=_message_handler,
                      lookupd_http_addresses=["http://test.local:4161"],
                      max_in_flight=5,
                      io_loop=io_loop)


def _get_conn(reader):
    global _conn_port
    with patch('nsq.async.tornado.iostream.IOStream', autospec=True):
        conn = reader.connect_to_nsqd('localhost', _conn_port)
    _conn_port += 1
    conn.trigger('ready', conn=conn)
    return conn


def _send_message(conn):
    msg = _get_message(conn)
    conn.trigger('message', conn=conn, message=msg)
    return msg


def _get_message(conn):
    msg = nsq.Message("1234", "{}", 1234, 0)
    msg.on('finish', conn._on_message_finish)
    msg.on('requeue', conn._on_message_requeue)
    return msg


def test_backoff_easy():
    mock_ioloop = create_autospec(IOLoop)
    r = _get_reader(mock_ioloop)
    conn = _get_conn(r)

    msg = _send_message(conn)

    msg.trigger('finish', message=msg)
    assert r.backoff_block is False
    assert r.backoff_timer.get_interval() == 0

    msg = _send_message(conn)

    msg.trigger('requeue', message=msg)
    assert r.backoff_block is True
    assert r.backoff_timer.get_interval() > 0
    assert mock_ioloop.add_timeout.called

    timeout_args, timeout_kwargs = mock_ioloop.add_timeout.call_args
    timeout_args[1]()
    assert r.backoff_block is False
    send_args, send_kwargs = conn.stream.write.call_args
    assert send_args[0] == 'RDY 1\n'

    msg = _send_message(conn)

    msg.trigger('finish', message=msg)
    assert r.backoff_block is False
    assert r.backoff_timer.get_interval() == 0

    expected_args = [
        'SUB test test\n',
        'RDY 1\n', 'RDY 5\n',
        'FIN 1234\n', 'REQ 1234 0\n',
        'RDY 0\n', 'RDY 1\n',
        'FIN 1234\n', 'RDY 5\n'
    ]
    assert conn.stream.write.call_args_list == [((arg,),) for arg in expected_args]


def test_backoff_hard():
    mock_ioloop = create_autospec(IOLoop)
    r = _get_reader(io_loop=mock_ioloop)
    conn = _get_conn(r)

    expected_args = ['SUB test test\n', 'RDY 1\n', 'RDY 5\n']

    num_fails = 0
    fail = True
    last_timeout_time = 0
    for i in range(50):
        msg = _send_message(conn)

        if fail:
            msg.trigger('requeue', message=msg)
            num_fails += 1

            expected_args.append('REQ 1234 0\n')
            expected_args.append('RDY 0\n')
        else:
            msg.trigger('finish', message=msg)
            num_fails -= 1

            expected_args.append('FIN 1234\n')
            expected_args.append('RDY 0\n')

        assert r.backoff_block is True
        assert r.backoff_timer.get_interval() > 0
        assert mock_ioloop.add_timeout.called

        timeout_args, timeout_kwargs = mock_ioloop.add_timeout.call_args
        if timeout_args[0] != last_timeout_time:
            timeout_args[1]()
            last_timeout_time = timeout_args[0]
        assert r.backoff_block is False
        expected_args.append('RDY 1\n')

        fail = True
        if random.random() < 0.3 and num_fails > 1:
            fail = False

    for i in range(num_fails - 1):
        msg = _send_message(conn)

        msg.trigger('finish', message=msg)
        expected_args.append('FIN 1234\n')
        timeout_args, timeout_kwargs = mock_ioloop.add_timeout.call_args
        if timeout_args[0] != last_timeout_time:
            timeout_args[1]()
            last_timeout_time = timeout_args[0]
        expected_args.append('RDY 0\n')
        expected_args.append('RDY 1\n')

    msg = _send_message(conn)

    msg.trigger('finish', message=msg)
    expected_args.append('FIN 1234\n')
    expected_args.append('RDY 5\n')

    assert r.backoff_block is False
    assert r.backoff_timer.get_interval() == 0

    for i, call in enumerate(conn.stream.write.call_args_list):
        print "%d: %s" % (i, call)
    assert conn.stream.write.call_args_list == [((arg,),) for arg in expected_args]


def test_backoff_many_conns():
    mock_ioloop = create_autospec(IOLoop)
    r = _get_reader(io_loop=mock_ioloop)

    num_conns = 5
    conns = []
    for i in range(num_conns):
        conn = _get_conn(r)
        conn.expected_args = ['SUB test test\n', 'RDY 1\n']
        conn.fails = 0
        conns.append(conn)

    fail = True
    total_fails = 0
    last_timeout_time = 0
    conn = random.choice(conns)
    for i in range(50):
        msg = _send_message(conn)

        if r.backoff_timer.get_interval() == 0:
            conn.expected_args.append('RDY 1\n')

        if fail or not conn.fails:
            msg.trigger('requeue', message=msg)
            total_fails += 1
            conn.fails += 1

            conn.expected_args.append('REQ 1234 0\n')
            for c in conns:
                c.expected_args.append('RDY 0\n')
        else:
            msg.trigger('finish', message=msg)
            total_fails -= 1
            conn.fails -= 1

            conn.expected_args.append('FIN 1234\n')
            for c in conns:
                c.expected_args.append('RDY 0\n')

        assert r.backoff_block is True
        assert r.backoff_timer.get_interval() > 0
        assert mock_ioloop.add_timeout.called

        timeout_args, timeout_kwargs = mock_ioloop.add_timeout.call_args
        if timeout_args[0] != last_timeout_time:
            conn = timeout_args[1]()
            last_timeout_time = timeout_args[0]
        assert r.backoff_block is False
        conn.expected_args.append('RDY 1\n')

        fail = True
        if random.random() < 0.3 and total_fails > 1:
            fail = False

    while total_fails:
        print "%r: %d fails (%d total_fails)" % (conn, c.fails, total_fails)

        if not conn.fails:
            # force an idle connection
            for c in conns:
                if c.rdy > 0:
                    c.last_msg_timestamp = time.time() - 60
                    c.expected_args.append('RDY 0\n')
            conn = r._redistribute_rdy_state()
            conn.expected_args.append('RDY 1\n')
            continue

        msg = _send_message(conn)

        msg.trigger('finish', message=msg)
        total_fails -= 1
        conn.fails -= 1

        conn.expected_args.append('FIN 1234\n')

        timeout_args, timeout_kwargs = mock_ioloop.add_timeout.call_args
        if timeout_args[0] != last_timeout_time:
            conn = timeout_args[1]()
            last_timeout_time = timeout_args[0]

        if total_fails > 0:
            for c in conns:
                c.expected_args.append('RDY 0\n')
            conn.expected_args.append('RDY 1\n')
        else:
            for c in conns:
                c.expected_args.append('RDY 1\n')

    assert r.backoff_block is False
    assert r.backoff_timer.get_interval() == 0

    for c in conns:
        for i, call in enumerate(c.stream.write.call_args_list):
            print "%d: %s" % (i, call)
        assert c.stream.write.call_args_list == [((arg,),) for arg in c.expected_args]


def test_backoff_conns_disconnect():
    mock_ioloop = create_autospec(IOLoop)
    r = _get_reader(io_loop=mock_ioloop)

    num_conns = 5
    conns = []
    for i in range(num_conns):
        conn = _get_conn(r)
        conn.expected_args = ['SUB test test\n', 'RDY 1\n']
        conn.fails = 0
        conns.append(conn)

    fail = True
    total_fails = 0
    last_timeout_time = 0
    conn = random.choice(conns)
    for i in range(50):
        if i % 5 == 0:
            if len(r.conns) == num_conns:
                conn.trigger('close', conn=conn)
                conns.remove(conn)
                if conn.rdy and r.backoff_timer.get_interval():
                    assert r.need_rdy_redistributed
                conn = r._redistribute_rdy_state()
                if not conn:
                    conn = random.choice(conns)
                else:
                    conn.expected_args.append('RDY 1\n')
                continue
            else:
                c = _get_conn(r)
                c.expected_args = ['SUB test test\n']
                c.fails = 0
                conns.append(c)

        msg = _send_message(conn)

        if r.backoff_timer.get_interval() == 0:
            conn.expected_args.append('RDY 1\n')

        if fail or not conn.fails:
            msg.trigger('requeue', message=msg)
            total_fails += 1
            conn.fails += 1

            conn.expected_args.append('REQ 1234 0\n')
            for c in conns:
                c.expected_args.append('RDY 0\n')
        else:
            msg.trigger('finish', message=msg)
            total_fails -= 1
            conn.fails -= 1

            conn.expected_args.append('FIN 1234\n')
            for c in conns:
                c.expected_args.append('RDY 0\n')

        assert r.backoff_block is True
        assert r.backoff_timer.get_interval() > 0
        assert mock_ioloop.add_timeout.called

        timeout_args, timeout_kwargs = mock_ioloop.add_timeout.call_args
        if timeout_args[0] != last_timeout_time:
            conn = timeout_args[1]()
            last_timeout_time = timeout_args[0]
        assert r.backoff_block is False
        conn.expected_args.append('RDY 1\n')

        fail = True
        if random.random() < 0.3 and total_fails > 1:
            fail = False

    while total_fails:
        print "%r: %d fails (%d total_fails)" % (conn, c.fails, total_fails)

        msg = _send_message(conn)

        msg.trigger('finish', message=msg)
        total_fails -= 1

        conn.expected_args.append('FIN 1234\n')

        timeout_args, timeout_kwargs = mock_ioloop.add_timeout.call_args
        if timeout_args[0] != last_timeout_time:
            conn = timeout_args[1]()
            last_timeout_time = timeout_args[0]

        if total_fails > 0:
            for c in conns:
                c.expected_args.append('RDY 0\n')
            conn.expected_args.append('RDY 1\n')
        else:
            for c in conns:
                c.expected_args.append('RDY 1\n')

    assert r.backoff_block is False
    assert r.backoff_timer.get_interval() == 0

    for c in conns:
        for i, call in enumerate(c.stream.write.call_args_list):
            print "%d: %s" % (i, call)
        assert c.stream.write.call_args_list == [((arg,),) for arg in c.expected_args]

########NEW FILE########
__FILENAME__ = test_basic
from decimal import Decimal
import os
import sys

# shunt '..' into sys.path since we are in a 'tests' subdirectory
base_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
if base_dir not in sys.path:
    sys.path.insert(0, base_dir)

from nsq import BackoffTimer
import nsq


def pytest_generate_tests(metafunc):
    if metafunc.function == test_topic_names:
        for name, good in [
                ('valid_name', True),
                ('invalid name with space', False),
                ('invalid_name_due_to_length_this_is_really_really_really_really_long', False),
                ('test-with_period.', True),
                ('test#ephemeral', False),
                ('test:ephemeral', False),
                ]:
            metafunc.addcall(funcargs=dict(name=name, good=good))
    if metafunc.function == test_channel_names:
        for name, good in [
                ('test', True),
                ('test-with_period.', True),
                ('test#ephemeral', True),
                ('invalid_name_due_to_length_this_is_really_really_really_really_long', False),
                ('invalid name with space', False),
                ]:
            metafunc.addcall(funcargs=dict(name=name, good=good))


def test_topic_names(name, good):
    assert nsq.valid_topic_name(name) == good


def test_channel_names(name, good):
    assert nsq.valid_channel_name(name) == good


def test_backoff_timer():
    timer = BackoffTimer(.1, 120, long_length=1000)
    assert timer.get_interval() == .1
    timer.success()
    assert timer.get_interval() == .1
    timer.failure()
    interval = '%0.2f' % timer.get_interval()
    assert interval == '3.19'
    assert timer.min_interval == Decimal('.1')
    assert timer.short_interval == Decimal('2.9975')
    assert timer.long_interval == Decimal('0.089925')

    timer.failure()
    interval = '%0.2f' % timer.get_interval()
    assert interval == '6.27'
    timer.success()
    interval = '%0.2f' % timer.get_interval()
    assert interval == '3.19'
    for i in range(25):
        timer.failure()
    interval = '%0.2f' % timer.get_interval()
    assert interval == '32.41'

########NEW FILE########
__FILENAME__ = test_command
import struct
import pytest
import os
import sys

try:
    import simplejson as json
except ImportError:
    import json  # pyflakes.ignore

# shunt '..' into sys.path since we are in a 'tests' subdirectory
base_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
if base_dir not in sys.path:
    sys.path.insert(0, base_dir)

import nsq


def pytest_generate_tests(metafunc):
    identify_dict_ascii = {'a': 1, 'b': 2}
    identify_dict_unicode = {'c': u'w\xc3\xa5\xe2\x80\xa0'}
    identify_body_ascii = json.dumps(identify_dict_ascii)
    identify_body_unicode = json.dumps(identify_dict_unicode)

    msgs = ['asdf', 'ghjk', 'abcd']
    mpub_body = struct.pack('>l', len(msgs)) + ''.join(struct.pack('>l', len(m)) + m for m in msgs)
    if metafunc.function == test_command:
        for cmd_method, kwargs, result in [
                (nsq.identify,
                    {'data': identify_dict_ascii},
                    'IDENTIFY\n' + struct.pack('>l', len(identify_body_ascii)) +
                    identify_body_ascii),
                (nsq.identify,
                    {'data': identify_dict_unicode},
                    'IDENTIFY\n' + struct.pack('>l', len(identify_body_unicode)) +
                    identify_body_unicode),
                (nsq.subscribe,
                    {'topic': 'test_topic', 'channel': 'test_channel'},
                    'SUB test_topic test_channel\n'),
                (nsq.finish,
                    {'id': 'test'},
                    'FIN test\n'),
                (nsq.finish,
                    {'id': u'\u2020est \xfcn\xee\xe7\xf8\u2202\xe9'},
                    'FIN \xe2\x80\xa0est \xc3\xbcn\xc3\xae\xc3\xa7\xc3\xb8\xe2\x88\x82\xc3\xa9\n'),
                (nsq.requeue,
                    {'id': 'test'},
                    'REQ test 0\n'),
                (nsq.requeue,
                    {'id': 'test', 'time_ms': 60},
                    'REQ test 60\n'),
                (nsq.touch,
                    {'id': 'test'},
                    'TOUCH test\n'),
                (nsq.ready,
                    {'count': 100},
                    'RDY 100\n'),
                (nsq.nop,
                    {},
                    'NOP\n'),
                (nsq.pub,
                    {'topic': 'test', 'data': msgs[0]},
                    'PUB test\n' + struct.pack('>l', len(msgs[0])) + msgs[0]),
                (nsq.mpub,
                    {'topic': 'test', 'data': msgs},
                    'MPUB test\n' + struct.pack('>l', len(mpub_body)) + mpub_body)
                ]:
            metafunc.addcall(funcargs=dict(cmd_method=cmd_method, kwargs=kwargs, result=result))


def test_command(cmd_method, kwargs, result):
    assert cmd_method(**kwargs) == result


def test_unicode_body():
    pytest.raises(AssertionError, nsq.pub, 'topic', u'unicode body')

########NEW FILE########
__FILENAME__ = test_reader
import os
import sys
import signal
import subprocess
import time
import ssl

import tornado.httpclient
import tornado.testing

# shunt '..' into sys.path since we are in a 'tests' subdirectory
base_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
if base_dir not in sys.path:
    sys.path.insert(0, base_dir)

import nsq


class ReaderIntegrationTest(tornado.testing.AsyncTestCase):
    identify_options = {
        'user_agent': 'sup',
        'snappy': True,
        'tls_v1': True,
        'tls_options': {'cert_reqs': ssl.CERT_NONE},
        'heartbeat_interval': 10,
        'output_buffer_size': 4096,
        'output_buffer_timeout': 50
    }

    def setUp(self):
        super(ReaderIntegrationTest, self).setUp()
        self.processes = []
        proc = subprocess.Popen(['nsqd', '--verbose', '--snappy',
                                 '--tls-key=%s/tests/key.pem' % base_dir,
                                 '--tls-cert=%s/tests/cert.pem' % base_dir])
        self.processes.append(proc)
        http = tornado.httpclient.HTTPClient()
        start = time.time()
        while True:
            try:
                resp = http.fetch('http://127.0.0.1:4151/ping')
                if resp.body == 'OK':
                    break
                continue
            except:
                if time.time() - start > 5:
                    raise
                time.sleep(0.1)
                continue

    def tearDown(self):
        super(ReaderIntegrationTest, self).tearDown()
        for proc in self.processes:
            os.kill(proc.pid, signal.SIGKILL)
            proc.wait()

    def test_conn_identify(self):
        c = nsq.async.AsyncConn('127.0.0.1', 4150, io_loop=self.io_loop)
        c.on('identify_response', self.stop)
        c.connect()
        response = self.wait()
        print response
        assert response['conn'] is c
        assert isinstance(response['data'], dict)

    def test_conn_identify_options(self):
        c = nsq.async.AsyncConn('127.0.0.1', 4150, io_loop=self.io_loop,
                                **self.identify_options)
        c.on('identify_response', self.stop)
        c.connect()
        response = self.wait()
        print response
        assert response['conn'] is c
        assert isinstance(response['data'], dict)
        assert response['data']['snappy'] is True
        assert response['data']['tls_v1'] is True

    def test_conn_subscribe(self):
        topic = 'test_conn_suscribe_%s' % time.time()
        c = nsq.async.AsyncConn('127.0.0.1', 4150, io_loop=self.io_loop,
                                **self.identify_options)

        def _on_ready(*args, **kwargs):
            c.on('response', self.stop)
            c.send(nsq.subscribe(topic, 'ch'))

        c.on('ready', _on_ready)
        c.connect()
        response = self.wait()
        print response
        assert response['conn'] is c
        assert response['data'] == 'OK'

    def _send_messages(self, topic, count, body):
        c = nsq.async.AsyncConn('127.0.0.1', 4150, io_loop=self.io_loop)
        c.connect()

        def _on_ready(*args, **kwargs):
            for i in range(count):
                c.send(nsq.pub(topic, body))

        c.on('ready', _on_ready)

    def test_conn_messages(self):
        self.msg_count = 0

        topic = 'test_conn_suscribe_%s' % time.time()
        self._send_messages(topic, 5, 'sup')

        c = nsq.async.AsyncConn('127.0.0.1', 4150, io_loop=self.io_loop,
                                **self.identify_options)

        def _on_message(*args, **kwargs):
            self.msg_count += 1
            if c.rdy == 0:
                self.stop()

        def _on_ready(*args, **kwargs):
            c.on('message', _on_message)
            c.send(nsq.subscribe(topic, 'ch'))
            c.send_rdy(5)

        c.on('ready', _on_ready)
        c.connect()

        self.wait()
        assert self.msg_count == 5

    def test_reader_messages(self):
        self.msg_count = 0
        num_messages = 500

        topic = 'test_reader_msgs_%s' % time.time()
        self._send_messages(topic, num_messages, 'sup')

        def handler(msg):
            assert msg.body == 'sup'
            self.msg_count += 1
            if self.msg_count >= num_messages:
                self.stop()
            return True

        nsq.Reader(nsqd_tcp_addresses=['127.0.0.1:4150'], topic=topic, channel='ch',
                   io_loop=self.io_loop, message_handler=handler, max_in_flight=100,
                   **self.identify_options)

        self.wait()

    def test_reader_heartbeat(self):
        this = self
        this.count = 0

        def handler(msg):
            return True

        class HeartbeatReader(nsq.Reader):
            def heartbeat(self, conn):
                this.count += 1
                if this.count == 2:
                    this.stop()

        topic = 'test_reader_hb_%s' % time.time()
        HeartbeatReader(nsqd_tcp_addresses=['127.0.0.1:4150'], topic=topic, channel='ch',
                        io_loop=self.io_loop, message_handler=handler, max_in_flight=100,
                        heartbeat_interval=1)
        self.wait()

########NEW FILE########
__FILENAME__ = test_sync
import struct
import time
import os
import sys

# shunt '..' into sys.path since we are in a 'tests' subdirectory
base_dir = os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))
if base_dir not in sys.path:
    sys.path.insert(0, base_dir)

import mock_socket
import nsq
nsq.sync.socket = mock_socket


def mock_write(c, data):
    c.s.queue_recv(data)


def mock_response_write(c, frame_type, data):
    body_size = 4 + len(data)
    body_size_packed = struct.pack('>l', body_size)
    frame_type_packed = struct.pack('>l', frame_type)
    mock_write(c, body_size_packed + frame_type_packed + data)


def mock_response_write_message(c, timestamp, attempts, id, body):
    timestamp_packed = struct.pack('>q', timestamp)
    attempts_packed = struct.pack('>h', attempts)
    id = "%016d" % id
    mock_response_write(c, nsq.FRAME_TYPE_MESSAGE, timestamp_packed + attempts_packed + id + body)


def test_sync_authenticate_subscribe():
    c = nsq.SyncConn()
    c.connect("127.0.0.1", 4150)

    c.send(nsq.identify({'short_id': 'test', 'long_id': 'test.example'}))
    c.send(nsq.subscribe('test', 'ch'))

    mock_response_write(c, nsq.FRAME_TYPE_RESPONSE, 'OK')
    mock_response_write(c, nsq.FRAME_TYPE_RESPONSE, 'OK')

    resp = c.read_response()
    unpacked = nsq.unpack_response(resp)
    assert unpacked[0] == nsq.FRAME_TYPE_RESPONSE
    assert unpacked[1] == 'OK'

    resp = c.read_response()
    unpacked = nsq.unpack_response(resp)
    assert unpacked[0] == nsq.FRAME_TYPE_RESPONSE
    assert unpacked[1] == 'OK'


def test_sync_receive_messages():
    c = nsq.SyncConn()
    c.connect("127.0.0.1", 4150)

    c.send(nsq.identify({'short_id': 'test', 'long_id': 'test.example'}))
    c.send(nsq.subscribe('test', 'ch'))

    mock_response_write(c, nsq.FRAME_TYPE_RESPONSE, 'OK')
    mock_response_write(c, nsq.FRAME_TYPE_RESPONSE, 'OK')

    resp = c.read_response()
    unpacked = nsq.unpack_response(resp)
    assert unpacked[0] == nsq.FRAME_TYPE_RESPONSE
    assert unpacked[1] == 'OK'

    resp = c.read_response()
    unpacked = nsq.unpack_response(resp)
    assert unpacked[0] == nsq.FRAME_TYPE_RESPONSE
    assert unpacked[1] == 'OK'

    for i in range(10):
        c.send(nsq.ready(1))
        body = '{"data": {"test_key": %d}}' % i
        ts = int(time.time() * 1000 * 1000)
        mock_response_write_message(c, ts, 0, i, body)
        resp = c.read_response()
        unpacked = nsq.unpack_response(resp)
        assert unpacked[0] == nsq.FRAME_TYPE_MESSAGE
        msg = nsq.decode_message(unpacked[1])
        assert msg.timestamp == ts
        assert msg.id == "%016d" % i
        assert msg.attempts == 0
        assert msg.body == body

########NEW FILE########
