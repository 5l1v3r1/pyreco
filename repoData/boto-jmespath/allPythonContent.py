__FILENAME__ = conf
# -*- coding: utf-8 -*-
import guzzle_sphinx_theme

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'sphinx.ext.viewcode']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'JMESPath'
copyright = u'2014, James Saryerwinnie'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.4'
# The full version, including alpha/beta/rc tags.
release = '0.4.1'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'guzzle_sphinx_theme.GuzzleStyle'

html_translator_class = 'guzzle_sphinx_theme.HTMLTranslator'
html_theme_path = guzzle_sphinx_theme.html_theme_path()
html_theme = 'guzzle_sphinx_theme'
extensions.append("guzzle_sphinx_theme")


# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
html_theme_options = html_theme_options = {
    # Set the name of the project to appear in the nav menu
    "project_nav_name": "JMESPath",

    # Set your GitHub user and repo to enable GitHub stars links
    "github_user": "boto",
    "github_repo": "jmespath",

    # Set to true to bind left and right key events to turn the page
    "bind_key_events": 1,
}

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# Output file base name for HTML help builder.
htmlhelp_basename = 'JMESPathdoc'

########NEW FILE########
__FILENAME__ = ast
import operator
import math
import json

from jmespath.compat import with_repr_method
from jmespath.compat import string_type as STRING_TYPE
from jmespath.compat import zip_longest
from jmespath.exceptions import JMESPathTypeError, UnknownFunctionError


NUMBER_TYPE = (float, int)
_VARIADIC = object()


# python types -> jmespath types
TYPES_MAP = {
    'bool': 'boolean',
    'list': 'array',
    'dict': 'object',
    'NoneType': 'null',
    'unicode': 'string',
    'str': 'string',
    'float': 'number',
    'int': 'number',
    'OrderedDict': 'object',
    '_Projection': 'array',
    '_Expression': 'expref',
}


# jmespath types -> python types
REVERSE_TYPES_MAP = {
    'boolean': ('bool',),
    'array': ('list', '_Projection'),
    'object': ('dict', 'OrderedDict',),
    'null': ('None',),
    'string': ('unicode', 'str'),
    'number': ('float', 'int'),
    'expref': ('_Expression',),
}


class _Arg(object):
    __slots__ = ('types',)

    def __init__(self, types=None):
        self.types = types


@with_repr_method
class AST(object):

    def __init__(self):
        self.children = []

    def search(self, value):
        pass

    def pretty_print(self, indent=''):
        return super(AST, self).__repr__()

    def __repr__(self):
        return self.pretty_print()


class Identity(AST):
    def search(self, value):
        return value

    def pretty_print(self, indent=''):
        return "%sIdentity()" % indent


class SubExpression(AST):
    """Represents a subexpression match.

    A subexpression match has a parent and a child node.  A simple example
    would be something like 'foo.bar' which is represented as::

        SubExpression(Field(foo), Field(bar))

    """
    def __init__(self, parent, child):
        self.children = [parent, child]

    def search(self, value):
        # To evaluate a subexpression we first evaluate the parent object
        # and then feed the match of the parent node into the child node.
        sub_value = self.children[0].search(value)
        found = self.children[1].search(sub_value)
        return found

    def pretty_print(self, indent=''):
        sub_indent = indent + ' ' * 4
        return "%s%s(\n%s%s,\n%s%s)" % (
            indent, self.__class__.__name__,
            sub_indent, self.children[0].pretty_print(sub_indent),
            sub_indent, self.children[1].pretty_print(sub_indent))


# This is used just to differentiate between
# subexpressions and indexexpressions (wildcards can hang
# off of an indexexpression).
class IndexExpression(SubExpression):
    pass


class Field(AST):

    def __init__(self, name):
        self.name = name
        self.children = []

    def pretty_print(self, indent=''):
        return "%sField(%s)" % (indent, self.name)

    def search(self, value):
        if value is not None:
            try:
                return value.get(self.name)
            except AttributeError:
                return None


class BaseMultiField(AST):
    def __init__(self, nodes):
        self.children = list(nodes)

    def search(self, value):
        if value is None:
            return None
        return self._multi_get(value)

    def _multi_get(self, value):
        # Subclasses must define this method.
        raise NotImplementedError("_multi_get")

    def pretty_print(self, indent=''):
        return "%s%s(%s)" % (indent, self.__class__.__name__, self.children)


class MultiFieldDict(BaseMultiField):

    def _multi_get(self, value):
        collected = {}
        for node in self.children:
            collected[node.key_name] = node.search(value)
        return collected


class MultiFieldList(BaseMultiField):

    def _multi_get(self, value):
        collected = []
        for node in self.children:
            collected.append(node.search(value))
        return collected


class KeyValPair(AST):
    def __init__(self, key_name, node):
        self.key_name = key_name
        self.children = [node]

    def search(self, value):
        return self.children[0].search(value)

    def pretty_print(self, indent=''):
        return "%sKeyValPair(key_name=%s, node=%s)" % (
            indent, self.key_name, self.children[0])


class Index(AST):

    def __init__(self, index):
        super(Index, self).__init__()
        self.index = index

    def pretty_print(self, indent=''):
        return "%sIndex(%s)" % (indent, self.index)

    def search(self, value):
        # Even though we can index strings, we don't
        # want to support that.
        if not isinstance(value, list):
            return None
        try:
            return value[self.index]
        except IndexError:
            return None


class ORExpression(AST):
    def __init__(self, first, remaining):
        self.children = [first, remaining]

    def search(self, value):
        matched = self.children[0].search(value)
        if self._is_false(matched):
            matched = self.children[1].search(value)
        return matched

    def _is_false(self, value):
        # This looks weird, but we're explicitly using equality checks
        # because the truth/false values are different between
        # python and jmespath.
        return (value == '' or value == [] or value == {} or value is None or
                value == False)

    def pretty_print(self, indent=''):
        return "%sORExpression(%s, %s)" % (indent, self.children[0],
                                           self.children[1])


class FilterExpression(AST):

    def __init__(self, expression):
        self.children = [expression]

    def search(self, value):
        if not isinstance(value, list):
            return None
        result = []
        for element in value:
            if self.children[0].search(element):
                result.append(element)
        return result

    def pretty_print(self, indent=''):
        return '%sFilterExpression(%s)' % (indent, self.children[0])


class Literal(AST):

    def __init__(self, literal_value):
        super(Literal, self).__init__()
        self.literal_value = literal_value

    def search(self, value):
        return self.literal_value

    def pretty_print(self, indent=''):
        return '%sLiteral(%s)' % (indent, self.literal_value)


class Comparator(AST):
    # Subclasses must define the operation function.
    operation = None

    def __init__(self, first, second):
        self.children = [first, second]

    def search(self, data):
        return self.operation(self.children[0].search(data),
                              self.children[1].search(data))

    def pretty_print(self, indent=''):
        return '%s%s(%s, %s)' % (indent, self.__class__.__name__,
                                 self.children[0], self.children[1])


class OPEquals(Comparator):
    def _equals(self, first, second):
        if self._is_special_integer_case(first, second):
            return False
        else:
            return first == second

    def _is_special_integer_case(self, first, second):
        # We need to special case comparing 0 or 1 to
        # True/False.  While normally comparing any
        # integer other than 0/1 to True/False will always
        # return False.  However 0/1 have this:
        # >>> 0 == True
        # False
        # >>> 0 == False
        # True
        # >>> 1 == True
        # True
        # >>> 1 == False
        # False
        #
        # Also need to consider that:
        # >>> 0 in [True, False]
        # True
        if first is 0 or first is 1:
            return second is True or second is False
        elif second is 0 or second is 1:
            return first is True or first is False

    operation = _equals


class OPNotEquals(OPEquals):
    def _not_equals(self, first, second):
        return not super(OPNotEquals, self)._equals(first, second)

    operation = _not_equals


class OPLessThan(Comparator):
    operation = operator.lt


class OPLessThanEquals(Comparator):
    operation = operator.le


class OPGreaterThan(Comparator):
    operation = operator.gt


class OPGreaterThanEquals(Comparator):
    operation = operator.ge


class CurrentNode(AST):
    def search(self, value):
        return value


class FunctionExpression(AST):

    def __init__(self, name, args):
        self.name = name
        # The .children attribute is to support homogeneous
        # children nodes, but .args is a better name for all the
        # code that uses the children, so we support both.
        self.children = args
        self.args = args
        try:
            self.function = getattr(self, '_func_%s' % name)
        except AttributeError:
            raise UnknownFunctionError("Unknown function: %s" % self.name)
        self.arity = self.function.arity
        self.variadic = self.function.variadic
        self.function = self._resolve_arguments_wrapper(self.function)

    def pretty_print(self, indent=''):
        return "%sFunctionExpression(name=%s, args=%s)" % (
            indent, self.name, self.args)

    def search(self, value):
        return self.function(value)

    def _resolve_arguments_wrapper(self, function):
        def _call_with_resolved_args(value):
            # Before calling the function, we have two things to do:
            # 1. Resolve the arguments (evaluate the arg expressions
            #    against the passed in input.
            # 2. Type check the arguments
            resolved_args = []
            for arg_expression, arg_spec in zip_longest(
                    self.args, function.argspec,
                    fillvalue=function.argspec[-1]):
                # 1. Resolve the arguments.
                current = arg_expression.search(value)
                # 2. Type check (provided we have type information).
                if arg_spec.types is not None:
                    _type_check(arg_spec.types, current)
                resolved_args.append(current)
            return function(*resolved_args)

        def _get_allowed_pytypes(types):
            allowed_types = []
            allowed_subtypes = []
            for t in types:
                type_ = t.split('-', 1)
                if len(type_) == 2:
                    type_, subtype = type_
                    allowed_subtypes.append(REVERSE_TYPES_MAP[subtype])
                else:
                    type_ = type_[0]
                allowed_types.extend(REVERSE_TYPES_MAP[type_])
            return allowed_types, allowed_subtypes

        def _type_check(types, current):
            # Type checking involves checking the top level type,
            # and in the case of arrays, potentially checking the types
            # of each element.
            allowed_types, allowed_subtypes = _get_allowed_pytypes(types)
            # We're not using isinstance() on purpose.
            # The type model for jmespath does not map
            # 1-1 with python types (booleans are considered
            # integers in python for example).
            actual_typename = type(current).__name__
            if actual_typename not in allowed_types:
                raise JMESPathTypeError(self.name, current,
                                        TYPES_MAP.get(actual_typename,
                                                      'unknown'),
                                        types)
            # If we're dealing with a list type, we can have
            # additional restrictions on the type of the list
            # elements (for example a function can require a
            # list of numbers or a list of strings).
            # Arrays are the only types that can have subtypes.
            if allowed_subtypes:
                _subtype_check(current, allowed_subtypes, types)

        def _subtype_check(current, allowed_subtypes, types):
            if len(allowed_subtypes) == 1:
                # The easy case, we know up front what type
                # we need to validate.
                allowed_subtypes = allowed_subtypes[0]
                for element in current:
                    actual_typename = type(element).__name__
                    if actual_typename not in allowed_subtypes:
                        raise JMESPathTypeError(self.name, element,
                                                actual_typename,
                                                types)
            elif len(allowed_subtypes) > 1 and current:
                # Dynamic type validation.  Based on the first
                # type we see, we validate that the remaining types
                # match.
                first = type(current[0]).__name__
                for subtypes in allowed_subtypes:
                    if first in subtypes:
                        allowed = subtypes
                        break
                else:
                    raise JMESPathTypeError(self.name, current[0],
                                            first, types)
                for element in current:
                    actual_typename = type(element).__name__
                    if actual_typename not in allowed:
                        raise JMESPathTypeError(self.name, element,
                                                actual_typename,
                                                types)

        return _call_with_resolved_args

    def signature(*arguments, **kwargs):
        def _record_arity(func):
            func.arity = len(arguments)
            func.variadic = kwargs.get('variadic', False)
            func.argspec = arguments
            return func
        return _record_arity

    @signature(_Arg(), variadic=True)
    def _func_not_null(self, *arguments):
        for argument in arguments:
            if argument is not None:
                return argument

    @signature(_Arg(types=['number']))
    def _func_abs(self, arg):
        return abs(arg)

    @signature(_Arg(types=['array-number']))
    def _func_avg(self, arg):
        return sum(arg) / float(len(arg))

    @signature(_Arg())
    def _func_to_string(self, arg):
        if isinstance(arg, STRING_TYPE):
            return arg
        else:
            return json.dumps(arg, separators=(',', ':'))

    @signature(_Arg())
    def _func_to_number(self, arg):
        if isinstance(arg, (list, dict, bool)):
            return None
        elif arg is None:
            return None
        elif isinstance(arg, (int, float)):
            return arg
        else:
            try:
                if '.' in arg:
                    return float(arg)
                else:
                    return int(arg)
            except ValueError:
                return None

    @signature(_Arg(types=['array', 'string']), _Arg())
    def _func_contains(self, subject, search):
        return search in subject

    @signature(_Arg(types=['string', 'array', 'object']))
    def _func_length(self, arg):
        return len(arg)

    @signature(_Arg(types=['number']))
    def _func_ceil(self, arg):
        return math.ceil(arg)

    @signature(_Arg(types=['number']))
    def _func_floor(self, arg):
        return math.floor(arg)

    @signature(_Arg(types=['string']), _Arg(types=['array-string']))
    def _func_join(self, separator, array):
        return separator.join(array)

    @signature(_Arg(types=['array-number']))
    def _func_max(self, arg):
        if arg:
            return max(arg)
        else:
            return None

    @signature(_Arg(types=['array-number']))
    def _func_min(self, arg):
        if arg:
            return min(arg)
        else:
            return None

    @signature(_Arg(types=['array-string', 'array-number']))
    def _func_sort(self, arg):
        return list(sorted(arg))

    @signature(_Arg(types=['array-number']))
    def _func_sum(self, arg):
        return sum(arg)

    @signature(_Arg(types=['object']))
    def _func_keys(self, arg):
        # To be consistent with .values()
        # should we also return the indices of a list?
        return list(arg.keys())

    @signature(_Arg(types=['object']))
    def _func_values(self, arg):
        return list(arg.values())

    @signature(_Arg())
    def _func_type(self, arg):
        if isinstance(arg, STRING_TYPE):
            return "string"
        elif isinstance(arg, bool):
            return "boolean"
        elif isinstance(arg, list):
            return "array"
        elif isinstance(arg, dict):
            return "object"
        elif isinstance(arg, (float, int)):
            return "number"
        elif arg is None:
            return "null"

    def _create_key_func(self, expression, allowed_types):
        py_types = []
        for type_ in allowed_types:
            py_types.extend(REVERSE_TYPES_MAP[type_])
        def keyfunc(x):
            result = expression.search(x)
            type_name = type(result).__name__
            if type_name not in py_types:
                raise JMESPathTypeError(self.name,
                                        result,
                                        type_name,
                                        allowed_types)
            return result
        return keyfunc

    @signature(_Arg(types=['array']), _Arg(types=['expref']))
    def _func_sort_by(self, array, expref):
        # sort_by allows for the expref to be either a number of
        # a string, so we have some special logic to handle this.
        # We evaluate the first array element and verify that it's
        # either a string of a number.  We then create a key function
        # that validates that type, which requires that remaining array
        # elements resolve to the same type as the first element.
        if not array:
            return array
        required_type = TYPES_MAP.get(
            type(expref.search(array[0])).__name__)
        if required_type not in ['number', 'string']:
            raise JMESPathTypeError(self.name,
                                    array[0],
                                    required_type,
                                    ['string', 'number'])
        keyfunc = self._create_key_func(expref, [required_type])
        return list(sorted(array, key=keyfunc))

    @signature(_Arg(types=['array']), _Arg(types=['expref']))
    def _func_max_by(self, array, expref):
        keyfunc = self._create_key_func(expref, ['number'])
        return max(array, key=keyfunc)

    @signature(_Arg(types=['array']), _Arg(types=['expref']))
    def _func_min_by(self, array, expref):
        keyfunc = self._create_key_func(expref, ['number'])
        return min(array, key=keyfunc)


class ExpressionReference(AST):
    def __init__(self, expression):
        self.children = [expression]

    def search(self, value):
        return _Expression(self.children[0])


class _Expression(AST):
    def __init__(self, expression):
        self.expression = expression

    def search(self, value):
        return self.expression.search(value)


class Pipe(AST):
    def __init__(self, parent, child):
        self.children = [parent, child]

    def search(self, value):
        left = self.children[0].search(value)
        return self.children[1].search(left)

    def pretty_print(self, indent=''):
        sub_indent = indent + ' ' * 4
        return "%s%s(\n%s%s,\n%s%s)" % (
            indent, self.__class__.__name__,
            sub_indent, self.children[0].pretty_print(sub_indent),
            sub_indent, self.children[1].pretty_print(sub_indent))


class Projection(AST):
    def __init__(self, left, right):
        self.children = [left, right]

    def search(self, value):
        base = self._evaluate_left_child(value)
        if base is None:
            return None
        else:
            collected = self._evaluate_right_child(base)
            return collected

    def _evaluate_left_child(self, value):
        base = self.children[0].search(value)
        if isinstance(base, list):
            return base
        else:
            # Invalid type, so we return None.
            return None

    def _evaluate_right_child(self, value):
        collected = []
        for element in value:
            current = self.children[1].search(element)
            if current is not None:
                collected.append(current)
        return collected

    def pretty_print(self, indent=''):
        sub_indent = indent + ' ' * 4
        return "%s%s(\n%s%s,\n%s%s)" % (
            indent, self.__class__.__name__,
            sub_indent, self.children[0].pretty_print(sub_indent),
            sub_indent, self.children[1].pretty_print(sub_indent))


class ValueProjection(Projection):
    def _evaluate_left_child(self, value):
        base_hash = self.children[0].search(value)
        try:
            return base_hash.values()
        except AttributeError:
            return None


class FilterProjection(Projection):
    # A filter projection is a left projection that
    # filter elements against an expression before allowing
    # them to be right evaluated.
    def __init__(self, left, right, comparator):
        self.children = [left, right, comparator]

    def _evaluate_right_child(self, value):
        result = []
        for element in value:
            if self.children[2].search(element):
                result.append(element)
        return super(FilterProjection, self)._evaluate_right_child(result)

    def pretty_print(self, indent=''):
        sub_indent = indent + ' ' * 4
        return "%s%s(\n%s%s,\n%s%s,\n%s%s)" % (
            indent, self.__class__.__name__,
            sub_indent, self.children[0].pretty_print(sub_indent),
            sub_indent, self.children[2].pretty_print(sub_indent),
            sub_indent, self.children[1].pretty_print(sub_indent),
        )


class Flatten(AST):
    def __init__(self, element):
        self.children = [element]

    def pretty_print(self, indent=''):
        return "%s%s(%s)" % (
            indent, self.__class__.__name__,
            self.children[0].pretty_print(indent).lstrip())

    def search(self, value):
        original = self.children[0].search(value)
        if not isinstance(original, list):
            return None
        merged_list = []
        for element in original:
            if isinstance(element, list):
                merged_list.extend(element)
            else:
                merged_list.append(element)
        return merged_list

########NEW FILE########
__FILENAME__ = compat
import sys

PY2 = sys.version_info[0] == 2

if PY2:
    text_type = unicode
    string_type = basestring
    from itertools import izip_longest as zip_longest
    LR_TABLE = 'jmespath._lrtable'

    def with_str_method(cls):
        """Class decorator that handles __str__ compat between py2 and py3."""
        # In python2, the __str__ should be __unicode__
        # and __str__ should return bytes.
        cls.__unicode__ = cls.__str__
        def __str__(self):
            return self.__unicode__().encode('utf-8')
        cls.__str__ = __str__
        return cls
    def with_repr_method(cls):
        """Class decorator that handle __repr__ with py2 and py3."""
        # This is almost the same thing as with_str_method *except*
        # it uses the unicode_escape encoding.  This also means we need to be
        # careful encoding the input multiple times, so we only encode
        # if we get a unicode type.
        original_repr_method = cls.__repr__
        def __repr__(self):
            original_repr = original_repr_method(self)
            if isinstance(original_repr, text_type):
                original_repr = original_repr.encode('unicode_escape')
            return original_repr
        cls.__repr__ = __repr__
        return cls
else:
    LR_TABLE = 'jmespath._lrtable3'
    text_type = str
    string_type = str
    from itertools import zip_longest
    def with_str_method(cls):
        # In python3, we don't need to do anything, we return a str type.
        return cls
    def with_repr_method(cls):
        return cls


if sys.version_info[:2] == (2, 6):
    from ordereddict import OrderedDict
    import simplejson as json
else:
    from collections import OrderedDict
    import json

########NEW FILE########
__FILENAME__ = exceptions
from jmespath.compat import with_str_method


class JMESPathError(ValueError):
    pass


@with_str_method
class ParseError(JMESPathError):
    _ERROR_MESSAGE = 'Invalid jmespath expression'
    def __init__(self, lex_position, token_value, token_type,
                 msg=_ERROR_MESSAGE):
        super(ParseError, self).__init__(lex_position, token_value, token_type)
        self.lex_position = lex_position
        self.token_value = token_value
        self.token_type = token_type.upper()
        self.msg = msg
        # Whatever catches the ParseError can fill in the full expression
        self.expression = None

    def __str__(self):
        # self.lex_position +1 to account for the starting double quote char.
        underline = ' ' * (self.lex_position + 1) + '^'
        return (
            '%s: Parse error at column %s near '
            'token "%s" (%s) for expression:\n"%s"\n%s' % (
                self.msg, self.lex_position, self.token_value, self.token_type,
                self.expression, underline))


@with_str_method
class IncompleteExpressionError(ParseError):
    def set_expression(self, expression):
        self.expression = expression
        self.lex_position = len(expression)
        self.token_type = None
        self.token_value = None

    def __str__(self):
        # self.lex_position +1 to account for the starting double quote char.
        underline = ' ' * (self.lex_position + 1) + '^'
        return (
            'Invalid jmespath expression: Incomplete expression:\n'
            '"%s"\n%s' % (self.expression, underline))


@with_str_method
class LexerError(ParseError):
    def __init__(self, lexer_position, lexer_value, message, expression=None):
        self.lexer_position = lexer_position
        self.lexer_value = lexer_value
        self.message = message
        super(LexerError, self).__init__(lexer_position,
                                         lexer_value,
                                         message)
        # Whatever catches LexerError can set this.
        self.expression = expression

    def __str__(self):
        underline = ' ' * self.lexer_position + '^'
        return 'Bad jmespath expression: %s:\n%s\n%s' % (
            self.message, self.expression, underline)


@with_str_method
class ArityError(ParseError):
    def __init__(self, function_node):
        self.expected_arity = function_node.arity
        self.actual_arity = len(function_node.args)
        self.function_name = function_node.name
        self.expression = None

    def __str__(self):
        return ("Expected %s arguments for function %s, "
                "received %s" % (self.expected_arity,
                                 self.function_name,
                                 self.actual_arity))


@with_str_method
class VariadictArityError(ArityError):
    def __str__(self):
        return ("Expected at least %s arguments for function %s, "
                "received %s" % (self.expected_arity,
                                 self.function_name,
                                 self.actual_arity))


@with_str_method
class JMESPathTypeError(JMESPathError):
    def __init__(self, function_name, current_value, actual_type,
                 expected_types):
        self.function_name = function_name
        self.current_value = current_value
        self.actual_type = actual_type
        self.expected_types = expected_types

    def __str__(self):
        return ('In function %s(), invalid type for value: %s, '
                'expected one of: %s, received: "%s"' % (
                    self.function_name, self.current_value,
                    self.expected_types, self.actual_type))


class UnknownFunctionError(JMESPathError):
    pass

########NEW FILE########
__FILENAME__ = lexer
import re
from json import loads

from jmespath.exceptions import LexerError


class Lexer(object):
    TOKENS = (
        r'(?P<number>-?\d+)|'
        r'(?P<unquoted_identifier>([a-zA-Z_][a-zA-Z_0-9]*))|'
        r'(?P<quoted_identifier>("(?:\\\\|\\"|[^"])*"))|'
        r'(?P<literal>(`(?:\\\\|\\`|[^`])*`))|'
        r'(?P<filter>\[\?)|'
        r'(?P<or>\|\|)|'
        r'(?P<pipe>\|)|'
        r'(?P<ne>!=)|'
        r'(?P<rbrace>\})|'
        r'(?P<eq>==)|'
        r'(?P<dot>\.)|'
        r'(?P<star>\*)|'
        r'(?P<gte>>=)|'
        r'(?P<lparen>\()|'
        r'(?P<lbrace>\{)|'
        r'(?P<lte><=)|'
        r'(?P<flatten>\[\])|'
        r'(?P<rbracket>\])|'
        r'(?P<lbracket>\[)|'
        r'(?P<rparen>\))|'
        r'(?P<comma>,)|'
        r'(?P<colon>:)|'
        r'(?P<lt><)|'
        r'(?P<expref>&)|'
        r'(?P<gt>>)|'
        r'(?P<current>@)|'
        r'(?P<skip>[ \t]+)'
    )
    def __init__(self):
        self.master_regex = re.compile(self.TOKENS)

    def tokenize(self, expression):
        previous_column = 0
        for match in self.master_regex.finditer(expression):
            value = match.group()
            start = match.start()
            end = match.end()
            if match.lastgroup == 'skip':
                # Ignore whitespace.
                previous_column = end
                continue
            if start != previous_column:
                bad_value = expression[previous_column:start]
                # Try to give a good error message.
                if bad_value == '"':
                    raise LexerError(
                        lexer_position=previous_column,
                        lexer_value=value,
                        message='Starting quote is missing the ending quote',
                        expression=expression)
                raise LexerError(lexer_position=previous_column,
                                 lexer_value=value,
                                 message='Unknown character',
                                 expression=expression)
            previous_column = end
            token_type = match.lastgroup
            handler = getattr(self, '_token_%s' % token_type.lower(), None)
            if handler is not None:
                value = handler(value, start, end)
            yield {'type': token_type, 'value': value, 'start': start, 'end': end}
        # At the end of the loop make sure we've consumed all the input.
        # If we haven't then we have unidentified characters.
        if end != len(expression):
            msg = "Unknown characters at the end of the expression"
            raise LexerError(lexer_position=end,
                             lexer_value='',
                             message=msg, expression=expression)
        else:
            yield {'type': 'eof', 'value': '',
                   'start': len(expression), 'end': len(expression)}

    def _token_number(self, value, start, end):
        return int(value)

    def _token_quoted_identifier(self, value, start, end):
        try:
            return loads(value)
        except ValueError as e:
            error_message = str(e).split(':')[0]
            raise LexerError(lexer_position=start,
                             lexer_value=value,
                             message=error_message)

    def _token_literal(self, value, start, end):
        actual_value = value[1:-1]
        actual_value = actual_value.replace('\\`', '`').lstrip()
        # First, if it looks like JSON then we parse it as
        # JSON and any json parsing errors propogate as lexing
        # errors.
        if self._looks_like_json(actual_value):
            try:
                return loads(actual_value)
            except ValueError:
                raise LexerError(lexer_position=start,
                                lexer_value=value,
                                message="Bad token %s" % value)
        else:
            potential_value = '"%s"' % actual_value
            try:
                # There's a shortcut syntax where string literals
                # don't have to be quoted.  This is only true if the
                # string doesn't start with chars that could start a valid
                # JSON value.
                return loads(potential_value)
            except ValueError:
                raise LexerError(lexer_position=start,
                                lexer_value=value,
                                message="Bad token %s" % value)

    def _looks_like_json(self, value):
        # Figure out if the string "value" starts with something
        # that looks like json.
        if not value:
            return False
        elif value[0] in ['"', '{', '[']:
            return True
        elif value in ['true', 'false', 'null']:
            return True
        elif value[0] in ['-', '0', '1', '2', '3', '4', '5',
                          '6', '7', '8', '9']:
            # Then this is JSON, return True.
            try:
                loads(value)
                return True
            except ValueError:
                return False
        else:
            return False

########NEW FILE########
__FILENAME__ = parser
"""Top down operator precedence parser.

This is an implementation of Vaughan R. Pratt's
"Top Down Operator Precedence" parser.
(http://dl.acm.org/citation.cfm?doid=512927.512931).

These are some additional resources that help explain the
general idea behind a Pratt parser:

* http://effbot.org/zone/simple-top-down-parsing.htm
* http://javascript.crockford.com/tdop/tdop.html
* http://journal.stuffwithstuff.com/2011/03/19/pratt-parsers-expression-parsing-made-easy/

A few notes on the implementation.

* All the nud/led tokens are on the Parser class itself, and are dispatched
  using getattr().  This keeps all the parsing logic contained to a single class.
* We use two passes through the data.  One to create a list of token,
  then one pass through the tokens to create the AST.  While the lexer actually
  yields tokens, we convert it to a list so we can easily implement two tokens
  of lookahead.  A previous implementation used a fixed circular buffer, but it
  was significantly slower.  Also, the average jmespath expression typically
  does not have a large amount of token so this is not an issue.  And
  interestingly enough, creating a token list first is actually faster than
  consuming from the token iterator one token at a time.

"""
import random

from jmespath import lexer
from jmespath.compat import with_repr_method
from jmespath import ast
from jmespath import exceptions


class Parser(object):
    BINDING_POWER = {
        'eof': 0,
        'unquoted_identifier': 0,
        'quoted_identifier': 0,
        'rbracket': 0,
        'rparen': 0,
        'comma': 0,
        'rbrace': 0,
        'number': 0,
        'current': 0,
        'expref': 0,
        'pipe': 1,
        'eq': 2,
        'gt': 2,
        'lt': 2,
        'gte': 2,
        'lte': 2,
        'ne': 2,
        'or': 5,
        'flatten': 6,
        'star': 20,
        'dot': 40,
        'lbrace': 50,
        'filter': 50,
        'lbracket': 55,
        'lparen': 60,
    }
    # The _MAX_SIZE most recent expressions are cached in
    # _CACHE dict.
    _CACHE = {}
    _MAX_SIZE = 128

    def __init__(self, lookahead=2):
        self.tokenizer = None
        self._tokens = [None] * lookahead
        self._buffer_size = lookahead
        self._index = 0

    def parse(self, expression):
        cached = self._CACHE.get(expression)
        if cached is not None:
            return cached
        parsed_result = self._do_parse(expression)
        self._CACHE[expression] = parsed_result
        if len(self._CACHE) > self._MAX_SIZE:
            self._free_cache_entries()
        return parsed_result

    def _do_parse(self, expression):
        try:
            return self._parse(expression)
        except exceptions.LexerError as e:
            e.expression = expression
            raise
        except exceptions.IncompleteExpressionError as e:
            e.set_expression(expression)
            raise
        except exceptions.ParseError as e:
            e.expression = expression
            raise

    def _parse(self, expression):
        self.tokenizer = lexer.Lexer().tokenize(expression)
        self._tokens = list(self.tokenizer)
        self._index = 0
        parsed = self._expression(binding_power=0)
        if not self._current_token() == 'eof':
            t = self._lookahead_token(0)
            raise exceptions.ParseError(t['start'], t['value'], t['type'],
                                        "Unexpected token: %s" % t['value'])
        return ParsedResult(expression, parsed)

    def _expression(self, binding_power=0):
        left_token = self._lookahead_token(0)
        self._advance()
        nud_function = getattr(
            self, '_token_nud_%s' % left_token['type'],
            self._error_nud_token)
        left = nud_function(left_token)
        current_token = self._current_token()
        while binding_power < self.BINDING_POWER[current_token]:
            led = getattr(self, '_token_led_%s' % current_token, None)
            if led is None:
                error_token = self._lookahead_token(0)
                self._error_led_token(error_token)
            else:
                self._advance()
                left = led(left)
                current_token = self._current_token()
        return left

    def _token_nud_literal(self, token):
        return ast.Literal(token['value'])

    def _token_nud_unquoted_identifier(self, token):
        return ast.Field(token['value'])

    def _token_nud_quoted_identifier(self, token):
        field = ast.Field(token['value'])
        # You can't have a quoted identifier as a function
        # name.
        if self._current_token() == 'lparen':
            t = self._lookahead_token(0)
            raise exceptions.ParseError(
                0, t['value'], t['type'],
                'Quoted identifier not allowed for function names.')
        return field

    def _token_nud_star(self, token):
        left = ast.Identity()
        if self._current_token() == 'rbracket':
            right = ast.Identity()
        else:
            right = self._parse_projection_rhs(self.BINDING_POWER['star'])
        return ast.ValueProjection(left, right)

    def _token_nud_filter(self, token):
        return self._token_led_filter(ast.Identity())

    def _token_nud_lbrace(self, token):
        return self._parse_multi_select_hash()

    def _token_nud_flatten(self, token):
        left = ast.Flatten(ast.Identity())
        right = self._parse_projection_rhs(
            self.BINDING_POWER['flatten'])
        return ast.Projection(left, right)

    def _token_nud_lbracket(self, token):
        if self._current_token() == 'number':
            node = ast.Index(self._lookahead_token(0)['value'])
            self._advance()
            self._match('rbracket')
            return node
        elif self._current_token() == 'star' and self._lookahead(1) == 'rbracket':
            self._advance()
            self._advance()
            right = self._parse_projection_rhs(self.BINDING_POWER['star'])
            return ast.Projection(ast.Identity(), right)
        else:
            return self._parse_multi_select_list()

    def _token_nud_expref(self, token):
        expression = self._expression(self.BINDING_POWER['expref'])
        return ast.ExpressionReference(expression)

    def _token_led_dot(self, left):
        if not self._current_token() == 'star':
            right = self._parse_dot_rhs(self.BINDING_POWER['dot'])
            return ast.SubExpression(left, right)
        else:
            # We're creating a projection.
            self._advance()
            right = self._parse_projection_rhs(
                self.BINDING_POWER['dot'])
            return ast.ValueProjection(left, right)

    def _token_led_pipe(self, left):
        right = self._expression(self.BINDING_POWER['pipe'])
        return ast.Pipe(left, right)

    def _token_led_or(self, left):
        right = self._expression(self.BINDING_POWER['or'])
        return ast.ORExpression(left, right)

    def _token_led_lparen(self, left):
        name = left.name
        args = []
        while not self._current_token() == 'rparen':
            if self._current_token() == 'current':
                expression = ast.CurrentNode()
                self._advance()
            else:
                expression = self._expression()
            if self._current_token() == 'comma':
                self._match('comma')
            args.append(expression)
        self._match('rparen')
        function_node = ast.FunctionExpression(name, args)
        if function_node.variadic:
            if len(function_node.args) < function_node.arity:
                raise exceptions.VariadictArityError(function_node)
        elif function_node.arity != len(function_node.args):
            raise exceptions.ArityError(function_node)
        return function_node

    def _token_led_filter(self, left):
        # Filters are projections.
        condition = self._expression(0)
        self._match('rbracket')
        if self._current_token() == 'flatten':
            right = ast.Identity()
        else:
            right = self._parse_projection_rhs(self.BINDING_POWER['filter'])
        return ast.FilterProjection(left, right, condition)

    def _token_led_eq(self, left):
        return self._parse_comparator(left, 'eq')

    def _token_led_ne(self, left):
        return self._parse_comparator(left, 'ne')

    def _token_led_gt(self, left):
        return self._parse_comparator(left, 'gt')

    def _token_led_gte(self, left):
        return self._parse_comparator(left, 'gte')

    def _token_led_lt(self, left):
        return self._parse_comparator(left, 'lt')

    def _token_led_lte(self, left):
        return self._parse_comparator(left, 'lte')

    def _token_led_flatten(self, left):
        left = ast.Flatten(left)
        right = self._parse_projection_rhs(
            self.BINDING_POWER['flatten'])
        return ast.Projection(left, right)

    def _token_led_lbracket(self, left):
        token = self._lookahead_token(0)
        if token['type'] == 'number':
            self._match('number')
            right = ast.Index(token['value'])
            self._match('rbracket')
            return ast.IndexExpression(left, right)
        else:
            # We have a projection
            self._match('star')
            self._match('rbracket')
            right = self._parse_projection_rhs(self.BINDING_POWER['star'])
            return ast.Projection(left, right)

    def _parse_comparator(self, left, comparator):
        op_map = {
            'lt': ast.OPLessThan,
            'lte': ast.OPLessThanEquals,
            'eq': ast.OPEquals,
            'gt': ast.OPGreaterThan,
            'gte': ast.OPGreaterThanEquals,
            'ne': ast.OPNotEquals,
        }
        right = self._expression(self.BINDING_POWER[comparator])
        return op_map[comparator](left, right)

    def _parse_multi_select_list(self):
        expressions = []
        while not self._current_token() == 'rbracket':
            expression = self._expression()
            expressions.append(expression)
            if self._current_token() == 'comma':
                self._match('comma')
                self._assert_not_token('rbracket')
        self._match('rbracket')
        return ast.MultiFieldList(expressions)

    def _parse_multi_select_hash(self):
        pairs = []
        while True:
            key_token = self._lookahead_token(0)
            # Before getting the token value, verify it's
            # an identifier.
            self._match_multiple_tokens(
                token_types=['quoted_identifier', 'unquoted_identifier'])
            key_name = key_token['value']
            self._match('colon')
            value = self._expression(0)
            node = ast.KeyValPair(key_name=key_name, node=value)
            pairs.append(node)
            if self._current_token() == 'comma':
                self._match('comma')
            elif self._current_token() == 'rbrace':
                self._match('rbrace')
                break
        return ast.MultiFieldDict(nodes=pairs)

    def _parse_projection_rhs(self, binding_power):
        # Parse the right hand side of the projection.
        if self.BINDING_POWER[self._current_token()] < 10:
            # BP of 10 are all the tokens that stop a projection.
            right = ast.Identity()
        elif self._current_token() == 'lbracket':
            right = self._expression(binding_power)
        elif self._current_token() == 'filter':
            right = self._expression(binding_power)
        elif self._current_token() == 'dot':
            self._match('dot')
            right = self._parse_dot_rhs(binding_power)
        else:
            t = self._lookahead_token(0)
            lex_position = t['start']
            actual_value = t['value']
            actual_type = t['type']
            raise exceptions.ParseError(lex_position, actual_value,
                                        actual_type, 'syntax error')
        return right

    def _parse_dot_rhs(self, binding_power):
        # From the grammar:
        # expression '.' ( identifier /
        #                  multi-select-list /
        #                  multi-select-hash /
        #                  function-expression /
        #                  *
        # In terms of tokens that means that after a '.',
        # you can have:
        lookahead = self._current_token()
        # Common case "foo.bar", so first check for an identifier.
        if lookahead in ['quoted_identifier', 'unquoted_identifier', 'star']:
            return self._expression(binding_power)
        elif lookahead == 'lbracket':
            self._match('lbracket')
            return self._parse_multi_select_list()
        elif lookahead == 'lbrace':
            self._match('lbrace')
            return self._parse_multi_select_hash()
        else:
            t = self._lookahead_token(0)
            allowed = ['quoted_identifier', 'unquoted_identifier',
                       'lbracket', 'lbrace']
            lex_position = t['start']
            actual_value = t['value']
            actual_type = t['type']
            raise exceptions.ParseError(
                lex_position, actual_value, actual_type,
                "Expecting: %s, got: %s" % (allowed,
                                            actual_type))

    def _assert_not_token(self, *token_types):
        if self._current_token() in token_types:
            t = self._lookahead_token(0)
            lex_position = t['start']
            actual_value = t['value']
            actual_type = t['type']
            raise exceptions.ParseError(
                lex_position, actual_value, actual_type,
                "Token %s not allowed to be: %s" % (actual_type, token_types))

    def _error_nud_token(self, token):
        raise exceptions.ParseError(token['start'], token['value'],
                                    token['type'], 'Invalid token.')

    def _error_led_token(self, token):
        raise exceptions.ParseError(token['start'], token['value'],
                                    token['type'], 'Invalid token')

    def _match(self, token_type=None):
        if self._current_token() == token_type:
            self._advance()
        else:
            t = self._lookahead_token(0)
            lex_position = t['start']
            actual_value = t['value']
            actual_type = t['type']
            if actual_type == 'eof':
                raise exceptions.IncompleteExpressionError(
                    lex_position, actual_value, actual_type)
            else:
                message = 'Expecting: %s, got: %s' % (token_type,
                                                      actual_type)
            raise exceptions.ParseError(
                lex_position, actual_value, actual_type, message)

    def _match_multiple_tokens(self, token_types):
        if self._current_token() not in token_types:
            t = self._lookahead_token(0)
            lex_position = t['start']
            actual_value = t['value']
            actual_type = t['type']
            if actual_type == 'eof':
                raise exceptions.IncompleteExpressionError(
                    lex_position, actual_value, actual_type)
            else:
                message = 'Expecting: %s, got: %s' % (token_types,
                                                      actual_type)
            raise exceptions.ParseError(
                lex_position, actual_value, actual_type, message)
        self._advance()

    def _advance(self):
        self._index += 1

    def _current_token(self):
        return self._tokens[self._index]['type']

    def _lookahead(self, number):
        return self._tokens[self._index + number]['type']

    def _lookahead_token(self, number):
        return self._tokens[self._index + number]

    def _free_cache_entries(self):
        for key in random.sample(self._CACHE.keys(), int(self._MAX_SIZE / 2)):
            del self._CACHE[key]

    @classmethod
    def purge(cls):
        """Clear the expression compilation cache."""
        cls._CACHE.clear()


@with_repr_method
class ParsedResult(object):
    def __init__(self, expression, parsed):
        self.expression = expression
        self.parsed = parsed

    def search(self, value):
        return self.parsed.search(value)

    def pretty_print(self, indent=''):
        return self.parsed.pretty_print(indent=indent)

    def __repr__(self):
        return repr(self.parsed)

########NEW FILE########
__FILENAME__ = perftest
#!/usr/bin/env python
"""Generate performance diagnostics.

The purpose of this script is to generate performance diagnostics for
various jmespath expressions to be able to track the performance
over time.  The test files are data driven similar to the
compliance tests.
"""
import argparse
import time
import os
import json
import sys

from jmespath.parser import Parser
from jmespath.lexer import Lexer


DIRECTORY = os.path.join(os.path.dirname(os.path.abspath(__file__)))
DEFAULT_NUM_LOOP = 100


def run_tests(tests):
    times = []
    for test in tests:
        given = test['given']
        expression = test['expression']
        lex_time = _lex_time(expression)
        parse_time = _parse_time(expression)
        search_time = _search_time(expression, given)
        sys.stdout.write(
            "lex_time: %.8f, parse_time: %.8fms, search_time: %.8fms" % (
                1000 * lex_time, 1000 * parse_time, 1000 * search_time))
        sys.stdout.write(" description: %s " % test['description'])
        sys.stdout.write("name: %s\n" % test['name'])


def _lex_time(expression):
    best = float('inf')
    lex = Lexer()
    for i in range(DEFAULT_NUM_LOOP):
        start = time.time()
        list(lex.tokenize(expression))
        end = time.time()
        total = end - start
        if total < best:
            best = total
    return best


def _search_time(expression, given):
    p = Parser()
    parsed = p.parse(expression)
    best = float('inf')
    for i in range(DEFAULT_NUM_LOOP):
        start = time.time()
        parsed.search(given)
        end = time.time()
        total = end - start
        if total < best:
            best = total
    return best

def _parse_time(expression):
    best = float('inf')
    p = Parser()
    for i in range(DEFAULT_NUM_LOOP):
        p.purge()
        start = time.time()
        p.parse(expression)
        end = time.time()
        total = end - start
        if total < best:
            best = total
    return best

def load_tests(filename):
    loaded = []
    with open(filename) as f:
        data = json.load(f)
    if isinstance(data, list):
        for i, d in enumerate(data):
            _add_cases(d, loaded, '%s-%s' % (filename, i))
    else:
        _add_cases(data, loaded, filename)
    return loaded


def _add_cases(data, loaded, filename):
    for case in data['cases']:
        current = {'description': data.get('description', filename),
                   'given': data['given'],
                   'name': case.get('name', case['expression']),
                   'expression': case['expression'],
                   'result': case.get('result')}
        loaded.append(current)
    return loaded


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-d', '--directory', default=DIRECTORY)
    parser.add_argument('-f', '--filename')
    args = parser.parse_args()
    collected_tests = []
    if args.filename:
        collected_tests.extend(load_tests(args.filename))
    else:
        for filename in os.listdir(args.directory):
            if filename.endswith('.json'):
                full_path = os.path.join(args.directory, filename)
                collected_tests.extend(load_tests(full_path))
    run_tests(collected_tests)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = test_ast
#!/usr/bin/env python

from tests import OrderedDict, unittest

from jmespath import ast


class TestAST(unittest.TestCase):
    def setUp(self):
        pass

    def test_field(self):
        # jmespath: foo
        field = ast.Field('foo')
        match = field.search({'foo': 'bar'})
        self.assertEqual(match, 'bar')

    def test_field_no_match(self):
        # jmespath: foo
        field = ast.Field('foo')
        match = field.search({'bar': 'bar'})
        self.assertEqual(match, None)

    def test_field_when_dict(self):
        # jmespath: foo
        field = ast.Field('foo')
        match = field.search({'foo': {'bar': 'baz'}})
        self.assertEqual(match, {'bar': 'baz'})

    def test_field_when_list(self):
        # jmespath: foo
        field = ast.Field('foo')
        match = field.search({'foo': ['bar', 'baz']})
        self.assertEqual(match, ['bar', 'baz'])

    def test_dot_syntax(self):
        # jmespath: foo.bar
        child = ast.SubExpression(ast.Field('foo'), ast.Field('bar'))
        match = child.search({'foo': {'bar': 'correct', 'baz': 'wrong'}})
        self.assertEqual(match, 'correct')

    def test_multiple_nestings(self):
        # jmespath: foo.bar.baz
        child = ast.SubExpression(
            ast.Field('foo'),
            ast.SubExpression(ast.Field('bar'), ast.Field('baz')))
        match = child.search(
            {'foo': {'bar': {'baz': 'correct'}}})
        self.assertEqual(match, 'correct')

        self.assertEqual(
            child.search({'foo': {'bar': {'wrong': 'wrong'}}}), None)
        self.assertEqual(child.search({}), None)
        self.assertEqual(child.search([]), None)
        self.assertEqual(child.search(''), None)

    def test_index(self):
        # jmespath: foo[1]
        child = ast.SubExpression(ast.Field('foo'), ast.Index(1))
        match = child.search(
            {'foo': ['one', 'two', 'three']})
        self.assertEqual(match, 'two')

    def test_bad_index(self):
        # jmespath: foo[100]
        child = ast.SubExpression(ast.Field('foo'), ast.Index(100))
        match = child.search(
            {'foo': ['one', 'two', 'three']})
        self.assertEqual(match, None)

    def test_negative_index(self):
        # jmespath: foo[-1]
        child = ast.SubExpression(ast.Field('foo'), ast.Index(-1))
        match = child.search(
            {'foo': ['one', 'two', 'last']})
        self.assertEqual(match, 'last')

    def test_index_with_children(self):
        # jmespath: foo.bar[-1]
        child = ast.SubExpression(
            ast.Field('foo'),
            ast.SubExpression(ast.Field('bar'), ast.Index(-1)))
        match = child.search(
            {'foo': {'bar': ['first', 'middle', 'last']}})
        self.assertEqual(match, 'last')

    def test_multiple_indices(self):
        # jmespath: foo[1].bar[1]
        child = ast.SubExpression(
            ast.SubExpression(
                ast.Field('foo'), ast.Index(1)),
            ast.SubExpression(
                ast.Field('bar'), ast.Index(1)))
        match = child.search(
            {'foo': ['one', {'bar': ['zero', 'one']}]})
        self.assertEqual(match, 'one')

    def test_associative(self):
        data = {'foo': {'bar': ['one']}}
        # jmespath: foo.bar[0]
        first = ast.SubExpression(
            ast.Field('foo'),
            ast.SubExpression(ast.Field('bar'), ast.Index(0)))
        second = ast.SubExpression(
            ast.SubExpression(ast.Field('foo'), ast.Field('bar')),
            ast.Index(0))
        self.assertEqual(first.search(data), 'one')
        self.assertEqual(second.search(data), 'one')

    def test_wildcard_branches_with_index(self):
        # foo[*].bar
        child = ast.Projection(
            ast.Field('foo'), ast.Field('bar'))
        match = child.search(
            {'foo': [{'bar': 'one'}, {'bar': 'two'}]})
        self.assertTrue(isinstance(match, list))
        self.assertEqual(match, ['one', 'two'])

    def test_or_expression(self):
        # foo or bar
        field_foo = ast.Field('foo')
        field_bar = ast.Field('bar')
        or_expression = ast.ORExpression(field_foo, field_bar)
        self.assertEqual(or_expression.search({'foo': 'foo'}), 'foo')
        self.assertEqual(or_expression.search({'bar': 'bar'}), 'bar')
        self.assertEqual(or_expression.search(
            {'foo': 'foo', 'bar': 'bar'}), 'foo')

    def test_multiselect_dict(self):
        # foo.{bar,baz
        field_foo = ast.KeyValPair(key_name='foo', node=ast.Field('foo'))
        field_bar = ast.KeyValPair(key_name='bar', node=ast.Field('bar'))
        field_baz = ast.KeyValPair(key_name='baz', node=ast.Field('baz'))
        multiselect = ast.MultiFieldDict([field_bar, field_baz])
        subexpr = ast.SubExpression(field_foo, multiselect)
        self.assertEqual(
            subexpr.search({'foo': {'bar': 1, 'baz': 2, 'qux': 3}}),
            {'bar': 1, 'baz': 2})

    def test_multiselect_different_key_names(self):
        field_foo = ast.KeyValPair(key_name='arbitrary', node=ast.Field('foo'))
        field_bar = ast.KeyValPair(key_name='arbitrary2', node=ast.Field('bar'))
        multiselect = ast.MultiFieldDict([field_foo, field_bar])
        self.assertEqual(multiselect.search({'foo': 'value1', 'bar': 'value2'}),
                         {'arbitrary': 'value1', 'arbitrary2': 'value2'})

    def test_multiselect_list(self):
        # foo.[bar,baz]
        field_foo = ast.Field('foo')
        field_bar = ast.Field('bar')
        field_baz = ast.Field('baz')
        multiselect = ast.MultiFieldList([field_bar, field_baz])
        subexpr = ast.SubExpression(field_foo, multiselect)
        self.assertEqual(
            subexpr.search({'foo': {'bar': 1, 'baz': 2, 'qux': 3}}),
            [1, 2])

    def test_flattened_wildcard(self):
        # foo[].bar
        parsed = ast.Projection(ast.Flatten(ast.Field('foo')),
                                ast.Field('bar'))
        data = {'foo': [{'bar': 1}, {'bar': 2}, {'bar': 3}]}
        self.assertEqual(parsed.search(data), [1, 2, 3])

    def test_multiple_nested_wildcards(self):
        # foo[].bar[].baz
        parsed = ast.Projection(
            ast.Flatten(ast.Projection(ast.Flatten(ast.Field('foo')),
                                       ast.Field('bar'))),
            ast.Field('baz'))
        data = {
            "foo": [
                {"bar": [{"baz": 1}, {"baz": 2}]},
                {"bar": [{"baz": 3}, {"baz": 4}]},
            ]
        }
        self.assertEqual(parsed.search(data), [1, 2, 3, 4])

    def test_multiple_nested_wildcards_with_list_values(self):
        # foo[].bar[].baz
        parsed = ast.Projection(
            ast.Flatten(ast.Projection(ast.Flatten(ast.Field('foo')),
                                       ast.Field('bar'))),
            ast.Field('baz'))
        data = {
            "foo": [
                {"bar": [{"baz": [1]}, {"baz": [2]}]},
                {"bar": [{"baz": [3]}, {"baz": [4]}]},
            ]
        }
        self.assertEqual(parsed.search(data), [[1], [2], [3], [4]])

    def test_flattened_multiselect_list(self):
        # foo[].[bar,baz]
        field_bar = ast.Field('bar')
        field_baz = ast.Field('baz')
        multiselect = ast.MultiFieldList([field_bar, field_baz])
        projection = ast.Projection(
            ast.Flatten(ast.Field('foo')),
            multiselect)
        self.assertEqual(
            projection.search({'foo': [{'bar': 1, 'baz': 2, 'qux': 3}]}),
            [[1, 2]])

    def test_flattened_multiselect_with_none(self):
        # foo[].[bar,baz]
        field_bar = ast.Field('bar')
        field_baz = ast.Field('baz')
        multiselect = ast.MultiFieldList([field_bar, field_baz])
        projection = ast.Projection(
            ast.Flatten(ast.Field('foo')),
            multiselect)
        self.assertEqual(
            projection.search({'bar': [{'bar': 1, 'baz': 2, 'qux': 3}]}),
            None)

    def test_operator_eq(self):
        field_foo = ast.Field('foo')
        field_foo2 = ast.Field('foo')
        eq = ast.OPEquals(field_foo, field_foo2)
        self.assertTrue(eq.search({'foo': 'bar'}))

    def test_operator_not_equals(self):
        field_foo = ast.Field('foo')
        field_bar = ast.Field('bar')
        eq = ast.OPNotEquals(field_foo, field_bar)
        self.assertTrue(eq.search({'foo': '1', 'bar': '2'}))

    def test_operator_lt(self):
        field_foo = ast.Field('foo')
        field_bar = ast.Field('bar')
        eq = ast.OPLessThan(field_foo, field_bar)
        self.assertTrue(eq.search({'foo': 1, 'bar': 2}))

    def test_filter_expression(self):
        # foo[?bar==`yes`]
        field_foo = ast.Field('foo')
        field_bar = ast.Field('bar')
        literal = ast.Literal('yes')
        eq = ast.OPEquals(field_bar, literal)
        filter_expression = ast.FilterExpression(eq)
        full_expression = ast.SubExpression(field_foo, filter_expression)
        match = full_expression.search(
            {'foo': [{'bar': 'yes', 'v': 1},
                     {'bar': 'no', 'v': 2},
                     {'bar': 'yes', 'v': 3},]})
        self.assertEqual(match, [{'bar': 'yes', 'v': 1},
                                 {'bar': 'yes', 'v': 3}])

    def test_projection_simple(self):
        # foo[*].bar
        field_foo = ast.Field('foo')
        field_bar = ast.Field('bar')
        projection = ast.Projection(field_foo, field_bar)
        data = {'foo': [{'bar': 1}, {'bar': 2}, {'bar': 3}]}
        self.assertEqual(projection.search(data), [1, 2, 3])

    def test_projection_no_left(self):
        # [*].bar
        field_bar = ast.Field('bar')
        projection = ast.Projection(ast.Identity(), field_bar)
        data = [{'bar': 1}, {'bar': 2}, {'bar': 3}]
        self.assertEqual(projection.search(data), [1, 2, 3])

    def test_projection_no_right(self):
        # foo[*]
        field_foo = ast.Field('foo')
        projection = ast.Projection(field_foo, ast.Identity())
        data = {'foo': [{'bar': 1}, {'bar': 2}, {'bar': 3}]}
        self.assertEqual(projection.search(data),
                         [{'bar': 1}, {'bar': 2}, {'bar': 3}])

    def test_bare_projection(self):
        # [*]
        projection = ast.Projection(ast.Identity(), ast.Identity())
        data = [{'bar': 1}, {'bar': 2}, {'bar': 3}]
        self.assertEqual(projection.search(data), data)

    def test_base_projection_on_invalid_type(self):
        # [*]
        data = {'foo': [{'bar': 1}, {'bar': 2}, {'bar': 3}]}
        projection = ast.Projection(ast.Identity(), ast.Identity())
        # search() should return None because the evaluated
        # type is a dict, not a list.
        self.assertIsNone(projection.search(data))

    def test_multiple_projections(self):
        # foo[*].bar[*].baz
        field_foo = ast.Field('foo')
        field_bar = ast.Field('bar')
        field_baz = ast.Field('baz')
        second_projection = ast.Projection(field_bar, field_baz)
        first_projection = ast.Projection(field_foo, second_projection)
        data = {
            'foo': [
                {'bar': [{'baz': 1}, {'baz': 2}, {'baz': 3}],
                 'other': 1},
                {'bar': [{'baz': 4}, {'baz': 5}, {'baz': 6}],
                 'other': 2},
            ]
        }
        self.assertEqual(first_projection.search(data),
                         [[1, 2, 3], [4, 5, 6]])

    def test_values_projection(self):
        # foo.*.bar
        field_foo = ast.Field('foo')
        field_bar = ast.Field('bar')
        projection = ast.ValueProjection(field_foo, field_bar)
        data = {
            'foo': {
                'a': {'bar': 1},
                'b': {'bar': 2},
                'c': {'bar': 3},
            }

        }
        result = list(sorted(projection.search(data)))
        self.assertEqual(result, [1, 2, 3])

    def test_root_value_projection(self):
        # *
        projection = ast.ValueProjection(ast.Identity(), ast.Identity())
        data = {
            'a': 1,
            'b': 2,
            'c': 3,
        }
        result = list(sorted(projection.search(data)))
        self.assertEqual(result, [1, 2, 3])

    def test_no_left_node_value_projection(self):
        # *.bar
        field_bar = ast.Field('bar')
        projection = ast.ValueProjection(ast.Identity(), field_bar)
        data = {
            'a': {'bar': 1},
            'b': {'bar': 2},
            'c': {'bar': 3},
        }
        result = list(sorted(projection.search(data)))
        self.assertEqual(result, [1, 2, 3])

    def test_no_right_node_value_projection(self):
        # foo.*
        field_foo = ast.Field('foo')
        projection = ast.ValueProjection(field_foo, ast.Identity())
        data = {
            'foo': {
                'a': 1,
                'b': 2,
                'c': 3,
            }
        }
        result = list(sorted(projection.search(data)))
        self.assertEqual(result, [1, 2, 3])

    def test_filter_projection(self):
        # foo[?bar==`1`].baz
        field_foo = ast.Field('foo')
        field_bar = ast.Field('bar')
        field_baz = ast.Field('baz')
        literal = ast.Literal(1)
        comparator = ast.OPEquals(field_bar, literal)
        filter_projection = ast.FilterProjection(field_foo, field_baz, comparator)
        data = {
            'foo': [{'bar': 1}, {'bar': 2}, {'bar': 1, 'baz': 3}]
        }
        result = filter_projection.search(data)
        self.assertEqual(result, [3])

    def test_nested_filter_projection(self):
        data = {
            "reservations": [
                {
                    "instances": [
                        {
                            "foo": 1,
                            "bar": 2
                        },
                        {
                            "foo": 1,
                            "bar": 3
                        },
                        {
                            "foo": 1,
                            "bar": 2
                        },
                        {
                            "foo": 2,
                            "bar": 1
                        }
                    ]
                }
            ]
        }
        projection = ast.Projection(
            ast.Flatten(ast.Field('reservations')),
            ast.FilterProjection(
                ast.Field('instances'),
                ast.Identity(),
                ast.OPEquals(ast.Field('bar'), ast.Literal(1))))
        self.assertEqual(projection.search(data), [[{'bar': 1, 'foo': 2}]])


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_compliance
import os
from pprint import pformat
from tests import OrderedDict
from tests import json

from nose.tools import assert_equal

import jmespath


TEST_DIR = os.path.join(
    os.path.dirname(os.path.abspath(__file__)),
    'compliance')
NOT_SPECIFIED = object()


def test_compliance():
    for full_path in _walk_files():
        if full_path.endswith('.json'):
            for given, expression, result, error in _load_cases(full_path):
                if error is NOT_SPECIFIED and result is not NOT_SPECIFIED:
                    yield (_test_expression, given, expression,
                        result, os.path.basename(full_path))
                elif result is NOT_SPECIFIED and error is not NOT_SPECIFIED:
                    yield (_test_error_expression, given, expression,
                           error, os.path.basename(full_path))
                else:
                    parts = (given, expression, result, error)
                    raise RuntimeError("Invalid test description: %s" % parts)


def _walk_files():
    # Check for a shortcut when running the tests interactively.
    # If a JMESPATH_TEST is defined, that file is used as the
    # only test to run.  Useful when doing feature development.
    single_file = os.environ.get('JMESPATH_TEST')
    if single_file is not None:
        yield os.path.abspath(single_file)
    else:
        for root, dirnames, filenames in os.walk(TEST_DIR):
            for filename in filenames:
                yield os.path.join(root, filename)


def _load_cases(full_path):
    all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict)
    for test_data in all_test_data:
        given = test_data['given']
        for case in test_data['cases']:
            yield (given, case['expression'],
                   case.get('result', NOT_SPECIFIED),
                   case.get('error', NOT_SPECIFIED))


def _test_expression(given, expression, expected, filename):
    import jmespath.parser
    try:
        parsed = jmespath.compile(expression)
    except ValueError as e:
        raise AssertionError(
            'jmespath expression failed to compile: "%s", error: %s"' %
            (expression, e))
    actual = parsed.search(given)
    expected_repr = json.dumps(expected, indent=4)
    actual_repr = json.dumps(actual, indent=4)
    error_msg = ("\n\n  (%s) The expression '%s' was suppose to give:\n%s\n"
                 "Instead it matched:\n%s\nparsed as:\n%s\ngiven:\n%s" % (
                     filename, expression, expected_repr,
                     actual_repr, parsed, json.dumps(given, indent=4)))
    error_msg = error_msg.replace(r'\n', '\n')
    assert_equal(actual, expected, error_msg)


def _test_error_expression(given, expression, error, filename):
    import jmespath.parser
    if error not in ('syntax', 'invalid-type',
                     'unknown-function', 'invalid-arity'):
        raise RuntimeError("Unknown error type '%s'" % error)
    try:
        parsed = jmespath.compile(expression)
        parsed.search(given)
    except ValueError as e:
        # Test passes, it raised a parse error as expected.
        pass
    else:
        error_msg = ("\n\n  (%s) The expression '%s' was suppose to be a "
                     "syntax error, but it successfully parsed as:\n\n%s" % (
                         filename, expression, parsed))
        error_msg = error_msg.replace(r'\n', '\n')
        raise AssertionError(error_msg)

########NEW FILE########
__FILENAME__ = test_lexer
from tests import unittest

from jmespath import lexer
from jmespath.exceptions import LexerError


class TestRegexLexer(unittest.TestCase):

    def setUp(self):
        self.lexer = lexer.Lexer()

    def assert_tokens(self, actual, expected):
        # The expected tokens only need to specify the
        # type and value.  The line/column numbers are not
        # checked, and we use assertEqual for the tests
        # that check those line numbers.
        stripped = []
        for item in actual:
            stripped.append({'type': item['type'], 'value': item['value']})
        # Every tokenization should end in eof, so we automatically
        # check that value, strip it off the end, and then
        # verify the remaining tokens against the expected.
        # That way the tests don't need to add eof to every
        # assert_tokens call.
        self.assertEqual(stripped[-1]['type'], 'eof')
        stripped.pop()
        self.assertEqual(stripped, expected)

    def test_field(self):
        tokens = list(self.lexer.tokenize('foo'))
        self.assert_tokens(tokens, [{'type': 'unquoted_identifier',
                                     'value': 'foo'}])

    def test_number(self):
        tokens = list(self.lexer.tokenize('24'))
        self.assert_tokens(tokens, [{'type': 'number',
                                     'value': 24}])

    def test_negative_number(self):
        tokens = list(self.lexer.tokenize('-24'))
        self.assert_tokens(tokens, [{'type': 'number',
                                     'value': -24}])

    def test_quoted_identifier(self):
        tokens = list(self.lexer.tokenize('"foobar"'))
        self.assert_tokens(tokens, [{'type': 'quoted_identifier',
                                     'value': "foobar"}])

    def test_json_escaped_value(self):
        tokens = list(self.lexer.tokenize('"\u2713"'))
        self.assert_tokens(tokens, [{'type': 'quoted_identifier',
                                     'value': u"\u2713"}])

    def test_number_expressions(self):
        tokens = list(self.lexer.tokenize('foo.bar.baz'))
        self.assert_tokens(tokens, [
            {'type': 'unquoted_identifier', 'value': 'foo'},
            {'type': 'dot', 'value': '.'},
            {'type': 'unquoted_identifier', 'value': 'bar'},
            {'type': 'dot', 'value': '.'},
            {'type': 'unquoted_identifier', 'value': 'baz'},
        ])

    def test_space_separated(self):
        tokens = list(self.lexer.tokenize('foo.bar[*].baz | a || b'))
        self.assert_tokens(tokens, [
            {'type': 'unquoted_identifier', 'value': 'foo'},
            {'type': 'dot', 'value': '.'},
            {'type': 'unquoted_identifier', 'value': 'bar'},
            {'type': 'lbracket', 'value': '['},
            {'type': 'star', 'value': '*'},
            {'type': 'rbracket', 'value': ']'},
            {'type': 'dot', 'value': '.'},
            {'type': 'unquoted_identifier', 'value': 'baz'},
            {'type': 'pipe', 'value': '|'},
            {'type': 'unquoted_identifier', 'value': 'a'},
            {'type': 'or', 'value': '||'},
            {'type': 'unquoted_identifier', 'value': 'b'},
        ])

    def test_literal(self):
        tokens = list(self.lexer.tokenize('`[0, 1]`'))
        self.assert_tokens(tokens, [
            {'type': 'literal', 'value': [0, 1]},
        ])

    def test_literal_string(self):
        tokens = list(self.lexer.tokenize('`foobar`'))
        self.assert_tokens(tokens, [
            {'type': 'literal', 'value': "foobar"},
        ])

    def test_literal_number(self):
        tokens = list(self.lexer.tokenize('`2`'))
        self.assert_tokens(tokens, [
            {'type': 'literal', 'value': 2},
        ])

    def test_position_information(self):
        tokens = list(self.lexer.tokenize('foo'))
        self.assertEqual(
            tokens,
            [{'type': 'unquoted_identifier', 'value': 'foo',
              'start': 0, 'end': 3},
              {'type': 'eof', 'value': '', 'start': 3, 'end': 3}]
        )

    def test_position_multiple_tokens(self):
        tokens = list(self.lexer.tokenize('foo.bar'))
        self.assertEqual(
            tokens,
            [{'type': 'unquoted_identifier', 'value': 'foo',
              'start': 0, 'end': 3},
             {'type': 'dot', 'value': '.',
              'start': 3, 'end': 4},
             {'type': 'unquoted_identifier', 'value': 'bar',
              'start': 4, 'end': 7},
             {'type': 'eof', 'value': '',
              'start': 7, 'end': 7},
             ]
        )

    def test_unknown_character(self):
        with self.assertRaises(LexerError):
            tokens = list(self.lexer.tokenize('foo[0^]'))

    def test_bad_first_character(self):
        with self.assertRaises(LexerError):
            tokens = list(self.lexer.tokenize('^foo[0]'))


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
__FILENAME__ = test_parser
#!/usr/bin/env python

from tests import unittest, as_s_expression

from jmespath import parser
from jmespath import ast
from jmespath import lexer
from jmespath import compat
from jmespath import exceptions


class TestParser(unittest.TestCase):
    def setUp(self):
        self.parser = parser.Parser()

    def test_field(self):
        parsed = self.parser.parse('foo')
        self.assertEqual(parsed.search({'foo': 'bar'}), 'bar')

    def test_dot_syntax(self):
        parsed = self.parser.parse('foo.bar')
        self.assertEqual(parsed.search({'foo': {'bar': 'baz'}}), 'baz')

    def test_multiple_dots(self):
        parsed = self.parser.parse('foo.bar.baz')
        self.assertEqual(
            parsed.search({'foo': {'bar': {'baz': 'correct'}}}), 'correct')

    def test_index(self):
        parsed = self.parser.parse('foo[1]')
        self.assertEqual(
            parsed.search({'foo': ['zero', 'one', 'two']}),
            'one')

    def test_quoted_subexpression(self):
        parsed = self.parser.parse('"foo"."bar"')
        self.assertIsInstance(parsed.parsed, ast.SubExpression)
        self.assertEqual(parsed.parsed.children[0].name, 'foo')
        self.assertEqual(parsed.parsed.children[1].name, 'bar')

    def test_wildcard(self):
        parsed = self.parser.parse('foo[*]')
        self.assertEqual(
            parsed.search({'foo': ['zero', 'one', 'two']}),
            ['zero', 'one', 'two'])

    def test_wildcard_with_children(self):
        parsed = self.parser.parse('foo[*].bar')
        self.assertEqual(
            parsed.search({'foo': [{'bar': 'one'}, {'bar': 'two'}]}),
            ['one', 'two'])

    def test_or_expression(self):
        parsed = self.parser.parse('foo || bar')
        self.assertEqual(parsed.search({'foo': 'foo'}), 'foo')
        self.assertEqual(parsed.search({'bar': 'bar'}), 'bar')
        self.assertEqual(parsed.search({'foo': 'foo', 'bar': 'bar'}), 'foo')
        self.assertEqual(parsed.search({'bad': 'bad'}), None)

    def test_complex_or_expression(self):
        parsed = self.parser.parse('foo.foo || foo.bar')
        self.assertEqual(parsed.search({'foo': {'foo': 'foo'}}), 'foo')
        self.assertEqual(parsed.search({'foo': {'bar': 'bar'}}), 'bar')
        self.assertEqual(parsed.search({'foo': {'baz': 'baz'}}), None)

    def test_or_repr(self):
        parsed = self.parser.parse('foo || bar')
        self.assertEqual(repr(parsed), 'ORExpression(Field(foo), Field(bar))')

    def test_unicode_literals_escaped(self):
        parsed = self.parser.parse(r'`"\u2713"`')
        if compat.PY2:
            self.assertEqual(repr(parsed), r'Literal(\u2713)')
        else:
            self.assertEqual(repr(parsed), u'Literal(\u2713)')

    def test_unicode_pretty_print(self):
        parsed = self.parser.parse(r'`"\u2713"`')
        self.assertEqual(parsed.pretty_print(), u'Literal(\u2713)')

    def test_multiselect(self):
        parsed = self.parser.parse('foo.{bar: bar,baz: baz}')
        self.assertEqual(
            parsed.search({'foo': {'bar': 'bar', 'baz': 'baz', 'qux': 'qux'}}),
            {'bar': 'bar', 'baz': 'baz'})

    def test_multiselect_subexpressions(self):
        parsed = self.parser.parse('foo.{"bar.baz": bar.baz, qux: qux}')
        foo = parsed.search({'foo': {'bar': {'baz': 'CORRECT'}, 'qux': 'qux'}})
        self.assertEqual(
            parsed.search({'foo': {'bar': {'baz': 'CORRECT'}, 'qux': 'qux'}}),
            {'bar.baz': 'CORRECT', 'qux': 'qux'})

    def test_multiselect_with_all_quoted_keys(self):
        parsed = self.parser.parse('foo.{"bar": bar.baz, "qux": qux}')
        result = parsed.search({'foo': {'bar': {'baz': 'CORRECT'}, 'qux': 'qux'}})
        self.assertEqual(result, {"bar": "CORRECT", "qux": "qux"})


class TestErrorMessages(unittest.TestCase):

    def setUp(self):
        self.parser = parser.Parser()

    def assert_error_message(self, expression, error_message,
                             exception=exceptions.ParseError):
        try:
            self.parser.parse(expression)
        except exception as e:
            self.assertEqual(error_message, str(e))
            return
        except Exception as e:
            self.fail(
                "Unexpected error raised (%s: %s) for bad expression: %s" %
                (e.__class__.__name__, e, expression))
        else:
            self.fail(
                "ParseError not raised for bad expression: %s" % expression)

    def test_bad_parse(self):
        with self.assertRaises(exceptions.ParseError):
            parsed = self.parser.parse('foo]baz')

    def test_bad_parse_error_message(self):
        error_message = (
            'Unexpected token: ]: Parse error at column 3 '
            'near token "]" (RBRACKET) for expression:\n'
            '"foo]baz"\n'
            '    ^')
        self.assert_error_message('foo]baz', error_message)

    def test_bad_parse_error_message_with_multiselect(self):
        error_message = (
            'Invalid jmespath expression: Incomplete expression:\n'
            '"foo.{bar: baz,bar: bar"\n'
            '                       ^')
        self.assert_error_message('foo.{bar: baz,bar: bar', error_message)

    def test_bad_lexer_values(self):
        error_message = (
            'Bad jmespath expression: '
            'Starting quote is missing the ending quote:\n'
            'foo."bar\n'
            '    ^')
        self.assert_error_message('foo."bar', error_message,
                                  exception=exceptions.LexerError)

    def test_bad_lexer_literal_value_with_json_object(self):
        error_message = ('Bad jmespath expression: '
                         'Bad token `{{}`:\n`{{}`\n^')
        self.assert_error_message('`{{}`', error_message,
                                  exception=exceptions.LexerError)


    def test_bad_unicode_string(self):
        error_message = ('Bad jmespath expression: '
                         'Invalid \\uXXXX escape:\n"\\uAZ12"\n^')
        self.assert_error_message(r'"\uAZ12"', error_message,
                                  exception=exceptions.LexerError)


class TestParserWildcards(unittest.TestCase):
    def setUp(self):
        self.parser = parser.Parser()
        self.data = {
            'foo': [
                {'bar': [{'baz': 'one'}, {'baz': 'two'}]},
                {'bar': [{'baz': 'three'}, {'baz': 'four'}, {'baz': 'five'}]},
            ]
        }

    def test_multiple_index_wildcards(self):
        parsed = self.parser.parse('foo[*].bar[*].baz')
        self.assertEqual(parsed.search(self.data),
                         [['one', 'two'], ['three', 'four', 'five']])

    def test_wildcard_mix_with_indices(self):
        parsed = self.parser.parse('foo[*].bar[0].baz')
        self.assertEqual(parsed.search(self.data),
                         ['one', 'three'])

    def test_wildcard_mix_last(self):
        parsed = self.parser.parse('foo[0].bar[*].baz')
        self.assertEqual(parsed.search(self.data),
                         ['one', 'two'])

    def test_indices_out_of_bounds(self):
        parsed = self.parser.parse('foo[*].bar[2].baz')
        self.assertEqual(parsed.search(self.data),
                         ['five'])

    def test_root_indices(self):
        parsed = self.parser.parse('[0]')
        self.assertEqual(parsed.search(['one', 'two']), 'one')

    def test_root_wildcard(self):
        parsed = self.parser.parse('*.foo')
        data = {'top1': {'foo': 'bar'}, 'top2': {'foo': 'baz'},
                'top3': {'notfoo': 'notfoo'}}
        # Sorted is being used because the order of the keys are not
        # required to be in any specific order.
        self.assertEqual(sorted(parsed.search(data)), sorted(['bar', 'baz']))
        self.assertEqual(sorted(self.parser.parse('*.notfoo').search(data)),
                         sorted(['notfoo']))

    def test_only_wildcard(self):
        parsed = self.parser.parse('*')
        data = {'foo': 'a', 'bar': 'b', 'baz': 'c'}
        self.assertEqual(sorted(parsed.search(data)), sorted(['a', 'b', 'c']))

    def test_escape_sequences(self):
        self.assertEqual(self.parser.parse(r'"foo\tbar"').search(
            {'foo\tbar': 'baz'}), 'baz')
        self.assertEqual(self.parser.parse(r'"foo\nbar"').search(
            {'foo\nbar': 'baz'}), 'baz')
        self.assertEqual(self.parser.parse(r'"foo\bbar"').search(
            {'foo\bbar': 'baz'}), 'baz')
        self.assertEqual(self.parser.parse(r'"foo\fbar"').search(
            {'foo\fbar': 'baz'}), 'baz')
        self.assertEqual(self.parser.parse(r'"foo\rbar"').search(
            {'foo\rbar': 'baz'}), 'baz')

    def test_consecutive_escape_sequences(self):
        parsed = self.parser.parse(r'"foo\\nbar"')
        self.assertEqual(parsed.search({'foo\\nbar': 'baz'}), 'baz')

        parsed = self.parser.parse(r'"foo\n\t\rbar"')
        self.assertEqual(parsed.search({'foo\n\t\rbar': 'baz'}), 'baz')

    def test_escape_sequence_at_end_of_string_not_allowed(self):
        with self.assertRaises(ValueError):
            parsed = self.parser.parse('foobar\\')

    def test_wildcard_with_multiselect(self):
        parsed = self.parser.parse('foo.*.{a: a, b: b}')
        data = {
            'foo': {
                'one': {
                    'a': {'c': 'CORRECT', 'd': 'other'},
                    'b': {'c': 'ALSOCORRECT', 'd': 'other'},
                },
                'two': {
                    'a': {'c': 'CORRECT', 'd': 'other'},
                    'c': {'c': 'WRONG', 'd': 'other'},
                },
            }
        }
        match = parsed.search(data)
        self.assertEqual(len(match), 2)
        self.assertIn('a', match[0])
        self.assertIn('b', match[0])
        self.assertIn('a', match[1])
        self.assertIn('b', match[1])

class TestMergedLists(unittest.TestCase):
    def setUp(self):
        self.parser = parser.Parser()
        self.data = {
            "foo": [
                [["one", "two"], ["three", "four"]],
                [["five", "six"], ["seven", "eight"]],
                [["nine"], ["ten"]]
            ]
        }

    def test_merge_with_indices(self):
        parsed = self.parser.parse('foo[][0]')
        match = parsed.search(self.data)
        self.assertEqual(match, ["one", "three", "five", "seven",
                                 "nine", "ten"])

    def test_trailing_merged_operator(self):
        parsed = self.parser.parse('foo[]')
        match = parsed.search(self.data)
        self.assertEqual(
            match,
            [["one", "two"], ["three", "four"],
             ["five", "six"], ["seven", "eight"],
             ["nine"], ["ten"]])


class TestParserCaching(unittest.TestCase):
    def test_compile_lots_of_expressions(self):
        # We have to be careful here because this is an implementation detail
        # that should be abstracted from the user, but we need to make sure we
        # exercise the code and that it doesn't blow up.
        p = parser.Parser()
        compiled = []
        compiled2 = []
        for i in range(parser.Parser._MAX_SIZE + 1):
            compiled.append(p.parse('foo%s' % i))
        # Rerun the test and half of these entries should be from the
        # cache but they should still be equal to compiled.
        for i in range(parser.Parser._MAX_SIZE + 1):
            compiled2.append(p.parse('foo%s' % i))
        self.assertEqual(len(compiled), len(compiled2))
        s = as_s_expression
        self.assertEqual(
            [s(expr.parsed) for expr in compiled],
            [s(expr.parsed) for expr in compiled2])

    def test_cache_purge(self):
        p = parser.Parser()
        first = p.parse('foo')
        cached = p.parse('foo')
        p.purge()
        second = p.parse('foo')
        self.assertEqual(as_s_expression(first.parsed),
                         as_s_expression(second.parsed))
        self.assertEqual(as_s_expression(first.parsed),
                         as_s_expression(cached.parsed))


class TestParserAddsExpressionAttribute(unittest.TestCase):
    def test_expression_available_from_parser(self):
        p = parser.Parser()
        parsed = p.parse('foo.bar')
        self.assertEqual(parsed.expression, 'foo.bar')


if __name__ == '__main__':
    unittest.main()

########NEW FILE########
