__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# Kuyruk documentation build configuration file, created by
# sphinx-quickstart on Fri Apr 26 09:36:49 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('..'))
import kuyruk

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'Kuyruk'
copyright = u'2013, Cenk Alt覺'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = kuyruk.__version__
# The full version, including alpha/beta/rc tags.
release = version

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
#keep_warnings = False


# -- Options for HTML output ---------------------------------------------------

sys.path.append(os.path.abspath('_themes'))

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'flask'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
html_theme_path = ['_themes']

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
html_logo = '_static/kuyruk-logo.png'

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
html_favicon = '../kuyruk/manager/static/favicon.ico'

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
# html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'Kuyrukdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Kuyruk.tex', u'Kuyruk Documentation',
   u'Cenk Alt覺', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'kuyruk', u'Kuyruk Documentation',
     [u'Cenk Alt覺'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'Kuyruk', u'Kuyruk Documentation',
   u'Cenk Alt覺', 'Kuyruk', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

# If true, do not generate a @detailmenu in the "Top" node's menu.
#texinfo_no_detailmenu = False

########NEW FILE########
__FILENAME__ = flask_theme_support
# flasky extensions.  flasky pygments style based on tango style
from pygments.style import Style
from pygments.token import Keyword, Name, Comment, String, Error, \
     Number, Operator, Generic, Whitespace, Punctuation, Other, Literal


class FlaskyStyle(Style):
    background_color = "#f8f8f8"
    default_style = ""

    styles = {
        # No corresponding class for the following:
        #Text:                     "", # class:  ''
        Whitespace:                "underline #f8f8f8",      # class: 'w'
        Error:                     "#a40000 border:#ef2929", # class: 'err'
        Other:                     "#000000",                # class 'x'

        Comment:                   "italic #8f5902", # class: 'c'
        Comment.Preproc:           "noitalic",       # class: 'cp'

        Keyword:                   "bold #004461",   # class: 'k'
        Keyword.Constant:          "bold #004461",   # class: 'kc'
        Keyword.Declaration:       "bold #004461",   # class: 'kd'
        Keyword.Namespace:         "bold #004461",   # class: 'kn'
        Keyword.Pseudo:            "bold #004461",   # class: 'kp'
        Keyword.Reserved:          "bold #004461",   # class: 'kr'
        Keyword.Type:              "bold #004461",   # class: 'kt'

        Operator:                  "#582800",   # class: 'o'
        Operator.Word:             "bold #004461",   # class: 'ow' - like keywords

        Punctuation:               "bold #000000",   # class: 'p'

        # because special names such as Name.Class, Name.Function, etc.
        # are not recognized as such later in the parsing, we choose them
        # to look the same as ordinary variables.
        Name:                      "#000000",        # class: 'n'
        Name.Attribute:            "#c4a000",        # class: 'na' - to be revised
        Name.Builtin:              "#004461",        # class: 'nb'
        Name.Builtin.Pseudo:       "#3465a4",        # class: 'bp'
        Name.Class:                "#000000",        # class: 'nc' - to be revised
        Name.Constant:             "#000000",        # class: 'no' - to be revised
        Name.Decorator:            "#888",           # class: 'nd' - to be revised
        Name.Entity:               "#ce5c00",        # class: 'ni'
        Name.Exception:            "bold #cc0000",   # class: 'ne'
        Name.Function:             "#000000",        # class: 'nf'
        Name.Property:             "#000000",        # class: 'py'
        Name.Label:                "#f57900",        # class: 'nl'
        Name.Namespace:            "#000000",        # class: 'nn' - to be revised
        Name.Other:                "#000000",        # class: 'nx'
        Name.Tag:                  "bold #004461",   # class: 'nt' - like a keyword
        Name.Variable:             "#000000",        # class: 'nv' - to be revised
        Name.Variable.Class:       "#000000",        # class: 'vc' - to be revised
        Name.Variable.Global:      "#000000",        # class: 'vg' - to be revised
        Name.Variable.Instance:    "#000000",        # class: 'vi' - to be revised

        Number:                    "#990000",        # class: 'm'

        Literal:                   "#000000",        # class: 'l'
        Literal.Date:              "#000000",        # class: 'ld'

        String:                    "#4e9a06",        # class: 's'
        String.Backtick:           "#4e9a06",        # class: 'sb'
        String.Char:               "#4e9a06",        # class: 'sc'
        String.Doc:                "italic #8f5902", # class: 'sd' - like a comment
        String.Double:             "#4e9a06",        # class: 's2'
        String.Escape:             "#4e9a06",        # class: 'se'
        String.Heredoc:            "#4e9a06",        # class: 'sh'
        String.Interpol:           "#4e9a06",        # class: 'si'
        String.Other:              "#4e9a06",        # class: 'sx'
        String.Regex:              "#4e9a06",        # class: 'sr'
        String.Single:             "#4e9a06",        # class: 's1'
        String.Symbol:             "#4e9a06",        # class: 'ss'

        Generic:                   "#000000",        # class: 'g'
        Generic.Deleted:           "#a40000",        # class: 'gd'
        Generic.Emph:              "italic #000000", # class: 'ge'
        Generic.Error:             "#ef2929",        # class: 'gr'
        Generic.Heading:           "bold #000080",   # class: 'gh'
        Generic.Inserted:          "#00A000",        # class: 'gi'
        Generic.Output:            "#888",           # class: 'go'
        Generic.Prompt:            "#745334",        # class: 'gp'
        Generic.Strong:            "bold #000000",   # class: 'gs'
        Generic.Subheading:        "bold #800080",   # class: 'gu'
        Generic.Traceback:         "bold #a40000",   # class: 'gt'
    }

########NEW FILE########
__FILENAME__ = config
RABBIT_HOST = 'localhost'
RABBIT_PORT = 5672
RABBIT_USER = 'guest'
RABBIT_PASSWORD = 'guest'
EAGER = False
QUEUES = {
    'host.example.com': 'kuyruk, 2*another, @local'
}

########NEW FILE########
__FILENAME__ = tasks
from kuyruk import Kuyruk
import config

kuyruk = Kuyruk(config)


@kuyruk.task
def echo(message):
    print message

########NEW FILE########
__FILENAME__ = usage
import logging
logging.basicConfig(level=logging.INFO)
logging.getLogger('pika').level = logging.INFO

from tasks import echo

# Since echo method is wrapped with task decorator
# it is going to be send to queue to run in background.
echo('hello world')

########NEW FILE########
__FILENAME__ = config
import os
import ast
import types
import logging
import multiprocessing


logger = logging.getLogger(__name__)


class Config(object):
    """Kuyruk configuration object. Default values are defined as
    class attributes.

    """
    # Worker Options
    ################

    WORKER_CLASS = 'kuyruk.worker.Worker'
    """Worker implementation class. It can be replaced with a subclass of
    :class:`~kuyruk.worker.Worker` to change specific behavior."""

    IMPORT_PATH = None
    """Worker imports tasks from this directory."""

    IMPORTS = []
    """By default worker imports the task modules lazily when it receive a
    task from the queue. If you specify the modules here they will be
    imported when the worker is started."""

    EAGER = False
    """Run tasks in the process without sending to queue. Useful in tests."""

    MAX_LOAD = None
    """Stop consuming queue when the load goes above this level."""

    MAX_WORKER_RUN_TIME = None
    """Gracefully shutdown worker after running this seconds.
    Master will detect that the worker is exited and will spawn a new
    worker with identical config.
    Can be used to force loading of new application code."""

    MAX_TASK_RUN_TIME = None
    """Fail the task if it takes more than this seconds."""

    SAVE_FAILED_TASKS = False
    """Save failed tasks to a queue (named kuyruk_failed) for inspecting and
    requeueing later."""

    QUEUES = {}
    """You can specify the hostnames and correspondant queues here.
    Master starts workers by getting the list of queues from this dictionary
    by hostname. If the hostname is not found in the keys then the master
    start a single worker to run on default queue (``kuyruk``).

    Keys are hostnames (socket.gethostname()), values are comma seperated
    list of queue names.

    Example::

        {'host1.example.com': 'a, 2*b'}

    host1 will run 3 worker processes; 1 for "a" and 2 for "b" queue."""

    DEFAULT_QUEUES = 'kuyruk, @kuyruk'
    """If the hostname is not found in :attr:`~kuyruk.Config.QUEUES`
    this value will be used as default."""

    LOGGING_LEVEL = 'INFO'
    """Logging level of root logger."""

    LOGGING_CONFIG = None
    """INI style logging configuration file.
    This has pecedence over ``LOGGING_LEVEL``."""

    SENTRY_DSN = None
    """Send exceptions to Sentry. Raven must be installed in order that
    this feature to work."""

    SENTRY_PROJECT_URL = None
    """Sentry project URL. Required to generate links to Sentry in Manager."""

    CLOSE_FDS = True
    """Patch subprocess.Popen constructor to always set close_fds=True.
    See `subprocess.Popen
    <http://docs.python.org/2/library/subprocess.html#popen-constructor>`_
    for additional information."""

    # Connection Options
    ####################

    RABBIT_HOST = 'localhost'
    """RabbitMQ host."""

    RABBIT_PORT = 5672
    """RabbitMQ port."""

    RABBIT_VIRTUAL_HOST = '/'
    """RabbitMQ virtual host."""

    RABBIT_USER = 'guest'
    """RabbitMQ user."""

    RABBIT_PASSWORD = 'guest'
    """RabbitMQ password."""

    REDIS_HOST = 'localhost'
    """Redis host."""

    REDIS_PORT = 6379
    """Redis port."""

    REDIS_DB = 0
    """Redis database."""

    REDIS_PASSWORD = None
    """Redis password."""

    # Manager Options
    #################

    MANAGER_HOST = None
    """Manager host that the workers will connect and send stats."""

    MANAGER_PORT = 16501
    """Manager port that the workers will connect and send stats."""

    MANAGER_HTTP_PORT = 16500
    """Manager HTTP port that the Flask application will run on."""

    def from_object(self, obj):
        """Load values from an object."""
        for key in dir(obj):
            if key.isupper():
                value = getattr(obj, key)
                setattr(self, key, value)
        logger.info("Config is loaded from object: %r", obj)

    def from_dict(self, d):
        """Load values from a dict."""
        for key, value in d.iteritems():
            if key.isupper():
                setattr(self, key, value)
        logger.info("Config is loaded from dict: %r", d)

    def from_pyfile(self, filename):
        """Load values from a Python file."""
        # Read the config file from a seperate process because it may contain
        # import statements doing import from user code. No user code should
        # be imported to master because they have to be imported in workers
        # after the master has forked. Otherwise newly created workers
        # cannot load new code after the master has started.
        def readfile(conn):
            logger.debug("Reading config file from seperate process...")
            try:
                globals_, locals_ = {}, {}
                execfile(filename, globals_, locals_)
                values = {}
                for key, value in locals_.iteritems():
                    if (key.isupper() and
                            not isinstance(value, types.ModuleType)):
                        values[key] = value
                conn.send(values)
                logger.debug("Config read successfully")
            except:
                logger.debug("Cannot read config")
                conn.send(None)
        parent_conn, child_conn = multiprocessing.Pipe()
        process = multiprocessing.Process(target=readfile,
                                          args=(child_conn, ))
        process.start()
        values = parent_conn.recv()
        process.join()
        assert values is not None, "Cannot load config file: %s" % filename
        self.from_dict(values)
        logger.info("Config is loaded from file: %s", filename)

    def from_env_vars(self):
        """Load values from environment variables."""
        for key, value in os.environ.iteritems():
            if key.startswith('KUYRUK_'):
                key = key.lstrip('KUYRUK_')
                self._eval_item(key, value)

    def from_cmd_args(self, args):
        """Load values from command line arguments."""
        def to_attr(option):
            return option.upper().replace('-', '_')

        for key, value in vars(args).iteritems():
            if value is not None:
                key = to_attr(key)
                self._eval_item(key, value)

    def _eval_item(self, key, value):
        if hasattr(Config, key):
            try:
                value = ast.literal_eval(value)
            except (ValueError, SyntaxError):
                pass
            setattr(self, key, value)

########NEW FILE########
__FILENAME__ = connection
import logging
from threading import RLock
from collections import defaultdict

from pika.adapters.blocking_connection import (BlockingConnection,
                                               BlockingChannel)

logger = logging.getLogger(__name__)


class Connection(BlockingConnection):

    def __init__(self, parameters=None):
        self._lock = RLock()
        super(Connection, self).__init__(parameters)

    def channel(self, channel_number=None):
        """Create a new channel with the next available or specified channel #.

        :param int channel_number: Specify the channel number

        """
        self._channel_open = False
        if not channel_number:
            channel_number = self._next_channel_number()
        logger.debug('Opening channel %i', channel_number)
        self._channels[channel_number] = Channel(self, channel_number)
        return self._channels[channel_number]

    def process_data_events(self):
        with self._lock:
            return super(Connection, self).process_data_events()

    def send_method(self, channel_number, method_frame, content=None):
        with self._lock:
            super(Connection, self).send_method(channel_number, method_frame,
                                                content)


class Channel(BlockingChannel):
    """Remembers the queues decalared and does not redeclare them."""

    SKIP_REDECLARE_QUEUE = True

    def __init__(self, connection, channel_number):
        super(Channel, self).__init__(connection, channel_number)
        self.declared = defaultdict(bool)

    def queue_declare(self, queue='', passive=False, durable=False,
                      exclusive=False, auto_delete=False, nowait=False,
                      arguments=None, force=False):
        if self.SKIP_REDECLARE_QUEUE and self.declared[queue] and not force:
            logger.debug("Queue is already declared, skipped declare.")
        else:
            rv = super(Channel, self).queue_declare(
                queue, passive, durable, exclusive,
                auto_delete, nowait, arguments)
            self.declared[queue] = True
            return rv

########NEW FILE########
__FILENAME__ = consumer
from __future__ import absolute_import
import logging
import threading
from Queue import Queue, Empty
from contextlib import contextmanager

from kuyruk.helpers import queue_get_all, start_daemon_thread
from kuyruk.message import Message

logger = logging.getLogger(__name__)


class Consumer(object):

    def __init__(self, queue):
        self.queue = queue
        self._generator = None
        self._generator_messages = Queue()
        self._stop_processing_data_events = threading.Event()
        self._message_iterator = None
        self.consuming = False

    @contextmanager
    def consume(self):
        """Blocking consumption of a queue."""
        # Issue Basic.Consume
        self._consume()

        # Start data events thread
        self._stop_processing_data_events.clear()
        start_daemon_thread(self._process_data_events)

        # Return message iterator
        self._message_iterator = MessageIterator(
            self._generator_messages, self.queue)
        yield self._message_iterator

        # Issue Basic.Cancel
        self._cancel()

        # Stop data events thread
        self._stop_processing_data_events.set()

    def stop(self):
        """Stop consumption of messages."""
        if self._message_iterator:
            self._message_iterator.stop()

    def pause(self, seconds):
        logger.info('Pausing for %i seconds...', seconds)
        self._cancel()
        self.queue.channel.connection.sleep(seconds)
        logger.info('Resuming')
        self._consume()

    def _consume(self):
        logger.debug('Start consuming')
        assert self._generator is None
        self._generator = self.queue.basic_consume(self._generator_callback)
        self.consuming = True

    def _generator_callback(self, unused, method, properties, body):
        """Called when a message is received from RabbitMQ and appended to the
        list of messages to be returned when a message is received by RabbitMQ.

        """
        logger.debug('Adding a message to generator messages')
        self._generator_messages.put((method, properties, body))

    def _cancel(self):
        """Cancel the consumption of a queue, rejecting all pending messages.

        """
        logger.debug('_cancel is called')
        count_messages = 0
        self.queue.basic_cancel(self._generator)
        if not self._generator_messages.empty():
            logger.debug('There are message pending, nacking all')
            remaining_messages = queue_get_all(self._generator_messages)
            last_message = remaining_messages[-1]
            method, properties, body = last_message
            count_messages = len(remaining_messages)
            logger.info('Requeueing %i messages with delivery tag %s',
                        count_messages, method.delivery_tag)
            self.queue.nack(method.delivery_tag, multiple=True, requeue=True)

        self._generator = None
        self.consuming = False
        return count_messages

    def _process_data_events(self):
        while not self._stop_processing_data_events.is_set():
            self.queue.channel.connection.process_data_events()


class MessageIterator(object):

    def __init__(self, messages, queue):
        self.messages = messages
        self.queue = queue
        self._stop = threading.Event()

    def __iter__(self):
        return self

    def next(self):
        logger.info('Waiting for new message...')
        while not self._stop.is_set():
            try:
                message = self.messages.get(timeout=0.1)
            except Empty:
                pass
            else:
                return Message(message, self.queue)

        logger.debug("Exiting from iterator")
        raise StopIteration

    def stop(self):
        self._stop.set()

########NEW FILE########
__FILENAME__ = events
"""
This module is called "events" instead of "signals" not to confuse with OS
level signals.

"""
from blinker import Namespace

_signals = Namespace()

task_prerun = _signals.signal('task-prerun')
task_postrun = _signals.signal('task-postrun')
task_success = _signals.signal('task-success')
task_failure = _signals.signal('task-failure')
task_presend = _signals.signal('task-presend')
task_postsend = _signals.signal('task-postsend')


class EventMixin(object):
    """
    This mixin class contains some decorator methods for wrapping handler
    functions to run on certain signals.

    """
    def on_prerun(self, f):
        """
        Registers a function to run before processing the task.
        If the registered function raises an exception the task will be
        considered as failed.

        """
        self.connect_signal(task_prerun, f)
        return f

    def on_postrun(self, f):
        """
        Registers a function to run after processing the task.
        The registered function will run regardless of condition that
        the task is successful or failed. You can attach some cleanup code
        here i.e. closing sockets or deleting files.

        """
        self.connect_signal(task_postrun, f)
        return f

    def on_success(self, f):
        """
        Registers a function to run after the task has returned.
        The registered function will only be run if the task is successful.

        """
        self.connect_signal(task_success, f)
        return f

    def on_failure(self, f):
        """
        Registers a function to run if task raises an exception.

        """
        self.connect_signal(task_failure, f)
        return f

    def on_presend(self, f):
        """
        Registers a function to run before the task is sent to the queue.
        Note that this is executed in the client process,
        the one sending the task, not in the worker.

        """
        self.connect_signal(task_presend, f)
        return f

    def on_postsend(self, f):
        """
        Registers a function to run after the task is sent to the queue.
        Note that this is executed in the client process,
        the one sending the task, not in the worker.

        """
        self.connect_signal(task_postsend, f)
        return f

    def connect_signal(self, signal, handler):
        signal.connect(handler, sender=self, weak=False)

########NEW FILE########
__FILENAME__ = exceptions
class KuyrukError(Exception):
    """Base class for Kuyruk related exceptions."""
    pass


class Reject(KuyrukError):
    """
    The task should raise this if it does not want to process the message.
    In this case message will be requeued and delivered to another worker.

    """
    pass


class ObjectNotFound(KuyrukError):
    """Internal exception that is raised
    when the worker cannot fetch the object of a class task."""
    pass


class Timeout(KuyrukError):
    """Raised if a task exceeds it's allowed run time."""
    pass


class InvalidTask(KuyrukError):
    """Raised when the message from queue is not valid."""
    pass

########NEW FILE########
__FILENAME__ = json_datetime
from __future__ import absolute_import

import json
import datetime

DATETIME_FORMAT = "%Y-%m-%dT%H:%M:%SZ"  # ISO 8601


class JSONEncoder(json.JSONEncoder):

    def default(self, obj):
        if isinstance(obj, datetime.datetime):
            return obj.strftime(DATETIME_FORMAT)
        else:
            return super(JSONEncoder, self).default(obj)


class JSONDecoder(json.JSONDecoder):

    def __init__(self, *args, **kwargs):
        json.JSONDecoder.__init__(self, *args,
                                  object_hook=self.object_hook, **kwargs)

    def object_hook(self, obj):
        if isinstance(obj, dict):
            for key in obj:
                if not isinstance(obj[key], basestring):
                    continue
                try:
                    obj[key] = datetime.datetime.strptime(obj[key],
                                                          DATETIME_FORMAT)
                except ValueError:
                    pass

        return obj


if __name__ == "__main__":
    encoder = JSONEncoder()
    print encoder.encode({"date": datetime.datetime.now()})

    decoder = JSONDecoder()
    print decoder.decode(encoder.encode({"date": datetime.datetime.now()}))

########NEW FILE########
__FILENAME__ = importer
import os
import sys
import logging
import importlib
from collections import namedtuple
from contextlib import contextmanager

logger = logging.getLogger(__name__)


def import_task(module_name, class_name, function_name, path=None):
    """Find and return the function for given function name."""
    namespace = import_module(module_name, path)
    if class_name:
        cls = getattr(namespace, class_name)
        namespace = cls

    return getattr(namespace, function_name)


def import_module(module_name, path=None):
    """Import module by searching main module, current working directory and
    Python path.

    """
    logger.debug("Importing module %r" % module_name)
    main_module, main_module_name = get_main_module()
    if module_name == main_module_name:
        return main_module

    if path is None:
        path = os.getcwd()

    with custom_path(path):
        return importlib.import_module(module_name)


def import_class_str(s):
    module, cls = s.rsplit('.', 1)
    module = import_module(module)
    return getattr(module, cls)


@contextmanager
def custom_path(path):
    if path in sys.path:
        yield
    else:
        sys.path.insert(0, path)
        try:
            yield
        finally:
            sys.path.remove(path)


def get_main_module():
    """Returns main module and module name pair."""
    main_module = sys.modules['__main__']
    if not hasattr(main_module, '__file__'):
        # if run from interactive shell
        return None, None
    filename = os.path.basename(main_module.__file__)
    module_name, ext = os.path.splitext(filename)
    MainModule = namedtuple('MainModule', ['module', 'name'])
    return MainModule(module=main_module, name=module_name)

########NEW FILE########
__FILENAME__ = master
from __future__ import absolute_import
import os
import sys
import errno
import signal
import socket
import string
import logging
import resource
import itertools
import threading
from time import sleep
from collections import namedtuple

import rpyc
from setproctitle import setproctitle

from kuyruk import __version__, helpers
from kuyruk.process import KuyrukProcess


logger = logging.getLogger(__name__)


class Master(KuyrukProcess):
    """Spawn multiple workers and supervise them by reading the list of queues
    from configuration.

    """
    def __init__(self, config):
        super(Master, self).__init__(config)
        self.workers = {}
        self.lock = threading.Lock()

    def run(self):
        super(Master, self).run()
        setproctitle('kuyruk: master')
        self.maybe_start_manager_rpc_service()
        self.start_workers()
        self.wait_for_workers()
        logger.debug('End run master')

    def rpc_service_class(self):
        class _Service(rpyc.Service):
            exposed_get_stats = self.get_stats
            exposed_warm_shutdown = self.warm_shutdown
            exposed_cold_shutdown = self.cold_shutdown
            exposed_abort = self.abort
        return _Service

    def start_workers(self):
        """Start a new worker for each queue."""
        queues = self.get_queues()
        queues = parse_queues_str(queues)
        logger.info('Starting to work on queues: %s', queues)
        for queue in queues:
            self.create_new_worker(queue)

    def get_queues(self):
        """Return queues string by reading from configuration."""
        if isinstance(self.config.QUEUES, basestring):
            return self.config.QUEUES

        hostname = socket.gethostname()
        try:
            return self.config.QUEUES[hostname]
        except KeyError:
            logger.warning(
                'No queues specified for host %r. '
                'Listening on default queues.', hostname)
            return self.config.DEFAULT_QUEUES

    def shutdown_workers(self, kill=False):
        """Send shutdown signal to all workers."""
        if kill:
            fn, sig = os.killpg, signal.SIGKILL
        else:
            fn, sig = os.kill, signal.SIGTERM

        for worker in self.workers.itervalues():
            try:
                logger.info("Sending signal %s to PID %s", sig, worker.pid)
                fn(worker.pid, sig)
            except OSError as e:
                if e.errno == errno.ESRCH:
                    # Worker may be restarting and dead
                    logger.warning("Cannot find PID %s", worker.pid)
                else:
                    raise

    def wait_for_workers(self):
        """Loop until any of the self.workers is alive.
        If a worker is dead and Kuyruk is running state, spawns a new worker.

        """
        retry_wait = helpers.retry_on_eintr(os.wait)
        while self.workers:
            pid, status = retry_wait()
            logger.info("Worker: %s is dead", pid)
            worker = self.workers[pid]
            worker.kill_pg()
            del self.workers[pid]
            if not self.shutdown_pending.is_set():
                self.respawn_worker(worker)
                sleep(1)  # Prevent cpu burning in case a worker cant start

    def respawn_worker(self, worker):
        """Spawn a new process with parameters same as the old worker."""
        logger.warning("Respawning worker %s", worker)
        self.create_new_worker(worker.queue)
        logger.debug(self.workers)

    def create_new_worker(self, queue):
        if self.shutdown_pending.is_set():
            logger.info("Shutdown is pending. Skipped spawning new worker.")
            return
        worker = WorkerProcess(self.kuyruk, queue)
        with self.lock:
            worker.start()
        self.workers[worker.pid] = worker

    def register_signals(self):
        super(Master, self).register_signals()
        signal.signal(signal.SIGTERM, self.handle_sigterm)
        signal.signal(signal.SIGQUIT, self.handle_sigquit)
        signal.signal(signal.SIGABRT, self.handle_sigabrt)

    def handle_sigterm(self, signum, frame):
        logger.warning("Handling SIGTERM")
        self.warm_shutdown()

    def handle_sigquit(self, signum, frame):
        logger.warning("Handling SIGQUIT")
        self.cold_shutdown()

    def handle_sigabrt(self, signum, frame):
        logger.warning("Handling SIGABRT")
        self.abort()

    def warm_shutdown(self):
        logger.warning("Warm shutdown")
        self.shutdown_pending.set()
        self.shutdown_workers()

    def cold_shutdown(self):
        logger.warning("Cold shutdown")
        self.shutdown_pending.set()
        self.shutdown_workers(kill=True)
        self._exit(0)

    def abort(self):
        """Exit immediately making workers orphan."""
        logger.warning("Aborting")
        self._exit(0)

    def get_stats(self):
        """Generate stats to be sent to manager."""
        return {
            'type': 'master',
            'hostname': socket.gethostname(),
            'uptime': self.uptime,
            'pid': os.getpid(),
            'version': __version__,
            'workers': len(self.workers),
            'load': os.getloadavg(),
        }


class WorkerProcess(object):

    def __init__(self, kuyruk, queue):
        self.kuyruk = kuyruk
        self.queue = queue
        self.pid = None

    def start(self):
        logger.debug("Forking...")
        pid = os.fork()
        if pid:
            # parent
            logger.debug("I am parent")
            self.pid = pid
        else:
            # child
            logger.debug("I am child")
            self.run_worker()

    def run_worker(self):
        logger.debug("Hello from worker")

        logger.debug("Setting new process group")
        os.setpgrp()

        self.close_fds()

        # Fake command line queue argument
        Args = namedtuple('Args', 'queue')
        args = Args(queue=self.queue)

        # This will run as if called "kuyruk worker" from command line
        from kuyruk.__main__ import run_worker
        logger.debug("Running worker command")
        run_worker(self.kuyruk, args)

        sys.exit(0)

    def close_fds(self):
        logger.debug("Closing open file descriptors...")
        maxfd = resource.getrlimit(resource.RLIMIT_NOFILE)[1]
        if maxfd == resource.RLIM_INFINITY:
            maxfd = 1024

        # Do not close stdin, stdout and stderr (0, 1, 2)
        os.closerange(3, maxfd)

    def kill_pg(self):
        """Kill the process with their children. Does not raise exception
        if the process is not alive.

        """
        logger.debug("Killing worker's process group with SIGKILL")
        try:
            os.killpg(self.pid, signal.SIGKILL)
        except OSError as e:
            if e.errno != errno.ESRCH:  # No such process
                raise


def parse_queues_str(s):
    """Parse command line queues string.

    :param s: Command line or configuration queues string
    :return: list of queue names
    """
    queues = (q.strip() for q in s.split(','))
    return list(itertools.chain.from_iterable(expand_count(q) for q in queues))


def expand_count(q):
    parts = q.split('*', 1)
    parts = map(string.strip, parts)
    if len(parts) > 1:
        try:
            return int(parts[0]) * [parts[1]]
        except ValueError:
            return int(parts[1]) * [parts[0]]
    return [parts[0]]

########NEW FILE########
__FILENAME__ = message
import json
import pickle
import logging

from .helpers.json_datetime import JSONEncoder, JSONDecoder

logger = logging.getLogger(__name__)


class Message(object):

    def __init__(self, message, queue):
        """
        :param message: return value of pika.Channel.basic_get
        :param queue: kuyruk.queue.Queue instance

        """
        self.queue = queue
        self.method = message[0]
        self.properties = message[1]
        self.body = message[2]

    def ack(self):
        self.queue.ack(self.method.delivery_tag)

    def reject(self):
        self.queue.reject(self.method.delivery_tag)

    def discard(self):
        self.queue.discard(self.method.delivery_tag)

    def get_object(self):
        """Decode and return wrapped message body."""
        return self.decode((self.method, self.properties, self.body))[1]

    @staticmethod
    def encode(obj):
        return json.dumps(obj, cls=JSONEncoder)

    @staticmethod
    def decode(message):
        method, properties, body = message
        if body is None:
            return None, None

        if properties.content_type == 'application/json':
            obj = json.loads(body, cls=JSONDecoder)
        elif properties.content_type == 'application/python-pickle':
            obj = pickle.loads(body)
        else:
            raise TypeError('Unknown content type')

        logger.debug('Message decoded: %s', obj)
        return method.delivery_tag, obj

########NEW FILE########
__FILENAME__ = process
from __future__ import absolute_import
import os
import sys
import signal
import logging
import logging.config
import threading
import subprocess
from time import time

import rpyc

from kuyruk.helpers import monkeypatch_method, print_stack, start_daemon_thread, retry


logger = logging.getLogger(__name__)


class KuyrukProcess(object):
    """Base class for Master and Worker classes.
    Contains some shared code betwee these 2 classes.

    """
    def __init__(self, kuyruk):
        from kuyruk import Kuyruk
        assert isinstance(kuyruk, Kuyruk)
        self.kuyruk = kuyruk
        self.shutdown_pending = threading.Event()
        self.manager_thread = None
        self.popen = None

    @property
    def config(self):
        return self.kuyruk.config

    def run(self):
        if self.config.CLOSE_FDS is True:
            self.patch_popen()

        self.setup_logging()
        self.register_signals()
        logger.debug('PID: %s PGID: %s', os.getpid(), os.getpgrp())
        self.started = time()

    def patch_popen(self):
        """Patch subprocess.Popen constructor to close_fds by default."""
        original_init = subprocess.Popen.__init__

        @monkeypatch_method(subprocess.Popen)
        def __init__(self, *args, **kwargs):
            close_fds = kwargs.pop('close_fds', True)
            original_init(self, *args, close_fds=close_fds, **kwargs)

    def register_signals(self):
        signal.signal(signal.SIGINT, self.handle_sigint)
        signal.signal(signal.SIGUSR1, print_stack)  # for debugging

    def handle_sigint(self, signum, frame):
        """If running from terminal pressing Ctrl-C will initiate a warm
        shutdown. The second interrupt will do a cold shutdown.

        """
        logger.warning("Handling SIGINT")
        if sys.stdin.isatty() and not self.shutdown_pending.is_set():
            self.warm_shutdown()
        else:
            self.cold_shutdown()
        logger.debug("Handled SIGINT")

    @staticmethod
    def _exit(status=0):
        """Flush output buffers and exit from process."""
        sys.stdout.flush()
        sys.stderr.flush()
        os._exit(status)

    def maybe_start_manager_rpc_service(self):
        if self.config.MANAGER_HOST:
            start_daemon_thread(target=retry()(self._connect_rpc))

    def _connect_rpc(self):
        conn = rpyc.connect(self.config.MANAGER_HOST,
                            self.config.MANAGER_PORT,
                            service=self.rpc_service_class(),
                            config={"allow_pickle": True})
        rpyc.BgServingThread(conn)._thread.join()

    def rpc_service_class(self):
        raise NotImplementedError

    def get_stats(self):
        """Returns the stats for sending to Kuyruk Manager.
        Called by the manager thread. (self.manager_thread)"""
        raise NotImplementedError

    def on_action(self, sock, action):
        """Run the function sent by the manager."""
        f, args, kwargs = action
        logger.info("action: f=%s, args=%s, kwargs=%s", f, args, kwargs)
        f = getattr(self, f)
        f(*args, **kwargs)

    @property
    def uptime(self):
        return int(time() - self.started)

    def setup_logging(self):
        if self.config.LOGGING_CONFIG:
            logging.config.fileConfig(self.config.LOGGING_CONFIG)
        else:
            logging.getLogger('pika').level = logging.WARNING
            level = getattr(logging, self.config.LOGGING_LEVEL.upper())
            fmt = "%(levelname).1s %(process)d " \
                  "%(name)s.%(funcName)s:%(lineno)d - %(message)s"
            logging.basicConfig(level=level, format=fmt)

########NEW FILE########
__FILENAME__ = queue
from __future__ import absolute_import
import socket
import logging

import pika

from kuyruk.message import Message


logger = logging.getLogger(__name__)


class Queue(object):

    def __init__(self, name, channel, local=False):
        if channel is None:
            from kuyruk import Kuyruk
            channel = Kuyruk().channel()

        self.name = name
        self.channel = channel
        self.local = local
        self.canceling = False

        if self.local:
            self.name = "%s.%s" % (self.name, socket.gethostname())

        self.declare()

    def __len__(self):
        return self.declare(force=True).method.message_count

    def declare(self, force=False):
        logger.debug('Declaring queue: %s', self.name)
        return self.channel.queue_declare(
            queue=self.name, durable=True,
            exclusive=False, auto_delete=False, force=force)

    def receive(self):
        """Get a single message from queue."""
        message = self.channel.basic_get(self.name)
        return Message.decode(message)

    def send(self, obj, expiration=None):
        """Send a single message to the queue.
        obj must be JSON serializable."""
        logger.info('sending to queue: %r message: %r', self.name, obj)
        properties = pika.BasicProperties(
            content_type='application/json',
            expiration=expiration,
            delivery_mode=2)
        return self.channel.basic_publish(
            exchange='',
            routing_key=self.name,
            body=Message.encode(obj),
            properties=properties)

    def ack(self, delivery_tag):
        logger.debug('Acking message')
        return self.channel.basic_ack(delivery_tag=delivery_tag)

    def nack(self, delivery_tag, multiple=False, requeue=True):
        logger.debug('Nacking message')
        return self.channel.basic_nack(
            delivery_tag=delivery_tag, multiple=multiple, requeue=requeue)

    def reject(self, delivery_tag):
        """Reject the message. Message will be delivered to another worker."""
        logger.debug('Rejecting message')
        return self.channel.basic_reject(
            delivery_tag=delivery_tag, requeue=True)

    def discard(self, delivery_tag):
        """Discard the message. Discarded messages will be lost."""
        logger.debug('Discarding message')
        return self.channel.basic_reject(
            delivery_tag=delivery_tag, requeue=False)

    def recover(self):
        logger.debug('Recovering messages')
        return self.channel.basic_recover(requeue=True)

    def delete(self):
        logger.warning('Deleting queue')
        return self.channel.queue_delete(queue=self.name)

    def basic_consume(self, callback):
        logger.debug('Issuing Basic.Consume')
        return self.channel.basic_consume(callback, self.name)

    def basic_cancel(self, consumer_id):
        logger.debug('Issuing Basic.Cancel')
        return self.channel.basic_cancel(consumer_id)

    def basic_qos(self, *args, **kwargs):
        return self.channel.basic_qos(*args, **kwargs)

    def tx_select(self):
        return self.channel.tx_select()

    def tx_commit(self):
        return self.channel.tx_commit()

########NEW FILE########
__FILENAME__ = requeue
from __future__ import absolute_import
import json
import logging

from kuyruk.queue import Queue

logger = logging.getLogger(__name__)


class Requeuer(object):

    def __init__(self, kuyruk):
        import redis
        self.kuyruk = kuyruk
        self.redis = redis.StrictRedis(
            host=self.kuyruk.config.REDIS_HOST,
            port=self.kuyruk.config.REDIS_PORT,
            db=self.kuyruk.config.REDIS_DB,
            password=self.kuyruk.config.REDIS_PASSWORD)

    def run(self):
        tasks = self.redis.hvals('failed_tasks')
        channel = self.kuyruk.channel()
        for task in tasks:
            task = json.loads(task)
            print "Requeueing task: %r" % task
            Requeuer.requeue(task, channel, self.redis)

        print "%i failed tasks have been requeued." % len(tasks)

    @staticmethod
    def requeue(task_description, channel, redis):
        queue_name = task_description['queue']
        del task_description['queue']
        count = task_description.get('requeue_count', 0)
        task_description['requeue_count'] = count + 1
        task_queue = Queue(queue_name, channel)
        task_queue.send(task_description)
        redis.hdel('failed_tasks', task_description['id'])

########NEW FILE########
__FILENAME__ = task
from __future__ import absolute_import
import os
import sys
import signal
import socket
import logging
from time import time
from uuid import uuid1
from datetime import datetime
from functools import wraps
from contextlib import contextmanager

from kuyruk import events, importer
from kuyruk.queue import Queue
from kuyruk.events import EventMixin
from kuyruk.exceptions import Timeout, InvalidTask, ObjectNotFound

logger = logging.getLogger(__name__)


def profile(f):
    """Logs the time spent while running the task."""
    @wraps(f)
    def inner(self, *args, **kwargs):
        start = time()
        result = f(self, *args, **kwargs)
        end = time()
        logger.info("%s finished in %i seconds." % (self.name, end - start))
        return result
    return inner


def object_to_id(f):
    """
    If the Task is a class task, converts the first parameter to the id.

    """
    @wraps(f)
    def inner(self, *args, **kwargs):
        cls = self.cls or self.arg_class
        if cls:
            try:
                obj = args[0]
            except IndexError:
                msg = "You must give an instance of %s as first argument." % cls
                raise TypeError(msg)

            if not isinstance(obj, cls):
                msg = "First argument must be an instance of %s." % cls
                raise TypeError(msg)

            args = list(args)
            args[0] = args[0].id
            assert isinstance(args[0], (int, long, basestring))
        return f(self, *args, **kwargs)
    return inner


def id_to_object(f):
    """
    If the Task is a class task, converts the first argument to an object
    by calling the get function of the class with the id.

    """
    @wraps(f)
    def inner(self, *args, **kwargs):
        cls = self.arg_class or self.cls
        if cls:
            if not args:
                raise InvalidTask

            obj_id = args[0]
            if not isinstance(obj_id, (int, long, basestring)):
                raise InvalidTask

            obj = cls.get(obj_id)
            if obj is None:
                raise ObjectNotFound

            if not isinstance(obj, cls):
                msg = "%s is not an instance of %s." % (obj, cls)
                raise ObjectNotFound(msg)

            args = list(args)
            args[0] = obj

        return f(self, *args, **kwargs)
    return inner


def send_client_signals(f):
    """Sends the presend and postsend signals."""
    @wraps(f)
    def inner(self, *args, **kwargs):
        self.send_signal(events.task_presend, args, kwargs, reverse=True)
        rv = f(self, *args, **kwargs)
        self.send_signal(events.task_postsend, args, kwargs)
        return rv
    return inner


class Task(EventMixin):

    def __init__(self, f, kuyruk, queue='kuyruk', local=False, eager=False,
                 retry=0, max_run_time=None, arg_class=None):
        self.f = f
        self.kuyruk = kuyruk
        self.queue_name = queue
        self.local = local
        self.eager = eager
        self.retry = retry
        self.max_run_time = max_run_time
        self.cls = None
        self.arg_class = arg_class
        self.setup()

    def setup(self):
        """Convenience function for extending classes
        that run after __init__."""
        pass

    def __repr__(self):
        return "<Task of %r>" % self.name

    @send_client_signals
    @object_to_id
    def __call__(self, *args, **kwargs):
        """When a fucntion is wrapped with a task decorator it will be
        converted to a Task object. By overriding __call__ method we are
        sending this task to queue instead of invoking the function
        without changing the client code.

        """
        logger.debug("Task.__call__ args=%r, kwargs=%r", args, kwargs)

        # These keyword argument allow the sender to override
        # the destination of the message.
        host = kwargs.pop('kuyruk_host', None)
        local = kwargs.pop('kuyruk_local', None)
        expiration = kwargs.pop('kuyruk_expiration', None)

        if self.eager or self.kuyruk.config.EAGER:
            # Run the task in process
            task_result = self._run(*args, **kwargs)
        else:
            # Send it to the queue
            task_result = TaskResult(self)
            task_result.id = self.send_to_queue(args, kwargs,
                                                host=host, local=local,
                                                expiration=expiration)

        return task_result

    def __get__(self, obj, objtype):
        """If the task is accessed from an instance via attribute syntax
        returns a bound task object that wraps the task itself, otherwise
        returns the task itself.

        This is done for allowing a method to be converted to task without
        modifying the client code. When a function decorated inside a class
        there is no way of accessing that class at that time because methods
        are bounded at run time when they are accessed. The trick here is that
        we set self.cls when the Task is accessed first time via attribute
        syntax.

        """
        self.cls = objtype
        if obj:
            # Class tasks needs to know what the object is so they can
            # inject that object in front of args.
            # We are returning a BoundTask instance here wrapping this task
            # that will do the injection.
            logger.debug("Creating bound task with obj=%r", obj)
            return BoundTask(self, obj)
        return self

    def send_to_queue(self, args, kwargs, host=None, local=None,
                      expiration=None):
        """
        Sends this task to queue.

        :param args: Arguments that will be passed to task on execution.
        :param kwargs: Keyword arguments that will be passed to task
            on execution.
        :param host: Send this task to specific host. ``host`` will be
            appended to the queue name.
        :param local: Send this task to this host. Hostname of this host will
            be appended to the queue name.
        :param expiration: Expire message after expiration milliseconds.
        :return: :const:`None`

        """
        logger.debug("Task.send_to_queueue args=%r, kwargs=%r", args, kwargs)

        with self.queue(host=host, local=local) as queue:
            desc = self.get_task_description(args, kwargs, queue.name)
            queue.send(desc, expiration=expiration)

        # We are returning the unique identifier of the task sent to queue
        # so we can query the result backend for completion.
        # TODO no result backend is available yet
        return desc['id']

    @contextmanager
    def queue(self, host=None, local=None):
        queue_ = self.queue_name
        local_ = self.local

        if local is not None:
            local_ = local

        if host:
            queue_ = "%s.%s" % (self.queue_name, host)
            local_ = False

        yield Queue(queue_, self.kuyruk.channel(), local_)

    def get_task_description(self, args, kwargs, queue):
        """Return the dictionary to be sent to the queue."""
        return {
            'id': uuid1().hex,
            'queue': queue,
            'args': args,
            'kwargs': kwargs,
            'module': self.module_name,
            'function': self.f.__name__,
            'class': self.class_name,
            'retry': self.retry,
            'sender_timestamp': datetime.utcnow(),
            'sender_hostname': socket.gethostname(),
            'sender_pid': os.getpid(),
            'sender_cmd': ' '.join(sys.argv),
        }

    def send_signal(self, sig, args, kwargs, reverse=False, **extra):
        """
        Sends a signal for each sender.
        This allows the user to register for a specific sender.

        """
        senders = (self, self.__class__, self.kuyruk)
        if reverse:
            senders = reversed(senders)

        for sender in senders:
            sig.send(sender, task=self, args=args, kwargs=kwargs, **extra)

    @send_client_signals
    @object_to_id
    def apply(self, *args, **kwargs):
        logger.debug("Task.apply args=%r, kwargs=%r", args, kwargs)
        return self._run(*args, **kwargs)

    @profile
    @id_to_object
    def _run(self, *args, **kwargs):
        """Run the wrapped function and event handlers."""
        def send_signal(sig, reverse=False, **extra):
            self.send_signal(sig, args, kwargs, reverse, **extra)

        logger.debug("Task._apply args=%r, kwargs=%r", args, kwargs)

        result = TaskResult(self)

        limit = (self.max_run_time or
                 self.kuyruk.config.MAX_TASK_RUN_TIME or 0)

        logger.debug("Applying %r, args=%r, kwargs=%r", self, args, kwargs)
        try:
            send_signal(events.task_prerun, reverse=True)
            with time_limit(limit):
                return_value = self.run(*args, **kwargs)
        except Exception:
            send_signal(events.task_failure, exc_info=sys.exc_info())
            raise
        else:
            send_signal(events.task_success, return_value=return_value)
            result.result = return_value
        finally:
            send_signal(events.task_postrun)

        # We are returning a TaskResult here because __call__ returns a
        # TaskResult object too. Return value must be consistent whether
        # task is sent to queue or executed in process with apply().
        return result

    def run(self, *args, **kwargs):
        """
        Runs the wrapped function.
        This method is intended to be overriden from subclasses.

        """
        return self.f(*args, **kwargs)

    @property
    def name(self):
        """Location for the wrapped function.
        This value is by the worker to find the task.

        """
        if self.class_name:
            return "%s:%s.%s" % (
                self.module_name, self.class_name, self.f.__name__)
        else:
            return "%s:%s" % (self.module_name, self.f.__name__)

    @property
    def module_name(self):
        """Module name of the function wrapped."""
        name = self.f.__module__
        if name == '__main__':
            name = importer.get_main_module().name
        return name

    @property
    def class_name(self):
        """Name of the class if this is a class task,
        otherwise :const:`None`."""
        if self.cls:
            return self.cls.__name__


class BoundTask(Task):
    """
    This class wraps the Task and inject the bound object in front of args
    when it is called.

    """
    def __init__(self, task, obj):
        self.task = task
        self.obj = obj

    def __getattr__(self, item):
        """Delegates all attributes to real Task."""
        return getattr(self.task, item)

    @wraps(Task.__call__)
    def __call__(self, *args, **kwargs):
        logger.debug("BoundTask.__call__ args=%r, kwargs=%r", args, kwargs)
        # Insert the bound object as a first argument to __call__
        args = list(args)
        args.insert(0, self.obj)
        return super(BoundTask, self).__call__(*args, **kwargs)

    @wraps(Task.apply)
    def apply(self, *args, **kwargs):
        logger.debug("BoundTask.apply args=%r, kwargs=%r", args, kwargs)
        args = list(args)
        args.insert(0, self.obj)
        return super(BoundTask, self).apply(*args, **kwargs)


class TaskResult(object):
    """Insance of this class is returned after the task is sent to queue.
    Since Kuyruk does not support a result backend yet it will raise
    exception on any attribute or item access.

    """
    def __init__(self, task):
        self.task = task

    def __getattr__(self, name):
        raise NotImplementedError(name)

    def __setattr__(self, name, value):
        if name not in ('task', 'id', 'result'):
            raise NotImplementedError(name)
        super(TaskResult, self).__setattr__(name, value)

    def __getitem__(self, key):
        raise NotImplementedError(key)

    def __setitem__(self, key, value):
        raise NotImplementedError(key)

    def __repr__(self):
        return "<TaskResult of %r>" % self.task.name


@contextmanager
def time_limit(seconds):
    def signal_handler(signum, frame):
        raise Timeout
    signal.signal(signal.SIGALRM, signal_handler)
    signal.alarm(seconds)
    try:
        yield
    finally:
        signal.alarm(0)

########NEW FILE########
__FILENAME__ = forever
import os
import sys
import signal
import subprocess
from time import sleep

from kuyruk import Kuyruk


kuyruk = Kuyruk()


@kuyruk.task(queue='forever')
def run_forever():
    # Execute this script
    path = os.path.abspath(__file__)
    p = subprocess.Popen([sys.executable, path])
    p.wait()
    print "Done."


if __name__ == '__main__':

    # A script that cannot be interrupted

    def handle_signal(signum, frame):
        print 'SIGNAL', signum

    # Ignore all signals
    signal.signal(signal.SIGINT, handle_signal)
    signal.signal(signal.SIGTERM, handle_signal)
    signal.signal(signal.SIGABRT, handle_signal)
    signal.signal(signal.SIGALRM, handle_signal)
    signal.signal(signal.SIGQUIT, handle_signal)
    signal.signal(signal.SIGCHLD, handle_signal)
    signal.signal(signal.SIGUSR1, handle_signal)

    # Sleep forever
    i = 0
    while True:
        i += 1
        print i
        sleep(1)

########NEW FILE########
__FILENAME__ = main
from tasks import print_message

print_message('asdf')

########NEW FILE########
__FILENAME__ = tasks
from kuyruk import Kuyruk

kuyruk = Kuyruk()


@kuyruk.task
def print_message(m):
    print m

########NEW FILE########
__FILENAME__ = main
from .tasks import print_message

print_message('asdf')

########NEW FILE########
__FILENAME__ = send_message
from ..tasks import print_message

print_message('asdf')

########NEW FILE########
__FILENAME__ = tasks
from kuyruk import Kuyruk

kuyruk = Kuyruk()


@kuyruk.task
def print_message(m):
    print m

########NEW FILE########
__FILENAME__ = onefile
from kuyruk import Kuyruk

kuyruk = Kuyruk()


@kuyruk.task
def print_message(m):
    print m


if __name__ == '__main__':
    print_message('asdf')

########NEW FILE########
__FILENAME__ = test_kuyruk
import signal
import logging
import unittest

import redis
from mock import patch

from kuyruk import Task
from kuyruk.task import BoundTask
from kuyruk.test import tasks
from kuyruk.test.integration.util import *
from kuyruk.connection import Channel

Channel.SKIP_REDECLARE_QUEUE = False

logger = logging.getLogger(__name__)

logger.debug('Process id: %s', os.getpid())
logger.debug('Process group id: %s', os.getpgrp())


class KuyrukTestCase(unittest.TestCase):
    """
    Tests here are mostly integration tests. They require a running
    RabbitMQ and Redis instances to pass succesfully.
    Just like the normal user they spawn a real master/worker process
    and make some assertion in their output.

    """
    def setUp(self):
        delete_queue('kuyruk')

    def test_simple_task(self):
        """Run a task on default queue"""
        tasks.print_task('hello world')
        with run_kuyruk() as worker:
            worker.expect('hello world')

    def test_another_queue(self):
        """Run a task on different queue"""
        tasks.print_task2('hello another')
        with run_kuyruk(queue='another_queue') as worker:
            worker.expect('another_queue')
            worker.expect('hello another')
            worker.expect('Task is processed')

    def test_exception(self):
        """Errored task message is discarded"""
        tasks.raise_exception()
        with run_kuyruk() as worker:
            worker.expect('ZeroDivisionError')
        assert is_empty('kuyruk')

    def test_retry(self):
        """Errored tasks must be retried"""
        tasks.retry_task()
        with run_kuyruk() as worker:
            worker.expect('ZeroDivisionError')
            worker.expect('ZeroDivisionError')
        assert is_empty('kuyruk')

    def test_cold_shutdown(self):
        """If the worker is stuck on the task it can be stopped by
        invoking cold shutdown"""
        tasks.loop_forever()
        with run_kuyruk(process='master', terminate=False) as master:
            master.expect('looping forever')
            master.send_signal(signal.SIGINT)
            master.expect('Warm shutdown')
            master.expect('Handled SIGINT')
            master.send_signal(signal.SIGINT)
            master.expect('Cold shutdown')
            master.expect_exit(0)
            wait_until(not_running, timeout=TIMEOUT)

    def test_reject(self):
        """Rejected tasks must be requeued again"""
        tasks.rejecting_task()
        with run_kuyruk() as worker:
            worker.expect('Task is rejected')
            worker.expect('Task is rejected')
        assert not is_empty('kuyruk')

    def test_respawn(self):
        """Respawn a new worker if dead

        This test also covers the broker disconnect case because when the
        connection drops the master worker will raise an unhandled exception.
        This exception will cause the worker to exit. After exiting, master
        worker will spawn a new master worker.

        """
        def get_worker_pids():
            pids = get_pids('kuyruk: worker')
            assert len(pids) == 2
            return pids

        with run_kuyruk(process='master') as master:
            master.expect('Start consuming')
            master.expect('Start consuming')
            pids_old = get_worker_pids()
            for pid in pids_old:
                os.kill(pid, signal.SIGKILL)
            master.expect('Respawning worker')
            master.expect('Waiting for new message')
            master.expect('Waiting for new message')
            pids_new = get_worker_pids()

        assert pids_new[0] > pids_old[0]  # kuyruk
        assert pids_new[1] > pids_old[1]  # kuyruk.localhost

    def test_save_failed(self):
        """Failed tasks are saved to Redis"""
        tasks.raise_exception()
        with run_kuyruk(save_failed_tasks=True) as worker:
            worker.expect('ZeroDivisionError')
            worker.expect('No retry left')
            worker.expect('Saving failed task')
            worker.expect('Saved')
            worker.expect('Task is processed')

        assert is_empty('kuyruk')
        r = redis.StrictRedis()
        assert r.hvals('failed_tasks')

        run_requeue()
        assert not r.hvals('failed_tasks')
        assert not is_empty('kuyruk')

    def test_save_failed_class_task(self):
        """Failed tasks are saved to Redis (class tasks)"""
        cat = tasks.Cat(1, 'Felix')

        cat.raise_exception()
        with run_kuyruk(save_failed_tasks=True) as worker:
            worker.expect('raise Exception')
            worker.expect('Saving failed task')
            worker.expect('Saved')

        assert is_empty('kuyruk')
        r = redis.StrictRedis()
        assert r.hvals('failed_tasks')

        run_requeue()
        assert not r.hvals('failed_tasks')
        assert not is_empty('kuyruk')

    def test_save_failed_arg_class(self):
        """Failed tasks are saved to Redis (arg class)"""
        cat = tasks.Cat(1, 'Felix')

        tasks.jump_fail(cat)
        with run_kuyruk(save_failed_tasks=True) as worker:
            worker.expect('ZeroDivisionError')
            worker.expect('Saving failed task')
            worker.expect('Saved')

        assert is_empty('kuyruk')
        r = redis.StrictRedis()
        assert r.hvals('failed_tasks')

        run_requeue()
        assert not r.hvals('failed_tasks')
        assert not is_empty('kuyruk')

    def test_dead_master(self):
        """If master is dead worker should exit gracefully"""
        tasks.print_task('hello world')
        with run_kuyruk(terminate=False) as worker:
            worker.expect('hello world')
            worker.kill()
            worker.expect_exit(-signal.SIGKILL)
            wait_until(not_running, timeout=TIMEOUT)

    @patch('kuyruk.test.tasks.must_be_called')
    def test_before_after(self, mock_func):
        """Before and after task functions are run"""
        tasks.task_with_functions('hello world')
        mock_func.assert_called_once_with()
        with run_kuyruk() as worker:
            worker.expect('function1')
            worker.expect('function2')
            worker.expect('hello world')
            worker.expect('function3')
            worker.expect('function4')
            worker.expect('function5')

    def test_extend(self):
        """Extend task class"""
        tasks.use_session()
        with run_kuyruk() as worker:
            worker.expect('Opening session')
            worker.expect('Closing session')

    def test_class_task(self):
        cat = tasks.Cat(1, 'Felix')
        self.assertTrue(isinstance(tasks.Cat.meow, Task))
        self.assertTrue(isinstance(cat.meow, BoundTask))

        cat.meow('Oh my god')
        with run_kuyruk() as worker:
            worker.expect('Oh my god')

    def test_arg_class(self):
        cat = tasks.Cat(1, 'Felix')
        tasks.jump(cat)
        with run_kuyruk() as worker:
            worker.expect('Felix jumps high!')
            worker.expect('Called with Felix')

    def test_max_run_time(self):
        """Timeout long running task"""
        run_time = tasks.sleeping_task.max_run_time + 1
        tasks.sleeping_task(run_time)
        with run_kuyruk() as worker:
            worker.expect('raise Timeout')

    def test_worker_sigquit(self):
        """Ack current message and exit"""
        tasks.loop_forever()
        with run_kuyruk(terminate=False) as worker:
            worker.expect('looping forever')
            pid = get_pid('kuyruk: worker')
            os.kill(pid, signal.SIGQUIT)
            worker.expect('Acking current task')
            worker.expect('Exiting')
            worker.expect_exit(0)
        assert is_empty('kuyruk'), worker.get_output()

########NEW FILE########
__FILENAME__ = test_loader
import os
import sys
import unittest

from what import What

from kuyruk import Kuyruk
from kuyruk.queue import Queue
from kuyruk.test.integration.util import delete_queue


class LoaderTestCase(unittest.TestCase):

    def test_function_name(self):
        cases = [
            (
                'onefile.py',
                'loader',
                'onefile.print_message'
            ),
            (
                'main.py',
                'loader/appdirectory',
                'tasks.print_message'
            ),
            (
                '-m apppackage.main',
                'loader',
                'apppackage.tasks.print_message'
            ),
            (
                '-m apppackage.scripts.send_message',
                'loader',
                'apppackage.tasks.print_message'
            ),
        ]
        for args, cwd, name in cases:
            print cwd, args, name
            delete_queue('kuyruk')
            run_python(args, cwd=cwd)  # Every call sends a task to the queue
            name_from_queue = get_name()
            assert name_from_queue == name  # Can we load the task by name?


def run_python(args, cwd):
    dirname = os.path.dirname(__file__)
    cwd = os.path.join(dirname, cwd)
    What(sys.executable, *args.split(' '), cwd=cwd).expect_exit(0)


def get_name():
    desc = Queue('kuyruk', Kuyruk().channel()).receive()[1]
    return '.'.join([desc['module'], desc['function']])

########NEW FILE########
__FILENAME__ = util
import os
import sys
import errno
import signal
import logging
import subprocess
from time import time, sleep
from functools import partial
from contextlib import contextmanager

from pika.exceptions import ChannelClosed
from what import What

from kuyruk import Kuyruk
from kuyruk.queue import Queue as RabbitQueue


if os.environ.get('TRAVIS', '') == 'true':
    TIMEOUT = 30
else:
    TIMEOUT = 5

logger = logging.getLogger(__name__)


def delete_queue(*queues):
    """Delete queues from RabbitMQ"""
    for name in queues:
        try:
            ch = Kuyruk().channel()
            RabbitQueue(name, ch).delete()
            ch.close()
        except ChannelClosed:
            pass


def is_empty(queue):
    queue = RabbitQueue(queue, Kuyruk().channel())
    return len(queue) == 0


@contextmanager
def run_kuyruk(queue='kuyruk', save_failed_tasks=False, terminate=True,
               process='worker'):
    assert not_running()
    args = [
        sys.executable, '-u',
        '-m', 'kuyruk.__main__',  # run main module
        '--max-load', '999',  # do not pause because of load
    ]
    args.extend(['--logging-level=DEBUG'])
    if save_failed_tasks:
        args.extend(['--save-failed-tasks', 'True'])

    args.append(process)
    if process == 'worker':
        args.extend(['--queue', queue])

    environ = os.environ.copy()
    environ['COVERAGE_PROCESS_START'] = '.coveragerc'

    popen = What(*args, preexec_fn=os.setsid, env=environ)
    popen.timeout = TIMEOUT
    try:
        yield popen

        if terminate:
            # Send SIGTERM to worker for gracefull shutdown
            popen.terminate()
            popen.expect("End run %s" % process)

        popen.expect_exit()

    finally:
        # We need to make sure that not any process of kuyruk is running
        # after the test is finished.

        # Kill master process and wait until it is dead
        try:
            popen.kill()
            popen.wait()
        except OSError as e:
            if e.errno != errno.ESRCH:  # No such process
                raise

        logger.debug('Worker return code: %s', popen.returncode)

        # Kill worker processes by sending SIGKILL to their process group id
        try:
            logger.info('Killing process group: %s', popen.pid)
            os.killpg(popen.pid, signal.SIGTERM)
        except OSError as e:
            if e.errno != errno.ESRCH:  # No such process
                raise

        try:
            wait_while(lambda: get_pids('kuyruk:'))
        except KeyboardInterrupt:
            print popen.get_output()
            # Do not raise KeyboardInterrupt here because nose does not print
            # captured stdout and logging on KeyboardInterrupt
            raise Exception


def not_running():
    return not is_running()


def is_running():
    return bool(get_pids('kuyruk:'))


def run_requeue():
    from kuyruk.__main__ import run_requeue
    run_requeue(Kuyruk(), None)


def get_pids(pattern):
    logger.debug('get_pids: %s', pattern)
    cmd = "pgrep -fl '%s'" % pattern
    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)
    out = p.communicate()[0]
    logger.debug("\n%s", out)
    lines = out.splitlines()
    # filter pgrep itself. travis runs it like "/bin/sh -c pgrep -fl 'kuyruk:'"
    lines = filter(lambda x: not cmd in x, lines)
    pids = [int(l.split()[0]) for l in lines]  # take first column
    logger.debug('pids: %s', pids)
    return pids


def get_pid(pattern):
    pids = get_pids(pattern)
    assert len(pids) == 1
    return pids[0]


def wait_until(f, timeout=None):
    return wait_while(lambda: not f(), timeout)


def wait_while(f, timeout=None):
    do_while(partial(sleep, 0.1), f, timeout)


def do_until(f_do, f_cond, timeout=None):
    do_while(f_do, lambda: not f_cond(), timeout)


def do_while(f_do, f_condition, timeout=None):
    def should_do():
        if timeout and timeout < 0:
            raise Timeout
        return f_condition()

    original_timeout = timeout
    start = time()
    while should_do():
        f_do()
        if timeout:
            passed = time() - start
            timeout = original_timeout - passed


class Timeout(Exception):
    pass

########NEW FILE########
__FILENAME__ = tasks
"""
Contains sample tasks to be used in unit and integration tests.

"""

import sys
import string
import random
from time import sleep

from kuyruk import Kuyruk, Task
from kuyruk.events import task_prerun, task_postrun


kuyruk = Kuyruk()
# These functions below needs to be at module level in order that
# Kuyruk worker to determine their fully qualified name.


@kuyruk.task
def print_task(message):
    print message
    must_be_called()


@kuyruk.task(queue='another_queue')
def print_task2(message):
    print message


@kuyruk.task
def flood():
    s = ''.join(random.choice(string.ascii_uppercase) for _ in xrange(70))
    while True:
        sys.stdout.write(s)
        sys.stdout.write('\n')
        sys.stdout.flush()
        sys.stderr.write(s)
        sys.stderr.write('\n')
        sys.stderr.flush()


@kuyruk.task
def raise_exception():
    return 1 / 0


@kuyruk.task(retry=1)
def retry_task():
    return 1 / 0


@kuyruk.task
def loop_forever():
    while 1:
        print 'looping forever'
        sleep(1)


@kuyruk.task(eager=True)
def eager_task():
    must_be_called()


@kuyruk.task
def rejecting_task():
    raise kuyruk.Reject


@kuyruk.task(max_run_time=1)
def sleeping_task(seconds):
    sleep(seconds)


# Another kuyruk instance for testing before and after task decorators
kuyruk2 = Kuyruk()


@kuyruk2.task
def task_with_functions(message):
    print message
    return 42


@kuyruk2.on_presend
def function0(sender, task, args, kwargs):
    must_be_called()


@kuyruk2.on_prerun
def function1(sender, task, args, kwargs):
    print 'function1'
    print sender, task, args, kwargs
    assert sender is kuyruk2
    assert isinstance(task, Task)
    assert list(args) == ['hello world']
    assert kwargs == {}


@task_with_functions.on_prerun
def function2(sender, task, args, kwargs):
    print 'function2'


@task_with_functions.on_success
def function3(sender, task, args, kwargs, return_value):
    print 'function3'
    assert return_value == 42


@task_with_functions.on_postrun
def function4(sender, task, args, kwargs):
    print 'function4'


@kuyruk2.on_postrun
def function5(sender, task, args, kwargs):
    print 'function5'


class Cat(object):

    def __init__(self, id, name):
        self.id = id
        self.name = name

    def __repr__(self):
        return "Cat(%r, %r)" % (self.id, self.name)

    @classmethod
    def get(cls, id):
        if id == 1:
            return cls(1, 'Felix')

    @kuyruk.task
    def meow(self, message):
        print "Felix says:", message
        must_be_called()

    @kuyruk.task(eager=True)
    def meow_eager(self, message):
        print "Felix says:", message
        must_be_called()

    @kuyruk.task
    def raise_exception(self):
        raise Exception


@kuyruk.task(arg_class=Cat)
def jump(cat):
    print "%s jumps high!" % cat.name
    must_be_called(cat.name)


@kuyruk.task(arg_class=Cat, eager=True)
def jump_eager(cat):
    print "%s jumps high!" % cat.name
    must_be_called(cat.name)


@kuyruk.task(arg_class=Cat)
def jump_fail(cat):
    1/0


def must_be_called(arg=None):
    """
    This function is patched in tests to see the caller is doing it's job.

    """
    print 'Yes, it is called.'
    print 'Called with %s' % arg


class DatabaseTask(Task):

    def setup(self):
        self.connect_signal(task_prerun, self.open_session)
        self.connect_signal(task_postrun, self.close_session)

    def open_session(self, sender, task, args, kwargs):
        print 'Opening session'
        self.session = object()

    def close_session(self, sender, task, args, kwargs):
        print 'Closing session'
        self.session = None


@kuyruk.task(task_class=DatabaseTask)
def use_session():
    print use_session.session


@kuyruk.task
def spawn_process(args=['sleep', '60']):
    import subprocess
    subprocess.check_call(args)

########NEW FILE########
__FILENAME__ = config
RABBIT_HOST = 'localhost'
RABBIT_PORT = 5672
RABBIT_USER = 'guest'
RABBIT_PASSWORD = 'guest'
IMPORT_PATH = None
EAGER = False
MAX_LOAD = 20
MAX_WORKER_RUN_TIME = None
SAVE_FAILED_TASKS = False
MANAGER_HOST = 'localhost'
MANAGER_PORT = 16501
MANAGER_HTTP_PORT = 16500
QUEUES = {
    'aslan.local': 'a, 2*b, c*3, @d'
}
IMPORTS = [
    'kuyruk.test.config',
    'kuyruk',
]

########NEW FILE########
__FILENAME__ = test_config
import os
import unittest

from kuyruk.config import Config
from kuyruk.master import parse_queues_str
from kuyruk.test.unit import config as user_config


class ConfigTestCase(unittest.TestCase):

    def test_from_pyfile(self):
        dirname = os.path.dirname(__file__)
        path = os.path.join(dirname, 'config.py')
        config = Config()
        config.from_pyfile(path)
        self.assertEqual(config.MAX_LOAD, 20)

    def test_from_object(self):
        user_config.MAX_LOAD = 21
        config = Config()
        config.from_object(user_config)
        self.assertEqual(config.MAX_LOAD, 21)

    def test_queues_string(self):
        assert parse_queues_str(" a,b, c, 2*d, e*3, 2*@f ") == \
            ['a', 'b', 'c', 'd', 'd', 'e', 'e', 'e', '@f', '@f']

########NEW FILE########
__FILENAME__ = test_manager
import unittest

from kuyruk import manager, Kuyruk


class ManagerTestCase(unittest.TestCase):

    def setUp(self):
        self.kuyruk = Kuyruk()
        self.manager = manager.Manager(self.kuyruk)
        self.app = self.manager.test_client()

    def test_empty_db(self):
        rv = self.app.get('/')
        # assert '' in rv.data

########NEW FILE########
__FILENAME__ = test_task
import logging

import unittest

from kuyruk.test import tasks


logger = logging.getLogger(__name__)


class TaskTestCase(unittest.TestCase):

    def test_invalid_call(self):
        """Invalid call of task raises exception"""
        self.assertRaises(TypeError, tasks.jump)
        self.assertRaises(TypeError, tasks.jump, object())
        felix = tasks.Cat(1, 'felix')
        del felix.id
        self.assertRaises(AttributeError, tasks.jump, felix)

########NEW FILE########
__FILENAME__ = test_various
import logging
import unittest

from mock import patch

from kuyruk import Task
from kuyruk.task import TaskResult
from kuyruk.test import tasks


logger = logging.getLogger(__name__)


class KuyrukTestCase(unittest.TestCase):

    def test_task_decorator(self):
        """Does task decorator works correctly?"""
        # Decorator without args
        self.assertTrue(isinstance(tasks.print_task, Task))
        # Decorator with args
        self.assertTrue(isinstance(tasks.print_task2, Task))

    @patch('kuyruk.test.tasks.must_be_called')
    def test_eager(self, mock_func):
        """Test eager mode for using in test environments"""
        result = tasks.eager_task()
        assert isinstance(result, TaskResult)
        mock_func.assert_called_once_with()

    @patch('kuyruk.test.tasks.must_be_called')
    def test_apply(self, mock_func):
        """Test Task.apply()"""
        result = tasks.print_task.apply("hello")
        assert isinstance(result, TaskResult)
        mock_func.assert_called_once_with()

    @patch('kuyruk.test.tasks.must_be_called')
    def test_class_task_eager(self, mock_func):
        cat = tasks.Cat(1, 'Felix')
        cat.meow_eager('Oh my god')
        mock_func.assert_called_once_with()

    @patch('kuyruk.test.tasks.must_be_called')
    def test_class_task_apply(self, mock_func):
        cat = tasks.Cat(1, 'Felix')
        cat.meow.apply('Oh my god')
        mock_func.assert_called_once_with()

    @patch('kuyruk.test.tasks.must_be_called')
    def test_arg_class_eager(self, mock_func):
        cat = tasks.Cat(1, 'Felix')
        tasks.jump_eager(cat)
        mock_func.assert_called_once_with('Felix')

    @patch('kuyruk.test.tasks.must_be_called')
    def test_arg_class_apply(self, mock_func):
        cat = tasks.Cat(1, 'Felix')
        tasks.jump.apply(cat)
        mock_func.assert_called_once_with('Felix')

    def test_task_name(self):
        self.assertEqual(tasks.Cat.meow.name, 'kuyruk.test.tasks:Cat.meow')
        self.assertEqual(tasks.print_task.name, 'kuyruk.test.tasks:print_task')

########NEW FILE########
__FILENAME__ = worker
from __future__ import absolute_import
import os
import sys
import socket
import signal
import logging
import traceback
import multiprocessing
from time import time, sleep
from datetime import datetime
from functools import wraps
from contextlib import contextmanager

import rpyc
from setproctitle import setproctitle

import kuyruk
from kuyruk import importer
from kuyruk.queue import Queue
from kuyruk.process import KuyrukProcess
from kuyruk.helpers import start_daemon_thread
from kuyruk.consumer import Consumer
from kuyruk.exceptions import Reject, ObjectNotFound, Timeout, InvalidTask
from kuyruk.helpers.json_datetime import JSONEncoder

logger = logging.getLogger(__name__)


def set_current_task(f):
    """Save current task and it's arguments in self so we can send them to
    manager as stats."""
    @wraps(f)
    def inner(self, task, args, kwargs):
        self.current_task = task
        self.current_args = args
        self.current_kwargs = kwargs
        try:
            return f(self, task, args, kwargs)
        finally:
            self.current_task = None
            self.current_args = None
            self.current_kwargs = None
    return inner


class Worker(KuyrukProcess):
    """Consumes messages from a queue and runs tasks.

    :param kuyruk: A :class:`~kuyurk.kuyruk.Kuyruk` instance
    :param queue_name: The queue name to work on

    """
    def __init__(self, kuyruk, queue_name):
        super(Worker, self).__init__(kuyruk)
        self.channel = self.kuyruk.channel()
        is_local = queue_name.startswith('@')
        queue_name = queue_name.lstrip('@')
        self.queue = Queue(queue_name, self.channel, local=is_local)
        self.consumer = Consumer(self.queue)
        self.current_message = None
        self.current_task = None
        self.current_args = None
        self.current_kwargs = None
        self.daemon_threads = [
            self.watch_master,
            self.watch_load,
            self.shutdown_timer,
        ]
        if self.config.MAX_LOAD is None:
            self.config.MAX_LOAD = multiprocessing.cpu_count()

        if self.config.SENTRY_DSN:
            import raven
            self.sentry = raven.Client(self.config.SENTRY_DSN)
        else:
            self.sentry = None

        if self.config.SAVE_FAILED_TASKS:
            import redis
            self.redis = redis.StrictRedis(
                host=self.config.REDIS_HOST,
                port=self.config.REDIS_PORT,
                db=self.config.REDIS_DB,
                password=self.config.REDIS_PASSWORD)
        else:
            self.redis = None

    def run(self):
        """Runs the worker and opens a connection to RabbitMQ.
        After connection is opened, starts consuming messages.
        Consuming is cancelled if an external signal is received.

        """
        super(Worker, self).run()
        setproctitle("kuyruk: worker on %s" % self.queue.name)
        self.queue.declare()
        self.queue.basic_qos(prefetch_count=1)
        self.import_modules()
        self.start_daemon_threads()
        self.maybe_start_manager_rpc_service()
        self.consume_messages()
        logger.debug("End run worker")

    def rpc_service_class(this):
        class _Service(rpyc.Service):
            exposed_get_stats = this.get_stats
            exposed_warm_shutdown = this.warm_shutdown
            exposed_cold_shutdown = this.cold_shutdown
            exposed_quit_task = this.quit_task
        return _Service

    def consume_messages(self):
        """Consumes messages from the queue and run tasks until
        consumer is cancelled via a signal or another thread.

        """
        with self.consumer.consume() as messages:
            for message in messages:
                with self._set_current_message(message):
                    self.process_message(message)

    @contextmanager
    def _set_current_message(self, message):
        """Save current message being processed so we can send ack
        before exiting when SIGQUIT is received."""
        self.current_message = message
        try:
            yield message
        finally:
            self.current_message = None

    def start_daemon_threads(self):
        """Start the function as threads listed in self.daemon_thread."""
        for f in self.daemon_threads:
            start_daemon_thread(f)

    def import_modules(self):
        """Import modules defined in the configuration.
        This method is called before start consuming messages.

        """
        for module in self.config.IMPORTS:
            importer.import_module(module, self.config.IMPORT_PATH)

    def process_message(self, message):
        """Processes the message received from the queue."""
        try:
            task_description = message.get_object()
            logger.info("Processing task: %r", task_description)
        except Exception:
            message.ack()
            logger.error("Canot decode message. Dropped!")
            return

        try:
            task = self.import_task(task_description)
            task.message = message
            args, kwargs = task_description['args'], task_description['kwargs']
            self.apply_task(task, args, kwargs)
        except Reject:
            logger.warning('Task is rejected')
            sleep(1)  # Prevent cpu burning
            message.reject()
        except ObjectNotFound:
            self.handle_not_found(message, task_description)
        except Timeout:
            self.handle_timeout(message, task_description)
        except InvalidTask:
            self.handle_invalid(message, task_description)
        except Exception:
            self.handle_exception(message, task_description)
        else:
            logger.info('Task is successful')
            delattr(task, 'message')
            message.ack()
        finally:
            logger.debug("Task is processed")

    def handle_exception(self, message, task_description):
        """Handles the exception while processing the message."""
        logger.error('Task raised an exception')
        logger.error(traceback.format_exc())
        retry_count = task_description.get('retry', 0)
        if retry_count:
            logger.debug('Retry count: %s', retry_count)
            message.discard()
            task_description['retry'] = retry_count - 1
            self.queue.send(task_description)
        else:
            logger.debug('No retry left')
            self.capture_exception(task_description)
            message.discard()
            if self.config.SAVE_FAILED_TASKS:
                self.save_failed_task(task_description)

    def handle_not_found(self, message, task_description):
        """Called if the task is class task but the object with the given id
        is not found. The default action is logging the error and acking
        the message.

        """
        logger.error(
            "<%s.%s id=%r> is not found",
            task_description['module'],
            task_description['class'],
            task_description['args'][0])
        message.ack()

    def handle_timeout(self, message, task_description):
        """Called when the task is timed out while running the wrapped
        function.

        """
        logger.error('Task has timed out.')
        self.handle_exception(message, task_description)

    def handle_invalid(self, message, task_description):
        """Called when the message from queue is invalid."""
        logger.error("Invalid message.")
        self.capture_exception(task_description)
        message.discard()

    def save_failed_task(self, task_description):
        """Saves the task to ``kuyruk_failed`` queue. Failed tasks can be
        investigated later and requeued with ``kuyruk reuqueue`` command.

        """
        logger.info('Saving failed task')
        task_description['queue'] = self.queue.name
        task_description['worker_hostname'] = socket.gethostname()
        task_description['worker_pid'] = os.getpid()
        task_description['worker_cmd'] = ' '.join(sys.argv)
        task_description['worker_timestamp'] = datetime.utcnow()
        task_description['exception'] = traceback.format_exc()
        exc_type = sys.exc_info()[0]
        task_description['exception_type'] = "%s.%s" % (
            exc_type.__module__, exc_type.__name__)

        try:
            self.redis.hset('failed_tasks', task_description['id'],
                            JSONEncoder().encode(task_description))
            logger.debug('Saved')
        except Exception:
            logger.error('Cannot save failed task to Redis!')
            logger.debug(traceback.format_exc())
            if self.sentry:
                self.sentry.captureException()

    @set_current_task
    def apply_task(self, task, args, kwargs):
        """Imports and runs the wrapped function in task."""
        result = task._run(*args, **kwargs)
        logger.debug('Result: %r', result)

    def import_task(self, task_description):
        """This is the method where user modules are loaded."""
        module, function, cls = (
            task_description['module'],
            task_description['function'],
            task_description['class'])
        return importer.import_task(
            module, cls, function, self.config.IMPORT_PATH)

    def capture_exception(self, task_description):
        """Sends the exceptin in current stack to Sentry."""
        if self.sentry:
            ident = self.sentry.get_ident(self.sentry.captureException(
                extra={
                    'task_description': task_description,
                    'hostname': socket.gethostname(),
                    'pid': os.getpid(),
                    'uptime': self.uptime}))
            logger.error("Exception caught; reference is %s", ident)
            task_description['sentry_id'] = ident

    def is_master_alive(self):
        ppid = os.getppid()
        if ppid == 1:
            return False

        try:
            os.kill(ppid, 0)
            return True
        except OSError:
            return False

    def watch_master(self):
        """Watch the master and shutdown gracefully when it is dead."""
        while True:
            if not self.is_master_alive():
                logger.critical('Master is dead')
                self.warm_shutdown()
            sleep(1)

    def watch_load(self):
        """Pause consuming messages if lood goes above the allowed limit."""
        while not self.shutdown_pending.is_set():
            load = os.getloadavg()[0]
            if load > self.config.MAX_LOAD:
                logger.warning('Load is high (%s), pausing consume', load)
                self.consumer.pause(10)
            sleep(1)

    def shutdown_timer(self):
        """Counts down from MAX_WORKER_RUN_TIME. When it reaches zero sutdown
        gracefully.

        """
        if not self.config.MAX_WORKER_RUN_TIME:
            return

        while True:
            passed = time() - self.started
            remaining = self.config.MAX_WORKER_RUN_TIME - passed
            if remaining > 0:
                sleep(remaining)
            else:
                logger.warning('Run time reached zero')
                self.warm_shutdown()
                break

    def register_signals(self):
        super(Worker, self).register_signals()
        signal.signal(signal.SIGTERM, self.handle_sigterm)
        signal.signal(signal.SIGQUIT, self.handle_sigquit)

    def handle_sigterm(self, signum, frame):
        """Initiates a warm shutdown."""
        logger.warning("Catched SIGTERM")
        self.warm_shutdown()

    def handle_sigquit(self, signum, frame):
        """Send ACK for the current task and exit."""
        logger.warning("Catched SIGQUIT")
        if self.current_message:
            try:
                logger.warning("Acking current task...")
                self.current_message.ack()
            except Exception:
                logger.critical("Cannot send ACK for the current task.")
                traceback.print_exc()
        logger.warning("Exiting...")
        self._exit(0)

    def quit_task(self):
        self.handle_sigquit(None, None)

    def warm_shutdown(self):
        """Exit after the last task is finished."""
        logger.warning("Warm shutdown")
        self.consumer.stop()
        self.shutdown_pending.set()

    def cold_shutdown(self):
        """Exit immediately."""
        logger.warning("Cold shutdown")
        self._exit(0)

    def get_stats(self):
        """Generate stats to be sent to manager."""
        method = self.queue.declare(force=True).method
        try:
            current_task = self.current_task.name
        except AttributeError:
            current_task = None

        return {
            'type': 'worker',
            'hostname': socket.gethostname(),
            'uptime': self.uptime,
            'pid': os.getpid(),
            'ppid': os.getppid(),
            'version': kuyruk.__version__,
            'current_task': current_task,
            'current_args': self.current_args,
            'current_kwargs': self.current_kwargs,
            'consuming': self.consumer.consuming,
            'queue': {
                'name': method.queue,
                'messages_ready': method.message_count,
                'consumers': method.consumer_count,
            }
        }

########NEW FILE########
__FILENAME__ = __main__
"""
This is the entry point for main "kuyruk" executable command.
It implements the command line parsing for subcommands and configuration.

"""
from __future__ import absolute_import
import os
import logging
import argparse

from kuyruk import __version__, importer, Kuyruk
from kuyruk.master import Master
from kuyruk.config import Config
from kuyruk.requeue import Requeuer
from kuyruk.manager import Manager


logger = logging.getLogger(__name__)


def run_worker(kuyruk, args):
    worker_class = importer.import_class_str(kuyruk.config.WORKER_CLASS)
    w = worker_class(kuyruk, args.queue)
    w.run()


def run_master(kuyruk, args):
    m = Master(kuyruk)
    m.run()


def run_requeue(kuyruk, args):
    r = Requeuer(kuyruk)
    r.run()


def run_manager(kuyruk, args):
    m = Manager(kuyruk)
    m.run()


def main():
    parser = argparse.ArgumentParser(conflict_handler='resolve')

    # Add common options
    parser.add_argument(
        '-v', '--version', action='version', version=__version__)
    parser.add_argument(
        '-c', '--config',
        help='Python file containing Kuyruk configuration parameters')
    add_config_options(parser)

    subparsers = parser.add_subparsers(help='sub-command name')

    # Parser for the "worker" sub-command
    parser_worker = subparsers.add_parser('worker', help='run a worker')
    parser_worker.set_defaults(func=run_worker)
    parser_worker.add_argument(
        '-q', '--queue', default='kuyruk', help='consume tasks from')

    # Parser for the "master" sub-command
    parser_master = subparsers.add_parser('master', help='run a master')
    parser_master.set_defaults(func=run_master)
    parser_master.add_argument(
        '-q', '--queues', help='comma seperated list of queues')

    # Parser for the "requeue" sub-command
    parser_master = subparsers.add_parser('requeue',
                                          help='requeue failed tasks')
    parser_master.set_defaults(func=run_requeue)

    # Parser for the "manager" sub-command
    parser_master = subparsers.add_parser('manager', help='run manager')
    parser_master.set_defaults(func=run_manager)

    # Parse arguments
    args = parser.parse_args()
    config = create_config(args)
    kuyruk = Kuyruk(config)

    # Run the sub-command function
    args.func(kuyruk, args)


def add_config_options(parser):
    """Adds options for overriding values in config."""
    config_group = parser.add_argument_group('override values in config')

    def to_option(attr):
        """Convert config key to command line option."""
        return '--%s' % attr.lower().replace('_', '-')

    # Add every attribute in Config as command line option
    for key in sorted(dir(Config)):
        if key.isupper():
            config_group.add_argument(to_option(key))


def create_config(args):
    """Creates Config object and overrides it's values from args."""
    config = Config()

    if args.config:
        # Load config file from command line option
        config.from_pyfile(args.config)
    else:
        # Load config file from environment variable
        env_config = os.environ.get('KUYRUK_CONFIG')
        if env_config:
            assert os.path.isabs(env_config)
            config.from_pyfile(env_config)

    config.from_env_vars()
    config.from_cmd_args(args)
    return config


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = sitecustomize
import coverage
coverage.process_startup()

########NEW FILE########
__FILENAME__ = tasks
from invoke import run, task


@task
def bump(part):
    assert part in ('major', 'minor', 'patch')
    run("bumpversion %s" % part)


@task
def upload():
    run("python setup.py sdist upload")


@task
def coverage():
    run("nosetests")
    run("coverage combine")
    run("coverage html")
    run("open coverage_html_report/index.html")

########NEW FILE########
