__FILENAME__ = arxiv_rss
import datetime
import os
import pickle
import re
import time
import feedparser

a_re = re.compile(r"""<a.*?>(.*?)</a>""", re.DOTALL)
p_re = re.compile(r"""<p>(.*?)</p>""", re.DOTALL)
title_re = re.compile(r"""(.*?)\.\s\(arXiv:(.*?)\[(.*?)\].*?\)""", re.DOTALL)

arvix_categories = map(lambda x: x.strip(), "astro-ph, cond-mat, cs, gr-qc, hep-ex, hep-lat, hep-ph, hep-th, math, math-ph, nlin, nucl-ex, nucl-th, physics, q-bio, q-fin, quant-ph, stat".split(','))

cache_filename = "arxiv.pickle"

#def load_categories(filename="arxiv_categories.txt"):
    #categories = []
    #with open(filename) as handle:
        #for line in handle:
            #categories.append(line.strip())
    #return categories

def rss_gen(categories=None, base_url="http://export.arxiv.org/rss/", verbose=False):
    """Generator for Arxiv RSS feeds."""
    #attribute_map = {'title': 'title', 'url': 'link', 'author': 'authors', }
    if not categories:
        categories = arvix_categories
    for category in categories:
        url = base_url + category
        feed = feedparser.parse(url)
        if verbose:
            print "Downloading category %s: %s" %(category, url)
        num_entries = len(feed['entries'])
        for feed_entry in feed['entries']:
            paper = dict()
            paper['url'] = feed_entry['links'][0]['href'].strip()
            paper['authors'] = tuple(a_re.findall(feed_entry['author']))
            m = p_re.findall(feed_entry['summary'])
            paper['abstract'] = m[0].strip()
            m = title_re.findall(feed_entry['title'])[0]
            paper['title'] = m[0].strip()
            paper['arxiv-topic-area'] = m[2].strip()
            paper['id'] = m[1].strip()
            paper['year'] = datetime.datetime.now().year
            yield paper
        if verbose:
            print "  Received %s entries. Sleeping for 20 seconds as requested in robots.txt" % str(num_entries)
        time.sleep(20) #robots.txt

def load_cache(filename=cache_filename):
    if not os.path.isfile(filename):
        print "Cache file %s does not exist. You must invoke the feed downloader at least once with get_papers(download=True)" % filename
        exit()
    with open(filename) as cache_file:
        papers = pickle.load(cache_file)
    return papers

def append_to_cache(papers, filename=cache_filename):
    """Add newly downloaded files to the cache, making sure not to add duplicates. May not scale well for a large number of papers."""
    hashable_papers = set()
    if os.path.isfile(filename):
        with open(filename) as cache_file:
            cached_papers = pickle.load(cache_file)
            for paper in cached_papers:
                hashable_papers.add(frozenset(tuple(paper.items())))
    for paper in papers:
        hashable_papers.add(frozenset(tuple(paper.items())))
    new_cache = []
    for paper in hashable_papers:
        new_cache.append(dict(paper))
    with open(filename, 'w') as cache_file:
        pickle.dump(new_cache, cache_file)
        
def get_papers(download=False, verbose=False, filename=cache_filename):
    """Returns the papers in the cache. Downloads new papers if the cache is empty or if cache=True."""
    if not download:
        return load_cache(filename)
    all_papers = []
    for paper in rss_gen(verbose=verbose):
        all_papers.append(paper)
    append_to_cache(all_papers, filename)
    if verbose:
        "Papers cached in %s ." % filename 
    return load_cache(filename)
        
if __name__ == '__main__':
    if os.path.isfile(cache_filename):
        papers = load_cache()
        print "Cache contains %s papers." % str(len(papers))
    else:
        print "No cache file. Downloading papers from the Arxiv."
    papers = get_papers(download=True, verbose=True)
    papers = load_cache()
    print "Cache contains %s papers." % str(len(papers))

    

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# SelectedPapers.net documentation build configuration file, created by
# sphinx-quickstart on Sun Mar 24 23:37:06 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'spnet'
copyright = u'2013, SelectedPapers.net'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.1'
# The full version, including alpha/beta/rc tags.
release = '0.1'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []

# If true, keep warnings as "system message" paragraphs in the built documents.
#keep_warnings = False


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'SelectedPapersnetdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index2', 'spnet.tex', u'spnet Documentation',
   u'SPnet', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index2', 'spnet', u'spnet Documentation',
     [u'SPnet'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index2', 'spnet', u'spnet Documentation',
   u'SPnet', 'spnet', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

# If true, do not generate a @detailmenu in the "Top" node's menu.
#texinfo_no_detailmenu = False

########NEW FILE########
__FILENAME__ = apptree
import rest
import core
import view
import gplus
import errors
from bson import ObjectId
import json
from sessioninfo import get_session
from urllib import urlencode


class ArrayDocCollection(rest.Collection):
    def _GET(self, docID, parents):
        return self.klass.find_obj_in_parent(parents.values()[0], docID)

class InterestCollection(ArrayDocCollection):
    '/papers/PAPER/likes/PERSON REST interface for AJAX calls'
    def check_permission(self, method, personID, *args, **kwargs):
        if method == 'GET': # permitted
            return False
        try:
            if personID != get_session()['person']._id:
                return view.report_error('TRAP set_interest by different user!', 403,
                                      "You cannot change someone else's settings!")
        except (KeyError,AttributeError):
            return view.report_error('TRAP set_interest, not logged in!', 401,
                                     'You must log in to access this interface')
    def _POST(self, personID, topic, state, parents, topic2=''):
        'add or remove topic from PaperInterest depending on state'
        topic = topic or topic2 # use whichever is non-empty
        topic = core.SIG.standardize_id(topic) # must follow hashtag rules
        personID = ObjectId(personID)
        state = int(state)
        if state: # make sure topic exists
            sig = core.SIG.find_or_insert(topic)
        interest = self.set_interest(personID, topic, state, parents)
        get_session()['person'].force_reload(True) # refresh user
        return interest
    def set_interest(self, personID, topic, state, parents):
        try:
            interest = self._GET(personID, parents)
        except KeyError:
            if state:
                person = core.Person(personID)
                docData = dict(author=personID, topics=[topic],
                               authorName=person.name)
                return core.PaperInterest(docData=docData,
                                          parent=parents['paper'])
            else: # trying to rm something that doesn't exist
                raise
        if state:
            interest.add_topic(topic)
        else:
            interest.remove_topic(topic)
        return interest
    def post_json(self, interest, **kwargs):
        return json.dumps(dict(interest='very good'))
    def post_html(self, interest, **kwargs):
        'display interest change by re-displaying the paper page'
        return view.redirect(interest.parent.get_value('local_url'))


class PaperCollection(rest.Collection):
    def _search(self, searchString, searchType):
        searchString = searchString.strip()
        if not searchString:
            s = view.report_error('empty searchString', 400,
                                  'You did not provide a search string.')
            return rest.Response(s)
        # user may type "Google Search:..." into Google Search box
        if searchString.lower().startswith('arxiv:'):
            searchString = searchString[6:].strip()
            searchType = 'arxivID'
        if searchType == 'arxivID':
            return rest.Redirect('/arxiv/%s' % searchString.replace('/', '_'))
        elif searchType == 'arxiv':
            return rest.Redirect('/arxiv?' + urlencode(dict(searchString=searchString)))
        elif searchType == 'PMID':
            return rest.Redirect('/pubmed/%s' % searchString)
        elif searchType == 'pubmed':
            return rest.Redirect('/pubmed?' + urlencode(dict(searchString=searchString)))
        elif searchType == 'ncbipubmed':
            return rest.Redirect('http://www.ncbi.nlm.nih.gov/sites/entrez?'
                                 + urlencode(dict(term=searchString,
                                                  db='pubmed')))
        elif searchType == 'shortDOI':
            return rest.Redirect('/shortDOI/%s' % searchString)
        elif searchType == 'DOI':
            dpd = core.DoiPaperData(DOI=searchString, insertNew='findOrInsert')
            return rest.Redirect('/shortDOI/%s' % dpd.id)
        elif searchType == 'spnetPerson':
            return rest.Redirect('/people?' + urlencode(dict(searchString=searchString)))
        elif searchType == 'topic':
            return rest.Redirect('/topics?' + urlencode(dict(searchString=searchString)))
        elif searchType == 'comment':
            return rest.Redirect('/posts?' + urlencode(dict(searchAll=searchString)))
        else:
            raise KeyError('unknown searchType ' + searchType)
                                 
    

class ParentCollection(rest.Collection):
    def _GET(self, docID, parents=None):
        try: # use cached query results if present
            queryResults = get_session()['queryResults']
        except (AttributeError, KeyError):
            pass
        else:
            try: # use cached docData if found for this docID
                docData = queryResults.get_doc_data(docID,
                                           self.collectionArgs['uri'])
            except KeyError: # not in query results
                pass
            else:
                return self.klass(docData=docData,
                                  insertNew='findOrInsert').parent
        return self.klass(docID, insertNew='findOrInsert').parent
    def _search(self, searchID):
        return rest.Redirect('%s/%s' % (self.collectionArgs['uri'], 
                                        searchID.replace('/', '_')))

class ArxivCollection(ParentCollection):
    def _POST(self, docID, showLatex=None):
        paper = self._GET(docID)
        if showLatex: # save on user session
            showLatex = int(showLatex)
            paper.update({'texDollars': showLatex and 1 or -1}, op='$inc')
            viewArgs = view.get_view_options()
            viewArgs.setdefault('showLatex', {})[paper] = showLatex
        return paper
    def post_html(self, paper, **kwargs):
        return self.get_html(paper, **kwargs)
    def _search(self, searchString=None, searchID=None, ipage=0,
                block_size=10, session=None):
        import arxiv
        ipage = int(ipage)
        block_size = int(block_size)
        if session is None:
            session = get_session()
        if searchID: # just get this ID
            return ParentCollection._search(self, searchID)
        if not searchString:
            s = view.report_error('empty searchString', 400,
                                  'You did not provide a search string.')
            return rest.Response(s)
        elif arxiv.is_id_string(searchString): # just get this ID
            return ParentCollection._search(self, searchString)
        try: # get from existing query results
            queryResults = session['queryResults']
            if queryResults.get_page(ipage, self.collectionArgs['uri'],
                                     searchString=searchString):
                return queryResults
        except KeyError:
            pass # no stored queryResults, so construct it
        pbl = view.PaperBlockLoader(arxiv.search_arxiv,
                                    uri=self.collectionArgs['uri'])
        queryResults = view.MultiplePages(pbl, block_size, ipage,
                                          self.collectionArgs['uri'],
                                          'arXiv.org Search Results',
                                          searchString=searchString)
        session['queryResults'] = queryResults # keep for this user
        return queryResults

class PubmedCollection(ParentCollection):
    def _search(self, searchString=None, searchID=None, ipage=0,
                block_size=20):
        import pubmed
        if not searchString:
            s = view.report_error('empty searchString', 400,
                                  'You did not provide a search string.')
            return rest.Response(s)
        ipage = int(ipage)
        block_size = int(block_size)
        try: # get from existing query results
            queryResults = get_session()['queryResults']
            if queryResults.get_page(ipage, self.collectionArgs['uri'],
                                     searchString=searchString):
                return queryResults
        except KeyError:
            pass # no stored queryResults, so construct it
        try:
            ps = pubmed.PubmedSearch(searchString, block_size)
            pbl = view.PaperBlockLoader(ps, uri=self.collectionArgs['uri'])
            queryResults = view.MultiplePages(pbl, block_size, ipage,
                                              self.collectionArgs['uri'],
                                              'Pubmed Search Results',
                                              searchString=searchString)
        except (errors.BackendFailure,KeyError):
            s = view.report_error('eutils error: ' + searchString, 502,
                                  '''Unfortunately, the NCBI eutils server
failed to perform the requested query.  
To run the <A HREF="/papers?%s">same search</A> on
NCBI Pubmed, please click here.  When you find a paper
of interest, you can copy its PMID (Pubmed ID) and
paste it in the search box on this page.''' 
                                  % urlencode(dict(searchType='ncbipubmed',
                                                   searchString=searchString)))
            return rest.Response(s)
        get_session()['queryResults'] = queryResults # keep for this user
        return queryResults
        


class PersonCollection(rest.Collection):
    def _GET(self, docID, getUpdates=False, timeframe=None, **kwargs):
        user = get_session().get('person', None)
        if user and docID == user._id:
            person = user # use cached Person object so we can mark it for refresh
        else:
            person = rest.Collection._GET(self, docID, **kwargs)
        if getUpdates:
            try:
                gpd = person.gplus
            except AttributeError:
                pass
            else: # get list of new posts
                if timeframe == 'all': # get last 10 years
                    l = gpd.update_posts(3650, recentEvents=view.recentEventsDeque)
                else:
                    l = gpd.update_posts(recentEvents=view.recentEventsDeque)
                if l: # need to update our object representation to see them
                    person = rest.Collection._GET(self, docID, **kwargs)
        return person
    def _search(self, searchString):
        if not searchString:
            raise KeyError('empty query')
        searchString = '(?i)' + searchString # default: case-insensitive
        l = list(self.klass.find_obj({'name': {'$regex': searchString}}))
        if not l:
            raise KeyError('no matches')
        return l

class PersonAuthBase(rest.Collection):
    'only allow logged-in user to POST his own settings'
    def check_permission(self, method, *args, **kwargs):
        if method == 'GET': # permitted
            return False
        user = get_session().get('person', None)
        if not user:
            return view.report_error('TRAP set_interest, not logged in!', 401,
                                     'You must log in to access this interface')
        person = kwargs['parents'].values()[0]
        if person != user:
            return view.report_error('TRAP set_interest by different user!', 403,
                                     "You cannot change someone else's settings!")

class ReadingList(PersonAuthBase):
    '/people/PERSON/reading/PAPER REST interface for AJAX calls'
    def _POST(self, paperID, state, parents):
        person = parents.values()[0]
        paperID = ObjectId(paperID)
        included = paperID in person._dbDocDict.get('readingList', ())
        state = (int(state) or False) and True # convert to boolean
        if state == included: # matches current state, so nothing to do
            return 0
        elif state: # add to reading list
            person.array_append('readingList', paperID)
            result = 1
        else:
            person.array_del('readingList', paperID)
            result = -1
        person.force_reload(True) # refresh user
        return result
    def post_json(self, status, **kwargs):
        return json.dumps(dict(status=status))

class PersonTopics(PersonAuthBase):
    '/people/PERSON/topics/TOPIC REST interface for AJAX calls'
    def _POST(self, topic, field, state, parents):
        person = parents.values()[0]
        try:
            tOpt = core.TopicOptions.find_obj_in_parent(person, topic)
        except KeyError:
            tOpt = core.TopicOptions(docData={'topic':topic, field:state}, 
                                     parent=person)
        else:
            tOpt.update({field:state})
        person.force_reload(True) # refresh user
        return 1
    def post_json(self, status, **kwargs):
        return json.dumps(dict(status=status))

class PersonSubscriptions(PersonAuthBase):
    '/people/PERSON/subscriptions/PERSON REST interface for AJAX calls'
    def _POST(self, author, field, state, parents):
        person = parents.values()[0]
        author = ObjectId(author)
        try:
            sub = core.Subscription.find_obj_in_parent(person, author)
        except KeyError:
            sub = core.Subscription(docData={'author':author, field:state}, 
                                     parent=person)
        else:
            sub.update({field:state})
        person.force_reload(True) # refresh user
        return 1
    def post_json(self, status, **kwargs):
        return json.dumps(dict(status=status))

class TopicCollection(rest.Collection):
    def _search(self, searchString=None, stem=None):
        if stem:
            return self.stem_search(stem)
        if not searchString:
            raise KeyError('empty query')
        searchString = '(?i)' + searchString # default: case-insensitive
        l = list(self.klass.find_obj({'_id': {'$regex': searchString}}))
        if not l:
            raise KeyError('no matches')
        return l
    def stem_search(self, stem): # return list of topics beginning with stem
        if not stem:
            return []
        return list(self.klass.find({'_id': {'$regex': '^' + stem}}))
    def search_json(self, data, **kwargs):
        return json.dumps(data)

class PostCollection(rest.Collection):
    def _search(self, searchAll=None):
        if not searchAll:
            raise KeyError('empty query')
        searchAll = '(?i)' + searchAll # default: case-insensitive
        l = list(core.Post.find_obj({'posts.text': {'$regex': searchAll}}))
        l += list(core.Reply.find_obj({'replies.text': {'$regex': searchAll}}))
        if not l:
            raise KeyError('no matches')
        return l

    
def get_collections(templateDir='_templates'):
    gplusClientID = gplus.get_keys()['client_id'] # most templates need this
    templateEnv = view.get_template_env(templateDir)
    view.report_error.bind_template(templateEnv, 'error.html') # error page

    # access Papers using our object ID
    papers = PaperCollection('paper', core.Paper, templateEnv, templateDir,
                             gplusClientID=gplusClientID)
    # using arxivID
    arxivPapers = ArxivCollection('paper', core.ArxivPaperData, templateEnv,
                                   templateDir, gplusClientID=gplusClientID,
                             collectionArgs=dict(uri='/arxiv'))
    # using shortDOI
    doiPapers = ParentCollection('paper', core.DoiPaperData, templateEnv,
                                 templateDir, gplusClientID=gplusClientID,
                          collectionArgs=dict(uri='/shortDOI'))
    # using pubmedID
    pubmedPapers = PubmedCollection('paper', core.PubmedPaperData,
                                    templateEnv, templateDir,
                                    gplusClientID=gplusClientID,
                             collectionArgs=dict(uri='/pubmed'))

    ## recs = ArrayDocCollection('rec', core.Recommendation,
    ##                           templateEnv, templateDir,
    ##                           gplusClientID=gplusClientID)
    ## papers.recs = recs # bind as subcollection

    likes = InterestCollection('like', core.PaperInterest, templateEnv,
                               templateDir, gplusClientID=gplusClientID)
    papers.likes = likes # bind as subcollection

    people = PersonCollection('person', core.Person, templateEnv, templateDir,
                              gplusClientID=gplusClientID)
    readingList = ReadingList('reading', core.Paper, templateEnv, templateDir,
                              gplusClientID=gplusClientID)
    people.reading = readingList
    personTopics = PersonTopics('topics', core.SIG, templateEnv, templateDir,
                                gplusClientID=gplusClientID)
    people.topics = personTopics
    personSubs = PersonSubscriptions('subscriptions', core.Subscription, 
                                     templateEnv, templateDir,
                                     gplusClientID=gplusClientID)
    people.subscriptions = personSubs
    topics = TopicCollection('topic', core.SIG, templateEnv, templateDir,
                             gplusClientID=gplusClientID)

    posts = PostCollection('post', core.Post, templateEnv, templateDir,
                           gplusClientID=gplusClientID)
    replies = PostCollection('reply', core.Reply, templateEnv, templateDir,
                             gplusClientID=gplusClientID)

    # load homepage template
    homePage = view.TemplateView(templateEnv.get_template('index.html'),
                                 gplusClientID=gplusClientID)

    # what collections to bind on the server root
    return dict(papers=papers,
                arxiv=arxivPapers,
                shortDOI=doiPapers,
                pubmed=pubmedPapers,
                people=people,
                topics=topics,
                posts=posts,
                replies=replies,
                index=homePage)

########NEW FILE########
__FILENAME__ = arxiv
import feedparser
import twitter
#import core
from time import mktime
from datetime import datetime
import urllib
import re

arxivApiUrl = 'http://export.arxiv.org/api/query?'

def get_arxiv_id(arxivURL):
    'extract arxivID from arxiv URL, replacing / --> _'
    l = arxivURL.split('/')
    s = '_'.join(l[4:])
    try:
        return s[:s.rindex('v')] # remove version info
    except ValueError:
        return s

def normalize_arxiv_dict(d):
    'convert id and author names to fit our standard'
    d['id'] = get_arxiv_id(d['id']) # replace URL by arxivID
    d['authorNames'] = [ad['name'] for ad in d['authors']]
    return d


def is_id_string(s,
                 dottedNumber=re.compile(
                     r'[0-9][0-9][0-9]+\.[0-9]+[0-9v][0-9]+$'),
                 fieldSlashNumber=re.compile(
                     '[a-z][a-z]+[a-z-][a-z]+/[0-9]+[0-9v][0-9]+$')):
    'True if s looks like an arXiv paper string'
    return dottedNumber.match(s) or fieldSlashNumber.match(s)

def lookup_papers(id_list, **kwargs):
    'retrieve a list of arxiv IDs, as a generator function'
    d = kwargs.copy()
    for i in range(0, len(id_list), 10):
        d['id_list'] = ','.join(id_list[i:i + 10])
        url = arxivApiUrl + urllib.urlencode(d)
        f = feedparser.parse(url)
        for e in f.entries:
            paper = normalize_arxiv_dict(e)
            if not paper['id'].startswith('error'):
                yield paper

def search_arxiv(searchString, start=0, block_size=25):
    'retrieve list of block_size results for specified search'
    q = dict(search_query=searchString, max_results=str(block_size),
             start=str(start))
    url = arxivApiUrl + urllib.urlencode(q)
    f = feedparser.parse(url)
    l = []
    for e in f.entries:
        l.append(normalize_arxiv_dict(e))
    return l


def search_arxiv_iter(search_query, block_size=25):
    'iterate over arxiv papers matching search_query'
    start = 0
    q = dict(search_query=search_query, max_results=str(block_size))
    while True:
        q['start'] = str(start)
        url = arxivApiUrl + urllib.urlencode(q)
        f = feedparser.parse(url)
        for e in f.entries:
            yield normalize_arxiv_dict(e)
        start += block_size
        if len(f.entries) < block_size:
            break
        

excludeUsers = set((154769981,)) # just a robot, so ignore!


def recent_tweets(query='http://arxiv.org'):
    'latest tweets of arxiv paper references'
    for tweet in twitter.get_recent(query):
        if tweet.from_user_id in excludeUsers:
            continue
        for arxivID in twitter.extract_arxiv_id(tweet):
            yield arxivID, tweet
            
    

########NEW FILE########
__FILENAME__ = base
from bson.objectid import ObjectId
from bson.errors import InvalidId
from time import mktime, struct_time
from datetime import datetime
import re


class IdString(str):
    def __cmp__(self, other):
        if isinstance(other, ObjectId):
            return cmp(ObjectId(self), other)
        else:
            return cmp(self, other)

class LinkDescriptor(object):
    '''property that fetches data only when accessed.
    Typical usage: for a foreign key that accesses another collection.
    Caches obj.ATTR link data as obj._ATTR_link'''
    def __init__(self, attr, fetcher, noData=False,
                 missingData=False, **kwargs):
        self.attr = attr
        self.fetcher = fetcher
        self.kwargs = kwargs
        self.noData = noData
        self.missingData = missingData
    def __get__(self, obj, objtype):
        'actually fetch the object(s) specified by cached data'
        try: # return the cached attribute
            return obj.__dict__[self.attr]
        except KeyError:
            pass
        if self.noData: # just fetch using object
            target = self.fetcher(obj, **self.kwargs)
        else: # fetching using cached data
            try:
                data = getattr(obj, '_' + self.attr + '_link')
            except AttributeError:
                if self.missingData is not False:
                    return self.missingData
                raise
            target = self.fetcher(obj, data, **self.kwargs)
        # Save in __dict__ to evade __set__.
        obj.__dict__[self.attr] = target
        return target
    def __set__(self, obj, data):
        'cache some link data for fetching later'
        if isinstance(data, Document):
            obj.__dict__[self.attr] = data # bypass LinkDescriptor mechanism
            data = data._id # get its ID
        setattr(obj, '_' + self.attr + '_link', data)

def _get_object_id(fetchID):
    try:
        return ObjectId(fetchID)
    except InvalidId, e:
        raise KeyError(str(e))

def del_list_value(l, v):
    for i,v2 in enumerate(l):
        if v2 == v:
            del l[i]
            return True

def convert_to_id(v):
    try:
        return v._id
    except AttributeError:
        return v

def convert_times(d):
    'convert times to format that pymongo can serialize'
    for k,v in d.items():
        if isinstance(v, struct_time):
            d[k] = datetime.fromtimestamp(mktime(v))
            

# base document classes

# two different mechanisms for subobject references:
#
# LinkDescriptor: for foreign key; defers construction until getattr()
# _attrHandler: for subdocuments; constructs them immediately on loading
#               the parent document.

def base_find_or_insert(klass, fetchID, **kwargs):
    'save to db if not already present'
    try:
        return klass(fetchID)
    except KeyError:
        return klass(docData=dict(_id=fetchID, **kwargs))

class Document(object):
    'base class provides flexible method for storing dict as attr objects'
    useObjectId = True
    
    def __init__(self, fetchID=None, docData=None, insertNew=True):
        '''data can be passed in either as object IDs or as objects

        If fetchID provided, retrieves that doc, or raises KeyError.
        Otherwise, the object is initialized from docData,
        and if insertNew, ALSO inserted into the database.'''
        if fetchID:
            docData = self._get_doc(fetchID)
        elif insertNew:
            self.check_required_fields(docData)
            self.insert(docData) # save to database
        self._dbDocDict = docData
        self.set_attrs(docData) # expose as object attributes

    def check_required_fields(self, docData):
        for attr in getattr(self, '_requiredFields', ()):
            if attr not in docData:
                raise ValueError('missing required field %s' % attr)

    def _get_doc(self, fetchID):
        'get doc dict from DB'
        if getattr(self, 'useObjectId', False):
            fetchID = _get_object_id(fetchID)
        d = self.coll.find_one(fetchID)
        if not d:
            raise KeyError('%s %s not found'
                           % (self.__class__.__name__, fetchID))
        return d

    def set_attrs(self, d):
        'set attributes based on specified dict of items'
        attrHandler = getattr(self, '_attrHandler', {})
        l = []
        for attr, v in d.items():
            try:
                l.append((attrHandler[attr], attr, v))
            except KeyError:
                setattr(self, attr, v)
        for saveFunc, attr, v in l: # run saveFuncs after regular attrs
            saveFunc(self, attr, v)

    def insert(self, d):
        'insert into DB'
        try: # if class defines _idField, store it as mongodb _id
            d['_id'] = d[self._idField]
        except AttributeError:
            pass
        self._id = self.coll.insert(convert_obj_to_id(d))
        self._dbDocDict = d
        self._isNewInsert = True

    def update(self, updateDict, op='$set'):
        'update the specified fields in the DB'
        self.coll.update({'_id': self._id}, {op: updateDict})
        self._dbDocDict.update(updateDict)
        self.set_attrs(updateDict)
        
    def delete(self):
        'delete this record from the DB'
        self.coll.remove(self._id)

    def array_append(self, attr, v):
        'append v to array stored as attr'
        v = convert_to_id(v)
        self.coll.update({'_id': self._id}, {'$push': {attr: v}})

    def array_del(self, attr, v):
        'remove element v from array stored as attr'
        v = convert_to_id(v)
        self.coll.update({'_id': self._id}, {'$pull': {attr: v}})

    def __cmp__(self, other):
        try:
            return cmp(self._id, other._id)
        except AttributeError:
            return cmp(id(self), id(other))

    def __hash__(self):
        return hash(self._id)

    def get_value(self, stem='spnet_url', **kwargs):
        'get specified kind of value by dispatching to specific paper type'
        for b in getattr(self, '_get_value_attrs', ()):
            try:
                o = getattr(self, b)
            except AttributeError:
                pass
            else:
                return getattr(o, 'get_' + stem)(**kwargs)
        return getattr(self, 'get_' + stem)(**kwargs)
    def get_spnet_url(self):
        return self._spnet_url_base + self.get_local_url()

    @classmethod
    def find(klass, queryDict={}, fields=None, idOnly=True,
             sortKeys=None, limit=None, **kwargs):
        'generic class method for searching a specific collection'
        if fields:
            idOnly = False
        if idOnly:
            fields = {'_id':1}
        if sortKeys or limit: # use new mongodb aggregation framework
            pipeline = []
            if sortKeys:
                pipeline.append({'$sort':sortKeys})
            if limit:
                pipeline.append({'$limit':limit})
            r = klass.coll.aggregate(pipeline)
            it = iter(r['result'])
        else:
            it = klass.coll.find(queryDict, fields, **kwargs)
        for d in it:
            if idOnly:
                yield d['_id']
            else:
                yield d

    @classmethod
    def find_obj(klass, queryDict={}, **kwargs):
        'same as find() but returns objects'
        for d in klass.find(queryDict, None, False, **kwargs):
            yield klass(docData=d, insertNew=False)
    find_or_insert = classmethod(base_find_or_insert)

def convert_obj_to_id(d):
    'replace Document objects by their IDs'
    d = d.copy()
    for k,v in d.items():
        if isinstance(v, Document):
            d[k] = v._id
    return d

class EmbeddedDocBase(Document):
    def _set_parent(self, parent):
        if hasattr(parent, 'coll'):
            self._parent_link = parent._id # save its ID
            self.__dict__['parent'] = parent # bypass LinkDescriptor mech
        else:
            self._parent_link = parent
    def get_parent_url(self):
        return self._parent_url % self._parent_link

class EmbeddedDocument(EmbeddedDocBase):
    'stores a document inside another document in mongoDB'
    def __init__(self, fetchID=None, docData=None, parent=None,
                 insertNew=True):
        if parent is not None:
            self._set_parent(parent)
        if insertNew == 'findOrInsert':
            if not fetchID:
                fetchID = docData[self._dbfield.split('.')[-1]]
            try: # retrieve from database
                Document.__init__(self, fetchID)
            except KeyError: # insert new record in database
                if not docData: # get data from external query
                    docData = self._query_external(fetchID)
                fetchID2 = docData[self._dbfield.split('.')[-1]]
                if fetchID2 != fetchID:
                    try:
                        Document.__init__(self, fetchID2)
                        return # already have this doc, don't duplicate!
                    except KeyError:
                        pass
                if parent is None:
                    self._set_parent(self._insert_parent(docData))
                    subdocField = self._dbfield.split('.')[0]
                    setattr(self.parent, subdocField, self)
                self.insert(docData) # save to database
                self.set_attrs(docData) # expose as object attributes
        else:
            Document.__init__(self, fetchID, docData, insertNew)
    def _get_doc(self, fetchID):
        'retrieve DB array record containing this document'
        subdocField = self._dbfield.split('.')[0]
        d = self.coll.find_one({self._dbfield: fetchID},
                               {subdocField: 1, '_id':1})
        if not d:
            raise KeyError('no such record: _id=%s' % fetchID)
        self._parent_link = d['_id']
        return d[subdocField]
    def insert(self, d):
        subdocField = self._dbfield.split('.')[0]
        convert_times(d)
        self.coll.update({'_id': self._parent_link},
                         {'$set': {subdocField: convert_obj_to_id(d)}})
        self._dbDocDict = d
        self._isNewInsert = True
    def update(self, updateDict):
        'update the existing embedded doc fields in the parent document'
        subdocField = self._dbfield.split('.')[0]
        d = {}
        for k,v in updateDict.items():
            d[subdocField + '.' + k] = v
        self.coll.update({'_id': self._parent_link}, {'$set': d})
        self._dbDocDict.update(updateDict)
        self.set_attrs(updateDict)
    def __cmp__(self, other):
        try:
            return cmp((self._parent_link,self._dbfield),
                       (other._parent_link,other._dbfield))
        except AttributeError:
            return cmp(id(self), id(other))
    def __hash__(self):
        return hash((self._parent_link,self._dbfield))

def find_one_array_doc(array, keyField, subID):
    'find record in the array with keyField matching subID'
    for record in array:
        if record[keyField] == subID:
            return record
    raise KeyError('no matching ArrayDocument!')
        
def filter_array_docs(array, keyField, subID):
    'find records in the array with keyField matching subID'
    for record in array:
        try:
            v = record[keyField]
        except KeyError:
            continue # lacks this field, so no match
        if isinstance(v, list): # handle array fields specially
            if subID in v:
                yield record
        elif v == subID: # regular field
            yield record
        elif isinstance(subID, dict):
            subquery = subID.items()
            if len(subquery) == 1 and subquery[0][0] == '$regex':
                if re.search(subquery[0][1], v):
                    yield record
            else:
                raise ValueError('non-regex query ops not implemented')
        
class ArrayDocument(EmbeddedDocBase):
    '''stores a document inside an array in mongoDB.
    Assumes we need to use a tuple of (parentID,subID) to
    uniquely identify each document.  For example, we specify
    a Recommendation by (paperID,authorID).
    Subclasses MUST provide _dbfield attribute of the form
    field.subfield, indicating how to search for fetchID in the parent
    document.'''
    def __init__(self, fetchID=None, docData=None, parent=None,
                 insertNew=True):
        self._set_parent(parent)
        if insertNew == 'findOrInsert':
            try: # retrieve from database
                if fetchID is None:
                    if parent is None:
                        raise ValueError('missing parent ID')
                    fetchID = self._extract_fetchID(docData)
                Document.__init__(self, fetchID)
                return # found our data, nothing further to do
            except KeyError: # insert new record in database
                fetchID = None
        Document.__init__(self, fetchID, docData, insertNew)
    def _get_doc(self, fetchID):
        'retrieve DB array record containing this document'
        self._parent_link,subID = fetchID
        if getattr(self, 'useObjectId', False):
            self._parent_link = _get_object_id(self._parent_link)
        arrayField, keyField = self._dbfield.split('.')
        d = self.coll.find_one(self._parent_link, {arrayField: 1})
        if not d:
            raise KeyError('no such record: _id=%s' % self._parent_link)
        return find_one_array_doc(d[arrayField], keyField, subID)
    def _extract_fetchID(self, docData):
        return (self._parent_link, docData[self._dbfield.split('.')[-1]])
    def _get_id(self):
        'return subID for this array record'
        keyField = self._dbfield.split('.')[1]
        return self._dbDocDict[keyField]
    def insert(self, d):
        'append to the target array in the parent document'
        try:
            if self._timeStampField not in d:
                d[self._timeStampField] = datetime.utcnow() # add timestamp
        except AttributeError:
            pass
        self._dbDocDict = d
        arrayField = self._dbfield.split('.')[0]
        self.coll.update({'_id': self._parent_link},
                         {'$push': {arrayField: convert_obj_to_id(d)}})
        self._isNewInsert = True
    def update(self, updateDict):
        'update the existing record in the array in the parent document'
        arrayField = self._dbfield.split('.')[0]
        d = {}
        for k,v in updateDict.items():
            d['.'.join((arrayField, '$', k))] = v
        subID = self._get_id()
        self.coll.update({'_id': self._parent_link, self._dbfield: subID},
                         {'$set': d})
        self._dbDocDict.update(updateDict)
        self.set_attrs(updateDict)

    def delete(self):
        'delete this record from the array in the parent document'
        arrayField, keyField = self._dbfield.split('.')
        subID = self._get_id()
        self.coll.update({'_id': self._parent_link},
                         {'$pull': {arrayField: {keyField: subID}}})

    def _array_update(self, attr, l):
        'replace attr array in db by the specified list l'
        arrayField = self._dbfield.split('.')[0]
        subID = self._get_id()
        target = '.'.join((arrayField, '$', attr))
        self.coll.update({'_id': self._parent_link, self._dbfield: subID},
                         {'$set': {target: l}})

    def array_append(self, attr, v):
        'append v to array stored as attr'
        v = convert_to_id(v)
        try:
            self._dbDocDict[attr].append(v)
        except KeyError:
            self._dbDocDict[attr] = [v]
        self._array_update(attr, self._dbDocDict[attr])

    def array_del(self, attr, v):
        'remove element v from array stored as attr'
        v = convert_to_id(v)
        l = self._dbDocDict[attr]
        if del_list_value(l, v):
            self._array_update(attr, l)
        else:
            raise IndexError('array %s does not contain %s'
                             % (attr, str(v)))
        
    def __cmp__(self, other):
        try:
            return cmp((self._parent_link, self._get_id()),
                       (other._parent_link, other._get_id()))
        except AttributeError:
            return cmp(id(self), id(other))
    def __hash__(self):
        return hash((self._parent_link, self._get_id()))

    @classmethod
    def _id_only(klass, d, d2, keyField):
        return d['_id'], d2[keyField]

    @classmethod
    def find(klass, queryDict={}, fields=None, idOnly=True, parentID=False,
             **kwargs):
        'generic class method for searching a specific collection'
        if fields:
            idOnly = False
        if idOnly:
            fields = {klass._dbfield:1}
        arrayField, keyField = klass._dbfield.split('.')
        if not fields: # get this array
            fields = {arrayField:1}
        filters = []
        for k,v in queryDict.items():
            queryFields = k.split('.')
            if queryFields[0] == arrayField:
                filters.append((queryFields[1], v))
        if not queryDict: # get docs containing this array
            queryDict = {arrayField: {'$exists':True}}
        for d in klass.coll.find(queryDict, fields, **kwargs): # query db
            try:
                array = d[arrayField]
            except KeyError:
                continue # not present in this record, so skip
            for k,v in filters: # apply filters consecutively
                array = list(filter_array_docs(array, k, v))
            for d2 in array: # return the filtered results appropriately
                if idOnly:
                    yield klass._id_only(d, d2, keyField)
                elif parentID:
                    yield d['_id'], d2
                else:
                    yield d2

    @classmethod
    def find_obj(klass, queryDict={}, **kwargs):
        'same as find() but returns objects'
        arrayField = klass._dbfield.split('.')[0]
        for parentID, d in klass.find(queryDict, None, False, True, **kwargs):
            yield klass(docData=d, parent=parentID, insertNew=False)

    @classmethod
    def find_obj_in_parent(klass, parent, subID):
        'search parent document for specified ArrayDocument'
        arrayField = klass._dbfield.split('.')[0]
        for o in getattr(parent, arrayField, ()):
            if o._get_id() == subID:
                return o
        raise KeyError('%s not found in %s' % (subID, klass._dbfield))


class UniqueArrayDocument(ArrayDocument):
    '''For array documents with unique ID values '''
    def _extract_fetchID(self, docData):
        return docData[self._dbfield.split('.')[-1]]
    def _get_doc(self, fetchID):
        'retrieve DB array record containing this document'
        arrayField, keyField = self._dbfield.split('.')
        d = self.coll.find_one({self._dbfield: fetchID}, {arrayField: 1})
        if not d:
            raise KeyError('no such record: %s=%s' % (self._dbfield, fetchID))
        self._parent_link = d['_id'] # save parent ID
        return find_one_array_doc(d[arrayField], keyField, fetchID)

    @classmethod
    def _id_only(klass, d, d2, keyField):
        return d2[keyField]

class AutoIdArrayDocument(UniqueArrayDocument):
    'makes a unique ID automatically for you'
    def __init__(self, fetchID=None, docData=(), parent=None, insertNew=True):
        keyField = self._dbfield.split('.')[1]
        if fetchID is None and keyField not in docData: # create an ID
            try: # copy dict before modifying it, to prevent side-effects
                docData = docData.copy()
            except AttributeError:
                docData = {}
            docData[keyField] = ObjectId() # use pymongo's ID generator
        elif fetchID and not isinstance(fetchID, ObjectId):
            fetchID = ObjectId(fetchID)
        super(AutoIdArrayDocument, self).__init__(fetchID, docData, parent,
                                                  insertNew)
            

# generic retrieval classes

class FetchObj(object):
    def __init__(self, klass, **kwargs):
        self.klass = klass
        self.kwargs = kwargs
    def __call__(self, obj, fetchID):
        return self.klass(fetchID, **self.kwargs)


class FetchList(FetchObj):
    def __call__(self, obj, fetchIDs):
        l = []
        for fetchID in fetchIDs:
            l.append(self.klass(fetchID, **self.kwargs))
        return l

class FetchQuery(FetchObj):
    def __init__(self, klass, queryFunc, **kwargs):
        FetchObj.__init__(self, klass, **kwargs)
        self.queryFunc = queryFunc
    def __call__(self, obj, **kwargs):
        query = self.queryFunc(obj, **kwargs)
        return list(self.klass.find_obj(query))

class FetchParent(FetchObj):
    def __call__(self, obj):
        return self.klass(obj._parent_link, **self.kwargs)

class FetchObjByAttr(FetchObj):
    def __init__(self, klass, attr, **kwargs):
        self.attr = attr
        FetchObj.__init__(self, klass, **kwargs)
    def __call__(self, obj):
        return self.klass(getattr(obj, self.attr), **self.kwargs)

class SaveAttr(object):
    'unwrap dict using specified klass'
    def __init__(self, klass, arg='parent', postprocess=None, **kwargs):
        self.klass = klass
        self.kwargs = kwargs
        self.arg = arg
        self.postprocess = postprocess
    def __call__(self, obj, attr, data):
        kwargs = self.kwargs.copy()
        kwargs[self.arg] = obj
        o = self.klass(docData=data, **kwargs)
        if self.postprocess:
            self.postprocess(obj, attr, o)
        setattr(obj, attr, o)

class SaveAttrList(SaveAttr):
    'unwrap list of dicts using specified klass'
    def __call__(self, obj, attr, data):
        l = []
        for d in data:
            kwargs = self.kwargs.copy()
            kwargs[self.arg] = obj
            l.append(self.klass(docData=d, **kwargs))
        if self.postprocess:
            self.postprocess(obj, attr, l)
        setattr(obj, attr, l)


########NEW FILE########
__FILENAME__ = boost_gplus
import core

def find_user_posts(**kwargs):
    'query gplus for posts from all users with posts in our db'
    oldposts = set(core.Post.find())
    l = []
    gplusUsers = set([d['actor']['id'] for d in  core.Post.find(fields={'posts.actor.id':1})])
    for gplusID in gplusUsers:
        gpd = core.GplusPersonData(gplusID)
        print 'retrieving', gpd.displayName, gplusID
        for post in gpd.update_posts(**kwargs):
            if post.id not in oldposts:
                print '  new post', post.id
                l.append(post)
    return l

########NEW FILE########
__FILENAME__ = bulk
import core
from datetime import datetime

def find_people_topics():
    'construct dict of person:topics from Recs, Posts, PaperInterests'
    people = {}
    for r in core.Post.find_obj():
        authorID = r._dbDocDict['author']
        topics = r._dbDocDict.get('sigs', ())
        try:
            people[authorID].update(topics)
        except KeyError:
            people[authorID] = set(topics)
    for r in core.PaperInterest.find_obj():
        authorID = r._dbDocDict['author']
        topics = r._dbDocDict['topics']
        try:
            people[authorID].update(topics)
        except KeyError:
            people[authorID] = set(topics)
    return people

def insert_people_topics(peopleTopics):
    'add topics to each Person.topics array'
    for personID,topics in peopleTopics.items():
        core.Person.coll.update({'_id':personID},
                                {'$addToSet': {'topics': {'$each':list(topics)}}})

def get_people_subs():
    'get dicts of {topic:[subscribers]} and {person:[subscribers]}'
    topics = {}
    subs = {}
    for d in core.Person.find({}, {'topics':1, 'topicOptions':1, 'subscriptions':1}):
        personID = d['_id']
        hideSet = set([dd['topic'] for dd in d.get('topicOptions', ())
                       if dd.get('fromOthers', 0) == 'hide'])
        for topic in d.get('topics', ()): # index topics this person wants to get
            if topic in hideSet: # user doesn't want all recs on this topic
                continue
            try:
                topics[topic].append(personID)
            except KeyError:
                topics[topic] = [personID]
        for sub in d.get('subscriptions', ()): # index people this person wants to get
            author = sub['author']
            try:
                subs[author].append(personID)
            except KeyError:
                subs[author] = [personID]
    return topics, subs

def deliver_rec(paperID, r, topics, subs):
    author = r['author']
    sigs = r.get('sigs', ())
    docData = {'paper':paperID, 'from':author, 'topics':sigs,
               'name':r.get('actor', {}).get('displayName', 'Unknown'),
               'published':r.get('published', datetime.utcnow()),
               'title':r.get('title', 'New Post'), 'post':r['id'],}
    for personID in subs.get(author, ()): # deliver to subscribers
        core.Person.coll.update({'_id':personID},
                                {'$addToSet': {'received': docData}})
    for topic in sigs:
        for personID in topics.get(topic, ()): # deliver to subscribers
            if personID == author: # don't deliver back to author!
                continue
            core.Person.coll.update({'_id':personID},
                                    {'$addToSet': {'received': docData}})


def deliver_recs(topics, subs):
    'insert appropriate recs to Person.received array records'
    for paperID, r in core.Post.find(idOnly=False, parentID=True):
        deliver_rec(paperID, r, topics, subs)

########NEW FILE########
__FILENAME__ = connect

from dbconn import DBConnection
from core import Paper, Person, EmailAddress, Issue, IssueVote, SIG, GplusPersonData, Post, Reply, ArxivPaperData, PubmedPaperData, DoiPaperData, PaperInterest, GplusSubscriptions, Subscription, TopicOptions, Citation
import json
from pymongo.errors import ConnectionFailure

connectDict = {
    Paper:'spnet.paper',
    Person:'spnet.person',
    EmailAddress:'spnet.person',
    Issue:'spnet.issue',
    IssueVote:'spnet.issue',
    SIG:'spnet.sig',
    GplusPersonData:'spnet.person',
    GplusSubscriptions:'spnet.gplus_subs',
    Post:'spnet.paper',
    Reply:'spnet.paper',
    Citation:'spnet.paper',
    ArxivPaperData:'spnet.paper',
    PubmedPaperData:'spnet.paper',
    DoiPaperData:'spnet.paper',
    PaperInterest:'spnet.paper',
    Subscription:'spnet.person',
    TopicOptions:'spnet.person',
    }



def init_connection(spnetUrlBase='https://selectedpapers.net', 
                    dbconfFile='../mongodb/access.json', **kwargs):
    'set klass.coll on each db class to give it db connection'
    try:
        with open(dbconfFile) as ifile:
            dbconfig = json.load(ifile)
        kwargs.update(dbconfig)
        print 'read db connection settings from', dbconfFile
    except IOError:
        pass
    try:
        dbconn = DBConnection(connectDict, **kwargs)
    except ConnectionFailure:
        print '''ERROR: database connection failed with settings: %s
If you are running a test / development platform, 
make sure that your mongod is running
and accepting connections (from localhost, without a password).''' % str(kwargs)
        raise
    for klass in connectDict: # set default URL
        klass._spnet_url_base = spnetUrlBase
    return dbconn
    

########NEW FILE########
__FILENAME__ = core
from base import *
from hashlib import sha1
import re
from datetime import datetime
import errors
import thread
import latex
import time


##########################################################

# fetch functions for use with LinkDescriptor 

def fetch_recs(obj):
    'get subset of posts that are recommendations'
    return filter(lambda p:p.is_rec(), obj.posts)


def merge_sigs(person, attr, sigLinks):
    'postprocess list of SIGLinks to handle mergeIn requests'
    d = {}
    merges = []
    for sl in sigLinks:
        d[sl.dbDocDict['sig']] = sl
        if hasattr(sl, 'mergeIn'):
            merges.append(sl)
    for sl in merges:
        d[sl.mergeIn]._add_merge(sl)
    


######################################################

# forward declarations to avoid circular ref problem
fetch_paper = FetchObj(None)
fetch_person = FetchObj(None)
fetch_post = FetchObj(None)
fetch_sig = FetchObj(None)
fetch_sigs = FetchList(None)
fetch_people = FetchList(None)
fetch_papers = FetchList(None)
fetch_parent_issue = FetchParent(None)
fetch_parent_person = FetchParent(None)
fetch_parent_paper = FetchParent(None)
fetch_author_papers = FetchQuery(None, lambda author:dict(authors=author._id))
fetch_subscribers = FetchQuery(None, lambda person:
                               {'subscriptions.author':person._id})
fetch_sig_members = FetchQuery(None, lambda sig: {'sigs.sig':sig._id})
fetch_sig_papers = FetchQuery(None, lambda sig: {'sigs':sig._id})
fetch_post_citations = FetchQuery(None, lambda post: {'citations.post':post.id})
fetch_sig_posts = FetchQuery(None, lambda sig:
                            {'posts.sigs':sig._id})
fetch_sig_interests = FetchQuery(None, lambda sig:
                                 {'interests.topics':sig._id})
fetch_issues = FetchQuery(None, lambda paper:dict(paper=paper._id))
fetch_person_posts = FetchQuery(None, lambda author:
                            {'posts.author':author._id})
fetch_person_replies = FetchQuery(None, lambda author:
                            {'replies.author':author._id})
fetch_person_interests = FetchQuery(None, lambda author:
                            {'interests.author':author._id})
fetch_gplus_by_id = FetchObjByAttr(None, '_id')
fetch_gplus_subs = FetchObjByAttr(None, 'id')

# main object classes

class EmailAddress(UniqueArrayDocument):
    _dbfield = 'email.address' # dot.name for updating

    parent = LinkDescriptor('parent', fetch_parent_person, noData=True)


class AuthorInfo(object):
    def get_author_name(self):
        try:
            return self.actor['displayName']
        except (AttributeError, KeyError):
            return self.author.name # fall back to database query
    def get_author_url(self):
        return '/people/' + str(self._dbDocDict['author'])
    def get_text(self, showLatex=True):
        if showLatex:
            return latex.convert_tex_dollars(self.text)
        else:
            return self.text

def report_topics(self, d, attr='sigs', method='insert', personAttr='author'):
    'wrap insert() or update() to insert topics into author Person record'
    try:
        topics = d[attr]
    except KeyError:
        pass
    else:
        try:
            personID = d[personAttr]
        except KeyError:
            personID = self._dbDocDict[personAttr]
        Person.coll.update({'_id': personID},
                           {'$addToSet': {'topics': {'$each':topics}}})
    return getattr(super(self.__class__, self), method)(d)

class Post(UniqueArrayDocument, AuthorInfo):
    _dbfield = 'posts.id' # dot.name for updating
    _timeStampField = 'published' # auto-add timestamp if missing
    _parent_url = '/papers/%s' # link for full paper record
    # attrs that will only be fetched if accessed by getattr
    parent = LinkDescriptor('parent', fetch_parent_paper, noData=True)
    citations = LinkDescriptor('citations', fetch_post_citations, noData=True)
    author = LinkDescriptor('author', fetch_person)
    sigs = LinkDescriptor('sigs', fetch_sigs, missingData=())
    def get_replies(self):
        'get all replies for this post'
        try:
            docID = self.id
        except AttributeError:
            return
        for r in getattr(self.parent, 'replies', ()):
            if r._dbDocDict['replyTo'] == docID:
                yield r
    insert = lambda self,d:report_topics(self, d)
    update = lambda self,d:report_topics(self, d, method='update')
    def get_topics(self):
        l = []
        for topicID in self._dbDocDict.get('sigs', ()):
            l.append(SIG(docData=dict(_id=topicID, name='#' + topicID), 
                         insertNew=False))
        return l
    def delete(self):
        for c in self.citations:
            c.delete()
        UniqueArrayDocument.delete(self)
    def get_local_url(self):
        return '/posts/' + self.id
    def is_rec(self):
        return getattr(self, 'citationType', 'discuss') \
            in ('mustread', 'recommend')
    def add_citations(self, papers, citationType='discuss'):
        'save paper citations to db'
        if not getattr(self, 'hasCitations', False):
            self.update(dict(hasCitations=True))
        l = []
        for paper in papers:
            d = dict(post=self.id, authorName=self.author.name,
                     title=self.title, published=self.published,
                     citationType=citationType)
            l.append(Citation(docData=d, parent=paper))
        return l


def fetch_reply_post(obj, fetchID):
    for post in getattr(obj.parent, 'posts', ()):
        if getattr(post, 'id', ('uNmAtChAbLe',)) == fetchID:
            return post
    raise KeyError('No post found with id=' + str(fetchID))


class Reply(UniqueArrayDocument, AuthorInfo):
    _dbfield = 'replies.id' # dot.name for updating
    _timeStampField = 'published' # auto-add timestamp if missing
    _parent_url = '/papers/%s' # link for full paper record
    # attrs that will only be fetched if accessed by getattr
    parent = LinkDescriptor('parent', fetch_parent_paper, noData=True)
    author = LinkDescriptor('author', fetch_person)
    replyTo = LinkDescriptor('replyTo', fetch_reply_post)
    def get_local_url(self):
        return self.get_post_url() + '#' + self.id
    def get_post_url(self):
        return '/posts/' + self._dbDocDict['replyTo']

class Citation(ArrayDocument):
    _dbfield = 'citations.post' # dot.name for updating
    _timeStampField = 'published' # auto-add timestamp if missing
    # attrs that will only be fetched if accessed by getattr
    parent = LinkDescriptor('parent', fetch_parent_paper, noData=True)
    post = LinkDescriptor('post', fetch_post)


class PaperInterest(ArrayDocument):
    _dbfield = 'interests.author' # dot.name for updating
    # attrs that will only be fetched if accessed by getattr
    parent = LinkDescriptor('parent', fetch_parent_paper, noData=True)
    author = LinkDescriptor('author', fetch_person)
    topics = LinkDescriptor('topics', fetch_sigs, missingData=())
    insert = lambda self,d:report_topics(self, d, 'topics')
    update = lambda self,d:report_topics(self, d, 'topics', method='update')
    def add_topic(self, topic):
        topics = set(self._dbDocDict.get('topics', ()))
        topics.add(topic)
        self.update(dict(topics=list(topics)))
        return self
    def remove_topic(self, topic):
        topics = set(self._dbDocDict.get('topics', ()))
        topics.remove(topic)
        if topics:
            self.update(dict(topics=list(topics)))
            return self
        else: # PaperInterest empty, so remove completely
            self.delete()
            return None
    def get_local_url(self):
        return '/papers/' + str(self._parent_link) + '/likes/' + \
               str(self._dbDocDict['author'])

class IssueVote(ArrayDocument):
    _dbfield = 'votes.person' # dot.name for updating
    person = LinkDescriptor('person', fetch_person)
    parent = LinkDescriptor('parent', fetch_parent_issue, noData=True)


class Issue(Document):
    '''interface for a question raised about a paper '''

    # attrs that will only be fetched if accessed by user
    paper = LinkDescriptor('paper', fetch_paper)
    author = LinkDescriptor('author', fetch_person)

    # custom attr constructors
    _attrHandler = dict(
        votes=SaveAttrList(IssueVote, insertNew=False),
        )

class SIG(Document):
    '''interface for a Specific Interest Group'''
    useObjectId = False # input data will supply _id
    _requiredFields = ('name',)

    # attrs that will only be fetched if accessed by user
    members = LinkDescriptor('members', fetch_sig_members, noData=True)
    papers = LinkDescriptor('papers', fetch_sig_papers, noData=True)
    recommendations  = LinkDescriptor('recommendations', fetch_recs,
                                      noData=True)
    posts  = LinkDescriptor('posts', fetch_sig_posts, noData=True)
    interests  = LinkDescriptor('interests', fetch_sig_interests, noData=True)
    tagRE = re.compile('[A-Za-z][A-Za-z0-9_]+$') # string allowed after #
    @classmethod
    def standardize_id(klass, fetchID):
        'ID must follow hashtag rules (or raise KeyError); rm leading #'
        if fetchID.startswith('#'): # don't include hash in ID
            fetchID = fetchID[1:]
        if not klass.tagRE.match(fetchID):
            raise KeyError('topic does not satisfy hashtag character rules: '
                           + fetchID)
        return fetchID
    @classmethod
    def find_or_insert(klass, fetchID, published=None, **kwargs):
        'save to db if not already present, after checking ID validity'
        fetchID = klass.standardize_id(fetchID)
        if published is None:
            published = datetime.utcnow() # ensure timestamp
        return base_find_or_insert(klass, fetchID, name='#' + fetchID,
                                   published=published, **kwargs)
    def get_interests(self):
        'return dict of paper:[people]'
        d = {}
        for interest in self.interests:
            try:
                d[interest.parent].append(interest.author)
            except KeyError:
                d[interest.parent] = [interest.author]
        return d
    def get_local_url(self):
        return '/topics/' + str(self._id)



class TopicOptions(ArrayDocument):
    _dbfield = 'topicOptions.topic' # dot.name for updating
    topic = LinkDescriptor('topic', fetch_sig)

class GplusPersonData(EmbeddedDocument):
    'store Google+ data for a user as subdocument of Person'
    _dbfield = 'gplus.id'
    parent = LinkDescriptor('parent', fetch_parent_person, noData=True)
    subscriptions = LinkDescriptor('subscriptions', fetch_gplus_subs,
                                   noData=True)
    def _query_external(self, userID):
        'obtain user info from Google+ API server'
        import gplus
        return gplus.publicAccess.get_person_info(userID)
    def _insert_parent(self, d):
        'create Person document in db for this gplus.id'
        p = Person(docData=dict(name=d['displayName']))
        docData = dict(author=p._id, gplusID=d['id'], topics=[])
        thread.start_new_thread(p.update_subscribers,
                                (GplusSubscriptions, docData, d['id']))
        return p
    def update_posts(self, maxDays=20, **kwargs):
        'get new posts from this person, updating old posts with new replies'
        import gplus
        oauth = gplus.publicAccess
        postIt = oauth.get_person_posts(self.id)
        l = [p for p in oauth.find_or_insert_posts(postIt, maxDays=maxDays,
                                                   **kwargs)
             if getattr(p, '_isNewInsert', False)]
        return l
    def init_subscriptions(self, doc, subs):
        'save GplusSubscriptions to db, from doc and subs data'
        d = dict(_id=self.id, subs=list(subs), etag=doc['etag'],
                 totalItems=doc['totalItems'])
        gps = GplusSubscriptions(docData=d)
        self.__dict__['subscriptions'] =  gps # bypass LinkDescriptor
    def update_subscriptions(self, doc, subs):
        try:
            gplusSub = self.subscriptions # use existing record
        except KeyError:
            newSubs = self.init_subscriptions(doc, subs) # create new
        else: # see if we have new subscriptions
            newSubs = gplusSub.update_subscriptions(doc, subs)
            if newSubs is None: # nothing to do
                return
        self.update_subs_from_gplus(newSubs) # update Person.subscriptions

    def update_subs_from_gplus(self, subs=None):
        '''see if we can update Person.subscriptions based on
        subs (list of NEW gplus person IDs).
        If subs is None, update based on our GplusSubscriptions.subs'''
        gplusSubs = set([d['id'] for d in self.subscriptions.subs])
        l = []
        for d in self.parent._dbDocDict.get('subscriptions', ()):
            try: # filter old subscriptions
                if d['gplusID'] in gplusSubs: # still in our subscriptions
                    l.append(d)
            except KeyError:
                l.append(d) # from some other service, so keep
        if subs is None: # all subscriptions are new
            subs = gplusSubs
        for gplusID in subs: # append new additions
            try: # find subset that map to Person
                p = self.__class__(gplusID).parent # find Person record
                l.append(dict(author=p._id, gplusID=gplusID, topics=[]))
            except KeyError:
                pass
        self.parent.update(dict(subscriptions=l)) # save to db
        self.parent._forceReload = True # safe way to refresh view



class GplusSubscriptions(Document):
    'for a gplus member, store his array of gplus subscriptions (his circles)'
    useObjectId = False # input data will supply _id
    _subscriptionIdField = 'subs.id' # query to find a subscription by ID
    gplusPerson = LinkDescriptor('gplusPerson', fetch_gplus_by_id,
                                 noData=True)
    def update_subscriptions(self, doc, subs):
        '''if G+ subscriptions changed, save and return the new list;
        otherwise return None'''
        if getattr(self, 'etag', None) == doc['etag']:
            return None # no change, so nothing to do
        subs = list(subs) # actually get the data from iterator
        oldSubsSet = set([d['id'] for d in self.subs])
        self.update(dict(subs=subs, etag=doc['etag'],
                         totalItems=doc['totalItems'])) # save to db
        self.subs = subs
        newSubsSet = set([d['id'] for d in subs])
        return newSubsSet - oldSubsSet # new subscriptions


def get_interests_sorted(d):
    'get topics sorted from most-used to least-used'
    l = [(len(v),k) for (k,v) in d.items()]
    l.sort(reverse=True)
    return [(t[1], d[t[1]]) for t in l]


class Subscription(ArrayDocument):
    _dbfield = 'subscriptions.author' # dot.name for updating
    # attrs that will only be fetched if accessed by user
    author = LinkDescriptor('author', fetch_person)
    topics = LinkDescriptor('topics', fetch_sigs, missingData=())



class Person(Document):
    '''interface to a stable identity tied to a set of publications '''
    _requiredFields = ('name',)
    # attrs that will only be fetched if accessed by user
    papers = LinkDescriptor('papers', fetch_author_papers, noData=True)
    recommendations = LinkDescriptor('recommendations', fetch_recs,
                                     noData=True)
    subscribers = LinkDescriptor('subscribers', fetch_subscribers,
                                 noData=True)
    posts = LinkDescriptor('posts', fetch_person_posts, noData=True)
    replies = LinkDescriptor('replies', fetch_person_replies, noData=True)
    interests = LinkDescriptor('interests', fetch_person_interests, noData=True)
    readingList = LinkDescriptor('readingList', fetch_papers, missingData=())

    # custom attr constructors
    _attrHandler = dict(
        email=SaveAttrList(EmailAddress, insertNew=False),
        gplus=SaveAttr(GplusPersonData, insertNew=False),
        subscriptions = SaveAttrList(Subscription, insertNew=False),
        topicOptions = SaveAttrList(TopicOptions, insertNew=False),
        )

    def authenticate(self, password):
        try:
            return self.password == sha1(password).hexdigest()
        except AttributeError:
            return False
    def set_password(self, password):
        self.update(dict(password=sha1(password).hexdigest()))
    def get_interests(self, sorted=False):
        'return dict of {topicID:[paperID,]}'
        d = {}
        for interest in self.interests:
            for topicID in interest._dbDocDict['topics']:
                try:
                    d[topicID].append(interest._parent_link)
                except KeyError:
                    d[topicID] = [interest._parent_link]
        if sorted:
            return get_interests_sorted(d)
        return d
    def get_local_url(self):
        return '/people/' + str(self._id)
    def update_subscribers(self, klass, docData, subscriptionID):
        '''when Person first inserted to db, connect to pending
        subscriptions by appending our new personID.'''
        for subID in klass.find({klass._subscriptionIdField: subscriptionID}):
            p = self.coll.find_one({'gplus.id': subID}, {'_id':1})
            if p is not None:
                personID = p['_id']
                Subscription((personID, self._id), docData=docData,
                             parent=personID, insertNew='findOrInsert')
    def get_topics(self, order = dict(hide=0, low=1, medium=2, high=3)):
        topicOptions = {}
        for tOpt in getattr(self, 'topicOptions', ()):
            topicOptions[tOpt._dbDocDict['topic']] = tOpt
        l = []
        for topic in getattr(self, 'topics', ()):
            try:
                tOpt = topicOptions[topic]
                l.append((topic, getattr(tOpt, 'fromMySubs', 'medium'),
                         getattr(tOpt, 'fromOthers', 'low')))
            except KeyError:
                l.append((topic, 'medium', 'low'))
        l.sort(lambda x,y:cmp(order.get(x[1], -1), order.get(y[1], -1)), 
               reverse=True)
        return l
    def get_deliveries(self, order = dict(hide=0, low=1, medium=2, high=3)):
        topics = {}
        for topic, fromMySubs, fromOthers in self.get_topics():
            fromMySubs = order[fromMySubs]
            if fromOthers == 'same':
                fromOthers = fromMySubs
            else:
                fromOthers = order[fromOthers]
            if fromMySubs > 0:
                topics[topic] = (fromMySubs, fromOthers)
        subs = {}
        for s in getattr(self, 'subscriptions', ()):
            subs[s._get_id()] = s
        priority = 0
        l = []
        for r in getattr(self, 'received', ()):
            try:
                sub = subs[r['from']]
                i = 0
            except KeyError:
                sub = None
                i = 1
            for topic in r['topics']:
                try:
                    priority = max(priority, topics[topic][i])
                except KeyError:
                    pass
            if sub:
                if priority > 0:
                    level = getattr(sub, 'onMyTopics', 'topic')
                else:
                    level = getattr(sub, 'onOthers', 'low')
                if level != 'topic':
                    priority = max(priority, order[level])
            if priority > 0:
                r['priority'] = priority
                l.append(r)
        l.sort(lambda x,y:cmp((x['priority'],x['published']), 
                              (y['priority'],y['published'])), reverse=True)
        return l
    def force_reload(self, state=None, delay=300):
        if state is not None:
            self._forceReload = state
        else:
            try: # check if timer has expired
                if time.time() > self._reloadTime:
                    return True # force reload
            except AttributeError: # start the timer
                self._reloadTime = time.time() + delay
        return getattr(self, '_forceReload', False)
                
                    

class ArxivPaperData(EmbeddedDocument):
    'store arxiv data for a paper as subdocument of Paper'
    _dbfield = 'arxiv.id'
    def __init__(self, fetchID=None, docData=None, parent=None,
                 insertNew=True):
        if fetchID:
            s = fetchID.lower()
            i = s.rfind('v')
            if i > 0 and fetchID[i + 1:].isdigit():
                fetchID = fetchID[:i] # remove version suffix
        EmbeddedDocument.__init__(self, fetchID, docData, parent, insertNew)
    def _query_external(self, arxivID):
        'obtain arxiv data from arxiv.org'
        import arxiv
        arxivID = arxivID.replace('_', '/')
        try:
            return arxiv.lookup_papers((arxivID,)).next()
        except StopIteration:
            raise KeyError('arxivID not found: ' + arxivID)
    parent = LinkDescriptor('parent', fetch_parent_paper, noData=True)
    def _insert_parent(self, d):
        'create Paper document in db for this arxiv.id'
        return Paper(docData=dict(title=d['title'],
                                  authorNames=d['authorNames']))
    def get_local_url(self):
        return '/arxiv/' + self.id
    def get_source_url(self):
        return 'http://arxiv.org/abs/' + self.id.replace('_', '/')
    def get_downloader_url(self):
        return 'http://arxiv.org/pdf/%s.pdf' % self.id.replace('_', '/')
    def get_hashtag(self):
        return '#arxiv_' + self.id.replace('.', '_').replace('-', '_')
    def get_doctag(self):
        return 'arXiv:' + self.id.replace('_', '/')
    def get_abstract(self, showLatex=False):
        if showLatex:
            return latex.convert_tex_dollars(self.summary)
        else:
            return self.summary

class PubmedPaperData(EmbeddedDocument):
    'store pubmed data for a paper as subdocument of Paper'
    _dbfield = 'pubmed.id'
    def _query_external(self, pubmedID):
        'obtain pubmed doc data from NCBI'
        import pubmed
        return pubmed.get_pubmed_dict(str(pubmedID))
    parent = LinkDescriptor('parent', fetch_parent_paper, noData=True)
    def _insert_parent(self, d):
        'create Paper document in db for this arxiv.id'
        try: # connect with DOI record
            DOI = d['doi']
            return DoiPaperData(DOI=DOI, insertNew='findOrInsert',
                                getPubmed=False).parent
        except KeyError: # no DOI, so save as usual
            return Paper(docData=dict(title=d['title'],
                                      authorNames=d['authorNames']))
    def get_local_url(self):
        return '/pubmed/' + self.id
    def get_source_url(self):
        return 'http://www.ncbi.nlm.nih.gov/pubmed/' + str(self.id)
    def get_downloader_url(self):
        try:
            return 'http://dx.doi.org/' + self.doi
        except AttributeError:
            return self.get_source_url()
    def get_hashtag(self):
        return '#pubmed_' + str(self.id)
    def get_doctag(self):
        return 'PMID:' + str(self.id)
    def get_abstract(self, **kwargs):
        return self.summary


class DoiPaperData(EmbeddedDocument):
    'store DOI data for a paper as subdocument of Paper'
    _dbfield = 'doi.id'
    def __init__(self, fetchID=None, docData=None, parent=None,
                 insertNew=True, DOI=None, getPubmed=False):
        '''Note the fetchID must be shortDOI; to search for DOI, pass
        DOI kwarg.
        docData, if provided should include keys: id=shortDOI, doi=DOI'''
        import doi
        self._getPubmed = getPubmed
        if fetchID is None and DOI: # must convert to shortDOI
            # to implement case-insensitive search, convert to uppercase
            d = self.coll.find_one({'doi.DOI':DOI.upper()}, {'doi':1})
            if d: # found DOI in our DB
                insertNew = False 
                docData = d['doi']
                self._parent_link = d['_id']
            else: # have to query shortdoi.org to get shortDOI
                fetchID = doi.map_to_shortdoi(DOI)
                self._DOI = DOI # cache this temporarily
        EmbeddedDocument.__init__(self, fetchID, docData, parent, insertNew)
    def _query_external(self, fetchID):
        'obtain doc data from crossref / NCBI'
        import doi
        try:
            DOI = self._DOI # use cached value
        except AttributeError: # retrieve from shortdoi.org
            DOI = doi.map_to_doi(fetchID)
        doiDict = doi.get_doi_dict(DOI)
        doiDict['id'] = fetchID # shortDOI
        doiDict['doi'] = DOI
        if self._getPubmed:
            try:
                pubmedDict = doi.get_pubmed_from_doi(DOI)
                if pubmedDict:
                    self._pubmedDict = pubmedDict # cache for saving to parent
            except errors.TimeoutError:
                pass
        return doiDict
    parent = LinkDescriptor('parent', fetch_parent_paper, noData=True)
    def _insert_parent(self, d):
        'create Paper document in db for this arxiv.id'
        d = dict(title=d['title'], authorNames=d['authorNames'])
        try:
            d['pubmed'] = self._pubmedDict # save associated pubmed data
        except AttributeError:
            pass
        return Paper(docData=d)
    def insert(self, d):
        d['DOI'] = d['doi'].upper() # for case-insensitive search
        return EmbeddedDocument.insert(self, d)
    def get_local_url(self):
        return '/shortDOI/' + self.id
    def get_source_url(self):
        try:
            return 'http://www.ncbi.nlm.nih.gov/pubmed/' + \
                   str(self.parent.pubmed.id)
        except AttributeError:
            return self.get_downloader_url()
    def get_downloader_url(self):
        return 'http://dx.doi.org/' + self.doi
    def get_hashtag(self):
        return '#shortDOI_' + str(self.id)
    def get_doctag(self):
        return 'shortDOI:' + str(self.id)
    def get_abstract(self, **kwargs):
        try:
            return self.summary
        except AttributeError:
            return 'Click <A HREF="%s">here</A> for the Abstract' \
                   % self.get_downloader_url()

class Paper(Document):
    '''interface to a specific paper '''
    # attrs that will only be fetched if accessed by user
    authors = LinkDescriptor('authors', fetch_people)
    references = LinkDescriptor('references', fetch_papers,
                                missingData=())
    issues = LinkDescriptor('issues', fetch_issues,
                            noData=True, missingData=())
    sigs = LinkDescriptor('sigs', fetch_sigs, missingData=())
    recommendations = LinkDescriptor('recommendations', fetch_recs,
                                     noData=True)

    # custom attr constructors
    _attrHandler = dict(
        posts=SaveAttrList(Post, insertNew=False),
        citations=SaveAttrList(Citation, insertNew=False),
        replies=SaveAttrList(Reply, insertNew=False),
        interests=SaveAttrList(PaperInterest, insertNew=False),
        arxiv=SaveAttr(ArxivPaperData, insertNew=False),
        pubmed=SaveAttr(PubmedPaperData, insertNew=False),
        doi=SaveAttr(DoiPaperData, insertNew=False),
        )
    _get_value_attrs = ('arxiv', 'pubmed', 'doi')
    def get_interests(self, people=None, sorted=False):
        'return dict of {topicID:[person,]}'
        d = {}
        for interest in getattr(self, 'interests', ()):
            personID = interest._dbDocDict['author']
            if people and personID not in people:
                continue # only include interests of these people
            p = Person(docData=dict(_id=personID, name=interest._dbDocDict.
                                    get('authorName', 'user')), 
                       insertNew=False) # dummy object only has name attr
            for topicID in interest._dbDocDict['topics']: # no db query!
                try:
                    d[topicID].append(p)
                except KeyError:
                    d[topicID] = [p]
        if sorted:
            return get_interests_sorted(d)
        return d
    def get_local_url(self):
        return '/paper/' + str(self._id)
    def get_all_posts(self, isRec=False):
        l = getattr(self, 'posts', [])
        if isRec is not None:
            l = filter(lambda p:p.is_rec() == isRec, l)
        for c in getattr(self, 'citations', ()):
            if isRec is None or (c.citationType in ('mustread', 'recommend')) \
                    == isRec:
                l.append(c.post)
        return l


class Tag(Document):
    'a specific keyword tag'
    pass
    ## def _new_fields(self):
    ##     'check that new tag is unique before inserting'
    ##     try:
    ##         name = self._dbDocDict['name']
    ##     except KeyError:
    ##         raise ValueError('new Tag has no name attribute!')
    ##     if list(self.__class__.find(dict(name=name))):
    ##         raise ValueError('Tag "%s" already exists!' % name)

# connect forward declarations to their target classes
fetch_paper.klass = Paper
fetch_parent_issue.klass = Issue
fetch_sig.klass = SIG
fetch_sigs.klass = SIG
fetch_person.klass = Person
fetch_post.klass = Post
fetch_papers.klass = Paper
fetch_people.klass = Person
fetch_parent_person.klass = Person
fetch_parent_paper.klass = Paper
fetch_author_papers.klass = Paper
fetch_subscribers.klass = Person
fetch_sig_members.klass = Person
fetch_sig_papers.klass = Paper
fetch_post_citations.klass = Citation
fetch_sig_posts.klass = Post
fetch_sig_interests.klass = PaperInterest
fetch_issues.klass = Issue
fetch_person_posts.klass = Post
fetch_person_replies.klass = Reply
fetch_person_interests.klass = PaperInterest
fetch_gplus_by_id.klass = GplusPersonData
fetch_gplus_subs.klass = GplusSubscriptions

##################################################################



########NEW FILE########
__FILENAME__ = dbclean
import core
import incoming

##############################################################
# utilities for converting old Paper.recommendations storage
# to new unified Paper.posts storage

# this conversion already performed, so this code probably not needed anymore

def delete_recs(q={'recommendations':{'$exists':True}}):
    'delete the old Paper.recommendations storage'
    core.Paper.coll.update(q, {'$unset': {'recommendations':''}}, multi=True)

def add_delivery_post_id():
    'add postID to each Person.received record'
    d = {} # construct mapping of (paperID,authorID) to postID
    for p in core.Post.find_obj():
        if p.is_rec():
            d[(p._parent_link,p._dbDocDict['author'])] = p.id
    for person in core.Person.find_obj({'received':{'$exists':True}}):
        received = person._dbDocDict['received']
        for r in received:
            r['post'] = d[(r['paper'],r['from'])] # add postID
        person.update(dict(received=received))

def add_post_citations(citationType2='discuss'):
    posts = list(core.Post.find_obj()) # get full list before changing db!
    for p in posts: # add citationType to existing posts
        content = p.get_text()
        hashtagDict = incoming.get_hashtag_dict(content) # extract tags and IDs
        papers = hashtagDict['paper']
        if len(papers) > 1:
            print 'multiple citations for %s: primary %s' \
                % (p.id, papers[0].get_value('local_url'))
            if papers[0] != p.parent:
                print 'fixing primary paper mismatch %s -> %s' \
                    % (p.parent.get_value('local_url'), 
                       papers[0].get_value('local_url'))
                data = p._dbDocDict
                p.delete() # delete Post record from p.parent paper
                p = core.Post(docData=data, parent=papers[0])
            for c in p.add_citations(papers[1:], citationType2):
                print '  added citation to %s' % c.parent.get_value('local_url')

def unified_posts():
    print 'deleting old rec records...'
    delete_recs()
    print 'updating deliveries received...'
    add_delivery_post_id()
    print 'adding multiple citations...'
    add_post_citations()
            
################################################################
# utilities for cleaning up / merging Paper records

def delete_papers(query={'arxiv.id': {'$regex':'error'}},
                  paperColl=core.Paper.coll, personColl=core.Person.coll):
    '''delete papers matching query from both paper collection
    and Person reading lists'''
    n = 0
    for d in paperColl.find(query, {'_id':1}):
        paperID = d['_id']
        personColl.update({'readingList': paperID}, 
                          {'$pull': {'readingList': paperID}}, 
                          multi=True) # delete from readingLists
        personColl.update({'received.paper': paperID}, 
                          {'$pull': {'received': {'paper': paperID}}}, 
                          multi=True) # update readingLists
        n += 1
    paperColl.remove(query) # delete papers
    print 'deleted %d papers.' % n

def check_papers_unique():
    '''Search for duplicate paper records with same arxiv.id
    (or pubmed.id, or doi.id).  '''
    d = {}
    for p in core.Paper.find_obj():
        try:
            pid = p.arxiv.id
        except AttributeError:
            try:
                pid = p.pubmed.id
            except AttributeError:
                pid = p.doi.id

        d.setdefault(pid, []).append(p)
    counts = {}
    for pid,papers in d.items():
        c = len(papers)
        if c > 1:
            counts.setdefault(c, []).append(pid)
    return counts, d

def get_index(p, attr, keyAttr='id'):
    d = {}
    for r in getattr(p, attr, ()):
        d[r._dbDocDict[keyAttr]] = r
    return d

def update_index(p, attr, d, keyAttr='id'):
    changed = 0
    for r in getattr(p, attr, ()):
        rid = getattr(r, keyAttr)
        if rid not in d or r.updated > d[rid].updated:
            d[rid] = r
            changed = 1
    return changed

def update_interests(p, attr, d):
    changed = 0
    for r in getattr(p, attr, ()):
        try:
            iset1 = set(d[r._dbDocDict['author']]._dbDocDict['topics'])
            iset2 = set(r._dbDocDict['topics'])
            if iset2.issubset(iset1):
                continue # no new topics, so nothing to save
            r._dbDocDict['topics'] = list(iset1 | iset2) # union of topics
        except KeyError:
            pass
        d[r._dbDocDict['author']] = r
        changed = 1
    return changed

def update_paper_array(p, attr, pleaseUpdate, docs, pid):
    if pleaseUpdate:
        data = [doc._dbDocDict for doc in docs.values()]
        p.update({attr: data})
        print 'unified %d %s on paper %s' % (len(docs), attr, pid)

def replace_paper(p, newID, savecoll=None, personColl=core.Person.coll):
    '''delete paper and update reading lists to repliace ir with newID'''
    personColl.update({'readingList': p._id}, 
                      {'$set': {'readingList.$': newID}}, 
                      multi=True) # update readingLists
    personColl.update({'received.paper': p._id}, 
                      {'$set': {'received.$.paper': newID}}, 
                      multi=True) # update readingLists
    if savecoll: # backup to another collection
        savecoll.insert(p._dbDocDict)
    p.delete() # delete from papers collection
    print 'deleted paper %s' % p._id

def merge_duplicate_papers(d, savecoll=None):
    '''Merge duplicate paper records found by check_papers_unique(),
    combining posts, replies, interests onto a unique paper
    record for a given arxiv.id (or pubmed.id or doi.id)
    and deleting duplicate records of that paper.
    If savecoll is not None, duplicate records are archived to
    that mongodb collection.

    Usage:
    >>> import core, dbclean, connect
    >>> dbconn = connect.init_connection()
    >>> counts, d = dbclean.check_papers_unique()
    >>> duplicate_papers = core.Paper.coll.database.duplicate_papers
    >>> dbclean.merge_duplicate_papers(d, duplicate_papers)
    '''
    for pid, papers in d.items():
        if len(papers) == 1:
            continue # no duplicates to merge
        p0 = papers[0]
        citations = get_index(p0, 'citations', 'post')
        posts = get_index(p0, 'posts')
        replies = get_index(p0, 'replies')
        interests = get_index(p0, 'interests', 'author')
        newCits = newPosts = newReplies = newInterests = 0
        for p in papers[1:]:
            newCits |= update_index(p, 'citations', citations, 'post')
            newPosts |= update_index(p, 'posts', posts)
            newReplies |= update_index(p, 'replies', replies)
            newInterests |= update_interests(p, 'interests', interests)
        update_paper_array(p0, 'citations', newCits, citations, pid)
        update_paper_array(p0, 'posts', newPosts, posts, pid)
        update_paper_array(p0, 'replies', newReplies, replies, pid)
        update_paper_array(p0, 'interests', newInterests, interests, pid)

        for p in papers[1:]: # finally, delete duplicate papers
            replace_paper(p, p0._id, savecoll)

########NEW FILE########
__FILENAME__ = dbconn
import pymongo


class DBConnection(object):
    'store different collection objects on specified classes'
    def __init__(self, classDict, user=None, password=None, **kwargs):
        '''Each keyword argument specifies an attr:collection pair.
        If collection is a string, it must be of the form dbname.collname.
        Otherwise it must be a collection object to be saved as-is.'''
        self._conn = pymongo.connection.Connection(**kwargs)
        if user == 'admin': # authenticating to admin DB gives access to all DB
            adminDB = self._conn['admin']
            adminDB.authenticate(user, password)
        for klass, v in classDict.items():
            if isinstance(v, str):
                db, coll = v.split('.')
                klass.coll = self._conn[db][coll]
            else:
                klass.coll = v

########NEW FILE########
__FILENAME__ = doi
import urllib
import requests
from lxml import etree
import unicodedata

def map_to_shortdoi(doi, uri='http://shortdoi.org/'):
    'find shortDOI for the specified DOI'
    s = urllib.urlencode(dict(a=doi))[2:]
    r = requests.get(uri + s, params=dict(format='json'))
    if r.status_code == 400:
        raise KeyError('DOI not found: ' + doi)
    return r.json()['ShortDOI'].split('/')[-1]


def decode_url_chars(s):
    'replace %hh by ASCII char, and %% by %'
    out = ''
    while s:
        try:
            i = s.index('%')
        except ValueError:
            return out + s
        out = out + s[:i]
        if s[i + 1] == '%':
            out = out + '%'
            s = s[i + 2:]
        else:
            out = out + chr(int(s[i + 1:i + 3], 16))
            s = s[i + 3:]
    return out
    

def map_to_doi(shortdoi, uri='http://doi.org/'):
    'find DOI for the specified shortDOI'
    r = requests.get(uri + shortdoi, allow_redirects=False)
    if r.status_code == 301:
        url = r.headers['location']
        i = url.index('doi.org/')
        return decode_url_chars(url[i + 8:]) # return the DOI
    elif r.status_code == 404:
        raise KeyError('shortdoi not found: ' + shortdoi)
    raise ValueError('unexpected status_code %d' % r.status_code)

def find_doi_metadata(doi, uri='http://www.crossref.org/openurl/', pid='leec@chem.ucla.edu', format='unixref', noredirect='true', **kwargs):
    params = dict(id=doi, pid=pid, format=format, noredirect=noredirect,
                  **kwargs)
    r = requests.get(uri, params=params)
    return r.content # lxml can't handle unicode encoded...

def safe_text(o, k):
    v = o.find(k)
    if v is not None:
        return v.text
    else:
        return ''

def doi_dict_from_xml(xml, title='title', year='publication_date.year',
                      volume='journal_volume.volume',
                      source_url='doi_data.resource', **kwargs):
    import pubmed
    d, root = pubmed.dict_from_xml(xml, title=title, year=year, volume=volume,
                                   source_url=source_url, **kwargs)
    authorNames = [] # extract list of author names
    for o in root.findall('.//person_name'):
        authorNames.append(safe_text(o, 'given_name') + ' ' +
                           safe_text(o, 'surname'))
    d['authorNames'] = authorNames
    return d

def extract_html_elements(html, minLength=100):
    'get text elements from HTML doc above specified size, biggest first'
    root = etree.HTML(html)
    l = [(len(e.text), e.text) for e in root.iterdescendants() if e.text
         and len(e.text) > minLength]
    l.sort(reverse=True)
    return l

# trick for counting non-letter/space characters in a string
textTrans = ''.join(['a' if chr(c).isalpha() or chr(c).isspace() else '_'
                     for c in range(256)])

def count_nonletterspace(t):
    'get count of chars that are not letter or space, len(t)'
    if isinstance(t, unicode): # force it to simple string
        t = unicodedata.normalize('NFKD', t).encode('ascii', 'ignore')
    return t.translate(textTrans).count('_'), float(len(t))


def find_abstract(uri, minLength=200, maxFrac=0.11):
    'abstract should have the lowest fraction of non-letter/space chars'
    r = requests.get(uri)
    l = extract_html_elements(r.content, minLength)
    m = []
    for c, t in l:
        f, total = count_nonletterspace(t) 
        m.append((f / total, t))
    m.sort()
    if m[0][0] <= maxFrac:
        return m[0][1]
    else:
        raise KeyError('(no abstract found)')

def get_doi_dict(doi, extractDicts=('doi_records.doi_record.crossref.journal.*'.split('.'),)):
    'get dict of crossref metadata for this DOI'
    import pubmed
    xml = find_doi_metadata(doi)
    doiDict = doi_dict_from_xml(xml)
    doiDict.update(pubmed.extract_subtrees(xml, extractDicts))
    return doiDict

def get_pubmed_from_doi(doi):
    'try to get pubmed dict for this DOI, or None if not found'
    import pubmed
    xml = pubmed.search_pubmed(doi, retmax='1', field='LID')
    try:
        d, root = pubmed.dict_from_xml(xml, pubmedID='!Id')
    except KeyError: # some DOI not properly indexed in pubmed?!?!
        xml = pubmed.search_pubmed(doi, retmax='1')
        try:
            d, root = pubmed.dict_from_xml(xml, pubmedID='!Id')
        except KeyError:
            pass
        else: # have to check whether title matches
            pubmedDict = pubmed.get_pubmed_dict(d['pubmedID'])
            if pubmedDict.get('title')[:50].lower() == \
               doiDict.get('title')[:50].lower():
                return pubmedDict
            
    else:
        return pubmed.get_pubmed_dict(d['pubmedID'])
        
    

        

        

########NEW FILE########
__FILENAME__ = errors

class BackendFailure(ValueError):
    pass

class UnexpectedStatus(BackendFailure):
    pass

class TimeoutError(BackendFailure):
    pass



########NEW FILE########
__FILENAME__ = gplus
import random
import urllib
import thread
import string
import json
import requests
from datetime import datetime, timedelta
import dateutil.parser
import dateutil.tz
from oauth2client.client import OAuth2Credentials, _extract_id_token
# supposed to work but doesn't... maybe in a newer version?
#from oauth2client import GOOGLE_AUTH_URI
#from oauth2client import GOOGLE_REVOKE_URI
#from oauth2client import GOOGLE_TOKEN_URI
import httplib2
from apiclient.discovery import build
import time
from incoming import find_or_insert_posts
import traceback

# Question: what's worse than an F+ ?
# Answer: this.

GOOGLE_AUTH_URI = 'https://accounts.google.com/o/oauth2/auth'
GOOGLE_REVOKE_URI = 'https://accounts.google.com/o/oauth2/revoke'
GOOGLE_TOKEN_URI = 'https://accounts.google.com/o/oauth2/token'

##################################################################
# timestamp mangling

def convert_timestamp(s):
    'convert string to UTC (tz-stripped) datetime'
    t = dateutil.parser.parse(s)
    t = t.astimezone(dateutil.tz.tzutc()) # force to UTC
    return t.replace(tzinfo=None) # strip to prevent stupid crashes
 
def convert_timestamps(d, fields=('published', 'updated')):
    'convert G+ timestamp string fields to datetime objects'
    for f in fields:
        try:
            s = d[f]
        except KeyError:
            pass
        else:
            d[f] = convert_timestamp(s)

def get_gplus_timestamp(d):
    'safe method for getting a timestamp from gplus data dict'
    try:
        return convert_timestamp(d['updated'])
    except KeyError:
        return datetime.utcnow()

####################################################################
# authentication interface to gplus

# we could just use Google's oauth2client.OAuth2WebServerFlow
# but for some reason it doesn't implement the state check
# protection against CSRF attack.  So I implemented simple code
# that does protect against that.  Oh, and activities().search()
# doesn't work at all yet -- won't take any arguments!

class OAuth(object):
    def __init__(self, auth_uri=GOOGLE_AUTH_URI,
                 token_uri=GOOGLE_TOKEN_URI,
                 revoke_uri=GOOGLE_REVOKE_URI,
                 scope='https://www.googleapis.com/auth/plus.login',
                 requestvisibleactions='http://schemas.google.com/AddActivity',
                 response_type='code', access_type='offline', user_agent=None,
                 keys=None, **kwargs):
        self.auth_uri = auth_uri
        self.token_uri = token_uri
        self.revoke_uri = revoke_uri
        self.user_agent = None
        self.state = ''.join([random.choice(string.ascii_uppercase + string.digits)
                              for x in xrange(32)])
        if not keys:
            keys = get_keys()
        self.keys = keys
        d = dict(state=self.state, redirect_uri=self.keys['redirect_uri'],
                 client_id=self.keys['client_id'], access_type=access_type,
                 response_type=response_type, scope=scope,
                 requestvisibleactions=requestvisibleactions)
        d.update(kwargs)
        self.login_url = auth_uri + '?' + urllib.urlencode(d)

    def get_authorize_url(self):
        'get URL to redirect user to sign-in with Google'
        return self.login_url
        
    def get_credentials(self, code, state, **kwargs):
        'use callback data to get an access token'
        if state != self.state:
            return False # CSRF attack?!?
        d = dict(code=code, client_id=self.keys['client_id'],
                 client_secret=self.keys['client_secret'],
                 redirect_uri=self.keys['redirect_uri'],
                 grant_type='authorization_code')
        r = requests.post(self.token_uri, data=d)
        self.access_data = ad = r.json()
        try:
            token_expiry = datetime.utcnow() \
                           + timedelta(seconds=int(ad['expires_in']))
        except KeyError:
            token_expiry = None
        if 'id_token' in ad:
            ad['id_token'] = _extract_id_token(ad['id_token'])
        c = OAuth2Credentials(ad['access_token'], d['client_id'],
                              d['client_secret'],
                              ad.get('refresh_token', None), token_expiry,
                              self.token_uri, self.user_agent,
                              id_token=ad.get('id_token', None))
        self.credentials = c
        return c

    def get_person(self):
        'get Person record (or create one) for authenticated user'
        import core
        http = httplib2.Http()
        self.http = http = self.credentials.authorize(http)
        self.service = service = build('plus', 'v1', http=http)
        person = service.people().get(userId='me').execute(http=http)
        gpd = core.GplusPersonData(docData=person, insertNew='findOrInsert')
        # retrieve circles, save subscriptions in background thread
        thread.start_new_thread(self.update_subscriptions, (gpd,))
        p = gpd.parent
        if 'refresh_token' in self.access_data:
            p.update(dict(gplusAccess=self.access_data))
        return p

    def update_subscriptions(self, gplusPersonData):
        '''when a new G+ user is added to Person, we need to retrieve
        their circles, save that in a GplusSubscriptions record,
        translate that to their Person.subscriptions record.
        This will typically be run in
        a separate thread when G+ user first signs in.
        Note we MUST have oauth sign-in as the user to run this.'''
        it = self.api_iter('people', userId='me', collection='visible',
                           getResponse=True)
        doc = it.next() # 1st item is the response document
        gplusPersonData.update_subscriptions(doc, it)

    # direct access to Google APIs
    # -- because Google's apiclient search is currently broken!!
    def request(self, uri, headers=None, **params):
        'perform API query using our access token or API key'
        if 'access_token' in getattr(self, 'access_data', ()):
            try:
                headers = headers.copy()
            except AttributeError:
                headers = {}
            headers['Authorization'] = 'Bearer ' + \
                                       self.access_data['access_token']
        else: # use simple API key
            params = params.copy()
            params['key'] = self.keys['apikey']
        r = requests.get(uri, params=params, headers=headers)
        return r.json()

    def request_iter(self, uri, results=None, **kwargs):
        'generate results from query using paging'
        params = kwargs.copy()
        if results is None: # perform the initial query
            results = self.request(uri, **params)
        while results.get('items', False):
            for item in results['items']:
                yield item
            try:
                params['pageToken'] = results['nextPageToken'] # get next page
            except KeyError:
                break
            results = self.request(uri, **params)

    def search_activities(self, uri='https://www.googleapis.com/plus/v1/activities', **kwargs):
        'like activities().search() only it WORKS'
        return self.request_iter(uri, **kwargs)

    def get_person_info(self, userID):
        'short cut to people().get(), does authentication for you'
        return self.request('https://www.googleapis.com/plus/v1/people/'
                            + str(userID))

    def get_person_posts(self, userID):
        'short cut to activities().list(), does authentication for you'
        return self.request_iter('https://www.googleapis.com/plus/v1/people/'
                                 + str(userID) + '/activities/public')

    def get_post_comments(self, postID):
        'short cut to comments().list(), does authentication for you'
        return self.request_iter('https://www.googleapis.com/plus/v1/activities/'
                                 + str(postID) + '/comments')

    def find_or_insert_posts(self, posts,
                             get_content=lambda x:x['object']['content'],
                             get_user=lambda x:x['actor']['id'],
                             get_replycount=lambda x:
                             x['object']['replies']['totalItems'],
                             get_id=lambda x:x['id'],
                             get_timestamp=get_gplus_timestamp,
                             is_reshare=lambda x: 
                             x['object']['objectType'] == 'activity' and
                             x['object'].get('id', False),
                             **kwargs):
        'save google+ posts to core.find_or_insert_posts()'
        import core
        return find_or_insert_posts(posts, self.get_post_comments,
                                    lambda x:core.GplusPersonData(x,
                               insertNew='findOrInsert').parent,
                                    get_content, get_user, get_replycount,
                                    get_id, get_timestamp, is_reshare, 'gplusPost',
                                    convert_timestamps, convert_timestamps,
                                    **kwargs)

    def api_iter(self, resourceName='activities', verb='list',
                 getResponse=False, **kwargs):
        'use Google apiclient to iterate over results from request'
        try:
            service = self.service
            http = self.http
        except AttributeError:
            service = build('plus', 'v1', developerKey=self.keys['apikey'])
            http = None
        rsrc = getattr(service, resourceName)()
        request = getattr(rsrc, verb)(**kwargs)
        while request is not None:
            doc = request.execute(http=http)
            if getResponse: # caller requested the response document
                getResponse = False # only yield doc as the first item
                yield doc
            for item in doc['items']:
                yield item
            request = getattr(rsrc, verb + '_next')(request, doc)

    def poll_recent_spnetwork(self, interval=300, *args, **kwargs):
        'query for recent #spnetwork traffic every interval seconds'
        self._continuePoll = True
        while self._continuePoll:
            n = 0
            try: # catch all errors
                for post in self.load_recent_spnetwork(*args, **kwargs):
                    n += 1
            except StandardError: # don't stop polling...
                traceback.print_exc() # please change this to real logging!
            self._pollReceived = n # count of posts screened
            time.sleep(interval)

    def halt_poll(self):
        'halt poll_recent_spnetwork()'
        self._continuePoll = False

    def start_poll(self, *args):
        'start polling in a background thread'
        thread.start_new_thread(self.poll_recent_spnetwork, args)        

    def load_recent_spnetwork(self, maxDays=10, recentEvents=None, **kwargs):
        'scan recent G+ posts for updates, and save updates to DB'
        postIt = self.search_activities(query='#spnetwork', orderBy='recent')
        return self.find_or_insert_posts(postIt, maxDays=maxDays,
                                         recentEvents=recentEvents, **kwargs)

##################################################################
# default clientID, API key etc. access

def get_keys(keyfile='../google/keys.json'):
    with open(keyfile, 'r') as ifile:
        keys = json.loads(ifile.read())
    return keys



publicAccess = OAuth() # gives API key based access (search public data)

if __name__ == '__main__':
    import connect
    dbconn = connect.init_connection()
    n = 0
    for post in publicAccess.load_recent_spnetwork():
        n += 1
    print 'received %d new posts' % n


########NEW FILE########
__FILENAME__ = incoming
import re
import core
import errors
from datetime import datetime
import bulk

#################################################################
# hashtag processors
def get_paper(ID, paperType):
    if paperType == 'arxiv':
        return core.ArxivPaperData(ID, insertNew='findOrInsert').parent
    elif paperType == 'pubmed':
        try: # eutils horribly unreliable, handle its failure gracefully
            return core.PubmedPaperData(ID, insertNew='findOrInsert').parent
        except errors.TimeoutError:
            raise KeyError('eutils timed out, unable to retrive pubmedID')
    elif paperType == 'DOI':
        return core.DoiPaperData(DOI=ID, insertNew='findOrInsert').parent
    elif paperType == 'shortDOI':
        return core.DoiPaperData(ID, insertNew='findOrInsert').parent
    else:
        raise Exception('Unrecognized paperType')


def hashtag_to_spnetID(s, subs=((re.compile('([a-z])_([a-z])'), r'\1-\2'),
                                (re.compile('([0-9])_([0-9])'), r'\1.\2'))):
    'convert 1234_5678 --> 1234.5678 and gr_qc_12345 --> gr-qc_12345'
    for pattern, replace in subs:
        s = pattern.sub(replace, s)
    return s

def get_hashtag_arxiv(m):
    arxivID = hashtag_to_spnetID(str(m))
    return arxivID

def get_arxiv_paper(m):
    arxivID = str(m).replace('/', '_')
    return arxivID

def get_hashtag_pubmed(m):
    pubmedID = str(m)
    return pubmedID

def get_hashtag_doi(m):
    shortDOI = str(m)
    return shortDOI

def get_doi_paper(m):
    DOI = m # look out, DOI can include any unicode character
    return DOI

#################################################################
# hashtag parsing
refPats = (
    (re.compile('#arxiv_([a-z0-9_]+)'), 'arxiv', get_hashtag_arxiv),
    (re.compile('ar[xX]iv:\s?[a-zA-Z.-]+/([0-9]+\.[0-9]+v?[0-9]+)'),
     'arxiv', get_arxiv_paper),
    (re.compile('ar[xX]iv:\s?([a-zA-Z.-]+/[0-9]+v?[0-9]+)'), 'arxiv',
     get_arxiv_paper),
    (re.compile('ar[xX]iv:\s?([0-9]+\.[0-9]+v?[0-9]+)'), 'arxiv',
     get_arxiv_paper),
    (re.compile('http://arxiv.org/[abspdf]{3}/[a-zA-Z.-]+/([0-9]+\.[0-9]+v?[0-9]+)'),
     'arxiv', get_arxiv_paper),
    (re.compile('http://arxiv.org/[abspdf]{3}/([a-zA-Z.-]+/[0-9]+v?[0-9]+)'),
     'arxiv', get_arxiv_paper),
    (re.compile('http://arxiv.org/[abspdf]{3}/([0-9]+\.[0-9]+v?[0-9]+)'),
     'arxiv', get_arxiv_paper),
    (re.compile('#pubmed_([0-9]+)'), 'pubmed', get_hashtag_pubmed),
    (re.compile('PMID:\s?([0-9]+)'), 'pubmed', get_hashtag_pubmed),
    (re.compile('#shortDOI_([a-zA-Z0-9]+)'), 'shortDOI', get_hashtag_doi),
    (re.compile('[dD][oO][iI]:\s?(10\.[a-zA-Z0-9._;()/-]+)'), 'DOI', get_doi_paper),
    (re.compile('shortDOI:\s?([a-zA-Z0-9]+)'), 'shortDOI', get_hashtag_doi)
    )

tagPattern = re.compile('#([a-zA-Z][a-zA-Z0-9_]+)')
citationTypes = ('recommend', 'discuss', 'announce', 'mustread')

def get_citations_types_and_topics(content, spnetworkOnly=True):
    """Process the body of a post.  Return
        - a dictionary with each entry of the form {reference: (refType, citationType}.  Here
          reference is a reference to a paper and citationType is one of
          {recommend discuss announce mustread}
          while refType is one of
          {arxiv pubmed DOI shortDOI}
        - and a list of topic tags

        Assumptions:
        - Each citationType or topic begins with a hash '#'
        - Only one citationType can be applied to each reference in a given post
        - Only one citationType can appear in each line; it will apply to each
          reference in that line
        - The citationType for each reference must appear in the same line as the reference
        - The following are considered (user) errors:
            - multiple citationTypes appear in a line with a reference; in this
              case, the first one will be used for all references
            - a citationType appears in a line with no citations
    """
    citations = {}
    topics = []

    # Split post by lines
    lines = content.split('\n') # Also need to handle HTML "line breaks"
    for line in lines:
        lineRefs = []
        # Find all citations in line
        for refpat, refType, patFun in refPats:
            for reference in refpat.findall(line):
                ref = patFun(reference)
                lineRefs.append( (ref, refType) )

        # Find topics and citationTypes in line
        tags = tagPattern.findall(line)
        tags = [t for t in tags if t != 'spnetwork']
        topicTags = [t for t in tags if t not in citationTypes]
        citationTags = [t for t in tags if t in citationTypes]
        topics.extend(topicTags)

        if len(citationTags)>0:
            citeType = citationTags[0]
        else:
            citeType = 'discuss'

        # Store references with citation types and reference types
        for ref in lineRefs:
            cite = ref[0]
            refType = ref[1]
            if not (cite in citations.keys()):
                citations[cite] = (citeType, refType)
            elif citations[cite][0]=='discuss':
                citations[cite] = (citeType, refType)

    # Remove duplicates
    topics = list(set(topics))

    # Now find #spnetwork and get first reference after it
    try:
        spTagLoc = re.compile('#spnetwork').search(content).start()
    except AttributeError: # no spnetwork tag in this string
        if spnetworkOnly:
            raise Exception('No #spnetwork tag in post')
        else: # Take first reference as primary
            spTagLoc = 0
    remainder = content[spTagLoc:]

    refs = [refPat.search(remainder) for refPat, _, _ in refPats]
    # If no references after #spnetwork, take first ref in content
    if refs.count(None) == len(refs):
        refs = [refPat.search(content) for refPat, _, _ in refPats]
        # If no refences at all, then return primary = None
        if refs.count(None) == len(refs):
            return citations, topics, None

    refs = [ref for ref in refs if ref is not None]
    locations = [ref.start() for ref in refs]

    firstRef = refs[locations.index(min(locations))]
    primary = patFun(firstRef.group(1))

    return citations, topics, primary


#################################################################
# process post content looking for #spnetwork tags

# replace this by class that queries for ignore=1 topics just once,
# keeps cache
def screen_topics(topicWords, skipAttr='ignore', **kwargs):
    'return list of topic object, filtered by the skipAttr attribute'
    l = []
    for t in topicWords:
        topic = core.SIG.find_or_insert(t, **kwargs)
        if not getattr(topic, skipAttr, False):
            l.append(topic)
    return l



def get_topicIDs(topics, docID, timestamp, source):
    """return list of topic IDs for a post, saving to db if needed

        Input variable topics should be a list of strings."""
    topics = screen_topics(topics, origin=dict(source=source, id=docID),
                           published=timestamp)
    return [t._id for t in topics] # IDs for storing to db, etc.


def find_or_insert_posts(posts, get_post_comments, find_or_insert_person,
                         get_content, get_user, get_replycount,
                         get_id, get_timestamp, is_reshare, source,
                         process_post=None, process_reply=None,
                         recentEvents=None, maxDays=None,
                         citationType='discuss', citationType2='discuss',
                         get_title=lambda x:x['title'],
                         spnetworkOnly=True):
    'generate each post that has a paper hashtag, adding to DB if needed'
    now = datetime.utcnow()
    saveEvents = []
    for d in posts:
        post = None
        timeStamp = get_timestamp(d)
        if maxDays is not None and (now - timeStamp).days > maxDays:
            break
        if is_reshare(d): # just a duplicate (reshared) post, so skip
            continue
        content = get_content(d)
        try:
            post = core.Post(get_id(d))
            if getattr(post, 'etag', None) == d.get('etag', ''):
                yield post
                continue # matches DB record, so nothing to do
        except KeyError:
            pass
        if spnetworkOnly and content.find('#spnetwork') < 0:
            if post:
                post.delete() # remove old Post: no longer tagged!
            continue # ignore posts lacking our spnetwork hashtag
        # extract tags and IDs:
        citations, topics, primary = get_citations_types_and_topics(content)
        try:
            primary_paper_ID = citations[primary]
            paper = get_paper(primary,primary_paper_ID[1])
        except KeyError:
            continue # no link to a paper, so nothing to save.
        if post and post.parent != paper: # changed primary binding!
            post.delete() # delete old binding
            post = None # must resave to new binding
        d['text'] =  content
        if process_post:
            process_post(d)
        d['sigs'] = get_topicIDs(topics, get_id(d),timeStamp, source)
        d['citationType'] = citations[primary][0]
        oldCitations = {}
        if post is None: # save to DB
            userID = get_user(d)
            author = find_or_insert_person(userID)
            d['author'] = author._id
            post = core.Post(docData=d, parent=paper)
            try:
                topicsDict
            except NameError:
                topicsDict, subsDict = bulk.get_people_subs()
            bulk.deliver_rec(paper._id, d, topicsDict, subsDict)
            if recentEvents is not None: # add to monitor deque
                saveEvents.append(post)
        else: # update DB with new data and etag
            post.update(d)
            for c in getattr(post, 'citations', ()): # index old citations
                oldCitations[c.parent] = c
        for ref, meta in citations.iteritems(): # add / update new citations
            if ref != primary:
                paper2 = get_paper(ref, meta[1])
                try: # if already present, just update citationType if changed
                    c = oldCitations[paper2]
                    if c.citationType != meta[0]:
                        c.update(dict(citationType=meta[0]))
                    del oldCitations[paper2] # don't treat as old citation
                except KeyError:
                    post.add_citations([paper2], meta[0])
        for c in oldCitations.values():
            c.delete() # delete citations no longer present in updated post
        yield post
        if get_replycount(d) > 0:
            for c in get_post_comments(get_id(d)):
                if process_reply:
                    process_reply(c)
                try:
                    r = core.Reply(get_id(c))
                    if getattr(r, 'etag', None) != c.get('etag', ''):
                        # update DB record with latest data
                        r.update(dict(etag=c.get('etag', ''),
                                      text=get_content(c),
                                      updated=c.get('updated', '')))
                    continue # already stored in DB, no need to save
                except KeyError:
                    pass
                userID = get_user(c)
                author = find_or_insert_person(userID)
                c['author'] = author._id
                c['text'] =  get_content(c)
                c['replyTo'] = get_id(d)
                r = core.Reply(docData=c, parent=post._parent_link)
                if recentEvents is not None: # add to monitor deque
                    saveEvents.append(r)

    if saveEvents and recentEvents is not None:
        saveEvents.sort(lambda x,y:cmp(x.published, y.published))
        for r in saveEvents:
            recentEvents.appendleft(r) # add to monitor deque


########NEW FILE########
__FILENAME__ = keeprunning
import subprocess

while True:
    print 'Starting server...'
    subprocess.call(['python', 'watchmem.py'])


########NEW FILE########
__FILENAME__ = latex
import re

def convert_tex_dollars(t):
    'do our best to convert $inlinemath$ to \(inlinemath\)'
    i = 0
    l = []
    n = len(t)
    while True:
        j = t.find('$', i)
        if j < 0:
            break
        elif j > 0 and t[j - 1] == '\\': # ignore escaped $
            continue
        if len(l) % 2: # looking for close
            if not t[j - 1].isspace(): # must be preceded by non-ws character
                l.append(j)
        elif j + 1 < n and not t[j + 1].isspace() and t[j + 1] not in '.,:;':
            l.append(j) # open must be followed by non-ws
        i = j + 1
    if len(l) % 2:
        return t + ' (TeX $ CONVERSION FAILED: unbalanced $)'
    out = []
    last = 0
    for i in range(0, len(l), 2):
        start, stop = l[i:i + 2]
        out.append(t[last:start])
        out.append('\\(' + t[start + 1:stop] + '\\)')
        last = stop + 1
    out.append(t[last:])
    return ''.join(out)

########NEW FILE########
__FILENAME__ = poll_gplus
import time
import subprocess

# just polls Google+ for latest updates every 5 minutes

while True:
    print 'Starting poll...'
    subprocess.call(['python', 'gplus.py'])
    time.sleep(300)


########NEW FILE########
__FILENAME__ = pubmed
from lxml import etree
import xmltodict
import requests
import time
import errors


def bfs_search_results(d, searches, results):
    '''find all keys in searches, and save values to results.
    WARNING: deletes keys from searches dict!'''
    try:
        l = d.items()
    except AttributeError:
        return
    for k, v in l:
        try:
            f = searches[k] # found a match
        except KeyError:
            continue
        if callable(f): # let f generate whatever results it wants
            try:
                r = f(v, k, results)
            except KeyError: # didn't find what it wanted, so ignore
                pass
            else:
                results.update(r)
        elif f: # save with alias key
            results[f] = v
        else: # save with original key
            results[k] = v
        del searches[k]
        if not searches: # nothing more to do
            return
    for k, v in l:
        bfs_search_results(v, searches, results)
        if not searches: # nothing more to do
            return

def bfs_search(d, searches):
    'find all keys in searches, and save values to results.'
    results = {}
    bfs_search_results(d, searches.copy(), results)
    return results

def get_val(v, k, r):
    return {k:v['#text']}

def get_author_names(v, k, r):
    return {'authorNames':[d['ForeName'] + ' ' + d['LastName']
                           for d in list_wrap(v)]}

def get_abstract(v, k, r):
    'properly handle multi-part AbstractText'
    if isinstance(v, list):
        l = [('%s: %s' % (d.get('@Label', ''), d['#text'])) for d in v]
        return {'summary':'  '.join(l)}
    else:
        return {'summary':v}

def list_wrap(v):
    if isinstance(v, list):
        return v
    else:
        return [v]

def get_doi(v, k, r):
    for d in list_wrap(v):
        if d.get('@IdType', '') == 'doi':
            return {'doi':d['#text']}
    raise KeyError('no doi found')

pubmedExtracts = dict(ArticleTitle='title', AbstractText=get_abstract,
                      PMID=lambda v,k,r:{'id':v['#text']},
                      ArticleDate=lambda v,k,r:{'year':v['Year']},
                      ISOAbbreviation='journal', ISSN=None,
                      Affiliation='affiliation', Author=get_author_names,
                      ArticleId=get_doi)

def normalize_pubmed_dict(d, extracts=pubmedExtracts):
    return bfs_search(d, extracts)


def extract_subtrees(xml, extractDicts):
    'extract subtrees as OrderedDict objects'
    d = {}
    doc = xmltodict.parse(xml)
    for xd in extractDicts: # extract desired subtrees
        dd = doc
        required = False
        for k in xd:
            if k.startswith('!'):
                required = True
                k = k[1:]
            if k == '*': # copy all items in dd to d
                d.update(dd)
                k = None # nothing further to save
                break
            try: # go one level deeper in doc
                dd = dd[k]
            except (TypeError,KeyError):
                if required:
                    raise KeyError('required subtree missing')
                k = None # nothing to save
                break
        if k: # save to result dictionary
            d[k] = dd
    return d

def dict_from_xml(xml, **kwargs):
    root = etree.XML(xml)
    d = {}
    for k,v in kwargs.items():
        if v is None:
            continue
        if v[0] == '!': # required field
            required = True
            v = v[1:]
        else:
            required = False
        f = v.split('.')
        o = root.find('.//' + f[0]) # search for top field
        for subfield in f[1:]: # contains subfield
            if o is None:
                break
            o = o.find(subfield)
        if o is not None:
            d[k] = o.text
        elif required:
            raise KeyError('required field not found: ' + v)
    return d, root

def pubmed_dict_from_xml(xml, title='!ArticleTitle',
                         summary='AbstractText', id='!PMID',
                         year='ArticleDate.Year', journal='ISOAbbreviation',
                         ISSN='ISSN', affiliation='Affiliation',
                         extractDicts=('!PubmedArticleSet.PubmedArticle.MedlineCitation'.split('.'),),
                         **kwargs):
    'extract fields + authorNames + DOI from xml, return as dict'
    d, root = dict_from_xml(xml, title=title, summary=summary, id=id,
                            year=year, journal=journal, ISSN=ISSN,
                            affiliation=affiliation, **kwargs)
    d.update(extract_subtrees(xml, extractDicts))
    authorNames = [] # extract list of author names
    for o in root.findall('.//Author'):
        authorNames.append(o.find('ForeName').text + ' ' +
                           o.find('LastName').text)
    d['authorNames'] = authorNames
    for o in root.findall('.//ELocationID'): # extract DOI
        if o.get('EIdType', '') == 'doi':
            d['doi'] = o.text
    if 'doi' not in d:
        for o in root.findall('.//ArticleId'): # extract DOI
            if o.get('IdType', '') == 'doi':
                d['doi'] = o.text
    return d

def query_pubmed(uri='http://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi',
                 tool='spnet', email='leec@chem.ucla.edu', db='pubmed',
                 retmode='xml', nretry=2, acceptStatus=(200,),
                 retryStatus=(404, 502, 503), retryTime=1, **kwargs):
    '''perform a query using eutils API, retrying if service unavailable
    WARNING: eutils is flaky!  lots of 503s, 502, 404 (when searching
    for "cancer"!?!), who knows what else!'''
    d = kwargs.copy()
    if tool:
        d['tool'] = tool
    if email:
        d['email'] = email
    params = dict(db=db, retmode=retmode, **d)
    for i in range(nretry): # often service unavailable (503), so retry
        r = requests.get(uri, params=params)
        if r.status_code in acceptStatus: # success
            return r.content
        elif r.status_code == 400: # badly formed query
            raise KeyError('invalid query: ' + str(kwargs))
        elif r.status_code not in retryStatus: # unexpected status
            raise errors.UnexpectedStatus('query_pubmed: status=%d, query=%s'
                                   % (r.status_code, str(d)))
        if retryTime:
            time.sleep(retryTime)
    raise errors.TimeoutError('%s still unavailable after %d retries'
                              % (uri, nretry))

def search_pubmed(term, uri='http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi',
                  **kwargs):
    return query_pubmed(uri, term=term, **kwargs)
    

def query_pubmed_id(pubmedID, **kwargs):
    return query_pubmed(id=pubmedID, **kwargs)

def get_pubmed_dict(pubmedID):
    'get paper data for specified pubmed ID, from NCBI eutils API'
    xml = query_pubmed_id(pubmedID)
    return pubmed_dict_from_xml(xml)

def get_training_abstracts(terms=('cancer', 'transcription', 'evolution',
                                  'physics', 'statistics', 'review'),
                           **kwargs):
    'generate a training set of 20 abstracts per search term'
    for t in terms:
        xml = search_pubmed(t, usehistory='y', tool=None, email=None,
                            **kwargs)
        d, root = dict_from_xml(xml, WebEnv='!WebEnv', query_key='!QueryKey')
        xml = query_pubmed(retstart='0', retmax='20', tool=None, email=None,
                           **d)
        root = etree.XML(xml)
        for o in root.findall('.//AbstractText'):
            yield o.text

class PubmedSearch(object):
    def __init__(self, searchString, block_size=20,
                 email='leec@chem.ucla.edu', **kwargs):
        self.block_size =  block_size
        self.email = email
        xml = search_pubmed(searchString, usehistory='y', tool='spnet',
                            email=email, retmax=str(block_size), **kwargs)
        d, root = dict_from_xml(xml, WebEnv='!WebEnv', query_key='!QueryKey')
        self.queryArgs = d
    def __call__(self, searchString, start=0, block_size=20):
        'get list of PubmedArticle dicts'
        xml = query_pubmed(retstart=str(start), retmax=str(block_size),
                           tool='spnet', email=self.email, **self.queryArgs)
        d = extract_subtrees(xml, ('PubmedArticleSet.PubmedArticle'.split('.'),))
        l = []
        for dd in d['PubmedArticle']:
            dd.update(normalize_pubmed_dict(dd))
            l.append(dd)
        return l


########NEW FILE########
__FILENAME__ = rest
import cherrypy
import glob
import os.path
from base import IdString
import view

def request_tuple():
    mimeType = 'html'
    try:
        accept = cherrypy.request.headers['Accept']
        if 'application/json' in accept or accept == '*/*':
            mimeType = 'json'
    except KeyError:
        pass
    return cherrypy.request.method, mimeType

class Response(object):
    '_GET etc. methods can return this to pass back HTML output'
    def __init__(self, content):
        self.content = content
    def __call__(self):
        return self.content

class Redirect(Response):
    '_GET etc. methods can return this to force redirection to a URL'
    def __call__(self):
        return view.redirect(self.content)


class Collection(object):
    '''subclass this by adding the following kinds of methods:

    1. HTTP verbs, e.g. GET, POST, DELETE, as follows
    _POST(self, docID, **kwargs): create the specified document.
    _search(self, **kwargs): search the collection based on kwargs.

    2. representation generators for a specific verb and mimeType, e.g.
    get_html(self, doc, **kwargs): for a GET request,
    return HTML representation of the doc object.
    This will typically be a renderer of a Jinja2 template.
    '''
    def __init__(self, name, klass, templateEnv=None, templateDir='_templates',
                 docArgs=None, collectionArgs=None, **templateArgs):
        self.name = name
        self.klass = klass
        if docArgs is None:
            docArgs = {}
        self.docArgs = docArgs
        self.collectionArgs = collectionArgs
        if templateEnv: # load our template files
            self.bind_templates(templateEnv, templateDir, **templateArgs)

    def default(self, docID=None, *args, **kwargs):
        'process all requests for this collection'
        try:
            method, mimeType = request_tuple()
            if docID: # a specific document from this collection
                docID = IdString(docID) # implements proper cmp() vs. ObjectId
                invalidResponse = self.check_permission(method, docID, *args, 
                                                        **kwargs)
                if invalidResponse:
                    return invalidResponse
                if not args: # perform the request
                    return self._request(method, mimeType, docID, **kwargs)
                else: # pass request on to subcollection
                    try:
                        subcoll = getattr(self, args[0])
                    except AttributeError:
                        return view.report_error('no such subcollection: %s.%s'
                                                 % (self.name, args[0]), 404)
                    try:
                        parents = kwargs['parents'].copy()
                    except KeyError:
                        parents = {}
                    try:
                        parents[self.name] = self._GET(docID, parents=parents)
                    except KeyError:
                        return view.report_error('invalid ID: %s' % docID, 404,
                                                 """Sorry, the data ID %s that
    you requested does not exist in the database.
    Please check whether you have the correct ID.""" % docID)
                    kwargs['parents'] = parents # pass dict of parents
                    return subcoll.default(*args[1:], **kwargs)
            elif method == 'GET': # search the collection
                return self._request('search', mimeType, **kwargs)
            else:
                return view.report_error('REST does not permit collection-%s' 
                                         % method, 405)
        except Exception:
            return view.report_error('REST collection error', 500)
    default.exposed = True

    def _request(self, method, mimeType, *args, **kwargs):
        'dispatch to proper handler method, or return appropriate error'
        try: # do we support this method?
            action = getattr(self, '_' + method)
        except AttributeError:
            return view.report_error('%s objects do not allow %s' 
                                     % (self.name, method), 405)
        try: # execute the request
            o = action(*args, **kwargs)
        except KeyError:
            return view.report_error('Not found: %s: args=%s, kwargs=%s'
                   % (self.name, str(args), str(kwargs)), status=404,
                                     webMsg="""Sorry, the data that
you requested does not exist in the database.
Please check whether you have the correct ID or spelling.""")
        if isinstance(o, Response):
            return o() # send the redirect
        try: # do we support this mimeType?
            viewFunc = getattr(self, method.lower() + '_' + mimeType)
        except AttributeError:
            return view.report_error('%s objects cannot return %s' 
                                     % (self.name, mimeType), 406)
        try:
            return viewFunc(o, **kwargs)
        except Exception:
            return view.report_error('view function error', 500)

    def _GET(self, docID, parents={}, **kwargs):
        'default GET method'
        kwargs.update(self.docArgs)
        if not parents: # works with documents with unique ID
            return self.klass(docID, **kwargs)
        elif len(parents) == 1: # works with ArrayDocument
            return self.klass((parents.values()[0]._id, docID), **kwargs)
        else: # multiple parents
            return self.klass(docID, parents=parents, **kwargs)
            
    def bind_templates(self, env, dirpath, **kwargs):
        '''load template files of the form get_paper.html, bind as
        attrs of the form get_html'''
        for fname in glob.glob(os.path.join(dirpath,
                                            '*_%s.html' % self.name)):
            basename = os.path.basename(fname)
            template = env.get_template(basename)
            methodName = basename.split('_')[0] + '_html'
            v = view.TemplateView(template, self.name, **kwargs)
            setattr(self, methodName, v)
            
    def check_permission(self, method, docID, *args, **kwargs):
        'this authentication stub allows all requests'
        return False

########NEW FILE########
__FILENAME__ = sessioninfo
import cherrypy

class SessionInfo(object):
    def __call__(self):
        try:
            return cherrypy.session
        except AttributeError:
            if hasattr(self, 'sessionDict'):
                return self.sessionDict
            raise

get_session = SessionInfo()

########NEW FILE########
__FILENAME__ = test
import core, connect
import gplus
##import pubmed
##import pickle
import doi
from bson import ObjectId
import apptree
import incoming
from datetime import datetime
import time
import sessioninfo
import bulk

def destroy_db_and_test():
    '''tests progressively building an spnet db starting from a blank
    slate, adding papers, people, posts, topics, etc. and verifying
    the expected results.  NB: this is a destructive test, i.e. 
    it FLUSHES whatever is in the spnet database and fills it with
    its own test data.'''
    dbconn = connect.init_connection()
    dbconn._conn.drop_database('spnet') # start test from a blank slate
    rootColl = apptree.get_collections()

    lorem = '''Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'''

    jojo = core.Person(docData=dict(name='jojo', age=37))
    assert jojo != None
    assert jojo.force_reload(delay=1) is False # set timer
    assert jojo.force_reload() is False # timer still waiting
    time.sleep(2)
    assert jojo.force_reload() # timer done

    a1 = core.EmailAddress(docData=dict(address='jojo@nowhere.edu', current=True),
                           parent=jojo)
    fred = core.Person(docData=dict(name='fred', age=56))
    a2 = core.EmailAddress(docData=dict(address='fred@dotzler.com',
                                        authenticated=False), parent=fred)
    a3 = core.EmailAddress(docData=dict(address='fred@gmail.com',
                                        note='personal account'), parent=fred)

    paper1 = core.ArxivPaperData('1302.4871', insertNew='findOrInsert').parent
    paper1.update(dict(authors=[jojo._id]))
    paper2 = core.ArxivPaperData('1205.6541', insertNew='findOrInsert').parent
    paper2.update(dict(authors=[fred._id, jojo._id]))

    assert paper1.arxiv.id == '1302.4871'
    assert paper2.arxiv.id == '1205.6541'

    jojoGplus = core.GplusPersonData(docData=dict(id=1234, displayName='Joseph Nye', image={'url':'http://www.nobelprize.org/nobel_prizes/physics/laureates/1921/einstein.jpg'}),
                                    parent=jojo)
    jojoGplus.update(dict(etag='oldversion'))

    sig1 = core.SIG.find_or_insert('cosmology')
    sig2 = core.SIG.find_or_insert('lambdaCDMmodel')

    topicWords = incoming.get_topicIDs(['cosmology', 'astrophysics'],
                                       1, datetime.utcnow(), 'test')
    assert topicWords == ['cosmology', 'astrophysics']
    astroSIG = core.SIG('astrophysics')
    assert astroSIG.name == '#astrophysics'
    assert astroSIG.origin == dict(source='test', id=1)


    int1 = core.PaperInterest(docData=dict(author=jojo._id, topics=[sig1._id]),
                              parent=paper1)
    assert core.Paper(paper1._id).interests == [int1]
    assert core.Paper(paper1._id).get_interests() == {sig1._id:[jojo]}
    assert core.Person(jojo._id).interests == [int1]
    assert core.Person(jojo._id).topics == [sig1._id]
    assert core.SIG(sig1._id).interests == [int1]
    assert core.SIG(sig1._id).get_interests() == {paper1:[jojo]}

    intAgain = core.PaperInterest((paper1._id, jojo._id))
    assert intAgain == int1
    try:
        intAgain.remove_topic(sig2._id)
    except KeyError:
        pass
    else:
        raise AssertionError('failed to catch bad remove_topic()')
    assert intAgain.remove_topic(sig1._id) is None
    assert core.Paper(paper1._id).interests == []

    # test creation via POST
    paperLikes = rootColl['papers'].likes
    sessioninfo.get_session.sessionDict = dict(person=fred)
    int2 = paperLikes._POST(fred._id, sig2._id, '1',
                            parents=dict(paper=paper2))
    assert int2.parent == paper2
    assert int2.author == fred
    assert int2.topics == [sig2]
    assert core.Paper(paper2._id).interests == [int2]
    assert core.Person(fred._id).interests == [int2]
    assert core.Person(fred._id).topics == [sig2._id]
    assert core.SIG(sig2._id).interests == [int2]
    try:
        paperLikes._POST(fred._id, 'this is not allowed', '1',
                         parents=dict(paper=paper2))
    except KeyError:
        pass
    else:
        raise AssertionError('failed to trap bad topic string')
    # test removal via POST
    assert paperLikes._POST(fred._id, sig2._id, '0',
                            parents=dict(paper=core.Paper(paper2._id))) == int2
    assert core.Paper(paper2._id).interests == []

    int3 = paperLikes._POST(fred._id, '#silicene', '1',
                            parents=dict(paper=paper2))
    assert core.SIG('silicene').interests == [int3]
    assert set(core.Person(fred._id).topics) == set([sig2._id, 'silicene'])


    gplus2 = core.GplusPersonData(docData=dict(id=1234, displayName='Joseph Nye'),
                                  insertNew='findOrInsert')

    assert gplus2 == jojoGplus

    gplus3 = core.GplusPersonData(docData=dict(id=5678, displayName='Fred Eiserling'),
                                  insertNew='findOrInsert')
    assert gplus3.parent.name == 'Fred Eiserling'

    rec1 = core.Post(docData=dict(author=fred._id, citationType='recommend', id='1',
                                  title='Why You Need to Read This Important Extension of the CDM Model',
                                  text=lorem),
                               parent=paper1)
    rec2 = core.Post(docData=dict(author=jojo._id, text='must read!',
                                  citationType='mustread', id='2',
                                  sigs=[sig1._id, sig2._id]),
                               parent=paper2._id)
    assert set(core.Person(jojo._id).topics) == set([sig1._id, sig2._id])

    post1 = core.Post(docData=dict(author=fred._id, text='interesting paper!',
                                   id=98765, sigs=[sig1._id]), parent=paper1)
    assert set(core.Person(fred._id).topics) == set([sig1._id, sig2._id, 'silicene'])
    reply1 = core.Reply(docData=dict(author=jojo._id, text='I disagree with Fred.',
                                     id=7890, replyTo=98765), parent=paper1)

    issue1 = core.Issue(docData=dict(paper=paper1, title='The claims are garbage',
                                     category='validity', author=jojo._id,
                                     description='there is a major flaw in the first step of your proof'))

    vote1 = core.IssueVote(docData=dict(person=jojo, rating='crucial',
                                        status='open'),
                           parent=issue1)

    assert core.Person(jojo._id).email == [a1]
    assert core.Person(jojo._id).replies == [reply1]
    jgp = core.GplusPersonData(1234)
    assert jgp.parent == jojo
    assert jgp.etag == 'oldversion'
    assert len(rec1.parent.authors) == 1
    assert rec1.parent.authors[0] == jojo
    assert len(rec2.parent.authors) == 2
    assert jojo in rec2.parent.authors
    assert fred in rec2.parent.authors
    assert len(rec2.parent.recommendations) == 1
    assert len(jojo.recommendations) == 1
    assert jojo.recommendations[0] == rec2
    assert len(jojo.papers) == 2
    assert len(fred.papers) == 1
    assert len(paper2.authors[0].email) == 2
    assert issue1.author == jojo
    p = core.Paper(paper1._id)
    assert len(p.issues) == 1
    posts1 = p.get_all_posts()
    assert len(posts1) == 1
    assert posts1 == [post1]
    assert posts1[0].text == 'interesting paper!'
    assert list(posts1[0].get_replies()) == [reply1]
    assert core.Post(98765).author == fred
    assert core.Reply(7890).replyTo == post1
    assert core.Reply(7890).parent == paper1
    assert filter(lambda x:not x.is_rec(), core.Person(fred._id).posts) == [post1]
    assert filter(lambda x:not x.is_rec(), core.SIG(sig1._id).posts) == [post1]
    assert core.Post(98765).sigs == [sig1]

    replyAgain = core.Reply(docData=dict(author=fred._id, text='interesting paper!',
                                     id=7890, replyTo=98765), parent=paper1,
                            insertNew='findOrInsert')
    assert replyAgain == reply1
    assert core.Paper(paper1._id).replies == [reply1]

    reply2 = core.Reply(docData=dict(author=jojo._id, text='This paper really made me think.',
                                     id=7891, replyTo=98765), parent=paper1,
                            insertNew='findOrInsert')

    assert core.Paper(paper1._id).replies == [reply1, reply2]
    assert core.Paper(str(paper1._id)) == paper1, 'auto ID conversion failed'

    assert p.issues[0] == issue1
    assert len(p.issues[0].votes) == 1
    assert len(rec2.sigs) == 2
    assert rec2.sigs[0] == sig1
    assert sig1.recommendations == [rec2]

    rec1.array_append('sigs', sig2)

    assert len(sig2.recommendations) == 2
    assert core.Post(rec1.id).sigs == [sig2]

    rec2.update(dict(text='totally fascinating!', score=27))
    rec3 = core.Post(rec2.id)
    assert rec3.score == 27

    a4 = core.EmailAddress('fred@dotzler.com')
    assert a4._parent_link == fred._id
    assert a4.parent == fred

    try:
        p = core.Person('abcdefg')
    except KeyError:
        pass
    else:
        raise AssertionError('failed to trap bad personID')

    try:
        a = core.EmailAddress('bob@yoyo.com')
    except KeyError:
        pass
    else:
        raise AssertionError('failed to trap bad email')

    try:
        jojo = core.Person(docData=dict(name2='jojo', age=37))
    except ValueError:
        pass
    else:
        raise AssertionError('failed to trap Person w/o name')

    fred.array_append('numbers', 17)
    assert core.Person(fred._id).numbers == [17]
    fred.array_append('numbers', 6)
    assert core.Person(fred._id).numbers == [17, 6]
    fred.array_del('numbers', 17)
    assert core.Person(fred._id).numbers == [6]

    a4.array_append('numbers', 17)
    assert core.EmailAddress(a4.address).numbers == [17]
    a4.array_append('numbers', 6)
    assert core.EmailAddress(a4.address).numbers == [17, 6]
    a4.array_del('numbers', 17)
    assert core.EmailAddress(a4.address).numbers == [6]

    rec3 = core.Post(docData=dict(author=fred._id, citationType='recommend',
                                  text='I think this is a major breakthrough.',
                                  sigs=[sig2._id], id=3456),
                               parent=paper2._id)

    assert core.SIG(sig1._id).recommendations == [rec2]
    assert len(core.SIG(sig2._id).recommendations) == 3

    it = gplus.publicAccess.get_person_posts('107295654786633294692')
    testPosts = list(gplus.publicAccess.find_or_insert_posts(it))
    assert len(testPosts) > 0

    nposts = len(core.Paper(paper1._id).posts)
    nreplies = len(core.Paper(paper1._id).replies)
    it = gplus.publicAccess.get_person_posts('107295654786633294692')
    testPosts2 = list(gplus.publicAccess.find_or_insert_posts(it))
    assert testPosts == testPosts2
    assert nposts == len(core.Paper(paper1._id).posts)
    assert nreplies == len(core.Paper(paper1._id).replies)

    gpd = core.GplusPersonData('112634568601116338347',
                               insertNew='findOrInsert')
    assert gpd.displayName == 'Meenakshi Roy'
    gpd.update_subscriptions(dict(etag='foo', totalItems=1),
                             [dict(id='114744049040264263224')])
    gps = gpd.subscriptions
    assert gps.gplusPerson == gpd
    mrID = gpd.parent._id
    subscriptions = core.Person(mrID).subscriptions
    assert len(subscriptions) == 0

    gpd2 = core.GplusPersonData('114744049040264263224',
                                insertNew='findOrInsert')
    time.sleep(2)
    subscriptions = core.Person(mrID).subscriptions
    assert len(subscriptions) == 1
    assert subscriptions[0].author == gpd2.parent

    cjlposts = gpd2.update_posts(999) # retrieve some recs
    assert len(cjlposts) > 0 # got some
    assert len(core.Person(mrID).received) > 0 # and they were delivered
    assert len(core.Person(mrID).get_deliveries()) > 0 # and UI can retrieve them

    recReply = core.Reply(docData=dict(author=jojo._id, id=78901, replyTo=3456,
                          text='Fred, thanks for your comments!  Your insights are really helpful.'),
                          parent=paper2._id)

    # make sure timestamps present on all recs
    l = [r.published for r in core.Post.find_obj()]
    l = [r.published for r in core.Reply.find_obj()]

    assert recReply.replyTo == rec3
    assert list(recReply.replyTo.get_replies()) == [recReply]

    # pubmed eutils network server constantly failing now??
    ## pubmedDict = pubmed.get_pubmed_dict('23482246')
    ## with open('../pubmed/test1.pickle') as ifile:
    ##     correctDict = pickle.load(ifile)
    ## assert pubmedDict == correctDict

    ## paper3 = core.PubmedPaperData('23482246', insertNew='findOrInsert').parent
    ## paper3.update(dict(authors=[fred._id]))

    ## ppd = core.PubmedPaperData('23139441', insertNew='findOrInsert')
    ## assert ppd.doi.upper() == '10.1016/J.MSEC.2012.05.020'

    ## assert paper3.pubmed.id == '23482246'
    ## assert paper3.title[:40] == correctDict['title'][:40]

    s = 'aabbe'
    t = doi.map_to_doi(s)
    assert t == '10.1002/(SICI)1097-0258(19980815/30)17:15/16<1661::AID-SIM968>3.0.CO;2-2'
    assert s == doi.map_to_shortdoi(t)

    paper4 = core.DoiPaperData(DOI=t, insertNew='findOrInsert').parent
    paper4.update(dict(authors=[fred._id]))
    assert paper4.doi.id == s
    assert paper4.doi.doi == t
    assert paper4.doi.DOI == t.upper()
    paper5 = core.DoiPaperData(s, insertNew='findOrInsert').parent
    assert paper4 == paper5
    assert rootColl['shortDOI']._GET(s) == paper4
    txt = 'some text ' + paper4.doi.get_hashtag()
    refs, topics, primary = incoming.get_citations_types_and_topics(txt,spnetworkOnly=False)
    assert incoming.get_paper(primary,refs[primary][1]) == paper4

    spnetPaper = core.DoiPaperData(DOI='10.3389/fncom.2012.00001',
                                   insertNew='findOrInsert').parent
    assert spnetPaper.title.lower() == 'open peer review by a selected-papers network'
    txt = 'a long comment ' + spnetPaper.doi.get_doctag() + ', some more text'
    refs, topics, primary = incoming.get_citations_types_and_topics(txt,spnetworkOnly=False)
    assert incoming.get_paper(primary,refs[primary][1]) == spnetPaper

    topics, subs = bulk.get_people_subs()
    bulk.deliver_recs(topics, subs)
    assert len(core.Person(jojo._id).received) == 4
    assert len(core.Person(fred._id).received) == 2


########NEW FILE########
__FILENAME__ = test_incoming
import connect
import incoming
import core
from datetime import datetime

def setup():
    'get a db connection prior to running tests'
    connect.init_connection()


post1 = dict(
    id='temporaryPost',
    content='''this is my post.
It talks about three papers including  arXiv:0910.4103
and http://arxiv.org/abs/1310.2239
#spnetwork arXiv:0804.2682 #gt  arXiv:0910.4103 arXiv:0804.2682 #gt''',
    user='114744049040264263224',
    etag='old data',
    title='Such an interesting Post!',
)

def submit_posts(posts):
    it = incoming.find_or_insert_posts([d.copy() for d in posts], None, 
        lambda x:core.GplusPersonData(x, insertNew='findOrInsert').parent,
        lambda x:x['content'], lambda x:x['user'], lambda x:0,
        lambda x:x['id'], lambda x:datetime.now(),
        lambda x:False, 'gplusPost')
    return list(it)

def test_multiple_citations(d=post1, citationType='discuss'):
    'add post with multiple citations, redundancy, ordering issues'
    l = submit_posts([d]) # create test post
    assert len(l) == 1
    post = l[0]
    try:
        assert post.parent.arxiv.id == '0804.2682'
        assert post.citationType == citationType
        assert len(post.citations) == 2
        ids = [c.parent.arxiv.id for c in post.citations]
        assert '0910.4103' in ids and '1310.2239' in ids
        assert len(post.citations[0].parent.citations) == 1
        assert post.citations[0].title == 'Such an interesting Post!'
        assert len(post.citations[1].parent.citations) == 1
    finally: # clean up by deleting our test post
        post.delete()
    # check that citations to this post were deleted
    assert len(core.ArxivPaperData('0910.4103').parent.citations) == 0
    assert len(core.ArxivPaperData('1310.2239').parent.citations) == 0

def test_text_content(t='''
#discuss arXiv:0910.4103
#recommend arXiv:0804.2682
#announce http://arxiv.org/abs/1310.2239
#spnetwork #gt
'''):
    'check basic citationType binding'
    l = check_parse(t, '0910.4103', 'arxiv', 'discuss', ['gt'])
    refs = l[0]
    assert refs['0804.2682'] == ('recommend', 'arxiv')
    assert refs['1310.2239'] == ('announce', 'arxiv')

def test_html_content(t='''
<A HREF="http://some.url.com/some/path1">#discuss</A> arXiv:0910.4103
<A HREF="http://some.url.com/some/path2">#recommend</A> arXiv:0804.2682
<A HREF="http://some.url.com/some/path3">#announce</A> http://arxiv.org/abs/1310.2239
<A HREF="http://some.url.com/some/path4">#spnetwork</A> <A HREF="http://some.url.com/some/path5">#gt</A>
'''):
    'check that HTML tags do not break citationType binding'
    test_text_content(t)

def test_post_update(newText='update #spnetwork arXiv:0804.2682 #cat'):
    'check that etag value will force updating'
    submit_posts([post1])
    try:
        d = post1.copy()
        d['content'] = newText
        submit_posts([d])
        p = core.Post(d['id']) # retrieve from DB
        assert p.get_text() == post1['content'] # no update b/c etag unchanged!
        d = post1.copy()
        d['content'] = newText
        d['etag'] = 'new and improved'
        submit_posts([d])
        p = core.Post(d['id']) # retrieve from DB
        assert p.etag == 'new and improved'
        assert p.get_text() == newText
    finally:
        core.Post(post1['id']).delete()

def test_paper_update(t1='update #spnetwork arXiv:0804.2682 arXiv:0910.4103 #cat',
                      t2='update #spnetwork arXiv:1310.2239 arXiv:1302.4871 #cat'):
    'check updating of primary paper and citation binding'
    args = (('0804.2682', 'posts'), ('0910.4103', 'citations'),
            ('1310.2239', 'posts'), ('1302.4871', 'citations'))
    b = count_posts(*args) # get initial state of these papers
    d = post1.copy()
    d['content'] = t1
    submit_posts([d])
    try:
        p = core.Post(d['id']) # retrieve from DB
        paper1 = p.parent
        assert paper1.arxiv.id == '0804.2682'
        b2 = count_posts(*args)
        assert b2 == [b[0] + 1, b[1] + 1] + b[2:] # should bind first 2 papers
        assert paper1.posts == [p]
        d = post1.copy()
        d['content'] = t2
        d['etag'] = 'new and improved'
        submit_posts([d])
        b2 = count_posts(*args)
        assert b2 == b[:2] + [b[2] + 1, b[3] + 1] # should bind last 2 papers
        p2 = core.Post(d['id']) # retrieve from DB
        paper2 = p2.parent
        assert core.Paper(paper1._id).posts == []
        assert paper2.arxiv.id == '1310.2239'
        assert paper2.posts == [p2]
    finally: # cleanup so no effect on other tests
        core.Post(d['id']).delete()
    assert count_posts(*args) == b # returned to original state


def test_bad_tag():
    'check if #recommended crashes incoming'
    postDict = dict(
        id='temporaryPost',
        content='''this is my post.
    It talks about three papers including  arXiv:0910.4103
    and http://arxiv.org/abs/1310.2239
    #spnetwork #recommended arXiv:0804.2682 #gt  arXiv:0910.4103 arXiv:0804.2682 #gt''',
        user='114744049040264263224',
        title='Such an interesting Post!',
    )
    test_multiple_citations(postDict)

def test_simple_text():
    'check simple doi rec parsing'
    t = 'this is text #spnetwork #recommend doi: 10.3389/fncom.2012.00001 i like doi: this #cosmology'
    check_parse(t, '10.3389/fncom.2012.00001', 'DOI', 'recommend', 
                ['cosmology'])

def test_pair_text():
    'check pair of paper refs'
    t = 'this is text #spnetwork #recommend arXiv:1302.4871 PMID: 22291635 #cosmology'
    l = check_parse(t, '1302.4871', 'arxiv', 'recommend', ['cosmology'])
    refs = l[0]
    assert refs['22291635'] == ('recommend', 'pubmed')
    assert refs['1302.4871'] == ('recommend', 'arxiv')

def test_arxiv_href():
    'check handling of arxiv URL in HTML <A HREF>'
    content = '''
I want to discuss <A HREF="http://arxiv.org/abs/0906.0213">this paper</A>
#spnetwork
'''
    check_parse(content)

def test_no_spnetwork():
    'check proper handling of post lacking #spnetwork tag'
    d = post1.copy()
    d['content'] = '''
I want to discuss <A HREF="http://arxiv.org/abs/0906.0213">this paper</A>
'''
    l = submit_posts([d, post1])
    try:
        assert len(l) == 1
    finally:
        for p in l:
            p.delete()

def count_posts(*arxivIDs):
    return [len(getattr(core.ArxivPaperData(a, insertNew='findOrInsert')
                        .parent, attr, ())) 
            for (a,attr) in arxivIDs]

def test_no_spnetwork_update():
    'check proper updating of post that removes #spnetwork tag'
    args = (('0906.0213', 'posts'), ('1302.4871', 'citations')) # papers to check
    b = count_posts(*args) # get initial state of these papers
    d = post1.copy()
    d['content'] = '''
I want to discuss <A HREF="http://arxiv.org/abs/0906.0213">this paper</A>
and this paper arxiv:1302.4871
#spnetwork
'''
    l = submit_posts([d])
    try:
        assert len(l) == 1
        b2 = count_posts(*args)
        assert b2 == [b[0] + 1, b[1] + 1] # papers now bound to this post
        d = post1.copy()
        d['content'] = '''
I want to discuss <A HREF="http://arxiv.org/abs/0906.0213">this paper</A>
and this paper arxiv:1302.4871
'''
        d['etag'] = 'new and improved'
        submit_posts([d])
        b2 = count_posts(*args)
        assert b2 == b # returned to original state
    finally: # cleanup: delete this post if it still exists
        try:
            p = core.Post(d['id'])
        except KeyError:
            pass
        else:
            p.delete()
    

def check_parse(t, primaryID='0906.0213', primaryType='arxiv', 
                primaryRole='discuss', tags=[]):
    refs, topics, primary = incoming.get_citations_types_and_topics(t)
    assert set(topics) == set(tags)
    assert primary == primaryID
    assert refs[primary][1] == primaryType
    assert refs[primary][0] == primaryRole
    return refs, topics, primary
    

def test_arxiv_versions(arxivIDs=('1108.1172', '1108.1172v3')):
    'check if versioned arxiv IDs create duplicate records'
    l = [core.ArxivPaperData(s, insertNew='findOrInsert').parent
         for s in arxivIDs]
    for p in l[1:]:
        if p._id != l[0]._id:
            raise AssertionError('arxiv versions map to different Paper records!')

def test_arxiv_versions2(arxivIDs=('math.HO_9404236', 'math.HO_9404236')):
    'check if math.HO/9404236 style arxiv IDs create duplicate records'
    test_arxiv_versions(arxivIDs)

def check_paper_pair(args1, args2):
    p1 = incoming.get_paper(*args1)
    p2 = incoming.get_paper(*args2)
    assert p1 == p2
    assert p1._id == p2._id

def test_doi_vs_pubmed():
    'check that pubmed and DOI map to identical paper record'
    check_paper_pair(('22291635', 'pubmed'), 
                     ('10.3389/fncom.2012.00001', 'DOI'))

def test_doi_finding():
    'check handling of DOI terminated by HTML <tag>'
    t = '''Interesting work on zero-determinant strategies following last year&#39;s paper by Press and Dyson doi:10.1073/pnas.1206569109<br /><br />Stewart and Plotkin describe a subclass of zero-determinant strategies, called generous zero-determinant strategies, that can invade a population of other zero-determinant strategies, are evolutionarily stable, and in some cases can dominate the traditional &quot;best strategy&quot; for the iterated prisoner&#39;s dilemma, win-stay-lose-shift!<br /><br /><a class="ot-hashtag" href="https://plus.google.com/s/%23spnetwork">#spnetwork</a> <a class="ot-hashtag" href="https://plus.google.com/s/%23game_theory">#game_theory</a> <a class="ot-hashtag" href="https://plus.google.com/s/%23evolutionary_game_theory">#evolutionary_game_theory</a> <a class="ot-hashtag" href="https://plus.google.com/s/%23zero_determinent_strategies">#zero_determinent_strategies</a> arXiv:1304.7205'''
    refs, topics, primary = check_parse(t, '1304.7205', 
                                        tags=['game_theory', 
                                              'evolutionary_game_theory',
                                              'zero_determinent_strategies'])
    assert '10.1073/pnas.1206569109' in refs
    assert refs['10.1073/pnas.1206569109'] == ('discuss', 'DOI')


########NEW FILE########
__FILENAME__ = test_load
import core
import connect
import pickle
import random
from bson.objectid import ObjectId

cache_filename = 'arxiv.pickle'

def drop_paper_db():
    'empty the paper database table'
    core.Paper.coll.database.drop_collection(core.Paper.coll)
def drop_person_db():
    'empty the person database table'
    core.Person.coll.database.drop_collection(core.Person.coll)

def load_papers(papers):
    'insert the set of paper dictionaries into the database'
    n = 0
    for paper in papers:
        paper['_id'] = paperID = 'arxiv:' + paper['id'].split('/')[-1]
        if not list(core.Paper.find(dict(_id=paperID), limit=1)):
            p = core.Paper(docData=paper) # creates new record in DB
            n += 1
    return n

def update_person_db():
    'use Paper database authors to construct Person db'
    authors = {}
    for d in core.Person.find(fields=dict(name=1)):
        authors[d['name']] = d['_id']
    papersToUpdate = []
    n = 0
    for paper in core.Paper.find(fields=dict(authors=1), idOnly=False):
        if isinstance(paper['authors'][0], ObjectId):
            continue # already saved as Person records
        paperID = paper['_id']
        papersToUpdate.append(paperID)
        for a in paper['authors']:
            if a not in authors: # create new person record
                p = core.Person(docData=dict(name=a))
                authors[a] = p._id
                n += 1
    print 'Saved %d new author records...' % n
    print 'Updating %d paper records...' % len(papersToUpdate)
    for paperID in papersToUpdate:
        paper = core.Paper(paperID)
        authorIDs = [authors[a] for a in paper._dbDocDict['authors']]
        paper.update(dict(authors=authorIDs))
    return n

def add_random_recs():
    'for each person, add one rec to a randomly selected paper'
    papers = list(core.Paper.find()) # get all paperID
    for personID in core.Person.find():
        paperID = random.choice(papers)
        core.Recommendation(docData=dict(author=personID,
                                         text='I like this paper'),
                            parent=paperID)

def new_email(email, name):
    'add an email address to the specified name'
    l = list(core.Person.find(dict(name=name)))
    if l:
        return core.EmailAddress(docData=dict(address=email), parent=l[0])
    else:
        raise ValueError('Person "%s" not found' % name)

def set_password(password, name):
    'set password on the specified name'
    l = list(core.Person.find_obj(dict(name=name)))
    if l:
        l[0].set_password(password)
    else:
        raise ValueError('Person "%s" not found' % name)

def add_dummy_login(email='me@u.edu', password='testme', query={}):
    for person in core.Person.find_obj(query):
        print 'Adding login for %s: %s, %s' % (person.name, email, password)
        person.set_password(password)
        return core.EmailAddress(docData=dict(address=email), parent=person)
    raise ValueError('No person matching query: ' + str(query))

def add_paper_sigs(field='arxiv-topic-area'):
    'add SIGs based on paper field strings, and tag papers accordingly'
    sigs = {}
    n = 0
    ntag = 0
    for sig in core.SIG.find_obj():
        sigs[sig.name] = sig
    for paper in core.Paper.find_obj():
        try:
            name = paper._dbDocDict[field]
        except KeyError:
            continue # paper has no field name, so nothing to do 
        try:
            sig = sigs[name]
        except KeyError:
            sig = core.SIG(docData=dict(name=name))
            sigs[name] = sig
            n += 1
        if sig not in paper.sigs:
            paper.array_append('sigs', sig)
            ntag += 1
    return n, ntag


if __name__ == '__main__':
    import sys
    dbconn = connect.init_connection()
    if len(sys.argv) > 1 and sys.argv[1] == '--drop':
        dbconn._conn.drop_database('spnet')
        print 'Erased existing spnet database.'
    with open(cache_filename) as cache_file:
        papers = pickle.load(cache_file)
    print 'Loading %d papers...' % len(papers)
    n = load_papers(papers)
    print 'Loaded %d new papers.' % n
    n = update_person_db()
    if n:
        add_dummy_login()
        print 'Adding random recommendations...'
        add_random_recs()
        n, ntag = add_paper_sigs()
        print 'Added %d SIGs, tagged %d papers.' % (n, ntag)

########NEW FILE########
__FILENAME__ = twitter
try:
    import tweepy
except ImportError:
    pass
import urllib2
#import core

###############################################################
# OAuth functions

def read_keyfile(keyfile='../twitter/keys'):
    'read our application keys for twitter'
    d = {}
    with open(keyfile, 'rU') as ifile:
        for line in ifile:
            line = line.strip()
            t = line.split('\t')
            d[t[0]] = t[1]
    return d

def get_auth(callback=None, keyDict=read_keyfile(), **kwargs):
    'create an authentication object, using optional kwargs'
    d = keyDict.copy()
    d.update(kwargs) # allow kwargs to override stored settings
    auth = tweepy.OAuthHandler(d['consumer_key'], d['consumer_secret'],
                               callback)
    if d.get('access_token', False):
        auth.set_access_token(d['access_token'], d['access_token_secret'])
    return auth

def start_oauth(callback_url):
    'get URL for user to login to twitter'
    auth = get_auth(callback=callback_url, access_token=None)
    redirect_url = auth.get_authorization_url()
    return redirect_url, (auth.request_token.key, auth.request_token.secret)

def complete_oauth(request_token, request_token_secret, oauth_verifier):
    'get access token using info passed on by twitter'
    auth = get_auth(access_token=None)
    auth.set_request_token(request_token, request_token_secret)
    auth.get_access_token(oauth_verifier)
    return auth

######################################################################
# search functions

def get_recent(query, api=None, **kwargs):
    if api is None:
        api = tweepy.API(get_auth())
    for tweet in tweepy.Cursor(api.search, q=query, count=100,
                               result_type='recent',
                               include_entities=True, **kwargs).items():
        yield tweet


class HeadRequest(urllib2.Request):
    def get_method(self): return 'HEAD'
 
def get_real_url(url):
    res = urllib2.urlopen(HeadRequest(url))
    return res.geturl()

def extract_arxiv_id(tweet, arxivFormats=('abs', 'pdf', 'other')):
    'get arxiv ID from URLs in tweet'
    for u in tweet.entities.get('urls', ()):
        if u.get('expanded_url', '') == 'http://arxiv.org':
            continue
        try:
            url = get_real_url(u['url'])
        except KeyError:
            continue
        l = url.split('/')
        if url.startswith('http://arxiv.org/') and len(l) > 4 and \
               l[3] in arxivFormats:
            arxivID = l[4].split('v')[0]
            yield arxivID
    
def extract_pubmed_id(tweet):
    'get PMID from both #pubmed12345 hashtags and URLs'
    for hdict in tweet.entities.get('hashtags', ()):
        hashtag = hdict.get('text', '')
        if hashtag.startswith('pubmed') and hashtag[6:].isdigit():
            yield hashtag[6:]
    for u in tweet.entities.get('urls', ()):
        if u.get('expanded_url', '') == 'http://www.ncbi.nlm.nih.gov':
            continue
        try:
            url = get_real_url(u['url'])
            print u['url'], '-->', url
        except KeyError:
            continue
        l = url.split('/')
        if url.startswith('http://www.ncbi.nlm.nih.gov/'):
            for i,word in enumerate(l[3:-1]):
                if word == 'pubmed':
                    pubmedID = l[i + 4]
                    yield pubmedID
                    break

####################################################################
# user functions

def get_auth_user(auth, api):
    username = auth.get_username()
    return api.get_user(username)
    
def get_person(u, access_token):
    try:
        p = core.Person.find_obj({'twitter.id_str':u.id_str}).next()
    except StopIteration: # no matching record
        twitterData = dict(id_str=u.id_str,screen_name=u.screen_name,
                           created_at=u.created_at,
                           access_token=access_token.key,
                           access_secret=access_token.secret)
        p = core.Person(docData=dict(name=u.name, twitter=twitterData))
    return p

def get_auth_person(auth):
    'get Person record (or create one) for authenticated user'
    api = tweepy.API(auth)
    user = get_auth_user(auth, api)
    p = get_person(user, auth.access_token)
    return p, user, api

    

    

########NEW FILE########
__FILENAME__ = view
import cherrypy
from jinja2 import Environment, FileSystemLoader
import urllib
from datetime import datetime, timedelta
import collections
from sessioninfo import get_session
import webui

def redirect(path='/', body=None, delay=0):
    'redirect browser, if desired after showing a message'
    s = '<HTML><HEAD>\n'
    s += '<meta http-equiv="Refresh" content="%d; url=%s">\n' % (delay, path)
    s += '</HEAD>\n'
    if body:
        s += '<BODY>%s</BODY>\n' % body
    s += '</HTML>\n'
    return s

def people_link_list(people, maxNames=2):
    l = []
    for p in people[:maxNames]:
        l.append('<A HREF="%s">%s</A>' % (p.get_local_url(), p.name))
    s = ', '.join(l)
    if len(people) > maxNames:
        s += ' and %d others' % (len(people) - maxNames)
    return s

timeUnits = (('seconds', timedelta(minutes=1), lambda t:int(t.seconds)),
             ('minutes', timedelta(hours=1), lambda t:int(t.seconds / 60)),
             ('hours', timedelta(1), lambda t:int(t.seconds / 3600)),
             ('days', timedelta(7), lambda t:t.days))

monthStrings = ('Jan.', 'Feb.', 'Mar.', 'Apr.', 'May', 'Jun.', 'Jul.',
                'Aug.', 'Sep.', 'Oct.', 'Nov.', 'Dec.')

def display_datetime(dt):
    'get string that sidesteps timezone issues thus: 27 minutes ago'
    def singularize(i, s):
        if i == 1:
            return s[:-1]
        return s
    diff = datetime.utcnow() - dt
    for unit, td, f in timeUnits:
        if diff < td:
            n = f(diff)
            return '%d %s ago' % (n, singularize(n, unit))
    return '%s %d, %d' % (monthStrings[dt.month - 1], dt.day, dt.year)

def timesort(stuff, cmpfunc=lambda x,y:cmp(x.published,y.published),
             reverse=True, **kwargs):
    'sort items by timestamp, most recent first by default'
    l = list(stuff)
    l.sort(cmpfunc, reverse=reverse, **kwargs)
    return l

def map_helper(it, attr=None, **kwargs):
    '''jinja2 lacks list-comprehensions, map, lambda, etc... so we need
    some help with those kind of operations'''
    if attr:
        f = lambda x: getattr(x, attr)
    return map(f, it)

################################################################
# error reporting template

class ErrorPage(object):
    def __call__(self, logMsg='Trapped exception', status=404,
                 webMsg="""Drat!  Something went wrong during
the rendering of this page.  The error has been logged, to aid debugging.
If this problem is inconveniencing you, please add your information
on how to reproduce this error, on 
<A HREF="https://github.com/cjlee112/spnet/issues">our issue tracker</A>.
That will accelerate efforts to track down and squish
this bug.""", traceback=True):
        'log traceback if desired, set status code'
        cherrypy.log.error(logMsg, traceback=traceback)
        cherrypy.response.status = status
        try:
            tv = self.templateView
        except AttributeError:
            return webMsg # no template, so just return error string as is
        else: # apply our template to the error message
            return tv(webMsg)
    def bind_template(self, env, templateName, name='errorMessage'):
        template = env.get_template(templateName)
        self.templateView = TemplateView(template, name)

report_error = ErrorPage() # our standard error reporting function

#################################################################
# template loading and rendering

def get_template_env(dirpath):
    loader = FileSystemLoader(dirpath)
    return Environment(loader=loader)

class TemplateView(object):
    exposed = True
    def __init__(self, template, name=None, **kwargs):
        self.template = template
        self.kwargs = kwargs
        self.name = name

    def __call__(self, doc=None, **kwargs):
        f = self.template.render
        kwargs.update(self.kwargs)
        session = get_session()
        try:
            kwargs.update(session['viewArgs'])
        except KeyError:
            pass
        if doc is not None:
            kwargs[self.name] = doc
        try:
            user = session['person']
        except KeyError:
            user = session['person'] = None
        if user and user.force_reload():
            user = user.__class__(user._id) # reload from DB
            session['person'] = user # save on session
        return f(kwargs=kwargs, hasattr=hasattr, enumerate=enumerate,
                 urlencode=urllib.urlencode, list_people=people_link_list,
                 getattr=getattr, str=str, map=map_helper, user=user,
                 display_datetime=display_datetime, timesort=timesort,
                 recentEvents=recentEventsDeque, len=len,
                 messageOfTheDay=messageOfTheDay,
                 Selection=webui.Selection, **kwargs) # apply template

def get_view_options():
    'get dict of session kwargs passed to view templates'
    try:
        return get_session()['viewArgs']
    except KeyError:
        d = {}
        get_session()['viewArgs'] = d
        return d

##################################################################

class MultiplePages(object):
    'Interface for paging through result sets'
    def __init__(self, f, block_size, ipage, uri, title='Search Results', 
                 **queryArgs):
        self.f = f
        self.pages = []
        self.block_size = block_size
        self.queryArgs = queryArgs
        self.uri = uri
        self.title = title
        self.results = () # default: no results
        self.get_page(ipage, uri, **queryArgs)
    def get_page(self, ipage, uri, **queryArgs):
        '''returns True if we can serve the specified query;
        otherwise False.  Raises StopIteration'''
        if self.uri != uri or queryArgs != self.queryArgs:
            return False # a different search!
        self.error = '' # default: no error
        while ipage >= len(self.pages):
            l = self.f(start=ipage * self.block_size,
                       block_size=self.block_size, **self.queryArgs)
            if l:
                self.pages.append(l)
            if len(l) < self.block_size:
                self.totalPages = len(self.pages)
                if not self.pages:
                    self.error = 'No results matched your query.'
                    return True # report these search results
                if not l:
                    self.error = 'There are no more results matching your query.'
                ipage = len(self.pages) - 1 # last page
                break
        self.ipage = ipage
        self.start = ipage * self.block_size + 1
        self.results = self.pages[ipage]
        self.end = self.start + len(self.results) - 1
        return True # report these search results
    def get_page_url(self, step=1):
        'get URL for page incremented by step'
        return self.uri + '?' + \
               urllib.urlencode(dict(ipage=self.ipage + step,
                                     **self.queryArgs))
    def get_doc_data(self, docID, uri=None):
        'return docData dict for specified ID and collection URI'
        if uri and uri != self.uri:
            raise KeyError('request from different URI: ' + uri)
        return self.f.get_doc_data(docID)

class SimpleObj(object):
    'wrapper looks like a Paper object, for storing search results'
    def __init__(self, docData, **kwargs):
        self.__dict__.update(docData)
        self.__dict__.update(kwargs)
        self.parent = self
    def get_value(self, val='spnet_url'):
        f = getattr(self, 'get_' + val)
        return f()
    def get_local_url(self):
        return self.uri + '/' + self.id

class PaperBlockLoader(object):
    'callable that loads one list of dicts into paper objects'
    def __init__(self, f, klass=SimpleObj, **kwargs):
        '''wraps function f so its results [d,...] are returned as
        [klass(docData=d, **kwargs),...]'''
        self.f = f
        self.klass = klass
        self.kwargs = kwargs
        self.docs = {}
    def __call__(self, **kwargs):
        l = []
        for d in self.f(**kwargs):
            l.append(self.klass(docData=d, **self.kwargs).parent)
            try:
                docID = d['id']
                self.docs[docID] = d
            except KeyError:
                pass
        return l
    def get_doc_data(self, docID):
        'return docData dict for specified ID'
        return self.docs[docID]


#######################################################################
# recent events deque

recentEventsDeque = collections.deque(maxlen=20)

def load_recent_events(paperClass, topicClass, dq=recentEventsDeque,
                       limit=20):
    'obtain list of recent events stored in our database'
    l = []
    for paper in paperClass.find_obj(sortKeys={'posts.published':-1},
                                     limit=limit):
        for r in getattr(paper, 'posts', ()):
            l.append(r)
    for paper in paperClass.find_obj(sortKeys={'replies.published':-1},
                                     limit=limit):
        for r in getattr(paper, 'replies', ()):
            l.append(r)
    for topic in topicClass.find_obj(sortKeys={'published':-1}, limit=limit):
        l.append(topic)
    l.sort(lambda x,y:cmp(x.published, y.published)) # oldest first
    for r in l:
        dq.appendleft(r)


messageOfTheDay = 'Welcome!'        
        
def poll_recent_events(paperClass, topicClass, interval=300):
    'update recentEventsDeque every interval; run in separate thread'
    import time
    import gc
    global messageOfTheDay
    dq = collections.deque(maxlen=20)
    while True:
        load_recent_events(paperClass, topicClass, dq)
        recentEventsDeque.clear()
        recentEventsDeque.extend(dq)
        dq.clear()
        with open('_templates/motd.html') as ifile:
            messageOfTheDay = ifile.read()
        gc.collect() # frequent GC seems to keep RSS from growing unsustainably
        time.sleep(interval)

########NEW FILE########
__FILENAME__ = watchmem
import resource
import web
import thread
import view
import time

# runs spnet web server with explicit memory limit
# as workaround for Python memory usage going up and up
# due to Python memory fragmentation
# see http://revista.python.org.ar/2/en/html/memory-fragmentation.html
# run keeprunning.py to call this and automatically restart whenever
# memory usage limit exceeded.

maxmem = 150000 # max RSS in KB
checkInterval = 60 # seconds between mem usage checks

s = web.Server() # initialize db connection, REST apptree etc.
thread.start_new_thread(view.poll_recent_events, (s.papers.klass, s.topics.klass)) # start polling db for "recent events"
s.start() # run web server in separate thread

def mem_usage():
    'report RSS memory usage by our process in KB'
    return resource.getrusage(resource.RUSAGE_SELF).ru_maxrss

while mem_usage() < maxmem: # if exceeded memory limit, exit and restart
    time.sleep(checkInterval) # wait a bit before next check

########NEW FILE########
__FILENAME__ = web
import cherrypy
import thread
import core, connect
import twitter
import gplus
import apptree
import view
from sessioninfo import get_session

class Server(object):
    def __init__(self, dbconn=None, colls=None, **kwargs):
        if not dbconn:
            dbconn = connect.init_connection(**kwargs)
        self.dbconn = dbconn
        self.gplus_keys = gplus.get_keys()
        self.reload_views(colls)

    def start(self):
        'start cherrypy server as background thread, retaining control of main thread'
        self.threadID = thread.start_new_thread(self.serve_forever, ())

    def serve_forever(self):
        cherrypy.quickstart(self, '/', 'cp.conf')

    def reload_views(self, colls=None):
        'reload view templates from disk'
        if not colls:
            colls = apptree.get_collections()
        for attr, c in colls.items(): # bind collections to server root
            setattr(self, attr, c)

    def login(self, email, password):
        'check password and create session if authenticated'
        try:
            a = core.EmailAddress(email)
        except KeyError:
            return 'no such email address'
        p = a.parent
        if p.authenticate(password):
            get_session()['email'] = email
            get_session()['person'] = p
        else:
            return 'bad password'
        return view.redirect('/view?view=person&person=' + str(p._id))
    login.exposed = True

    def twitter_login(self):
        redirect_url, tokens = twitter.start_oauth('http://localhost:8000/twitter_oauth')
        get_session()['twitter_request_token'] = tokens
        return view.redirect(redirect_url)
    twitter_login.exposed = True

    def twitter_oauth(self, oauth_token, oauth_verifier):
        t = get_session()['twitter_request_token']
        auth = twitter.complete_oauth(t[0], t[1], oauth_verifier)
        p, user, api = twitter.get_auth_person(auth)
        get_session()['person'] = p
        get_session()['twitter_user'] = user
        get_session()['twitter_api'] = api
        self.twitter_auth = auth # just for hand testing
        return 'Logged in to twitter'
    twitter_oauth.exposed = True

    def gplus_login(self):
        oauth = gplus.OAuth(keys=self.gplus_keys)
        get_session()['gplus_oauth'] = oauth
        return view.redirect(oauth.get_authorize_url())
    gplus_login.exposed = True

    def oauth2callback(self, error=False, **kwargs):
        if error:
            return error
        oauth = get_session()['gplus_oauth']
        oauth.get_credentials(**kwargs)
        get_session()['person'] = oauth.get_person()
        return view.redirect('/')
    oauth2callback.exposed = True

    def signout(self):
        'force this session to expire immediately'
        cherrypy.lib.sessions.expire()
        return view.redirect('/')
    signout.exposed = True
            

if __name__ == '__main__':
    s = Server()
    thread.start_new_thread(view.poll_recent_events, (s.papers.klass, s.topics.klass))
    print 'starting server...'
    s.start()
    #print 'starting gplus #spnetwork polling...'
    #gplus.publicAccess.start_poll(300, 10, view.recentEventsDeque)


########NEW FILE########
__FILENAME__ = webui

class Table(list):
    def __init__(self, caption=None, headings=None):
        list.__init__(self)
        self.caption = caption
        self.headings = headings

    def __str__(self):
        s = '<TABLE BORDER=1>\n'
        if self.caption:
            s += '\t<CAPTION>%s</CAPTION>\n' % self.caption
        if self.headings:
            s += '\t<TR>'
            for head in self.headings:
                s += '<TH>%s</TH>' % head
            s += '\t</TR>\n'
        for row in self:
            s += '\t<TR>'
            for col in row:
                s += '\t\t<TD>%s</TD>\n' % col
            s += '\t</TR>\n'
        s += '</TABLE>\n'
        return s
        
class Data(list):
    def __str__(self):
        try:
            s='<%s>' % self.format
        except AttributeError:
            s=''
        for v in self:
            s+=str(v)
        try:
            s+='</%s>' % self.format
        except AttributeError:
            pass
        return s

class Body(Data):
    format='BODY'
class Head(Data):
    format='HEAD'
class Title(Data):
    format='TITLE'
class Document(Data):
    format='HTML'
    def __init__(self,title):
        head=Head([Title([title])])
        try:
            head.append(self._defaultHeader)
        except AttributeError:
            pass
        list.__init__(self,[head,Body()])
        self.head=head
        self.methods={}
        self.n = 0
    def append(self,x):
        'just append to body'
        self[-1].append(x)
    def __call__(self,**kwargs):
        return str(self)
    def add_text(self,text,format=None):
        data=Data([text])
        if format is not None:
            data.format=format
        self.append(data)
    def add_method(self,m):
        try:
            return self.methods[id(m)]
        except KeyError:
            pass
        if isinstance(m,Function):
            method = m
        elif callable(m):
            method = XMLRPCMethod(m)
        else:
            raise TypeError('m must be a Function or callable')
        if len(self.methods)==0:
            self.head.append('''
<script type="text/javascript" src="/jsolait/jsolait.js"></script>
<script type="text/javascript">jsolait.baseURI="/jsolait";</script>
<script type="text/javascript">

var xmlrpc=null;
try{
  var xmlrpc = imprt("xmlrpc");
}catch(e){
  alert(e);
  throw "importing of xmlrpc module failed.";
}
pyshServer="http://localhost:8000";
</script>
            ''')
        self.head.append(str(method))
        self.methods[id(m)] = method
        return method
    def assign_ID(self,e,label='input'):
        if not hasattr(e,'ID'):
            e.ID = label+str(self.n)
            self.n += 1

class Function(object):
    'to make a js function, create an instance and add name and code attributes'
    def __str__(self):
        return self.code

class ValueSetter(Function):
    def __init__(self,name,target,doc):
        self.name = name
        self.target = target
        doc.assign_ID(target)
        doc.add_method(self)
    def __str__(self):
        s = '<script type="text/javascript">\n'
        s += '%s=function(rslt,errmsg)\n{\n  %s=rslt;\n}\n</script>\n' \
             % (self.name,get_element_js(self.target))
        return s


class XMLRPCMethod(Function):
    def __init__(self,m):
        try:
            self.xmlrpc = m.__module__ + '.' + m.__name__
        except AttributeError:
            self.xmlrpc = m
        self.name = '_'.join(self.xmlrpc.split('.'))
    def __str__(self):
        s = '<script type="text/javascript">\n'
        s += '%s=new xmlrpc.XMLRPCMethod(pyshServer,"%s",null,null);\n</script>\n' \
             % (self.name,self.xmlrpc)
        return s

class Action(object):
    def __init__(self,label,doc,m,*args,**kwargs):
        self.method = doc.add_method(m)
        self.args = []
        for arg in args:
            if isinstance(arg,str):
                self.args.append("'%s'" % arg)
            elif isinstance(arg,int) or isinstance(arg,long):
                self.args.append("'%d'" % arg)
            else:
                doc.assign_ID(arg)
                try:
                    self.args.append(get_element_js(arg))
                except TypeError:
                    self.args.append(str(arg))
        try: # ADD ASYNCHRONOUS CALLBACK
            self.args.append(kwargs['callback'].name)
        except KeyError:
            pass
        self.label = label
    def __str__(self):
        return '<button type="button" onclick="%s(%s)">%s</button>' \
               % (self.method.name,','.join(self.args),self.label)


class Link(object):
    def __init__(self,url,txt,label=None):
        self.url = get_method_path(url)
        self.txt = txt
        self.label = label
    def __str__(self):
        if self.label is not None:
            return '<A HREF="%s" TITLE="%s">%s</A>' % (self.url,self.label,self.txt)
        else:
            return '<A HREF="%s">%s</A>' % (self.url,self.txt)


def get_method_path(m):
    if isinstance(m,str):
        return m
    else:
        l = ['']+m.__module__.split('.')+[m.__name__]
        return '/'.join(l)

def get_element_jsvalue(e):
    return "document.getElementById('%s').value" % e.ID

def get_element_jstext(e):
    return "document.getElementById('%s').innerHTML" % e.ID

def get_element_js(e):
    if isinstance(e,Variable):
        return get_element_jsvalue(e)
    elif isinstance(e,Data):
        return get_element_jstext(e)
    else:
        raise TypeError('e must be Variable or Data!')


class Form(list):
    def __init__(self,m,method="POST",label='Go!',**kwargs):
        list.__init__(self)
        self.url = get_method_path(m)
        self.method = method
        self.label = label
        self.kwargs = kwargs
        self.enctype = None
    def __str__(self):
        s='<FORM METHOD="%s" ACTION="%s"' % (self.method,self.url)
        if self.enctype:
            s += ' enctype="%s"' % self.enctype
        s += '>\n'
        for v in self:
            s+=str(v)
        if self.label is not None:
            s += str(Input('','submit',self.label))
        for k,v in self.kwargs.items():
            s += str(Input(k,'hidden',v))
        s += '</FORM>\n\n'
        return s
    def append(self, v):
        'automatically sets right encoding if file upload input appended'
        if isinstance(v, Upload):
            self.enctype = "multipart/form-data"
        list.append(self, v)

class Separator(object):
    def __str__(self):
        return '<BR><HR><BR>\n'

class Variable(object):
    pass

class Input(Variable):
    def __init__(self,name,type='text',value='',size=20,maxlength=None,
                 checked=None,separator=''):
        self.name=name
        self.type=type
        self.size=size
        try:
            self.value=value.items()
        except AttributeError:
            self.value=value
        self.maxlength=maxlength
        self.checked=checked
        self.separator=separator
    def field_list(self,*args):
        s = ''
        for arg in args:
            try:
                s += ' %s="%s"' % (arg,getattr(self,arg))
            except AttributeError:
                pass
        return s
    def __str__(self):
        if self.type=='text' or self.type == 'password':
            return '\t<INPUT%s/>\n' \
                   % self.field_list('type','name','size','value','ID')
        elif self.type=='hidden':
            return '\t<INPUT%s/>\n' \
                   % self.field_list('type','name','value','ID')
        elif self.type=='submit':
            return '\t<INPUT TYPE="%s" VALUE="%s"/>\n' % (self.type,self.value)
        elif self.type=='reset':
            return '\t<INPUT TYPE="%s"/>\n' % self.type
        s=''
        for k,v in self.value:
            if k==self.checked:
                s+='\t<INPUT TYPE="%s" NAME="%s" VALUE="%s" CHECKED/>%s%s\n' \
                    % (self.type,self.name,k,v,self.separator)
            else:
                s+='\t<INPUT TYPE="%s" NAME="%s" VALUE="%s"/>%s%s\n' \
                    % (self.type,self.name,k,v,self.separator)
        return s

class Upload(Variable):
    def __init__(self, name):
        self.name = name

    def __str__(self):
        return '<input type="file" name="%s" />' % self.name

class Textarea(Variable):
    def __init__(self, name, value='', cols='80', rows='24', wrap='off'):
        self.name=name
        self.value = value
        self.cols = cols
        self.rows = rows
        self.wrap = wrap
    def __str__(self):
        s = '''<TEXTAREA NAME="%s" COLS=%s ROWS=%s WRAP="%s">%s</TEXTAREA>\n''' \
            % (self.name, self.cols, self.rows, self.wrap, self.value)
        return s

class Selection(Variable):
    def __init__(self, name, value, size=None, multiple=False, selected=None,
                 **kwargs):
        self.name=name
        self.size=size
        try:
            self.value=value.items()
        except AttributeError:
            self.value=value
        self.multiple=multiple
        self.selected=selected
        self.kwargs = kwargs
    def __str__(self):
        s='\t<SELECT NAME="%s"' % self.name
        if self.size is not None:
            s+=' SIZE=%d' % self.size
        if self.multiple:
            s+=' MULTIPLE'
        for t in self.kwargs.items():
            s += ' %s="%s"' % t
        s+='>\n'
        for k,v in self.value:
            if k==self.selected:
                s+='\t\t<OPTION SELECTED VALUE="%s">%s</OPTION>\n' % (k,v)
            else:
                s+='\t\t<OPTION VALUE="%s">%s</OPTION>\n' % (k,v)
        s+='\t</SELECT>\n'
        return s

class RadioSelection(Selection):
    _type = 'radio'
    def __str__(self):
        s = ''
        for k,v in self.value:
            if k == self.selected:
                s += '\t\t<INPUT TYPE="%s" NAME="%s" VALUE="%s" CHECKED>%s<BR>\n' \
                    % (self._type, self.name, k, v)
            else:
                s += '\t\t<INPUT TYPE="%s" NAME="%s" VALUE="%s">%s<BR>\n' \
                    % (self._type, self.name, k, v)
        return s
    
class CheckboxSelection(RadioSelection):
    _type = 'checkbox'
    def __init__(self, *args, **kwargs):
        RadioSelection.__init__(self, *args, **kwargs)
        if self.multiple: # make form return array of values
            self.name = self.name + '[]'

########NEW FILE########
