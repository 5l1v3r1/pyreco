__FILENAME__ = conf
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import errno
import io
import json
import logging
import os
import sys

from .util import ServiceCoord, Address, async_config


logger = logging.getLogger(__name__)


class Config(object):
    """This class will contain the configuration for CMS. This needs
    to be populated at the initilization stage. This is loaded by
    default with some sane data. See cms.conf.sample in the examples
    for information on the meaning of the fields.

    """
    def __init__(self):
        """Default values for configuration, plus decide if this
        instance is running from the system path or from the source
        directory.

        """
        self.async = async_config

        # System-wide
        self.temp_dir = "/tmp"
        self.backdoor = False

        # Database.
        self.database = "postgresql+psycopg2://cmsuser@localhost/cms"
        self.database_debug = False
        self.twophase_commit = False

        # Worker.
        self.keep_sandbox = True
        self.use_cgroups = True
        self.sandbox_implementation = 'isolate'

        # WebServers.
        self.secret_key = "8e045a51e4b102ea803c06f92841a1fb"
        self.tornado_debug = False

        # ContestWebServer.
        self.contest_listen_address = [""]
        self.contest_listen_port = [8888]
        self.cookie_duration = 1800
        self.submit_local_copy = True
        self.submit_local_copy_path = "%s/submissions/"
        self.tests_local_copy = True
        self.tests_local_copy_path = "%s/tests/"
        self.ip_lock = True
        self.block_hidden_users = False
        self.is_proxy_used = False
        self.max_submission_length = 100000
        self.max_input_length = 5000000
        self.stl_path = "/usr/share/doc/stl-manual/html/"
        self.allow_questions = True
        # Prefix of 'iso-codes'[1] installation. It can be found out
        # using `pkg-config --variable=prefix iso-codes`, but it's
        # almost universally the same (i.e. '/usr') so it's hardly
        # necessary to change it.
        # [1] http://pkg-isocodes.alioth.debian.org/
        self.iso_codes_prefix = "/usr"
        # Prefix of 'shared-mime-info'[2] installation. It can be found
        # out using `pkg-config --variable=prefix shared-mime-info`, but
        # it's almost universally the same (i.e. '/usr') so it's hardly
        # necessary to change it.
        # [2] http://freedesktop.org/wiki/Software/shared-mime-info
        self.shared_mime_info_prefix = "/usr"

        # AdminWebServer.
        self.admin_listen_address = ""
        self.admin_listen_port = 8889

        # ProxyService.
        self.rankings = ["http://usern4me:passw0rd@localhost:8890/"]
        self.https_certfile = None

        # Installed or from source?
        self.installed = sys.argv[0].startswith("/usr/") and \
            sys.argv[0] != '/usr/bin/ipython' and \
            sys.argv[0] != '/usr/bin/python2' and \
            sys.argv[0] != '/usr/bin/python'

        if self.installed:
            self.log_dir = os.path.join("/", "var", "local", "log", "cms")
            self.cache_dir = os.path.join("/", "var", "local", "cache", "cms")
            self.data_dir = os.path.join("/", "var", "local", "lib", "cms")
            self.run_dir = os.path.join("/", "var", "local", "run", "cms")
            paths = [os.path.join("/", "usr", "local", "etc", "cms.conf"),
                     os.path.join("/", "etc", "cms.conf")]
        else:
            self.log_dir = "log"
            self.cache_dir = "cache"
            self.data_dir = "lib"
            self.run_dir = "run"
            paths = [os.path.join(".", "examples", "cms.conf")]
            if '__file__' in globals():
                paths += [os.path.abspath(os.path.join(
                          os.path.dirname(__file__),
                          '..', 'examples', 'cms.conf'))]
            paths += [os.path.join("/", "usr", "local", "etc", "cms.conf"),
                      os.path.join("/", "etc", "cms.conf")]

        # Allow user to override config file path using environment
        # variable 'CMS_CONFIG'.
        CMS_CONFIG_ENV_VAR = "CMS_CONFIG"
        if CMS_CONFIG_ENV_VAR in os.environ:
            paths = [os.environ[CMS_CONFIG_ENV_VAR]] + paths

        # Attempt to load a config file.
        self._load(paths)

    def _load(self, paths):
        """Try to load the config files one at a time, until one loads
        correctly.

        """
        for conf_file in paths:
            if self._load_unique(conf_file):
                break
        else:
            logging.warning("No configuration file found: "
                            "falling back to default values.")

    def _load_unique(self, path):
        """Populate the Config class with everything that sits inside
        the JSON file path (usually something like /etc/cms.conf). The
        only pieces of data treated differently are the elements of
        core_services and other_services that are sent to async
        config.

        Services whose name begins with an underscore are ignored, so
        they can be commented out in the configuration file.

        path (string): the path of the JSON config file.

        """
        # Load config file.
        try:
            with io.open(path, 'rt', encoding='utf-8') as f:
                data = json.load(f)
        except IOError as error:
            if error.errno == errno.ENOENT:
                logger.debug("Couldn't find config file %s.", path)
            else:
                logger.warning("I/O error while opening file %s: [%s] %s",
                               path, errno.errorcode[error.errno],
                               os.strerror(error.errno))
            return False
        except ValueError as error:
            logger.warning("Invalid syntax in file %s: %s", path, error)
            return False

        logger.info("Using configuration file %s.", path)

        # Put core and test services in async_config, ignoring those
        # whose name begins with "_".
        for service in data["core_services"]:
            if service.startswith("_"):
                continue
            for shard_number, shard in \
                    enumerate(data["core_services"][service]):
                coord = ServiceCoord(service, shard_number)
                self.async.core_services[coord] = Address(*shard)
        del data["core_services"]

        for service in data["other_services"]:
            if service.startswith("_"):
                continue
            for shard_number, shard in \
                    enumerate(data["other_services"][service]):
                coord = ServiceCoord(service, shard_number)
                self.async.other_services[coord] = Address(*shard)
        del data["other_services"]

        # Put everything else in self.
        for key, value in data.iteritems():
            setattr(self, key, value)

        return True


config = Config()

########NEW FILE########
__FILENAME__ = base
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013-2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from datetime import datetime, timedelta

from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm.exc import ObjectDeletedError
from sqlalchemy.orm.session import object_session
from sqlalchemy.orm import \
    class_mapper, object_mapper, ColumnProperty, RelationshipProperty
from sqlalchemy.types import \
    Boolean, Integer, Float, String, Unicode, DateTime, Interval, Enum

import six

from . import RepeatedUnicode, engine


_TYPE_MAP = {
    Boolean: bool,
    Integer: six.integer_types,
    Float: float,
    String: six.string_types,  # TODO Use six.binary_type.
    Unicode: six.string_types,  # TODO Use six.text_type.
    DateTime: datetime,
    Interval: timedelta,
    Enum: six.string_types,  # TODO Use six.text_type.
    RepeatedUnicode: list,  # TODO Use a type that checks also the content.
}


class Base(object):
    """Base class for all classes managed by SQLAlchemy. Extending the
    base class given by SQLAlchemy.

    """
    @property
    def sa_mapper(self):
        return object_mapper(self)

    @property
    def sa_session(self):
        return object_session(self)

    @property
    def sa_primary_key(self):
        return self.sa_mapper.primary_key_from_instance(self)

    @property
    def sa_identity_key(self):
        return self.sa_mapper.identity_key_from_instance(self)

    # This method gets called after the mapper has been initialized
    # (i.e. all properties, both columns and relationships) are ready to use
    @classmethod
    def __declare_last__(cls):
        """Analyze and extract properties of mapper and save them in cls

        Split the properties into column and relationship properties.

        raise (RuntimeError): if something isn't correctly understood.

        """
        # Divide all properties into column and relationship ones.
        cls._col_props = list()
        cls._rel_props = list()

        for prp in class_mapper(cls).iterate_properties:
            if isinstance(prp, ColumnProperty):
                if len(prp.columns) != 1:
                    raise RuntimeError(
                        "Unexpected number of columns for ColumnProperty %s of"
                        " %s: %d" % (prp.key, cls.__name__, len(prp.columns)))
                col = prp.columns[0]
                col_type = type(col.type)

                # Ignore IDs and foreign keys
                if col.primary_key or col.foreign_keys:
                    continue

                # Check that we understand the type
                if col_type not in _TYPE_MAP:
                    raise RuntimeError(
                        "Unknown SQLAlchemy column type for ColumnProperty "
                        "%s of %s: %s" % (prp.key, cls.__name__, col_type))

                cls._col_props.append(prp)
            elif isinstance(prp, RelationshipProperty):
                cls._rel_props.append(prp)
            else:
                raise RuntimeError(
                    "Unknown SQLAlchemy property type for %s of %s: %s" %
                    (prp.key, cls.__name__, type(prp)))

    def __init__(self, *args, **kwargs):
        """Initialize a new object with the given properties

        The properties we're referring to are the SQLAlchemy ones,
        specified as class-level attributes during class definition.
        They can be of two types: column or relationship properties
        (note that a relationship can also be created by a backref
        of another relationship property). For the purpose of this
        method the column properties that are part of a primary key
        or of a foreign key constraint are ignored.

        This constructor behaves like the following one (which uses
        Python 3 syntax, see [1][2][3] for additional information):

            obj = cls(<col props>, *, <rel props>)

        This means that the column properties can be specified both
        as positional and as keyword arguments while the relationship
        properties have to be given as keyword arguments. The order
        in which column properties appear as positional arguments is
        the order they were defined as class-attributes.

        All relationship properties are optional. Column properties
        are optional only if they're nullable or if they have a not
        null default value. The default value is the one specified
        in the Column() call used to define the property (None by
        default). This could cause us to violate a restriction that
        the Python syntax (see [2]) imposes on other functions: an
        argument without a default value may come after one with it.

        Additionally, this function also does some type-checking for
        column properties: a TypeError is raised if a given argument
        has a type that doesn't match the one of the corresponding
        property. Relationship properties aren't checked (for now).

        [1] http://www.python.org/dev/peps/pep-3102/
        [2] http://docs.python.org/3/reference/compound_stmts.html#function
        [3] http://docs.python.org/3/reference/expressions.html#calls

        """
        cls = type(self)

        # Check the number of positional argument
        if len(args) > len(self._col_props):
            raise TypeError(
                "%s.__init__() takes at most %d positional arguments (%d "
                "given)" % (cls.__name__, len(self._col_props), len(args)))

        # Copy the positional arguments into the keyword ones
        for arg, prp in zip(args, self._col_props):
            if prp.key in kwargs:
                raise TypeError(
                    "%s.__init__() got multiple values for keyword "
                    "argument '%s'" % (cls.__name__, prp.key))
            kwargs[prp.key] = arg

        for prp in self._col_props:
            col = prp.columns[0]
            col_type = type(col.type)

            if prp.key not in kwargs:
                # We're assuming the default value, if specified, has
                # the correct type
                if col.default is None and not col.nullable:
                    raise TypeError(
                        "%s.__init__() didn't get required keyword "
                        "argument '%s'" % (cls.__name__, prp.key))
                # We're setting the default ourselves, since we may
                # want to use the object before storing it in the DB.
                # FIXME This code doesn't work with callable defaults.
                # We can use the is_callable and is_scalar properties
                # (and maybe the is_sequence and is_clause_element ones
                # too) to detect the type. Note that callables require a
                # ExecutionContext argument (which we don't have).
                if col.default is not None:
                    setattr(self, prp.key, col.default.arg)
            else:
                val = kwargs.pop(prp.key)

                if val is None:
                    if not col.nullable:
                        raise TypeError(
                            "%s.__init__() got None for keyword argument '%s',"
                            " which is not nullable" % (cls.__name__, prp.key))
                    setattr(self, prp.key, val)
                else:
                    # TODO col_type.python_type contains the type that
                    # SQLAlchemy thinks is more appropriate. We could
                    # use that and drop _TYPE_MAP...
                    if not isinstance(val, _TYPE_MAP[col_type]):
                        raise TypeError(
                            "%s.__init__() got a '%s' for keyword argument "
                            "'%s', which requires a '%s'" %
                            (cls.__name__, type(val), prp.key,
                             _TYPE_MAP[col_type]))
                    setattr(self, prp.key, val)

        for prp in self._rel_props:
            if prp.key not in kwargs:
                # If the property isn't given we leave the default
                # value instead of explictly setting it ourself.
                pass
            else:
                val = kwargs.pop(prp.key)

                # TODO Some type validation (take a look at prp.uselist)
                setattr(self, prp.key, val)

        # Check if there were unknown arguments
        if kwargs:
            raise TypeError(
                "%s.__init__() got an unexpected keyword argument '%s'" %
                (cls.__name__, kwargs.popitem()[0]))

    @classmethod
    def get_from_id(cls, id_, session):
        """Retrieve an object from the database by its ID.

        Use the given session to fetch the object of this class with
        the given ID, and return it. If it doesn't exist return None.

        cls (type): the class to which the method is attached.
        id_ (tuple, int or string): the ID of the object we want; in
            general it will be a tuple (one int for each of the columns
            that make up the primary key) but if there's only one then
            a single int (even encoded as unicode or bytes) will work.
        session (Session): the session to query.

        return (Base|None): the desired object, or None if not found.

        """
        try:
            # The .get() method returns None if the object isn't in the
            # identity map of the session nor in the database, but
            # raises ObjectDeletedError in case it was in the identity
            # map, got marked as expired but couldn't be found in the
            # database again.
            return session.query(cls).get(id_)
        except ObjectDeletedError:
            return None

    def clone(self):
        """Copy all the column properties into a new object

        Create a new object of this same type and set the values of all
        its column properties to the ones of this "old" object. Leave
        the relationship properties unset.

        return (object): a clone of this object

        """
        cls = type(self)
        args = list(getattr(self, prp.key) for prp in self._col_props)
        return cls(*args)

    def get_attrs(self):
        """Return self.__dict__.

        Limited to SQLAlchemy column properties.

        return ({string: object}): the properties of this object.

        """
        attrs = dict()
        for prp in self._col_props:
            if hasattr(self, prp.key):
                attrs[prp.key] = getattr(self, prp.key)
        return attrs

    def set_attrs(self, attrs):
        """Do self.__dict__.update(attrs) with validation.

        Limited to SQLAlchemy column and relationship properties.

        attrs ({string: object}): the new properties we want to set on
            this object.

        """
        # We want to pop items without altering the caller's object.
        attrs = attrs.copy()

        for prp in self._col_props:
            col = prp.columns[0]
            col_type = type(col.type)

            if prp.key in attrs:
                val = attrs.pop(prp.key)

                if val is None:
                    if not col.nullable:
                        raise TypeError(
                            "set_attrs() got None for keyword argument '%s',"
                            " which is not nullable" % prp.key)
                    setattr(self, prp.key, val)
                else:
                    # TODO col_type.python_type contains the type that
                    # SQLAlchemy thinks is more appropriate. We could
                    # use that and drop _TYPE_MAP...
                    if not isinstance(val, _TYPE_MAP[col_type]):
                        raise TypeError(
                            "set_attrs() got a '%s' for keyword argument "
                            "'%s', which requires a '%s'" %
                            (type(val), prp.key, _TYPE_MAP[col_type]))
                    setattr(self, prp.key, val)

        for prp in self._rel_props:
            if prp.key in attrs:
                val = attrs.pop(prp.key)

                # TODO Some type validation (take a look at prp.uselist)
                setattr(self, prp.key, val)

        # Check if there were unknown arguments
        if attrs:
            raise TypeError(
                "set_attrs() got an unexpected keyword argument '%s'" %
                attrs.popitem()[0])


Base = declarative_base(engine, cls=Base, constructor=None)


metadata = Base.metadata

########NEW FILE########
__FILENAME__ = contest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012-2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Bernard Blackham <bernard@largestprime.net>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Contest-related database interface for SQLAlchemy.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from datetime import datetime, timedelta

from sqlalchemy.schema import Column, ForeignKey, CheckConstraint
from sqlalchemy.types import Integer, Unicode, DateTime, Interval, Enum
from sqlalchemy.orm import relationship, backref

from . import Base, RepeatedUnicode

from cms import DEFAULT_LANGUAGES
from cmscommon.datetime import make_datetime


class Contest(Base):
    """Class to store a contest (which is a single day of a
    programming competition).

    """
    __tablename__ = 'contests'
    __table_args__ = (
        CheckConstraint("start <= stop"),
        CheckConstraint("token_gen_initial <= token_gen_max"),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Short name of the contest, and longer description. Both human
    # readable.
    name = Column(
        Unicode,
        nullable=False)
    description = Column(
        Unicode,
        nullable=False)

    # The list of language codes of the localizations that contestants
    # are allowed to use.
    allowed_localizations = Column(
        RepeatedUnicode(),
        nullable=False,
        default=[])

    # The list of languages shorthand allowed in the contest,
    # e.g. cpp. The codes must be the same as those in cms.LANGUAGES.
    languages = Column(
        RepeatedUnicode(),
        nullable=False,
        default=DEFAULT_LANGUAGES)

    # The parameters that control contest-tokens follow. Note that
    # their effect during the contest depends on the interaction with
    # the parameters that control task-tokens, defined on each Task.

    # The "kind" of token rules that will be active during the contest.
    # - disabled: The user will never be able to use any token.
    # - finite: The user has a finite amount of tokens and can choose
    #   when to use them, subject to some limitations. Tokens may not
    #   be all available at start, but given periodically during the
    #   contest instead.
    # - infinite: The user will always be able to use a token.
    token_mode = Column(
        Enum("disabled", "finite", "infinite", name="token_mode"),
        nullable=False,
        default="infinite")

    # The maximum number of tokens a contestant is allowed to use
    # during the whole contest (on all tasks).
    token_max_number = Column(
        Integer,
        CheckConstraint("token_max_number > 0"),
        nullable=True)

    # The minimum interval between two successive uses of tokens for
    # the same user (on any task).
    token_min_interval = Column(
        Interval,
        CheckConstraint("token_min_interval >= '0 seconds'"),
        nullable=False,
        default=timedelta())

    # The parameters that control generation (if mode is "finite"):
    # the user starts with "initial" tokens and receives "number" more
    # every "interval", but their total number is capped to "max".
    token_gen_initial = Column(
        Integer,
        CheckConstraint("token_gen_initial >= 0"),
        nullable=False,
        default=2)
    token_gen_number = Column(
        Integer,
        CheckConstraint("token_gen_number >= 0"),
        nullable=False,
        default=2)
    token_gen_interval = Column(
        Interval,
        CheckConstraint("token_gen_interval > '0 seconds'"),
        nullable=False,
        default=timedelta(minutes=30))
    token_gen_max = Column(
        Integer,
        CheckConstraint("token_gen_max > 0"),
        nullable=True)

    # Beginning and ending of the contest.
    start = Column(
        DateTime,
        nullable=False,
        default=datetime(2000, 01, 01))
    stop = Column(
        DateTime,
        nullable=False,
        default=datetime(2100, 01, 01))

    # Timezone for the contest. All timestamps in CWS will be shown
    # using the timezone associated to the logged-in user or (if it's
    # None or an invalid string) the timezone associated to the
    # contest or (if it's None or an invalid string) the local
    # timezone of the server. This value has to be a string like
    # "Europe/Rome", "Australia/Sydney", "America/New_York", etc.
    timezone = Column(
        Unicode,
        nullable=True)

    # Max contest time for each user in seconds.
    per_user_time = Column(
        Interval,
        nullable=True)

    # Maximum number of submissions or user_tests allowed for each user
    # during the whole contest or None to not enforce this limitation.
    max_submission_number = Column(
        Integer,
        CheckConstraint("max_submission_number > 0"),
        nullable=True)
    max_user_test_number = Column(
        Integer,
        CheckConstraint("max_user_test_number > 0"),
        nullable=True)

    # Minimum interval between two submissions or user_tests, or None to
    # not enforce this limitation.
    min_submission_interval = Column(
        Interval,
        CheckConstraint("min_submission_interval > '0 seconds'"),
        nullable=True)
    min_user_test_interval = Column(
        Interval,
        CheckConstraint("min_user_test_interval > '0 seconds'"),
        nullable=True)

    # The scores for this contest will be rounded to this number of
    # decimal places.
    score_precision = Column(
        Integer,
        CheckConstraint("score_precision >= 0"),
        nullable=False,
        default=0)

    # Follows the description of the fields automatically added by
    # SQLAlchemy.
    # tasks (list of Task objects)
    # announcements (list of Announcement objects)
    # users (list of User objects)

    # Moreover, we have the following methods.
    # get_submissions (defined in __init__.py)
    # get_submission_results (defined in __init__.py)
    # get_user_tests (defined in __init__.py)
    # get_user_test_results (defined in __init__.py)

    # FIXME - Use SQL syntax
    def get_task(self, task_name):
        """Return the first task in the contest with the given name.

        task_name (string): the name of the task we are interested in.

        return (Task): the corresponding task object.

        raise (KeyError): if no tasks with the given name are found.

        """
        for task in self.tasks:
            if task.name == task_name:
                return task
        raise KeyError("Task not found")

    # FIXME - Use SQL syntax
    def get_task_index(self, task_name):
        """Return the index of the first task in the contest with the
        given name.

        task_name (string): the name of the task we are interested in.

        return (int): the index of the corresponding task.

        raise (KeyError): if no tasks with the given name are found.

        """
        for idx, task in enumerate(self.tasks):
            if task.name == task_name:
                return idx
        raise KeyError("Task not found")

    # FIXME - Use SQL syntax
    def get_user(self, username):
        """Return the first user in the contest with the given name.

        username (string): the name of the user we are interested in.

        return (User): the corresponding user object.

        raise (KeyError): if no users with the given name are found.

        """
        for user in self.users:
            if user.username == username:
                return user
        raise KeyError("User not found")

    def enumerate_files(self, skip_submissions=False, skip_user_tests=False,
                        skip_generated=False):
        """Enumerate all the files (by digest) referenced by the
        contest.

        return (set): a set of strings, the digests of the file
                      referenced in the contest.

        """
        # Here we cannot use yield, because we want to detect
        # duplicates
        files = set()
        for task in self.tasks:

            # Enumerate statements
            for file_ in task.statements.itervalues():
                files.add(file_.digest)

            # Enumerate attachments
            for file_ in task.attachments.itervalues():
                files.add(file_.digest)

            # Enumerate managers
            for dataset in task.datasets:
                for file_ in dataset.managers.itervalues():
                    files.add(file_.digest)

            # Enumerate testcases
            for dataset in task.datasets:
                for testcase in dataset.testcases.itervalues():
                    files.add(testcase.input)
                    files.add(testcase.output)

        if not skip_submissions:
            for submission in self.get_submissions():

                # Enumerate files
                for file_ in submission.files.itervalues():
                    files.add(file_.digest)

                # Enumerate executables
                if not skip_generated:
                    for sr in submission.results:
                        for file_ in sr.executables.itervalues():
                            files.add(file_.digest)

        if not skip_user_tests:
            for user_test in self.get_user_tests():

                files.add(user_test.input)

                if not skip_generated:
                    for ur in user_test.results:
                        if ur.output is not None:
                            files.add(ur.output)

                # Enumerate files
                for file_ in user_test.files.itervalues():
                    files.add(file_.digest)

                # Enumerate managers
                for file_ in user_test.managers.itervalues():
                    files.add(file_.digest)

                # Enumerate executables
                if not skip_generated:
                    for ur in user_test.results:
                        for file_ in ur.executables.itervalues():
                            files.add(file_.digest)

        return files

    def phase(self, timestamp):
        """Return: -1 if contest isn't started yet at time timestamp,
                    0 if the contest is active at time timestamp,
                    1 if the contest has ended.

        timestamp (datetime): the time we are iterested in.
        return (int): contest phase as above.

        """
        if self.start is not None and self.start > timestamp:
            return -1
        if self.stop is None or self.stop > timestamp:
            return 0
        return 1

    @staticmethod
    def _tokens_available(token_timestamps, token_mode,
                          token_max_number, token_min_interval,
                          token_gen_initial, token_gen_number,
                          token_gen_interval, token_gen_max, start, timestamp):
        """Do exactly the same computation stated in tokens_available,
        but ensuring only a single set of token_* directive.
        Basically, tokens_available call this twice for contest-wise
        and task-wise parameters and then assemble the result.

        token_timestamps ([datetime]): list of timestamps of used
            tokens, sorted in chronological order.
        token_* (int): the parameters we want to enforce.
        start (datetime): the time from which we start accumulating
            tokens.
        timestamp (datetime): the time relative to which make the
            calculation (has to be greater than or equal to all
            elements of token_timestamps).

        return ((int, datetime|None, datetime|None)): same as
            tokens_available.

        """
        # If tokens are disabled there are no tokens available.
        if token_mode == "disabled":
            return (0, None, None)

        # If tokens are infinite there are always tokens available.
        if token_mode == "infinite":
            return (-1, None, None)

        # expiration is the timestamp at which all min_intervals for
        # the tokens played up to now have expired (i.e. the first
        # time at which we can play another token). If no tokens have
        # been played so far, this time is the start of the contest.
        expiration = \
            token_timestamps[-1] + token_min_interval \
            if len(token_timestamps) > 0 else start

        # If we already played the total number allowed, we don't have
        # anything left.
        played_tokens = len(token_timestamps)
        if token_max_number is not None and played_tokens >= token_max_number:
            return (0, None, None)

        # avail is the current number of available tokens. We are
        # going to rebuild all the history to know how many of them we
        # have now.
        # We start with the initial number (it's already capped to max
        # by the DB). token_gen_initial can be ignored after this.
        avail = token_gen_initial

        def generate_tokens(prev_time, next_time):
            """Compute how many tokens have been generated between the
            two timestamps.

            prev_time (datetime): timestamp of begin of interval.
            next_time (datetime): timestamp of end of interval.
            return (int): number of tokens generated.

            """
            # How many generation times we passed from start to
            # the previous considered time?
            before_prev = int((prev_time - start).total_seconds()
                              / token_gen_interval.total_seconds())
            # And from start to the current considered time?
            before_next = int((next_time - start).total_seconds()
                              / token_gen_interval.total_seconds())
            # So...
            return token_gen_number * (before_next - before_prev)

        # Previous time we considered
        prev_token = start

        # Simulating!
        for token in token_timestamps:
            # Increment the number of tokens because of generation.
            avail += generate_tokens(prev_token, token)
            if token_gen_max is not None:
                avail = min(avail, token_gen_max)

            # Play the token.
            avail -= 1

            prev_token = token

        avail += generate_tokens(prev_token, timestamp)
        if token_gen_max is not None:
            avail = min(avail, token_gen_max)

        # Compute the time in which the next token will be generated.
        next_gen_time = None
        if token_gen_number > 0 and \
                (token_gen_max is None or avail < token_gen_max):
            next_gen_time = \
                start + token_gen_interval * \
                int((timestamp - start).total_seconds() /
                    token_gen_interval.total_seconds() + 1)

        # If we have more tokens than how many we are allowed to play,
        # cap it, and note that no more will be generated.
        if token_max_number is not None:
            if avail >= token_max_number - played_tokens:
                avail = token_max_number - played_tokens
                next_gen_time = None

        return (avail,
                next_gen_time,
                expiration if expiration > timestamp else None)

    def tokens_available(self, username, task_name, timestamp=None):
        """Return three pieces of data:

        [0] the number of available tokens for the user to play on the
            task (independently from the fact that (s)he can play it
            right now or not due to a min_interval wating for
            expiration); -1 means infinite tokens;

        [1] the next time in which a token will be generated (or
            None); from the user perspective, i.e.: if the user will
            do nothing, [1] is the first time in which his number of
            available tokens will be greater than [0];

        [2] the time when the min_interval will expire, or None

        In particular, let r the return value of this method. We can
        sketch the code in the following way.:

        if r[0] > 0 or r[0] == -1:
            we have tokens
            if r[2] is None:
                we can play a token
            else:
                we must wait till r[2] to play a token
            if r[1] is not None:
                next one will be generated at r[1]
            else:
                no other tokens will be generated (max/total reached ?)
        else:
            we don't have tokens right now
            if r[1] is not None:
                next one will be generated at r[1]
                if r[2] is not None and r[2] > r[1]:
                    but we must wait also till r[2] to play it
            else:
                no other tokens will be generated (max/total reached ?)

        Note also that this method assumes that all played tokens were
        regularly played, and that there are no tokens played in the
        future. Also, if r[0] == 0 and r[1] is None, then r[2] should
        be ignored.

        username (string): the username of the user.
        task_name (string): the name of the task.
        timestamp (datetime): the time relative to which making the
            calculation.

        return ((int, datetime|None, datetime|None)): see description
            above.

        """
        if timestamp is None:
            timestamp = make_datetime()

        user = self.get_user(username)
        task = self.get_task(task_name)

        # Take the list of the tokens already played (sorted by time).
        tokens = user.get_tokens()
        token_timestamps_contest = sorted([token.timestamp
                                           for token in tokens])
        token_timestamps_task = sorted([
            token.timestamp for token in tokens
            if token.submission.task.name == task_name])

        # If the contest is USACO-style (i.e., the time for each user
        # start when he/she logs in for the first time), then we start
        # accumulating tokens from the user starting time; otherwise,
        # from the start of the contest.
        start = self.start
        if self.per_user_time is not None:
            start = user.starting_time

        # Compute separately for contest-wise and task-wise.
        res_contest = Contest._tokens_available(
            token_timestamps_contest, self.token_mode,
            self.token_max_number, self.token_min_interval,
            self.token_gen_initial, self.token_gen_number,
            self.token_gen_interval, self.token_gen_max, start, timestamp)
        res_task = Contest._tokens_available(
            token_timestamps_task, task.token_mode,
            task.token_max_number, task.token_min_interval,
            task.token_gen_initial, task.token_gen_number,
            task.token_gen_interval, task.token_gen_max, start, timestamp)

        # Merge the results.

        # First, the "expiration".
        if res_contest[2] is None:
            expiration = res_task[2]
        elif res_task[2] is None:
            expiration = res_contest[2]
        else:
            expiration = max(res_task[2], res_contest[2])

        # Then, check if both are infinite
        if res_contest[0] == -1 and res_task[0] == -1:
            res = (-1, None, expiration)
        # Else, "combine" them appropriately.
        else:
            # Having infinite contest tokens, in this situation, is the
            # same as having a finite number that is strictly greater
            # than the task tokens. The same holds the other way, too.
            if res_contest[0] == -1:
                res_contest = (res_task[0] + 1, None, None)
            if res_task[0] == -1:
                res_task = (res_contest[0] + 1, None, None)

            # About next token generation time: we need to see when the
            # *minimum* between res_contest[0] and res_task[0] is
            # increased by one, so if there is an actual minimum we
            # need to consider only the next generation time for it.
            # Otherwise, if they are equal, we need both to generate an
            # additional token and we store the maximum between the two
            # next times of generation.
            if res_contest[0] < res_task[0]:
                # We have more task-tokens than contest-tokens.
                # We just need a contest-token to be generated.
                res = (res_contest[0], res_contest[1], expiration)
            elif res_task[0] < res_contest[0]:
                # We have more contest-tokens than task-tokens.
                # We just need a task-token to be generated.
                res = (res_task[0], res_task[1], expiration)
            else:
                # Darn, we need both!
                if res_contest[1] is None or res_task[1] is None:
                    res = (res_task[0], None, expiration)
                else:
                    res = (res_task[0], max(res_contest[1], res_task[1]),
                           expiration)

        return res


class Announcement(Base):
    """Class to store a messages sent by the contest managers to all
    the users.

    """
    __tablename__ = 'announcements'

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Time, subject and text of the announcement.
    timestamp = Column(
        DateTime,
        nullable=False)
    subject = Column(
        Unicode,
        nullable=False)
    text = Column(
        Unicode,
        nullable=False)

    # Contest (id and object) owning the announcement.
    contest_id = Column(
        Integer,
        ForeignKey(Contest.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    contest = relationship(
        Contest,
        backref=backref(
            'announcements',
            order_by=[timestamp],
            cascade="all, delete-orphan",
            passive_deletes=True))

########NEW FILE########
__FILENAME__ = drop
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Drop all content and tables in the database.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging

from psycopg2 import ProgrammingError
from sqlalchemy.engine.url import make_url

from cms import config

from . import custom_psycopg2_connection


logger = logging.getLogger(__name__)


def drop_db():
    """Drop everything in the database. In theory metadata.drop_all()
    should do the same; in practice, it isn't able to handle cases
    when the data present in the database doesn't fit the schema known
    by metadata.

    This method is psycopg2 and PostgreSQL-specific. Technically, what
    it does is to drop the schema "public", create it again and fix
    corresponding privileges. This doesn't work if for some reason the
    database was set up to use a different schema: this situation is
    strange enough for us to just ignore it.

    return (bool): True if successful.

    """
    connection = custom_psycopg2_connection()
    connection.autocommit = True
    cursor = connection.cursor()

    # See
    # http://stackoverflow.com/questions/3327312/drop-all-tables-in-postgresql
    try:
        cursor.execute("DROP SCHEMA public CASCADE")
    except ProgrammingError:
        logger.error("Couldn't drop schema \"public\", probably you don't "
                     "have the privileges. Please execute as database "
                     "superuser: \"ALTER SCHEMA public OWNER TO %s;\" and "
                     "run again" % make_url(config.database).username)
        return False
    cursor.execute("CREATE SCHEMA public")

    # But we also have to drop the large objects
    try:
        cursor.execute("SELECT oid FROM pg_largeobject_metadata")
    except ProgrammingError:
        logger.error("Couldn't list large objects, probably you don't have "
                     "the privileges. Please execute as database superuser: "
                     "\"GRANT SELECT ON pg_largeobject TO %s;\" and run "
                     "again" % make_url(config.database).username)
        return False
    rows = cursor.fetchall()
    for row in rows:
        cursor.execute("SELECT lo_unlink(%d)" % (row[0]))

    cursor.close()

    return True

########NEW FILE########
__FILENAME__ = filecacher
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2014 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Cache to store and retrieve files, assumed to be binary.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import hashlib
import io
import logging
import os
import tempfile

import gevent

from sqlalchemy.exc import IntegrityError

from cms import config, mkdir
from cms.db import SessionGen, FSObject
from cms.io.GeventUtils import copyfileobj, move, rmtree


logger = logging.getLogger(__name__)


class FileCacherBackend(object):
    """Abstract base class for all FileCacher backends.

    """

    def get_file(self, digest):
        """Retrieve a file from the storage.

        digest (unicode): the digest of the file to retrieve.

        return (fileobj): a readable binary file-like object from which
            to read the contents of the file.

        raise (KeyError): if the file cannot be found.

        """
        raise NotImplementedError("Please subclass this class.")

    def put_file(self, digest, desc=""):
        """Store a file to the storage.

        digest (unicode): the digest of the file to store.
        desc (unicode): the optional description of the file to
            store, intended for human beings.

        return (fileobj): a writable binary file-like object on which
            to write the contents of the file, or None if the file is
            already stored.

        """
        raise NotImplementedError("Please subclass this class.")

    def describe(self, digest):
        """Return the description of a file given its digest.

        digest (unicode): the digest of the file to describe.

        return (unicode): the description of the file.

        raise (KeyError): if the file cannot be found.

        """
        raise NotImplementedError("Please subclass this class.")

    def get_size(self, digest):
        """Return the size of a file given its digest.

        digest (unicode): the digest of the file to calculate the size
            of.

        return (int): the size of the file, in bytes.

        raise (KeyError): if the file cannot be found.

        """
        raise NotImplementedError("Please subclass this class.")

    def delete(self, digest):
        """Delete a file from the storage.

        digest (unicode): the digest of the file to delete.

        """
        raise NotImplementedError("Please subclass this class.")

    def list(self):
        """List the files available in the storage.

        return ([(unicode, unicode)]): a list of pairs, each
            representing a file in the form (digest, description).

        """
        raise NotImplementedError("Please subclass this class.")


class FSBackend(FileCacherBackend):
    """This class implements a backend for FileCacher that keeps all
    the files in a file system directory, named after their digest. Of
    course this directory can be shared, for example with NFS, acting
    as an actual remote file storage.

    TODO: Actually store the descriptions, that get discarded at the
    moment.

    TODO: Use an additional level of directories, to alleviate the
    work of the file system driver (e.g., 'ROOT/a/abcdef...' instead
    of 'ROOT/abcdef...'.

    """

    def __init__(self, path):
        """Initialize the backend.

        path (string): the base path for the storage.

        """
        self.path = path

        # Create the directory if it doesn't exist
        try:
            os.makedirs(self.path)
        except OSError:
            pass

    def get_file(self, digest):
        """See FileCacherBackend.get_file().

        """
        file_path = os.path.join(self.path, digest)

        if not os.path.exists(file_path):
            raise KeyError("File not found.")

        return io.open(file_path, 'rb')

    def put_file(self, digest, desc=""):
        """See FileCacherBackend.put_file().

        """
        file_path = os.path.join(self.path, digest)

        if os.path.exists(file_path):
            return None

        return io.open(file_path, 'wb')

    def describe(self, digest):
        """See FileCacherBackend.describe().

        """
        file_path = os.path.join(self.path, digest)

        if not os.path.exists(file_path):
            raise KeyError("File not found.")

        return ""

    def get_size(self, digest):
        """See FileCacherBackend.get_size().

        """
        file_path = os.path.join(self.path, digest)

        if not os.path.exists(file_path):
            raise KeyError("File not found.")

        return os.stat(file_path).st_size

    def delete(self, digest):
        """See FileCacherBackend.delete().

        """
        file_path = os.path.join(self.path, digest)

        try:
            os.unlink(file_path)
        except OSError:
            pass

    def list(self):
        """See FileCacherBackend.list().

        """
        return list((x, "") for x in os.listdir(self.path))


class DBBackend(FileCacherBackend):
    """This class implements an actual backend for FileCacher that
    stores the files as lobjects (encapsuled in a FSObject) into a
    PostgreSQL database.

    """

    def get_file(self, digest):
        """See FileCacherBackend.get_file().

        """
        with SessionGen() as session:
            fso = FSObject.get_from_digest(digest, session)

            if fso is None:
                raise KeyError("File not found.")

            return fso.get_lobject(mode='rb')

    def put_file(self, digest, desc=""):
        """See FileCacherBackend.put_file().

        """
        try:
            with SessionGen() as session:
                fso = FSObject.get_from_digest(digest, session)

                # Check digest uniqueness
                if fso is not None:
                    logger.debug("File %s already stored on database, not "
                                 "sending it again." % digest)
                    session.rollback()
                    return None

                # If it is not already present, copy the file into the
                # lobject
                else:
                    fso = FSObject(description=desc)
                    fso.digest = digest

                    session.add(fso)

                    logger.debug("File %s stored on the database." % digest)

                    # FIXME There is a remote possibility that someone
                    # will try to access this file, believing it has
                    # already been stored (since its FSObject exists),
                    # while we're still sending its content.

                    lobject = fso.get_lobject(mode='wb')

                    session.commit()

                    return lobject

        except IntegrityError:
            logger.warning("File %s caused an IntegrityError, ignoring..." %
                           digest)

    def describe(self, digest):
        """See FileCacherBackend.describe().

        """
        with SessionGen() as session:
            fso = FSObject.get_from_digest(digest, session)

            if fso is None:
                raise KeyError("File not found.")

            return fso.description

    def get_size(self, digest):
        """See FileCacherBackend.get_size().

        """
        # TODO - The business logic may be moved in FSObject, for
        # better generality
        with SessionGen() as session:
            fso = FSObject.get_from_digest(digest, session)

            if fso is None:
                raise KeyError("File not found.")

            with fso.get_lobject(mode='rb') as lobj:
                return lobj.seek(0, io.SEEK_END)

    def delete(self, digest):
        """See FileCacherBackend.delete().

        """
        with SessionGen() as session:
            fso = FSObject.get_from_digest(digest, session)

            if fso is None:
                session.rollback()
                return

            fso.delete()

            session.commit()

    def list(self, session=None):
        """See FileCacherBackend.list().

        This implementation also accepts an additional (and optional)
        parameter: a SQLAlchemy session to use to query the database.

        session (Session): the session to use; if not given a temporary
            one will be created and used.

        """
        def _list(session):
            """Do the work assuming session is valid.

            """
            return list((x.digest, x.description)
                        for x in session.query(FSObject))

        if session is not None:
            return _list(session)
        else:
            with SessionGen() as session:
                return _list(session)


class NullBackend(FileCacherBackend):
    """This backend is always empty, it just drops each file that
    receives. It looks mostly like /dev/null. It is useful when you
    want to just rely on the caching capabilities of FileCacher for
    very short-lived and local storages.

    """

    def get_file(self, digest):
        raise KeyError("File not found.")

    def put_file(self, digest, desc=""):
        return None

    def describe(self, digest):
        raise KeyError("File not found.")

    def get_size(self, digest):
        raise KeyError("File not found.")

    def delete(self, digest):
        pass

    def list(self):
        return list()


class FileCacher(object):
    """This class implement a local cache for files stored as FSObject
    in the database.

    """

    # This value is very arbitrary, and in this case we want it to be a
    # one-size-fits-all, since we use it for many conversions. It has
    # been chosen arbitrarily based on performance tests on my machine.
    # A few consideration on the value it could assume follow:
    # - The page size of large objects is LOBLKSIZE, which is BLCKSZ/4
    #   (BLCKSZ is the block size of the PostgreSQL database, which is
    #   set during pre-build configuration). BLCKSZ is by default 8192,
    #   therefore LOBLKSIZE is 2048. See:
    #   http://www.postgresql.org/docs/9.0/static/catalog-pg-largeobject.html
    # - The `io' module defines a DEFAULT_BUFFER_SIZE constant, whose
    #   value is 8192.
    # CHUNK_SIZE should be a multiple of these values.
    CHUNK_SIZE = 2 ** 14  # 16348

    def __init__(self, service=None, path=None, null=False):
        """Initialize.

        By default the database-powered backend will be used, but this
        can be changed using the parameters.

        service (Service): the service we are running for. Only used to
            determine the location of the file-system cache (and to
            provide the shard number to the Sandbox... sigh!).
        path (string): if specified, back the FileCacher with a file
            system-based storage instead of the default database-based
            one. The specified directory will be used as root for the
            storage and it will be created if it doesn't exist.
        null (bool): if True, back the FileCacher with a NullBackend,
            that just discards every file it receives. This setting
            takes priority over path.

        """
        self.service = service

        if null:
            self.backend = NullBackend()
        elif path is None:
            self.backend = DBBackend()
        else:
            self.backend = FSBackend(path)

        if service is None:
            self.file_dir = tempfile.mkdtemp(dir=config.temp_dir)
        else:
            self.file_dir = os.path.join(
                config.cache_dir,
                "fs-cache-%s-%d" % (service.name, service.shard))

        self.temp_dir = os.path.join(self.file_dir, "_temp")

        if not mkdir(config.cache_dir) or not mkdir(self.file_dir) \
                or not mkdir(self.temp_dir):
            logger.error("Cannot create necessary directories.")
            raise RuntimeError("Cannot create necessary directories.")

    def load(self, digest):
        """Load the file with the given digest into the cache.

        Ask the backend to provide the file and, if it's available,
        copy its content into the file-system cache.

        digest (unicode): the digest of the file to load.

        raise (KeyError): if the backend cannot find the file.

        """
        ftmp_handle, temp_file_path = tempfile.mkstemp(dir=self.temp_dir,
                                                       text=False)
        ftmp = os.fdopen(ftmp_handle, 'w')
        cache_file_path = os.path.join(self.file_dir, digest)

        fobj = self.backend.get_file(digest)

        # Copy the file to a temporary position
        try:
            copyfileobj(fobj, ftmp, self.CHUNK_SIZE)
        finally:
            ftmp.close()
            fobj.close()

        # Then move it to its real location (this operation is atomic
        # by POSIX requirement)
        os.rename(temp_file_path, cache_file_path)

    def get_file(self, digest):
        """Retrieve a file from the storage.

        If it's available in the cache use that copy, without querying
        the backend. Otherwise ask the backend to provide it, and store
        it in the cache for the benefit of future accesses.

        The file is returned as a file-object. Other interfaces are
        available as `get_file_content', `get_file_to_fobj' and `get_
        file_to_path'.

        digest (unicode): the digest of the file to get.

        return (fileobj): a readable binary file-like object from which
            to read the contents of the file.

        raise (KeyError): if the file cannot be found.

        """
        cache_file_path = os.path.join(self.file_dir, digest)

        logger.debug("Getting file %s." % digest)

        if not os.path.exists(cache_file_path):
            logger.debug("File %s not in cache, downloading "
                         "from database." % digest)

            self.load(digest)

            logger.debug("File %s downloaded." % digest)

        return io.open(cache_file_path, 'rb')

    def get_file_content(self, digest):
        """Retrieve a file from the storage.

        See `get_file'. This method returns the content of the file, as
        a binary string.

        digest (unicode): the digest of the file to get.

        return (bytes): the content of the retrieved file.

        raise (KeyError): if the file cannot be found.

        """
        with self.get_file(digest) as src:
            return src.read()

    def get_file_to_fobj(self, digest, dst):
        """Retrieve a file from the storage.

        See `get_file'. This method will write the content of the file
        to the given file-object.

        digest (unicode): the digest of the file to get.
        dst (fileobj): a writable binary file-like object on which to
            write the contents of the file.

        raise (KeyError): if the file cannot be found.

        """
        with self.get_file(digest) as src:
            copyfileobj(src, dst, self.CHUNK_SIZE)

    def get_file_to_path(self, digest, dst_path):
        """Retrieve a file from the storage.

        See `get_file'. This method will write the content of a file
        to the given file-system location.

        digest (unicode): the digest of the file to get.
        dst_path (string): an accessible location on the file-system on
            which to write the contents of the file.

        raise (KeyError): if the file cannot be found.

        """
        with self.get_file(digest) as src:
            with io.open(dst_path, 'wb') as dst:
                copyfileobj(src, dst, self.CHUNK_SIZE)

    def save(self, digest, desc=""):
        """Save the file with the given digest into the backend.

        Use to local copy, available in the file-system cache, to store
        the file in the backend, if it's not already there.

        digest (unicode): the digest of the file to load.
        desc (unicode): the (optional) description to associate to the
            file.

        """
        cache_file_path = os.path.join(self.file_dir, digest)

        fobj = self.backend.put_file(digest, desc)

        if fobj is None:
            return

        try:
            with io.open(cache_file_path, 'rb') as src:
                copyfileobj(src, fobj, self.CHUNK_SIZE)
        finally:
            fobj.close()

    def put_file_from_fobj(self, src, desc=""):
        """Store a file in the storage.

        If it's already (for some reason...) in the cache send that
        copy to the backend. Otherwise store it in the file-system
        cache first.

        The file is obtained from a file-object. Other interfaces are
        available as `put_file_content', `put_file_from_path'.

        src (fileobj): a readable binary file-like object from which
            to read the contents of the file.
        desc (unicode): the (optional) description to associate to the
            file.

        return (unicode): the digest of the stored file.

        """
        logger.debug("Reading input file to store on the database.")

        # Unfortunately, we have to read the whole file-obj to compute
        # the digest but we take that chance to save it to a temporary
        # path so that we then just need to move it. Hoping that both
        # locations will be on the same filesystem, that should be way
        # faster than reading the whole file-obj again (as it could be
        # compressed or require network communication).
        # XXX We're *almost* reimplementing copyfileobj.
        with tempfile.NamedTemporaryFile('wb', delete=False,
                                         dir=config.temp_dir) as dst:
            hasher = hashlib.sha1()
            buf = src.read(self.CHUNK_SIZE)
            while len(buf) > 0:
                hasher.update(buf)
                while len(buf) > 0:
                    written = dst.write(buf)
                    # Cooperative yield.
                    gevent.sleep(0)
                    if written is None:
                        break
                    buf = buf[written:]
                buf = src.read(self.CHUNK_SIZE)
            digest = hasher.hexdigest().decode("ascii")
            dst.flush()

            logger.debug("File has digest %s." % digest)

            cache_file_path = os.path.join(self.file_dir, digest)

            if not os.path.exists(cache_file_path):
                move(dst.name, cache_file_path)
            else:
                os.unlink(dst.name)

        # Store the file in the backend. We do that even if the file
        # was already in the cache (that is, we ignore the check above)
        # because there's a (small) chance that the file got removed
        # from the backend but somehow remained in the cache.
        self.save(digest, desc)

        return digest

    def put_file_content(self, content, desc=""):
        """Store a file in the storage.

        See `put_file_from_fobj'. This method will read the content of
        the file from the given binary string.

        content (bytes): the content of the file to store.
        desc (unicode): the (optional) description to associate to the
            file.

        return (unicode): the digest of the stored file.

        """
        with io.BytesIO(content) as src:
            return self.put_file_from_fobj(src, desc)

    def put_file_from_path(self, src_path, desc=""):
        """Store a file in the storage.

        See `put_file_from_fobj'. This method will read the content of
        the file from the given file-system location.

        src_path (string): an accessible location on the file-system
            from which to read the contents of the file.
        desc (unicode): the (optional) description to associate to the
            file.

        return (unicode): the digest of the stored file.

        """
        with io.open(src_path, 'rb') as src:
            return self.put_file_from_fobj(src, desc)

    def describe(self, digest):
        """Return the description of a file given its digest.

        digest (unicode): the digest of the file to describe.

        return (unicode): the description of the file.

        raise (KeyError): if the file cannot be found.

        """
        return self.backend.describe(digest)

    def get_size(self, digest):
        """Return the size of a file given its digest.

        digest (unicode): the digest of the file to calculate the size
            of.

        return (int): the size of the file, in bytes.

        raise (KeyError): if the file cannot be found.

        """
        return self.backend.get_size(digest)

    def delete(self, digest):
        """Delete a file from the backend and the local cache.

        digest (unicode): the digest of the file to delete.

        """
        self.drop(digest)
        self.backend.delete(digest)

    def drop(self, digest):
        """Delete a file only from the local cache.

        digest (unicode): the file to delete.

        """
        cache_file_path = os.path.join(self.file_dir, digest)

        try:
            os.unlink(cache_file_path)
        except OSError:
            pass

    def purge_cache(self):
        """Empty the local cache.

        """
        self.destroy_cache()
        if not mkdir(config.cache_dir) or not mkdir(self.file_dir):
            logger.error("Cannot create necessary directories.")
            raise RuntimeError("Cannot create necessary directories.")

    def destroy_cache(self):
        """Completely remove and destroy the cache.

        Nothing that could have been created by this object will be
        left on disk. After that, this instance isn't usable anymore.

        """
        rmtree(self.file_dir)

    def list(self):
        """List the files available in the storage.

        return ([(unicode, unicode)]): a list of pairs, each
            representing a file in the form (digest, description).

        """
        return self.backend.list()

    def check_backend_integrity(self, delete=False):
        """Check the integrity of the backend.

        Request all the files from the backend. For each of them the
        digest is recomputed and checked against the one recorded in
        the backend.

        If mismatches are found, they are reported with ERROR
        severity. The method returns False if at least a mismatch is
        found, True otherwise.

        delete (bool): if True, files with wrong digest are deleted.

        """
        clean = True
        for digest, description in self.list():
            fobj = self.backend.get_file(digest)
            hasher = hashlib.sha1()
            try:
                buf = fobj.read(self.CHUNK_SIZE)
                while len(buf) > 0:
                    hasher.update(buf)
                    buf = fobj.read(self.CHUNK_SIZE)
            finally:
                fobj.close()
            computed_digest = hasher.hexdigest().decode("ascii")
            if digest != computed_digest:
                logger.error("File with hash %s actually has hash %s" %
                             (digest, computed_digest))
                if delete:
                    self.delete(digest)
                clean = False

        return clean

########NEW FILE########
__FILENAME__ = fsobject
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""SQLAlchemy interfaces to store files in the database.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import io

import six

from sqlalchemy.schema import Column
from sqlalchemy.types import Integer, String, Unicode

import psycopg2
import psycopg2.extensions

from . import Base, custom_psycopg2_connection


class LargeObject(io.RawIOBase):

    """Present a PostgreSQL large object as a Python file-object.

    A LargeObject creates and maintains (i.e. closes when done) its
    own connection to the database. This approach is preferred over
    using one of the connections pooled by SQLAlchemy (for example by
    "borrowing" the one of the Session of the FSObject that created the
    LO instance, if any!) to make these objects independent from the
    Session (in particular, to allow them to live longer) and to avoid
    polluting the connections in the SQLAlchemy pool (because executing
    queries on the underlying DB API driver connection means kind of
    "abusing" the SQLAlchemy API, and also because we don't want to
    interfere with the life-cycle of these connections).

    We cannot use the lobject interface provided by psycopg2 because
    it's incompatible with asynchronous connections and thus coroutine
    libraries (like gevent). Hence, we use functions called via SQL
    queries to manipulate large objects on the server.

    This class extends RawIOBase and is therefore a fully-compliant
    file-like object. As for I/O, it just needs to provide the
    `readinto' and `write' methods, as all others (`read', `readall',
    `readline', `readlines', `writelines') are implemented in terms of
    these two by the parent class. See the official documentation for
    IOBase and RawIOBase to know the interface this class provides:
    http://docs.python.org/2.7/library/io.html

    """

    # Some constants from libpq, that are not published by psycopg2.
    INV_READ = 0x40000
    INV_WRITE = 0x20000

    def __init__(self, loid, mode='rb'):
        """Open a large object, creating it if required.

        loid (int): the large object ID.
        mode (string): how to open the file (`r' -> read, `w' -> write,
            `b' -> binary, which must be always specified). If not
            given, `rb' is used.

        """
        io.RawIOBase.__init__(self)

        self.loid = loid

        # Check mode value.
        mode = set(mode)
        if not mode.issubset('rwb'):
            raise ValueError("Only valid characters in mode are r, w and b.")
        if mode.isdisjoint('rw'):
            raise ValueError("Character r or b must be specified in mode.")
        if 'b' not in mode:
            raise ValueError("Character b must be specified in mode.")

        self._readable = 'r' in mode
        self._writable = 'w' in mode

        self._conn = custom_psycopg2_connection()
        cursor = self._conn.cursor()

        # If the loid is 0, create the large object.
        if self.loid == 0:
            creat_mode = LargeObject.INV_READ | LargeObject.INV_WRITE
            self.loid = self._execute("SELECT lo_creat(%(mode)s);",
                                      {'mode': creat_mode},
                                      "Couldn't create large object.", cursor)
            if self.loid == 0:
                raise IOError("Couldn't create large object.")

        # Open the large object.
        open_mode = (LargeObject.INV_READ if self._readable else 0) | \
                    (LargeObject.INV_WRITE if self._writable else 0)
        self._fd = self._execute("SELECT lo_open(%(loid)s, %(mode)s);",
                                 {'loid': self.loid, 'mode': open_mode},
                                 "Couldn't open large object.", cursor)

        cursor.close()

    def _execute(self, operation, parameters, message, cursor=None):
        """Run the given query making many success checks.

        Execute the given SQL statement, instantiated with the given
        parameters, on the given cursor, making sure the connection is
        in an usable condition and is left such! Also check that there
        is a single return value and, if it's a status code, that it's
        not negative.

        operation (unicode): the SQL query to perform, with named
            string  placeholders (i.e. "%(name)s").
        parameters (dict): the parameters to fill in the operation.
        message (unicode): a description to tell humans what we were
            doing in case something went wrong.
        cursor (cursor): the cursor to use to execute the statement
            (create and use a temporary one if not given).

        """
        if cursor is None:
            cursor = self._conn.cursor()
            res = self._execute(operation, parameters, message, cursor)
            cursor.close()
            return res

        try:
            assert self._conn.status in (
                psycopg2.extensions.STATUS_READY,
                psycopg2.extensions.STATUS_BEGIN)
            assert self._conn.get_transaction_status() in (
                psycopg2.extensions.TRANSACTION_STATUS_IDLE,
                psycopg2.extensions.TRANSACTION_STATUS_INTRANS)

            cursor.execute(operation, parameters)

            assert self._conn.status == \
                psycopg2.extensions.STATUS_BEGIN
            assert self._conn.get_transaction_status() == \
                psycopg2.extensions.TRANSACTION_STATUS_INTRANS

            res, = cursor.fetchone()

            assert len(cursor.fetchall()) == 0
            if isinstance(res, six.integer_types):
                assert res >= 0
        except (AssertionError, ValueError, psycopg2.DatabaseError):
            raise IOError(message)
        else:
            return res

    def readable(self):
        """See IOBase.readable().

        """
        return self._readable

    def writable(self):
        """See IOBase.writable().

        """
        return self._writable

    def seekable(self):
        """See IOBase.seekable().

        """
        return True

    @property
    def closed(self):
        """See IOBase.closed().

        """
        return self._fd is None

    def readinto(self, buf):
        """Read from the large object, and write to the given buffer.

        Try to read as much data as we can fit into buf. If less is
        obtained, stop and don't do further SQL calls. The number of
        retrieved bytes is returned.

        buf (bytearray): buffer into which to write data.

        return (int): the number of bytes read.

        """
        if self._fd is None:
            raise io.UnsupportedOperation("Large object is closed.")

        if not self._readable:
            raise io.UnsupportedOperation("Large object hasn't been "
                                          "opened in 'read' mode.")

        data = self._execute("SELECT loread(%(fd)s, %(len)s);",
                             {'fd': self._fd, 'len': len(buf)},
                             "Couldn't write to large object.")
        buf[:len(data)] = data
        return len(data)

    def write(self, buf):
        """Write to the large object, reading from the given buffer.

        Try to write as much data as we have available. If less is
        stored, stop and don't do further SQL calls. The number of sent
        bytes is returned.

        buf (bytes or bytearray): buffer from which to read data.

        return (int): the number of bytes written.

        """
        if self._fd is None:
            raise io.UnsupportedOperation("Large object is closed.")

        if not self._writable:
            raise io.UnsupportedOperation("Large object hasn't been "
                                          "opened in 'write' mode.")

        len_ = self._execute("SELECT lowrite(%(fd)s, %(buf)s);",
                             {'fd': self._fd, 'buf': psycopg2.Binary(buf)},
                             "Couldn't write to large object.")
        return len_

    def seek(self, offset, whence=io.SEEK_SET):
        """Move the stream position in large object.

        offset (int): offset from the reference point.
        whence (int): reference point, expressed like in os.seek().

        return (int): the new absolute position.

        """
        if self._fd is None:
            raise io.UnsupportedOperation("Large object is closed.")

        pos = self._execute("SELECT lo_lseek(%(fd)s, %(offset)s, %(whence)s);",
                            {'fd': self._fd,
                             'offset': offset,
                             'whence': whence},
                            "Couldn't seek large object.")
        return pos

    def tell(self):
        """Tell the stream position in a large object.

        return (int): the absolute position.

        """
        if self._fd is None:
            raise io.UnsupportedOperation("Large object is closed.")

        pos = self._.execute("SELECT lo_tell(%(fd)s);",
                             {'fd': self._fd},
                             "Couldn't tell large object.")
        return pos

    def truncate(self, size=None):
        """Trucate a large object.

        size (int): the desired new size. If not given defaults to
            current position.

        return (int): the new actual size.

        """
        if self._fd is None:
            raise io.UnsupportedOperation("Large object is closed.")

        if not self._writable:
            raise io.UnsupportedOperation("Large object hasn't been "
                                          "opened in 'write' mode.")

        if size is None:
            size = self.tell()

        self._execute("SELECT lo_truncate(%(fd)s, %(size)s);",
                      {'fd': self._fd, 'size': size},
                      "Couldn't truncate large object.")
        return size

    def close(self):
        """Close the large object.

        After this call the object is not usable anymore. It is
        allowed to close an object more than once, with the calls
        after the first doing nothing.

        """
        # If the large object has already been closed, don't close it
        # again.
        if self._fd is None:
            return

        self._execute("SELECT lo_close(%(fd)s);",
                      {'fd': self._fd},
                      "Couldn't close large object.")

        self._conn.commit()

        # We delete the fd number to avoid writing on another file by
        # mistake
        self._fd = None

    @staticmethod
    def unlink(loid, conn=None):
        """Delete the large object, removing its content.

        After an unlink, the content can't be restored anymore, so use
        with caution!

        """
        if conn is None:
            conn = custom_psycopg2_connection()
            conn.autocommit = True

        # FIXME Use context manager as soon as we require psycopg2-2.5.
        cursor = conn.cursor()
        cursor.execute("SELECT lo_unlink(%(loid)s);",
                       {'loid': loid})
        cursor.close()


class FSObject(Base):
    """Class to describe a file stored in the database.

    """

    __tablename__ = 'fsobjects'

    # Here we use the digest (SHA1 sum) of the file as primary key;
    # ideally all the columns that refer to digests could be declared
    # as foreign keys against this column, but we intentionally avoid
    # doing this to keep the database uncoupled from the file storage.
    digest = Column(
        String,
        primary_key=True,
        nullable=False)

    # OID of the large object in the database
    loid = Column(
        Integer,
        nullable=False,
        default=0)

    # Human-readable description, primarily meant for debugging (i.e,
    # should have no semantic value from the viewpoint of CMS)
    description = Column(
        Unicode,
        nullable=True)

    def get_lobject(self, mode='rb'):
        """Return an open file bound to the represented large object.

        The returned value acts as a context manager, so it can be used
        inside a `with' clause, this way:

            with fsobject.get_lobject() as lobj:

        mode (string): how to open the file (`r' -> read, `w' -> write,
             `b' -> binary, which must be always specified). If not
             given, `rb' is used.

        """
        # Here we rely on the fact that we're using psycopg2 as
        # PostgreSQL backend.
        lobj = LargeObject(self.loid, mode)
        self.loid = lobj.loid

        # FIXME Wrap with a io.BufferedReader/Writer/Random?
        return lobj

    def delete(self):
        """Delete this file.

        """
        LargeObject.unlink(self.loid)
        self.sa_session.delete(self)

    @classmethod
    def get_from_digest(cls, digest, session):
        """Return the FSObject with the specified digest, using the
        specified session.

        """
        return cls.get_from_id(digest, session)

    @classmethod
    def get_all(cls, session):
        """Iterate over all the FSObjects available in the database.

        """
        if cls.__table__.exists():
            return session.query(cls)
        else:
            return []

    @classmethod
    def delete_all(cls, session):
        """Delete all files stored in the database. This cannot be
        undone. Large objects not linked by some FSObject cannot be
        detected at the moment, so they don't get deleted.

        """
        for fso in cls.get_all(session):
            fso.delete()

########NEW FILE########
__FILENAME__ = init
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from . import metadata


def init_db():
    """Initialize the database.

    return (bool): True if successful.

    """
    metadata.create_all()

    return True

########NEW FILE########
__FILENAME__ = session
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Utilities related to SQLAlchemy sessions.

Contains context managers and custom methods to create sessions to
interact with SQLAlchemy objects.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import psycopg2

from sqlalchemy.orm import sessionmaker, scoped_session
from sqlalchemy.engine.url import make_url

from cms import config

from . import engine


Session = sessionmaker(engine, twophase=config.twophase_commit)
ScopedSession = scoped_session(Session)

# For two-phases transactions:
# Session = sessionmaker(db, twophase=True)


class SessionGen(object):
    """This allows us to create handy local sessions simply with:

    with SessionGen() as session:
        session.do_something()

    and at the end the session is automatically rolled back and
    closed. If one wants to commit the session, they have to call
    commit() explicitly.

    """
    def __init__(self):
        self.session = None

    def __enter__(self):
        self.session = Session()
        return self.session

    def __exit__(self, unused1, unused2, unused3):
        self.session.rollback()
        self.session.close()


def custom_psycopg2_connection(**kwargs):
    """Establish a new psycopg2.connection to the database.

    The returned connection won't be in the SQLAlchemy pool and has to
    be closed manually by the caller when it's done with it.

    All psycopg2-specific code in CMS is supposed to obtain a function
    this way.

    kwargs (dict): additional values to use as query parameters in the
        connection URL.

    return (connection): a new, shiny connection object.

    raise (AssertionError): if CMS (actually SQLAlchemy) isn't
        configured to use psycopg2 as the DB-API driver.

    """
    database_url = make_url(config.database)
    assert database_url.get_dialect().driver == "psycopg2"
    database_url.query.update(kwargs)

    return psycopg2.connect(
        host=database_url.host,
        port=database_url.port,
        user=database_url.username,
        password=database_url.password,
        database=database_url.database,
        **database_url.query)

########NEW FILE########
__FILENAME__ = smartmappedcollection
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import weakref

from sqlalchemy import util
from sqlalchemy import event
from sqlalchemy.orm import class_mapper
from sqlalchemy.orm.collections import \
    collection, collection_adapter, MappedCollection, \
    __set as sa_set, __del as sa_del


# XXX When dropping support for SQLAlchemy pre-0.7.6, remove these two
# lines.
from sqlalchemy.orm.collections import _instrument_class
_instrument_class(MappedCollection)


# XXX When SQLAlchemy will support removal of attribute events, remove
# the following class and global variable:

class _EventManager(object):
    def __init__(self):
        self.handlers = dict()

    def listen(self, cls, prp, handler):
        if (cls, prp) not in self.handlers:
            self.handlers[(cls, prp)] = weakref.WeakSet()
            event.listen(class_mapper(cls)._props[prp],
                         'set', self.make_callback(cls, prp))

        self.handlers[(cls, prp)].add(handler)

    # No need to enable handlers to be removed: they'll automatically
    # vanish when they are garbage collected, as they are weakrefs.

    def make_callback(self, cls, prp):
        def callback(target, new_key, old_key, _sa_initiator):
            for handler in self.handlers[(cls, prp)]:
                handler._on_column_change(target, new_key, old_key,
                                          _sa_initiator)
        return callback


_event_manager = _EventManager()


class SmartMappedCollection(MappedCollection):

    def __init__(self, column):
        self._column = column

        self._linked = False

        self._parent_rel = None
        self._parent_obj = None
        self._parent_cls = None
        self._child_rel = None
        self._child_cls = None

    def __eq__(self, other):
        return self is other

    def __hash__(self):
        return hash(id(self))

    # XXX When dropping support for SQLAlchemy pre-0.8, rename this
    # decorator to 'collection.linker'.
    @collection.link
    def _link(self, adapter):
        assert adapter == collection_adapter(self)

        if adapter is not None:
            # LINK
            assert not self._linked
            self._linked = True

            assert self is adapter.data

            self._parent_rel = adapter.attr.key
            self._parent_obj = adapter.owner_state.obj()
            self._parent_cls = type(self._parent_obj)
            parent_rel_prop = \
                class_mapper(self._parent_cls)._props[self._parent_rel]
            self._child_rel = parent_rel_prop.back_populates
            self._child_cls = parent_rel_prop.mapper.class_
            # This is at the moment not used, but may be in the future.
            # child_rel_prop = \
            #     class_mapper(self._child_cls)._props[self._child_rel]

            # XXX When SQLAlchemy will support removal of attribute
            # events, use the following code:
            #event.listen(class_mapper(self._child_cls)._props[self._column],
            #             'set', self._on_column_change)
            # In the meanwhile we have to use this:
            _event_manager.listen(self._child_cls, self._column, self)

        else:
            # UNLINK
            assert self._linked
            self._linked = False

            # XXX When SQLAlchemy will support removal of attribute
            # events, use the following code:
            #event.remove(class_mapper(self._child_cls)._props[self._column],
            #             'set', self._on_column_change)
            # In the meanwhile we just rely on EventManager + weakrefs.

            self._parent_rel = None
            self._parent_obj = None
            self._parent_cls = None
            self._child_rel = None
            self._child_cls = None

    # XXX When dropping support for SQLAlchemy pre-0.8, remove this line.
    _sa_on_link = _link

    # The following two methods do all the hard work. Their mission is
    # to keep everything consistent, that is to get from a (hopefully
    # consistent) initial state to a consistent final state after
    # having dome something useful (i.e. what they were written for).
    # This is what we consider a consistent state:
    # - self.values() is equal to all and only those objects of the
    #   self._child_cls class that have the self._child_rel attribute
    #   set to self._parent_obj;
    # - all elements of self.values() are distinct (that is, this is an
    #   invertible mapping);
    # - for each (key, value) in self.items(), key is equal to the
    #   self._column attribute of value.

    # This method is called before the attribute is really changed, and
    # trying to change it again from inside this method will cause an
    # infinte recursion loop. Don't do that! This also means that the
    # method does actually leave the collection in an inconsistent
    # state, but SQLAlchemy will fix that immediately after.
    def _on_column_change(self, value, new_key, old_key, _sa_initiator):
        assert self._linked
        if getattr(value, self._child_rel) is self._parent_obj:
            # Get the old_key (the parameter may not be reliable) and
            # do some consistency checks.
            assert value in self.itervalues()
            old_key = list(k for k, v in self.iteritems() if v is value)
            assert len(old_key) == 1
            old_key = old_key[0]
            assert old_key == getattr(value, self._column)

            # If necessary, move this object (and remove any old object
            # with this key).
            if new_key != old_key:
                dict.__delitem__(self, old_key)
                if new_key in self:
                    sa_del(self,
                           dict.__getitem__(self, new_key),
                           _sa_initiator)
                    dict.__delitem__(self, new_key)
                dict.__setitem__(self, new_key, value)

    # When this method gets called, the child object may think it's
    # already bound to the collection (i.e. its self._child_rel is set
    # to self._parent_obj) but it actually isn't (i.e. it's not in
    # self.values()). This method has to fix that.
    @collection.internally_instrumented
    def __setitem__(self, new_key, value, _sa_initiator=None):
        # TODO We could check if the object's type is correct.
        assert self._linked
        if value in self.itervalues():
            # Just some consistency checks, for extra safety!
            assert getattr(value, self._child_rel) is self._parent_obj
            old_key = list(k for k, v in self.iteritems() if v is value)
            assert len(old_key) == 1
            old_key = old_key[0]
            assert old_key == getattr(value, self._column)

            # If needed, we make SQLAlchemy call _on_column_changed to
            # do the rest of the job (and repeat the above checks).
            if new_key != getattr(value, self._column):
                setattr(value, self._column, new_key)
        else:
            # We change the attribute before adding it to the collection
            # to prevent the (unavoidable) call to _on_column_change
            # from doing any damage.
            if new_key != getattr(value, self._column):
                setattr(value, self._column, new_key)

            # Remove any old object with this key and add this instead.
            if new_key in self:
                sa_del(self, dict.__getitem__(self, new_key), _sa_initiator)
                dict.__delitem__(self, new_key)
            value = sa_set(self, value, _sa_initiator)
            dict.__setitem__(self, new_key, value)

    def keyfunc(self, value):
        return getattr(value, self._column)

    @collection.converter
    def _convert(self, collection):
        # TODO We could check if the objects' type is correct.
        type_ = util.duck_type_collection(collection)
        if type_ is dict:
            for key, value in util.dictlike_iteritems(collection):
                if key != self.keyfunc(value):
                    raise TypeError(
                        "Found incompatible key '%r' for value '%r'" %
                        (key, value))
                yield value
        elif type_ in (list, set):
            for value in collection:
                yield value
        else:
            raise TypeError("Object '%r' is not dict-like nor iterable" %
                            collection)

    # XXX When dropping support for SQLAlchemy pre-0.8, remove this.
    _sa_converter = _convert

    def __iadd__(self, collection):
        for value in self._convert(collection):
            self.set(value)
        return self


def smart_mapped_collection(column):
    return lambda: SmartMappedCollection(column)

########NEW FILE########
__FILENAME__ = submission
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Bernard Blackham <bernard@largestprime.net>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Submission-related database interface for SQLAlchemy.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from sqlalchemy.schema import Column, ForeignKey, ForeignKeyConstraint, \
    UniqueConstraint
from sqlalchemy.types import Integer, Float, String, Unicode, DateTime
from sqlalchemy.orm import relationship, backref

from . import Base, User, Task, Dataset, Testcase
from .smartmappedcollection import smart_mapped_collection

from cmscommon.datetime import make_datetime


class Submission(Base):
    """Class to store a submission.

    """
    __tablename__ = 'submissions'

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # User (id and object) that did the submission.
    user_id = Column(
        Integer,
        ForeignKey(User.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    user = relationship(
        User,
        backref=backref("submissions",
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Task (id and object) of the submission.
    task_id = Column(
        Integer,
        ForeignKey(Task.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    task = relationship(
        Task,
        backref=backref("submissions",
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Time of the submission.
    timestamp = Column(
        DateTime,
        nullable=False)

    # Language of submission, or None if not applicable.
    language = Column(
        String,
        nullable=True)

    # Follows the description of the fields automatically added by
    # SQLAlchemy.
    # files (dict of File objects indexed by filename)
    # token (Token object or None)
    # results (list of SubmissionResult objects)

    def get_result(self, dataset=None):
        """Return the result associated to a dataset.

        dataset (Dataset|None): the dataset for which the caller wants
            the submission result; if None, the active one is used.

        return (SubmissionResult|None): the submission result
            associated to this submission and the given dataset, if it
            exists in the database, otherwise None.

        """
        if dataset is not None:
            # Use IDs to avoid triggering a lazy-load query.
            assert self.task_id == dataset.task_id
            dataset_id = dataset.id
        else:
            dataset_id = self.task.active_dataset_id

        return SubmissionResult.get_from_id(
            (self.id, dataset_id), self.sa_session)

    def get_result_or_create(self, dataset=None):
        """Return and, if necessary, create the result for a dataset.

        dataset (Dataset|None): the dataset for which the caller wants
            the submission result; if None, the active one is used.

        return (SubmissionResult): the submission result associated to
            the this submission and the given dataset; if it
            does not exists, a new one is created.

        """
        if dataset is None:
            dataset = self.task.active_dataset

        submission_result = self.get_result(dataset)

        if submission_result is None:
            submission_result = SubmissionResult(submission=self,
                                                 dataset=dataset)

        return submission_result

    def tokened(self):
        """Return if the user played a token against the submission.

        return (bool): True if tokened, False otherwise.

        """
        return self.token is not None


class File(Base):
    """Class to store information about one file submitted within a
    submission.

    """
    __tablename__ = 'files'
    __table_args__ = (
        UniqueConstraint('submission_id', 'filename'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Submission (id and object) owning the file.
    submission_id = Column(
        Integer,
        ForeignKey(Submission.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    submission = relationship(
        Submission,
        backref=backref('files',
                        collection_class=smart_mapped_collection('filename'),
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Filename and digest of the submitted file.
    filename = Column(
        Unicode,
        nullable=False)
    digest = Column(
        String,
        nullable=False)


class Token(Base):
    """Class to store information about a token.

    """
    __tablename__ = 'tokens'
    __table_args__ = (
        UniqueConstraint('submission_id'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Submission (id and object) the token has been used on.
    submission_id = Column(
        Integer,
        ForeignKey(Submission.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    submission = relationship(
        Submission,
        backref=backref(
            "token",
            uselist=False,
            cascade="all, delete-orphan",
            passive_deletes=True),
        single_parent=True)

    # Time the token was played.
    timestamp = Column(
        DateTime,
        nullable=False,
        default=make_datetime)


class SubmissionResult(Base):
    """Class to store the evaluation results of a submission.

    """
    __tablename__ = 'submission_results'
    __table_args__ = (
        UniqueConstraint('submission_id', 'dataset_id'),
    )

    # Primary key is (submission_id, dataset_id).
    submission_id = Column(
        Integer,
        ForeignKey(Submission.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        primary_key=True)
    submission = relationship(
        Submission,
        backref=backref(
            "results",
            cascade="all, delete-orphan",
            passive_deletes=True))

    dataset_id = Column(
        Integer,
        ForeignKey(Dataset.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        primary_key=True)
    dataset = relationship(
        Dataset)

    # Now below follow the actual result fields.

    # Compilation outcome (can be None = yet to compile, "ok" =
    # compilation successful and we can evaluate, "fail" =
    # compilation unsuccessful, throw it away).
    compilation_outcome = Column(
        String,
        nullable=True)

    # String containing output from the sandbox.
    compilation_text = Column(
        String,
        nullable=True)

    # Number of attempts of compilation.
    compilation_tries = Column(
        Integer,
        nullable=False,
        default=0)

    # The compiler stdout and stderr.
    compilation_stdout = Column(
        Unicode,
        nullable=True)
    compilation_stderr = Column(
        Unicode,
        nullable=True)

    # Other information about the compilation.
    compilation_time = Column(
        Float,
        nullable=True)
    compilation_wall_clock_time = Column(
        Float,
        nullable=True)
    compilation_memory = Column(
        Integer,
        nullable=True)

    # Worker shard and sandbox where the compilation was performed.
    compilation_shard = Column(
        Integer,
        nullable=True)
    compilation_sandbox = Column(
        Unicode,
        nullable=True)

    # Evaluation outcome (can be None = yet to evaluate, "ok" =
    # evaluation successful). At any time, this should be equal to
    # evaluations != [].
    evaluation_outcome = Column(
        String,
        nullable=True)

    # Number of attempts of evaluation.
    evaluation_tries = Column(
        Integer,
        nullable=False,
        default=0)

    # Score as computed by ScoringService. Null means not yet scored.
    score = Column(
        Float,
        nullable=True)

    # Score details. It's a JSON-encoded string containing information
    # that is given to ScoreType.get_html_details to generate an HTML
    # snippet that is shown on AWS and, if the user used a token, on
    # CWS to display the details of the submission.
    # For example, results for each testcases, subtask, etc.
    score_details = Column(
        String,
        nullable=True)

    # The same as the last two fields, but from the point of view of
    # the user (when he/she did not play a token).
    public_score = Column(
        Float,
        nullable=True)
    public_score_details = Column(
        String,
        nullable=True)

    # Ranking score details. It is a list of strings that are going to
    # be shown in a single row in the table of submission in RWS. JSON
    # encoded.
    ranking_score_details = Column(
        String,
        nullable=True)

    # Follows the description of the fields automatically added by
    # SQLAlchemy.
    # executables (dict of Executable objects indexed by filename)
    # evaluations (list of Evaluation objects)

    def get_evaluation(self, testcase):
        """Return the Evaluation of this SR on the given Testcase, if any

        testcase (Testcase): the testcase the returned evaluation will
                             belong to
        return (Evaluation): the (only!) evaluation of this submission
                             result on the given testcase, or None if
                             there isn't any.

        """
        # Use IDs to avoid triggering a lazy-load query.
        assert self.dataset_id == testcase.dataset_id

        # XXX If self.evaluations is already loaded we can walk over it
        # and spare a query.
        # (We could use .one() and avoid a LIMIT but we would need to
        # catch a NoResultFound exception.)
        self.sa_session.query(Evaluation)\
            .filter(Evaluation.submission_result == self)\
            .filter(Evaluation.testcase == testcase)\
            .first()

    def compiled(self):
        """Return whether the submission result has been compiled.

        return (bool): True if compiled, False otherwise.

        """
        return self.compilation_outcome is not None

    def compilation_failed(self):
        """Return whether the submission result did not compile.

        return (bool): True if the compilation failed (in the sense
            that there is a problem in the user's source), False if
            not yet compiled or compilation was successful.

        """
        return self.compilation_outcome == "fail"

    def compilation_succeeded(self):
        """Return whether the submission compiled.

        return (bool): True if the compilation succeeded (in the sense
            that an executable was created), False if not yet compiled
            or compilation was unsuccessful.

        """
        return self.compilation_outcome == "ok"

    def evaluated(self):
        """Return whether the submission result has been evaluated.

        return (bool): True if evaluated, False otherwise.

        """
        return self.evaluation_outcome is not None

    def needs_scoring(self):
        """Return whether the submission result needs to be scored.

        return (bool): True if in need of scoring, False otherwise.

        """
        return (self.compilation_failed() or self.evaluated()) and \
            not self.scored()

    def scored(self):
        """Return whether the submission result has been scored.

        return (bool): True if scored, False otherwise.

        """
        return all(getattr(self, k) is not None for k in [
            "score", "score_details",
            "public_score", "public_score_details",
            "ranking_score_details"])

    def invalidate_compilation(self):
        """Blank all compilation and evaluation outcomes, and the score.

        """
        self.invalidate_evaluation()
        self.compilation_outcome = None
        self.compilation_text = None
        self.compilation_tries = 0
        self.compilation_time = None
        self.compilation_wall_clock_time = None
        self.compilation_memory = None
        self.compilation_shard = None
        self.compilation_sandbox = None
        self.executables = {}

    def invalidate_evaluation(self):
        """Blank the evaluation outcomes and the score.

        """
        self.invalidate_score()
        self.evaluation_outcome = None
        self.evaluation_tries = 0
        self.evaluations = []

    def invalidate_score(self):
        """Blank the score.

        """
        self.score = None
        self.score_details = None
        self.public_score = None
        self.public_score_details = None
        self.ranking_score_details = None

    def set_compilation_outcome(self, success):
        """Set the compilation outcome based on the success.

        success (bool): if the compilation was successful.

        """
        self.compilation_outcome = "ok" if success else "fail"

    def set_evaluation_outcome(self):
        """Set the evaluation outcome (always ok now).

        """
        self.evaluation_outcome = "ok"


class Executable(Base):
    """Class to store information about one file generated by the
    compilation of a submission.

    """
    __tablename__ = 'executables'
    __table_args__ = (
        ForeignKeyConstraint(
            ('submission_id', 'dataset_id'),
            (SubmissionResult.submission_id, SubmissionResult.dataset_id),
            onupdate="CASCADE", ondelete="CASCADE"),
        UniqueConstraint('submission_id', 'dataset_id', 'filename'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Submission (id and object) owning the executable.
    submission_id = Column(
        Integer,
        ForeignKey(Submission.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    submission = relationship(
        Submission)

    # Dataset (id and object) owning the executable.
    dataset_id = Column(
        Integer,
        ForeignKey(Dataset.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    dataset = relationship(
        Dataset)

    # SubmissionResult owning the executable.
    submission_result = relationship(
        SubmissionResult,
        backref=backref('executables',
                        collection_class=smart_mapped_collection('filename'),
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Filename and digest of the generated executable.
    filename = Column(
        Unicode,
        nullable=False)
    digest = Column(
        String,
        nullable=False)


class Evaluation(Base):
    """Class to store information about the outcome of the evaluation
    of a submission against one testcase.

    """
    __tablename__ = 'evaluations'
    __table_args__ = (
        ForeignKeyConstraint(
            ('submission_id', 'dataset_id'),
            (SubmissionResult.submission_id, SubmissionResult.dataset_id),
            onupdate="CASCADE", ondelete="CASCADE"),
        UniqueConstraint('submission_id', 'dataset_id', 'testcase_id'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Submission (id and object) owning the evaluation.
    submission_id = Column(
        Integer,
        ForeignKey(Submission.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    submission = relationship(
        Submission)

    # Dataset (id and object) owning the evaluation.
    dataset_id = Column(
        Integer,
        ForeignKey(Dataset.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    dataset = relationship(
        Dataset)

    # SubmissionResult owning the evaluation.
    submission_result = relationship(
        SubmissionResult,
        backref=backref('evaluations',
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Testcase (id and object) this evaluation was performed on.
    testcase_id = Column(
        Integer,
        ForeignKey(Testcase.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    testcase = relationship(
        Testcase)

    # String containing the outcome of the evaluation (usually 1.0,
    # ...) not necessary the points awarded, that will be computed by
    # the score type.
    outcome = Column(
        Unicode,
        nullable=True)

    # String containing output from the grader (usually "Correct",
    # "Time limit", ...).
    text = Column(
        String,
        nullable=True)

    # Evaluation's time and wall-clock time, in seconds.
    execution_time = Column(
        Float,
        nullable=True)
    execution_wall_clock_time = Column(
        Float,
        nullable=True)

    # Memory used by the evaluation, in bytes.
    execution_memory = Column(
        Integer,
        nullable=True)

    # Worker shard and sandbox where the evaluation was performed.
    evaluation_shard = Column(
        Integer,
        nullable=True)
    evaluation_sandbox = Column(
        Unicode,
        nullable=True)

    @property
    def codename(self):
        """Return the codename of the testcase."""
        return self.testcase.codename

########NEW FILE########
__FILENAME__ = task
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012-2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Bernard Blackham <bernard@largestprime.net>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Task-related database interface for SQLAlchemy.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from datetime import timedelta

from sqlalchemy.schema import Column, ForeignKey, CheckConstraint, \
    UniqueConstraint, ForeignKeyConstraint
from sqlalchemy.types import Boolean, Integer, Float, String, Unicode, \
    Interval, Enum
from sqlalchemy.orm import relationship, backref
from sqlalchemy.ext.orderinglist import ordering_list

from . import Base, Contest
from .smartmappedcollection import smart_mapped_collection


class Task(Base):
    """Class to store a task.

    """
    __tablename__ = 'tasks'
    __table_args__ = (
        UniqueConstraint('contest_id', 'num'),
        UniqueConstraint('contest_id', 'name'),
        ForeignKeyConstraint(
            ("id", "active_dataset_id"),
            ("datasets.task_id", "datasets.id"),
            onupdate="SET NULL", ondelete="SET NULL",
            # Use an ALTER query to set this foreign key after
            # both tables have been CREATEd, to avoid circular
            # dependencies.
            use_alter=True,
            name="fk_active_dataset_id"
        ),
        CheckConstraint("token_gen_initial <= token_gen_max"),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True,
        # Needed to enable autoincrement on integer primary keys that
        # are referenced by a foreign key defined on this table.
        autoincrement='ignore_fk')

    # Number of the task for sorting.
    num = Column(
        Integer,
        nullable=False)

    # Contest (id and object) owning the task.
    contest_id = Column(
        Integer,
        ForeignKey(Contest.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    contest = relationship(
        Contest,
        backref=backref('tasks',
                        collection_class=ordering_list('num'),
                        order_by=[num],
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Short name and long human readable title of the task.
    name = Column(
        Unicode,
        nullable=False)
    title = Column(
        Unicode,
        nullable=False)

    # A JSON-encoded lists of strings: the language codes of the
    # statements that will be highlighted to all users for this task.
    primary_statements = Column(
        String,
        nullable=False,
        default="[]")

    # The parameters that control task-tokens follow. Note that their
    # effect during the contest depends on the interaction with the
    # parameters that control contest-tokens, defined on the Contest.

    # The "kind" of token rules that will be active during the contest.
    # - disabled: The user will never be able to use any token.
    # - finite: The user has a finite amount of tokens and can choose
    #   when to use them, subject to some limitations. Tokens may not
    #   be all available at start, but given periodically during the
    #   contest instead.
    # - infinite: The user will always be able to use a token.
    token_mode = Column(
        Enum("disabled", "finite", "infinite", name="token_mode"),
        nullable=False,
        default="disabled")

    # The maximum number of tokens a contestant is allowed to use
    # during the whole contest (on this tasks).
    token_max_number = Column(
        Integer,
        CheckConstraint("token_max_number > 0"),
        nullable=True)

    # The minimum interval between two successive uses of tokens for
    # the same user (on this task).
    token_min_interval = Column(
        Interval,
        CheckConstraint("token_min_interval >= '0 seconds'"),
        nullable=False,
        default=timedelta())

    # The parameters that control generation (if mode is "finite"):
    # the user starts with "initial" tokens and receives "number" more
    # every "interval", but their total number is capped to "max".
    token_gen_initial = Column(
        Integer,
        CheckConstraint("token_gen_initial >= 0"),
        nullable=False,
        default=2)
    token_gen_number = Column(
        Integer,
        CheckConstraint("token_gen_number >= 0"),
        nullable=False,
        default=2)
    token_gen_interval = Column(
        Interval,
        CheckConstraint("token_gen_interval > '0 seconds'"),
        nullable=False,
        default=timedelta(minutes=30))
    token_gen_max = Column(
        Integer,
        CheckConstraint("token_gen_max > 0"),
        nullable=True)

    # Maximum number of submissions or user_tests allowed for each user
    # on this task during the whole contest or None to not enforce
    # this limitation.
    max_submission_number = Column(
        Integer,
        CheckConstraint("max_submission_number > 0"),
        nullable=True)
    max_user_test_number = Column(
        Integer,
        CheckConstraint("max_user_test_number > 0"),
        nullable=True)

    # Minimum interval between two submissions or user_tests for this
    # task, or None to not enforce this limitation.
    min_submission_interval = Column(
        Interval,
        CheckConstraint("min_submission_interval > '0 seconds'"),
        nullable=True)
    min_user_test_interval = Column(
        Interval,
        CheckConstraint("min_user_test_interval > '0 seconds'"),
        nullable=True)

    # The scores for this task will be rounded to this number of
    # decimal places.
    score_precision = Column(
        Integer,
        CheckConstraint("score_precision >= 0"),
        nullable=False,
        default=0)

    # Active Dataset (id and object) currently being used for scoring.
    # The ForeignKeyConstraint for this column is set at table-level.
    active_dataset_id = Column(
        Integer,
        nullable=True)
    active_dataset = relationship(
        'Dataset',
        foreign_keys=[active_dataset_id],
        # XXX In SQLAlchemy 0.8 we could remove this:
        primaryjoin='Task.active_dataset_id == Dataset.id',
        # Use an UPDATE query *after* an INSERT query (and *before* a
        # DELETE query) to set (and unset) the column associated to
        # this relationship.
        post_update=True)

    # Follows the description of the fields automatically added by
    # SQLAlchemy.
    # datasets (list of Dataset objects)
    # statements (dict of Statement objects indexed by language code)
    # attachments (dict of Attachment objects indexed by filename)
    # submission_format (list of SubmissionFormatElement objects)
    # submissions (list of Submission objects)
    # user_tests (list of UserTest objects)


class Statement(Base):
    """Class to store a translation of the task statement.

    """
    __tablename__ = 'statements'
    __table_args__ = (
        UniqueConstraint('task_id', 'language'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Task (id and object) the statement is for.
    task_id = Column(
        Integer,
        ForeignKey(Task.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    task = relationship(
        Task,
        backref=backref('statements',
                        collection_class=smart_mapped_collection('language'),
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Code for the language the statement is written in.
    # It can be an arbitrary string, but if it's in the form "en" or "en_US"
    # it will be rendered appropriately on the interface (i.e. "English" and
    # "English (United States of America)"). These codes need to be taken from
    # ISO 639-1 and ISO 3166-1 respectively.
    language = Column(
        Unicode,
        nullable=False)

    # Digest of the file.
    digest = Column(
        String,
        nullable=False)


class Attachment(Base):
    """Class to store additional files to give to the user together
    with the statement of the task.

    """
    __tablename__ = 'attachments'
    __table_args__ = (
        UniqueConstraint('task_id', 'filename'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Task (id and object) owning the attachment.
    task_id = Column(
        Integer,
        ForeignKey(Task.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    task = relationship(
        Task,
        backref=backref('attachments',
                        collection_class=smart_mapped_collection('filename'),
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Filename and digest of the provided attachment.
    filename = Column(
        Unicode,
        nullable=False)
    digest = Column(
        String,
        nullable=False)


class SubmissionFormatElement(Base):
    """Class to store the requested files that a submission must
    include. Filenames may include %l to represent an accepted
    language extension.

    """
    __tablename__ = 'submission_format_elements'

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Task (id and object) owning the submission format element.
    task_id = Column(
        Integer,
        ForeignKey(Task.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    task = relationship(
        Task,
        backref=backref('submission_format',
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Format of the given submission file.
    filename = Column(
        Unicode,
        nullable=False)


class Dataset(Base):
    """Class to store the information about a data set.

    """
    __tablename__ = 'datasets'
    __table_args__ = (
        UniqueConstraint('task_id', 'description'),
        # Useless, in theory, because 'id' is already unique. Yet, we
        # need this because it's a target of a foreign key.
        UniqueConstraint('id', 'task_id'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Task (id and object) owning the dataset.
    task_id = Column(
        Integer,
        ForeignKey(Task.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False)
    task = relationship(
        Task,
        foreign_keys=[task_id],
        # XXX In SQLAlchemy 0.8 we could remove this:
        primaryjoin='Task.id == Dataset.task_id',
        backref=backref('datasets',
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # A human-readable text describing the dataset.
    description = Column(
        Unicode,
        nullable=False)

    # Whether this dataset will be automatically judged by ES and SS
    # "in background", together with the active dataset of each task.
    autojudge = Column(
        Boolean,
        nullable=False,
        default=False)

    # Time and memory limits for every testcase.
    time_limit = Column(
        Float,
        nullable=True)
    memory_limit = Column(
        Integer,
        nullable=True)

    # Name of the TaskType child class suited for the task.
    task_type = Column(
        String,
        nullable=False)

    # Parameters for the task type class, JSON encoded.
    task_type_parameters = Column(
        String,
        nullable=False)

    # Name of the ScoreType child class suited for the task.
    score_type = Column(
        String,
        nullable=False)

    # Parameters for the score type class, JSON encoded.
    score_type_parameters = Column(
        String,
        nullable=False)

    # Follows the description of the fields automatically added by
    # SQLAlchemy.
    # managers (dict of Manager objects indexed by filename)
    # testcases (dict of Testcase objects indexed by codename)


class Manager(Base):
    """Class to store additional files needed to compile or evaluate a
    submission (e.g., graders).

    """
    __tablename__ = 'managers'
    __table_args__ = (
        UniqueConstraint('dataset_id', 'filename'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Dataset (id and object) owning the manager.
    dataset_id = Column(
        Integer,
        ForeignKey(Dataset.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    dataset = relationship(
        Dataset,
        backref=backref('managers',
                        collection_class=smart_mapped_collection('filename'),
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Filename and digest of the provided manager.
    filename = Column(
        Unicode,
        nullable=False)
    digest = Column(
        String,
        nullable=False)


class Testcase(Base):
    """Class to store the information about a testcase.

    """
    __tablename__ = 'testcases'
    __table_args__ = (
        UniqueConstraint('dataset_id', 'codename'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Dataset (id and object) owning the testcase.
    dataset_id = Column(
        Integer,
        ForeignKey(Dataset.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    dataset = relationship(
        Dataset,
        backref=backref('testcases',
                        collection_class=smart_mapped_collection('codename'),
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Codename identifying the testcase.
    codename = Column(
        Unicode,
        nullable=False)

    # If the testcase outcome is going to be showed to the user (even
    # without playing a token).
    public = Column(
        Boolean,
        nullable=False,
        default=False)

    # Digests of the input and output files.
    input = Column(
        String,
        nullable=False)
    output = Column(
        String,
        nullable=False)

########NEW FILE########
__FILENAME__ = types
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Custom types for SQLAlchemy.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from sqlalchemy.types import TypeDecorator, Unicode


class RepeatedUnicode(TypeDecorator):
    """Implement (short) lists of unicode strings.

    All values need to contain some non-whitespace characters and no
    comma. Leading and trailing whitespace will be stripped.

    """
    impl = Unicode

    def process_bind_param(self, value, unused_dialect):
        """Encode value in a single unicode.

        value ([unicode]): the list to encode.

        return (unicode): the unicode string encoding value.

        raise (ValueError): if some string contains "," or is composed
            only by whitespace.

        """
        # This limitation may be removed if necessary.
        if any("," in val for val in value):
            raise ValueError("Comma cannot be encoded.")
        if any(len(val) == 0 or val.isspace() for val in value):
            raise ValueError("Cannot be only whitespace.")
        return ",".join(val.strip() for val in value)

    def process_result_value(self, value, unused_dialect):
        """Decode values from a single unicode.

        value (unicode): the unicode string to decode.

        return ([unicode]): the decoded list.

        """
        return list(val.strip() for val in value.split(",")
                    if len(val) > 0 and not val.isspace())

########NEW FILE########
__FILENAME__ = user
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""User-related database interface for SQLAlchemy.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from datetime import timedelta

from sqlalchemy.schema import Column, ForeignKey, UniqueConstraint
from sqlalchemy.types import Boolean, Integer, String, Unicode, DateTime, \
    Interval
from sqlalchemy.orm import relationship, backref

from . import Base, Contest


def generate_random_password():
    import random
    chars = "abcdefghijklmnopqrstuvwxyz"
    return "".join([random.choice(chars) for _ in xrange(6)])


class User(Base):
    """Class to store a 'user participating in a contest'.

    """
    # TODO: we really need to split this as a user (as in: not paired
    # with a contest) and a participation.
    __tablename__ = 'users'
    __table_args__ = (
        UniqueConstraint('contest_id', 'username'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Real name (human readable) of the user.
    first_name = Column(
        Unicode,
        nullable=False)
    last_name = Column(
        Unicode,
        nullable=False)

    # Username and password to log in the CWS.
    username = Column(
        Unicode,
        nullable=False)
    password = Column(
        Unicode,
        nullable=False,
        default=generate_random_password)

    # Email for any communications in case of remote contest.
    email = Column(
        Unicode,
        nullable=True)

    # User can log in CWS only from this IP address or subnet.
    ip = Column(
        Unicode,
        nullable=True)

    # A hidden user is used only for debugging purpose.
    hidden = Column(
        Boolean,
        nullable=False,
        default=False)

    # Contest (id and object) to which the user is participating.
    contest_id = Column(
        Integer,
        ForeignKey(Contest.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    contest = relationship(
        Contest,
        backref=backref("users",
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # A JSON-encoded dictionary of lists of strings: statements["a"]
    # contains the language codes of the statements that will be
    # highlighted to this user for task "a".
    primary_statements = Column(
        String,
        nullable=False,
        default="{}")

    # Timezone for the user. All timestamps in CWS will be shown using
    # the timezone associated to the logged-in user or (if it's None
    # or an invalid string) the timezone associated to the contest or
    # (if it's None or an invalid string) the local timezone of the
    # server. This value has to be a string like "Europe/Rome",
    # "Australia/Sydney", "America/New_York", etc.
    timezone = Column(
        Unicode,
        nullable=True)

    # Starting time: for contests where every user has at most x hours
    # of the y > x hours totally available, this is the time the user
    # decided to start his/her time-frame.
    starting_time = Column(
        DateTime,
        nullable=True)

    # An extra amount of time allocated for this user.
    extra_time = Column(
        Interval,
        nullable=False,
        default=timedelta())

    # Follows the description of the fields automatically added by
    # SQLAlchemy.
    # messages (list of Message objects)
    # questions (list of Question objects)
    # submissions (list of Submission objects)
    # user_tests (list of UserTest objects)

    # Moreover, we have the following methods.
    # get_tokens (defined in __init__.py)


class Message(Base):
    """Class to store a private message from the managers to the
    user.

    """
    __tablename__ = 'messages'

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Time the message was sent.
    timestamp = Column(
        DateTime,
        nullable=False)

    # Subject and body of the message.
    subject = Column(
        Unicode,
        nullable=False)
    text = Column(
        Unicode,
        nullable=False)

    # User (id and object) owning the message.
    user_id = Column(
        Integer,
        ForeignKey(User.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    user = relationship(
        User,
        backref=backref('messages',
                        order_by=[timestamp],
                        cascade="all, delete-orphan",
                        passive_deletes=True))


class Question(Base):
    """Class to store a private question from the user to the
    managers, and its answer.

    """
    __tablename__ = 'questions'

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # Time the question was made.
    question_timestamp = Column(
        DateTime,
        nullable=False)

    # Subject and body of the question.
    subject = Column(
        Unicode,
        nullable=False)
    text = Column(
        Unicode,
        nullable=False)

    # Time the reply was sent.
    reply_timestamp = Column(
        DateTime,
        nullable=True)

    # Has this message been ignored by the admins?
    ignored = Column(
        Boolean,
        nullable=False,
        default=False)

    # Short (as in 'chosen amongst some predetermined choices') and
    # long answer.
    reply_subject = Column(
        Unicode,
        nullable=True)
    reply_text = Column(
        Unicode,
        nullable=True)

    # User (id and object) owning the question.
    user_id = Column(
        Integer,
        ForeignKey(User.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    user = relationship(
        User,
        backref=backref('questions',
                        order_by=[question_timestamp, reply_timestamp],
                        cascade="all, delete-orphan",
                        passive_deletes=True))

########NEW FILE########
__FILENAME__ = usertest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2012-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""UserTest-related database interface for SQLAlchemy.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from sqlalchemy.schema import Column, ForeignKey, ForeignKeyConstraint, \
    UniqueConstraint
from sqlalchemy.types import Integer, Float, String, Unicode, DateTime
from sqlalchemy.orm import relationship, backref

from . import Base, User, Task, Dataset
from .smartmappedcollection import smart_mapped_collection


class UserTest(Base):
    """Class to store a test requested by a user.

    """
    __tablename__ = 'user_tests'

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # User (id and object) that requested the test.
    user_id = Column(
        Integer,
        ForeignKey(User.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    user = relationship(
        User,
        backref=backref("user_tests",
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Task (id and object) of the test.
    task_id = Column(
        Integer,
        ForeignKey(Task.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    task = relationship(
        Task,
        backref=backref("user_tests",
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Time of the request.
    timestamp = Column(
        DateTime,
        nullable=False)

    # Language of test, or None if not applicable.
    language = Column(
        String,
        nullable=True)

    # Input (provided by the user) file's digest for this test.
    input = Column(
        String,
        nullable=False)

    # Follows the description of the fields automatically added by
    # SQLAlchemy.
    # files (dict of UserTestFile objects indexed by filename)
    # managers (dict of UserTestManager objects indexed by filename)
    # results (list of UserTestResult objects)

    def get_result(self, dataset=None):
        """Return the result associated to a dataset.

        dataset (Dataset|None): the dataset for which the caller wants
            the user test result; if None, the active one is used.

        return (UserTestResult|None): the user test result associated
            to this user test and the given dataset, if it exists in
            the database, otherwise None.

        """
        if dataset is not None:
            # Use IDs to avoid triggering a lazy-load query.
            assert self.task_id == dataset.task_id
            dataset_id = dataset.id
        else:
            dataset_id = self.task.active_dataset_id

        return UserTestResult.get_from_id(
            (self.id, dataset_id), self.sa_session)

    def get_result_or_create(self, dataset=None):
        """Return and, if necessary, create the result for a dataset.

        dataset (Dataset|None): the dataset for which the caller wants
            the user test result; if None, the active one is used.

        return (UserTestResult): the user test result associated to
            the this user test and the given dataset; if it does not
            exists, a new one is created.

        """
        if dataset is None:
            dataset = self.task.active_dataset

        user_test_result = self.get_result(dataset)

        if user_test_result is None:
            user_test_result = UserTestResult(user_test=self,
                                              dataset=dataset)

        return user_test_result


class UserTestFile(Base):
    """Class to store information about one file submitted within a
    user_test.

    """
    __tablename__ = 'user_test_files'
    __table_args__ = (
        UniqueConstraint('user_test_id', 'filename'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # UserTest (id and object) owning the file.
    user_test_id = Column(
        Integer,
        ForeignKey(UserTest.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    user_test = relationship(
        UserTest,
        backref=backref('files',
                        collection_class=smart_mapped_collection('filename'),
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Filename and digest of the submitted file.
    filename = Column(
        String,
        nullable=False)
    digest = Column(
        String,
        nullable=False)


class UserTestManager(Base):
    """Class to store additional files needed to compile or evaluate a
    user test (e.g., graders).

    """
    __tablename__ = 'user_test_managers'
    __table_args__ = (
        UniqueConstraint('user_test_id', 'filename'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # UserTest (id and object) owning the manager.
    user_test_id = Column(
        Integer,
        ForeignKey(UserTest.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    user_test = relationship(
        UserTest,
        backref=backref('managers',
                        collection_class=smart_mapped_collection('filename'),
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Filename and digest of the submitted manager.
    filename = Column(
        String,
        nullable=False)
    digest = Column(
        String,
        nullable=False)


class UserTestResult(Base):
    """Class to store the execution results of a user_test.

    """
    __tablename__ = 'user_test_results'
    __table_args__ = (
        UniqueConstraint('user_test_id', 'dataset_id'),
    )

    # Primary key is (user_test_id, dataset_id).
    user_test_id = Column(
        Integer,
        ForeignKey(UserTest.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        primary_key=True)
    user_test = relationship(
        UserTest,
        backref=backref(
            "results",
            cascade="all, delete-orphan",
            passive_deletes=True))

    dataset_id = Column(
        Integer,
        ForeignKey(Dataset.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        primary_key=True)
    dataset = relationship(
        Dataset)

    # Now below follow the actual result fields.

    # Output file's digest for this test
    output = Column(
        String,
        nullable=True)

    # Compilation outcome (can be None = yet to compile, "ok" =
    # compilation successful and we can evaluate, "fail" =
    # compilation unsuccessful, throw it away).
    compilation_outcome = Column(
        String,
        nullable=True)

    # String containing output from the sandbox.
    compilation_text = Column(
        String,
        nullable=True)

    # Number of attempts of compilation.
    compilation_tries = Column(
        Integer,
        nullable=False,
        default=0)

    # The compiler stdout and stderr.
    compilation_stdout = Column(
        Unicode,
        nullable=True)
    compilation_stderr = Column(
        Unicode,
        nullable=True)

    # Other information about the compilation.
    compilation_time = Column(
        Float,
        nullable=True)
    compilation_wall_clock_time = Column(
        Float,
        nullable=True)
    compilation_memory = Column(
        Integer,
        nullable=True)

    # Worker shard and sandbox where the compilation was performed.
    compilation_shard = Column(
        Integer,
        nullable=True)
    compilation_sandbox = Column(
        String,
        nullable=True)

    # Evaluation outcome (can be None = yet to evaluate, "ok" =
    # evaluation successful).
    evaluation_outcome = Column(
        String,
        nullable=True)
    evaluation_text = Column(
        String,
        nullable=True)

    # Number of attempts of evaluation.
    evaluation_tries = Column(
        Integer,
        nullable=False,
        default=0)

    # Other information about the execution.
    execution_time = Column(
        Float,
        nullable=True)
    execution_wall_clock_time = Column(
        Float,
        nullable=True)
    execution_memory = Column(
        Integer,
        nullable=True)

    # Worker shard and sandbox where the evaluation was performed.
    evaluation_shard = Column(
        Integer,
        nullable=True)
    evaluation_sandbox = Column(
        String,
        nullable=True)

    # Follows the description of the fields automatically added by
    # SQLAlchemy.
    # executables (dict of UserTestExecutable objects indexed by filename)

    def compiled(self):
        """Return whether the user test result has been compiled.

        return (bool): True if compiled, False otherwise.

        """
        return self.compilation_outcome is not None

    def compilation_failed(self):
        """Return whether the user test result did not compile.

        return (bool): True if the compilation failed (in the sense
            that there is a problem in the user's source), False if
            not yet compiled or compilation was successful.

        """
        return self.compilation_outcome == "fail"

    def compilation_succeeded(self):
        """Return whether the user test compiled.

        return (bool): True if the compilation succeeded (in the sense
            that an executable was created), False if not yet compiled
            or compilation was unsuccessful.

        """
        return self.compilation_outcome == "ok"

    def evaluated(self):
        """Return whether the user test result has been evaluated.

        return (bool): True if evaluated, False otherwise.

        """
        return self.evaluation_outcome is not None

    def invalidate_compilation(self):
        """Blank all compilation and evaluation outcomes.

        """
        self.invalidate_evaluation()
        self.compilation_outcome = None
        self.compilation_text = None
        self.compilation_tries = 0
        self.compilation_time = None
        self.compilation_wall_clock_time = None
        self.compilation_memory = None
        self.compilation_shard = None
        self.compilation_sandbox = None
        self.executables = {}

    def invalidate_evaluation(self):
        """Blank the evaluation outcome.

        """
        self.evaluation_outcome = None
        self.evaluation_text = None
        self.evaluation_tries = 0
        self.execution_time = None
        self.execution_wall_clock_time = None
        self.execution_memory = None
        self.evaluation_shard = None
        self.evaluation_sandbox = None
        self.output = None

    def set_compilation_outcome(self, success):
        """Set the compilation outcome based on the success.

        success (bool): if the compilation was successful.

        """
        self.compilation_outcome = "ok" if success else "fail"

    def set_evaluation_outcome(self):
        """Set the evaluation outcome (always ok now).

        """
        self.evaluation_outcome = "ok"


class UserTestExecutable(Base):
    """Class to store information about one file generated by the
    compilation of a user test.

    """
    __tablename__ = 'user_test_executables'
    __table_args__ = (
        ForeignKeyConstraint(
            ('user_test_id', 'dataset_id'),
            (UserTestResult.user_test_id, UserTestResult.dataset_id),
            onupdate="CASCADE", ondelete="CASCADE"),
        UniqueConstraint('user_test_id', 'dataset_id', 'filename'),
    )

    # Auto increment primary key.
    id = Column(
        Integer,
        primary_key=True)

    # UserTest (id and object) owning the executable.
    user_test_id = Column(
        Integer,
        ForeignKey(UserTest.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    user_test = relationship(
        UserTest)

    # Dataset (id and object) owning the executable.
    dataset_id = Column(
        Integer,
        ForeignKey(Dataset.id,
                   onupdate="CASCADE", ondelete="CASCADE"),
        nullable=False,
        index=True)
    dataset = relationship(
        Dataset)

    # UserTestResult owning the executable.
    user_test_result = relationship(
        UserTestResult,
        backref=backref('executables',
                        collection_class=smart_mapped_collection('filename'),
                        cascade="all, delete-orphan",
                        passive_deletes=True))

    # Filename and digest of the generated executable.
    filename = Column(
        String,
        nullable=False)
    digest = Column(
        String,
        nullable=False)

########NEW FILE########
__FILENAME__ = util
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Utilities relying on the database.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import sys

from . import SessionGen, Contest


def get_contest_list(session=None):
    """Return all the contest objects available on the database.

    session (Session): if specified, use such session for connecting
        to the database; otherwise, create a temporary one and discard
        it after the operation (this means that no further expansion
        of lazy properties of the returned Contest objects will be
        possible).

    return ([Contest]): the list of contests in the DB.

    """
    if session is None:
        with SessionGen() as session:
            return get_contest_list(session)

    return session.query(Contest).all()


def is_contest_id(contest_id):
    """Return if there is a contest with the given id in the database.

    contest_id (int): the id to query.
    return (boolean): True if there is such a contest.

    """
    with SessionGen() as session:
        return Contest.get_from_id(contest_id, session) is not None


def ask_for_contest(skip=None):
    """Print a greeter that ask the user for a contest, if there is
    not an indication of which contest to use in the command line.

    skip (int|None): how many commandline arguments are already taken
        by other usages (None for no arguments already consumed).

    return (int): a contest_id.

    """
    if isinstance(skip, int) and len(sys.argv) > skip + 1:
        contest_id = int(sys.argv[skip + 1])

    else:

        with SessionGen() as session:
            contests = get_contest_list(session)
            # The ids of the contests are cached, so the session can
            # be closed as soon as possible
            matches = {}
            n_contests = len(contests)
            if n_contests == 0:
                print("No contests in the database.")
                print("You may want to use some of the facilities in "
                      "cmscontrib to import a contest.")
                sys.exit(0)
            print("Contests available:")
            for i, row in enumerate(contests):
                print("%3d  -  ID: %d  -  Name: %s  -  Description: %s" %
                      (i + 1, row.id, row.name, row.description), end='')
                matches[i + 1] = row.id
                if i == n_contests - 1:
                    print(" (default)")
                else:
                    print()

        contest_number = raw_input("Insert the row number next to the contest "
                                   "you want to load (not the id): ")
        if contest_number == "":
            contest_number = n_contests
        try:
            contest_id = matches[int(contest_number)]
        except (ValueError, KeyError):
            print("Insert a correct number.")
            sys.exit(1)

    return contest_id

########NEW FILE########
__FILENAME__ = Job
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Bernard Blackham <bernard@largestprime.net>
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A JobGroup is an abstraction of an "atomic" action of a Worker.

Jobs play a major role in the interface with TaskTypes: they are a
data structure containing all information about what the TaskTypes
should do. They are mostly used in the communication between ES and
the Workers, hence they contain only serializable data (for example,
the name of the task type, not the task type object itself).

A JobGroup represents an indivisible action of a Worker, that is, a
compilation or an evaluation. It contains one or more Jobs, for
example "compile the submission" or "evaluate the submission on a
certain testcase".

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import json
from copy import deepcopy

from cms.db import File, Manager, Executable, UserTestExecutable, Evaluation


class Job(object):
    """Base class for all jobs.

    Input data (usually filled by ES): task_type,
    task_type_parameters. Metadata: shard, sandboxes, info.

    """

    # TODO Move 'success' inside Job.

    def __init__(self, task_type=None, task_type_parameters=None,
                 shard=None, sandboxes=None, info=None):
        """Initialization.

        task_type (string): the name of the task type.
        task_type_parameters (string): the parameters for the creation
            of the correct task type.
        shard (int): the shard of the Worker completing this job.
        sandboxes ([string]): the paths of the sandboxes used in the
            Worker during the execution of the job.
        info (string): a human readable description of the job.

        """
        if task_type is None:
            task_type = ""
        if task_type_parameters is None:
            task_type_parameters = []
        if sandboxes is None:
            sandboxes = []
        if info is None:
            info = ""

        self.task_type = task_type
        self.task_type_parameters = task_type_parameters
        self.shard = shard
        self.sandboxes = sandboxes
        self.info = info

    def export_to_dict(self):
        res = {
            'task_type': self.task_type,
            'task_type_parameters': self.task_type_parameters,
            'shard': self.shard,
            'sandboxes': self.sandboxes,
            'info': self.info,
            }
        return res

    @staticmethod
    def import_from_dict_with_type(data):
        type_ = data['type']
        del data['type']
        if type_ == 'compilation':
            return CompilationJob.import_from_dict(data)
        elif type_ == 'evaluation':
            return EvaluationJob.import_from_dict(data)
        else:
            raise Exception("Couldn't import dictionary with type %s" %
                            (type_))

    @classmethod
    def import_from_dict(cls, data):
        return cls(**data)


class CompilationJob(Job):
    """Job representing a compilation.

    Can represent either the compilation of a user test, or of a
    submission, or of an arbitrary source (as used in cmsMake).

    Input data (usually filled by ES): language, files,
    managers. Output data (filled by the Worker): success,
    compilation_success, executables, text, plus.

    """

    def __init__(self, task_type=None, task_type_parameters=None,
                 shard=None, sandboxes=None, info=None,
                 language=None, files=None, managers=None,
                 success=None, compilation_success=None,
                 executables=None, text=None, plus=None):
        """Initialization.

        See base class for the remaining arguments.

        language (string): the language of the submission / user test.
        files ({string: File}): files submitted by the user.
        managers ({string: Manager}): managers provided by the admins.
        success (bool): whether the job succeeded.
        compilation_success (bool): whether the compilation implicit
            in the job succeeded, or there was a compilation error.
        executables ({string: Executable}): executables created in the
            job.
        text ([object]): description of the outcome of the job, to be
            presented to the user. The first item is a string,
            potentially with %-escaping; the following items are the
            values to be %-formatted into the first.
        plus ({}): additional metadata.

        """
        if files is None:
            files = {}
        if managers is None:
            managers = {}
        if executables is None:
            executables = {}

        Job.__init__(self, task_type, task_type_parameters,
                     shard, sandboxes, info)
        self.language = language
        self.files = files
        self.managers = managers
        self.success = success
        self.compilation_success = compilation_success
        self.executables = executables
        self.text = text
        self.plus = plus

    def export_to_dict(self):
        res = Job.export_to_dict(self)
        res.update({
            'type': 'compilation',
            'language': self.language,
            'files': dict((k, v.digest)
                          for k, v in self.files.iteritems()),
            'managers': dict((k, v.digest)
                             for k, v in self.managers.iteritems()),
            'success': self.success,
            'compilation_success': self.compilation_success,
            'executables': dict((k, v.digest)
                                for k, v in self.executables.iteritems()),
            'text': self.text,
            'plus': self.plus,
            })
        return res

    @classmethod
    def import_from_dict(cls, data):
        data['files'] = dict(
            (k, File(k, v)) for k, v in data['files'].iteritems())
        data['managers'] = dict(
            (k, Manager(k, v)) for k, v in data['managers'].iteritems())
        data['executables'] = dict(
            (k, Executable(k, v)) for k, v in data['executables'].iteritems())
        return cls(**data)


class EvaluationJob(Job):
    """Job representing an evaluation on a testcase.

    Can represent either the evaluation of a user test, or of a
    submission, or of an arbitrary source (as used in cmsMake).

    Input data (usually filled by ES): language, files, managers,
    executables, input, output, time_limit, memory_limit. Output data
    (filled by the Worker): success, outcome, text, user_output, plux.
    executables, text, plus. Metadata: only_execution, get_output.

    """
    def __init__(self, task_type=None, task_type_parameters=None,
                 shard=None, sandboxes=None, info=None,
                 language=None, files=None, managers=None,
                 executables=None, input=None, output=None,
                 time_limit=None, memory_limit=None,
                 success=None, outcome=None, text=None,
                 user_output=None, plus=None,
                 only_execution=False, get_output=False):
        """Initialization.

        See base class for the remaining arguments.

        language (string): the language of the submission / user test.
        files ({string: File}): files submitted by the user.
        managers ({string: Manager}): managers provided by the admins.
        executables ({string: Executable}): executables created in the
            compilation.
        input (string): digest of the input file.
        output (string): digest of the output file.
        time_limit (float): user time limit in seconds.
        memory_limit (int): memory limit in bytes.
        success (bool): whether the job succeeded.
        outcome (string): the outcome of the evaluation, from which to
            compute the score.
        text ([object]): description of the outcome of the job, to be
            presented to the user. The first item is a string,
            potentially with %-escaping; the following items are the
            values to be %-formatted into the first.
        user_output (unicode): if requested (with get_output), the
            digest of the file containing the output of the user
            program.
        plus ({}): additional metadata.
        only_execution (bool): whether to perform only the execution,
            or to compare the output with the reference solution too.
        get_output (bool): whether to retrieve the execution output
            (together with only_execution, useful for the user tests).

        """
        if files is None:
            files = {}
        if managers is None:
            managers = {}
        if executables is None:
            executables = {}

        Job.__init__(self, task_type, task_type_parameters,
                     shard, sandboxes, info)
        self.language = language
        self.files = files
        self.managers = managers
        self.executables = executables
        self.input = input
        self.output = output
        self.time_limit = time_limit
        self.memory_limit = memory_limit
        self.success = success
        self.outcome = outcome
        self.text = text
        self.user_output = user_output
        self.plus = plus
        self.only_execution = only_execution
        self.get_output = get_output

    def export_to_dict(self):
        res = Job.export_to_dict(self)
        res.update({
            'type': 'evaluation',
            'language': self.language,
            'files': dict((k, v.digest)
                          for k, v in self.files.iteritems()),
            'managers': dict((k, v.digest)
                             for k, v in self.managers.iteritems()),
            'executables': dict((k, v.digest)
                                for k, v in self.executables.iteritems()),
            'input': self.input,
            'output': self.output,
            'time_limit': self.time_limit,
            'memory_limit': self.memory_limit,
            'success': self.success,
            'outcome': self.outcome,
            'text': self.text,
            'user_output': self.user_output,
            'plus': self.plus,
            'only_execution': self.only_execution,
            'get_output': self.get_output,
            })
        return res

    @classmethod
    def import_from_dict(cls, data):
        data['files'] = dict(
            (k, File(k, v)) for k, v in data['files'].iteritems())
        data['managers'] = dict(
            (k, Manager(k, v)) for k, v in data['managers'].iteritems())
        data['executables'] = dict(
            (k, Executable(k, v)) for k, v in data['executables'].iteritems())
        return cls(**data)


class JobGroup(object):
    """A collection of jobs.

    This is the minimal unit of action for a Worker.

    """

    def __init__(self, jobs=None, success=None):
        """Initialization.

        jobs ({string: Job}): the jobs composing the group.
        success (bool): whether all jobs succeded.

        """
        if jobs is None:
            jobs = {}

        self.jobs = jobs
        self.success = success

    def export_to_dict(self):
        res = {
            'jobs': dict((k, v.export_to_dict())
                         for k, v in self.jobs.iteritems()),
            'success': self.success,
            }
        return res

    @classmethod
    def import_from_dict(cls, data):
        data['jobs'] = dict(
            (k, Job.import_from_dict_with_type(v))
            for k, v in data['jobs'].iteritems())
        return cls(**data)

    # Compilation

    @staticmethod
    def from_submission_compilation(submission, dataset):
        job = CompilationJob()

        # Job
        job.task_type = dataset.task_type
        job.task_type_parameters = dataset.task_type_parameters

        # CompilationJob; dict() is required to detach the dictionary
        # that gets added to the Job from the control of SQLAlchemy
        job.language = submission.language
        job.files = dict(submission.files)
        job.managers = dict(dataset.managers)
        job.info = "compile submission %d" % (submission.id)

        jobs = {"": job}

        return JobGroup(jobs)

    def to_submission_compilation(self, sr):
        # This should actually be useless.
        sr.invalidate_compilation()

        job = self.jobs[""]
        assert isinstance(job, CompilationJob)

        # No need to check self.success or job.success because this
        # method gets called only if the first (and therefore the
        # second!) is True.

        sr.set_compilation_outcome(job.compilation_success)
        sr.compilation_text = json.dumps(job.text, encoding='utf-8')
        sr.compilation_stdout = job.plus.get('stdout')
        sr.compilation_stderr = job.plus.get('stderr')
        sr.compilation_time = job.plus.get('execution_time')
        sr.compilation_wall_clock_time = \
            job.plus.get('execution_wall_clock_time')
        sr.compilation_memory = job.plus.get('execution_memory')
        sr.compilation_shard = job.shard
        sr.compilation_sandbox = ":".join(job.sandboxes)
        for executable in job.executables.itervalues():
            sr.executables += [executable]

    @staticmethod
    def from_user_test_compilation(user_test, dataset):
        job = CompilationJob()

        # Job
        job.task_type = dataset.task_type
        job.task_type_parameters = dataset.task_type_parameters

        # CompilationJob; dict() is required to detach the dictionary
        # that gets added to the Job from the control of SQLAlchemy
        job.language = user_test.language
        job.files = dict(user_test.files)
        job.managers = dict(user_test.managers)
        job.info = "compile user test %d" % (user_test.id)

        # Add the managers to be got from the Task; get_task_type must
        # be imported here to avoid circular dependencies
        from cms.grading.tasktypes import get_task_type
        task_type = get_task_type(dataset=dataset)
        auto_managers = task_type.get_auto_managers()
        if auto_managers is not None:
            for manager_filename in auto_managers:
                job.managers[manager_filename] = \
                    dataset.managers[manager_filename]
        else:
            for manager_filename in dataset.managers:
                if manager_filename not in job.managers:
                    job.managers[manager_filename] = \
                        dataset.managers[manager_filename]

        jobs = {"": job}

        return JobGroup(jobs)

    def to_user_test_compilation(self, ur):
        # This should actually be useless.
        ur.invalidate_compilation()

        job = self.jobs[""]
        assert isinstance(job, CompilationJob)

        # No need to check self.success or job.success because this
        # method gets called only if the first (and therefore the
        # second!) is True.

        ur.set_compilation_outcome(job.compilation_success)
        ur.compilation_text = json.dumps(job.text, encoding='utf-8')
        ur.compilation_stdout = job.plus.get('stdout')
        ur.compilation_stderr = job.plus.get('stderr')
        ur.compilation_time = job.plus.get('execution_time')
        ur.compilation_wall_clock_time = \
            job.plus.get('execution_wall_clock_time')
        ur.compilation_memory = job.plus.get('execution_memory')
        ur.compilation_shard = job.shard
        ur.compilation_sandbox = ":".join(job.sandboxes)
        for executable in job.executables.itervalues():
            ut_executable = UserTestExecutable(
                executable.filename, executable.digest)
            ur.executables += [ut_executable]

    # Evaluation

    @staticmethod
    def from_submission_evaluation(submission, dataset):
        job = EvaluationJob()

        # Job
        job.task_type = dataset.task_type
        job.task_type_parameters = dataset.task_type_parameters

        submission_result = submission.get_result(dataset)

        # This should have been created by now.
        assert submission_result is not None

        # EvaluationJob; dict() is required to detach the dictionary
        # that gets added to the Job from the control of SQLAlchemy
        job.language = submission.language
        job.files = dict(submission.files)
        job.managers = dict(dataset.managers)
        job.executables = dict(submission_result.executables)
        job.time_limit = dataset.time_limit
        job.memory_limit = dataset.memory_limit

        jobs = dict()

        for k, testcase in dataset.testcases.iteritems():
            job2 = deepcopy(job)

            job2.input = testcase.input
            job2.output = testcase.output
            job2.info = "evaluate submission %d on testcase %s" % \
                        (submission.id, testcase.codename)

            jobs[k] = job2

        return JobGroup(jobs)

    def to_submission_evaluation(self, sr):
        # This should actually be useless.
        sr.invalidate_evaluation()

        # No need to check self.success or job.success because this
        # method gets called only if the first (and therefore the
        # second!) is True.

        sr.set_evaluation_outcome()

        for test_name, job in self.jobs.iteritems():
            assert isinstance(job, EvaluationJob)

            sr.evaluations += [Evaluation(
                text=json.dumps(job.text, encoding='utf-8'),
                outcome=job.outcome,
                execution_time=job.plus.get('execution_time'),
                execution_wall_clock_time=job.plus.get(
                    'execution_wall_clock_time'),
                execution_memory=job.plus.get('execution_memory'),
                evaluation_shard=job.shard,
                evaluation_sandbox=":".join(job.sandboxes),
                testcase=sr.dataset.testcases[test_name])]

    @staticmethod
    def from_user_test_evaluation(user_test, dataset):
        job = EvaluationJob()

        # Job
        job.task_type = dataset.task_type
        job.task_type_parameters = dataset.task_type_parameters

        user_test_result = user_test.get_result(dataset)

        # This should have been created by now.
        assert user_test_result is not None

        # EvaluationJob; dict() is required to detach the dictionary
        # that gets added to the Job from the control of SQLAlchemy
        job.language = user_test.language
        job.files = dict(user_test.files)
        job.managers = dict(user_test.managers)
        job.executables = dict(user_test_result.executables)
        job.input = user_test.input
        job.time_limit = dataset.time_limit
        job.memory_limit = dataset.memory_limit
        job.info = "evaluate user test %d" % (user_test.id)

        # Add the managers to be got from the Task; get_task_type must
        # be imported here to avoid circular dependencies
        from cms.grading.tasktypes import get_task_type
        task_type = get_task_type(dataset=dataset)
        auto_managers = task_type.get_auto_managers()
        if auto_managers is not None:
            for manager_filename in auto_managers:
                job.managers[manager_filename] = \
                    dataset.managers[manager_filename]
        else:
            for manager_filename in dataset.managers:
                if manager_filename not in job.managers:
                    job.managers[manager_filename] = \
                        dataset.managers[manager_filename]

        job.get_output = True
        job.only_execution = True

        jobs = {"": job}

        return JobGroup(jobs)

    def to_user_test_evaluation(self, ur):
        # This should actually be useless.
        ur.invalidate_evaluation()

        job = self.jobs[""]
        assert isinstance(job, EvaluationJob)

        # No need to check self.success or job.success because this
        # method gets called only if the first (and therefore the
        # second!) is True.

        ur.evaluation_text = json.dumps(job.text, encoding='utf-8')
        ur.set_evaluation_outcome()  # FIXME use job.outcome
        ur.execution_time = job.plus.get('execution_time')
        ur.execution_wall_clock_time = \
            job.plus.get('execution_wall_clock_time')
        ur.execution_memory = job.plus.get('execution_memory')
        ur.evaluation_shard = job.shard
        ur.evaluation_sandbox = ":".join(job.sandboxes)
        ur.output = job.user_output

########NEW FILE########
__FILENAME__ = ParameterTypes
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

""" A collection of parameter type descriptions supported by AWS.

These parameter types can be used to specify the accepted parameters
of a task type or a score type. These types can cover 'basic' JSON
values, as task_type_parameters and score_type_parameters are
represented by JSON objects.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from tornado.template import Template


class ParameterType(object):
    """Base class for parameter types.

    """

    def __init__(self, name, short_name, description):
        """Initialization.

        name (string): name of the parameter.
        short_name (string): short name without spaces, used for HTML
                             element ids.
        description (string): describes the usage and effect of this
                              parameter.

        """
        self.name = name
        self.short_name = short_name
        self.description = description

    def parse_string(self, value):
        """Parse the specified string and returns the parsed value.

        Attempts to parse a value string (as received by a server from
        a form) and returns a value of the according type. If parsing
        fails, this method must raise a ValueError exception with
        an appropriate message.
        """
        raise NotImplementedError("Please subclass this class.")

    def parse_handler(self, handler, prefix):
        """Parse relevant parameters in the handler.

        Attempts to parse any relevant parameters in the specified handler.

        handler (tornado.web.RequestHandler): A handler containing
            the required parameters as arguments.
        prefix (string): The prefix of the relevant arguments in the handler.
        """
        return self.parse_string(handler.get_argument(
            prefix + self.short_name))

    def render(self, prefix, previous_value=None):
        raise NotImplementedError("Please subclass this class.")


class ParameterTypeString(ParameterType):
    """String parameter type."""

    TEMPLATE = "<input type=\"text\" name=\"{{parameter_name}}\" " \
        "value=\"{{parameter_value}}\" />"

    def parse_string(self, value):
        """Returns the specified string.
        """
        return value

    def render(self, prefix, previous_value=None):
        return Template(self.TEMPLATE).generate(
            parameter_name=prefix + self.short_name,
            parameter_value=previous_value)


class ParameterTypeFloat(ParameterType):
    """Numeric parameter type."""

    TEMPLATE = "<input type=\"text\" name=\"{{parameter_name}} \"" \
        "value=\"{{parameter_value}}\" />"

    def parse_string(self, value):
        """Attempts to parse the specified string as a float and
        returns the parsed value.
        """
        return float(value)

    def render(self, prefix, previous_value=None):
        return Template(self.TEMPLATE).generate(
            parameter_name=prefix + self.short_name,
            parameter_value=previous_value)


class ParameterTypeInt(ParameterType):
    """Numeric parameter type."""

    TEMPLATE = "<input type=\"text\" name=\"{{parameter_name}} \"" \
        "value=\"{{parameter_value}}\" />"

    def parse_string(self, value):
        """Attempts to parse the specified string as a float and
        returns the parsed value.
        """
        return int(value)

    def render(self, prefix, previous_value=None):
        return Template(self.TEMPLATE).generate(
            parameter_name=prefix + self.short_name,
            parameter_value=previous_value)


class ParameterTypeBoolean(ParameterType):
    """Boolean parameter type.
    """

    TEMPLATE = "<input type=\"checkbox\" name=\"{{parameter_name}} \"" \
        "{% if checked %}checked{% end %} />"

    def parse_string(self, value):
        """Returns True if the value is not None.
        """
        return value is not None

    def render(self, prefix, previous_value=False):
        return Template(self.TEMPLATE).generate(
            parameter_name=prefix + self.short_name,
            enabled=(previous_value is True))


class ParameterTypeChoice(ParameterType):
    """Parameter type representing a limited number of choices."""

    TEMPLATE = "<select name=\"{{parameter_name}}\">" \
        "{% for choice_value, choice_description "\
        " in choices.items() %}" \
        "<option value=\"{{choice_value}}\" " \
        "{% if choice_value == parameter_value %}" \
        "selected" \
        "{% end %}>" \
        "{{choice_description}}" \
        "</option>" \
        "{% end %}" \
        "</select>"

    def __init__(self, name, short_name, description, values):
        """
        values (dict): Short descriptions of the accepted choices,
            indexed by their respective accepted choices.
        """
        ParameterType.__init__(self, name, short_name, description)
        self.values = values

    def parse_string(self, value):
        """Tests whether the string is an accepted value.

        Returns the same string if it's an accepted value, otherwise it raises
        ValueError.
        """
        if value not in self.values:
            raise ValueError("Value %s doesn't match any allowed choice."
                             % value)
        return value

    def render(self, prefix, previous_value=None):
        return Template(self.TEMPLATE).generate(
            parameter_name=prefix + self.short_name,
            choices=self.values,
            parameter_value=previous_value)


class ParameterTypeArray(ParameterType):
    """Parameter type representing an arbitrary-size array of sub-parameters.

    Only a single sub-parameter type is supported.
    """

    TEMPLATE = "<a href=\"#\">Add element</a>" \
        "<table>" \
        "{% for element in elements%}" \
        "<tr><td>{{element.name}}</td>" \
        "<td>{% raw element.content %}</td></tr>" \
        "{% end %}" \
        "</table>"

    def __init__(self, name, short_name, description, subparameter):
        ParameterType.__init__(self, name, short_name, description)
        self.subparameter = subparameter

    def parse_string(self, value):
        pass

    def parse_handler(self, handler, prefix):
        parsed_values = []
        i = 0
        old_prefix = "%s%s_%d" % (prefix, self.short_name, i)
        while handler.get_argument(old_prefix) is not None:
            new_prefix = "%s%s_%d_" % (prefix, self.short_name, i)
            parsed_values.append(
                self.subparameter.parse_handler(handler, new_prefix))
        return parsed_values

    def render(self, prefix, previous_value=[]):
        elements = []
        for i in range(len(previous_value)):
            subparam_value = previous_value[i]
            new_prefix = "%s%s_%d_" % (prefix, self.short_name, i)
            elements.append({
                "name": self.subparameter.name,
                "content": self.subparameter.render(new_prefix,
                                                    subparam_value)})
        return Template(self.TEMPLATE).generate(elements=elements)


class ParameterTypeCollection(ParameterType):
    """A fixed-size list of subparameters."""

    TEMPLATE = "<table>" \
        "{% for element in elements %}" \
        "<tr><td>{{element['name']}}</td>" \
        "<td>{% raw element['content'] %}</td></tr>" \
        "{% end %}" \
        "</table>"

    def __init__(self, name, shortname, description, subparameters):
        ParameterType.__init__(self, name, shortname, description)
        self.subparameters = subparameters

    def parse_string(self, value):
        pass

    def parse_handler(self, handler, prefix):
        parsed_values = []
        for i in range(len(self.subparameters)):
            new_prefix = "%s%s_%d_" % (prefix, self.short_name, i)
            parsed_values.append(
                self.subparameters[i].parse_handler(handler, new_prefix))
        return parsed_values

    def render(self, prefix, previous_value=None):
        elements = []
        for i in range(len(self.subparameters)):
            try:
                subparam_value = previous_value[i]
            except:
                subparam_value = ''
            new_prefix = "%s%s_%d_" % (prefix, self.short_name, i)
            elements.append({
                "name": self.subparameters[i].name,
                "content": self.subparameters[i].render(new_prefix,
                                                        subparam_value)})
        return Template(self.TEMPLATE).generate(elements=elements)

########NEW FILE########
__FILENAME__ = Sandbox
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2014 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import io
import logging
import os
import re
import resource
import select
import stat
import tempfile
from functools import wraps, partial

import gevent
from gevent import subprocess
#import gevent_subprocess as subprocess

from cms import config
from cms.io.GeventUtils import copyfileobj, rmtree
from cmscommon.datetime import monotonic_time


logger = logging.getLogger(__name__)


class SandboxInterfaceException(Exception):
    pass


def with_log(func):
    """Decorator for presuming that the logs are present.

    """
    @wraps(func)
    def newfunc(self, *args, **kwargs):
        """If they are not present, get the logs.

        """
        if self.log is None:
            self.get_log()
        return func(self, *args, **kwargs)

    return newfunc


def pretty_print_cmdline(cmdline):
    """Pretty print a command line.

    Take a command line suitable to be passed to a Popen-like call and
    returns a string that represents it in a way that preserves the
    structure of arguments and can be passed to bash as is.

    More precisely, delimitate every item of the command line with
    single apstrophes and join all the arguments separating them with
    spaces.

    """
    return " ".join(["'%s'" % (x.replace("'", "'\"'\"'")) for x in cmdline])


def wait_without_std(procs):
    """Wait for the conclusion of the processes in the list, avoiding
    starving for input and output.

    procs (list): a list of processes as returned by Popen.

    return (list): a list of return codes.

    """
    def get_to_consume():
        """Amongst stdout and stderr of list of processes, find the
        ones that are alive and not closed (i.e., that may still want
        to write to).

        return (list): a list of open streams.

        """
        to_consume = []
        for process in procs:
            if process.poll() is None:  # If the process is alive.
                if process.stdout and not process.stdout.closed:
                    to_consume.append(process.stdout)
                if process.stderr and not process.stderr.closed:
                    to_consume.append(process.stderr)
        return to_consume

    # Close stdin; just saying stdin=None isn't ok, because the
    # standard input would be obtained from the application stdin,
    # that could interfere with the child process behaviour
    for process in procs:
        if process.stdin:
            process.stdin.close()

    # Read stdout and stderr to the end without having to block
    # because of insufficient buffering (and without allocating too
    # much memory). Unix specific.
    to_consume = get_to_consume()
    while len(to_consume) > 0:
        to_read = select.select(to_consume, [], [], 1.0)[0]
        for file_ in to_read:
            file_.read(8192)
        to_consume = get_to_consume()

    return [process.wait() for process in procs]


class Truncator(io.RawIOBase):
    """Wrap a file-like object to simulate truncation.

    This file-like object provides read-only access to a limited prefix
    of a wrapped file-like object. It provides a truncated version of
    the file without ever touching the object on the filesystem.

    This class is only able to wrap binary streams as it relies on the
    readinto method which isn't provided by text (unicode) streams.

    """
    def __init__(self, fobj, size):
        """Wrap fobj and give access to its first size bytes.

        fobj (fileobj): a file-like object to wrap.
        size (int): the number of bytes that will be accessible.

        """
        self.fobj = fobj
        self.size = size

    def close(self):
        """See io.IOBase.close."""
        self.fobj.close()

    @property
    def closed(self):
        """See io.IOBase.closed."""
        return self.fobj.closed

    def readable(self):
        """See io.IOBase.readable."""
        return True

    def seekable(self):
        """See io.IOBase.seekable."""
        return True

    def readinto(self, b):
        """See io.RawIOBase.readinto."""
        # This is the main "trick": we clip (i.e. mask, reduce, slice)
        # the given buffer so that it doesn't overflow into the area we
        # want to hide (that is, out of the prefix) and then we forward
        # it to the wrapped file-like object.
        b = memoryview(b)[:max(0, self.size - self.fobj.tell())]
        return self.fobj.readinto(b)

    def seek(self, offset, whence=io.SEEK_SET):
        """See io.IOBase.seek."""
        # We have to catch seeks relative to the end of the file and
        # adjust them to the new "imposed" size.
        if whence == io.SEEK_END:
            if self.fobj.seek(0, io.SEEK_END) > self.size:
                self.fobj.seek(self.size, io.SEEK_SET)
            return self.fobj.seek(offset, io.SEEK_CUR)
        else:
            return self.fobj.seek(offset, whence)

    def tell(self):
        """See io.IOBase.tell."""
        return self.fobj.tell()

    def write(self, b):
        """See io.RawIOBase.write."""
        raise io.UnsupportedOperation('write')


class SandboxBase(object):
    """A base class for all sandboxes, meant to contain common
    resources.

    """

    EXIT_SANDBOX_ERROR = 'sandbox error'
    EXIT_OK = 'ok'
    EXIT_SIGNAL = 'signal'
    EXIT_TIMEOUT = 'timeout'
    EXIT_FILE_ACCESS = 'file access'
    EXIT_SYSCALL = 'syscall'
    EXIT_NONZERO_RETURN = 'nonzero return'

    def __init__(self, file_cacher=None):
        """Initialization.

        file_cacher (FileCacher): an instance of the FileCacher class
            (to interact with FS).

        """
        self.file_cacher = file_cacher

    def get_stats(self):
        """Return a human-readable string representing execution time
        and memory usage.

        return (string): human-readable stats.

        """
        execution_time = self.get_execution_time()
        if execution_time is not None:
            time_str = "%.3f sec" % (execution_time)
        else:
            time_str = "(time unknown)"
        memory_used = self.get_memory_used()
        if memory_used is not None:
            mem_str = "%.2f MB" % (memory_used / (1024 * 1024))
        else:
            mem_str = "(memory usage unknown)"
        return "[%s - %s]" % (time_str, mem_str)

    def get_root_path(self):
        """Return the toplevel path of the sandbox.

        return (string): the root path.

        raise (NotImplementedError): if the subclass does not
            implement this method.

        """
        raise NotImplementedError("Subclasses must implement get_root_path.")

    def get_execution_time(self):
        """Return the time spent in the sandbox.

        return (float): time spent in the sandbox.

        raise (NotImplementedError): if the subclass does not
            implement this method.

        """
        raise NotImplementedError(
            "Subclasses must implement get_execution_time.")

    def get_memory_used(self):
        """Return the memory used by the sandbox.

        return (int): memory used by the sandbox (in bytes).

        raise (NotImplementedError): if the subclass does not
            implement this method.

        """
        raise NotImplementedError("Subclasses must implement get_memory_used.")

    def relative_path(self, path):
        """Translate from a relative path inside the sandbox to a
        system path.

        path (string): relative path of the file inside the sandbox.
        return (string): the absolute path.

        """
        return os.path.join(self.get_root_path(), path)

    def create_file(self, path, executable=False):
        """Create an empty file in the sandbox and open it in write
        binary mode.

        path (string): relative path of the file inside the sandbox.
        executable (bool): to set permissions.
        return (file): the file opened in write binary mode.

        """
        if executable:
            logger.debug("Creating executable file %s in sandbox." % path)
        else:
            logger.debug("Creating plain file %s in sandbox." % path)
        real_path = self.relative_path(path)
        file_ = io.open(real_path, "wb")
        mod = stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH | stat.S_IWUSR
        if executable:
            mod |= stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH
        os.chmod(real_path, mod)
        return file_

    def create_file_from_storage(self, path, digest, executable=False):
        """Write a file taken from FS in the sandbox.

        path (string): relative path of the file inside the sandbox.
        digest (string): digest of the file in FS.
        executable (bool): to set permissions.

        """
        file_ = self.create_file(path, executable)
        self.file_cacher.get_file_to_fobj(digest, file_)
        file_.close()

    def create_file_from_string(self, path, content, executable=False):
        """Write some data to a file in the sandbox.

        path (string): relative path of the file inside the sandbox.
        content (string): what to write in the file.
        executable (bool): to set permissions.

        """
        file_ = self.create_file(path, executable)
        file_.write(content)
        file_.close()

    def create_file_from_fileobj(self, path, file_obj, executable=False):
        """Write a file in the sandbox copying the content of an open
        file-like object.

        path (string): relative path of the file inside the sandbox.
        file_obj (file): where from read the file content.
        executable (bool): to set permissions.

        """
        dest = self.create_file(path, executable)
        copyfileobj(file_obj, dest)
        dest.close()

    def get_file(self, path, trunc_len=None):
        """Open a file in the sandbox given its relative path.

        path (string): relative path of the file inside the sandbox.
        trunc_len (int): if None, does nothing; otherwise, before
                         returning truncate it at the specified length.

        return (file): the file opened in read binary mode.

        """
        logger.debug("Retrieving file %s from sandbox" % (path))
        real_path = self.relative_path(path)
        file_ = io.open(real_path, "rb")
        if trunc_len is not None:
            file_ = Truncator(file_, trunc_len)
        return file_

    def get_file_to_string(self, path, maxlen=1024):
        """Return the content of a file in the sandbox given its
        relative path.

        path (string): relative path of the file inside the sandbox.
        maxlen (int): maximum number of bytes to read, or None if no
                      limit.
        return (string): the content of the file up to maxlen bytes.

        """
        file_ = self.get_file(path)
        try:
            if maxlen is None:
                content = file_.read()
            else:
                content = file_.read(maxlen)
        except UnicodeDecodeError:
            logger.error("Unable to interpret file as UTF-8.",
                         exc_info=True)
            return None
        file_.close()
        return content

    def get_file_to_storage(self, path, description="", trunc_len=None):
        """Put a sandbox file in FS and return its digest.

        path (string): relative path of the file inside the sandbox.
        description (string): the description for FS.
        trunc_len (int): if None, does nothing; otherwise, before
                         returning truncate it at the specified length.

        return (string): the digest of the file.

        """
        file_ = self.get_file(path, trunc_len=trunc_len)
        digest = self.file_cacher.put_file_from_fobj(file_, description)
        file_.close()
        return digest

    def stat_file(self, path):
        """Return the stats of a file in the sandbox.

        path (string): relative path of the file inside the sandbox.
        return (stat_result): the stat results.

        """
        return os.stat(self.relative_path(path))

    def file_exists(self, path):
        """Return if a file exists in the sandbox.

        path (string): relative path of the file inside the sandbox.
        return (bool): if the file exists.

        """
        return os.path.exists(self.relative_path(path))

    def remove_file(self, path):
        """Delete a file in the sandbox.

        path (string): relative path of the file inside the sandbox.

        """
        os.remove(self.relative_path(path))


class StupidSandbox(SandboxBase):
    """A stupid sandbox implementation. It has very few features and
    is not secure against things like box escaping and fork
    bombs. Yet, it is very portable and has no dependencies, so it's
    very useful for testing. Using in real contests is strongly
    discouraged.

    """

    def __init__(self, file_cacher=None, temp_dir=None):
        """Initialization.

        For arguments documentation, see SandboxBase.__init__.

        """
        SandboxBase.__init__(self, file_cacher)

        # Make box directory
        if temp_dir is None:
            temp_dir = config.temp_dir
        self.path = tempfile.mkdtemp(dir=temp_dir)

        self.cmd_file = "commands.log"
        self.exec_num = -1
        self.popen = None
        self.popen_time = None
        self.exec_time = None

        logger.debug("Sandbox in `%s' created, using stupid box." %
                     (self.path))

        # Box parameters
        self.chdir = self.path
        self.stdin_file = None
        self.stdout_file = None
        self.stderr_file = None
        self.stack_space = None
        self.address_space = None
        self.timeout = None
        self.wallclock_timeout = None
        self.extra_timeout = None

        # These parameters are not going to be used, but are here for
        # API compatibility
        self.box_id = 0
        self.cgroup = False
        self.dirs = []
        self.preserve_env = False
        self.inherit_env = []
        self.set_env = {}
        self.max_processes = 1
        self.verbosity = 0

        # Set common environment variables.
        # Specifically needed by Python, that searches the home for
        # packages.
        self.set_env["HOME"] = "./"

    # TODO - It returns wall clock time, because I have no way to
    # check CPU time (libev doesn't have wait4() support)
    def get_execution_time(self):
        """Return the time spent in the sandbox.

        return (float): time spent in the sandbox.

        """
        return self.get_execution_wall_clock_time()

    # TODO - It returns the best known approximation of wall clock
    # time; unfortunately I have no way to compute wall clock time
    # just after the child terminates, because I have no guarantee
    # about how the control will come back to this class
    def get_execution_wall_clock_time(self):
        """Return the total time from the start of the sandbox to the
        conclusion of the task.

        return (float): total time the sandbox was alive.

        """
        if self.exec_time:
            return self.exec_time
        if self.popen_time:
            self.exec_time = monotonic_time() - self.popen_time
            return self.exec_time
        return None

    # TODO - It always returns None, since I have no way to check
    # memory usage (libev doesn't have wait4() support)
    def get_memory_used(self):
        """Return the memory used by the sandbox.

        return (int): memory used by the sandbox (in bytes).

        """
        return None

    def get_killing_signal(self):
        """Return the signal that killed the sandboxed process.

        return (int): offending signal, or 0.

        """
        if self.popen.returncode < 0:
            return -self.popen.returncode
        return 0

    # This sandbox only discriminates between processes terminating
    # properly or being killed with a signal; all other exceptional
    # conditions (RAM or CPU limitations, ...) result in some signal
    # being delivered to the process
    def get_exit_status(self):
        """Get information about how the sandbox terminated.

        return (string): the main reason why the sandbox terminated.

        """
        if self.popen.returncode >= 0:
            return self.EXIT_OK
        else:
            return self.EXIT_SIGNAL

    def get_exit_code(self):
        """Return the exit code of the sandboxed process.

        return (float): exitcode, or 0.

        """
        return self.popen.returncode

    def get_human_exit_description(self):
        """Get the status of the sandbox and return a human-readable
        string describing it.

        return (string): human-readable explaination of why the
                         sandbox terminated.

        """
        status = self.get_exit_status()
        if status == self.EXIT_OK:
            return "Execution successfully finished (with exit code %d)" % \
                self.get_exit_code()
        elif status == self.EXIT_SIGNAL:
            return "Execution killed with signal %d" % \
                self.get_killing_signal()

    def _popen(self, command,
               stdin=None, stdout=None, stderr=None,
               preexec_fn=None, close_fds=True):
        """Execute the given command in the sandbox using
        subprocess.Popen, assigning the corresponding standard file
        descriptors.

        command ([string]): executable filename and arguments of the
            command.
        stdin (file): a file descriptor/object or None.
        stdout (file): a file descriptor/object or None.
        stderr (file): a file descriptor/object or None.
        preexec_fn (callable): to be called just before execve() or
                               None.
        close_fds (bool): close all file descriptor before executing.
        return (object): popen object.

        """
        self.exec_time = None
        self.exec_num += 1
        self.log = None
        logger.debug("Executing program in sandbox with command: %s" %
                     " ".join(command))
        with io.open(self.relative_path(self.cmd_file), 'at') as commands:
            commands.write("%s\n" % (pretty_print_cmdline(command)))
        try:
            p = subprocess.Popen(command,
                                 stdin=stdin, stdout=stdout, stderr=stderr,
                                 preexec_fn=preexec_fn, close_fds=close_fds)
        except OSError:
            logger.critical("Failed to execute program in sandbox "
                            "with command: %s" %
                            " ".join(command), exc_info=True)
            raise

        return p

    def execute_without_std(self, command, wait=False):
        """Execute the given command in the sandbox using
        subprocess.Popen and discarding standard input, output and
        error. More specifically, the standard input gets closed just
        after the execution has started; standard output and error are
        read until the end, in a way that prevents the execution from
        being blocked because of insufficient buffering.

        command ([string]): executable filename and arguments of the
            command.

        return (bool): True if the sandbox didn't report errors
            (caused by the sandbox itself), False otherwise

        """
        def preexec_fn(self):
            """Set limits for the child process.

            """
            if self.chdir:
                os.chdir(self.chdir)

            # TODO - We're not checking that setrlimit() returns
            # successfully (they may try to set to higher limits than
            # allowed to); anyway, this is just for testing
            # environment, not for real contests, so who cares.
            if self.timeout:
                rlimit_cpu = self.timeout
                if self.extra_timeout:
                    rlimit_cpu += self.extra_timeout
                rlimit_cpu = int(rlimit_cpu) + 1
                resource.setrlimit(resource.RLIMIT_CPU,
                                   (rlimit_cpu, rlimit_cpu))

            if self.address_space:
                rlimit_data = int(self.address_space * 1024)
                resource.setrlimit(resource.RLIMIT_DATA,
                                   (rlimit_data, rlimit_data))

            if self.stack_space:
                rlimit_stack = int(self.stack_space * 1024)
                resource.setrlimit(resource.RLIMIT_STACK,
                                   (rlimit_stack, rlimit_stack))

            # TODO - Doesn't work as expected
            #resource.setrlimit(resource.RLIMIT_NPROC, (1, 1))

        # Setup std*** redirection
        if self.stdin_file:
            stdin_fd = os.open(os.path.join(self.path, self.stdin_file),
                               os.O_RDONLY)
        else:
            stdin_fd = subprocess.PIPE
        if self.stdout_file:
            stdout_fd = os.open(os.path.join(self.path, self.stdout_file),
                                os.O_WRONLY | os.O_TRUNC | os.O_CREAT,
                                stat.S_IRUSR | stat.S_IRGRP |
                                stat.S_IROTH | stat.S_IWUSR)
        else:
            stdout_fd = subprocess.PIPE
        if self.stderr_file:
            stderr_fd = os.open(os.path.join(self.path, self.stderr_file),
                                os.O_WRONLY | os.O_TRUNC | os.O_CREAT,
                                stat.S_IRUSR | stat.S_IRGRP |
                                stat.S_IROTH | stat.S_IWUSR)
        else:
            stderr_fd = subprocess.PIPE

        # Note down execution time
        self.popen_time = monotonic_time()

        # Actually call the Popen
        self.popen = self._popen(command,
                                 stdin=stdin_fd,
                                 stdout=stdout_fd,
                                 stderr=stderr_fd,
                                 preexec_fn=partial(preexec_fn, self),
                                 close_fds=True)

        # Close file descriptors passed to the child
        if self.stdin_file:
            os.close(stdin_fd)
        if self.stdout_file:
            os.close(stdout_fd)
        if self.stderr_file:
            os.close(stderr_fd)

        if self.wallclock_timeout:
            # Kill the process after the wall clock time passed
            def timed_killer(timeout, popen):
                gevent.sleep(timeout)
                # TODO - Here we risk to kill some other process that gets
                # the same PID in the meantime; I don't know how to
                # properly solve this problem
                try:
                    popen.kill()
                except OSError:
                    # The process had died by itself
                    pass

            # Setup the killer
            full_wallclock_timeout = self.wallclock_timeout
            if self.extra_timeout:
                full_wallclock_timeout += self.extra_timeout
            gevent.spawn(timed_killer, full_wallclock_timeout, self.popen)

        # If the caller wants us to wait for completion, we also avoid
        # std*** to interfere with command. Otherwise we let the
        # caller handle these issues.
        if wait:
            return self.translate_box_exitcode(
                wait_without_std([self.popen])[0])
        else:
            return self.popen

    def translate_box_exitcode(self, exitcode):
        """The stupid box always terminates successfully (or it raises
        an exception).

        """
        return True

    def delete(self):
        """Delete the directory where the sandbox operated.

        """
        logger.debug("Deleting sandbox in %s" % self.path)

        # Delete the working directory.
        rmtree(self.path)


class IsolateSandbox(SandboxBase):
    """This class creates, deletes and manages the interaction with a
    sandbox. The sandbox doesn't support concurrent operation, not
    even for reading.

    The Sandbox offers API for retrieving and storing file, as well as
    executing programs in a controlled environment. There are anyway a
    few files reserved for use by the Sandbox itself:

     * commands.log: a text file with the commands ran into this
       Sandbox, one for each line;

     * run.log.N: for each N, the log produced by the sandbox when running
       command number N.

    """
    def __init__(self, file_cacher=None, temp_dir=None):
        """Initialization.

        For arguments documentation, see SandboxBase.__init__.

        """
        SandboxBase.__init__(self, file_cacher)

        # Get our shard number, to use as a unique identifier for the
        # sandbox on this machine. FIXME This is the only use of
        # FileCacher.service, and it's an improper use! Avoid it!
        if file_cacher is not None and file_cacher.service is not None:
            # We add 1 to avoid conflicting with console users of the
            # sandbox who use the default box id of 0.
            box_id = file_cacher.service.shard + 1
        else:
            box_id = 0

        # We create a directory "tmp" inside the outer temporary directory,
        # because the sandbox will bind-mount the inner one. The sandbox also
        # runs code as a different user, and so we need to ensure that they can
        # read and write to the directory. But we don't want everybody on the
        # system to, which is why the outer directory exists with no read
        # permissions.
        self.inner_temp_dir = "/tmp"
        if temp_dir is None:
            temp_dir = config.temp_dir
        self.outer_temp_dir = tempfile.mkdtemp(dir=temp_dir)
        # Don't use os.path.join here, because the absoluteness of /tmp will
        # bite you.
        self.path = self.outer_temp_dir + self.inner_temp_dir
        os.mkdir(self.path)
        os.chmod(self.path, 0777)

        self.exec_name = 'isolate'
        self.box_exec = self.detect_box_executable()
        self.info_basename = "run.log"   # Used for -M
        self.cmd_file = "commands.log"
        self.log = None
        self.exec_num = -1
        logger.debug("Sandbox in `%s' created, using box `%s'." %
                     (self.path, self.box_exec))

        # Default parameters for isolate
        self.box_id = box_id           # -b
        self.cgroup = config.use_cgroups  # --cg
        self.chdir = self.inner_temp_dir  # -c
        self.dirs = []                 # -d
        self.dirs += [(self.inner_temp_dir, self.path, "rw")]
        self.preserve_env = False      # -e
        self.inherit_env = []          # -E
        self.set_env = {}              # -E
        self.stdin_file = None         # -i
        self.stack_space = None        # -k
        self.address_space = None      # -m
        self.stdout_file = None        # -o
        self.max_processes = 1         # -p
        self.stderr_file = None        # -r
        self.timeout = None            # -t
        self.verbosity = 0             # -v
        self.wallclock_timeout = None  # -w
        self.extra_timeout = None      # -x

        # Set common environment variables.
        # Specifically needed by Python, that searches the home for
        # packages.
        self.set_env["HOME"] = "./"

        # Tell isolate to get the sandbox ready.
        box_cmd = [self.box_exec] + (["--cg"] if self.cgroup else []) \
            + ["--box-id=%d" % self.box_id] + ["--init"]
        ret = subprocess.call(box_cmd)
        if ret != 0:
            raise SandboxInterfaceException(
                "Failed to initialize sandbox with command: %s "
                "(error %d)" % (pretty_print_cmdline(box_cmd), ret))

    def get_root_path(self):
        """Return the toplevel path of the sandbox.

        return (string): the root path.

        """
        return self.path

    def detect_box_executable(self):
        """Try to find an isolate executable. It first looks in
        ./isolate/, then the local directory, then in a relative path
        from the file that contains the Sandbox module, then in the
        system paths.

        return (string): the path to a valid (hopefully) isolate.

        """
        paths = [os.path.join('.', 'isolate', self.exec_name),
                 os.path.join('.', self.exec_name)]
        if '__file__' in globals():
            paths += [os.path.abspath(os.path.join(
                      os.path.dirname(__file__),
                      '..', '..', 'isolate', self.exec_name))]
        paths += [self.exec_name]
        for path in paths:
            # Consider only non-directory, executable files.
            if os.path.exists(path) \
                    and not os.path.isdir(path) \
                    and os.access(path, os.X_OK):
                return path

        # As default, return self.exec_name alone, that means that
        # system path is used.
        return paths[-1]

    def build_box_options(self):
        """Translate the options defined in the instance to a string
        that can be postponed to mo-box as an arguments list.

        return ([string]): the arguments list as strings.

        """
        res = list()
        if self.box_id is not None:
            res += ["--box-id=%d" % self.box_id]
        if self.cgroup:
            res += ["--cg"]
        if self.chdir is not None:
            res += ["--chdir=%s" % self.chdir]
        for in_name, out_name, options in self.dirs:
            s = in_name
            if out_name is not None:
                s += "=" + out_name
            if options is not None:
                s += ":" + options
            res += ["--dir=%s" % s]
        if self.preserve_env:
            res += ["--full-env"]
        for var in self.inherit_env:
            res += ["--env=%s" % var]
        for var, value in self.set_env.items():
            res += ["--env=%s=%s" % (var, value)]
        if self.stdin_file is not None:
            res += ["--stdin=%s" % self.inner_absolute_path(self.stdin_file)]
        if self.stack_space is not None:
            res += ["--stack=%d" % self.stack_space]
        if self.address_space is not None:
            res += ["--mem=%d" % self.address_space]
        if self.stdout_file is not None:
            res += ["--stdout=%s" % self.inner_absolute_path(self.stdout_file)]
        if self.max_processes is not None:
            res += ["--processes=%d" % self.max_processes]
        else:
            res += ["--processes"]
        if self.stderr_file is not None:
            res += ["--stderr=%s" % self.inner_absolute_path(self.stderr_file)]
        if self.timeout is not None:
            res += ["--time=%g" % self.timeout]
        res += ["--verbose"] * self.verbosity
        if self.wallclock_timeout is not None:
            res += ["--wall-time=%g" % self.wallclock_timeout]
        if self.extra_timeout is not None:
            res += ["--extra-time=%g" % self.extra_timeout]
        res += ["--meta=%s" % self.relative_path("%s.%d" % (self.info_basename,
                                                            self.exec_num))]
        res += ["--run"]
        return res

    def get_log(self):
        """Read the content of the log file of the sandbox (usually
        run.log.N for some integer N), and set self.log as a dict
        containing the info in the log file (time, memory, status,
        ...).

        """
        # self.log is a dictionary of lists (usually lists of length
        # one).
        self.log = {}
        info_file = "%s.%d" % (self.info_basename, self.exec_num)
        try:
            with self.get_file(info_file) as log_file:
                for line in log_file:
                    key, value = line.strip().split(":", 1)
                    if key in self.log:
                        self.log[key].append(value)
                    else:
                        self.log[key] = [value]
        except IOError as error:
            raise IOError("Error while reading execution log file %s. %r" %
                          (info_file, error))

    @with_log
    def get_execution_time(self):
        """Return the time spent in the sandbox, reading the logs if
        necessary.

        return (float): time spent in the sandbox.

        """
        if 'time' in self.log:
            return float(self.log['time'][0])
        return None

    @with_log
    def get_execution_wall_clock_time(self):
        """Return the total time from the start of the sandbox to the
        conclusion of the task, reading the logs if necessary.

        return (float): total time the sandbox was alive.

        """
        if 'time-wall' in self.log:
            return float(self.log['time-wall'][0])
        return None

    @with_log
    def get_memory_used(self):
        """Return the memory used by the sandbox, reading the logs if
        necessary.

        return (int): memory used by the sandbox (in bytes).

        """
        if 'cg-mem' in self.log:
            return int(self.log['cg-mem'][0]) * 1024
        return None

    @with_log
    def get_killing_signal(self):
        """Return the signal that killed the sandboxed process,
        reading the logs if necessary.

        return (int): offending signal, or 0.

        """
        if 'exitsig' in self.log:
            return int(self.log['exitsig'][0])
        return 0

    @with_log
    def get_exit_code(self):
        """Return the exit code of the sandboxed process, reading the
        logs if necessary.

        return (int): exitcode, or 0.

        """
        if 'exitcode' in self.log:
            return int(self.log['exitcode'][0])
        return 0

    # TODO - Rather fragile interface...
    KILLING_SYSCALL_RE = re.compile("^Forbidden syscall (.*)$")

    @with_log
    def get_killing_syscall(self):
        """Return the syscall that triggered the killing of the
        sandboxed process, reading the log if necessary.

        return (string): offending syscall, or None.

        """
        if 'message' in self.log:
            match = self.KILLING_SYSCALL_RE.match(
                self.log['message'][0])
            if match is not None:
                return match.group(1)
        return None

    # TODO - Rather fragile interface...
    KILLING_FILE_ACCESS_RE = re.compile("^Forbidden access to file (.*)$")

    @with_log
    def get_forbidden_file_error(self):
        """Return the error that got us killed for forbidden file
        access.

        return (string): offending error, or None.

        """
        if 'message' in self.log:
            match = self.KILLING_FILE_ACCESS_RE.match(
                self.log['message'][0])
            if match is not None:
                return match.group(1)
        return None

    @with_log
    def get_status_list(self):
        """Reads the sandbox log file, and set and return the status
        of the sandbox.

        return (list): list of statuses of the sandbox.

        """
        if 'status' in self.log:
            return self.log['status']
        return []

    def get_exit_status(self):
        """Get the list of statuses of the sandbox and return the most
        important one.

        return (string): the main reason why the sandbox terminated.

        """
        status_list = self.get_status_list()
        if 'XX' in status_list:
            return self.EXIT_SANDBOX_ERROR
        # New version seems not to report OK
        #elif 'OK' in status_list:
        #    return self.EXIT_OK
        elif 'FO' in status_list:
            return self.EXIT_SYSCALL
        elif 'FA' in status_list:
            return self.EXIT_FILE_ACCESS
        elif 'TO' in status_list:
            return self.EXIT_TIMEOUT
        elif 'SG' in status_list:
            return self.EXIT_SIGNAL
        elif 'RE' in status_list:
            return self.EXIT_NONZERO_RETURN
        return self.EXIT_OK

    def get_human_exit_description(self):
        """Get the status of the sandbox and return a human-readable
        string describing it.

        return (string): human-readable explaination of why the
                         sandbox terminated.

        """
        status = self.get_exit_status()
        if status == self.EXIT_OK:
            return "Execution successfully finished (with exit code %d)" % \
                self.get_exit_code()
        elif status == self.EXIT_SANDBOX_ERROR:
            return "Execution failed because of sandbox error"
        elif status == self.EXIT_SYSCALL:
            return "Execution killed because of forbidden syscall %s" % \
                self.get_killing_syscall()
        elif status == self.EXIT_FILE_ACCESS:
            return "Execution killed because of forbidden file access: %s" \
                % self.get_forbidden_file_error()
        elif status == self.EXIT_TIMEOUT:
            return "Execution timed out"
        elif status == self.EXIT_SIGNAL:
            return "Execution killed with signal %d" % \
                self.get_killing_signal()
        elif status == self.EXIT_NONZERO_RETURN:
            return "Execution failed because the return code was nonzero"

    def inner_absolute_path(self, path):
        """Translate from a relative path inside the sandbox to an
        absolute path inside the sandbox.

        path (string): relative path of the file inside the sandbox.
        return (string): the absolute path of the file inside the sandbox.

        """
        return os.path.join(self.inner_temp_dir, path)

    # XXX - Temporarily disabled (i.e., set as private), in order to
    # ease interface while implementing other sandboxes, since it's
    # not used; anyway, it should probably factored through _popen in
    # order to avoid code duplication
    def _execute(self, command):
        """Execute the given command in the sandbox.

        command (list): executable filename and arguments of the
                        command.
        return (bool): True if the sandbox didn't report errors
                       (caused by the sandbox itself), False otherwise

        """
        self.exec_num += 1
        self.log = None
        args = [self.box_exec] + self.build_box_options() + ["--"] + command
        logger.debug("Executing program in sandbox with command: %s" %
                     pretty_print_cmdline(args))
        with io.open(self.relative_path(self.cmd_file), 'at') as commands:
            commands.write("%s\n" % (pretty_print_cmdline(args)))
        return self.translate_box_exitcode(subprocess.call(args))

    def _popen(self, command,
               stdin=None, stdout=None, stderr=None,
               close_fds=True):
        """Execute the given command in the sandbox using
        subprocess.Popen, assigning the corresponding standard file
        descriptors.

        command ([string]): executable filename and arguments of the
            command.
        stdin (int|None): a file descriptor.
        stdout (int|None): a file descriptor.
        stderr (int|None): a file descriptor.
        close_fds (bool): close all file descriptor before executing.

        return (Popen): popen object.

        """
        self.exec_num += 1
        self.log = None
        args = [self.box_exec] + self.build_box_options() + ["--"] + command
        logger.debug("Executing program in sandbox with command: %s" %
                     pretty_print_cmdline(args))
        with io.open(self.relative_path(self.cmd_file), 'at') as commands:
            commands.write("%s\n" % (pretty_print_cmdline(args)))
        try:
            p = subprocess.Popen(args,
                                 stdin=stdin, stdout=stdout, stderr=stderr,
                                 close_fds=close_fds)
        except OSError:
            logger.critical("Failed to execute program in sandbox "
                            "with command: %s" %
                            pretty_print_cmdline(args), exc_info=True)
            raise

        return p

    def execute_without_std(self, command, wait=False):
        """Execute the given command in the sandbox using
        subprocess.Popen and discarding standard input, output and
        error. More specifically, the standard input gets closed just
        after the execution has started; standard output and error are
        read until the end, in a way that prevents the execution from
        being blocked because of insufficient buffering.

        command ([string]): executable filename and arguments of the
            command.
        wait (bool): True if this call is blocking, False otherwise

        return (bool|Popen): if the call is blocking, then return True
            if the sandbox didn't report errors (caused by the sandbox
            itself), False otherwise; if the call is not blocking,
            return the Popen object from subprocess.

        """
        popen = self._popen(command, stdin=subprocess.PIPE,
                            stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                            close_fds=True)

        # If the caller wants us to wait for completion, we also avoid
        # std*** to interfere with command. Otherwise we let the
        # caller handle these issues.
        if wait:
            return self.translate_box_exitcode(wait_without_std([popen])[0])
        else:
            return popen

    def translate_box_exitcode(self, exitcode):
        """Translate the sandbox exit code according to the
        following table:
         * 0 -> everything ok -> returns True
         * 1 -> error in the program inside the sandbox ->
                returns True
         * 2 -> error in the sandbox itself -> returns False

        Basically, it recognizes whether the sandbox executed
        correctly or not.

        """
        if exitcode == 0 or exitcode == 1:
            return True
        elif exitcode == 2:
            return False
        else:
            raise SandboxInterfaceException("Sandbox exit status unknown")

    def delete(self):
        """Delete the directory where the sandbox operated.

        """
        logger.debug("Deleting sandbox in %s" % self.path)

        # Tell isolate to cleanup the sandbox.
        box_cmd = [self.box_exec] + (["--cg"] if self.cgroup else []) \
            + ["--box-id=%d" % self.box_id]
        subprocess.call(box_cmd + ["--cleanup"])

        # Delete the working directory.
        rmtree(self.outer_temp_dir)


Sandbox = {
    'stupid': StupidSandbox,
    'isolate': IsolateSandbox,
    }[config.sandbox_implementation]

########NEW FILE########
__FILENAME__ = ScoreType
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.


"""In this file there is the basic infrastructure from which we can
build a score type.

A score type is a class that receives all submissions for a task and
assign them a score, keeping the global state of the scoring for the
task.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import json
import logging

from tornado.template import Template


logger = logging.getLogger(__name__)


# Dummy function to mark translatable string.
def N_(message):
    return message


class ScoreType(object):
    """Base class for all score types, that must implement all methods
    defined here.

    """
    TEMPLATE = ""

    def __init__(self, parameters, public_testcases):
        """Initializer.

        parameters (object): format is specified in the subclasses.
        public_testcases (dict): associate to each testcase's codename
                                 a boolean indicating if the testcase
                                 is public.

        """
        self.parameters = parameters
        self.public_testcases = public_testcases

        # Preload the maximum possible scores.
        self.max_score, self.max_public_score, self.ranking_headers = \
            self.max_scores()

    def get_html_details(self, score_details, translator=None):
        """Return an HTML string representing the score details of a
        submission.

        score_details (dict): the data saved by the score type itself
                              in the database; can be public or
                              private.
        translator (function): the function to localize strings.
        return (string): an HTML string representing score_details.

        """
        if translator is None:
            translator = lambda string: string
        try:
            score_details = json.loads(score_details)
        except (TypeError, ValueError):
            # TypeError raised if score_details is None
            logger.error("Found a null or non-JSON score details string. "
                         "Try invalidating scores.")
            return translator("Score details temporarily unavailable.")
        else:
            return Template(self.TEMPLATE).generate(details=score_details,
                                                    _=translator)

    def max_scores(self):
        """Returns the maximum score that one could aim to in this
        problem. Also return the maximum score from the point of view
        of a user that did not play the token. Depend on the subclass.

        return (float, float): maximum score and maximum score with
                               only public testcases.

        """
        logger.error("Unimplemented method max_scores.")
        raise NotImplementedError("Please subclass this class.")

    def compute_score(self, submission_result):
        """Computes a score of a single submission. We don't know here
        how to do it, but our subclasses will.

        submission_id (int): the submission to evaluate.

        returns (float, str, float, str, [str]): respectively: the
            score, the HTML string with additional information (e.g.
            testcases' and subtasks' score), and the same information
            from the point of view of a user that did not play a
            token, the list of strings to send to RWS.

        """
        logger.error("Unimplemented method compute_score.")
        raise NotImplementedError("Please subclass this class.")


class ScoreTypeAlone(ScoreType):
    """Intermediate class to manage tasks where the score of a
    submission depends only on the submission itself and not on the
    other submissions' outcome. Remains to implement compute_score to
    obtain the score of a single submission and max_scores.

    """
    pass


class ScoreTypeGroup(ScoreTypeAlone):
    """Intermediate class to manage tasks whose testcases are
    subdivided in groups (or subtasks). The score type parameters must
    be in the form [[m, t, ...], [...], ...], where m is the maximum
    score for the given subtask and t is the number of testcases
    comprising the subtask (that are consumed from the first to the
    last, sorted by num).

    A subclass must implement the method 'get_public_outcome' and
    'reduce'.

    """
    # Mark strings for localization.
    N_("Subtask %d")
    N_("Outcome")
    N_("Details")
    N_("Execution time")
    N_("Memory used")
    N_("N/A")
    TEMPLATE = """\
{% from cms.grading import format_status_text %}
{% from cms.server import format_size %}
{% for st in details %}
    {% if "score" in st and "max_score" in st %}
        {% if st["score"] >= st["max_score"] %}
<div class="subtask correct">
        {% elif st["score"] <= 0.0 %}
<div class="subtask notcorrect">
        {% else %}
<div class="subtask partiallycorrect">
        {% end %}
    {% else %}
<div class="subtask undefined">
    {% end %}
    <div class="subtask-head">
        <span class="title">
            {{ _("Subtask %d") % st["idx"] }}
        </span>
    {% if "score" in st and "max_score" in st %}
        <span class="score">
            {{ '%g' % round(st["score"], 2) }} / {{ st["max_score"] }}
        </span>
    {% else %}
        <span class="score">
            {{ _("N/A") }}
        </span>
    {% end %}
    </div>
    <div class="subtask-body">
        <table class="testcase-list">
            <thead>
                <tr>
                    <th>{{ _("Outcome") }}</th>
                    <th>{{ _("Details") }}</th>
                    <th>{{ _("Execution time") }}</th>
                    <th>{{ _("Memory used") }}</th>
                </tr>
            </thead>
            <tbody>
    {% for tc in st["testcases"] %}
        {% if "outcome" in tc and "text" in tc %}
            {% if tc["outcome"] == "Correct" %}
                <tr class="correct">
            {% elif tc["outcome"] == "Not correct" %}
                <tr class="notcorrect">
            {% else %}
                <tr class="partiallycorrect">
            {% end %}
                    <td>{{ _(tc["outcome"]) }}</td>
                    <td>{{ format_status_text(tc["text"], _) }}</td>
                    <td>
            {% if "time" in tc and tc["time"] is not None %}
                        {{ _("%(seconds)0.3f s") % {'seconds': tc["time"]} }}
            {% else %}
                        {{ _("N/A") }}
            {% end %}
                    </td>
                    <td>
            {% if "memory" in tc and tc["memory"] is not None %}
                        {{ format_size(tc["memory"]) }}
            {% else %}
                        {{ _("N/A") }}
            {% end %}
                    </td>
        {% else %}
                <tr class="undefined">
                    <td colspan="4">
                        {{ _("N/A") }}
                    </td>
                </tr>
        {% end %}
    {% end %}
            </tbody>
        </table>
    </div>
</div>
{% end %}"""

    def max_scores(self):
        """Compute the maximum score of a submission.

        returns (float, float): maximum score overall and public.

        """
        score = 0.0
        public_score = 0.0
        headers = list()

        # XXX Lexicographical order by codename
        indices = sorted(self.public_testcases.keys())
        current = 0

        for i, parameter in enumerate(self.parameters):
            next_ = current + parameter[1]
            score += parameter[0]
            if all(self.public_testcases[idx]
                   for idx in indices[current:next_]):
                public_score += parameter[0]
            headers += ["Subtask %d (%g)" % (i + 1, parameter[0])]
            current = next_

        return score, public_score, headers

    def compute_score(self, submission_result):
        """Compute the score of a submission.

        submission_id (int): the submission to evaluate.
        returns (float): the score

        """
        # Actually, this means it didn't even compile!
        if not submission_result.evaluated():
            return 0.0, "[]", 0.0, "[]", \
                json.dumps(["%lg" % 0.0 for _ in self.parameters])

        # XXX Lexicographical order by codename
        indices = sorted(self.public_testcases.keys())
        evaluations = dict((ev.codename, ev)
                           for ev in submission_result.evaluations)
        subtasks = []
        public_subtasks = []
        ranking_details = []
        tc_start = 0
        tc_end = 0

        for st_idx, parameter in enumerate(self.parameters):
            tc_end = tc_start + parameter[1]
            st_score = self.reduce([float(evaluations[idx].outcome)
                                    for idx in indices[tc_start:tc_end]],
                                   parameter) * parameter[0]
            st_public = all(self.public_testcases[idx]
                            for idx in indices[tc_start:tc_end])
            tc_outcomes = dict((
                idx,
                self.get_public_outcome(
                    float(evaluations[idx].outcome), parameter)
                ) for idx in indices[tc_start:tc_end])

            testcases = []
            public_testcases = []
            for idx in indices[tc_start:tc_end]:
                testcases.append({
                    "idx": idx,
                    "outcome": tc_outcomes[idx],
                    "text": evaluations[idx].text,
                    "time": evaluations[idx].execution_time,
                    "memory": evaluations[idx].execution_memory,
                    })
                if self.public_testcases[idx]:
                    public_testcases.append(testcases[-1])
                else:
                    public_testcases.append({"idx": idx})
            subtasks.append({
                "idx": st_idx + 1,
                "score": st_score,
                "max_score": parameter[0],
                "testcases": testcases,
                })
            if st_public:
                public_subtasks.append(subtasks[-1])
            else:
                public_subtasks.append({
                    "idx": st_idx + 1,
                    "testcases": public_testcases,
                    })

            ranking_details.append("%g" % round(st_score, 2))

            tc_start = tc_end

        score = sum(st["score"] for st in subtasks)
        public_score = sum(st["score"]
                           for st in public_subtasks
                           if "score" in st)

        return score, json.dumps(subtasks), \
            public_score, json.dumps(public_subtasks), \
            json.dumps(ranking_details)

    def get_public_outcome(self, outcome, parameter):
        """Return a public outcome from an outcome.

        The public outcome is shown to the user, and this method
        return the public outcome associated to the outcome of a
        submission in a testcase contained in the group identified by
        parameter.

        outcome (float): the outcome of the submission in the
                         testcase.
        parameter (list): the parameters of the current group.

        return (float): the public output.

        """
        logger.error("Unimplemented method get_public_outcome.")
        raise NotImplementedError("Please subclass this class.")

    def reduce(self, outcomes, parameter):
        """Return the score of a subtask given the outcomes.

        outcomes ([float]): the outcomes of the submission in the
                            testcases of the group.
        parameter (list): the parameters of the group.

        return (float): the public output.

        """
        logger.error("Unimplemented method reduce.")
        raise NotImplementedError("Please subclass this class.")

########NEW FILE########
__FILENAME__ = GroupMin
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from cms.grading.ScoreType import ScoreTypeGroup


# Dummy function to mark translatable string.
def N_(message):
    return message


class GroupMin(ScoreTypeGroup):
    """The score of a submission is the sum of the product of the
    minimum of the ranges with the multiplier of that range.

    Parameters are [[m, t], ... ] (see ScoreTypeGroup).

    """

    def get_public_outcome(self, outcome, parameter):
        """See ScoreTypeGroup."""
        if outcome <= 0.0:
            return N_("Not correct")
        elif outcome >= 1.0:
            return N_("Correct")
        else:
            return N_("Partially correct")

    def reduce(self, outcomes, parameter):
        """See ScoreTypeGroup."""
        return min(outcomes)

########NEW FILE########
__FILENAME__ = GroupMul
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from cms.grading.ScoreType import ScoreTypeGroup


# Dummy function to mark translatable string.
def N_(message):
    return message


class GroupMul(ScoreTypeGroup):
    """The score of a submission is the sum of the product of the
    product of the ranges with the multiplier of that range.

    Parameters are [[m, t], ... ] (see ScoreTypeGroup).

    """

    def get_public_outcome(self, outcome, parameter):
        """See ScoreTypeGroup."""
        if outcome <= 0.0:
            return N_("Not correct")
        elif outcome >= 1.0:
            return N_("Correct")
        else:
            return N_("Partially correct")

    def reduce(self, outcomes, parameter):
        """See ScoreTypeGroup."""
        return reduce(lambda x, y: x * y, outcomes)

########NEW FILE########
__FILENAME__ = GroupThreshold
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from cms.grading.ScoreType import ScoreTypeGroup


# Dummy function to mark translatable string.
def N_(message):
    return message


class GroupThreshold(ScoreTypeGroup):
    """The score of a submission is the sum of: the multiplier of the
    range if all outcomes are between 0.0 and the threshold, or 0.0.

    Parameters are [[m, t, T], ... ] (see ScoreTypeGroup), where T is
    the threshold for the group.

    """

    def get_public_outcome(self, outcome, parameter):
        """See ScoreTypeGroup."""
        threshold = parameter[2]
        if 0.0 <= outcome <= threshold:
            return N_("Correct")
        else:
            return N_("Not correct")

    def reduce(self, outcomes, parameter):
        """See ScoreTypeGroup."""
        threshold = parameter[2]
        if all(0 <= outcome <= threshold
               for outcome in outcomes):
            return 1.0
        else:
            return 0.0

########NEW FILE########
__FILENAME__ = Sum
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import json

from cms.grading.ScoreType import ScoreTypeAlone


# Dummy function to mark translatable string.
def N_(message):
    return message


class Sum(ScoreTypeAlone):
    """The score of a submission is the sum of the outcomes,
    multiplied by the integer parameter.

    """
    # Mark strings for localization.
    N_("Outcome")
    N_("Details")
    N_("Execution time")
    N_("Memory used")
    N_("N/A")
    TEMPLATE = """\
{% from cms.grading import format_status_text %}
{% from cms.server import format_size %}
<table class="testcase-list">
    <thead>
        <tr>
            <th>{{ _("Outcome") }}</th>
            <th>{{ _("Details") }}</th>
            <th>{{ _("Execution time") }}</th>
            <th>{{ _("Memory used") }}</th>
        </tr>
    </thead>
    <tbody>
    {% for tc in details %}
        {% if "outcome" in tc and "text" in tc %}
            {% if tc["outcome"] == "Correct" %}
        <tr class="correct">
            {% elif tc["outcome"] == "Not correct" %}
        <tr class="notcorrect">
            {% else %}
        <tr class="partiallycorrect">
            {% end %}
            <td>{{ _(tc["outcome"]) }}</td>
            <td>{{ format_status_text(tc["text"], _) }}</td>
            <td>
            {% if tc["time"] is not None %}
                {{ _("%(seconds)0.3f s") % {'seconds': tc["time"]} }}
            {% else %}
                {{ _("N/A") }}
            {% end %}
            </td>
            <td>
            {% if tc["memory"] is not None %}
                {{ format_size(tc["memory"]) }}
            {% else %}
                {{ _("N/A") }}
            {% end %}
            </td>
        {% else %}
        <tr class="undefined">
            <td colspan="4">
                {{ _("N/A") }}
            </td>
        </tr>
        {% end %}
    {% end %}
    </tbody>
</table>"""

    def max_scores(self):
        """Compute the maximum score of a submission.

        returns (float, float): maximum score overall and public.

        """
        public_score = 0.0
        score = 0.0
        for public in self.public_testcases.itervalues():
            if public:
                public_score += self.parameters
            score += self.parameters
        return score, public_score, []

    def compute_score(self, submission_result):
        """Compute the score of a submission.

        See the same method in ScoreType for details.

        """
        # Actually, this means it didn't even compile!
        if not submission_result.evaluated():
            return 0.0, "[]", 0.0, "[]", json.dumps([])

        # XXX Lexicographical order by codename
        indices = sorted(self.public_testcases.keys())
        evaluations = dict((ev.codename, ev)
                           for ev in submission_result.evaluations)
        testcases = []
        public_testcases = []
        score = 0.0
        public_score = 0.0

        for idx in indices:
            this_score = float(evaluations[idx].outcome) * self.parameters
            tc_outcome = self.get_public_outcome(this_score)
            score += this_score
            testcases.append({
                "idx": idx,
                "outcome": tc_outcome,
                "text": evaluations[idx].text,
                "time": evaluations[idx].execution_time,
                "memory": evaluations[idx].execution_memory,
                })
            if self.public_testcases[idx]:
                public_score += this_score
                public_testcases.append(testcases[-1])
            else:
                public_testcases.append({"idx": idx})

        return score, json.dumps(testcases), \
            public_score, json.dumps(public_testcases), \
            json.dumps([])

    def get_public_outcome(self, outcome):
        """Return a public outcome from an outcome.

        outcome (float): the outcome of the submission.

        return (float): the public output.

        """
        if outcome <= 0.0:
            return N_("Not correct")
        elif outcome >= self.parameters:
            return N_("Correct")
        else:
            return N_("Partially correct")

########NEW FILE########
__FILENAME__ = TaskType
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012-2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""In this file there is the basic infrastructure from which we can
build a task type.

Basically, a task type is a class that receives a submission and knows
how to compile and evaluate it. A worker creates a task type to work
on a submission, and all low-level details on how to implement the
compilation and the evaluation are contained in the task type class.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging
import re
import traceback

from cms import config
from cms.grading import JobException
from cms.grading.Sandbox import Sandbox
from cms.grading.Job import CompilationJob, EvaluationJob


logger = logging.getLogger(__name__)


## Sandbox lifecycle. ##

def create_sandbox(file_cacher):
    """Create a sandbox, and return it.

    file_cacher (FileCacher): a file cacher instance.

    return (Sandbox): a sandbox.

    raise (JobException): if the sandbox cannot be created.

    """
    try:
        sandbox = Sandbox(file_cacher)
    except (OSError, IOError):
        err_msg = "Couldn't create sandbox."
        logger.error("%s\n%s" % (err_msg, traceback.format_exc()))
        raise JobException(err_msg)
    return sandbox


def delete_sandbox(sandbox):
    """Delete the sandbox, if the configuration allows it to be
    deleted.

    sandbox (Sandbox): the sandbox to delete.

    """
    if not config.keep_sandbox:
        try:
            sandbox.delete()
        except (IOError, OSError):
            err_msg = "Couldn't delete sandbox."
            logger.warning("%s\n%s" % (err_msg, traceback.format_exc()))


class TaskType(object):
    """Base class with common operation that (more or less) all task
    types must do sometimes.

    - finish_(compilation, evaluation_testcase, evaluation): these
      finalize the given operation, writing back to the submission the
      new information, and deleting the sandbox if needed;

    - *_sandbox_*: these are utility to create and delete the sandbox,
       and to ask it to do some operation. If the operation fails, the
       sandbox is deleted.

    - compile, evaluate_testcase, evaluate: these actually do the
      operations; must be overloaded.

    """
    # If ALLOW_PARTIAL_SUBMISSION is True, then we allow the user to
    # submit only some of the required files; moreover, we try to fill
    # the non-provided files with the one in the previous submission.
    ALLOW_PARTIAL_SUBMISSION = False

    # A list of all the accepted parameters for this task type.
    # Each item is an instance of TaskTypeParameter.
    ACCEPTED_PARAMETERS = []

    @classmethod
    def parse_handler(cls, handler, prefix):
        """Ensure that the parameters list template agrees with the
        parameters actually passed.

        handler (type): the Tornado handler with the parameters.
        prefix (string): the prefix of the parameter names in the
            handler.

        return (list): parameters list correctly formatted, or
            ValueError if the parameters are not correct.

        """
        new_parameters = []
        for parameter in cls.ACCEPTED_PARAMETERS:
            try:
                new_value = parameter.parse_handler(handler, prefix)
                new_parameters.append(new_value)
            except ValueError as error:
                raise ValueError("Invalid parameter %s: %s."
                                 % (parameter.name, error.message))
        return new_parameters

    def __init__(self, parameters):
        """Instantiate a new TaskType with the given parameters.

        parameters (list): a list of data structures that matches the
            format described in ACCEPTED_PARAMETERS (they often come
            from Dataset.task_type_parameters and, in that case, they
            have to be already decoded from JSON).

        """
        self.parameters = parameters

    @property
    def name(self):
        """Returns the name of the TaskType.

        Returns a human-readable name that is shown to the user in CWS
        to describe this TaskType.

        return (str): the name

        """
        # de-CamelCase the name, capitalize it and return it
        return re.sub("([A-Z])", " \g<1>",
                      self.__class__.__name__).strip().capitalize()

    testable = True

    def get_compilation_commands(self, submission_format):
        """Return the compilation commands for all supported languages

        submission_format ([string]): the list of files provided by the
            user that have to be compiled (the compilation command may
            contain references to other files like graders, stubs, etc...);
            they may contain the string "%l" as a language-wildcard.
        return ({string: [[string]]}|None): for each language (indexed
            by its shorthand code i.e. one of the cms.LANG_* constants)
            provide a list of commands, each as a list of tokens. That
            is because some languages may require multiple operations
            to compile or because some task types may require multiple
            independent compilations (e.g. encoder and decoder); return
            None if no compilation is required (e.g. output only).

        """
        raise NotImplementedError("Please subclass this class.")

    def get_user_managers(self):
        """Return the managers that must be provided by the user when
        requesting a user test.

        return (list of str): a list of filenames (they may include a
                              '%l' as a "language wildcard").

        """
        raise NotImplementedError("Please subclass this class.")

    def get_auto_managers(self):
        """Return the managers that must be provided by the
        EvaluationService (picking them from the Task) when compiling
        or evaluating a user test.

        return (list of str): a list of filenames (they may include a
                             '%l' as a "language wildcard").

        """
        raise NotImplementedError("Please subclass this class.")

    def compile(self, job, file_cacher):
        """Try to compile the given CompilationJob.

        Set job.success to True when *our infrastracture* is successful
        (i.e. the compilation may succeed or fail), and to False when
        the compilation fails because of environmental problems (trying
        again to compile the same submission in a sane environment
        should lead to True).

        job (CompilationJob): the data structure that contains details
                              about the work that has to be done and
                              that will hold its results.
        file_cacher (FileCacher): the file cacher to use to obtain the
                                  required files and to store the ones
                                  that are produced.

        """
        raise NotImplementedError("Please subclass this class.")

    def evaluate(self, job, file_cacher):
        """Try to evaluate the given EvaluationJob.

        Set job.success to True when *our infrastracture* is successful
        (i.e. the actual program may score or not), and to False when
        the evaluation fails because of environmental problems (trying
        again to compile the same submission in a sane environment
        should lead to True).

        job (EvaluationJob): the data structure that contains details
                             about the work that has to be done and
                             that will hold its results.
        file_cacher (FileCacher): the file cacher to use to obtain the
                                  required files and to store the ones
                                  that are produced.

        """
        raise NotImplementedError("Please subclass this class.")

    def execute_job(self, job, file_cacher):
        """Call compile() or execute() depending on the job passed
        when constructing the TaskType.

        """
        if isinstance(job, CompilationJob):
            self.compile(job, file_cacher)
        elif isinstance(job, EvaluationJob):
            self.evaluate(job, file_cacher)
        else:
            raise ValueError("The job isn't neither CompilationJob "
                             "or EvaluationJob")

########NEW FILE########
__FILENAME__ = Batch
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging

from cms import LANGUAGES, LANGUAGE_TO_SOURCE_EXT_MAP
from cms.grading import get_compilation_commands, get_evaluation_commands, \
    compilation_step, evaluation_step, human_evaluation_message, \
    is_evaluation_passed, extract_outcome_and_text, white_diff_step
from cms.grading.ParameterTypes import ParameterTypeCollection, \
    ParameterTypeChoice, ParameterTypeString
from cms.grading.TaskType import TaskType, \
    create_sandbox, delete_sandbox
from cms.db import Executable


logger = logging.getLogger(__name__)


# Dummy function to mark translatable string.
def N_(message):
    return message


class Batch(TaskType):
    """Task type class for a unique standalone submission source, with
    comparator (or not).

    Parameters needs to be a list of three elements.

    The first element is 'grader' or 'alone': in the first
    case, the source file is to be compiled with a provided piece of
    software ('grader'); in the other by itself.

    The second element is a 2-tuple of the input file name and output file
    name. The input file may be '' to denote stdin, and similarly the
    output filename may be '' to denote stdout.

    The third element is 'diff' or 'comparator' and says whether the
    output is compared with a simple diff algorithm or using a
    comparator.

    Note: the first element is used only in the compilation step; the
    others only in the evaluation step.

    A comparator can read argv[1], argv[2], argv[3] (respectively,
    input, correct output and user output) and should write the
    outcome to stdout and the text to stderr.

    """
    ALLOW_PARTIAL_SUBMISSION = False

    _COMPILATION = ParameterTypeChoice(
        "Compilation",
        "compilation",
        "",
        {"alone": "Submissions are self-sufficient",
         "grader": "Submissions are compiled with a grader"})

    _USE_FILE = ParameterTypeCollection(
        "I/O (blank for stdin/stdout)",
        "io",
        "",
        [
            ParameterTypeString("Input file", "inputfile", ""),
            ParameterTypeString("Output file", "outputfile", ""),
        ])

    _EVALUATION = ParameterTypeChoice(
        "Output evaluation",
        "output_eval",
        "",
        {"diff": "Outputs compared with white diff",
         "comparator": "Outputs are compared by a comparator"})

    ACCEPTED_PARAMETERS = [_COMPILATION, _USE_FILE, _EVALUATION]

    @property
    def name(self):
        """See TaskType.name."""
        # TODO add some details if a grader/comparator is used, etc...
        return "Batch"

    def get_compilation_commands(self, submission_format):
        """See TaskType.get_compilation_commands."""
        res = dict()
        for language in LANGUAGES:
            format_filename = submission_format[0]
            source_ext = LANGUAGE_TO_SOURCE_EXT_MAP[language]
            source_filenames = []
            # If a grader is specified, we add to the command line (and to
            # the files to get) the corresponding manager.
            if self.parameters[0] == "grader":
                source_filenames.append("grader%s" % source_ext)
            source_filenames.append(format_filename.replace(".%l", source_ext))
            executable_filename = format_filename.replace(".%l", "")
            commands = get_compilation_commands(language,
                                                source_filenames,
                                                executable_filename)
            res[language] = commands
        return res

    def get_user_managers(self, submission_format):
        """See TaskType.get_user_managers."""
        return []

    def get_auto_managers(self):
        """See TaskType.get_auto_managers."""
        return None

    def compile(self, job, file_cacher):
        """See TaskType.compile."""
        # Detect the submission's language. The checks about the
        # formal correctedness of the submission are done in CWS,
        # before accepting it.
        language = job.language
        source_ext = LANGUAGE_TO_SOURCE_EXT_MAP[language]

        # TODO: here we are sure that submission.files are the same as
        # task.submission_format. The following check shouldn't be
        # here, but in the definition of the task, since this actually
        # checks that task's task type and submission format agree.
        if len(job.files) != 1:
            job.success = True
            job.compilation_success = False
            job.text = [N_("Invalid files in submission")]
            logger.error("Submission contains %d files, expecting 1" %
                         len(job.files), extra={"operation": job.info})
            return True

        # Create the sandbox
        sandbox = create_sandbox(file_cacher)
        job.sandboxes.append(sandbox.path)

        # Prepare the source files in the sandbox
        files_to_get = {}
        format_filename = job.files.keys()[0]
        source_filenames = []
        source_filenames.append(format_filename.replace(".%l", source_ext))
        files_to_get[source_filenames[0]] = \
            job.files[format_filename].digest
        # If a grader is specified, we add to the command line (and to
        # the files to get) the corresponding manager. The grader must
        # be the first file in source_filenames.
        if self.parameters[0] == "grader":
            source_filenames.insert(0, "grader%s" % source_ext)
            files_to_get["grader%s" % source_ext] = \
                job.managers["grader%s" % source_ext].digest

        # Also copy all *.h and *lib.pas graders
        for filename in job.managers.iterkeys():
            if filename.endswith('.h') or \
                    filename.endswith('lib.pas'):
                files_to_get[filename] = \
                    job.managers[filename].digest

        for filename, digest in files_to_get.iteritems():
            sandbox.create_file_from_storage(filename, digest)

        # Prepare the compilation command
        executable_filename = format_filename.replace(".%l", "")
        commands = get_compilation_commands(language,
                                            source_filenames,
                                            executable_filename)

        # Run the compilation
        operation_success, compilation_success, text, plus = \
            compilation_step(sandbox, commands)

        # Retrieve the compiled executables
        job.success = operation_success
        job.compilation_success = compilation_success
        job.plus = plus
        job.text = text
        if operation_success and compilation_success:
            digest = sandbox.get_file_to_storage(
                executable_filename,
                "Executable %s for %s" %
                (executable_filename, job.info))
            job.executables[executable_filename] = \
                Executable(executable_filename, digest)

        # Cleanup
        delete_sandbox(sandbox)

    def evaluate(self, job, file_cacher):
        """See TaskType.evaluate."""
        # Create the sandbox
        sandbox = create_sandbox(file_cacher)

        # Prepare the execution
        executable_filename = job.executables.keys()[0]
        language = job.language
        commands = get_evaluation_commands(language, executable_filename)
        executables_to_get = {
            executable_filename:
            job.executables[executable_filename].digest
            }
        input_filename, output_filename = self.parameters[1]
        stdin_redirect = None
        stdout_redirect = None
        if input_filename == "":
            input_filename = "input.txt"
            stdin_redirect = input_filename
        if output_filename == "":
            output_filename = "output.txt"
            stdout_redirect = output_filename
        files_to_get = {
            input_filename: job.input
            }

        # Put the required files into the sandbox
        for filename, digest in executables_to_get.iteritems():
            sandbox.create_file_from_storage(filename, digest, executable=True)
        for filename, digest in files_to_get.iteritems():
            sandbox.create_file_from_storage(filename, digest)

        # Actually performs the execution
        success, plus = evaluation_step(
            sandbox,
            commands,
            job.time_limit,
            job.memory_limit,
            stdin_redirect=stdin_redirect,
            stdout_redirect=stdout_redirect)

        job.sandboxes = [sandbox.path]
        job.plus = plus

        outcome = None
        text = None

        # Error in the sandbox: nothing to do!
        if not success:
            pass

        # Contestant's error: the marks won't be good
        elif not is_evaluation_passed(plus):
            outcome = 0.0
            text = human_evaluation_message(plus)
            if job.get_output:
                job.user_output = None

        # Otherwise, advance to checking the solution
        else:

            # Check that the output file was created
            if not sandbox.file_exists(output_filename):
                outcome = 0.0
                text = [N_("Evaluation didn't produce file %s"),
                        output_filename]
                if job.get_output:
                    job.user_output = None

            else:
                # If asked so, put the output file into the storage
                if job.get_output:
                    job.user_output = sandbox.get_file_to_storage(
                        output_filename,
                        "Output file in job %s" % job.info,
                        trunc_len=100 * 1024)

                # If not asked otherwise, evaluate the output file
                if not job.only_execution:

                    # Put the reference solution into the sandbox
                    sandbox.create_file_from_storage(
                        "res.txt",
                        job.output)

                    # Check the solution with white_diff
                    if self.parameters[2] == "diff":
                        outcome, text = white_diff_step(
                            sandbox, output_filename, "res.txt")

                    # Check the solution with a comparator
                    elif self.parameters[2] == "comparator":
                        manager_filename = "checker"

                        if not manager_filename in job.managers:
                            logger.error("Configuration error: missing or "
                                         "invalid comparator (it must be "
                                         "named 'checker')",
                                         extra={"operation": job.info})
                            success = False

                        else:
                            sandbox.create_file_from_storage(
                                manager_filename,
                                job.managers[manager_filename].digest,
                                executable=True)
                            success, _ = evaluation_step(
                                sandbox,
                                [["./%s" % manager_filename,
                                  input_filename, "res.txt", output_filename]])
                        if success:
                            try:
                                outcome, text = \
                                    extract_outcome_and_text(sandbox)
                            except ValueError, e:
                                logger.error("Invalid output from "
                                             "comparator: %s" % (e.message,),
                                             extra={"operation": job.info})
                                success = False

                    else:
                        raise ValueError("Unrecognized third parameter"
                                         " `%s' for Batch tasktype." %
                                         self.parameters[2])

        # Whatever happened, we conclude.
        job.success = success
        job.outcome = str(outcome) if outcome is not None else None
        job.text = text

        delete_sandbox(sandbox)

########NEW FILE########
__FILENAME__ = Communication
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging
import os
import tempfile

from cms import LANGUAGES, LANGUAGE_TO_SOURCE_EXT_MAP, config
from cms.grading.Sandbox import wait_without_std
from cms.grading import get_compilation_commands, compilation_step, \
    human_evaluation_message, is_evaluation_passed, \
    extract_outcome_and_text, evaluation_step_before_run, \
    evaluation_step_after_run
from cms.grading.TaskType import TaskType, \
    create_sandbox, delete_sandbox
from cms.db import Executable
from cms.io.GeventUtils import rmtree


logger = logging.getLogger(__name__)


# Dummy function to mark translatable string.
def N_(message):
    return message


class Communication(TaskType):
    """Task type class for tasks that requires:

    - a *manager* that reads the input file, work out the perfect
      solution on its own, and communicate the input (maybe with some
      modifications) on its standard output; it then reads the
      response of the user's solution from the standard input and
      write the outcome;

    - a *stub* that compiles with the user's source, reads from
      standard input what the manager says, and write back the user's
      solution to stdout.

    """
    ALLOW_PARTIAL_SUBMISSION = False

    name = "Communication"

    def get_compilation_commands(self, submission_format):
        """See TaskType.get_compilation_commands."""
        res = dict()
        for language in LANGUAGES:
            format_filename = submission_format[0]
            source_ext = LANGUAGE_TO_SOURCE_EXT_MAP[language]
            source_filenames = []
            source_filenames.append("stub%s" % source_ext)
            source_filenames.append(format_filename.replace(".%l", source_ext))
            executable_filename = format_filename.replace(".%l", "")
            commands = get_compilation_commands(language,
                                                source_filenames,
                                                executable_filename)
            res[language] = commands
        return res

    def get_user_managers(self, submission_format):
        """See TaskType.get_user_managers."""
        return ["stub.%l"]

    def get_auto_managers(self):
        """See TaskType.get_auto_managers."""
        return ["manager"]

    def compile(self, job, file_cacher):
        """See TaskType.compile."""
        # Detect the submission's language. The checks about the
        # formal correctedness of the submission are done in CWS,
        # before accepting it.
        language = job.language
        source_ext = LANGUAGE_TO_SOURCE_EXT_MAP[language]

        # TODO: here we are sure that submission.files are the same as
        # task.submission_format. The following check shouldn't be
        # here, but in the definition of the task, since this actually
        # checks that task's task type and submission format agree.
        if len(job.files) != 1:
            job.success = True
            job.compilation_success = False
            job.text = [N_("Invalid files in submission")]
            logger.error("Submission contains %d files, expecting 1" %
                         len(job.files), extra={"operation": job.info})
            return True

        # Create the sandbox
        sandbox = create_sandbox(file_cacher)
        job.sandboxes.append(sandbox.path)

        # Prepare the source files in the sandbox
        files_to_get = {}
        format_filename = job.files.keys()[0]
        source_filenames = []
        # Stub.
        source_filenames.append("stub%s" % source_ext)
        files_to_get[source_filenames[-1]] = \
            job.managers["stub%s" % source_ext].digest
        # User's submission.
        source_filenames.append(format_filename.replace(".%l", source_ext))
        files_to_get[source_filenames[-1]] = \
            job.files[format_filename].digest
        for filename, digest in files_to_get.iteritems():
            sandbox.create_file_from_storage(filename, digest)

        # Prepare the compilation command
        executable_filename = format_filename.replace(".%l", "")
        commands = get_compilation_commands(language,
                                            source_filenames,
                                            executable_filename)

        # Run the compilation
        operation_success, compilation_success, text, plus = \
            compilation_step(sandbox, commands)

        # Retrieve the compiled executables
        job.success = operation_success
        job.compilation_success = compilation_success
        job.plus = plus
        job.text = text
        if operation_success and compilation_success:
            digest = sandbox.get_file_to_storage(
                executable_filename,
                "Executable %s for %s" %
                (executable_filename, job.info))
            job.executables[executable_filename] = \
                Executable(executable_filename, digest)

        # Cleanup
        delete_sandbox(sandbox)

    def evaluate(self, job, file_cacher):
        """See TaskType.evaluate."""
        # Create sandboxes and FIFOs
        sandbox_mgr = create_sandbox(file_cacher)
        sandbox_user = create_sandbox(file_cacher)
        fifo_dir = tempfile.mkdtemp(dir=config.temp_dir)
        fifo_in = os.path.join(fifo_dir, "in")
        fifo_out = os.path.join(fifo_dir, "out")
        os.mkfifo(fifo_in)
        os.mkfifo(fifo_out)
        os.chmod(fifo_dir, 0o755)
        os.chmod(fifo_in, 0o666)
        os.chmod(fifo_out, 0o666)

        # First step: we start the manager.
        manager_filename = "manager"
        manager_command = ["./%s" % manager_filename, fifo_in, fifo_out]
        manager_executables_to_get = {
            manager_filename:
            job.managers[manager_filename].digest
            }
        manager_files_to_get = {
            "input.txt": job.input
            }
        manager_allow_dirs = [fifo_dir]
        for filename, digest in manager_executables_to_get.iteritems():
            sandbox_mgr.create_file_from_storage(
                filename, digest, executable=True)
        for filename, digest in manager_files_to_get.iteritems():
            sandbox_mgr.create_file_from_storage(filename, digest)
        manager = evaluation_step_before_run(
            sandbox_mgr,
            manager_command,
            job.time_limit,
            0,
            allow_dirs=manager_allow_dirs,
            stdin_redirect="input.txt")

        # Second step: we start the user submission compiled with the
        # stub.
        executable_filename = job.executables.keys()[0]
        command = ["./%s" % executable_filename, fifo_out, fifo_in]
        executables_to_get = {
            executable_filename:
            job.executables[executable_filename].digest
            }
        user_allow_dirs = [fifo_dir]
        for filename, digest in executables_to_get.iteritems():
            sandbox_user.create_file_from_storage(
                filename, digest, executable=True)
        process = evaluation_step_before_run(
            sandbox_user,
            command,
            job.time_limit,
            job.memory_limit,
            allow_dirs=user_allow_dirs)

        # Consume output.
        wait_without_std([process, manager])
        # TODO: check exit codes with translate_box_exitcode.

        success_user, plus_user = \
            evaluation_step_after_run(sandbox_user)
        success_mgr, plus_mgr = \
            evaluation_step_after_run(sandbox_mgr)

        job.sandboxes = [sandbox_user.path,
                         sandbox_mgr.path]
        job.plus = plus_user

        # If at least one evaluation had problems, we report the
        # problems.
        if not success_user or not success_mgr:
            success, outcome, text = False, None, None
        # If the user sandbox detected some problem (timeout, ...),
        # the outcome is 0.0 and the text describes that problem.
        elif not is_evaluation_passed(plus_user):
            success = True
            outcome, text = 0.0, human_evaluation_message(plus_user)
        # Otherwise, we use the manager to obtain the outcome.
        else:
            success = True
            outcome, text = extract_outcome_and_text(sandbox_mgr)

        # If asked so, save the output file, provided that it exists
        if job.get_output:
            if sandbox_mgr.file_exists("output.txt"):
                job.user_output = sandbox_mgr.get_file_to_storage(
                    "output.txt",
                    "Output file in job %s" % job.info)
            else:
                job.user_output = None

        # Whatever happened, we conclude.
        job.success = success
        job.outcome = str(outcome) if outcome is not None else None
        job.text = text

        delete_sandbox(sandbox_mgr)
        delete_sandbox(sandbox_user)
        rmtree(fifo_dir)

########NEW FILE########
__FILENAME__ = OutputOnly
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Task type for output only tasks.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging

from cms.grading.TaskType import TaskType, \
    create_sandbox, delete_sandbox
from cms.grading.ParameterTypes import ParameterTypeChoice
from cms.grading import white_diff_step, evaluation_step, \
    extract_outcome_and_text


logger = logging.getLogger(__name__)


# Dummy function to mark translatable string.
def N_(message):
    return message


class OutputOnly(TaskType):
    """Task type class for output only tasks, with submission composed
    of testcase_number text files, to be evaluated diffing or using a
    comparator.

    Parameters are a list of string with one element (for future
    possible expansions), which maybe 'diff' or 'comparator', meaning that
    the evaluation is done via white diff or via a comparator.

    """
    ALLOW_PARTIAL_SUBMISSION = True

    _EVALUATION = ParameterTypeChoice(
        "Output evaluation",
        "output_eval",
        "",
        {"diff": "Outputs compared with white diff",
         "comparator": "Outputs are compared by a comparator"})

    ACCEPTED_PARAMETERS = [_EVALUATION]

    @property
    def name(self):
        """See TaskType.name."""
        # TODO add some details if a comparator is used, etc...
        return "Output only"

    testable = False

    def get_compilation_commands(self, submission_format):
        """See TaskType.get_compilation_commands."""
        return None

    def get_user_managers(self, submission_format):
        """See TaskType.get_user_managers."""
        return []

    def get_auto_managers(self):
        """See TaskType.get_auto_managers."""
        return []

    def compile(self, job, file_cacher):
        """See TaskType.compile."""
        # No compilation needed.
        job.success = True
        job.compilation_success = True
        job.text = [N_("No compilation needed")]
        job.plus = {}

    def evaluate(self, job, file_cacher):
        """See TaskType.evaluate."""
        sandbox = create_sandbox(file_cacher)
        job.sandboxes.append(sandbox.path)

        # Immediately prepare the skeleton to return
        job.sandboxes = [sandbox.path]
        job.plus = {}

        outcome = None
        text = None

        # Since we allow partial submission, if the file is not
        # present we report that the outcome is 0.
        if "output_%s.txt" % job._key not in job.files:
            job.success = True
            job.outcome = "0.0"
            job.text = [N_("File not submitted")]
            return True

        # First and only one step: diffing (manual or with manager).
        output_digest = job.files["output_%s.txt" %
                                  job._key].digest

        # Put the files into the sandbox
        sandbox.create_file_from_storage(
            "res.txt",
            job.output)
        sandbox.create_file_from_storage(
            "output.txt",
            output_digest)

        if self.parameters[0] == "diff":
            # No manager: I'll do a white_diff between the submission
            # file and the correct output res.txt.
            success = True
            outcome, text = white_diff_step(
                sandbox, "output.txt", "res.txt")

        elif self.parameters[0] == "comparator":
            # Manager present: wonderful, he'll do all the job.
            manager_filename = "checker"
            if not manager_filename in job.managers:
                logger.error("Configuration error: missing or "
                             "invalid comparator (it must be "
                             "named `checker')", extra={"operation": job.info})
                success = False
            else:
                sandbox.create_file_from_storage(
                    manager_filename,
                    job.managers[manager_filename].digest,
                    executable=True)
                input_digest = job.input
                sandbox.create_file_from_storage(
                    "input.txt",
                    input_digest)
                success, _ = evaluation_step(
                    sandbox,
                    [["./%s" % manager_filename,
                      "input.txt", "res.txt", "output.txt"]])
                if success:
                    outcome, text = extract_outcome_and_text(sandbox)

        else:
            raise ValueError("Unrecognized first parameter "
                             "`%s' for OutputOnly tasktype. "
                             "Should be `diff' or `comparator'." %
                             self.parameters[0])

        # Whatever happened, we conclude.
        job.success = success
        job.outcome = str(outcome) if outcome is not None else None
        job.text = text

        delete_sandbox(sandbox)

########NEW FILE########
__FILENAME__ = TwoSteps
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2014 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging
import os
import tempfile

from cms import LANGUAGES, LANGUAGE_TO_SOURCE_EXT_MAP, \
    LANGUAGE_TO_HEADER_EXT_MAP, config
from cms.grading.Sandbox import wait_without_std
from cms.grading import get_compilation_commands, compilation_step, \
    evaluation_step_before_run, evaluation_step_after_run, \
    is_evaluation_passed, human_evaluation_message, white_diff_step
from cms.grading.TaskType import TaskType, \
    create_sandbox, delete_sandbox
from cms.db import Executable


logger = logging.getLogger(__name__)


# Dummy function to mark translatable string.
def N_(message):
    return message


class TwoSteps(TaskType):
    """Task type class for tasks where the user must submit two files
    with a function each; the first function compute some data, that
    get passed to the second function that must recover some data.

    The admins must provide a manager source file (for each language),
    called manager.%l, that get compiled with both two user sources,
    get the input as stdin, and get two parameters: 0 if it is the
    first instance, 1 if it is the second instance, and the name of
    the pipe.

    Admins must provide also header files, named "foo{.h|lib.pas}" for
    the three sources (manager and user provided).

    """
    ALLOW_PARTIAL_SUBMISSION = False

    name = "Two steps"

    def get_compilation_commands(self, submission_format):
        """See TaskType.get_compilation_commands."""
        res = dict()
        for language in LANGUAGES:
            source_ext = LANGUAGE_TO_SOURCE_EXT_MAP[language]
            header_ext = LANGUAGE_TO_HEADER_EXT_MAP[language]
            source_filenames = []
            for filename in submission_format:
                source_filename = filename.replace(".%l", source_ext)
                source_filenames.append(source_filename)
                # Headers.
                header_filename = filename.replace(".%l", header_ext)
                source_filenames.append(header_filename)

            # Manager.
            manager_source_filename = "manager%s" % source_ext
            source_filenames.append(manager_source_filename)
            # Manager's header.
            manager_header_filename = "manager%s" % header_ext
            source_filenames.append(manager_header_filename)

            # Get compilation command and compile.
            executable_filename = "manager"
            commands = get_compilation_commands(language,
                                                source_filenames,
                                                executable_filename)
            res[language] = commands
        return res

    def compile(self, job, file_cacher):
        """See TaskType.compile."""
        # Detect the submission's language. The checks about the
        # formal correctedness of the submission are done in CWS,
        # before accepting it.
        language = job.language
        source_ext = LANGUAGE_TO_SOURCE_EXT_MAP[language]
        header_ext = LANGUAGE_TO_HEADER_EXT_MAP[language]

        # TODO: here we are sure that submission.files are the same as
        # task.submission_format. The following check shouldn't be
        # here, but in the definition of the task, since this actually
        # checks that task's task type and submission format agree.
        if len(job.files) != 2:
            job.success = True
            job.compilation_success = False
            job.text = [N_("Invalid files in submission")]
            logger.error("Submission contains %d files, expecting 2" %
                         len(job.files), extra={"operation": job.info})
            return True

        # First and only one compilation.
        sandbox = create_sandbox(file_cacher)
        job.sandboxes.append(sandbox.path)
        files_to_get = {}

        # User's submissions and headers.
        source_filenames = []
        for filename, file_ in job.files.iteritems():
            source_filename = filename.replace(".%l", source_ext)
            source_filenames.append(source_filename)
            files_to_get[source_filename] = file_.digest
            # Headers.
            header_filename = filename.replace(".%l", header_ext)
            source_filenames.append(header_filename)
            files_to_get[header_filename] = \
                job.managers[header_filename].digest

        # Manager.
        manager_filename = "manager%s" % source_ext
        source_filenames.append(manager_filename)
        files_to_get[manager_filename] = \
            job.managers[manager_filename].digest
        # Manager's header.
        manager_filename = "manager%s" % header_ext
        source_filenames.append(manager_filename)
        files_to_get[manager_filename] = \
            job.managers[manager_filename].digest

        for filename, digest in files_to_get.iteritems():
            sandbox.create_file_from_storage(filename, digest)

        # Get compilation command and compile.
        executable_filename = "manager"
        commands = get_compilation_commands(language,
                                            source_filenames,
                                            executable_filename)
        operation_success, compilation_success, text, plus = \
            compilation_step(sandbox, commands)

        # Retrieve the compiled executables
        job.success = operation_success
        job.compilation_success = compilation_success
        job.plus = plus
        job.text = text
        if operation_success and compilation_success:
            digest = sandbox.get_file_to_storage(
                executable_filename,
                "Executable %s for %s" %
                (executable_filename, job.info))
            job.executables[executable_filename] = \
                Executable(executable_filename, digest)

        # Cleanup
        delete_sandbox(sandbox)

    def evaluate(self, job, file_cacher):
        """See TaskType.evaluate."""
        # f stand for first, s for second.
        first_sandbox = create_sandbox(file_cacher)
        second_sandbox = create_sandbox(file_cacher)
        fifo_dir = tempfile.mkdtemp(dir=config.temp_dir)
        fifo = os.path.join(fifo_dir, "fifo")
        os.mkfifo(fifo)
        os.chmod(fifo_dir, 0o755)
        os.chmod(fifo, 0o666)

        # First step: we start the first manager.
        first_filename = "manager"
        first_command = ["./%s" % first_filename, "0", fifo]
        first_executables_to_get = {
            first_filename:
            job.executables[first_filename].digest
            }
        first_files_to_get = {
            "input.txt": job.input
            }
        first_allow_path = [fifo_dir]

        # Put the required files into the sandbox
        for filename, digest in first_executables_to_get.iteritems():
            first_sandbox.create_file_from_storage(filename,
                                                   digest,
                                                   executable=True)
        for filename, digest in first_files_to_get.iteritems():
            first_sandbox.create_file_from_storage(filename, digest)

        first = evaluation_step_before_run(
            first_sandbox,
            first_command,
            job.time_limit,
            job.memory_limit,
            first_allow_path,
            stdin_redirect="input.txt",
            wait=False)

        # Second step: we start the second manager.
        second_filename = "manager"
        second_command = ["./%s" % second_filename, "1", fifo]
        second_executables_to_get = {
            second_filename:
            job.executables[second_filename].digest
            }
        second_files_to_get = {}
        second_allow_path = [fifo_dir]

        # Put the required files into the second sandbox
        for filename, digest in second_executables_to_get.iteritems():
            second_sandbox.create_file_from_storage(filename,
                                                    digest,
                                                    executable=True)
        for filename, digest in second_files_to_get.iteritems():
            second_sandbox.create_file_from_storage(filename, digest)

        second = evaluation_step_before_run(
            second_sandbox,
            second_command,
            job.time_limit,
            job.memory_limit,
            second_allow_path,
            stdout_redirect="output.txt",
            wait=False)

        # Consume output.
        wait_without_std([second, first])
        # TODO: check exit codes with translate_box_exitcode.

        success_first, first_plus = \
            evaluation_step_after_run(first_sandbox)
        success_second, second_plus = \
            evaluation_step_after_run(second_sandbox)

        job.sandboxes = [first_sandbox.path,
                         second_sandbox.path]
        job.plus = second_plus

        success = True
        outcome = None
        text = None

        # Error in the sandbox: report failure!
        if not success_first or not success_second:
            success = False

        # Contestant's error: the marks won't be good
        elif not is_evaluation_passed(first_plus) or \
                not is_evaluation_passed(second_plus):
            outcome = 0.0
            if not is_evaluation_passed(first_plus):
                text = human_evaluation_message(first_plus)
            else:
                text = human_evaluation_message(second_plus)
            if job.get_output:
                job.user_output = None

        # Otherwise, advance to checking the solution
        else:

            # Check that the output file was created
            if not second_sandbox.file_exists('output.txt'):
                outcome = 0.0
                text = [N_("Evaluation didn't produce file %s"), "output.txt"]
                if job.get_output:
                    job.user_output = None

            else:
                # If asked so, put the output file into the storage
                if job.get_output:
                    job.user_output = second_sandbox.get_file_to_storage(
                        "output.txt",
                        "Output file in job %s" % job.info)

                # If not asked otherwise, evaluate the output file
                if not job.only_execution:
                    # Put the reference solution into the sandbox
                    second_sandbox.create_file_from_storage(
                        "res.txt",
                        job.output)

                    outcome, text = white_diff_step(
                        second_sandbox, "output.txt", "res.txt")

        # Whatever happened, we conclude.
        job.success = success
        job.outcome = str(outcome)
        job.text = text

        delete_sandbox(first_sandbox)
        delete_sandbox(second_sandbox)

########NEW FILE########
__FILENAME__ = GeventUtils
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# Some code snippets have been taken and readapted from file shutil.py
# in Python 2.7. For such pieces this copyright applies:
#
# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007 Python
# Software Foundation; All Rights Reserved
#
# They're distributed under the terms of the Python Software
# Foundation license version 2, which can be be found at
# <http://www.python.org/download/releases/2.7/license/>.
#
# You can find the original file at:
# http://hg.python.org/cpython/file/ab05e7dad2788/Lib/shutil.py

"""A collection of misc utilities that behave nicely towards gevent,
yielding from time to time in order to pass the control to other
greenlets.

If gevent is not being used, these routines will behave in the common
blocking way.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import os
import stat
import sys

# There are some calls that we take from shutil, since they're
# hopefully not taking too long and can avoid yielding
from shutil import copymode, copystat, _samefile, Error, \
    SpecialFileError, _basename, WindowsError, _destinsrc

import gevent


# XXX Use buffer_size=io.DEFAULT_BUFFER_SIZE?
def copyfileobj(fsrc, fdst, buffer_size=16 * 1024):
    """copy data from file-like object fsrc to file-like object fdst"""
    # file.write() behaves differently from io.FileIO.write(): the
    # latter returns the number of bytes written, the former doesn't.
    # This functions tries to support both behaviors by detecting which
    # one is used.
    buf = fsrc.read(buffer_size)
    while len(buf) > 0:
        while len(buf) > 0:
            written = fdst.write(buf)
            # Cooperative yield.
            gevent.sleep(0)
            if written is None:
                break
            buf = buf[written:]
        buf = fsrc.read(buffer_size)


def copyfile(src, dst):
    """Copy data from src to dst"""
    if _samefile(src, dst):
        raise Error("`%s` and `%s` are the same file" % (src, dst))

    for fn in [src, dst]:
        try:
            st = os.stat(fn)
        except OSError:
            # File most likely does not exist
            pass
        else:
            # XXX What about other special files? (sockets, devices...)
            if stat.S_ISFIFO(st.st_mode):
                raise SpecialFileError("`%s` is a named pipe" % fn)

    with open(src, 'rb') as fsrc:
        with open(dst, 'wb') as fdst:
            copyfileobj(fsrc, fdst)


def copy(src, dst):
    """Copy data and mode bits ("cp src dst").

    The destination may be a directory.

    """
    if os.path.isdir(dst):
        dst = os.path.join(dst, os.path.basename(src))
    copyfile(src, dst)
    copymode(src, dst)


def copy2(src, dst):
    """Copy data and all stat info ("cp -p src dst").

    The destination may be a directory.

    """
    if os.path.isdir(dst):
        dst = os.path.join(dst, os.path.basename(src))
    copyfile(src, dst)
    copystat(src, dst)


def copytree(src, dst, symlinks=False, ignore=None):
    """Recursively copy a directory tree using copy2().

    The destination directory must not already exist.
    If exception(s) occur, an Error is raised with a list of reasons.

    If the optional symlinks flag is true, symbolic links in the
    source tree result in symbolic links in the destination tree; if
    it is false, the contents of the files pointed to by symbolic
    links are copied.

    The optional ignore argument is a callable. If given, it
    is called with the `src` parameter, which is the directory
    being visited by copytree(), and `names` which is the list of
    `src` contents, as returned by os.listdir():

        callable(src, names) -> ignored_names

    Since copytree() is called recursively, the callable will be
    called once for each directory that is copied. It returns a
    list of names relative to the `src` directory that should
    not be copied.

    XXX Consider this example code rather than the ultimate tool.

    """
    names = os.listdir(src)
    if ignore is not None:
        ignored_names = ignore(src, names)
    else:
        ignored_names = set()

    os.makedirs(dst)
    errors = []
    for name in names:
        if name in ignored_names:
            continue
        srcname = os.path.join(src, name)
        dstname = os.path.join(dst, name)
        try:
            if symlinks and os.path.islink(srcname):
                linkto = os.readlink(srcname)
                os.symlink(linkto, dstname)
            elif os.path.isdir(srcname):
                copytree(srcname, dstname, symlinks, ignore)
                gevent.sleep(0)
            else:
                # Will raise a SpecialFileError for unsupported file types
                copy2(srcname, dstname)
        # catch the Error from the recursive copytree so that we can
        # continue with other files
        except Error, err:
            errors.extend(err.args[0])
        except EnvironmentError, why:
            errors.append((srcname, dstname, str(why)))
    try:
        copystat(src, dst)
    except OSError, why:
        if WindowsError is not None and isinstance(why, WindowsError):
            # Copying file access times may fail on Windows
            pass
        else:
            errors.extend((src, dst, str(why)))
    if errors:
        raise Error(errors)


def rmtree(path, ignore_errors=False, onerror=None):
    """Recursively delete a directory tree.

    If ignore_errors is set, errors are ignored; otherwise, if onerror
    is set, it is called to handle the error with arguments (func,
    path, exc_info) where func is os.listdir, os.remove, or os.rmdir;
    path is the argument to that function that caused it to fail; and
    exc_info is a tuple returned by sys.exc_info().  If ignore_errors
    is false and onerror is None, an exception is raised.

    """
    if ignore_errors:
        def onerror(*args):
            pass
    elif onerror is None:
        def onerror(*args):
            raise
    try:
        if os.path.islink(path):
            # symlinks to directories are forbidden, see bug #1669
            raise OSError("Cannot call rmtree on a symbolic link")
    except OSError:
        onerror(os.path.islink, path, sys.exc_info())
        # can't continue even if onerror hook returns
        return
    names = []
    try:
        names = os.listdir(path)
    except os.error:
        onerror(os.listdir, path, sys.exc_info())
    for name in names:
        fullname = os.path.join(path, name)
        try:
            mode = os.lstat(fullname).st_mode
        except os.error:
            mode = 0
        if stat.S_ISDIR(mode):
            rmtree(fullname, ignore_errors, onerror)
            gevent.sleep(0)
        else:
            try:
                os.remove(fullname)
            except os.error:
                onerror(os.remove, fullname, sys.exc_info())
    try:
        os.rmdir(path)
    except os.error:
        onerror(os.rmdir, path, sys.exc_info())


def move(src, dst):
    """Recursively move a file or directory to another location. This is
    similar to the Unix "mv" command.

    If the destination is a directory or a symlink to a directory, the source
    is moved inside the directory. The destination path must not already
    exist.

    If the destination already exists but is not a directory, it may be
    overwritten depending on os.rename() semantics.

    If the destination is on our current filesystem, then rename() is used.
    Otherwise, src is copied to the destination and then removed.
    A lot more could be done here...  A look at a mv.c shows a lot of
    the issues this implementation glosses over.

    """
    real_dst = dst
    if os.path.isdir(dst):
        if _samefile(src, dst):
            # We might be on a case insensitive filesystem,
            # perform the rename anyway.
            os.rename(src, dst)
            return

        real_dst = os.path.join(dst, _basename(src))
        if os.path.exists(real_dst):
            raise Error("Destination path '%s' already exists" % real_dst)
    try:
        os.rename(src, real_dst)
    except OSError:
        if os.path.isdir(src):
            if _destinsrc(src, dst):
                raise Error("Cannot move a directory '%s' "
                            "into itself '%s'." % (src, dst))
            copytree(src, real_dst, symlinks=True)
            rmtree(src)
        else:
            copy2(src, real_dst)
            os.unlink(src)

########NEW FILE########
__FILENAME__ = PsycoGevent
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# This file was taken from
# https://bitbucket.org/zzzeek/green_sqla/src/2732bb7ea9d06b9d4a61e8c \
# d587a95148ce2599b/green_sqla/psyco_gevent.py?at=default

"""A wait callback to allow psycopg2 cooperation with gevent.

Use `make_psycopg_green()` to enable gevent support in Psycopg.

"""

# Copyright (C) 2010 Daniele Varrazzo <daniele.varrazzo@gmail.com>
# Copyright (C) 2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# and licensed under the MIT license:
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

from contextlib import contextmanager

import psycopg2
from psycopg2 import extensions

from gevent.socket import wait_read, wait_write


def make_psycopg_green():
    """Configure Psycopg to be used with gevent in non-blocking way."""
    if not hasattr(extensions, 'set_wait_callback'):
        raise ImportError(
            "support for coroutines not available in this Psycopg version (%s)"
            % psycopg2.__version__)

    extensions.set_wait_callback(gevent_wait_callback)


def unmake_psycopg_green():
    """Undo make_psycopg_green()."""
    if not hasattr(extensions, 'set_wait_callback'):
        raise ImportError(
            "support for coroutines not available in this Psycopg version (%s)"
            % psycopg2.__version__)

    extensions.set_wait_callback(None)


def is_psycopg_green():
    """Test whether gevent compatibility layer is installed in psycopg."""
    if not hasattr(extensions, 'set_wait_callback'):
        raise ImportError(
            "support for coroutines not available in this Psycopg version (%s)"
            % psycopg2.__version__)

    return extensions.get_wait_callback() == gevent_wait_callback


def gevent_wait_callback(conn, timeout=None):
    """A wait callback useful to allow gevent to work with Psycopg."""
    while 1:
        state = conn.poll()
        if state == extensions.POLL_OK:
            break
        elif state == extensions.POLL_READ:
            wait_read(conn.fileno(), timeout=timeout)
        elif state == extensions.POLL_WRITE:
            wait_write(conn.fileno(), timeout=timeout)
        else:
            raise psycopg2.OperationalError(
                "Bad result from poll: %r" % state)


@contextmanager
def ungreen_psycopg():
    """Temporarily disable gevent support in psycopg.

    Inside this context manager you can use psycopg's features that
    are not compatible with coroutine support, such as large
    objects. Of course, at the expense of being blocking, so please
    stay inside the context manager as short as possible.

    """
    is_green = is_psycopg_green()
    if is_green:
        unmake_psycopg_green()
    try:
        yield
    finally:
        if is_green:
            make_psycopg_green()

########NEW FILE########
__FILENAME__ = rpc
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2014 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import functools
import json
import logging
import socket
import traceback
import uuid
from weakref import WeakSet

import gevent
import gevent.coros
import gevent.socket
import gevent.event

from cms import get_service_address


logger = logging.getLogger(__name__)


class RPCError(Exception):
    """Generic error during RPC communication."""
    pass


def rpc_method(func):
    """Decorator for a method that other services are allowed to call.

    Does not do a lot, just defines the right method's attribute.

    func (function): the method to make RPC callable.
    return (function): the decorated method.

    """
    func.rpc_callable = True
    return func


class RemoteServiceBase(object):
    """Base class for both ends of a RPC connection.

    Just provides some basic helpers for I/O. It alternates between two
    states:
    - disconnected (when self.connected is False) means that no socket
      is bound and therefore no I/O can be performed. Attempting to
      read or write will fail. This is the default state.
    - connected (when self.connected is True) means that a socket is
      bound and (presumably) active. I/O can be performed. This state
      can be entered by calling initialize and can be exited by calling
      disconnect. It will also be exited when errors occur.

    When the state changes the on_connect or on_disconnect handlers
    will be fired.

    """
    # Incoming messages larger than 1 MiB are dropped to avoid DOS
    # attacks. XXX Check that this size is sensible.
    MAX_MESSAGE_SIZE = 1024 * 1024

    def __init__(self, remote_address):
        """Prepare to handle a connection with the given remote address.

        remote_address (Address): the address of the other end of the
            connection (origin or target, depending on its direction).

        """
        self.remote_address = remote_address
        self.connected = False

        self._on_connect_handlers = list()
        self._on_disconnect_handlers = list()

    def add_on_connect_handler(self, handler):
        """Register a callback for connection establishment.

        handler (function): a no-args callable that gets notified when
            a new connection has been established.

        """
        self._on_connect_handlers.append(handler)

    def add_on_disconnect_handler(self, handler):
        """Register a callback for connection termination.

        handler (function): a no-args callable that gets notified when
            a connection has been closed.

        """
        self._on_disconnect_handlers.append(handler)

    def _repr_remote(self):
        """Describe the other end of the connection.

        return (unicode): a human-readable sensible identifier for the
            remote address, for use in log messages and exceptions.

        """
        return "%s:%d" % (self.remote_address)

    def initialize(self, sock, plus):
        """Activate the communication on the given socket.

        Put this class in its "connected" state, setting up all needed
        attributes. Call the on_connect callback.

        sock (socket): the socket acting as low-level communication
            channel.
        plus (object): object to pass to the on_connect callbacks.

        """
        if self.connected:
            raise RuntimeError("Already connected.")

        self._socket = sock
        self._reader = self._socket.makefile('rb')
        self._writer = self._socket.makefile('wb')
        self._read_lock = gevent.coros.RLock()
        self._write_lock = gevent.coros.RLock()
        self.connected = True

        logger.info("Established connection with %s.", self._repr_remote())

        for handler in self._on_connect_handlers:
            gevent.spawn(handler, plus)

    def finalize(self, reason=""):
        """Deactivate the communication on the current socket.

        Remove all I/O related attributes and take the class back to
        the disconnected state. Call the on_disconnect callback.

        reason (unicode): the human-readable reason for closing the
            connection, to be put in log messages and exceptions.

        """
        if not self.connected:
            return

        self.__dict__.pop("_socket", None)
        self.__dict__.pop("_reader", None)
        self.__dict__.pop("_writer", None)
        self.__dict__.pop("_read_lock", None)
        self.__dict__.pop("_write_lock", None)
        self.connected = False

        logger.info("Terminated connection with %s: %s", self._repr_remote(),
                    reason)

        for handler in self._on_disconnect_handlers:
            gevent.spawn(handler)

    def disconnect(self):
        """Gracefully close the connection.

        """
        if not self.connected:
            return

        try:
            self._socket.shutdown(socket.SHUT_RDWR)
            self._socket.close()
        except socket.error as error:
            logger.debug("Couldn't disconnect from %s: %s.",
                         self._repr_remote(), error)
        finally:
            self.finalize("Disconnection requested.")

    def _read(self):
        """Receive a message from the socket.

        Read from the socket until a "\\r\\n" is found. That is what we
        consider a "message" in the communication protocol.

        return (bytes): the retrieved message.

        raise (IOError): if reading fails.

        """
        if not self.connected:
            raise IOError("Not connected.")

        try:
            with self._read_lock:
                if not self.connected:
                    raise IOError("Not connected.")
                data = self._reader.readline(self.MAX_MESSAGE_SIZE)
                # If there weren't a "\r\n" between the last message
                # and the EOF we would have a false positive here.
                # Luckily there is one.
                if len(data) > 0 and not data.endswith(b"\r\n"):
                    logger.error(
                        "The client sent a message larger than %d bytes (that "
                        "is MAX_MESSAGE_SIZE). Consider raising that value if "
                        "the message seemed legit.", self.MAX_MESSAGE_SIZE)
                    self.finalize("Client misbehaving.")
                    raise IOError("Message too long.")
        except socket.error as error:
            logger.warning("Failed reading from socket: %s.", error)
            self.finalize("Read failed.")
            raise error

        return data

    def _write(self, data):
        """Send a message to the socket.

        Automatically append "\\r\\n" to make it a correct message.

        data (bytes): the message to transmit.

        raise (IOError): if writing fails.

        """
        if not self.connected:
            raise IOError("Not connected.")

        if len(data + b'\r\n') > self.MAX_MESSAGE_SIZE:
            logger.error(
                "A message wasn't sent to %r because it was larger than %d "
                "bytes (that is MAX_MESSAGE_SIZE). Consider raising that "
                "value if the message seemed legit.", self._repr_remote(),
                self.MAX_MESSAGE_SIZE)
            # No need to call finalize.
            raise IOError("Message too long.")

        try:
            with self._write_lock:
                if not self.connected:
                    raise IOError("Not connected.")
                # Does the same as self._socket.sendall.
                self._writer.write(data + b'\r\n')
                self._writer.flush()
        except socket.error as error:
            logger.warning("Failed writing to socket: %s.", error)
            self.finalize("Write failed.")
            raise error


class RemoteServiceServer(RemoteServiceBase):
    """The server side of a RPC communication.

    Considers all messages coming from the other end as requests for
    RPCs executions. Will perform them and send results as responses.

    After having created an instance and initialized it with a socket
    the reader loop should be started by calling run.

    """
    def __init__(self, local_service, remote_address):
        """Create a responder for the given service.

        local_service (Service): the object whose methods should be
            called via RPC.

        For other arguments see RemoteServiceBase.

        """
        super(RemoteServiceServer, self).__init__(remote_address)
        self.local_service = local_service

        self.pending_incoming_requests_threads = WeakSet()

    def finalize(self, reason=""):
        """See RemoteServiceBase.finalize."""
        super(RemoteServiceServer, self).finalize(reason)

        for thread in self.pending_incoming_requests_threads:
            thread.kill(RPCError(reason), block=False)

        self.pending_incoming_requests_threads.clear()

    def handle(self, socket_):
        self.initialize(socket_, self.remote_address)
        gevent.spawn(self.run)

    def run(self):
        """Start listening for requests, and go on forever.

        Read messages from the socket and issue greenlets to parse
        them, execute methods and send the response to the client.
        This method won't return as long as there's something to read,
        it's therefore advisable to spawn a greenlet to call it.

        """
        while True:
            try:
                data = self._read()
            except IOError:
                break

            if len(data) == 0:
                self.finalize("Connection closed.")
                break

            gevent.spawn(self.process_data, data)

    def process_data(self, data):
        """Handle the message.

        JSON-decode it and forward it to process_incoming_request
        (unconditionally!).

        data (bytes): the message read from the socket.

        """
        # Decode the incoming data.
        try:
            message = json.loads(data, encoding='utf-8')
        except ValueError:
            logger.warning("Cannot parse incoming message, discarding.")
            return

        self.process_incoming_request(message)

    def process_incoming_request(self, request):
        """Handle the request.

        Parse the request, execute the method it asks for, format the
        result and send the response.

        request (dict): the JSON-decoded request.

        """
        # Validate the request.
        if not {"__id", "__method", "__data"}.issubset(request.iterkeys()):
            logger.warning("Request is missing some fields, ingoring.")
            return

        # Determine the ID.
        id_ = request["__id"]

        # Store the request.
        self.pending_incoming_requests_threads.add(gevent.getcurrent())

        # Build the response.
        response = {"__id": id_,
                    "__data": None,
                    "__error": None}

        method_name = request["__method"]

        if not hasattr(self.local_service, method_name):
            response["__error"] = "Method %s doesn't exist." % method_name
        else:
            method = getattr(self.local_service, method_name)

            if not getattr(method, "rpc_callable", False):
                response["__error"] = "Method %s isn't callable." % method_name
            else:
                try:
                    response["__data"] = method(**request["__data"])
                except Exception as error:
                    response["__error"] = "%s: %s\n%s" % \
                        (error.__class__.__name__, error,
                         traceback.format_exc())

        # Encode it.
        try:
            data = json.dumps(response, encoding='utf-8')
        except (TypeError, ValueError):
            logger.warning("JSON encoding failed.")
            return

        # Send it.
        try:
            self._write(data)
        except IOError:
            # Log messages have already been produced.
            return


class RemoteServiceClient(RemoteServiceBase):
    """The client side of a RPC communication.

    Considers all messages coming from the other end as responses for
    RPCs previously sent. Will parse them and forward them to the
    original callers.

    It also offers an interface to issue RPCs, with execute_rpc and
    some syntactic sugar.

    After having created an instance and initialized it with a socket
    the reader loop should be started by calling run.

    """
    def __init__(self, remote_service_coord, auto_retry=None):
        """Create a caller for the service at the given coords.

        remote_service_coord (ServiceCoord): the coordinates (i.e. name
            and shard) of the service to which to send RPC requests.
        auto_retry (float|None): if a number is given then it's the
            interval (in seconds) between attempts to reconnect to the
            remote service in case the connection is lost; if not given
            no automatic reconnection attempts will occur.

        """
        super(RemoteServiceClient, self).__init__(
            get_service_address(remote_service_coord))
        self.remote_service_coord = remote_service_coord

        self.pending_outgoing_requests = dict()
        self.pending_outgoing_requests_results = dict()

        self.auto_retry = auto_retry

    def _repr_remote(self):
        """See RemoteServiceBase._repr_remote."""
        return "%s:%d (%r)" % (self.remote_address +
                               (self.remote_service_coord,))

    def finalize(self, reason=""):
        """See RemoteServiceBase.finalize."""
        super(RemoteServiceClient, self).finalize(reason)

        for result in self.pending_outgoing_requests_results.itervalues():
            result.set_exception(RPCError(reason))

        self.pending_outgoing_requests.clear()
        self.pending_outgoing_requests_results.clear()

    def _connect(self):
        """Establish a connection and initialize that socket.

        """
        try:
            sock = gevent.socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.connect(self.remote_address)
        except socket.error as error:
            logger.debug("Couldn't connect to %s: %s.",
                         self._repr_remote(), error)
        else:
            self.initialize(sock, self.remote_service_coord)

    def _run(self):
        """Maintain the connection up, if required.

        """
        if self.connected:
            self.run()

        if self.auto_retry is not None:
            while True:
                self._connect()
                while not self.connected:
                    gevent.sleep(self.auto_retry)
                    self._connect()
                self.run()

    def connect(self):
        """Connect and start the main loop.

        """
        self._connect()
        self._loop = gevent.spawn(self._run)

    def disconnect(self):
        """See RemoteServiceBase.disconnect."""
        super(RemoteServiceClient, self).disconnect()
        self._loop.kill()

    def run(self):
        """Start listening for responses, and go on forever.

        Read messages from the socket and issue greenlets to parse
        them, determine the request they're for and fill the results.
        This method won't return as long as there's something to read,
        it's therefore advisable to spawn a greenlet to call it.

        """
        while True:
            try:
                data = self._read()
            except IOError:
                break

            if len(data) == 0:
                self.finalize("Connection closed.")
                break

            gevent.spawn(self.process_data, data)

    def process_data(self, data):
        """Handle the message.

        JSON-decode it and forward it to process_incoming_response
        (unconditionally!).

        data (bytes): the message read from the socket.

        """
        # Decode the incoming data.
        try:
            message = json.loads(data, encoding='utf-8')
        except ValueError:
            logger.warning("Cannot parse incoming message, discarding.")
            return

        self.process_incoming_response(message)

    def process_incoming_response(self, response):
        """Handle the response.

        Parse the response, determine the request it's for and its
        associated result and fill it.

        response (dict): the JSON-decoded response.

        """
        # Validate the response.
        if not {"__id", "__data", "__error"}.issubset(response.iterkeys()):
            logger.warning("Response is missing some fields, ingoring.")
            return

        # Determine the ID.
        id_ = response["__id"]

        if id_ not in self.pending_outgoing_requests:
            logger.warning("No pending request with id %s found.", id_)
            return

        request = self.pending_outgoing_requests.pop(id_)
        result = self.pending_outgoing_requests_results.pop(id_)
        error = response["__error"]

        if error is not None:
            err_msg = "%s signaled RPC for method %s was unsuccessful: %s." % (
                self.remote_service_coord, request["__method"], error)
            logger.error(err_msg)
            result.set_exception(RPCError(error))
        else:
            result.set(response["__data"])

    def execute_rpc(self, method, data):
        """Send an RPC request to the remote service.

        method (string): the name of the method to call.
        data (dict): keyword arguments to pass to the methods.

        return (AsyncResult): an object that holds (or will hold) the
            result of the call, either the value or the error that
            prevented successful completion.

        """
        # Determine the ID.
        id_ = uuid.uuid4().hex

        # Build the request.
        request = {"__id": id_,
                   "__method": method,
                   "__data": data}

        result = gevent.event.AsyncResult()

        # Encode it.
        try:
            data = json.dumps(request, encoding='utf-8')
        except (TypeError, ValueError):
            result.set_exception(RPCError("JSON encoding failed."))
            return result

        # Send it.
        try:
            self._write(data)
        except IOError:
            result.set_exception(RPCError("Write failed."))
            return result

        # Store it.
        self.pending_outgoing_requests[id_] = request
        self.pending_outgoing_requests_results[id_] = result

        return result

    def __getattr__(self, method):
        """Syntactic sugar to enable a transparent proxy.

        All unresolved attributes on this object are interpreted as
        methods of the remote service and therefore a wrapper function
        is returned that will call execute_rpc with the proper args.

        As an additional comfort, one can also insert a "callback" and
        (optionally) a "plus" item among the keywords arguments in the
        call to the returned function to be notified when the RPC ends.
        The callback should be a callable able to receive the data and
        (optionally) the plus object as positional args and the error
        as a keyword arg. It will be run in a dedicated greenlet.

        method (string): the name of the accessed method.
        return (function): a proxy to a RPC.

        """
        def run_callback(func, plus, result):
            """Execute the given callback safely.

            Get data and/or error from result and call func passing it
            data, plus (if needed) and error. Catch, log and suppress
            all exceptions.

            func (function): the callback to invoke.
            plus (object): optional additional data.
            result (AsyncResult): the result of a (finished) RPC call.

            """
            data = result.value
            error = None if result.successful() else "%s" % result.exception
            try:
                if plus is None:
                    func(data, error=error)
                else:
                    func(data, plus, error=error)
            except Exception as error:
                logger.error("RPC callback for %s.%s raised exception.",
                             self.remote_service_coord.name, method,
                             exc_info=True)

        def remote_method(**data):
            """Forward arguments to execute_rpc.

            """
            callback = data.pop("callback", None)
            plus = data.pop("plus", None)
            result = self.execute_rpc(method=method, data=data)
            if callback is not None:
                callback = functools.partial(run_callback, callback, plus)
                result.rawlink(functools.partial(gevent.spawn, callback))
            return result

        return remote_method

########NEW FILE########
__FILENAME__ = service
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""This file defines classes to handle asynchronous RPC communication
using gevent and JSON encoding.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import errno
import functools
import logging
import os
import pwd
import signal
import socket
import _socket
import sys
import time

import gevent
import gevent.socket
import gevent.event
from gevent.server import StreamServer
from gevent.backdoor import BackdoorServer

from cms import config, mkdir, ServiceCoord, Address, get_service_address
from cms.log import root_logger, shell_handler, ServiceFilter, \
    CustomFormatter, LogServiceHandler, FileHandler
from cmscommon.datetime import monotonic_time

from .rpc import rpc_method, RemoteServiceServer, RemoteServiceClient


logger = logging.getLogger(__name__)


def repeater(func, period):
    """Repeatedly call the given function.

    Continuosly calls the given function, over and over. For a call to
    be issued the previous one needs to have returned, and at least the
    given number of seconds needs to have passed. Raised exceptions are
    caught, logged and then suppressed.

    func (function): the function to call.
    period (float): the desired interval between successive calls.

    """
    while True:
        call = monotonic_time()

        try:
            func()
        except Exception:
            logger.error("Unexpected error.", exc_info=True)

        gevent.sleep(max(call + period - monotonic_time(), 0))


class Service(object):

    def __init__(self, shard=0):
        signal.signal(signal.SIGINT, lambda unused_x, unused_y: self.exit())

        self.name = self.__class__.__name__
        self.shard = shard
        self._my_coord = ServiceCoord(self.name, self.shard)

        # Dictionaries of (to be) connected RemoteServiceClients.
        self.remote_services = {}

        self.initialize_logging()

        # We setup the listening address for services which want to
        # connect with us.
        try:
            address = get_service_address(self._my_coord)
        except KeyError:
            logger.critical("Couldn't find %r in the configuration.",
                            self._my_coord)
            sys.exit(1)

        self.rpc_server = StreamServer(address, self._connection_handler)
        self.backdoor = None

    def initialize_logging(self):
        """Set up additional logging handlers.

        What we do, in detail, is to add a logger to file (whose
        filename depends on the coords) and a remote logger to a
        LogService. We also attach the service coords to all log
        messages.

        """
        filter_ = ServiceFilter(self.name, self.shard)

        # Update shell handler to attach service coords.
        shell_handler.addFilter(filter_)

        # Determine location of log file, and make directories.
        log_dir = os.path.join(config.log_dir,
                               "%s-%d" % (self.name, self.shard))
        mkdir(config.log_dir)
        mkdir(log_dir)
        log_filename = "%d.log" % int(time.time())

        # Install a file handler.
        file_handler = FileHandler(os.path.join(log_dir, log_filename),
                                   mode='w', encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        file_handler.setFormatter(CustomFormatter(False))
        file_handler.addFilter(filter_)
        root_logger.addHandler(file_handler)

        # Provide a symlink to the latest log file.
        try:
            os.remove(os.path.join(log_dir, "last.log"))
        except OSError:
            pass
        os.symlink(log_filename, os.path.join(log_dir, "last.log"))

        # Setup a remote LogService handler (except when we already are
        # LogService, to avoid circular logging).
        if self.name != "LogService":
            log_service = self.connect_to(ServiceCoord("LogService", 0))
            remote_handler = LogServiceHandler(log_service)
            remote_handler.setLevel(logging.INFO)
            remote_handler.addFilter(filter_)
            root_logger.addHandler(remote_handler)

    def _connection_handler(self, sock, address):
        """Receive and act upon an incoming connection.

        A new RemoteServiceServer is spawned to take care of the new
        connection.

        """
        try:
            ipaddr, port = address
            ipaddr = gevent.socket.gethostbyname(ipaddr)
            address = Address(ipaddr, port)
        except socket.error:
            logger.warning("Unexpected error.", exc_info=True)
            return
        remote_service = RemoteServiceServer(self, address)
        remote_service.handle(sock)

    def connect_to(self, coord, on_connect=None, on_disconnect=None):
        """Return a proxy to a remote service.

        Obtain a communication channel to the remote service at the
        given coord (reusing an existing one, if possible), attach the
        on_connect and on_disconnect handlers and return it.

        coord (ServiceCoord): the coord of the service to connect to.
        on_connect (function): to be called when the service connects.
        on_disconnect (function): to be called when it disconnects.
        return (RemoteServiceClient): a proxy to that service.

        """
        if coord not in self.remote_services:
            service = RemoteServiceClient(coord, auto_retry=0.5)
            service.connect()
            self.remote_services[coord] = service
        else:
            service = self.remote_services[coord]

        if on_connect is not None:
            service.add_on_connect_handler(on_connect)

        return service

    def add_timeout(self, func, plus, seconds, immediately=False):
        """Register a function to be called repeatedly.

        func (function): the function to call.
        plus (object): additional data to pass to the function.
        seconds (float): the minimum interval between successive calls
            (may be larger if a call doesn't return on time).
        immediately (bool): whether to call right off or wait also
            before the first call.

        """
        if plus is None:
            plus = {}
        func = functools.partial(func, **plus)
        if immediately:
            gevent.spawn(repeater, func, seconds)
        else:
            gevent.spawn_later(seconds, repeater, func, seconds)

    def exit(self):
        """Terminate the service at the next step.

        """
        logger.warning("%r received request to shut down.", self._my_coord)
        self.rpc_server.stop()

    def get_backdoor_path(self):
        """Return the path for a UNIX domain socket to use as backdoor.

        """
        return os.path.join(config.run_dir, "%s_%d" % (self.name, self.shard))

    @rpc_method
    def start_backdoor(self, backlog=50):
        """Start a backdoor server on a local UNIX domain socket.

        """
        backdoor_path = self.get_backdoor_path()
        try:
            os.remove(backdoor_path)
        except OSError as error:
            if error.errno != errno.ENOENT:
                raise
        else:
            logger.warning("A backdoor socket has been found and deleted.")
        mkdir(os.path.dirname(backdoor_path))
        backdoor_sock = _socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        backdoor_sock.setblocking(0)
        backdoor_sock.bind(backdoor_path)
        user = pwd.getpwnam("cmsuser")
        # We would like to also set the user to "cmsuser" but only root
        # can do that. Therefore we limit ourselves to the group.
        os.chown(backdoor_path, os.getuid(), user.pw_gid)
        os.chmod(backdoor_path, 0o770)
        backdoor_sock.listen(backlog)
        self.backdoor = BackdoorServer(backdoor_sock, locals={'service': self})
        self.backdoor.start()

    @rpc_method
    def stop_backdoor(self):
        """Stop a backdoor server started by start_backdoor.

        """
        if self.backdoor is not None:
            self.backdoor.stop()
        backdoor_path = self.get_backdoor_path()
        try:
            os.remove(backdoor_path)
        except OSError as error:
            if error.errno != errno.ENOENT:
                raise

    def run(self):
        """Starts the main loop of the service.

        return (bool): True if successful.

        """
        try:
            self.rpc_server.start()

        # This must come before socket.error, because socket.gaierror
        # extends socket.error
        except socket.gaierror:
            logger.critical("Service %s could not listen on "
                            "specified address, because it cannot "
                            "be resolved.", self.name)
            return False

        except socket.error as error:
            if error.errno == errno.EADDRINUSE:
                logger.critical("Listening port %s for service %s is "
                                "already in use, quitting.",
                                self.rpc_server.address.port, self.name)
                return False
            elif error.errno == errno.EADDRNOTAVAIL:
                logger.critical("Service %s could not listen on "
                                "specified address, because it is not "
                                "available.", self.name)
                return False
            else:
                raise

        if config.backdoor:
            self.start_backdoor()

        logger.info("%s %d up and running!", *self._my_coord)

        # This call will block until self.rpc_server.stop() is called.
        self.rpc_server.serve_forever()

        logger.info("%s %d is shutting down", *self._my_coord)

        if config.backdoor:
            self.stop_backdoor()

        self._disconnect_all()
        return True

    def _disconnect_all(self):
        """Disconnect all remote services.

        """
        for service in self.remote_services.itervalues():
            if service.connected:
                service.disconnect()

    @rpc_method
    def echo(self, string):
        """Simple RPC method.

        string (string): the string to be echoed.
        return (string): string, again.

        """
        return string

    @rpc_method
    def quit(self, reason=""):
        """Shut down the service

        reason (string): why, oh why, you want me down?

        """
        logger.info("Trying to exit as asked by another service (%s).", reason)
        self.exit()

########NEW FILE########
__FILENAME__ = web_rpc
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import json
import logging

from werkzeug.wrappers import Request, Response
from werkzeug.routing import Map, Rule
from werkzeug.exceptions import HTTPException, BadRequest, NotFound, \
    NotAcceptable, UnsupportedMediaType, ServiceUnavailable
from werkzeug.wsgi import responder

from cms import ServiceCoord


logger = logging.getLogger(__name__)


class RPCMiddleware(object):
    """An HTTP interface to the internal RPC communications.

    This WSGI application provides a synchronous and unfiltered access,
    over an HTTP transport, to all remote services and all their RPC
    methods. Each of them can be called by making a POST request to the
    URL "/<service>/<shard>/<method>" where "<service>" is the name of
    the remote service (i.e. the name of the class), "<shard>" is the
    shard of the instance and "<method>" is the name of the method.

    POST has been used because it's neither a safe nor an idempotent
    method (see HTTP spec.) and is therefore less restricted in what
    clients expect it to do than a GET.

    Arguments for the RPC should be given as JSON-encoded object in the
    request body (should always be present, even if empty).

    A standard error code will be returned for all client-to-WSGI-app
    errors (mostly communication errors: the client didn't declare it
    produces and consumes JSON or the JSON was invalid). A 404 will be
    returned if the requested remote service isn't found in the list of
    remote services that the service we're proxying for knows about. A
    503 status code means that, at the moment, there's no connection to
    the remote service.

    As soon as the RPC has been sent, the HTTP request is considered
    successful (i.e. status code 200). The response body will contain
    a JSON object with two fields: data and error (possibly null). The
    first contains the JSON-encoded result of the RPC, the second a
    string describing the error that occured (if any).

    """
    def __init__(self, service):
        """Create an HTTP-to-RPC proxy for the given service.

        service (Service): the service this application is running for.
            Will usually be the AdminWebServer.

        """
        self._service = service
        self._url_map = Map([Rule("/<service>/<int:shard>/<method>",
                                  methods=["POST"], endpoint="rpc")],
                            encoding_errors="strict")

    def __call__(self, environ, start_response):
        """Execute this instance as a WSGI application.

        See the PEP for the meaning of parameters. The separation of
        __call__ and wsgi_app eases the insertion of middlewares.

        """
        return self.wsgi_app(environ, start_response)

    @responder
    def wsgi_app(self, environ, start_response):
        """Execute this instance as a WSGI application.

        See the PEP for the meaning of parameters. The separation of
        __call__ and wsgi_app eases the insertion of middlewares.

        """
        urls = self._url_map.bind_to_environ(environ)
        try:
            endpoint, args = urls.match()
        except HTTPException as exc:
            return exc

        assert endpoint == "rpc"

        request = Request(environ)
        request.encoding_errors = "strict"

        response = Response()

        remote_service = ServiceCoord(args['service'], args['shard'])

        if remote_service not in self._service.remote_services:
            return NotFound()

        # TODO Check content_encoding and content_md5.

        if request.mimetype != "application/json":
            return UnsupportedMediaType()

        if request.accept_mimetypes.quality("application/json") <= 0:
            return NotAcceptable()

        try:
            data = json.load(request.stream, encoding='utf-8')
        except ValueError:
            return BadRequest()

        if not self._service.remote_services[remote_service].connected:
            return ServiceUnavailable()

        result = self._service.remote_services[remote_service].execute_rpc(
            args['method'], data)

        # XXX We could set a timeout on the .wait().
        result.wait()

        response.status_code = 200
        response.mimetype = "application/json"
        response.data = json.dumps({
            "data": result.value,
            "error": None if result.successful() else "%s" % result.exception})

        return response

########NEW FILE########
__FILENAME__ = web_service
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging

import tornado.web
import tornado.escape
import tornado.wsgi

from gevent.pywsgi import WSGIServer

from werkzeug.wsgi import DispatcherMiddleware
from werkzeug.contrib.fixers import ProxyFix

from .service import Service
from .web_rpc import RPCMiddleware


logger = logging.getLogger(__name__)


class WebService(Service):
    """RPC service with Web server capabilities.

    """
    def __init__(self, listen_port, handlers, parameters, shard=0,
                 listen_address=""):
        super(WebService, self).__init__(shard)

        self.wsgi_app = tornado.wsgi.WSGIApplication(handlers, **parameters)
        self.wsgi_app.service = self

        if parameters.get('rpc_enabled', False):
            self.wsgi_app = DispatcherMiddleware(
                self.wsgi_app, {"/rpc": RPCMiddleware(self)})

        # If is_proxy_used is set to True we'll use the content of the
        # X-Forwarded-For HTTP header (if provided) to determine the
        # client IP address, ignoring the one the request came from.
        # This allows to use the IP lock behind a proxy. Activate it
        # only if all requests come from a trusted source (if clients
        # were allowed to directlty communicate with the server they
        # could fake their IP and compromise the security of IP lock).
        if parameters.get('is_proxy_used', False):
            self.wsgi_app = ProxyFix(self.wsgi_app)

        self.web_server = WSGIServer((listen_address, listen_port),
                                     self.wsgi_app)

    def run(self):
        """Start the WebService.

        Both the WSGI server and the RPC server are started.

        """
        self.web_server.start()
        Service.run(self)
        self.web_server.stop()

########NEW FILE########
__FILENAME__ = log
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

# Some code snippets have been taken and readapted from the logging
# package of Python 2.7. For such pieces this copyright applies:
#
# Copyright 2001-2013 by Vinay Sajip. All Rights Reserved.
#
# Permission to use, copy, modify, and distribute this software and its
# documentation for any purpose and without fee is hereby granted,
# provided that the above copyright notice appear in all copies and that
# both that copyright notice and this permission notice appear in
# supporting documentation, and that the name of Vinay Sajip
# not be used in advertising or publicity pertaining to distribution
# of the software without specific, written prior permission.
# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING
# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL
# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR
# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER
# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT
# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
#
# You can find the original files at:
# http://hg.python.org/cpython/file/69ee9b554eca/Lib/logging/__init__.py
# http://hg.python.org/cpython/file/69ee9b554eca/Lib/logging/handlers.py

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import curses
import logging
import sys

import gevent.coros


class StreamHandler(logging.StreamHandler):
    """Subclass to make gevent-aware.

    Use a gevent lock instead of a threading one to block only the
    current greenlet.

    """
    def createLock(self):
        """Set self.lock to a new gevent RLock.

        """
        self.lock = gevent.coros.RLock()


class FileHandler(logging.FileHandler):
    """Subclass to make gevent-aware.

    Use a gevent lock instead of a threading one to block only the
    current greenlet.

    """
    def createLock(self):
        """Set self.lock to a new gevent RLock.

        """
        self.lock = gevent.coros.RLock()


class LogServiceHandler(logging.Handler):
    """Send log messages to a remote centralized LogService.

    Support log collection by sending messages over the RPC system to
    the LogService. We try to send LogRecords as accurately as possible
    to rebuild them on the remote end and re-inject them in the logging
    system. Unfortunately, since they'll have to be JSON-encoded when
    sent on the wire, some tweaks have to be made. In particular, we
    need to somehow "encode" objects that cannot be converted to JSON.
    These are the exception info (if any) and, possibly, the args.

    For exceptions, we simply format them locally and store the text in
    exc_text, dropping the exc_info. That same field is also used, by
    convention, by formatters to store a cache of the exception and
    will therefore picked up seamlessly.

    For args, we just format them into msg to produce the message. We
    then store the message as msg and drop args.

    """
    def __init__(self, log_service):
        """Initialize the handler.

        Establish a connection to the given LogService.

        log_service (RemoteService): a handle for a remote LogService.

        """
        logging.Handler.__init__(self)
        self._log_service = log_service

    def createLock(self):
        """Set self.lock to a new gevent RLock.

        """
        self.lock = gevent.coros.RLock()

    # Taken from CPython, combining emit and makePickle, and adapted to
    # not pickle the dictionary and use its items as keyword parameters
    # for LogService.Log.
    def emit(self, record):
        try:
            ei = record.exc_info
            if ei:
                # just to get traceback text into record.exc_text ...
                self.format(record)
                record.exc_info = None  # to avoid Unpickleable error
            # See issue #14436: If msg or args are objects, they may not be
            # available on the receiving end. So we convert the msg % args
            # to a string, save it as msg and zap the args.
            d = dict(record.__dict__)
            d['msg'] = record.getMessage()
            d['args'] = None
            if ei:
                record.exc_info = ei  # for next handler
            self._log_service.Log(**d)
        except (KeyboardInterrupt, SystemExit):
            raise
        except:
            self.handleError(record)


def has_color_support(stream):
    """Try to determine if the given stream supports colored output.

    Return True only if the stream declares to be a TTY, if it has a
    file descriptor on which ncurses can initialize a terminal and if
    that terminal's entry in terminfo declares support for colors.

    stream (fileobj): a file-like object (that adheres to the API
        declared in the `io' package).

    return (bool): True if we're sure that colors are supported, False
        if they aren't or if we can't tell.

    """
    if stream.isatty():
        try:
            curses.setupterm(fd=stream.fileno())
            # See `man terminfo` for capabilities' names and meanings.
            if curses.tigetnum("colors") > 0:
                return True
        # fileno() can raise IOError or OSError (since Python 3.3).
        except Exception:
            pass
    return False


def get_color_hash(string):
    """Deterministically return a color based on the string's content.

    Determine one of curses.COLOR_* using only the data of the given
    string. The only condition is for the operation to give the same
    result when repeated.

    string (string): the string.

    return (int): a color, as a curses.COLOR_* constant..

    """
    # We get the default hash of the string and use it to pick a color.
    return [curses.COLOR_BLACK,
            curses.COLOR_RED,
            curses.COLOR_GREEN,
            curses.COLOR_YELLOW,
            curses.COLOR_BLUE,
            curses.COLOR_MAGENTA,
            curses.COLOR_CYAN,
            curses.COLOR_WHITE][hash(string) % 8]


def add_color_to_string(string, color):
    """Format the string to be printed with the given color.

    Insert formatting characters that, when printed on a terminal, will
    make the given string appear with the given foreground color.

    string (string): the string to color.
    color (int): the color as a curses constant, like
        curses.COLOR_BLACK.

    return (string): the formatted string.

    """
    # See `man terminfo` for capabilities' names and meanings.
    return "%s%s%s%s" % (curses.tparm(curses.tigetstr("setaf"), color),
                         curses.tparm(curses.tigetstr("bold")), string,
                         curses.tparm(curses.tigetstr("sgr0")))


class CustomFormatter(logging.Formatter):
    """Format log messages as we want them.

    A custom logging formatter to present the information that we want
    to see and show them in the way we want them to appear.
    The first such part only consists in adding the service coords and
    the operation to the displayed log message. This could almost be
    achieved by just using the standard formatter with a custom format
    string, but that wouldn't allow us to show the slash before the
    operation only if we actually have the operation data.
    The second part comprises selecting the fields we want to show,
    determining their order and their separators (all this can be done
    with the standard formatter) and coloring them if we're asked to
    (this cannot be done with the standard formatter!).

    """
    SEVERITY_COLORS = {logging.CRITICAL: curses.COLOR_RED,
                       logging.ERROR: curses.COLOR_RED,
                       logging.WARNING: curses.COLOR_YELLOW,
                       logging.INFO: curses.COLOR_GREEN,
                       logging.DEBUG: curses.COLOR_CYAN}

    def __init__(self, colors=False):
        """Initialize a formatter.

        colors (bool): whether to use colors in formatted output or
            not.

        """
        logging.Formatter.__init__(self, "", "%Y/%m/%d %H:%M:%S")
        self.colors = colors

    # Taken from CPython and adapted to remove assumptions that there
    # was a constant format string stored in _fmt. This meant removing
    # the call to usesTime() and substituing the _fmt % record.__dict__
    # expression with the more powerful do_format(record).
    def format(self, record):
        record.message = record.getMessage()
        record.asctime = self.formatTime(record, self.datefmt)
        s = self.do_format(record)
        if record.exc_info:
            # Cache the traceback text to avoid converting it multiple times
            # (it's constant anyway)
            if not record.exc_text:
                record.exc_text = self.formatException(record.exc_info)
        if record.exc_text:
            if s[-1:] != "\n":
                s = s + "\n"
            try:
                s = s + record.exc_text
            except UnicodeError:
                # Sometimes filenames have non-ASCII chars, which can lead
                # to errors when s is Unicode and record.exc_text is str
                # See issue 8924.
                # We also use replace for when there are multiple
                # encodings, e.g. UTF-8 for the filesystem and latin-1
                # for a script. See issue 13232.
                s = s + record.exc_text.decode(sys.getfilesystemencoding(),
                                               'replace')
        return s

    def do_format(self, record):
        """Produce a human-readable message from the given record.

        This is the "core" of the format method, but it has been split
        out to allow the code to focus only on formatting, rather than
        bookkeeping (putting args into their placeholders in msg and
        formatting time and exception).

        record (LogRecord): the data for the log message.

        return (string): the formatted log message.

        """
        # Determine the first part (time and severity) and its color.
        severity_str = record.asctime + " - " + record.levelname
        severity_col = self.SEVERITY_COLORS[record.levelno]

        # Determine the second part (service coords) and its color.
        if hasattr(record, "service_name") and \
                hasattr(record, "service_shard"):
            coord_str = record.service_name + "," + str(record.service_shard)
        else:
            coord_str = "None"
        coord_col = get_color_hash(coord_str)

        # Determine the third part (operation) and its color.
        if hasattr(record, "operation"):
            operation_str = record.operation
        else:
            operation_str = ""
        operation_col = get_color_hash(operation_str)

        # Colorize the strings.
        if self.colors:
            severity_str = add_color_to_string(severity_str, severity_col)
            coord_str = add_color_to_string(coord_str, coord_col)
            operation_str = add_color_to_string(operation_str, operation_col)

        # Put them all together.
        fmt = severity_str
        fmt += " [" + coord_str
        if hasattr(record, "operation"):
            fmt += "/" + operation_str
        fmt += "] " + record.message

        return fmt


class ServiceFilter(logging.Filter):
    """Add service coords to filtered log messages.

    The name is misleading: this class isn't there to filter messages
    (none of them will be dropped) but to add contextual data to them.
    We add the "service_name" and "service_shard" fields (if they are
    not already set) with the values given to the constructor.

    """
    def __init__(self, name, shard):
        """Initialize a filter for the given coords.

        name (string): the service name (its class).
        shard (int): its shard (the index of its entry in the config).

        """
        logging.Filter.__init__(self, "")
        self.name = name
        self.shard = shard

    def filter(self, record):
        """Add data to the given record.

        record (LogRecord): data for a log message, to analyze and (if
            needed) tamper with.

        return (bool): whether to keep the record or not (will always
            be True).

        """
        if not hasattr(record, "service_name") or \
                not hasattr(record, "service_shard"):
            record.service_name = self.name
            record.service_shard = self.shard
        return True


class OperationAdapter(logging.LoggerAdapter):
    """Helper to attach operation to messages.

    Wraps a logger and adds the operation given to the constructor to
    the "operation" field of the "extra" argument of all messages
    logged with this adapter. If "extra" doesn't exists it is created.
    If "operation" is already set it isn't altered.

    """
    def __init__(self, logger, operation):
        """Initialize an adapter to set the given operation.

        operation (string): a human-readable description of what the
            code will be performing while it's logging messages to
            this object instead of to the wrapped logger.

        """
        logging.LoggerAdapter.__init__(self, logger, {"operation": operation})
        self.operation = operation

    def process(self, msg, kwargs):
        """Inject the data in the log message.

        msg (string): the message given to one of debug(), info(),
            warning(), etc. methods.
        kwargs (dict): the keyword arguments given to such method (not
            the positional ones!).

        """
        kwargs.setdefault("extra", {}).setdefault("operation", self.operation)
        return msg, kwargs


# Get the root logger.
root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)

# Install a shell handler.
shell_handler = StreamHandler(sys.stdout)
shell_handler.setLevel(logging.INFO)
shell_handler.setFormatter(CustomFormatter(has_color_support(sys.stdout)))
root_logger.addHandler(shell_handler)

########NEW FILE########
__FILENAME__ = plugin
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import imp
import logging
import os.path
import pkgutil

from .conf import config


logger = logging.getLogger(__name__)


def _try_import(plugin_name, dir_name):
    """Try to import a module called plugin_name from a directory
    called dir_name.

    plugin_name (string): name of the module (without extensions).
    dir_name (string): name of the directory where to look.

    return (module): the module if found, None if not found.

    """
    try:
        file_, file_name, description = imp.find_module(plugin_name,
                                                        [dir_name])
    except ImportError:
        return None

    try:
        module = imp.load_module(plugin_name,
                                 file_, file_name, description)
    except ImportError as error:
        logger.warning("Unable to use task type %s from plugin in "
                       "directory %s.\n%r", plugin_name, dir_name, error)
        return None
    else:
        return module
    finally:
        file_.close()


def plugin_lookup(plugin_name, plugin_dir, plugin_family):
    """Try to lookup a plugin in the standard positions.

    plugin_name (string): the name of the plugin: it is both the name
                          of the module and of a class inside that
                          module.
    plugin_dir (string): the place inside cms hierarchy where
                         plugin_name is usually found (e.g.:
                         cms.grading.tasktypes).
    plugin_family (string): the name of the plugin type, as used in
                            <system_plugins_directory>/<plugin_family>.

    return (type): the correct plugin class.

    raise (KeyError): if either the module or the class is not found.

    """
    module = None

    # Try first if the plugin is provided by CMS by default.
    try:
        module = __import__("%s.%s" % (plugin_dir, plugin_name),
                            fromlist=plugin_name)
    except ImportError:
        pass

    # If not found, try in all possible plugin directories.
    if module is None:
        module = _try_import(plugin_name,
                             os.path.join(config.data_dir,
                                          "plugins", plugin_family))

    if module is None:
        raise KeyError("Module %s not found." % plugin_name)

    if plugin_name not in module.__dict__:
        logger.warning("Unable to find class %s in the plugin.", plugin_name)
        raise KeyError("Class %s not found." % plugin_name)

    return module.__dict__[plugin_name]


def plugin_list(plugin_dir, plugin_family):
    """Return the list of plugins classes of the given family.

    plugin_dir (string): the place inside cms hierarchy where
                         plugin_name is usually found (e.g.:
                         cms.grading.tasktypes).
    plugin_family (string): the name of the plugin type, as used in
                            <system_plugins_directory>/<plugin_family>.

    return ([type]): the correct plugin class.

    raise (KeyError): if either the module or the class is not found.

    """
    cms_root_path = os.path.dirname(os.path.dirname(__file__))
    rets = pkgutil.iter_modules([
        os.path.join(cms_root_path, plugin_dir.replace(".", "/")),
        os.path.join(config.data_dir, "plugins", plugin_family),
    ])
    modules = [ret[0].find_module(ret[1]).load_module(ret[1]) for ret in rets]
    return [module.__dict__[module.__name__]
            for module in modules if module.__name__ in module.__dict__]

########NEW FILE########
__FILENAME__ = AdminWebServer
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2014 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012-2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2014 Artem Iglikov <artem.iglikov@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Web server for administration of contests.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import base64
import json
import logging
import os
import pkg_resources
import re
import traceback
from datetime import datetime, timedelta
from StringIO import StringIO
import zipfile

from sqlalchemy.orm import joinedload
from sqlalchemy.exc import IntegrityError

import tornado.web
import tornado.locale

from cms import config, ServiceCoord, get_service_shards, get_service_address
from cms.io import WebService
from cms.db import Session, Contest, User, Announcement, Question, Message, \
    Submission, SubmissionResult, File, Task, Dataset, Attachment, Manager, \
    Testcase, SubmissionFormatElement, Statement
from cms.db.filecacher import FileCacher
from cms.grading import compute_changes_for_dataset
from cms.grading.tasktypes import get_task_type_class
from cms.grading.scoretypes import get_score_type_class
from cms.server import file_handler_gen, get_url_root, \
    CommonRequestHandler
from cmscommon.datetime import make_datetime, make_timestamp


logger = logging.getLogger(__name__)


def try_commit(session, handler):
    """Try to commit the session, if not successful display a warning
    in the webpage.

    session (Session): the session to commit.
    handler (BaseHandler): just to extract the information about AWS.

    return (bool): True if commit was successful, False otherwise.

    """
    try:
        session.commit()
    except IntegrityError as error:
        handler.application.service.add_notification(
            make_datetime(),
            "Operation failed.", str(error))
        return False
    else:
        handler.application.service.add_notification(
            make_datetime(),
            "Operation successful.", "")
        return True


def argument_reader(func, empty=None):
    """Return an helper method for reading and parsing form values.

    func (function): the parser and validator for the value.
    empty (object): the value to store if an empty string is retrieved.

    return (function): a function to be used as a method of a
        RequestHandler.

    """
    def helper(self, dest, name, empty=empty):
        """Read the argument called "name" and save it in "dest".

        self (RequestHandler): a thing with a get_argument method.
        dest (dict): a place to store the obtained value.
        name (string): the name of the argument and of the item.
        empty (object): overrides the default empty value.

        """
        value = self.get_argument(name, None)
        if value is None:
            return
        if value == "":
            dest[name] = empty
        else:
            dest[name] = func(value)
    return helper


def parse_int(value):
    """Parse and validate an integer."""
    try:
        return int(value)
    except:
        raise ValueError("Can't cast %s to int." % value)


def parse_timedelta_sec(value):
    """Parse and validate a timedelta (as number of seconds)."""
    try:
        return timedelta(seconds=float(value))
    except:
        raise ValueError("Can't cast %s to timedelta." % value)


def parse_timedelta_min(value):
    """Parse and validate a timedelta (as number of minutes)."""
    try:
        return timedelta(minutes=float(value))
    except:
        raise ValueError("Can't cast %s to timedelta." % value)


def parse_datetime(value):
    """Parse and validate a datetime (in pseudo-ISO8601)."""
    if '.' not in value:
        value += ".0"
    try:
        return datetime.strptime(value, "%Y-%m-%d %H:%M:%S.%f")
    except:
        raise ValueError("Can't cast %s to datetime." % value)


def parse_ip_address_or_subnet(value):
    """Validate an IP address or subnet."""
    address, sep, subnet = value.partition("/")
    if sep != "":
        subnet = int(subnet)
        assert 0 <= subnet < 32
    fields = address.split(".")
    assert len(fields) == 4
    for field in fields:
        num = int(field)
        assert 0 <= num < 256
    return value


class BaseHandler(CommonRequestHandler):
    """Base RequestHandler for this application.

    All the RequestHandler classes in this application should be a
    child of this class.

    """

    def safe_get_item(self, cls, ident, session=None):
        """Get item from database of class cls and id ident, using
        session if given, or self.sql_session if not given. If id is
        not found, raise a 404.

        cls (type): class of object to retrieve.
        ident (string): id of object.
        session (Session|None): session to use.

        return (object): the object with the given id.

        raise (HTTPError): 404 if not found.

        """
        if session is None:
            session = self.sql_session
        entity = cls.get_from_id(ident, session)
        if entity is None:
            raise tornado.web.HTTPError(404)
        return entity

    def prepare(self):
        """This method is executed at the beginning of each request.

        """
        # Attempt to update the contest and all its references
        # If this fails, the request terminates.
        self.set_header("Cache-Control", "no-cache, must-revalidate")

        self.sql_session = Session()
        self.sql_session.expire_all()
        self.contest = None

        if config.installed:
            localization_dir = os.path.join("/", "usr", "local", "share",
                                            "locale")
        else:
            localization_dir = os.path.join(os.path.dirname(__file__), "mo")
        if os.path.exists(localization_dir):
            tornado.locale.load_gettext_translations(localization_dir, "cms")

    def render_params(self):
        """Return the default render params used by almost all handlers.

        return (dict): default render params

        """
        params = {}
        params["timestamp"] = make_datetime()
        params["contest"] = self.contest
        params["url_root"] = get_url_root(self.request.path)
        if self.contest is not None:
            params["phase"] = self.contest.phase(params["timestamp"])
            # Keep "== None" in filter arguments. SQLAlchemy does not
            # understand "is None".
            params["unanswered"] = self.sql_session.query(Question)\
                .join(User)\
                .filter(User.contest_id == self.contest.id)\
                .filter(Question.reply_timestamp == None)\
                .filter(Question.ignored == False)\
                .count()  # noqa
        params["contest_list"] = self.sql_session.query(Contest).all()
        params["cookie"] = str(self.cookies)
        return params

    def finish(self, *args, **kwds):
        """Finish this response, ending the HTTP request.

        We override this method in order to properly close the database.

        TODO - Now that we have greenlet support, this method could be
        refactored in terms of context manager or something like
        that. So far I'm leaving it to minimize changes.

        """
        self.sql_session.close()
        try:
            tornado.web.RequestHandler.finish(self, *args, **kwds)
        except IOError:
            # When the client closes the connection before we reply,
            # Tornado raises an IOError exception, that would pollute
            # our log with unnecessarily critical messages
            logger.debug("Connection closed before our reply.")

    def write_error(self, status_code, **kwargs):
        if "exc_info" in kwargs and \
                kwargs["exc_info"][0] != tornado.web.HTTPError:
            exc_info = kwargs["exc_info"]
            logger.error(
                "Uncaught exception (%r) while processing a request: %s" %
                (exc_info[1], ''.join(traceback.format_exception(*exc_info))))

        # Most of the handlers raise a 404 HTTP error before r_params
        # is defined. If r_params is not defined we try to define it
        # here, and if it fails we simply return a basic textual error notice.
        if hasattr(self, 'r_params'):
            self.render("error.html", status_code=status_code, **self.r_params)
        else:
            try:
                self.r_params = self.render_params()
                self.render("error.html", status_code=status_code,
                            **self.r_params)
            except:
                self.write("A critical error has occurred :-(")
                self.finish()

    get_string = argument_reader(lambda a: a, empty="")

    # When a checkbox isn't active it's not sent at all, making it
    # impossible to distinguish between missing and False.
    def get_bool(self, dest, name):
        """Parse a boolean.

        dest (dict): a place to store the result.
        name (string): the name of the argument and of the item.

        """
        value = self.get_argument(name, False)
        try:
            dest[name] = bool(value)
        except:
            raise ValueError("Can't cast %s to bool." % value)

    get_int = argument_reader(parse_int)

    get_timedelta_sec = argument_reader(parse_timedelta_sec)

    get_timedelta_min = argument_reader(parse_timedelta_min)

    get_datetime = argument_reader(parse_datetime)

    get_ip_address_or_subnet = argument_reader(parse_ip_address_or_subnet)

    def get_submission_format(self, dest):
        """Parse the submission format.

        Using the two arguments "submission_format_choice" and
        "submission_format" set the "submission_format" item of the
        given dictionary.

        dest (dict): a place to store the result.

        """
        choice = self.get_argument("submission_format_choice", "other")
        if choice == "simple":
            filename = "%s.%%l" % dest["name"]
            format_ = [SubmissionFormatElement(filename)]
        elif choice == "other":
            value = self.get_argument("submission_format", "[]")
            if value == "":
                value = "[]"
            format_ = []
            try:
                for filename in json.loads(value):
                    format_ += [SubmissionFormatElement(filename)]
            except ValueError:
                raise ValueError("Submission format not recognized.")
        else:
            raise ValueError("Submission format not recognized.")
        dest["submission_format"] = format_

    def get_time_limit(self, dest, field):
        """Parse the time limit.

        Read the argument with the given name and use its value to set
        the "time_limit" item of the given dictionary.

        dest (dict): a place to store the result.
        field (string): the name of the argument to use.

        """
        value = self.get_argument(field, None)
        if value is None:
            return
        if value == "":
            dest["time_limit"] = None
        else:
            try:
                value = float(value)
            except:
                raise ValueError("Can't cast %s to float." % value)
            if not 0 <= value < float("+inf"):
                raise ValueError("Time limit out of range.")
            dest["time_limit"] = value

    def get_memory_limit(self, dest, field):
        """Parse the memory limit.

        Read the argument with the given name and use its value to set
        the "memory_limit" item of the given dictionary.

        dest (dict): a place to store the result.
        field (string): the name of the argument to use.

        """
        value = self.get_argument(field, None)
        if value is None:
            return
        if value == "":
            dest["memory_limit"] = None
        else:
            try:
                value = int(value)
            except:
                raise ValueError("Can't cast %s to float." % value)
            if not 0 < value:
                raise ValueError("Invalid memory limit.")
            dest["memory_limit"] = value

    def get_task_type(self, dest, name, params):
        """Parse the task type.

        Parse the arguments to get the task type and its parameters,
        and fill them in the "task_type" and "task_type_parameters"
        items of the given dictionary.

        dest (dict): a place to store the result.
        name (string): the name of the argument that holds the task
            type name.
        params (string): the prefix of the names of the arguments that
            hold the parameters.

        """
        name = self.get_argument(name, None)
        if name is None:
            raise ValueError("Task type not found.")
        try:
            class_ = get_task_type_class(name)
        except KeyError:
            raise ValueError("Task type not recognized: %s." % name)
        params = json.dumps(class_.parse_handler(self, params + name + "_"))
        dest["task_type"] = name
        dest["task_type_parameters"] = params

    def get_score_type(self, dest, name, params):
        """Parse the score type.

        Parse the arguments to get the score type and its parameters,
        and fill them in the "score_type" and "score_type_parameters"
        items of the given dictionary.

        dest (dict): a place to store the result.
        name (string): the name of the argument that holds the score
            type name.
        params (string): the name of the argument that hold the
            parameters.

        """
        name = self.get_argument(name, None)
        if name is None:
            raise ValueError("Score type not found.")
        try:
            get_score_type_class(name)
        except KeyError:
            raise ValueError("Score type not recognized: %s." % name)
        params = self.get_argument(params, None)
        if params is None:
            raise ValueError("Score type parameters not found.")
        dest["score_type"] = name
        dest["score_type_parameters"] = params


FileHandler = file_handler_gen(BaseHandler)


class AdminWebServer(WebService):
    """Service that runs the web server serving the managers.

    """

    QUICK_ANSWERS = {
        "yes": "Yes",
        "no": "No",
        "answered": "Answered in task description",
        "invalid": "Invalid question",
        "nocomment": "No comment",
    }

    def __init__(self, shard):
        parameters = {
            "login_url": "/",
            "template_path": pkg_resources.resource_filename(
                "cms.server", "templates/admin"),
            "static_path": pkg_resources.resource_filename(
                "cms.server", "static"),
            "cookie_secret": base64.b64encode(config.secret_key),
            "debug": config.tornado_debug,
            "rpc_enabled": True,
        }
        super(AdminWebServer, self).__init__(
            config.admin_listen_port,
            _aws_handlers,
            parameters,
            shard=shard,
            listen_address=config.admin_listen_address)

        # A list of pending notifications.
        self.notifications = []

        self.file_cacher = FileCacher(self)
        self.evaluation_service = self.connect_to(
            ServiceCoord("EvaluationService", 0))
        self.scoring_service = self.connect_to(
            ServiceCoord("ScoringService", 0))
        self.proxy_service = self.connect_to(
            ServiceCoord("ProxyService", 0))
        self.resource_services = []
        for i in xrange(get_service_shards("ResourceService")):
            self.resource_services.append(self.connect_to(
                ServiceCoord("ResourceService", i)))
        self.logservice = self.connect_to(ServiceCoord("LogService", 0))

    def add_notification(self, timestamp, subject, text):
        """Store a new notification to send at the first
        opportunity (i.e., at the first request for db notifications).

        timestamp (datetime): the time of the notification.
        subject (string): subject of the notification.
        text (string): body of the notification.

        """
        self.notifications.append((timestamp, subject, text))


class MainHandler(BaseHandler):
    """Home page handler, with queue and workers statuses.

    """

    def get(self, contest_id=None):
        if contest_id is not None:
            self.contest = self.safe_get_item(Contest, contest_id)

        self.r_params = self.render_params()
        self.render("welcome.html", **self.r_params)


def SimpleContestHandler(page):
    class Cls(BaseHandler):
        def get(self, contest_id):
            self.contest = self.safe_get_item(Contest, contest_id)

            self.r_params = self.render_params()
            self.render(page, **self.r_params)
    return Cls


class ResourcesListHandler(BaseHandler):
    def get(self, contest_id=None):
        if contest_id is not None:
            self.contest = self.safe_get_item(Contest, contest_id)

        self.r_params = self.render_params()
        self.r_params["resource_addresses"] = {}
        services = get_service_shards("ResourceService")
        for i in xrange(services):
            self.r_params["resource_addresses"][i] = get_service_address(
                ServiceCoord("ResourceService", i)).ip
        self.render("resourceslist.html", **self.r_params)


class ResourcesHandler(BaseHandler):
    def get(self, shard=None, contest_id=None):
        if contest_id is not None:
            self.contest = self.safe_get_item(Contest, contest_id)
            contest_address = "/%s" % contest_id
        else:
            contest_address = ""

        if shard is None:
            shard = "all"

        self.r_params = self.render_params()
        self.r_params["resource_shards"] = \
            get_service_shards("ResourceService")
        self.r_params["resource_addresses"] = {}
        if shard == "all":
            for i in xrange(self.r_params["resource_shards"]):
                self.r_params["resource_addresses"][i] = get_service_address(
                    ServiceCoord("ResourceService", i)).ip
        else:
            shard = int(shard)
            try:
                address = get_service_address(
                    ServiceCoord("ResourceService", shard))
            except KeyError:
                self.redirect("/resourceslist%s" % contest_address)
                return
            self.r_params["resource_addresses"][shard] = address.ip

        self.render("resources.html", **self.r_params)


class AddContestHandler(BaseHandler):
    """Adds a new contest.

    """
    def get(self):
        self.r_params = self.render_params()
        self.render("add_contest.html", **self.r_params)

    def post(self):
        try:
            attrs = dict()

            self.get_string(attrs, "name", empty=None)
            self.get_string(attrs, "description")

            assert attrs.get("name") is not None, "No contest name specified."

            allowed_localizations = \
                self.get_argument("allowed_localizations", "")
            if allowed_localizations:
                attrs["allowed_localizations"] = \
                    [x.strip() for x in allowed_localizations.split(",")
                     if len(x) > 0 and not x.isspace()]
            else:
                attrs["allowed_localizations"] = []

            attrs["languages"] = self.get_arguments("languages", [])

            self.get_string(attrs, "token_mode")
            self.get_int(attrs, "token_max_number")
            self.get_timedelta_sec(attrs, "token_min_interval")
            self.get_int(attrs, "token_gen_initial")
            self.get_int(attrs, "token_gen_number")
            self.get_timedelta_min(attrs, "token_gen_interval")
            self.get_int(attrs, "token_gen_max")

            self.get_int(attrs, "max_submission_number")
            self.get_int(attrs, "max_user_test_number")
            self.get_timedelta_sec(attrs, "min_submission_interval")
            self.get_timedelta_sec(attrs, "min_user_test_interval")

            self.get_datetime(attrs, "start")
            self.get_datetime(attrs, "stop")

            self.get_string(attrs, "timezone", empty=None)
            self.get_timedelta_sec(attrs, "per_user_time")
            self.get_int(attrs, "score_precision")

            # Create the contest.
            contest = Contest(**attrs)
            self.sql_session.add(contest)

        except Exception as error:
            self.application.service.add_notification(
                make_datetime(), "Invalid field(s)", repr(error))
            self.redirect("/contest/add")
            return

        if try_commit(self.sql_session, self):
            # Create the contest on RWS.
            self.application.service.proxy_service.reinitialize()
            self.redirect("/contest/%s" % contest.id)
        else:
            self.redirect("/contest/add")


class ContestHandler(BaseHandler):
    def get(self, contest_id):
        self.contest = self.safe_get_item(Contest, contest_id)

        self.r_params = self.render_params()
        self.render("contest.html", **self.r_params)

    def post(self, contest_id):
        contest = self.safe_get_item(Contest, contest_id)

        try:
            attrs = contest.get_attrs()

            self.get_string(attrs, "name", empty=None)
            self.get_string(attrs, "description")

            assert attrs.get("name") is not None, "No contest name specified."

            allowed_localizations = \
                self.get_argument("allowed_localizations", "")
            if allowed_localizations:
                attrs["allowed_localizations"] = \
                    [x.strip() for x in allowed_localizations.split(",")
                     if len(x) > 0 and not x.isspace()]
            else:
                attrs["allowed_localizations"] = []

            attrs["languages"] = self.get_arguments("languages", [])

            self.get_string(attrs, "token_mode")
            self.get_int(attrs, "token_max_number")
            self.get_timedelta_sec(attrs, "token_min_interval")
            self.get_int(attrs, "token_gen_initial")
            self.get_int(attrs, "token_gen_number")
            self.get_timedelta_min(attrs, "token_gen_interval")
            self.get_int(attrs, "token_gen_max")

            self.get_int(attrs, "max_submission_number")
            self.get_int(attrs, "max_user_test_number")
            self.get_timedelta_sec(attrs, "min_submission_interval")
            self.get_timedelta_sec(attrs, "min_user_test_interval")

            self.get_datetime(attrs, "start")
            self.get_datetime(attrs, "stop")

            self.get_string(attrs, "timezone", empty=None)
            self.get_timedelta_sec(attrs, "per_user_time")
            self.get_int(attrs, "score_precision")

            # Update the contest.
            contest.set_attrs(attrs)

        except Exception as error:
            self.application.service.add_notification(
                make_datetime(), "Invalid field(s).", repr(error))
            self.redirect("/contest/%s" % contest_id)
            return

        if try_commit(self.sql_session, self):
            # Update the contest on RWS.
            self.application.service.proxy_service.reinitialize()
        self.redirect("/contest/%s" % contest_id)


class AddStatementHandler(BaseHandler):
    """Add a statement to a task.

    """
    def get(self, task_id):
        task = self.safe_get_item(Task, task_id)
        self.contest = task.contest

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.render("add_statement.html", **self.r_params)

    def post(self, task_id):
        task = self.safe_get_item(Task, task_id)
        self.contest = task.contest

        language = self.get_argument("language", None)
        if language is None:
            self.application.service.add_notification(
                make_datetime(),
                "No language code specified",
                "The language code can be any string.")
            self.redirect("/add_statement/%s" % task_id)
            return
        statement = self.request.files["statement"][0]
        if not statement["filename"].endswith(".pdf"):
            self.application.service.add_notification(
                make_datetime(),
                "Invalid task statement",
                "The task statement must be a .pdf file.")
            self.redirect("/add_statement/%s" % task_id)
            return
        task_name = task.name
        self.sql_session.close()

        try:
            digest = self.application.service.file_cacher.put_file_content(
                statement["body"],
                "Statement for task %s (lang: %s)" % (task_name, language))
        except Exception as error:
            self.application.service.add_notification(
                make_datetime(),
                "Task statement storage failed",
                repr(error))
            self.redirect("/add_statement/%s" % task_id)
            return

        # TODO verify that there's no other Statement with that language
        # otherwise we'd trigger an IntegrityError for constraint violation

        self.sql_session = Session()
        task = self.safe_get_item(Task, task_id)
        self.contest = task.contest

        statement = Statement(language, digest, task=task)
        self.sql_session.add(statement)

        if try_commit(self.sql_session, self):
            self.redirect("/task/%s" % task_id)
        else:
            self.redirect("/add_statement/%s" % task_id)


class DeleteStatementHandler(BaseHandler):
    """Delete a statement.

    """
    def get(self, statement_id):
        statement = self.safe_get_item(Statement, statement_id)
        task = statement.task
        self.contest = task.contest

        self.sql_session.delete(statement)

        try_commit(self.sql_session, self)
        self.redirect("/task/%s" % task.id)


class AddAttachmentHandler(BaseHandler):
    """Add an attachment to a task.

    """
    def get(self, task_id):
        task = self.safe_get_item(Task, task_id)
        self.contest = task.contest

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.render("add_attachment.html", **self.r_params)

    def post(self, task_id):
        task = self.safe_get_item(Task, task_id)
        self.contest = task.contest

        attachment = self.request.files["attachment"][0]
        task_name = task.name
        self.sql_session.close()

        try:
            digest = self.application.service.file_cacher.put_file_content(
                attachment["body"],
                "Task attachment for %s" % task_name)
        except Exception as error:
            self.application.service.add_notification(
                make_datetime(),
                "Attachment storage failed",
                repr(error))
            self.redirect("/add_attachment/%s" % task_id)
            return

        # TODO verify that there's no other Attachment with that filename
        # otherwise we'd trigger an IntegrityError for constraint violation

        self.sql_session = Session()
        task = self.safe_get_item(Task, task_id)
        self.contest = task.contest

        attachment = Attachment(attachment["filename"], digest, task=task)
        self.sql_session.add(attachment)

        if try_commit(self.sql_session, self):
            self.redirect("/task/%s" % task_id)
        else:
            self.redirect("/add_attachment/%s" % task_id)


class DeleteAttachmentHandler(BaseHandler):
    """Delete an attachment.

    """
    def get(self, attachment_id):
        attachment = self.safe_get_item(Attachment, attachment_id)
        task = attachment.task
        self.contest = task.contest

        self.sql_session.delete(attachment)

        try_commit(self.sql_session, self)
        self.redirect("/task/%s" % task.id)


class AddManagerHandler(BaseHandler):
    """Add a manager to a dataset.

    """
    def get(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.r_params["dataset"] = dataset
        self.render("add_manager.html", **self.r_params)

    def post(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        manager = self.request.files["manager"][0]
        task_name = task.name
        self.sql_session.close()

        try:
            digest = self.application.service.file_cacher.put_file_content(
                manager["body"],
                "Task manager for %s" % task_name)
        except Exception as error:
            self.application.service.add_notification(
                make_datetime(),
                "Manager storage failed",
                repr(error))
            self.redirect("/add_manager/%s" % dataset_id)
            return

        self.sql_session = Session()
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        manager = Manager(manager["filename"], digest, dataset=dataset)
        self.sql_session.add(manager)

        if try_commit(self.sql_session, self):
            self.redirect("/task/%s" % task.id)
        else:
            self.redirect("/add_manager/%s" % dataset_id)


class DeleteManagerHandler(BaseHandler):
    """Delete a manager.

    """
    def get(self, manager_id):
        manager = self.safe_get_item(Manager, manager_id)
        task = manager.dataset.task
        self.contest = task.contest

        self.sql_session.delete(manager)

        try_commit(self.sql_session, self)
        self.redirect("/task/%s" % task.id)


# TODO: Move this somewhere more appropriate?
def copy_dataset(
        new_dataset, old_dataset, clone_results, clone_managers, sql_session):
    """Copy an existing dataset's test cases, and optionally
    submission results and managers.

    new_dataset (Dataset): target dataset to copy into.
    old_dataset (Dataset): original dataset to copy from.
    clone_results (bool): copy submission results.
    clone_managers (bool): copy dataset managers.
    sql_session (Session): the session to commit.

    """
    new_testcases = dict()
    for old_t in old_dataset.testcases.itervalues():
        new_t = old_t.clone()
        new_t.dataset = new_dataset
        new_testcases[new_t.codename] = new_t

    if clone_managers:
        for old_m in old_dataset.managers.itervalues():
            new_m = old_m.clone()
            new_m.dataset = new_dataset

    sql_session.flush()

    new_results = list()
    if clone_results:
        # We issue this query manually to optimize it: we load all
        # executables and evaluations at once instead of having SA
        # lazy-load them when we access them for each SubmissionResult,
        # one at a time. We need them because we want to copy them too,
        # recursively.
        old_results = \
            sql_session.query(SubmissionResult)\
                       .filter(SubmissionResult.dataset == old_dataset)\
                       .options(joinedload(SubmissionResult.submission))\
                       .options(joinedload(SubmissionResult.executables))\
                       .options(joinedload(SubmissionResult.evaluations)).all()

        for old_sr in old_results:
            # Create the submission result.
            new_sr = old_sr.clone()
            new_sr.submission = old_sr.submission
            new_sr.dataset = new_dataset

            # Create executables.
            for old_e in old_sr.executables.itervalues():
                new_e = old_e.clone()
                new_e.submission_result = new_sr

            # Create evaluations.
            for old_e in old_sr.evaluations:
                new_e = old_e.clone()
                new_e.submission_result = new_sr
                new_e.testcase = new_testcases[old_e.codename]

            # We need to keep a reference to the object to prevent it
            # from being deleted (as SQLAlchemy's Session holds just a
            # weak reference...).
            new_results += [new_sr]

    sql_session.flush()

    return new_results


class AddDatasetHandler(BaseHandler):
    """Add a dataset to a task.

    """
    def get(self, task_id, dataset_id_to_copy):
        task = self.safe_get_item(Task, task_id)
        self.contest = task.contest

        # We can either clone an existing dataset, or '-' for a new one.
        if dataset_id_to_copy == '-':
            original_dataset = None
            description = "Default"
        else:
            try:
                original_dataset = \
                    self.safe_get_item(Dataset, dataset_id_to_copy)
                description = "Copy of %s" % original_dataset.description
            except ValueError:
                raise tornado.web.HTTPError(404)

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.r_params["clone_id"] = dataset_id_to_copy
        self.r_params["original_dataset"] = original_dataset
        self.r_params["original_dataset_task_type_parameters"] = \
            json.loads(original_dataset.task_type_parameters) \
            if original_dataset is not None else None
        self.r_params["default_description"] = description
        self.render("add_dataset.html", **self.r_params)

    def post(self, task_id, dataset_id_to_copy):
        task = self.safe_get_item(Task, task_id)
        self.contest = task.contest

        # We can either clone an existing dataset, or '-' for a new one.
        if dataset_id_to_copy == '-':
            original_dataset = None
        else:
            try:
                original_dataset = \
                    self.safe_get_item(Dataset, dataset_id_to_copy)
            except ValueError:
                raise tornado.web.HTTPError(404)

        try:
            attrs = dict()

            self.get_string(attrs, "description")

            # Ensure description is unique.
            if any(attrs["description"] == d.description
                   for d in task.datasets):
                self.application.service.add_notification(
                    make_datetime(),
                    "Dataset name %r is already taken." % attrs["description"],
                    "Please choose a unique name for this dataset.")
                self.redirect("/add_dataset/%s/%s" % (task_id,
                                                      dataset_id_to_copy))
                return

            self.get_time_limit(attrs, "time_limit")
            self.get_memory_limit(attrs, "memory_limit")
            self.get_task_type(attrs, "task_type", "TaskTypeOptions_")
            self.get_score_type(attrs, "score_type", "score_type_parameters")

            # Create the dataset.
            attrs["autojudge"] = False
            attrs["task"] = task
            dataset = Dataset(**attrs)
            self.sql_session.add(dataset)

        except Exception as error:
            logger.warning("Invalid field: %s" % (traceback.format_exc()))
            self.application.service.add_notification(
                make_datetime(), "Invalid field(s)", repr(error))
            self.redirect("/add_dataset/%s/%s" % (task_id, dataset_id_to_copy))
            return

        if original_dataset is not None:
            # If we were cloning the dataset, copy all testcases across
            # too. If the user insists, clone all evaluation
            # information too.
            clone_results = bool(self.get_argument("clone_results", False))
            clone_managers = bool(self.get_argument("clone_managers", False))
            copy_dataset(dataset, original_dataset, clone_results,
                         clone_managers, self.sql_session)

        # If the task does not yet have an active dataset, make this
        # one active.
        if task.active_dataset is None:
            task.active_dataset = dataset

        if try_commit(self.sql_session, self):
            # self.application.service.scoring_service.reinitialize()
            self.redirect("/task/%s" % task_id)
        else:
            self.redirect("/add_dataset/%s/%s" % (task_id,
                                                  dataset_id_to_copy))


class RenameDatasetHandler(BaseHandler):
    """Rename the descripton of a dataset.

    """
    def get(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.r_params["dataset"] = dataset
        self.render("rename_dataset.html", **self.r_params)

    def post(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        description = self.get_argument("description", "")

        # Ensure description is unique.
        if any(description == d.description
               for d in task.datasets if d is not dataset):
            self.application.service.add_notification(
                make_datetime(),
                "Dataset name \"%s\" is already taken." % description,
                "Please choose a unique name for this dataset.")
            self.redirect("/rename_dataset/%s" % dataset_id)
            return

        dataset.description = description

        if try_commit(self.sql_session, self):
            self.redirect("/task/%s" % task.id)
        else:
            self.redirect("/rename_dataset/%s" % dataset_id)


class DeleteDatasetHandler(BaseHandler):
    """Delete a dataset from a task.

    """
    def get(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.r_params["dataset"] = dataset
        self.render("delete_dataset.html", **self.r_params)

    def post(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        self.sql_session.delete(dataset)

        if try_commit(self.sql_session, self):
            # self.application.service.scoring_service.reinitialize()
            pass
        self.redirect("/task/%s" % task.id)


class ActivateDatasetHandler(BaseHandler):
    """Set a given dataset to be the active one for a task.

    """
    def get(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        changes = compute_changes_for_dataset(task.active_dataset, dataset)
        notify_users = set()

        # By default, we will notify users who's public scores have changed, or
        # their non-public scores have changed but they have used a token.
        for c in changes:
            score_changed = c.old_score is not None or c.new_score is not None
            public_score_changed = c.old_public_score is not None or \
                c.new_public_score is not None
            if public_score_changed or \
                    (c.submission.tokened() and score_changed):
                notify_users.add(c.submission.user.id)

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.r_params["dataset"] = dataset
        self.r_params["changes"] = changes
        self.r_params["default_notify_users"] = notify_users
        self.render("activate_dataset.html", **self.r_params)

    def post(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        task.active_dataset = dataset

        if try_commit(self.sql_session, self):
            # self.application.service.scoring_service.reinitialize()
            self.application.service.proxy_service.dataset_updated(
                task_id=task.id)

            # This kicks off judging of any submissions which were previously
            # unloved, but are now part of an autojudged taskset.
            self.application.service.evaluation_service.search_jobs_not_done()
            self.application.service.scoring_service.search_jobs_not_done()

        # Now send notifications to contestants.
        datetime = make_datetime()

        r = re.compile('notify_([0-9]+)$')
        count = 0
        for k, v in self.request.arguments.iteritems():
            m = r.match(k)
            if not m:
                continue
            user = self.safe_get_item(User, m.group(1))
            message = Message(datetime,
                              self.get_argument("message_subject", ""),
                              self.get_argument("message_text", ""),
                              user=user)
            self.sql_session.add(message)
            count += 1

        if try_commit(self.sql_session, self):
            self.application.service.add_notification(
                make_datetime(),
                "Messages sent to %d users." % count, "")

        self.redirect("/task/%s" % task.id)


class ToggleAutojudgeDatasetHandler(BaseHandler):
    """Toggle whether a given dataset is judged automatically or not.

    """
    def get(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        dataset.autojudge = not dataset.autojudge

        if try_commit(self.sql_session, self):
            # self.application.service.scoring_service.reinitialize()

            # This kicks off judging of any submissions which were previously
            # unloved, but are now part of an autojudged taskset.
            self.application.service.evaluation_service.search_jobs_not_done()
            self.application.service.scoring_service.search_jobs_not_done()

        self.redirect("/task/%s" % dataset.task_id)


class AddTestcaseHandler(BaseHandler):
    """Add a testcase to a dataset.

    """
    def get(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.r_params["dataset"] = dataset
        self.render("add_testcase.html", **self.r_params)

    def post(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        codename = self.get_argument("codename")

        try:
            input_ = self.request.files["input"][0]
            output = self.request.files["output"][0]
        except KeyError:
            self.application.service.add_notification(
                make_datetime(),
                "Invalid data",
                "Please fill both input and output.")
            self.redirect("/add_testcase/%s" % dataset_id)
            return

        public = self.get_argument("public", None) is not None
        task_name = task.name
        self.sql_session.close()

        try:
            input_digest = \
                self.application.service.file_cacher.put_file_content(
                    input_["body"],
                    "Testcase input for task %s" % task_name)
            output_digest = \
                self.application.service.file_cacher.put_file_content(
                    output["body"],
                    "Testcase output for task %s" % task_name)
        except Exception as error:
            self.application.service.add_notification(
                make_datetime(),
                "Testcase storage failed",
                repr(error))
            self.redirect("/add_testcase/%s" % dataset_id)
            return

        self.sql_session = Session()
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        testcase = Testcase(
            codename, public, input_digest, output_digest, dataset=dataset)
        self.sql_session.add(testcase)

        if try_commit(self.sql_session, self):
            # max_score and/or extra_headers might have changed.
            self.application.service.proxy_service.reinitialize()
            self.redirect("/task/%s" % task.id)
        else:
            self.redirect("/add_testcase/%s" % dataset_id)


class AddTestcasesHandler(BaseHandler):
    """Add several testcases to a dataset.

    """
    def get(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.r_params["dataset"] = dataset
        self.render("add_testcases.html", **self.r_params)

    def post(self, dataset_id):
        # TODO: this method is quite long, some splitting is needed.
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        try:
            archive = self.request.files["archive"][0]
        except KeyError:
            self.application.service.add_notification(
                make_datetime(),
                "Invalid data",
                "Please choose tests archive.")
            self.redirect("/add_testcases/%s" % dataset_id)
            return

        public = self.get_argument("public", None) is not None
        overwrite = self.get_argument("overwrite", None) is not None

        # Get input/output file names templates, or use default ones.
        input_template = self.get_argument("input_template", None)
        if not input_template:
            input_template = "input.*"
        output_template = self.get_argument("output_template", None)
        if not output_template:
            output_template = "output.*"
        input_re = re.compile(re.escape(input_template).replace("\\*",
                              "(.*)") + "$")
        output_re = re.compile(re.escape(output_template).replace("\\*",
                               "(.*)") + "$")

        task_name = task.name
        self.sql_session.close()

        fp = StringIO(archive["body"])
        try:
            with zipfile.ZipFile(fp, "r") as archive_zfp:
                tests = dict()
                # Match input/output file names to testcases' codenames.
                for filename in archive_zfp.namelist():
                    match = input_re.match(filename)
                    if match:
                        codename = match.group(1)
                        if not codename in tests:
                            tests[codename] = [None, None]
                        tests[codename][0] = filename
                    else:
                        match = output_re.match(filename)
                        if match:
                            codename = match.group(1)
                            if not codename in tests:
                                tests[codename] = [None, None]
                            tests[codename][1] = filename

                skipped_tc = []
                overwritten_tc = []
                added_tc = []
                for codename, testdata in tests.iteritems():
                    # If input or output file isn't found, skip it.
                    if not testdata[0] or not testdata[1]:
                        continue
                    self.sql_session = Session()

                    # Check, whether current testcase already exists.
                    dataset = self.safe_get_item(Dataset, dataset_id)
                    task = dataset.task
                    self.contest = task.contest
                    if codename in dataset.testcases:
                        # If we are allowed, remove existing testcase.
                        # If not - skip this testcase.
                        if overwrite:
                            testcase = dataset.testcases[codename]
                            self.sql_session.delete(testcase)

                            if not try_commit(self.sql_session, self):
                                skipped_tc.append(codename)
                                continue
                            overwritten_tc.append(codename)
                        else:
                            skipped_tc.append(codename)
                            continue

                    # Add current testcase.
                    try:
                        input_ = archive_zfp.read(testdata[0])
                        output = archive_zfp.read(testdata[1])
                    except Exception as error:
                        self.application.service.add_notification(
                            make_datetime(),
                            "Reading testcase %s failed" % codename,
                            repr(error))
                        self.redirect("/add_testcases/%s" % dataset_id)
                        return
                    try:
                        input_digest = self.application.service\
                            .file_cacher.put_file_content(
                                input_,
                                "Testcase input for task %s" % task_name)
                        output_digest = self.application.service\
                            .file_cacher.put_file_content(
                                output,
                                "Testcase output for task %s" % task_name)
                    except Exception as error:
                        self.application.service.add_notification(
                            make_datetime(),
                            "Testcase storage failed",
                            repr(error))
                        self.redirect("/add_testcases/%s" % dataset_id)
                        return
                    testcase = Testcase(codename, public, input_digest,
                                        output_digest, dataset=dataset)
                    self.sql_session.add(testcase)

                    if not try_commit(self.sql_session, self):
                        self.application.service.add_notification(
                            make_datetime(),
                            "Couldn't add test %s" % codename,
                            "")
                        self.redirect("/add_testcases/%s" % dataset_id)
                        return
                    if not codename in overwritten_tc:
                        added_tc.append(codename)
        except zipfile.BadZipfile:
            self.application.service.add_notification(
                make_datetime(),
                "The selected file is not a zip file.",
                "Please select a valid zip file.")
            self.redirect("/add_testcases/%s" % dataset_id)
            return

        self.application.service.add_notification(
            make_datetime(),
            "Successfully added %d and overwritten %d testcase(s)" %
            (len(added_tc), len(overwritten_tc)),
            "Added: %s; overwritten: %s; skipped: %s" %
            (", ".join(added_tc) if added_tc else "none",
             ", ".join(overwritten_tc) if overwritten_tc else "none",
             ", ".join(skipped_tc) if skipped_tc else "none"))
        self.application.service.proxy_service.reinitialize()
        self.redirect("/task/%s" % task.id)


class DeleteTestcaseHandler(BaseHandler):
    """Delete a testcase.

    """
    def get(self, testcase_id):
        testcase = self.safe_get_item(Testcase, testcase_id)
        task = testcase.dataset.task
        self.contest = task.contest

        self.sql_session.delete(testcase)

        if try_commit(self.sql_session, self):
            # max_score and/or extra_headers might have changed.
            self.application.service.proxy_service.reinitialize()
        self.redirect("/task/%s" % task.id)


class AddTaskHandler(BaseHandler):
    def get(self, contest_id):
        self.contest = self.safe_get_item(Contest, contest_id)

        self.r_params = self.render_params()
        self.render("add_task.html", **self.r_params)

    def post(self, contest_id):
        self.contest = self.safe_get_item(Contest, contest_id)

        try:
            attrs = dict()

            self.get_string(attrs, "name", empty=None)
            self.get_string(attrs, "title")

            assert attrs.get("name") is not None, "No task name specified."

            self.get_string(attrs, "primary_statements")

            self.get_submission_format(attrs)

            self.get_string(attrs, "token_mode")
            self.get_int(attrs, "token_max_number")
            self.get_timedelta_sec(attrs, "token_min_interval")
            self.get_int(attrs, "token_gen_initial")
            self.get_int(attrs, "token_gen_number")
            self.get_timedelta_min(attrs, "token_gen_interval")
            self.get_int(attrs, "token_gen_max")

            self.get_int(attrs, "max_submission_number")
            self.get_int(attrs, "max_user_test_number")
            self.get_timedelta_sec(attrs, "min_submission_interval")
            self.get_timedelta_sec(attrs, "min_user_test_interval")

            self.get_int(attrs, "score_precision")

            # Create the task.
            attrs["num"] = len(self.contest.tasks)
            attrs["contest"] = self.contest
            task = Task(**attrs)
            self.sql_session.add(task)

        except Exception as error:
            self.application.service.add_notification(
                make_datetime(), "Invalid field(s)", repr(error))
            self.redirect("/add_task/%s" % contest_id)
            return

        try:
            attrs = dict()

            self.get_time_limit(attrs, "time_limit")
            self.get_memory_limit(attrs, "memory_limit")
            self.get_task_type(attrs, "task_type", "TaskTypeOptions_")
            self.get_score_type(attrs, "score_type", "score_type_parameters")

            # Create its first dataset.
            attrs["description"] = "Default"
            attrs["autojudge"] = True
            attrs["task"] = task
            dataset = Dataset(**attrs)
            self.sql_session.add(dataset)

            # Make the dataset active. Life works better that way.
            task.active_dataset = dataset

        except Exception as error:
            self.application.service.add_notification(
                make_datetime(), "Invalid field(s)", repr(error))
            self.redirect("/add_task/%s" % contest_id)
            return

        if try_commit(self.sql_session, self):
            # Create the task on RWS.
            self.application.service.proxy_service.reinitialize()
            self.redirect("/task/%s" % task.id)
        else:
            self.redirect("/add_task/%s" % contest_id)


class TaskHandler(BaseHandler):
    """Task handler, with a POST method to edit the task.

    """
    def get(self, task_id):
        task = self.safe_get_item(Task, task_id)
        self.contest = task.contest

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.r_params["submissions"] = \
            self.sql_session.query(Submission)\
                .join(Task).filter(Task.id == task_id)\
                .order_by(Submission.timestamp.desc()).all()
        self.render("task.html", **self.r_params)

    def post(self, task_id):
        task = self.safe_get_item(Task, task_id)
        self.contest = task.contest

        try:
            attrs = task.get_attrs()

            self.get_string(attrs, "name", empty=None)
            self.get_string(attrs, "title")

            assert attrs.get("name") is not None, "No task name specified."

            self.get_string(attrs, "primary_statements")

            self.get_submission_format(attrs)

            self.get_string(attrs, "token_mode")
            self.get_int(attrs, "token_max_number")
            self.get_timedelta_sec(attrs, "token_min_interval")
            self.get_int(attrs, "token_gen_initial")
            self.get_int(attrs, "token_gen_number")
            self.get_timedelta_min(attrs, "token_gen_interval")
            self.get_int(attrs, "token_gen_max")

            self.get_int(attrs, "max_submission_number")
            self.get_int(attrs, "max_user_test_number")
            self.get_timedelta_sec(attrs, "min_submission_interval")
            self.get_timedelta_sec(attrs, "min_user_test_interval")

            self.get_int(attrs, "score_precision")

            # Update the task.
            task.set_attrs(attrs)

        except Exception as error:
            self.application.service.add_notification(
                make_datetime(), "Invalid field(s)", repr(error))
            self.redirect("/task/%s" % task_id)
            return

        for dataset in task.datasets:
            try:
                attrs = dataset.get_attrs()

                self.get_time_limit(attrs, "time_limit_%d" % dataset.id)
                self.get_memory_limit(attrs, "memory_limit_%d" % dataset.id)
                self.get_task_type(attrs, "task_type_%d" % dataset.id,
                                   "TaskTypeOptions_%d_" % dataset.id)
                self.get_score_type(attrs, "score_type_%d" % dataset.id,
                                    "score_type_parameters_%d" % dataset.id)

                # Update the dataset.
                dataset.set_attrs(attrs)

            except Exception as error:
                self.application.service.add_notification(
                    make_datetime(), "Invalid field(s)", repr(error))
                self.redirect("/task/%s" % task_id)
                return

            for testcase in dataset.testcases.itervalues():
                testcase.public = bool(self.get_argument(
                    "testcase_%s_public" % testcase.id, False))

        if try_commit(self.sql_session, self):
            # Update the task on RWS.
            self.application.service.proxy_service.reinitialize()
        self.redirect("/task/%s" % task_id)


class DatasetSubmissionsHandler(BaseHandler):
    """Shows all submissions for this dataset, allowing the admin to
    view the results under different datasets.

    """
    def get(self, dataset_id):
        dataset = self.safe_get_item(Dataset, dataset_id)
        task = dataset.task
        self.contest = task.contest

        self.r_params = self.render_params()
        self.r_params["task"] = task
        self.r_params["active_dataset"] = task.active_dataset
        self.r_params["shown_dataset"] = dataset
        self.r_params["datasets"] = \
            self.sql_session.query(Dataset)\
                            .filter(Dataset.task == task)\
                            .order_by(Dataset.description).all()
        self.r_params["submissions"] = \
            self.sql_session.query(Submission)\
                            .filter(Submission.task == task)\
                            .options(joinedload(Submission.task))\
                            .options(joinedload(Submission.user))\
                            .options(joinedload(Submission.files))\
                            .options(joinedload(Submission.token))\
                            .options(joinedload(Submission.results))\
                            .order_by(Submission.timestamp.desc()).all()
        self.render("submissionlist.html", **self.r_params)


class RankingHandler(BaseHandler):
    """Shows the ranking for a contest.

    """
    def get(self, contest_id, format="online"):
        # This validates the contest id.
        self.safe_get_item(Contest, contest_id)

        # This massive joined load gets all the information which we will need
        # to generating the rankings.
        self.contest = self.sql_session.query(Contest)\
            .filter(Contest.id == contest_id)\
            .options(joinedload('users'))\
            .options(joinedload('users.submissions'))\
            .options(joinedload('users.submissions.token'))\
            .options(joinedload('users.submissions.results'))\
            .first()

        self.r_params = self.render_params()
        if format == "txt":
            self.set_header("Content-Type", "text/plain")
            self.set_header("Content-Disposition",
                            "attachment; filename=\"ranking.txt\"")
            self.render("ranking.txt", **self.r_params)
        elif format == "csv":
            self.set_header("Content-Type", "text/csv")
            self.set_header("Content-Disposition",
                            "attachment; filename=\"ranking.csv\"")
            self.render("ranking.csv", **self.r_params)
        else:
            self.render("ranking.html", **self.r_params)


class AddAnnouncementHandler(BaseHandler):
    """Called to actually add an announcement

    """
    def post(self, contest_id):
        self.contest = self.safe_get_item(Contest, contest_id)

        subject = self.get_argument("subject", "")
        text = self.get_argument("text", "")
        if subject != "":
            ann = Announcement(make_datetime(), subject, text,
                               contest=self.contest)
            self.sql_session.add(ann)
            try_commit(self.sql_session, self)
        self.redirect("/announcements/%s" % contest_id)


class RemoveAnnouncementHandler(BaseHandler):
    """Called to remove an announcement.

    """
    def get(self, ann_id):
        ann = self.safe_get_item(Announcement, ann_id)
        contest_id = ann.contest.id

        self.sql_session.delete(ann)

        try_commit(self.sql_session, self)
        self.redirect("/announcements/%s" % contest_id)


class UserViewHandler(BaseHandler):
    """Shows the details of a single user (submissions, questions,
    messages), and allows to send the latters.

    """
    def get(self, user_id):
        user = self.safe_get_item(User, user_id)
        self.contest = user.contest

        self.r_params = self.render_params()
        self.r_params["selected_user"] = user
        self.r_params["submissions"] = user.submissions
        self.render("user.html", **self.r_params)

    def post(self, user_id):
        user = self.safe_get_item(User, user_id)
        self.contest = user.contest

        try:
            attrs = user.get_attrs()

            self.get_string(attrs, "first_name")
            self.get_string(attrs, "last_name")
            self.get_string(attrs, "username", empty=None)
            self.get_string(attrs, "password")
            self.get_string(attrs, "email")

            assert attrs.get("username") is not None, \
                "No username specified."

            self.get_ip_address_or_subnet(attrs, "ip")

            self.get_string(attrs, "timezone", empty=None)
            self.get_datetime(attrs, "starting_time")
            self.get_timedelta_sec(attrs, "extra_time")

            self.get_bool(attrs, "hidden")
            self.get_string(attrs, "primary_statements")

            # Update the user.
            user.set_attrs(attrs)

        except Exception as error:
            self.application.service.add_notification(
                make_datetime(), "Invalid field(s)", repr(error))
            self.redirect("/user/%s" % user_id)
            return

        if try_commit(self.sql_session, self):
            # Update the user on RWS.
            self.application.service.proxy_service.reinitialize()
        self.redirect("/user/%s" % user_id)


class AddUserHandler(SimpleContestHandler("add_user.html")):
    def post(self, contest_id):
        self.contest = self.safe_get_item(Contest, contest_id)

        try:
            attrs = dict()

            self.get_string(attrs, "first_name")
            self.get_string(attrs, "last_name")
            self.get_string(attrs, "username", empty=None)
            self.get_string(attrs, "password")
            self.get_string(attrs, "email")

            assert attrs.get("username") is not None, \
                "No username specified."

            self.get_ip_address_or_subnet(attrs, "ip")

            self.get_string(attrs, "timezone", empty=None)
            self.get_datetime(attrs, "starting_time")
            self.get_timedelta_sec(attrs, "extra_time")

            self.get_bool(attrs, "hidden")
            self.get_string(attrs, "primary_statements")

            # Create the user.
            attrs["contest"] = self.contest
            user = User(**attrs)
            self.sql_session.add(user)

        except Exception as error:
            self.application.service.add_notification(
                make_datetime(), "Invalid field(s)", repr(error))
            self.redirect("/add_user/%s" % contest_id)
            return

        if try_commit(self.sql_session, self):
            # Create the user on RWS.
            self.application.service.proxy_service.reinitialize()
            self.redirect("/user/%s" % user.id)
        else:
            self.redirect("/add_user/%s" % contest_id)


class SubmissionViewHandler(BaseHandler):
    """Shows the details of a submission. All data is already present
    in the list of the submissions of the task or of the user, but we
    need a place where to link messages like 'Submission 42 failed to
    compile please check'.

    """
    def get(self, submission_id, dataset_id=None):
        submission = self.safe_get_item(Submission, submission_id)
        task = submission.task
        self.contest = task.contest

        if dataset_id is not None:
            dataset = self.safe_get_item(Dataset, dataset_id)
        else:
            dataset = task.active_dataset
        assert dataset.task is task

        self.r_params = self.render_params()
        self.r_params["s"] = submission
        self.r_params["active_dataset"] = task.active_dataset
        self.r_params["shown_dataset"] = dataset
        self.r_params["datasets"] = \
            self.sql_session.query(Dataset)\
                            .filter(Dataset.task == task)\
                            .order_by(Dataset.description).all()
        self.render("submission.html", **self.r_params)


class SubmissionFileHandler(FileHandler):
    """Shows a submission file.

    """
    # FIXME: Replace with FileFromDigestHandler?
    def get(self, file_id):
        sub_file = self.safe_get_item(File, file_id)
        submission = sub_file.submission
        self.contest = submission.task.contest

        real_filename = sub_file.filename
        if submission.language is not None:
            real_filename = real_filename.replace("%l", submission.language)
        digest = sub_file.digest

        self.sql_session.close()
        self.fetch(digest, "text/plain", real_filename)


class QuestionsHandler(BaseHandler):
    """Page to see and send messages to all the contestants.

    """
    def get(self, contest_id):
        self.contest = self.safe_get_item(Contest, contest_id)

        self.r_params = self.render_params()
        self.r_params["questions"] = self.sql_session.query(Question)\
            .join(User).filter(User.contest_id == contest_id)\
            .order_by(Question.question_timestamp.desc())\
            .order_by(Question.id).all()
        self.render("questions.html", **self.r_params)


class QuestionReplyHandler(BaseHandler):
    """Called when the manager replies to a question made by a user.

    """
    def post(self, question_id):
        ref = self.get_argument("ref", "/")
        question = self.safe_get_item(Question, question_id)
        self.contest = question.user.contest

        reply_subject_code = self.get_argument("reply_question_quick_answer",
                                               "")
        question.reply_text = self.get_argument("reply_question_text", "")

        # Ignore invalid answers
        if reply_subject_code not in AdminWebServer.QUICK_ANSWERS:
            question.reply_subject = ""
        else:
            # Quick answer given, ignore long answer.
            question.reply_subject = \
                AdminWebServer.QUICK_ANSWERS[reply_subject_code]
            question.reply_text = ""

        question.reply_timestamp = make_datetime()

        if try_commit(self.sql_session, self):
            logger.info("Reply sent to user %s for question with id %s." %
                        (question.user.username, question_id))

        self.redirect(ref)


class QuestionIgnoreHandler(BaseHandler):
    """Called when the manager chooses to ignore or stop ignoring a
    question.

    """
    def post(self, question_id):
        ref = self.get_argument("ref", "/")

        # Fetch form data.
        question = self.safe_get_item(Question, question_id)
        self.contest = question.user.contest

        should_ignore = self.get_argument("ignore", "no") == "yes"

        # Commit the change.
        question.ignored = should_ignore
        if try_commit(self.sql_session, self):
            logger.info("Question '%s' by user %s %s" %
                        (question.subject, question.user.username,
                         ["unignored", "ignored"][should_ignore]))

        self.redirect(ref)


class MessageHandler(BaseHandler):
    """Called when a message is sent to a specific user.

    """

    def post(self, user_id):
        user = self.safe_get_item(User, user_id)
        self.contest = user.contest

        message = Message(make_datetime(),
                          self.get_argument("message_subject", ""),
                          self.get_argument("message_text", ""),
                          user=user)
        self.sql_session.add(message)
        if try_commit(self.sql_session, self):
            logger.info("Message submitted to user %s."
                        % user.username)

        self.redirect("/user/%s" % user_id)


class FileFromDigestHandler(FileHandler):

    def get(self, digest, filename):
        # TODO: Accept a MIME type
        self.sql_session.close()
        self.fetch(digest, "text/plain", filename)


class NotificationsHandler(BaseHandler):
    """Displays notifications.

    """
    def get(self):
        res = []
        last_notification = make_datetime(
            float(self.get_argument("last_notification", "0")))

        # Keep "== None" in filter arguments. SQLAlchemy does not
        # understand "is None".
        questions = self.sql_session.query(Question)\
            .filter(Question.reply_timestamp == None)\
            .filter(Question.question_timestamp > last_notification)\
            .all()  # noqa

        for question in questions:
            res.append({"type": "new_question",
                        "timestamp":
                        make_timestamp(question.question_timestamp),
                        "subject": question.subject,
                        "text": question.text,
                        "contest_id": question.user.contest_id})

        # Simple notifications
        for notification in self.application.service.notifications:
            res.append({"type": "notification",
                        "timestamp": make_timestamp(notification[0]),
                        "subject": notification[1],
                        "text": notification[2]})
        self.application.service.notifications = []

        self.write(json.dumps(res))


_aws_handlers = [
    (r"/", MainHandler),
    (r"/([0-9]+)", MainHandler),
    (r"/contest/([0-9]+)", ContestHandler),
    (r"/announcements/([0-9]+)", SimpleContestHandler("announcements.html")),
    (r"/userlist/([0-9]+)", SimpleContestHandler("userlist.html")),
    (r"/tasklist/([0-9]+)", SimpleContestHandler("tasklist.html")),
    (r"/contest/add", AddContestHandler),
    (r"/ranking/([0-9]+)", RankingHandler),
    (r"/ranking/([0-9]+)/([a-z]+)", RankingHandler),
    (r"/task/([0-9]+)", TaskHandler),
    (r"/dataset/([0-9]+)", DatasetSubmissionsHandler),
    (r"/add_task/([0-9]+)", AddTaskHandler),
    (r"/add_statement/([0-9]+)", AddStatementHandler),
    (r"/delete_statement/([0-9]+)", DeleteStatementHandler),
    (r"/add_attachment/([0-9]+)", AddAttachmentHandler),
    (r"/delete_attachment/([0-9]+)", DeleteAttachmentHandler),
    (r"/add_manager/([0-9]+)", AddManagerHandler),
    (r"/delete_manager/([0-9]+)", DeleteManagerHandler),
    (r"/add_dataset/([0-9]+)/(-|[0-9]+)", AddDatasetHandler),
    (r"/rename_dataset/([0-9]+)", RenameDatasetHandler),
    (r"/delete_dataset/([0-9]+)", DeleteDatasetHandler),
    (r"/activate_dataset/([0-9]+)", ActivateDatasetHandler),
    (r"/autojudge_dataset/([0-9]+)", ToggleAutojudgeDatasetHandler),
    (r"/add_testcase/([0-9]+)", AddTestcaseHandler),
    (r"/add_testcases/([0-9]+)", AddTestcasesHandler),
    (r"/delete_testcase/([0-9]+)", DeleteTestcaseHandler),
    (r"/user/([0-9]+)", UserViewHandler),
    (r"/add_user/([0-9]+)", AddUserHandler),
    (r"/add_announcement/([0-9]+)", AddAnnouncementHandler),
    (r"/remove_announcement/([0-9]+)", RemoveAnnouncementHandler),
    (r"/submission/([0-9]+)(?:/([0-9]+))?", SubmissionViewHandler),
    (r"/submission_file/([0-9]+)", SubmissionFileHandler),
    (r"/file/([a-f0-9]+)/([a-zA-Z0-9_.-]+)", FileFromDigestHandler),
    (r"/message/([0-9]+)", MessageHandler),
    (r"/question/([0-9]+)", QuestionReplyHandler),
    (r"/ignore_question/([0-9]+)", QuestionIgnoreHandler),
    (r"/questions/([0-9]+)", QuestionsHandler),
    (r"/resourceslist", ResourcesListHandler),
    (r"/resourceslist/([0-9]+)", ResourcesListHandler),
    (r"/resources", ResourcesHandler),
    (r"/resources/([0-9]+|all)", ResourcesHandler),
    (r"/resources/([0-9]+|all)/([0-9]+)", ResourcesHandler),
    (r"/notifications", NotificationsHandler),
]

########NEW FILE########
__FILENAME__ = ContestWebServer
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2014 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012-2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Bernard Blackham <bernard@largestprime.net>
# Copyright © 2014 Artem Iglikov <artem.iglikov@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""ContestWebServer serves the webpage that contestants are using to:

- view information about the contest (times, ...);
- view tasks;
- view documentation (STL, ...);
- submit questions;
- view announcements and answer to questions;
- submit solutions;
- view the state and maybe the score of their submissions;
- release submissions to see their full score;
- query the test interface.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import base64
import gettext
import glob
import io
import json
import logging
import os
import pickle
import pkg_resources
import re
import socket
import struct
import tempfile
import traceback
from datetime import timedelta
from urllib import quote

import tornado.web

from sqlalchemy import func

from werkzeug.http import parse_accept_header
from werkzeug.datastructures import LanguageAccept

from cms import SOURCE_EXT_TO_LANGUAGE_MAP, config, ServiceCoord
from cms.io import WebService
from cms.db import Session, Contest, User, Task, Question, Submission, Token, \
    File, UserTest, UserTestFile, UserTestManager
from cms.db.filecacher import FileCacher
from cms.grading.tasktypes import get_task_type
from cms.grading.scoretypes import get_score_type
from cms.server import file_handler_gen, extract_archive, \
    actual_phase_required, get_url_root, filter_ascii, \
    CommonRequestHandler, format_size
from cmscommon.isocodes import is_language_code, translate_language_code, \
    is_country_code, translate_country_code, \
    is_language_country_code, translate_language_country_code
from cmscommon.crypto import encrypt_number
from cmscommon.datetime import make_datetime, make_timestamp, get_timezone
from cmscommon.mimetypes import get_type_for_file_name


logger = logging.getLogger(__name__)


def check_ip(client, wanted):
    """Return if client IP belongs to the wanted subnet.

    client (string): IP address to verify.
    wanted (string): IP address or subnet to check against.

    return (bool): whether client equals wanted (if the latter is an IP
        address) or client belongs to wanted (if it's a subnet).

    """
    wanted, sep, subnet = wanted.partition('/')
    subnet = 32 if sep == "" else int(subnet)
    snmask = 2**32 - 2**(32 - subnet)
    wanted = struct.unpack(">I", socket.inet_aton(wanted))[0]
    client = struct.unpack(">I", socket.inet_aton(client))[0]
    return (wanted & snmask) == (client & snmask)


class BaseHandler(CommonRequestHandler):
    """Base RequestHandler for this application.

    All the RequestHandler classes in this application should be a
    child of this class.

    """

    # Whether the login cookie duration has to be refreshed when
    # this handler is called. Useful to filter asynchronous
    # requests.
    refresh_cookie = True

    def prepare(self):
        """This method is executed at the beginning of each request.

        """
        self.timestamp = make_datetime()

        self.set_header("Cache-Control", "no-cache, must-revalidate")

        self.sql_session = Session()
        self.contest = Contest.get_from_id(self.application.service.contest,
                                           self.sql_session)

        self._ = self.locale.translate

        self.r_params = self.render_params()

    def get_current_user(self):
        """Gets the current user logged in from the cookies

        If a valid cookie is retrieved, return a User object with the
        username specified in the cookie. Otherwise, return None.

        """
        if self.get_secure_cookie("login") is None:
            return None

        # Parse cookie.
        try:
            cookie = pickle.loads(self.get_secure_cookie("login"))
            username = cookie[0]
            password = cookie[1]
            last_update = make_datetime(cookie[2])
        except:
            self.clear_cookie("login")
            return None

        # Check if the cookie is expired.
        if self.timestamp - last_update > \
                timedelta(seconds=config.cookie_duration):
            self.clear_cookie("login")
            return None

        # Load the user from DB.
        user = self.sql_session.query(User)\
            .filter(User.contest == self.contest)\
            .filter(User.username == username).first()

        # Check if user exists and is allowed to login.
        if user is None or user.password != password:
            self.clear_cookie("login")
            return None
        if config.ip_lock and user.ip is not None \
                and not check_ip(self.request.remote_ip, user.ip):
            self.clear_cookie("login")
            return None
        if config.block_hidden_users and user.hidden:
            self.clear_cookie("login")
            return None

        if self.refresh_cookie:
            self.set_secure_cookie("login",
                                   pickle.dumps((user.username,
                                                 user.password,
                                                 make_timestamp())),
                                   expires_days=None)

        return user

    def get_user_locale(self):
        self.langs = self.application.service.langs

        if self.contest.allowed_localizations:
            # We just check if a prefix of each language is allowed
            # because this way one can just type "en" to include also
            # "en-US" (and similar cases with other languages). It's
            # the same approach promoted by HTTP in its Accept header
            # parsing rules.
            # TODO Be more fussy with prefix checking: validate strings
            # (match with "[A-Za-z]+(-[A-Za-z]+)*") and verify that the
            # prefix is on the dashes.
            self.langs = [lang for lang in self.langs if any(
                lang.startswith(prefix)
                for prefix in self.contest.allowed_localizations)]

        if not self.langs:
            logger.warning("No allowed localization matches any installed one."
                           "Resorting to en-US.")
            self.langs = ["en-US"]
        else:
            # TODO Be more fussy with prefix checking: validate strings
            # (match with "[A-Za-z]+(-[A-Za-z]+)*") and verify that the
            # prefix is on the dashes.
            useless = [
                prefix for prefix in self.contest.allowed_localizations
                if not any(lang.startswith(prefix) for lang in self.langs)]
            if useless:
                logger.warning("The following allowed localizations don't "
                               "match any installed one: %s" %
                               ",".join(useless))

        # TODO We fallback on any available language if none matches:
        # we could return 406 Not Acceptable instead.
        # Select the one the user likes most.
        self.browser_lang = parse_accept_header(
            self.request.headers.get("Accept-Language", ""),
            LanguageAccept).best_match(self.langs, self.langs[0])

        self.cookie_lang = self.get_cookie("language", None)

        if self.cookie_lang in self.langs:
            lang = self.cookie_lang
        else:
            lang = self.browser_lang

        self.set_header("Content-Language", lang)
        lang = lang.replace("-", "_")

        iso_639_locale = gettext.translation(
            "iso_639",
            os.path.join(config.iso_codes_prefix, "share", "locale"),
            [lang],
            fallback=True)
        iso_3166_locale = gettext.translation(
            "iso_3166",
            os.path.join(config.iso_codes_prefix, "share", "locale"),
            [lang],
            fallback=True)
        shared_mime_info_locale = gettext.translation(
            "shared-mime-info",
            os.path.join(
                config.shared_mime_info_prefix, "share", "locale"),
            [lang],
            fallback=True)
        cms_locale = gettext.translation(
            "cms",
            self.application.service.localization_dir,
            [lang],
            fallback=True)
        cms_locale.add_fallback(iso_639_locale)
        cms_locale.add_fallback(iso_3166_locale)
        cms_locale.add_fallback(shared_mime_info_locale)

        # Add translate method to simulate tornado.Locale's interface
        def translate(message, plural_message=None, count=None):
            if plural_message is not None:
                assert count is not None
                return cms_locale.ungettext(message, plural_message, count)
            else:
                return cms_locale.ugettext(message)
        cms_locale.translate = translate

        return cms_locale

    @staticmethod
    def _get_token_status(obj):
        """Return the status of the tokens for the given object.

        obj (Contest or Task): an object that has the token_* attributes.
        return (int): one of 0 (disabled), 1 (enabled/finite) and 2
                      (enabled/infinite).

        """
        if obj.token_mode == "disabled":
            return 0
        elif obj.token_mode == "finite":
            return 1
        elif obj.token_mode == "infinite":
            return 2
        else:
            raise RuntimeError("Unknown token_mode value.")

    def render_params(self):
        """Return the default render params used by almost all handlers.

        return (dict): default render params

        """
        ret = {}
        ret["timestamp"] = self.timestamp
        ret["contest"] = self.contest
        ret["url_root"] = get_url_root(self.request.path)
        ret["cookie"] = str(self.cookies)  # FIXME really needed?

        ret["phase"] = self.contest.phase(self.timestamp)

        if self.current_user is not None:
            # "adjust" the phase, considering the per_user_time
            ret["actual_phase"] = 2 * ret["phase"]

            if ret["phase"] == -1:
                # pre-contest phase
                ret["current_phase_begin"] = None
                ret["current_phase_end"] = self.contest.start
            elif ret["phase"] == 0:
                # contest phase
                if self.contest.per_user_time is None:
                    # "traditional" contest: every user can compete for
                    # the whole contest time
                    ret["current_phase_begin"] = self.contest.start
                    ret["current_phase_end"] = self.contest.stop
                else:
                    # "USACO-like" contest: every user can compete only
                    # for a limited time frame during the contest time
                    if self.current_user.starting_time is None:
                        ret["actual_phase"] = -1
                        ret["current_phase_begin"] = self.contest.start
                        ret["current_phase_end"] = self.contest.stop
                    else:
                        user_end_time = min(
                            self.current_user.starting_time +
                            self.contest.per_user_time,
                            self.contest.stop)
                        if self.timestamp <= user_end_time:
                            ret["current_phase_begin"] = \
                                self.current_user.starting_time
                            ret["current_phase_end"] = user_end_time
                        else:
                            ret["actual_phase"] = +1
                            ret["current_phase_begin"] = user_end_time
                            ret["current_phase_end"] = self.contest.stop
            else:  # ret["phase"] == 1
                # post-contest phase
                ret["current_phase_begin"] = self.contest.stop
                ret["current_phase_end"] = None

            # compute valid_phase_begin and valid_phase_end (that is,
            # the time at which actual_phase started/will start and
            # stopped/will stop being zero, or None if unknown).
            ret["valid_phase_begin"] = None
            ret["valid_phase_end"] = None
            if self.contest.per_user_time is None:
                ret["valid_phase_begin"] = self.contest.start
                ret["valid_phase_end"] = self.contest.stop
            elif self.current_user.starting_time is not None:
                ret["valid_phase_begin"] = self.current_user.starting_time
                ret["valid_phase_end"] = min(
                    self.current_user.starting_time +
                    self.contest.per_user_time,
                    self.contest.stop)

            # consider the extra time
            if ret["valid_phase_end"] is not None:
                ret["valid_phase_end"] += self.current_user.extra_time
                if ret["valid_phase_begin"] <= \
                        self.timestamp <= \
                        ret["valid_phase_end"]:
                    ret["phase"] = 0
                    ret["actual_phase"] = 0
                    ret["current_phase_begin"] = ret["valid_phase_begin"]
                    ret["current_phase_end"] = ret["valid_phase_end"]

            # set the timezone used to format timestamps
            ret["timezone"] = get_timezone(self.current_user, self.contest)

        # some information about token configuration
        ret["tokens_contest"] = self._get_token_status(self.contest)

        t_tokens = sum(self._get_token_status(t) for t in self.contest.tasks)
        if t_tokens == 0:
            ret["tokens_tasks"] = 0  # all disabled
        elif t_tokens == 2 * len(self.contest.tasks):
            ret["tokens_tasks"] = 2  # all infinite
        else:
            ret["tokens_tasks"] = 1  # all finite or mixed

        ret["langs"] = self.langs

        # TODO Now all language names are shown in the active language.
        # It would be better to show them in the corresponding one.
        ret["lang_names"] = {}
        for lang in self.langs:
            language_name = None
            try:
                language_name = translate_language_country_code(
                    lang.replace("-", "_"), self.locale)
            except ValueError:
                language_name = translate_language_code(
                    lang.replace("-", "_"), self.locale)
            ret["lang_names"][lang] = language_name

        ret["cookie_lang"] = self.cookie_lang
        ret["browser_lang"] = self.browser_lang

        return ret

    def finish(self, *args, **kwds):
        """Finish this response, ending the HTTP request.

        We override this method in order to properly close the database.

        TODO - Now that we have greenlet support, this method could be
        refactored in terms of context manager or something like
        that. So far I'm leaving it to minimize changes.

        """
        if hasattr(self, "sql_session"):
            try:
                self.sql_session.close()
            except Exception as error:
                logger.warning("Couldn't close SQL connection: %r" % error)
        try:
            tornado.web.RequestHandler.finish(self, *args, **kwds)
        except IOError:
            # When the client closes the connection before we reply,
            # Tornado raises an IOError exception, that would pollute
            # our log with unnecessarily critical messages
            logger.debug("Connection closed before our reply.")

    def write_error(self, status_code, **kwargs):
        if "exc_info" in kwargs and \
                kwargs["exc_info"][0] != tornado.web.HTTPError:
            exc_info = kwargs["exc_info"]
            logger.error(
                "Uncaught exception (%r) while processing a request: %s" %
                (exc_info[1], ''.join(traceback.format_exception(*exc_info))))

        # We assume that if r_params is defined then we have at least
        # the data we need to display a basic template with the error
        # information. If r_params is not defined (i.e. something went
        # *really* bad) we simply return a basic textual error notice.
        if hasattr(self, 'r_params'):
            self.render("error.html", status_code=status_code, **self.r_params)
        else:
            self.write("A critical error has occurred :-(")
            self.finish()


FileHandler = file_handler_gen(BaseHandler)


class ContestWebServer(WebService):
    """Service that runs the web server serving the contestants.

    """
    def __init__(self, shard, contest):
        parameters = {
            "login_url": "/",
            "template_path": pkg_resources.resource_filename(
                "cms.server", "templates/contest"),
            "static_path": pkg_resources.resource_filename(
                "cms.server", "static"),
            "cookie_secret": base64.b64encode(config.secret_key),
            "debug": config.tornado_debug,
            "is_proxy_used": config.is_proxy_used,
        }
        super(ContestWebServer, self).__init__(
            config.contest_listen_port[shard],
            _cws_handlers,
            parameters,
            shard=shard,
            listen_address=config.contest_listen_address[shard])

        self.contest = contest

        # This is a dictionary (indexed by username) of pending
        # notification. Things like "Yay, your submission went
        # through.", not things like "Your question has been replied",
        # that are handled by the db. Each username points to a list
        # of tuples (timestamp, subject, text).
        self.notifications = {}

        # Retrieve the available translations.
        if config.installed:
            self.localization_dir = os.path.join(
                "/", "usr", "local", "share", "locale")
        else:
            self.localization_dir = os.path.join(
                os.path.dirname(__file__), "mo")
        self.langs = ["en-US"] + [
            path.split("/")[-3].replace("_", "-") for path in glob.glob(
                os.path.join(self.localization_dir,
                             "*", "LC_MESSAGES", "cms.mo"))]

        self.file_cacher = FileCacher(self)
        self.evaluation_service = self.connect_to(
            ServiceCoord("EvaluationService", 0))
        self.scoring_service = self.connect_to(
            ServiceCoord("ScoringService", 0))
        self.proxy_service = self.connect_to(
            ServiceCoord("ProxyService", 0))

    NOTIFICATION_ERROR = "error"
    NOTIFICATION_WARNING = "warning"
    NOTIFICATION_SUCCESS = "success"

    def add_notification(self, username, timestamp, subject, text, level):
        """Store a new notification to send to a user at the first
        opportunity (i.e., at the first request fot db notifications).

        username (string): the user to notify.
        timestamp (datetime): the time of the notification.
        subject (string): subject of the notification.
        text (string): body of the notification.
        level (string): one of NOTIFICATION_* (defined above)

        """
        if username not in self.notifications:
            self.notifications[username] = []
        self.notifications[username].append((timestamp, subject, text, level))


class MainHandler(BaseHandler):
    """Home page handler.

    """
    def get(self):
        self.render("overview.html", **self.r_params)


class DocumentationHandler(BaseHandler):
    """Displays the instruction (compilation lines, documentation,
    ...) of the contest.

    """
    @tornado.web.authenticated
    def get(self):
        self.render("documentation.html", **self.r_params)


class LoginHandler(BaseHandler):
    """Login handler.

    """
    def post(self):
        username = self.get_argument("username", "")
        password = self.get_argument("password", "")
        next_page = self.get_argument("next", "/")
        user = self.sql_session.query(User)\
            .filter(User.contest == self.contest)\
            .filter(User.username == username).first()

        filtered_user = filter_ascii(username)
        filtered_pass = filter_ascii(password)
        if user is None or user.password != password:
            logger.info("Login error: user=%s pass=%s remote_ip=%s." %
                        (filtered_user, filtered_pass, self.request.remote_ip))
            self.redirect("/?login_error=true")
            return
        if config.ip_lock and user.ip is not None \
                and not check_ip(self.request.remote_ip, user.ip):
            logger.info("Unexpected IP: user=%s pass=%s remote_ip=%s." %
                        (filtered_user, filtered_pass, self.request.remote_ip))
            self.redirect("/?login_error=true")
            return
        if user.hidden and config.block_hidden_users:
            logger.info("Hidden user login attempt: "
                        "user=%s pass=%s remote_ip=%s." %
                        (filtered_user, filtered_pass, self.request.remote_ip))
            self.redirect("/?login_error=true")
            return

        logger.info("User logged in: user=%s remote_ip=%s." %
                    (filtered_user, self.request.remote_ip))
        self.set_secure_cookie("login",
                               pickle.dumps((user.username,
                                             user.password,
                                             make_timestamp())),
                               expires_days=None)
        self.redirect(next_page)


class StartHandler(BaseHandler):
    """Start handler.

    Used by a user who wants to start his per_user_time.

    """
    @tornado.web.authenticated
    @actual_phase_required(-1)
    def post(self):
        user = self.get_current_user()

        logger.info("Starting now for user %s" % user.username)
        user.starting_time = self.timestamp
        self.sql_session.commit()

        self.redirect("/")


class LogoutHandler(BaseHandler):
    """Logout handler.

    """
    def get(self):
        self.clear_cookie("login")
        self.redirect("/")


class TaskDescriptionHandler(BaseHandler):
    """Shows the data of a task in the contest.

    """
    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        # FIXME are submissions actually needed by this handler?
        submissions = self.sql_session.query(Submission)\
            .filter(Submission.user == self.current_user)\
            .filter(Submission.task == task).all()

        for statement in task.statements.itervalues():
            lang_code = statement.language
            if is_language_country_code(lang_code):
                statement.language_name = \
                    translate_language_country_code(lang_code, self.locale)
            elif is_language_code(lang_code):
                statement.language_name = \
                    translate_language_code(lang_code, self.locale)
            elif is_country_code(lang_code):
                statement.language_name = \
                    translate_country_code(lang_code, self.locale)
            else:
                statement.language_name = lang_code

        self.render("task_description.html",
                    task=task, submissions=submissions, **self.r_params)


class TaskSubmissionsHandler(BaseHandler):
    """Shows the data of a task in the contest.

    """
    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        submissions = self.sql_session.query(Submission)\
            .filter(Submission.user == self.current_user)\
            .filter(Submission.task == task).all()

        self.render("task_submissions.html",
                    task=task, submissions=submissions, **self.r_params)


class TaskStatementViewHandler(FileHandler):
    """Shows the statement file of a task in the contest.

    """
    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name, lang_code):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        if lang_code not in task.statements:
            raise tornado.web.HTTPError(404)

        statement = task.statements[lang_code].digest
        self.sql_session.close()

        if lang_code != '':
            filename = "%s (%s).pdf" % (task.name, lang_code)
        else:
            filename = "%s.pdf" % task.name

        self.fetch(statement, "application/pdf", filename)


class TaskAttachmentViewHandler(FileHandler):
    """Shows an attachment file of a task in the contest.

    """
    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name, filename):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        if filename not in task.attachments:
            raise tornado.web.HTTPError(404)

        attachment = task.attachments[filename].digest
        self.sql_session.close()

        mimetype = get_type_for_file_name(filename)
        if mimetype is None:
            mimetype = 'application/octet-stream'

        self.fetch(attachment, mimetype, filename)


class SubmissionFileHandler(FileHandler):
    """Send back a submission file.

    """
    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name, submission_num, filename):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        submission = self.sql_session.query(Submission)\
            .filter(Submission.user == self.current_user)\
            .filter(Submission.task == task)\
            .order_by(Submission.timestamp)\
            .offset(int(submission_num) - 1).first()
        if submission is None:
            raise tornado.web.HTTPError(404)

        # The following code assumes that submission.files is a subset
        # of task.submission_format. CWS will always ensure that for new
        # submissions, yet, if the submission_format changes during the
        # competition, this may not hold anymore for old submissions.

        # filename follows our convention (e.g. 'foo.%l'), real_filename
        # follows the one we present to the user (e.g. 'foo.c').
        real_filename = filename
        if submission.language is not None:
            if filename in submission.files:
                real_filename = filename.replace("%l", submission.language)
            else:
                # We don't recognize this filename. Let's try to 'undo'
                # the '%l' -> 'c|cpp|pas' replacement before giving up.
                filename = re.sub('\.%s$' % submission.language, '.%l',
                                  filename)

        if filename not in submission.files:
            raise tornado.web.HTTPError(404)

        digest = submission.files[filename].digest
        self.sql_session.close()

        mimetype = get_type_for_file_name(real_filename)
        if mimetype is None:
            mimetype = 'application/octet-stream'

        self.fetch(digest, mimetype, real_filename)


class CommunicationHandler(BaseHandler):
    """Displays the private conversations between the logged in user
    and the contest managers..

    """
    @tornado.web.authenticated
    def get(self):
        self.set_secure_cookie("unread_count", "0")
        self.render("communication.html", **self.r_params)


class NotificationsHandler(BaseHandler):
    """Displays notifications.

    """

    refresh_cookie = False

    @tornado.web.authenticated
    def get(self):
        if not self.current_user:
            raise tornado.web.HTTPError(403)
        res = []
        last_notification = make_datetime(
            float(self.get_argument("last_notification", "0")))

        # Announcements
        for announcement in self.contest.announcements:
            if announcement.timestamp > last_notification \
                    and announcement.timestamp < self.timestamp:
                res.append({"type": "announcement",
                            "timestamp":
                            make_timestamp(announcement.timestamp),
                            "subject": announcement.subject,
                            "text": announcement.text})

        if self.current_user is not None:
            # Private messages
            for message in self.current_user.messages:
                if message.timestamp > last_notification \
                        and message.timestamp < self.timestamp:
                    res.append({"type": "message",
                                "timestamp": make_timestamp(message.timestamp),
                                "subject": message.subject,
                                "text": message.text})

            # Answers to questions
            for question in self.current_user.questions:
                if question.reply_timestamp is not None \
                        and question.reply_timestamp > last_notification \
                        and question.reply_timestamp < self.timestamp:
                    subject = question.reply_subject
                    text = question.reply_text
                    if question.reply_subject is None:
                        subject = question.reply_text
                        text = ""
                    elif question.reply_text is None:
                        text = ""
                    res.append({"type": "question",
                                "timestamp":
                                make_timestamp(question.reply_timestamp),
                                "subject": subject,
                                "text": text})

        # Update the unread_count cookie before taking notifications
        # into account because we don't want to count them.
        prev_unread_count = self.get_secure_cookie("unread_count")
        next_unread_count = len(res) + (
            int(prev_unread_count) if prev_unread_count is not None else 0)
        self.set_secure_cookie("unread_count", str(next_unread_count))

        # Simple notifications
        notifications = self.application.service.notifications
        username = self.current_user.username
        if username in notifications:
            for notification in notifications[username]:
                res.append({"type": "notification",
                            "timestamp": make_timestamp(notification[0]),
                            "subject": notification[1],
                            "text": notification[2],
                            "level": notification[3]})
            del notifications[username]

        self.write(json.dumps(res))


class QuestionHandler(BaseHandler):
    """Called when the user submits a question.

    """
    @tornado.web.authenticated
    def post(self):
        # User can post only if we want.
        if not config.allow_questions:
            raise tornado.web.HTTPError(404)

        question = Question(self.timestamp,
                            self.get_argument("question_subject", ""),
                            self.get_argument("question_text", ""),
                            user=self.current_user)
        self.sql_session.add(question)
        self.sql_session.commit()

        logger.info("Question submitted by user %s."
                    % self.current_user.username)

        # Add "All ok" notification.
        self.application.service.add_notification(
            self.current_user.username,
            self.timestamp,
            self._("Question received"),
            self._("Your question has been received, you will be "
                   "notified when the it will be answered."),
            ContestWebServer.NOTIFICATION_SUCCESS)

        self.redirect("/communication")


class SubmitHandler(BaseHandler):
    """Handles the received submissions.

    """
    @tornado.web.authenticated
    @actual_phase_required(0)
    def post(self, task_name):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        # Alias for easy access
        contest = self.contest

        # Enforce maximum number of submissions
        try:
            if contest.max_submission_number is not None:
                submission_c = self.sql_session\
                    .query(func.count(Submission.id))\
                    .join(Submission.task)\
                    .filter(Task.contest == contest)\
                    .filter(Submission.user == self.current_user).scalar()
                if submission_c >= contest.max_submission_number:
                    raise ValueError(
                        self._("You have reached the maximum limit of "
                               "at most %d submissions among all tasks.") %
                        contest.max_submission_number)
            if task.max_submission_number is not None:
                submission_t = self.sql_session\
                    .query(func.count(Submission.id))\
                    .filter(Submission.task == task)\
                    .filter(Submission.user == self.current_user).scalar()
                if submission_t >= task.max_submission_number:
                    raise ValueError(
                        self._("You have reached the maximum limit of "
                               "at most %d submissions on this task.") %
                        task.max_submission_number)
        except ValueError as error:
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Too many submissions!"),
                error.message,
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/tasks/%s/submissions" % quote(task.name, safe=''))
            return

        # Enforce minimum time between submissions
        try:
            if contest.min_submission_interval is not None:
                last_submission_c = self.sql_session.query(Submission)\
                    .join(Submission.task)\
                    .filter(Task.contest == contest)\
                    .filter(Submission.user == self.current_user)\
                    .order_by(Submission.timestamp.desc()).first()
                if last_submission_c is not None and \
                        self.timestamp - last_submission_c.timestamp < \
                        contest.min_submission_interval:
                    raise ValueError(
                        self._("Among all tasks, you can submit again "
                               "after %d seconds from last submission.") %
                        contest.min_submission_interval.total_seconds())
            # We get the last submission even if we may not need it
            # for min_submission_interval because we may need it later,
            # in case this is a ALLOW_PARTIAL_SUBMISSION task.
            last_submission_t = self.sql_session.query(Submission)\
                .filter(Submission.task == task)\
                .filter(Submission.user == self.current_user)\
                .order_by(Submission.timestamp.desc()).first()
            if task.min_submission_interval is not None:
                if last_submission_t is not None and \
                        self.timestamp - last_submission_t.timestamp < \
                        task.min_submission_interval:
                    raise ValueError(
                        self._("For this task, you can submit again "
                               "after %d seconds from last submission.") %
                        task.min_submission_interval.total_seconds())
        except ValueError as error:
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Submissions too frequent!"),
                error.message,
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/tasks/%s/submissions" % quote(task.name, safe=''))
            return

        # Ensure that the user did not submit multiple files with the
        # same name.
        if any(len(filename) != 1 for filename in self.request.files.values()):
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Invalid submission format!"),
                self._("Please select the correct files."),
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/tasks/%s/submissions" % quote(task.name, safe=''))
            return

        # If the user submitted an archive, extract it and use content
        # as request.files.
        if len(self.request.files) == 1 and \
                self.request.files.keys()[0] == "submission":
            archive_data = self.request.files["submission"][0]
            del self.request.files["submission"]

            # Extract the files from the archive.
            temp_archive_file, temp_archive_filename = \
                tempfile.mkstemp(dir=config.temp_dir)
            with os.fdopen(temp_archive_file, "w") as temp_archive_file:
                temp_archive_file.write(archive_data["body"])

            archive_contents = extract_archive(temp_archive_filename,
                                               archive_data["filename"])

            if archive_contents is None:
                self.application.service.add_notification(
                    self.current_user.username,
                    self.timestamp,
                    self._("Invalid archive format!"),
                    self._("The submitted archive could not be opened."),
                    ContestWebServer.NOTIFICATION_ERROR)
                self.redirect("/tasks/%s/submissions" % quote(task.name,
                                                              safe=''))
                return

            for item in archive_contents:
                self.request.files[item["filename"]] = [item]

        # This ensure that the user sent one file for every name in
        # submission format and no more. Less is acceptable if task
        # type says so.
        task_type = get_task_type(dataset=task.active_dataset)
        required = set([sfe.filename for sfe in task.submission_format])
        provided = set(self.request.files.keys())
        if not (required == provided or (task_type.ALLOW_PARTIAL_SUBMISSION
                                         and required.issuperset(provided))):
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Invalid submission format!"),
                self._("Please select the correct files."),
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/tasks/%s/submissions" % quote(task.name, safe=''))
            return

        # Add submitted files. After this, files is a dictionary indexed
        # by *our* filenames (something like "output01.txt" or
        # "taskname.%l", and whose value is a couple
        # (user_assigned_filename, content).
        files = {}
        for uploaded, data in self.request.files.iteritems():
            files[uploaded] = (data[0]["filename"], data[0]["body"])

        # If we allow partial submissions, implicitly we recover the
        # non-submitted files from the previous submission. And put them
        # in file_digests (i.e. like they have already been sent to FS).
        submission_lang = None
        file_digests = {}
        retrieved = 0
        if task_type.ALLOW_PARTIAL_SUBMISSION and \
                last_submission_t is not None:
            for filename in required.difference(provided):
                if filename in last_submission_t.files:
                    # If we retrieve a language-dependent file from
                    # last submission, we take not that language must
                    # be the same.
                    if "%l" in filename:
                        submission_lang = last_submission_t.language
                    file_digests[filename] = \
                        last_submission_t.files[filename].digest
                    retrieved += 1

        # We need to ensure that everytime we have a .%l in our
        # filenames, the user has the extension of an allowed
        # language, and that all these are the same (i.e., no
        # mixed-language submissions).
        def which_language(user_filename):
            """Determine the language of user_filename from its
            extension.

            user_filename (string): the file to test.
            return (string): the extension of user_filename, or None
                             if it is not a recognized language.

            """
            for source_ext, language in SOURCE_EXT_TO_LANGUAGE_MAP.iteritems():
                if user_filename.endswith(source_ext):
                    return language
            return None

        error = None
        for our_filename in files:
            user_filename = files[our_filename][0]
            if our_filename.find(".%l") != -1:
                lang = which_language(user_filename)
                if lang is None:
                    error = self._("Cannot recognize submission's language.")
                    break
                elif submission_lang is not None and \
                        submission_lang != lang:
                    error = self._("All sources must be in the same language.")
                    break
                elif lang not in contest.languages:
                    error = self._(
                        "Language %s not allowed in this contest." % lang)
                    break
                else:
                    submission_lang = lang
        if error is not None:
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Invalid submission!"),
                error,
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/tasks/%s/submissions" % quote(task.name, safe=''))
            return

        # Check if submitted files are small enough.
        if any([len(f[1]) > config.max_submission_length
                for f in files.values()]):
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Submission too big!"),
                self._("Each source file must be at most %d bytes long.") %
                config.max_submission_length,
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/tasks/%s/submissions" % quote(task.name, safe=''))
            return

        # All checks done, submission accepted.

        # Attempt to store the submission locally to be able to
        # recover a failure.
        if config.submit_local_copy:
            try:
                path = os.path.join(
                    config.submit_local_copy_path.replace("%s",
                                                          config.data_dir),
                    self.current_user.username)
                if not os.path.exists(path):
                    os.makedirs(path)
                # Pickle in ASCII format produces str, not unicode,
                # therefore we open the file in binary mode.
                with io.open(
                        os.path.join(path,
                                     str(int(make_timestamp(self.timestamp)))),
                        "wb") as file_:
                    pickle.dump((self.contest.id,
                                 self.current_user.id,
                                 task.id,
                                 files), file_)
            except Exception as error:
                logger.warning("Submission local copy failed - %s" %
                               traceback.format_exc())

        # We now have to send all the files to the destination...
        try:
            for filename in files:
                digest = self.application.service.file_cacher.put_file_content(
                    files[filename][1],
                    "Submission file %s sent by %s at %d." % (
                        filename, self.current_user.username,
                        make_timestamp(self.timestamp)))
                file_digests[filename] = digest

        # In case of error, the server aborts the submission
        except Exception as error:
            logger.error("Storage failed! %s" % error)
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Submission storage failed!"),
                self._("Please try again."),
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/tasks/%s/submissions" % quote(task.name, safe=''))
            return

        # All the files are stored, ready to submit!
        logger.info("All files stored for submission sent by %s" %
                    self.current_user.username)
        submission = Submission(self.timestamp,
                                submission_lang,
                                user=self.current_user,
                                task=task)

        for filename, digest in file_digests.items():
            self.sql_session.add(File(filename, digest, submission=submission))
        self.sql_session.add(submission)
        self.sql_session.commit()
        self.application.service.evaluation_service.new_submission(
            submission_id=submission.id)
        self.application.service.add_notification(
            self.current_user.username,
            self.timestamp,
            self._("Submission received"),
            self._("Your submission has been received "
                   "and is currently being evaluated."),
            ContestWebServer.NOTIFICATION_SUCCESS)
        # The argument (encripted submission id) is not used by CWS
        # (nor it discloses information to the user), but it is useful
        # for automatic testing to obtain the submission id).
        # FIXME is it actually used by something?
        self.redirect("/tasks/%s/submissions?%s" % (
            quote(task.name, safe=''),
            encrypt_number(submission.id)))


class UseTokenHandler(BaseHandler):
    """Called when the user try to use a token on a submission.

    """
    @tornado.web.authenticated
    @actual_phase_required(0)
    def post(self, task_name, submission_num):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        submission = self.sql_session.query(Submission)\
            .filter(Submission.user == self.current_user)\
            .filter(Submission.task == task)\
            .order_by(Submission.timestamp)\
            .offset(int(submission_num) - 1).first()
        if submission is None:
            raise tornado.web.HTTPError(404)

        # Don't trust the user, check again if (s)he can really play
        # the token.
        tokens_available = self.contest.tokens_available(
            self.current_user.username,
            task.name,
            self.timestamp)
        if tokens_available[0] == 0 or tokens_available[2] is not None:
            logger.warning("User %s tried to play a token "
                           "when it shouldn't."
                           % self.current_user.username)
            # Add "no luck" notification
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Token request discarded"),
                self._("Your request has been discarded because you have no "
                       "tokens available."),
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/tasks/%s/submissions" % quote(task.name, safe=''))
            return

        if submission.token is None:
            token = Token(self.timestamp, submission=submission)
            self.sql_session.add(token)
            self.sql_session.commit()
        else:
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Token request discarded"),
                self._("Your request has been discarded because you already "
                       "used a token on that submission."),
                ContestWebServer.NOTIFICATION_WARNING)
            self.redirect("/tasks/%s/submissions" % quote(task.name, safe=''))
            return

        # Inform ScoringService and eventually the ranking that the
        # token has been played.
        self.application.service.proxy_service.submission_tokened(
            submission_id=submission.id)

        logger.info("Token played by user %s on task %s."
                    % (self.current_user.username, task.name))

        # Add "All ok" notification
        self.application.service.add_notification(
            self.current_user.username,
            self.timestamp,
            self._("Token request received"),
            self._("Your request has been received "
                   "and applied to the submission."),
            ContestWebServer.NOTIFICATION_SUCCESS)

        self.redirect("/tasks/%s/submissions" % quote(task.name, safe=''))


class SubmissionStatusHandler(BaseHandler):

    refresh_cookie = False

    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name, submission_num):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        submission = self.sql_session.query(Submission)\
            .filter(Submission.user == self.current_user)\
            .filter(Submission.task == task)\
            .order_by(Submission.timestamp)\
            .offset(int(submission_num) - 1).first()
        if submission is None:
            raise tornado.web.HTTPError(404)

        sr = submission.get_result(task.active_dataset)
        score_type = get_score_type(dataset=task.active_dataset)

        # TODO: use some kind of constants to refer to the status.
        data = dict()
        if sr is None or not sr.compiled():
            data["status"] = 1
            data["status_text"] = self._("Compiling...")
        elif sr.compilation_failed():
            data["status"] = 2
            data["status_text"] = "%s <a class=\"details\">%s</a>" % (
                self._("Compilation failed"), self._("details"))
        elif not sr.evaluated():
            data["status"] = 3
            data["status_text"] = self._("Evaluating...")
        elif not sr.scored():
            data["status"] = 4
            data["status_text"] = self._("Scoring...")
        else:
            data["status"] = 5
            data["status_text"] = "%s <a class=\"details\">%s</a>" % (
                self._("Evaluated"), self._("details"))

            if score_type is not None and score_type.max_public_score != 0:
                data["max_public_score"] = "%g" % \
                    round(score_type.max_public_score, task.score_precision)
            data["public_score"] = "%g" % \
                round(sr.public_score, task.score_precision)
            if submission.token is not None:
                if score_type is not None and score_type.max_score != 0:
                    data["max_score"] = "%g" % \
                        round(score_type.max_score, task.score_precision)
                data["score"] = "%g" % \
                    round(sr.score, task.score_precision)

        self.write(data)


class SubmissionDetailsHandler(BaseHandler):

    refresh_cookie = False

    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name, submission_num):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        submission = self.sql_session.query(Submission)\
            .filter(Submission.user == self.current_user)\
            .filter(Submission.task == task)\
            .order_by(Submission.timestamp)\
            .offset(int(submission_num) - 1).first()
        if submission is None:
            raise tornado.web.HTTPError(404)

        sr = submission.get_result(task.active_dataset)
        score_type = get_score_type(dataset=task.active_dataset)

        details = None
        if sr is not None:
            if submission.tokened():
                details = sr.score_details
            else:
                details = sr.public_score_details

            if sr.scored():
                details = score_type.get_html_details(details, self._)
            else:
                details = None

        self.render("submission_details.html",
                    sr=sr,
                    details=details)


class UserTestInterfaceHandler(BaseHandler):
    """Serve the interface to test programs.

    """
    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self):
        user_tests = dict()
        default_task = None

        for task in self.contest.tasks:
            if self.request.query == task.name:
                default_task = task
            user_tests[task.id] = self.sql_session.query(UserTest)\
                .filter(UserTest.user == self.current_user)\
                .filter(UserTest.task == task).all()

        if default_task is None and len(self.contest.tasks) > 0:
            default_task = self.contest.tasks[0]

        self.render("test_interface.html", default_task=default_task,
                    user_tests=user_tests, **self.r_params)


class UserTestHandler(BaseHandler):

    refresh_cookie = False

    # The following code has been taken from SubmitHandler and adapted
    # for UserTests.

    @tornado.web.authenticated
    @actual_phase_required(0)
    def post(self, task_name):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        # Check that the task is testable
        task_type = get_task_type(dataset=task.active_dataset)
        if not task_type.testable:
            logger.warning("User %s tried to make test on task %s." %
                           (self.current_user.username, task_name))
            raise tornado.web.HTTPError(404)

        # Alias for easy access
        contest = self.contest

        # Enforce maximum number of user_tests
        try:
            if contest.max_user_test_number is not None:
                user_test_c = self.sql_session.query(func.count(UserTest.id))\
                    .join(UserTest.task)\
                    .filter(Task.contest == contest)\
                    .filter(UserTest.user == self.current_user).scalar()
                if user_test_c >= contest.max_user_test_number:
                    raise ValueError(
                        self._("You have reached the maximum limit of "
                               "at most %d tests among all tasks.") %
                        contest.max_user_test_number)
            if task.max_user_test_number is not None:
                user_test_t = self.sql_session.query(func.count(UserTest.id))\
                    .filter(UserTest.task == task)\
                    .filter(UserTest.user == self.current_user).scalar()
                if user_test_t >= task.max_user_test_number:
                    raise ValueError(
                        self._("You have reached the maximum limit of "
                               "at most %d tests on this task.") %
                        task.max_user_test_number)
        except ValueError as error:
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Too many tests!"),
                error.message,
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/testing?%s" % quote(task.name, safe=''))
            return

        # Enforce minimum time between user_tests
        try:
            if contest.min_user_test_interval is not None:
                last_user_test_c = self.sql_session.query(UserTest)\
                    .join(UserTest.task)\
                    .filter(Task.contest == contest)\
                    .filter(UserTest.user == self.current_user)\
                    .order_by(UserTest.timestamp.desc()).first()
                if last_user_test_c is not None and \
                        self.timestamp - last_user_test_c.timestamp < \
                        contest.min_user_test_interval:
                    raise ValueError(
                        self._("Among all tasks, you can test again "
                               "after %d seconds from last test.") %
                        contest.min_user_test_interval.total_seconds())
            # We get the last user_test even if we may not need it
            # for min_user_test_interval because we may need it later,
            # in case this is a ALLOW_PARTIAL_SUBMISSION task.
            last_user_test_t = self.sql_session.query(UserTest)\
                .filter(UserTest.task == task)\
                .filter(UserTest.user == self.current_user)\
                .order_by(UserTest.timestamp.desc()).first()
            if task.min_user_test_interval is not None:
                if last_user_test_t is not None and \
                        self.timestamp - last_user_test_t.timestamp < \
                        task.min_user_test_interval:
                    raise ValueError(
                        self._("For this task, you can test again "
                               "after %d seconds from last test.") %
                        task.min_user_test_interval.total_seconds())
        except ValueError as error:
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Tests too frequent!"),
                error.message,
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/testing?%s" % quote(task.name, safe=''))
            return

        # Ensure that the user did not submit multiple files with the
        # same name.
        if any(len(filename) != 1 for filename in self.request.files.values()):
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Invalid test format!"),
                self._("Please select the correct files."),
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/testing?%s" % quote(task.name, safe=''))
            return

        # If the user submitted an archive, extract it and use content
        # as request.files.
        if len(self.request.files) == 1 and \
                self.request.files.keys()[0] == "submission":
            archive_data = self.request.files["submission"][0]
            del self.request.files["submission"]

            # Extract the files from the archive.
            temp_archive_file, temp_archive_filename = \
                tempfile.mkstemp(dir=config.temp_dir)
            with os.fdopen(temp_archive_file, "w") as temp_archive_file:
                temp_archive_file.write(archive_data["body"])

            archive_contents = extract_archive(temp_archive_filename,
                                               archive_data["filename"])

            if archive_contents is None:
                self.application.service.add_notification(
                    self.current_user.username,
                    self.timestamp,
                    self._("Invalid archive format!"),
                    self._("The submitted archive could not be opened."),
                    ContestWebServer.NOTIFICATION_ERROR)
                self.redirect("/testing?%s" % quote(task.name, safe=''))
                return

            for item in archive_contents:
                self.request.files[item["filename"]] = [item]

        # This ensure that the user sent one file for every name in
        # submission format and no more. Less is acceptable if task
        # type says so.
        required = set([sfe.filename for sfe in task.submission_format] +
                       task_type.get_user_managers(task.submission_format) +
                       ["input"])
        provided = set(self.request.files.keys())
        if not (required == provided or (task_type.ALLOW_PARTIAL_SUBMISSION
                                         and required.issuperset(provided))):
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Invalid test format!"),
                self._("Please select the correct files."),
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/testing?%s" % quote(task.name, safe=''))
            return

        # Add submitted files. After this, files is a dictionary indexed
        # by *our* filenames (something like "output01.txt" or
        # "taskname.%l", and whose value is a couple
        # (user_assigned_filename, content).
        files = {}
        for uploaded, data in self.request.files.iteritems():
            files[uploaded] = (data[0]["filename"], data[0]["body"])

        # If we allow partial submissions, implicitly we recover the
        # non-submitted files from the previous submission. And put them
        # in file_digests (i.e. like they have already been sent to FS).
        submission_lang = None
        file_digests = {}
        retrieved = 0
        if task_type.ALLOW_PARTIAL_SUBMISSION and last_user_test_t is not None:
            for filename in required.difference(provided):
                if filename in last_user_test_t.files:
                    # If we retrieve a language-dependent file from
                    # last submission, we take not that language must
                    # be the same.
                    if "%l" in filename:
                        submission_lang = last_user_test_t.language
                    file_digests[filename] = \
                        last_user_test_t.files[filename].digest
                    retrieved += 1

        # We need to ensure that everytime we have a .%l in our
        # filenames, the user has one amongst ".cpp", ".c", or ".pas,
        # and that all these are the same (i.e., no mixed-language
        # submissions).
        def which_language(user_filename):
            """Determine the language of user_filename from its
            extension.

            user_filename (string): the file to test.
            return (string): the extension of user_filename, or None
                             if it is not a recognized language.

            """
            for source_ext, language in SOURCE_EXT_TO_LANGUAGE_MAP.iteritems():
                if user_filename.endswith(source_ext):
                    return language
            return None

        error = None
        for our_filename in files:
            user_filename = files[our_filename][0]
            if our_filename.find(".%l") != -1:
                lang = which_language(user_filename)
                if lang is None:
                    error = self._("Cannot recognize test's language.")
                    break
                elif submission_lang is not None and \
                        submission_lang != lang:
                    error = self._("All sources must be in the same language.")
                    break
                else:
                    submission_lang = lang
        if error is not None:
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Invalid test!"),
                error,
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/testing?%s" % quote(task.name, safe=''))
            return

        # Check if submitted files are small enough.
        if any([len(f[1]) > config.max_submission_length
                for n, f in files.items() if n != "input"]):
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Test too big!"),
                self._("Each source file must be at most %d bytes long.") %
                config.max_submission_length,
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/testing?%s" % quote(task.name, safe=''))
            return
        if len(files["input"][1]) > config.max_input_length:
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Input too big!"),
                self._("The input file must be at most %d bytes long.") %
                config.max_input_length,
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/testing?%s" % quote(task.name, safe=''))
            return

        # All checks done, submission accepted.

        # Attempt to store the submission locally to be able to
        # recover a failure.
        if config.tests_local_copy:
            try:
                path = os.path.join(
                    config.tests_local_copy_path.replace("%s",
                                                         config.data_dir),
                    self.current_user.username)
                if not os.path.exists(path):
                    os.makedirs(path)
                # Pickle in ASCII format produces str, not unicode,
                # therefore we open the file in binary mode.
                with io.open(
                        os.path.join(path,
                                     str(int(make_timestamp(self.timestamp)))),
                        "wb") as file_:
                    pickle.dump((self.contest.id,
                                 self.current_user.id,
                                 task.id,
                                 files), file_)
            except Exception as error:
                logger.error("Test local copy failed - %s" %
                             traceback.format_exc())

        # We now have to send all the files to the destination...
        try:
            for filename in files:
                digest = self.application.service.file_cacher.put_file_content(
                    files[filename][1],
                    "Test file %s sent by %s at %d." % (
                        filename, self.current_user.username,
                        make_timestamp(self.timestamp)))
                file_digests[filename] = digest

        # In case of error, the server aborts the submission
        except Exception as error:
            logger.error("Storage failed! %s" % error)
            self.application.service.add_notification(
                self.current_user.username,
                self.timestamp,
                self._("Test storage failed!"),
                self._("Please try again."),
                ContestWebServer.NOTIFICATION_ERROR)
            self.redirect("/testing?%s" % quote(task.name, safe=''))
            return

        # All the files are stored, ready to submit!
        logger.info("All files stored for test sent by %s" %
                    self.current_user.username)
        user_test = UserTest(self.timestamp,
                             submission_lang,
                             file_digests["input"],
                             user=self.current_user,
                             task=task)

        for filename in [sfe.filename for sfe in task.submission_format]:
            digest = file_digests[filename]
            self.sql_session.add(
                UserTestFile(filename, digest, user_test=user_test))
        for filename in task_type.get_user_managers(task.submission_format):
            digest = file_digests[filename]
            if submission_lang is not None:
                filename = filename.replace("%l", submission_lang)
            self.sql_session.add(
                UserTestManager(filename, digest, user_test=user_test))

        self.sql_session.add(user_test)
        self.sql_session.commit()
        self.application.service.evaluation_service.new_user_test(
            user_test_id=user_test.id)
        self.application.service.add_notification(
            self.current_user.username,
            self.timestamp,
            self._("Test received"),
            self._("Your test has been received "
                   "and is currently being executed."),
            ContestWebServer.NOTIFICATION_SUCCESS)
        self.redirect("/testing?%s" % quote(task.name, safe=''))


class UserTestStatusHandler(BaseHandler):

    refresh_cookie = False

    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name, user_test_num):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        user_test = self.sql_session.query(UserTest)\
            .filter(UserTest.user == self.current_user)\
            .filter(UserTest.task == task)\
            .order_by(UserTest.timestamp)\
            .offset(int(user_test_num) - 1).first()
        if user_test is None:
            raise tornado.web.HTTPError(404)

        ur = user_test.get_result(task.active_dataset)

        # TODO: use some kind of constants to refer to the status.
        data = dict()
        if ur is None or not ur.compiled():
            data["status"] = 1
            data["status_text"] = self._("Compiling...")
        elif ur.compilation_failed():
            data["status"] = 2
            data["status_text"] = "%s <a class=\"details\">%s</a>" % (
                self._("Compilation failed"), self._("details"))
        elif not ur.evaluated():
            data["status"] = 3
            data["status_text"] = self._("Executing...")
        else:
            data["status"] = 4
            data["status_text"] = "%s <a class=\"details\">%s</a>" % (
                self._("Executed"), self._("details"))
            if ur.execution_time is not None:
                data["time"] = self._("%(seconds)0.3f s") % {
                    'seconds': ur.execution_time}
            else:
                data["time"] = None
            if ur.execution_memory is not None:
                data["memory"] = format_size(ur.execution_memory)
            else:
                data["memory"] = None
            data["output"] = ur.output is not None

        self.write(data)


class UserTestDetailsHandler(BaseHandler):

    refresh_cookie = False

    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name, user_test_num):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        user_test = self.sql_session.query(UserTest)\
            .filter(UserTest.user == self.current_user)\
            .filter(UserTest.task == task)\
            .order_by(UserTest.timestamp)\
            .offset(int(user_test_num) - 1).first()
        if user_test is None:
            raise tornado.web.HTTPError(404)

        tr = user_test.get_result(task.active_dataset)

        self.render("user_test_details.html", task=task, tr=tr)


class UserTestIOHandler(FileHandler):
    """Send back a submission file.

    """
    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name, user_test_num, io):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        user_test = self.sql_session.query(UserTest)\
            .filter(UserTest.user == self.current_user)\
            .filter(UserTest.task == task)\
            .order_by(UserTest.timestamp)\
            .offset(int(user_test_num) - 1).first()
        if user_test is None:
            raise tornado.web.HTTPError(404)

        if io == "input":
            digest = user_test.input
        else:  # io == "output"
            tr = user_test.get_result(task.active_dataset)
            digest = tr.output if tr is not None else None
        self.sql_session.close()

        if digest is None:
            raise tornado.web.HTTPError(404)

        mimetype = 'text/plain'

        self.fetch(digest, mimetype, io)


class UserTestFileHandler(FileHandler):
    """Send back a submission file.

    """
    @tornado.web.authenticated
    @actual_phase_required(0)
    def get(self, task_name, user_test_num, filename):
        try:
            task = self.contest.get_task(task_name)
        except KeyError:
            raise tornado.web.HTTPError(404)

        user_test = self.sql_session.query(UserTest)\
            .filter(UserTest.user == self.current_user)\
            .filter(UserTest.task == task)\
            .order_by(UserTest.timestamp)\
            .offset(int(user_test_num) - 1).first()
        if user_test is None:
            raise tornado.web.HTTPError(404)

        # filename follows our convention (e.g. 'foo.%l'), real_filename
        # follows the one we present to the user (e.g. 'foo.c').
        real_filename = filename
        if user_test.language is not None:
            real_filename = filename.replace("%l", user_test.language)

        if filename in user_test.files:
            digest = user_test.files[filename].digest
        elif filename in user_test.managers:
            digest = user_test.managers[filename].digest
        else:
            raise tornado.web.HTTPError(404)
        self.sql_session.close()

        mimetype = get_type_for_file_name(real_filename)
        if mimetype is None:
            mimetype = 'application/octet-stream'

        self.fetch(digest, mimetype, real_filename)


class StaticFileGzHandler(tornado.web.StaticFileHandler):
    """Handle files which may be gzip-compressed on the filesystem."""
    def get(self, path, *args, **kwargs):
        # Unless told otherwise, default to text/plain.
        self.set_header("Content-Type", "text/plain")
        try:
            # Try an ordinary request.
            tornado.web.StaticFileHandler.get(self, path, *args, **kwargs)
        except tornado.web.HTTPError as error:
            if error.status_code == 404:
                # If that failed, try servicing it with a .gz extension.
                path = "%s.gz" % path

                tornado.web.StaticFileHandler.get(self, path, *args, **kwargs)

                # If it succeeded, then mark the encoding as gzip.
                self.set_header("Content-Encoding", "gzip")
            else:
                raise


_cws_handlers = [
    (r"/", MainHandler),
    (r"/login", LoginHandler),
    (r"/logout", LogoutHandler),
    (r"/start", StartHandler),
    (r"/tasks/(.*)/description", TaskDescriptionHandler),
    (r"/tasks/(.*)/submissions", TaskSubmissionsHandler),
    (r"/tasks/(.*)/statements/(.*)", TaskStatementViewHandler),
    (r"/tasks/(.*)/attachments/(.*)", TaskAttachmentViewHandler),
    (r"/tasks/(.*)/submit", SubmitHandler),
    (r"/tasks/(.*)/submissions/([1-9][0-9]*)", SubmissionStatusHandler),
    (r"/tasks/(.*)/submissions/([1-9][0-9]*)/details",
     SubmissionDetailsHandler),
    (r"/tasks/(.*)/submissions/([1-9][0-9]*)/files/(.*)",
     SubmissionFileHandler),
    (r"/tasks/(.*)/submissions/([1-9][0-9]*)/token", UseTokenHandler),
    (r"/tasks/(.*)/test", UserTestHandler),
    (r"/tasks/(.*)/tests/([1-9][0-9]*)", UserTestStatusHandler),
    (r"/tasks/(.*)/tests/([1-9][0-9]*)/details", UserTestDetailsHandler),
    (r"/tasks/(.*)/tests/([1-9][0-9]*)/(input|output)", UserTestIOHandler),
    (r"/tasks/(.*)/tests/([1-9][0-9]*)/files/(.*)", UserTestFileHandler),
    (r"/communication", CommunicationHandler),
    (r"/documentation", DocumentationHandler),
    (r"/notifications", NotificationsHandler),
    (r"/question", QuestionHandler),
    (r"/testing", UserTestInterfaceHandler),
    (r"/stl/(.*)", StaticFileGzHandler, {"path": config.stl_path}),
]

########NEW FILE########
__FILENAME__ = Checker
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Service that checks the answering times of all services.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging
import time

from cms import config, ServiceCoord
from cms.io import Service


logger = logging.getLogger(__name__)


class Checker(Service):
    """Service that checks the answering times of all services.

    """

    def __init__(self, shard):
        Service.__init__(self, shard)
        for service in config.async.core_services:
            self.connect_to(service)
        self.add_timeout(self.check, None, 90.0, immediately=True)

        self.waiting_for = {}

    def check(self):
        """For all services, send an echo request and logs the time of
        the request.

        """
        logger.debug("Checker.check")
        for coordinates, service in self.remote_services.iteritems():
            if coordinates in self.waiting_for:
                logger.info("Service %s timeout, retrying." % str(coordinates))
                del self.waiting_for[coordinates]

            if service.connected:
                now = time.time()
                self.waiting_for[coordinates] = now
                service.echo(string="%s %5.3lf" % (coordinates, now),
                             callback=self.echo_callback)
            else:
                logger.info("Service %s not connected." % str(coordinates))
        return True

    def echo_callback(self, data, error=None):
        """Callback for check.

        """
        current = time.time()
        logger.debug("Checker.echo_callback")
        if error is not None:
            return
        try:
            service, time_ = data.split()
            time_ = float(time_)
            name, shard = service.split(",")
            shard = int(shard)
            service = ServiceCoord(name, shard)
            if service not in self.waiting_for or current - time_ > 10:
                logger.warning("Got late reply (%5.3lf s) from %s."
                               % (current - time_, service))
            else:
                if time_ - self.waiting_for[service] > 0.001:
                    logger.warning("Someone cheated on the timestamp?!")
                logger.info("Got reply (%5.3lf s) from %s."
                            % (current - time_, service))
                del self.waiting_for[service]
        except KeyError:
            logger.error("Echo answer mis-shapen.")

########NEW FILE########
__FILENAME__ = EvaluationService
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Bernard Blackham <bernard@largestprime.net>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Evaluation service. It takes care of receiving submissions from the
contestants, transforming them in jobs (compilation, execution, ...),
queuing them with the right priority, and dispatching them to the
workers. Also, it collects the results from the workers and build the
current ranking.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging
import random
from datetime import timedelta
from collections import namedtuple

from cms import ServiceCoord, get_service_shards
from cms.io import Service, rpc_method
from cms.db import SessionGen, Contest, Dataset, Submission, \
    SubmissionResult, UserTest, UserTestResult
from cms.service import get_submission_results, get_datasets_to_judge
from cmscommon.datetime import make_datetime, make_timestamp
from cms.grading.Job import JobGroup


logger = logging.getLogger(__name__)


def to_compile(submission_result):
    """Return whether ES is interested in compiling the submission.

    submission_result (SubmissionResult): a submission result.

    return (bool): True if ES wants to compile the submission.

    """
    r = submission_result
    return r is None or \
        (not r.compiled() and
         r.compilation_tries < EvaluationService.MAX_COMPILATION_TRIES)


def to_evaluate(submission_result):
    """Return whether ES is interested in evaluating the submission.

    submission_result (SubmissionResult): a submission result.

    return (bool): True if ES wants to evaluate the submission.

    """
    r = submission_result
    return r is not None and r.compilation_succeeded() and \
        not r.evaluated() and \
        r.evaluation_tries < EvaluationService.MAX_EVALUATION_TRIES


def user_test_to_compile(user_test_result):
    """Return whether ES is interested in compiling the user test.

    user_test_result (UserTestResult): a user test result.

    return (bool): True if ES wants to compile the user test.

    """
    r = user_test_result
    return r is None or \
        (not r.compiled() and
         r.compilation_tries < EvaluationService.MAX_TEST_COMPILATION_TRIES)


def user_test_to_evaluate(user_test_result):
    """Return whether ES is interested in evaluating the user test.

    user_test_result (UserTestResult): a user test result.

    return (bool): True if ES wants to evaluate the user test.

    """
    r = user_test_result
    return r is not None and r.compilation_outcome == "ok" and \
        not r.evaluated() and \
        r.evaluation_tries < EvaluationService.MAX_TEST_EVALUATION_TRIES


# job_type is a constant defined in EvaluationService.
JobQueueEntry = namedtuple('JobQueueEntry',
                           ['job_type', 'object_id', 'dataset_id'])


class JobQueue(object):
    """An instance of this class will contains the (unique) priority
    queue of jobs (compilations, evaluations, ...) that the ES needs
    to process next.

    The queue is implemented as a custom min-heap.

    """

    def __init__(self):
        # The queue: a min-heap whose elements are of the form
        # (priority, timestamp, job), where job is the actual data.
        self._queue = []

        # Reverse lookup for the jobs in the queue: a dictionary
        # associating the index in the queue to each job.
        self._reverse = {}

    def __contains__(self, job):
        """Implement the 'in' operator for a job in the queue.

        job (JobQueueEntry): a job to search.

        return (bool): True if job is in the queue.

        """
        return job in self._reverse

    def _swap(self, idx1, idx2):
        """Swap two elements in the queue, keeping their reverse
        indices up to date.

        idx1 (int): the index of the first element.
        idx2 (int): the index of the second element.

        """
        self._queue[idx1], self._queue[idx2] = \
            self._queue[idx2], self._queue[idx1]
        self._reverse[self._queue[idx1][2]] = idx1
        self._reverse[self._queue[idx2][2]] = idx2

    def _up_heap(self, idx):
        """Take the element in position idx up in the heap until its
        position is the right one.

        idx (int): the index of the element to lift.

        return (int): the new index of the element.

        """
        while idx > 0:
            parent = (idx - 1) // 2
            if self._queue[parent] > self._queue[idx]:
                self._swap(parent, idx)
                idx = parent
            else:
                break
        return idx

    def _down_heap(self, idx):
        """Take the element in position idx down in the heap until its
        position is the right one.

        idx (int): the index of the element to lower.

        return (int): the new index of the element.

        """
        last = len(self._queue) - 1
        while 2 * idx + 1 <= last:
            child = 2 * idx + 1
            if 2 * idx + 2 <= last and \
                    self._queue[2 * idx + 2] < self._queue[child]:
                child = 2 * idx + 2
            if self._queue[child] < self._queue[idx]:
                self._swap(child, idx)
                idx = child
            else:
                break
        return idx

    def _updown_heap(self, idx):
        """Perform both operations of up_heap and down_heap on an
        element.

        idx (int): the index of the element to lift.

        return (int): the new index of the element.

        """
        idx = self._up_heap(idx)
        return self._down_heap(idx)

    def push(self, job, priority, timestamp=None):
        """Push a job in the queue. If timestamp is not specified,
        uses the current time.

        job (JobQueueEntry): the job to add to the queue.
        priority (int): the priority of the job.
        timestamp (datetime): the time of the submission.

        """
        if timestamp is None:
            timestamp = make_datetime()
        self._queue.append((priority, timestamp, job))
        last = len(self._queue) - 1
        self._reverse[job] = last
        self._up_heap(last)

    def top(self):
        """Returns the first element in the queue without extracting
        it. If the queue is empty raises an exception.

        returns ((int, datetime, JobQueueEntry)): first element in the
            queue.

        raise (LookupError): on empty queue.

        """
        if len(self._queue) > 0:
            return self._queue[0]
        else:
            raise LookupError("Empty queue.")

    def pop(self):
        """Extracts (and returns) the first element in the queue.

        returns ((int, datetime, JobQueueEntry)): first element in the
            queue.

        raise (LookupError): on empty queue.

        """
        top = self.top()
        last = len(self._queue) - 1
        self._swap(0, last)

        del self._reverse[top[2]]
        del self._queue[last]
        if last > 0:
            self._down_heap(0)
        return top

    def remove(self, job):
        """Remove a job from the queue. Raise a KeyError if not present.

        job (JobQueueEntry): the job to remove.

        return (int, int, job): priority, timestamp, and job.

        raise (KeyError): if job not present.

        """
        pos = self._reverse[job]
        last = len(self._queue) - 1
        self._swap(pos, last)

        del self._reverse[job]
        del self._queue[last]
        if pos != last:
            self._updown_heap(pos)

    def set_priority(self, job, priority):
        """Change the priority of a job inside the queue. Raises an
        exception if the job is not in the queue.

        job (JobQueueEntry): the job whose priority needs to change.
        priority (int): the new priority.

        raise (LookupError): if job not present.

        """
        pos = self._reverse[job]
        self._queue[pos] = (priority,
                            self._queue[pos][1],
                            self._queue[pos][2])
        self._updown_heap(pos)

    def length(self):
        """Returns the number of elements in the queue.

        returns (int): length of the queue
        """
        return len(self._queue)

    def empty(self):
        """Returns if the queue is empty.

        returns (bool): is the queue empty?
        """
        return self.length() == 0

    def get_status(self):
        """Returns the content of the queue. Note that the order may
        be not correct, but the first element is the one at the top.

        returns (list): a list of dictionary containing the
                        representation of the job, the priority and
                        the timestamp.
        """
        ret = []
        for data in self._queue:
            ret.append({'job': data[2],
                        'priority': data[0],
                        'timestamp': make_timestamp(data[1])})
        return ret


class WorkerPool(object):
    """This class keeps the state of the workers attached to ES, and
    allow the ES to get a usable worker when it needs it.

    """

    WORKER_INACTIVE = None
    WORKER_DISABLED = "disabled"

    def __init__(self, service):
        """service (Service): the EvaluationService using this
        WorkerPool.

        """
        self._service = service
        self._worker = {}
        # These dictionary stores data about the workers (identified
        # by their shard number). Side data is anything one want to
        # attach to the worker. Schedule disabling to True means that
        # we are going to disable the worker as soon as possible (when
        # it finishes the current job). The current job is also
        # discarded because we already re-assigned it. Ignore is true
        # if the next result coming from the worker should be
        # discarded.
        self._job = {}
        self._start_time = {}
        self._side_data = {}
        self._schedule_disabling = {}
        self._ignore = {}

    def __contains__(self, job):
        for shard in self._worker:
            if job == self._job[shard] and not self._ignore[shard]:
                return True
        return False

    def add_worker(self, worker_coord):
        """Add a new worker to the worker pool.

        worker_coord (ServiceCoord): the coordinates of the worker.

        """
        shard = worker_coord.shard
        # Instruct GeventLibrary to connect ES to the Worker.
        self._worker[shard] = self._service.connect_to(
            worker_coord,
            on_connect=self.on_worker_connected)

        # And we fill all data.
        self._job[shard] = WorkerPool.WORKER_INACTIVE
        self._start_time[shard] = None
        self._side_data[shard] = None
        self._schedule_disabling[shard] = False
        self._ignore[shard] = False
        logger.debug("Worker %s added." % shard)

    def on_worker_connected(self, worker_coord):
        """To be called when a worker comes alive after being
        offline. We use this callback to instruct the worker to
        precache all files concerning the contest.

        worker_coord (ServiceCoord): the coordinates of the worker
                                     that came online.

        """
        shard = worker_coord.shard
        logger.info("Worker %s online again." % shard)
        self._worker[shard].precache_files(contest_id=self._service.contest_id)
        # We don't requeue the job, because a connection lost does not
        # invalidate a potential result given by the worker (as the
        # problem was the connection and not the machine on which the
        # worker is).

    def acquire_worker(self, job, side_data=None):
        """Tries to assign a job to an available worker. If no workers
        are available then this returns None, otherwise this returns
        the chosen worker.

        job (JobQueueEntry): the job to assign to a worker.
        side_data (object): object to attach to the worker for later
            use.

        returns (int): None if no workers are available, the worker
            assigned to the job otherwise.

        """
        # We look for an available worker
        try:
            shard = self.find_worker(WorkerPool.WORKER_INACTIVE,
                                     require_connection=True,
                                     random_worker=True)
        except LookupError:
            return None

        # Then we fill the info for future memory
        self._job[shard] = job
        self._start_time[shard] = make_datetime()
        self._side_data[shard] = side_data
        logger.debug("Worker %s acquired." % shard)

        # And finally we ask the worker to do the job
        job_type, object_id, dataset_id = job
        timestamp = side_data[1]
        queue_time = self._start_time[shard] - timestamp
        logger.info("Asking worker %s to %s submission/user test %d(%d) "
                    " (%s after submission)." %
                    (shard, job_type, object_id, dataset_id, queue_time))

        with SessionGen() as session:
            if job_type == EvaluationService.JOB_TYPE_COMPILATION:
                submission = Submission.get_from_id(object_id, session)
                dataset = Dataset.get_from_id(dataset_id, session)
                job_group = \
                    JobGroup.from_submission_compilation(submission, dataset)
            elif job_type == EvaluationService.JOB_TYPE_EVALUATION:
                submission = Submission.get_from_id(object_id, session)
                dataset = Dataset.get_from_id(dataset_id, session)
                job_group = \
                    JobGroup.from_submission_evaluation(submission, dataset)
            elif job_type == EvaluationService.JOB_TYPE_TEST_COMPILATION:
                user_test = UserTest.get_from_id(object_id, session)
                dataset = Dataset.get_from_id(dataset_id, session)
                job_group = \
                    JobGroup.from_user_test_compilation(user_test, dataset)
            elif job_type == EvaluationService.JOB_TYPE_TEST_EVALUATION:
                user_test = UserTest.get_from_id(object_id, session)
                dataset = Dataset.get_from_id(dataset_id, session)
                job_group = \
                    JobGroup.from_user_test_evaluation(user_test, dataset)

            self._worker[shard].execute_job_group(
                job_group_dict=job_group.export_to_dict(),
                callback=self._service.action_finished,
                plus=(job_type, object_id, dataset_id, side_data, shard))
        return shard

    def release_worker(self, shard):
        """To be called by ES when it receives a notification that a
        job finished.

        Note: if the worker is scheduled to be disabled, then we
        disable it, and notify the ES to discard the outcome obtained
        by the worker.

        shard (int): the worker to release.

        returns (bool): if the result is to be ignored.

        """
        if self._job[shard] == WorkerPool.WORKER_INACTIVE:
            err_msg = "Trying to release worker while it's inactive."
            logger.error(err_msg)
            raise ValueError(err_msg)
        ret = self._ignore[shard]
        self._start_time[shard] = None
        self._side_data[shard] = None
        self._ignore[shard] = False
        if self._schedule_disabling[shard]:
            self._job[shard] = WorkerPool.WORKER_DISABLED
            self._schedule_disabling[shard] = False
            logger.info("Worker %s released and disabled." % shard)
        else:
            self._job[shard] = WorkerPool.WORKER_INACTIVE
            logger.debug("Worker %s released." % shard)
        return ret

    def find_worker(self, job, require_connection=False, random_worker=False):
        """Return a worker whose assigned job is job. Remember that
        there is a placeholder job to signal that the worker is not
        doing anything (or disabled).

        job (JobQueueEntry): the job we are looking for, or
            WorkerPool.WORKER_*.
        require_connection (bool): True if we want to find a worker
            doing the job and that is actually connected to us (i.e.,
            did not die).
        random_worker (bool): if True, choose uniformly amongst all
            workers doing the job.

        returns (int): the shard of the worker working on job.

        raise (LookupError): if nothing has been found.

        """
        pool = []
        for shard, worker_job in self._job.iteritems():
            if worker_job == job:
                if not require_connection or self._worker[shard].connected:
                    pool.append(shard)
                    if not random_worker:
                        return shard
        if pool == []:
            raise LookupError("No such job.")
        else:
            return random.choice(pool)

    def ignore_job(self, job):
        """Mark the job to be ignored, and try to inform the worker.

        job (JobQueueEntry): the job to ignore.

        raise (LookupError): if job is not found.

        """
        shard = self.find_worker(job)
        self._ignore[shard] = True
        self._worker[shard].ignore_job()

    def get_status(self):
        """Returns a dict with info about the current status of all
        workers.

        return (dict): dict of info: current job, starting time,
                       number of errors, and additional data specified
                       in the job.

        """
        result = dict()
        for shard in self._worker.keys():
            s_time = self._start_time[shard]
            s_time = make_timestamp(s_time) if s_time is not None else None
            s_data = self._side_data[shard]
            s_data = (s_data[0], make_timestamp(s_data[1])) \
                if s_data is not None else None

            result[str(shard)] = {
                'connected': self._worker[shard].connected,
                'job': self._job[shard],
                'start_time': s_time,
                'side_data': s_data}
        return result

    def check_timeouts(self):
        """Check if some worker is not responding in too much time. If
        this is the case, the worker is scheduled for disabling, and
        we send him a message trying to shut it down.

        return (list): list of tuples (priority, timestamp, job) of
                       jobs assigned to worker that timeout.

        """
        now = make_datetime()
        lost_jobs = []
        for shard in self._worker:
            if self._start_time[shard] is not None:
                active_for = now - self._start_time[shard]

                if active_for > EvaluationService.WORKER_TIMEOUT:
                    # Here shard is a working worker with no sign of
                    # intelligent life for too much time.
                    logger.error("Disabling and shutting down "
                                 "worker %d because of no response "
                                 "in %s." %
                                 (shard, active_for))
                    assert self._job[shard] != WorkerPool.WORKER_INACTIVE \
                        and self._job[shard] != WorkerPool.WORKER_DISABLED

                    # We return the job so ES can do what it needs.
                    if not self._ignore[shard]:
                        job = self._job[shard]
                        priority, timestamp = self._side_data[shard]
                        lost_jobs.append((priority, timestamp, job))

                    # Also, we are not trusting it, so we are not
                    # assigning him new jobs even if it comes back to
                    # life.
                    self._schedule_disabling[shard] = True
                    self._ignore[shard] = True
                    self.release_worker(shard)
                    self._worker[shard].quit("No response in %s." % active_for)

        return lost_jobs

    def check_connections(self):
        """Check if a worker we assigned a job to disconnects. In this
        case, requeue the job.

        return (list): list of tuples (priority, timestamp, job) of
                       jobs assigned to worker that disconnected.

        """
        lost_jobs = []
        for shard in self._worker:
            if not self._worker[shard].connected and \
                    self._job[shard] not in [WorkerPool.WORKER_DISABLED,
                                             WorkerPool.WORKER_INACTIVE]:
                if not self._ignore[shard]:
                    job = self._job[shard]
                    priority, timestamp = self._side_data[shard]
                    lost_jobs.append((priority, timestamp, job))
                self.release_worker(shard)

        return lost_jobs


class EvaluationService(Service):
    """Evaluation service.

    """

    JOB_PRIORITY_EXTRA_HIGH = 0
    JOB_PRIORITY_HIGH = 1
    JOB_PRIORITY_MEDIUM = 2
    JOB_PRIORITY_LOW = 3
    JOB_PRIORITY_EXTRA_LOW = 4

    JOB_TYPE_COMPILATION = "compile"
    JOB_TYPE_EVALUATION = "evaluate"
    JOB_TYPE_TEST_COMPILATION = "compile_test"
    JOB_TYPE_TEST_EVALUATION = "evaluate_test"

    MAX_COMPILATION_TRIES = 3
    MAX_EVALUATION_TRIES = 3
    MAX_TEST_COMPILATION_TRIES = 3
    MAX_TEST_EVALUATION_TRIES = 3

    INVALIDATE_COMPILATION = 0
    INVALIDATE_EVALUATION = 1

    # Seconds after which we declare a worker stale.
    WORKER_TIMEOUT = timedelta(seconds=600)
    # How often we check for stale workers.
    WORKER_TIMEOUT_CHECK_TIME = timedelta(seconds=300)

    # How often we check if a worker is connected.
    WORKER_CONNECTION_CHECK_TIME = timedelta(seconds=10)

    # How often we check if we can assign a job to a worker.
    CHECK_DISPATCH_TIME = timedelta(seconds=2)

    # How often we look for submission not compiled/evaluated.
    JOBS_NOT_DONE_CHECK_TIME = timedelta(seconds=117)

    def __init__(self, shard, contest_id):
        Service.__init__(self, shard)

        self.contest_id = contest_id

        self.queue = JobQueue()
        self.pool = WorkerPool(self)
        self.scoring_service = self.connect_to(
            ServiceCoord("ScoringService", 0))

        for i in xrange(get_service_shards("Worker")):
            worker = ServiceCoord("Worker", i)
            self.pool.add_worker(worker)

        self.add_timeout(self.dispatch_jobs, None,
                         EvaluationService.CHECK_DISPATCH_TIME
                         .total_seconds(),
                         immediately=True)
        self.add_timeout(self.check_workers_timeout, None,
                         EvaluationService.WORKER_TIMEOUT_CHECK_TIME
                         .total_seconds(),
                         immediately=False)
        self.add_timeout(self.check_workers_connection, None,
                         EvaluationService.WORKER_CONNECTION_CHECK_TIME
                         .total_seconds(),
                         immediately=False)
        self.add_timeout(self.search_jobs_not_done, None,
                         EvaluationService.JOBS_NOT_DONE_CHECK_TIME
                         .total_seconds(),
                         immediately=True)

    @rpc_method
    def search_jobs_not_done(self):
        """Look in the database for submissions that have not been
        compiled or evaluated for no good reasons. Put the missing job
        in the queue.

        """
        new_jobs = 0
        with SessionGen() as session:
            contest = session.query(Contest).\
                filter_by(id=self.contest_id).first()

            # Only adding submission not compiled/evaluated that have
            # not yet reached the limit of tries.
            for submission in contest.get_submissions():
                for dataset in get_datasets_to_judge(submission.task):
                    submission_result = \
                        submission.get_result_or_create(dataset)
                    if to_compile(submission_result):
                        if self.push_in_queue(
                                JobQueueEntry(
                                    EvaluationService.JOB_TYPE_COMPILATION,
                                    submission.id,
                                    dataset.id),
                                EvaluationService.JOB_PRIORITY_HIGH,
                                submission.timestamp):
                            new_jobs += 1
                    elif to_evaluate(submission_result):
                        if self.push_in_queue(
                                JobQueueEntry(
                                    EvaluationService.JOB_TYPE_EVALUATION,
                                    submission.id,
                                    dataset.id),
                                EvaluationService.JOB_PRIORITY_MEDIUM,
                                submission.timestamp):
                            new_jobs += 1

            # The same for user tests
            for user_test in contest.get_user_tests():
                for dataset in get_datasets_to_judge(user_test.task):
                    user_test_result = \
                        user_test.get_result_or_create(dataset)
                    if user_test_to_compile(user_test_result):
                        if self.push_in_queue(
                                JobQueueEntry(
                                    EvaluationService.
                                    JOB_TYPE_TEST_COMPILATION,
                                    user_test.id,
                                    dataset.id),
                                EvaluationService.JOB_PRIORITY_HIGH,
                                user_test.timestamp):
                            new_jobs += 1
                    elif user_test_to_evaluate(user_test_result):
                        if self.push_in_queue(
                                JobQueueEntry(
                                    EvaluationService.JOB_TYPE_TEST_EVALUATION,
                                    user_test.id,
                                    dataset.id),
                                EvaluationService.JOB_PRIORITY_MEDIUM,
                                user_test.timestamp):
                            new_jobs += 1

            session.commit()

        if new_jobs > 0:
            logger.info("Found %s submissions or user tests with "
                        "jobs to do." % new_jobs)

        # Run forever.
        return True

    def dispatch_jobs(self):
        """Check if there are pending jobs, and tries to distribute as
        many of them to the available workers.

        """
        pending = self.queue.length()
        if pending > 0:
            logger.info("%s jobs still pending." % pending)
        while self.dispatch_one_job():
            pass

        # We want this to run forever.
        return True

    def dispatch_one_job(self):
        """Try to dispatch exactly one job, if it exists, to one
        available worker, if it exists.

        return (bool): True if successfully dispatched, False if some
                       resource was missing.

        """
        try:
            priority, timestamp, job = self.queue.top()
        except LookupError:
            return False

        res = self.pool.acquire_worker(job, side_data=(priority, timestamp))
        if res is not None:
            self.queue.pop()
            return True
        else:
            return False

    @rpc_method
    def submissions_status(self):
        """Returns a dictionary of statistics about the number of
        submissions on a specific status. There are seven statuses:
        evaluated, compilation failed, evaluating, compiling, maximum
        number of attempts of compilations reached, the same for
        evaluations, and finally 'I have no idea what's
        happening'. The last three should not happen and require a
        check from the admin.

        The status of a submission is checked on its result for the
        active dataset of its task.

        return (dict): statistics on the submissions.

        """
        stats = {
            "scored": 0,
            "evaluated": 0,
            "compilation_fail": 0,
            "compiling": 0,
            "evaluating": 0,
            "max_compilations": 0,
            "max_evaluations": 0,
            "invalid": 0}
        with SessionGen() as session:
            contest = Contest.get_from_id(self.contest_id, session)
            for submission_result in contest.get_submission_results():
                if submission_result.compilation_failed():
                    stats["compilation_fail"] += 1
                elif not submission_result.compiled():
                    if submission_result.compilation_tries >= \
                            EvaluationService.MAX_COMPILATION_TRIES:
                        stats["max_compilations"] += 1
                    else:
                        stats["compiling"] += 1
                elif submission_result.compilation_succeeded():
                    if submission_result.evaluated():
                        if submission_result.scored():
                            stats["scored"] += 1
                        else:
                            stats["evaluated"] += 1
                    else:
                        if submission_result.evaluation_tries >= \
                                EvaluationService.MAX_EVALUATION_TRIES:
                            stats["max_evaluations"] += 1
                        else:
                            stats["evaluating"] += 1
                else:
                    # Should not happen.
                    stats["invalid"] += 1
        return stats

    @rpc_method
    def queue_status(self):
        """Returns a list whose elements are the jobs currently in the
        queue (see Queue.get_status).

        returns (list): the list with the queued elements.

        """
        return self.queue.get_status()

    @rpc_method
    def workers_status(self):
        """Returns a dictionary (indexed by shard number) whose values
        are the information about the corresponding worker. See
        WorkerPool.get_status for more details.

        returns (dict): the dict with the workers information.

        """
        return self.pool.get_status()

    def check_workers_timeout(self):
        """We ask WorkerPool for the unresponsive workers, and we put
        again their jobs in the queue.

        """
        lost_jobs = self.pool.check_timeouts()
        for priority, timestamp, job in lost_jobs:
            logger.info("Job %r put again in the queue because of "
                        "worker timeout." % (job,))
            self.push_in_queue(job, priority, timestamp)
        return True

    def check_workers_connection(self):
        """We ask WorkerPool for the unconnected workers, and we put
        again their jobs in the queue.

        """
        lost_jobs = self.pool.check_connections()
        for priority, timestamp, job in lost_jobs:
            logger.info("Job %r put again in the queue because of "
                        "disconnected worker." % (job,))
            self.push_in_queue(job, priority, timestamp)
        return True

    def submission_busy(self, submission_id, dataset_id):
        """Check if the submission has a related job in the queue or
        assigned to a worker.

        """
        jobs = [
            JobQueueEntry(
                EvaluationService.JOB_TYPE_COMPILATION,
                submission_id,
                dataset_id),
            JobQueueEntry(
                EvaluationService.JOB_TYPE_EVALUATION,
                submission_id,
                dataset_id),
        ]
        return any([job in self.queue or job in self.pool for job in jobs])

    def user_test_busy(self, user_test_id, dataset_id):
        """Check if the user test has a related job in the queue or
        assigned to a worker.

        """
        jobs = [
            JobQueueEntry(
                EvaluationService.JOB_TYPE_TEST_COMPILATION,
                user_test_id,
                dataset_id),
            JobQueueEntry(
                EvaluationService.JOB_TYPE_TEST_EVALUATION,
                user_test_id,
                dataset_id),
        ]
        return any([job in self.queue or job in self.pool for job in jobs])

    def job_busy(self, job):
        """Check the entity (submission or user test) related to a job
        has other related jobs in the queue or assigned to a worker.

        """
        job_type, object_id, dataset_id = job

        if job_type in (EvaluationService.JOB_TYPE_COMPILATION,
                        EvaluationService.JOB_TYPE_EVALUATION):
            return self.submission_busy(object_id, dataset_id)
        elif job_type in (EvaluationService.JOB_TYPE_TEST_COMPILATION,
                          EvaluationService.JOB_TYPE_TEST_EVALUATION):
            return self.user_test_busy(object_id, dataset_id)
        else:
            raise Exception("Wrong job type %s" % job_type)

    def push_in_queue(self, job, priority, timestamp):
        """Push a job in the job queue if the submission is not
        already in the queue or assigned to a worker.

        job (JobQueueEntry): the job to put in the queue.

        return (bool): True if pushed, False if not.

        """
        if self.job_busy(job):
            return False
        else:
            self.queue.push(job, priority, timestamp)
            return True

    def action_finished(self, data, plus, error=None):
        """Callback from a worker, to signal that is finished some
        action (compilation or evaluation).

        data (dict): a dictionary that describes a JobGroup instance.
        plus (tuple): the tuple (job_type,
                                 object_id,
                                 dataset_id,
                                 side_data=(priority, timestamp),
                                 shard_of_worker)

        """
        # Unpack the plus tuple. It's built in the RPC call to Worker's
        # execute_job_group method inside WorkerPool.acquire_worker.
        job_type, object_id, dataset_id, side_data, shard = plus

        # We notify the pool that the worker is available again for
        # further work (no matter how the current request turned out,
        # even if the worker encountered an error). If the pool informs
        # us that the data produced by the worker has to be ignored (by
        # returning True) we interrupt the execution of this method and
        # do nothing because in that case we know the job has returned
        # to the queue and perhaps already been reassigned to another
        # worker.
        if self.pool.release_worker(shard):
            return

        job_success = True
        if error is not None:
            logger.error("Received error from Worker: `%s'." % error)
            job_success = False

        else:
            try:
                job_group = JobGroup.import_from_dict(data)
            except:
                logger.error("[action_finished] Couldn't build JobGroup for "
                             "data %s." % data, exc_info=True)
                job_success = False

            else:
                if not job_group.success:
                    logger.error("Worker %s signaled action "
                                 "not successful." % shard)
                    job_success = False

        _, timestamp = side_data

        logger.info("Action %s for submission %s completed. Success: %s." %
                    (job_type, object_id, job_success))

        # We get the submission from DB and update it.
        with SessionGen() as session:
            if job_type == EvaluationService.JOB_TYPE_COMPILATION:
                submission_result = SubmissionResult.get_from_id(
                    (object_id, dataset_id), session)
                if submission_result is None:
                    logger.error("[action_finished] Couldn't find "
                                 "submission %d(%d) in the database." %
                                 (object_id, dataset_id))
                    return

                submission_result.compilation_tries += 1

                if job_success:
                    job_group.to_submission_compilation(submission_result)

                self.compilation_ended(submission_result)

            elif job_type == EvaluationService.JOB_TYPE_EVALUATION:
                submission_result = SubmissionResult.get_from_id(
                    (object_id, dataset_id), session)
                if submission_result is None:
                    logger.error("[action_finished] Couldn't find "
                                 "submission %d(%d) in the database." %
                                 (object_id, dataset_id))
                    return

                submission_result.evaluation_tries += 1

                if job_success:
                    job_group.to_submission_evaluation(submission_result)

                self.evaluation_ended(submission_result)

            elif job_type == EvaluationService.JOB_TYPE_TEST_COMPILATION:
                user_test_result = UserTestResult.get_from_id(
                    (object_id, dataset_id), session)
                if user_test_result is None:
                    logger.error("[action_finished] Couldn't find "
                                 "user test %d(%d) in the database." %
                                 (object_id, dataset_id))
                    return

                user_test_result.compilation_tries += 1

                if job_success:
                    job_group.to_user_test_compilation(user_test_result)

                self.user_test_compilation_ended(user_test_result)

            elif job_type == EvaluationService.JOB_TYPE_TEST_EVALUATION:
                user_test_result = UserTestResult.get_from_id(
                    (object_id, dataset_id), session)
                if user_test_result is None:
                    logger.error("[action_finished] Couldn't find "
                                 "user test %d(%d) in the database." %
                                 (object_id, dataset_id))
                    return

                user_test_result.evaluation_tries += 1

                if job_success:
                    job_group.to_user_test_evaluation(user_test_result)

                self.user_test_evaluation_ended(user_test_result)

            else:
                logger.error("Invalid job type %r." % job_type)
                return

            session.commit()

    def compilation_ended(self, submission_result):
        """Actions to be performed when we have a submission that has
        ended compilation . In particular: we queue evaluation if
        compilation was ok, we inform ScoringService if the
        compilation failed for an error in the submission, or we
        requeue the compilation if there was an error in CMS.

        submission_result (SubmissionResult): the submission result.

        """
        submission = submission_result.submission
        # Compilation was ok, so we evaluate.
        if submission_result.compilation_succeeded():
            self.push_in_queue(
                JobQueueEntry(
                    EvaluationService.JOB_TYPE_EVALUATION,
                    submission_result.submission_id,
                    submission_result.dataset_id),
                EvaluationService.JOB_PRIORITY_MEDIUM,
                submission.timestamp)
        # If instead submission failed compilation, we don't evaluate,
        # but we inform ScoringService of the new submission. We need
        # to commit before so it has up to date information.
        elif submission_result.compilation_failed():
            logger.info("Submission %d(%d) did not compile. Not going to "
                        "evaluate." %
                        (submission_result.submission_id,
                         submission_result.dataset_id))
            submission_result.sa_session.commit()
            self.scoring_service.new_evaluation(
                submission_id=submission_result.submission_id,
                dataset_id=submission_result.dataset_id)
        # If compilation failed for our fault, we requeue or not.
        elif submission_result.compilation_outcome is None:
            if submission_result.compilation_tries > \
                    EvaluationService.MAX_COMPILATION_TRIES:
                logger.error("Maximum tries reached for the compilation of "
                             "submission %d(%d). I will not try again." %
                             (submission_result.submission_id,
                              submission_result.dataset_id))
            else:
                # Note: lower priority (MEDIUM instead of HIGH) for
                # compilations that are probably failing again.
                self.push_in_queue(
                    JobQueueEntry(
                        EvaluationService.JOB_TYPE_COMPILATION,
                        submission_result.submission_id,
                        submission_result.dataset_id),
                    EvaluationService.JOB_PRIORITY_MEDIUM,
                    submission.timestamp)
        # Otherwise, error.
        else:
            logger.error("Compilation outcome %r not recognized." %
                         submission_result.compilation_outcome)

    def evaluation_ended(self, submission_result):
        """Actions to be performed when we have a submission that has
        been evaluated. In particular: we inform ScoringService on
        success, we requeue on failure.

        submission_result (SubmissionResult): the submission result.

        """
        submission = submission_result.submission
        # Evaluation successful, we inform ScoringService so it can
        # update the score. We need to commit the session beforehand,
        # otherwise the ScoringService wouldn't receive the updated
        # submission.
        if submission_result.evaluated():
            submission_result.sa_session.commit()
            self.scoring_service.new_evaluation(
                submission_id=submission_result.submission_id,
                dataset_id=submission_result.dataset_id)
        # Evaluation unsuccessful, we requeue (or not).
        elif submission_result.evaluation_tries > \
                EvaluationService.MAX_EVALUATION_TRIES:
            logger.error("Maximum tries reached for the evaluation of "
                         "submission %d(%d). I will not try again." %
                         (submission_result.submission_id,
                          submission_result.dataset_id))
        else:
            # Note: lower priority (LOW instead of MEDIUM) for
            # evaluations that are probably failing again.
            self.push_in_queue(
                JobQueueEntry(
                    EvaluationService.JOB_TYPE_EVALUATION,
                    submission_result.submission_id,
                    submission_result.dataset_id),
                EvaluationService.JOB_PRIORITY_LOW,
                submission.timestamp)

    def user_test_compilation_ended(self, user_test_result):
        """Actions to be performed when we have a user test that has
        ended compilation. In particular: we queue evaluation if
        compilation was ok; we requeue compilation if it failed.

        user_test_result (UserTestResult): the user test result.

        """
        user_test = user_test_result.user_test
        # Compilation was ok, so we evaluate
        if user_test_result.compilation_succeeded():
            self.push_in_queue(
                JobQueueEntry(
                    EvaluationService.JOB_TYPE_TEST_EVALUATION,
                    user_test_result.user_test_id,
                    user_test_result.dataset_id),
                EvaluationService.JOB_PRIORITY_MEDIUM,
                user_test.timestamp)
        # If instead user test failed compilation, we don't evaluatate
        elif user_test_result.compilation_failed():
            logger.info("User test %d(%d) did not compile. Not going to "
                        "evaluate." %
                        (user_test_result.user_test_id,
                         user_test_result.dataset_id))
        # If compilation failed for our fault, we requeue or not
        elif not user_test_result.compiled():
            if user_test_result.compilation_tries > \
                    EvaluationService.MAX_TEST_COMPILATION_TRIES:
                logger.error("Maximum tries reached for the compilation of "
                             "user test %d(%d). I will not try again." %
                             (user_test_result.user_test_id,
                              user_test_result.dataset_id))
            else:
                # Note: lower priority (MEDIUM instead of HIGH) for
                # compilations that are probably failing again
                self.push_in_queue(
                    JobQueueEntry(
                        EvaluationService.JOB_TYPE_TEST_COMPILATION,
                        user_test_result.user_test_id,
                        user_test_result.dataset_id),
                    EvaluationService.JOB_PRIORITY_MEDIUM,
                    user_test.timestamp)
        # Otherwise, error.
        else:
            logger.error("Compilation outcome %r not recognized." %
                         user_test_result.compilation_outcome)

    def user_test_evaluation_ended(self, user_test_result):
        """Actions to be performed when we have a user test that has
        been evaluated. In particular: we do nothing on success, we
        requeue on failure.

        user_test_result (UserTestResult): the user test result.

        """
        user_test = user_test_result.user_test
        if not user_test_result.evaluated():
            if user_test_result.evaluation_tries > \
                    EvaluationService.MAX_TEST_EVALUATION_TRIES:
                logger.error("Maximum tries reached for the evaluation of "
                             "user test %d(%d). I will no try again." %
                             (user_test_result.user_test_id,
                              user_test_result.dataset_id))
            else:
                # Note: lower priority (LOW instead of MEDIUM) for
                # evaluations that are probably failing again.
                self.push_in_queue(
                    JobQueueEntry(
                        EvaluationService.JOB_TYPE_TEST_EVALUATION,
                        user_test_result.user_test_id,
                        user_test_result.dataset_id),
                    EvaluationService.JOB_PRIORITY_LOW,
                    user_test.timestamp)

    @rpc_method
    def new_submission(self, submission_id):
        """This RPC prompts ES of the existence of a new
        submission. ES takes the right countermeasures, i.e., it
        schedules it for compilation.

        submission_id (int): the id of the new submission.

        returns (bool): True if everything went well.

        """
        with SessionGen() as session:
            submission = Submission.get_from_id(submission_id, session)
            if submission is None:
                logger.error("[new_submission] Couldn't find submission "
                             "%d in the database." % submission_id)
                return

            for dataset in get_datasets_to_judge(submission.task):
                submission_result = submission.get_result_or_create(dataset)

                if to_compile(submission_result):
                    self.push_in_queue(
                        JobQueueEntry(
                            EvaluationService.JOB_TYPE_COMPILATION,
                            submission.id,
                            dataset.id),
                        EvaluationService.JOB_PRIORITY_HIGH,
                        submission.timestamp)

            session.commit()

    @rpc_method
    def new_user_test(self, user_test_id):
        """This RPC prompts ES of the existence of a new user test. ES
        takes takes the right countermeasures, i.e., it schedules it
        for compilation.

        user_test_id (int): the id of the new user test.

        returns (bool): True if everything went well.

        """
        with SessionGen() as session:
            user_test = UserTest.get_from_id(user_test_id, session)
            if user_test is None:
                logger.error("[new_user_test] Couldn't find user test %d "
                             "in the database." % user_test_id)
                return

            for dataset in get_datasets_to_judge(user_test.task):
                user_test_result = user_test.get_result_or_create(dataset)

                if user_test_to_compile(user_test_result):
                    self.push_in_queue(
                        JobQueueEntry(
                            EvaluationService.JOB_TYPE_TEST_COMPILATION,
                            user_test.id,
                            dataset.id),
                        EvaluationService.JOB_PRIORITY_HIGH,
                        user_test.timestamp)

            session.commit()

    @rpc_method
    def invalidate_submission(self,
                              submission_id=None,
                              dataset_id=None,
                              user_id=None,
                              task_id=None,
                              level="compilation"):
        """Request to invalidate some computed data.

        Invalidate the compilation and/or evaluation data of the
        SubmissionResults that:
        - belong to submission_id or, if None, to any submission of
          user_id and/or task_id or, if both None, to any submission
          of the contest this service is running for.
        - belong to dataset_id or, if None, to any dataset of task_id
          or, if None, to any dataset of any task of the contest this
          service is running for.

        The data is cleared, the jobs involving the submissions
        currently enqueued are deleted, and the ones already assigned
        to the workers are ignored. New appropriate jobs are enqueued.

        submission_id (int): id of the submission to invalidate, or
                             None.
        dataset_id (int): id of the dataset to invalidate, or None.
        user_id (int): id of the user to invalidate, or None.
        task_id (int): id of the task to invalidate, or None.
        level (string): 'compilation' or 'evaluation'

        """
        logger.info("Invalidation request received.")

        # Validate arguments
        # TODO Check that all these objects belong to this contest.
        if level not in ("compilation", "evaluation"):
            raise ValueError(
                "Unexpected invalidation level `%s'." % level)

        with SessionGen() as session:
            submission_results = get_submission_results(
                # Give contest_id only if all others are None.
                self.contest_id
                if {user_id, task_id, submission_id, dataset_id} == {None}
                else None,
                user_id, task_id, submission_id, dataset_id, session)

            logger.info("Submission results to invalidate %s for: %d." %
                        (level, len(submission_results)))
            if len(submission_results) == 0:
                return

            for submission_result in submission_results:
                jobs = [
                    JobQueueEntry(
                        EvaluationService.JOB_TYPE_COMPILATION,
                        submission_result.submission_id,
                        submission_result.dataset_id),
                    JobQueueEntry(
                        EvaluationService.JOB_TYPE_EVALUATION,
                        submission_result.submission_id,
                        submission_result.dataset_id),
                    ]
                for job in jobs:
                    try:
                        self.queue.remove(job)
                    except KeyError:
                        pass  # Ok, the job wasn't in the queue.
                    try:
                        self.pool.ignore_job(job)
                    except LookupError:
                        pass  # Ok, the job wasn't in the pool.

                # We invalidate the appropriate data and queue the jobs to
                # recompute those data.
                if level == "compilation":
                    submission_result.invalidate_compilation()
                    if to_compile(submission_result):
                        self.push_in_queue(
                            JobQueueEntry(
                                EvaluationService.JOB_TYPE_COMPILATION,
                                submission_result.submission_id,
                                submission_result.dataset_id),
                            EvaluationService.JOB_PRIORITY_HIGH,
                            submission_result.submission.timestamp)
                elif level == "evaluation":
                    submission_result.invalidate_evaluation()
                    if to_evaluate(submission_result):
                        self.push_in_queue(
                            JobQueueEntry(
                                EvaluationService.JOB_TYPE_EVALUATION,
                                submission_result.submission_id,
                                submission_result.dataset_id),
                            EvaluationService.JOB_PRIORITY_MEDIUM,
                            submission_result.submission.timestamp)

            session.commit()

########NEW FILE########
__FILENAME__ = LogService
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Logger service.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import os
import time
import logging
from collections import deque

from cms import config, mkdir
from cms.log import root_logger, shell_handler, FileHandler, CustomFormatter
from cms.io import Service, rpc_method


logger = logging.getLogger(__name__)


class LogService(Service):
    """Logger service.

    """

    LAST_MESSAGES_COUNT = 100

    def __init__(self, shard):
        Service.__init__(self, shard)

        # Determine location of log file, and make directories.
        log_dir = os.path.join(config.log_dir, "cms")
        if not mkdir(config.log_dir) or \
                not mkdir(log_dir):
            logger.error("Cannot create necessary directories.")
            self.exit()
            return
        log_filename = "%d.log" % int(time.time())

        # Install a global file handler.
        self.file_handler = FileHandler(os.path.join(log_dir, log_filename),
                                        mode='w', encoding='utf-8')
        self.file_handler.setLevel(logging.DEBUG)
        self.file_handler.setFormatter(CustomFormatter(False))
        root_logger.addHandler(self.file_handler)

        # Provide a symlink to the latest log file.
        try:
            os.remove(os.path.join(log_dir, "last.log"))
        except OSError:
            pass
        os.symlink(log_filename,
                   os.path.join(log_dir, "last.log"))

        self._last_messages = deque(maxlen=self.LAST_MESSAGES_COUNT)

    @rpc_method
    def Log(self, **kwargs):
        """Log a message.

        Receive the attributes of a LogRecord, rebuild and handle it.
        The given keyword arguments will contain:
        msg (string): the message to log.
        levelname (string): the name of the level, one of "DEBUG",
            "INFO", "WARNING", "ERROR" and "CRITICAL".
        levelno (int): a numeric value denoting the level, one of the
            constants defined by the logging module. In fact, `levelno
            == getattr(logging, levelname)` (yes, it is redundant).
        created (float): when the log message was emitted (as an UNIX
            timestamp, seconds from epoch).

        And they may contain:
        service_name (string) and service_shard (int): the coords of
            the service where this message was generated.
        operation (string): a high-level description of the long-term
            operation that is going on in the service.
        exc_text (string): the text of the logged exception.

        """
        record = logging.makeLogRecord(kwargs)

        # Show in stdout, together with the messages we produce
        # ourselves.
        shell_handler.handle(record)
        # Write on the global log file.
        self.file_handler.handle(record)

        if record.levelno >= logging.WARNING:
            if hasattr(record, "service_name") and \
                    hasattr(record, "service_shard"):
                coord = "%s,%s" % (record.service_name, record.service_shard)
            else:
                coord = ""
            self._last_messages.append({
                "message": record.msg,
                "coord": coord,
                "operation": getattr(record, "operation", ""),
                "severity": record.levelname,
                "timestamp": record.created,
                "exc_text": getattr(record, "exc_text", None)})

    @rpc_method
    def last_messages(self):
        return list(self._last_messages)

########NEW FILE########
__FILENAME__ = ProxyService
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2014 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Bernard Blackham <bernard@largestprime.net>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""The service that forwards data to RankingWebServer.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import json
import logging
import string

import gevent
import gevent.queue

import requests
import requests.exceptions
from urlparse import urljoin, urlsplit

from cms import config
from cms.io import Service, rpc_method
from cms.db import SessionGen, Contest, Task, Submission
from cms.grading.scoretypes import get_score_type
from cmscommon.datetime import make_timestamp


logger = logging.getLogger(__name__)


class CannotSendError(Exception):
    pass


def encode_id(entity_id):
    """Encode the id using only A-Za-z0-9_.

    entity_id (unicode): the entity id to encode.
    return (unicode): encoded entity id.

    """
    encoded_id = ""
    for char in entity_id.encode('utf-8'):
        if char not in string.ascii_letters + string.digits:
            encoded_id += "_%x" % ord(char)
        else:
            encoded_id += unicode(char)
    return encoded_id


def safe_put_data(ranking, resource, data, operation):
    """Send some data to ranking using a PUT request.

    ranking (bytes): the URL of ranking server.
    resource (bytes): the relative path of the entity.
    data (dict): the data to JSON-encode and send.
    operation (unicode): a human-readable description of the operation
        we're performing (to produce log messages).

    raise (CannotSendError): in case of communication errors.

    """
    try:
        url = urljoin(ranking, resource)
        # XXX With requests-1.2 auth is automatically extracted from
        # the URL: there is no need for this.
        auth = urlsplit(url)
        res = requests.put(url, json.dumps(data, encoding="utf-8"),
                           auth=(auth.username, auth.password),
                           headers={'content-type': 'application/json'},
                           verify=config.https_certfile)
    except requests.exceptions.RequestException as error:
        msg = "%s while %s: %s." % (type(error).__name__, operation, error)
        logger.warning(msg)
        raise CannotSendError(msg)
    if 400 <= res.status_code < 600:
        msg = "Status %s while %s." % (res.status_code, operation)
        logger.warning(msg)
        raise CannotSendError(msg)


class RankingProxy(object):

    """A thread that sends data to one ranking.

    The object is used as a thread-local storage and its run method is
    the function that, started as a greenlet, uses it.

    It maintains a queue of data to send. At each "round" the queue is
    emptied (i.e. all jobs are fetched) and the data is then "combined"
    to minimize the number of actual HTTP requests: they'll be at most
    one per entity type.

    Each entity type is identified by a integral class-level constant.

    """

    # We use a single queue for all the data we have to send to the
    # ranking so we need to distingush the type of each item.
    CONTEST_TYPE = 0
    TASK_TYPE = 1
    TEAM_TYPE = 2
    USER_TYPE = 3
    SUBMISSION_TYPE = 4
    SUBCHANGE_TYPE = 5

    # The resource paths for the different entity types, relative to
    # the self.ranking URL.
    RESOURCE_PATHS = [
        b"contests",
        b"tasks",
        b"teams",
        b"users",
        b"submissions",
        b"subchanges"]

    # How many different entity types we know about.
    TYPE_COUNT = len(RESOURCE_PATHS)

    # How long we wait after having failed to push data to a ranking
    # before trying again.
    FAILURE_WAIT = 60.0

    def __init__(self, ranking):
        """Create a proxy for the ranking at the given URL.

        ranking (bytes): a complete URL (containing protocol, username,
            password, hostname, port and prefix) where a ranking is
            supposed to listen.

        """
        self.ranking = ranking
        self.data_queue = gevent.queue.Queue()

    def run(self):
        """Consume (i.e. send) the data put in the queue, forever.

        Pick all operations found in the queue (if there aren't any,
        block waiting until there are), combine them and send HTTP
        requests to the target ranking. Do it until something very bad
        happens (i.e. some exception is raised). If communication fails
        don't stop, just wait FAILURE_WAIT seconds before restarting
        the loop.

        Do all this cooperatively: yield at every blocking operation
        (queue fetch, request send, failure wait, etc.). Since the
        queue is joinable, also notify when the fetched jobs are done.

        """
        # The cumulative data that we will try to send to the ranking,
        # built by combining items in the queue.
        data = list(dict() for i in xrange(self.TYPE_COUNT))

        while True:
            # If we don't have anything left to do, block until we get
            # something new.
            if sum(len(data[i]) for i in xrange(self.TYPE_COUNT)) == 0:
                self.data_queue.peek()

            try:
                while True:
                    # Get other data if it's immediately available.
                    item = self.data_queue.get_nowait()

                    # Merge this item with the cumulative data.
                    data[item[0]].update(item[1])
            except gevent.queue.Empty:
                pass

            try:
                for i in xrange(self.TYPE_COUNT):
                    # Send entities of type i.
                    if len(data[i]) > 0:
                        # XXX We abuse the resource path as the english
                        # (plural) name for the entity type.
                        name = self.RESOURCE_PATHS[i]
                        operation = \
                            "sending %s to ranking %s" % (name, self.ranking)

                        logger.debug(operation.capitalize())
                        safe_put_data(
                            self.ranking, b"%s/" % name, data[i], operation)
                        data[i].clear()

            except CannotSendError:
                # A log message has already been produced.
                gevent.sleep(self.FAILURE_WAIT)
            except:
                # Whoa! That's unexpected!
                logger.error("Unexpected error.", exc_info=True)
                gevent.sleep(self.FAILURE_WAIT)


class ProxyService(Service):

    """Maintain the information held by rankings up-to-date.

    Discover (by receiving notifications and by periodically sweeping
    over the database) when relevant data changes happen and forward
    them to the rankings by putting them in the queues of the proxies.

    The "entry points" are submission_score, submission_tokened,
    dataset_updated and search_jobs_not_done. They can all be called
    via RPC and the latter is also periodically executed (each
    JOBS_NOT_DONE_CHECK_TIME). These methods fetch objects from the
    database, check their validity (existence, non-hiddenness, etc.)
    and status and, if needed, put call initialize, send_score and
    send_token that construct the data to send to rankings and put it
    in the queues of all proxies.

    """

    # How often we look for submission not scored/tokened.
    JOBS_NOT_DONE_CHECK_TIME = 347.0

    def __init__(self, shard, contest_id):
        """Start the service with the given parameters.

        Create an instance of the ProxyService and make it listen on
        the address corresponding to the given shard. Tell it to
        manage data for the contest with the given ID.

        shard (int): the shard of the service, i.e. this instance
            corresponds to the shard-th entry in the list of addresses
            (hostname/port pairs) for this kind of service in the
            configuration file.
        contest_id (int): the ID of the contest to manage.

        """
        Service.__init__(self, shard)

        self.contest_id = contest_id

        # Store what data we already sent to rankings. This is to aid
        # search_jobs_not_done determine what data we didn't send yet.
        self.scores_sent_to_rankings = set()
        self.tokens_sent_to_rankings = set()

        # Create and spawn threads to send data to rankings.
        self.rankings = list()
        for ranking in config.rankings:
            proxy = RankingProxy(ranking.encode('utf-8'))
            gevent.spawn(proxy.run)
            self.rankings.append(proxy)

        # Send some initial data to rankings.
        self.initialize()

        self.add_timeout(self.search_jobs_not_done, None,
                         ProxyService.JOBS_NOT_DONE_CHECK_TIME,
                         immediately=True)

    @rpc_method
    def search_jobs_not_done(self):
        """Sweep the database and search for work to do.

        Iterate over all submissions and look if they are in a suitable
        status to be sent (scored and not hidden) but, for some reason,
        haven't been sent yet (that is, their ID doesn't appear in the
        *_sent_to_rankings sets). In case, arrange for them to be sent.

        """
        logger.info("Going to search for unsent subchanges.")

        job_count = 0

        with SessionGen() as session:
            contest = Contest.get_from_id(self.contest_id, session)

            for submission in contest.get_submissions():
                if submission.user.hidden:
                    continue

                if submission.get_result().scored() and \
                        submission.id not in self.scores_sent_to_rankings:
                    self.send_score(submission)
                    job_count += 1

                if submission.tokened() and \
                        submission.id not in self.tokens_sent_to_rankings:
                    self.send_token(submission)
                    job_count += 1

        logger.info("Found %d unsent subchanges." % job_count)

    def initialize(self):
        """Send basic data to all the rankings.

        It's data that's supposed to be sent before the contest, that's
        needed to understand what we're talking about when we send
        submissions: contest, users, tasks.

        No support for teams, flags and faces.

        """
        logger.info("Initializing rankings.")

        with SessionGen() as session:
            contest = Contest.get_from_id(self.contest_id, session)

            if contest is None:
                logger.error("Received request for unexistent contest "
                             "id %s." % self.contest_id)
                raise KeyError("Contest not found.")

            contest_id = encode_id(contest.name)
            contest_data = {
                "name": contest.description,
                "begin": int(make_timestamp(contest.start)),
                "end": int(make_timestamp(contest.stop)),
                "score_precision": contest.score_precision}

            users = dict()

            for user in contest.users:
                if not user.hidden:
                    users[encode_id(user.username)] = \
                        {"f_name": user.first_name,
                         "l_name": user.last_name,
                         "team": None}

            tasks = dict()

            for task in contest.tasks:
                score_type = get_score_type(dataset=task.active_dataset)
                tasks[encode_id(task.name)] = \
                    {"short_name": task.name,
                     "name": task.title,
                     "contest": encode_id(contest.name),
                     "order": task.num,
                     "max_score": score_type.max_score,
                     "extra_headers": score_type.ranking_headers,
                     "score_precision": task.score_precision}

        for ranking in self.rankings:
            ranking.data_queue.put((ranking.CONTEST_TYPE,
                                    {contest_id: contest_data}))
            ranking.data_queue.put((ranking.USER_TYPE, users))
            ranking.data_queue.put((ranking.TASK_TYPE, tasks))

    def send_score(self, submission):
        """Send the score for the given submission to all rankings.

        Put the submission and its score subchange in all the proxy
        queues for them to be sent to rankings.

        """
        submission_result = submission.get_result()

        # Data to send to remote rankings.
        submission_id = str(submission.id)
        submission_data = {
            "user": encode_id(submission.user.username),
            "task": encode_id(submission.task.name),
            "time": int(make_timestamp(submission.timestamp))}

        subchange_id = "%d%ss" % (make_timestamp(submission.timestamp),
                                  submission_id)
        subchange_data = {
            "submission": submission_id,
            "time": int(make_timestamp(submission.timestamp))}

        # XXX This check is probably useless.
        if submission_result is not None and submission_result.scored():
            # We're sending the unrounded score to RWS
            subchange_data["score"] = submission_result.score
            subchange_data["extra"] = \
                json.loads(submission_result.ranking_score_details)

        # Adding operations to the queue.
        for ranking in self.rankings:
            ranking.data_queue.put((ranking.SUBMISSION_TYPE,
                                    {submission_id: submission_data}))
            ranking.data_queue.put((ranking.SUBCHANGE_TYPE,
                                    {subchange_id: subchange_data}))

        self.scores_sent_to_rankings.add(submission.id)

    def send_token(self, submission):
        """Send the token for the given submission to all rankings.

        Put the submission and its token subchange in all the proxy
        queues for them to be sent to rankings.

        """
        # Data to send to remote rankings.
        submission_id = str(submission.id)
        submission_data = {
            "user": encode_id(submission.user.username),
            "task": encode_id(submission.task.name),
            "time": int(make_timestamp(submission.timestamp))}

        subchange_id = "%d%st" % (make_timestamp(submission.token.timestamp),
                                  submission_id)
        subchange_data = {
            "submission": submission_id,
            "time": int(make_timestamp(submission.token.timestamp)),
            "token": True}

        # Adding operations to the queue.
        for ranking in self.rankings:
            ranking.data_queue.put((ranking.SUBMISSION_TYPE,
                                    {submission_id: submission_data}))
            ranking.data_queue.put((ranking.SUBCHANGE_TYPE,
                                    {subchange_id: subchange_data}))

        self.tokens_sent_to_rankings.add(submission.id)

    @rpc_method
    def reinitialize(self):
        """Repeat the initialization procedure for all rankings.

        This method is usually called via RPC when someone knows that
        some basic data (i.e. contest, tasks or users) changed and
        rankings need to be updated.

        """
        logger.info("Reinitializing rankings.")
        self.initialize()

    @rpc_method
    def submission_scored(self, submission_id):
        """Notice that a submission has been scored.

        Usually called by ScoringService when it's done with scoring a
        submission result. Since we don't trust anyone we verify that,
        and then send data about the score to the rankings.

        submission_id (int): the id of the submission that changed.
        dataset_id (int): the id of the dataset to use.

        """
        with SessionGen() as session:
            submission = Submission.get_from_id(submission_id, session)

            if submission is None:
                logger.error("[submission_scored] Received score request for "
                             "unexistent submission id %s." % submission_id)
                raise KeyError("Submission not found.")

            if submission.user.hidden:
                logger.info("[submission_scored] Score for submission %d "
                            "not sent because user is hidden." % submission_id)
                return

            # Update RWS.
            self.send_score(submission)

    @rpc_method
    def submission_tokened(self, submission_id):
        """Notice that a submission has been tokened.

        Usually called by ContestWebServer when it's processing a token
        request of an user. Since we don't trust anyone we verify that,
        and then send data about the token to the rankings.

        submission_id (int): the id of the submission that changed.

        """
        with SessionGen() as session:
            submission = Submission.get_from_id(submission_id, session)

            if submission is None:
                logger.error("[submission_tokened] Received token request for "
                             "unexistent submission id %s." % submission_id)
                raise KeyError("Submission not found.")

            if submission.user.hidden:
                logger.info("[submission_tokened] Token for submission %d "
                            "not sent because user is hidden." % submission_id)
                return

            # Update RWS.
            self.send_token(submission)

    @rpc_method
    def dataset_updated(self, task_id):
        """Notice that the active dataset of a task has been changed.

        Usually called by AdminWebServer when the contest administrator
        changed the active dataset of a task. This means that we should
        update all the scores for the task using the submission results
        on the new active dataset. If some of them are not available
        yet we keep the old scores (we don't delete them!) and wait for
        ScoringService to notify us that the new ones are available.

        task_id (int): the ID of the task whose dataset has changed.

        """
        with SessionGen() as session:
            task = Task.get_from_id(task_id, session)
            dataset = task.active_dataset

            logger.info("Dataset update for task %d (dataset now is %d)." % (
                task.id, dataset.id))

            # max_score and/or extra_headers might have changed.
            self.reinitialize()

            for submission in task.submissions:
                # Update RWS.
                if not submission.user.hidden and \
                        submission.get_result().scored():
                    self.send_score(submission)

########NEW FILE########
__FILENAME__ = ResourceService
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2014 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013-2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Service to be run once for each machine the system is running on,
that saves the resources usage in that machine.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import bisect
import logging
import os
import time

import psutil

from gevent import subprocess
#import gevent_subprocess as subprocess

from cms import config, get_safe_shard, ServiceCoord
from cms.io import Service, rpc_method, RemoteServiceClient


logger = logging.getLogger(__name__)


B_TO_MB = 1024.0 * 1024.0

# As psutil-2.0 introduced many backward-incompatible changes to its
# API we define this global flag to make it easier later on to decide
# which methods, properties, etc. to use.
PSUTIL2 = psutil.version_info >= (2, 0)

if PSUTIL2:
    PSUTIL_PROC_ATTRS = \
        ["cmdline", "cpu_times", "create_time", "memory_info", "num_threads"]
else:
    PSUTIL_PROC_ATTRS = \
        ["cmdline", "get_cpu_times", "create_time",
         "get_memory_info", "get_num_threads"]


class ResourceService(Service):
    """This service looks at the resources usage (CPU, load, memory,
    network) every seconds, stores it locally, and offer (new) data
    upon request.

    """
    def __init__(self, shard, contest_id=None):
        """If contest_id is not None, we assume the user wants the
        autorestart feature.

        """
        Service.__init__(self, shard)

        self.contest_id = contest_id

        # _local_store is a dictionary indexed by time in int(epoch)
        self._local_store = []
        # Floating point epoch using for precise measurement of percents
        self._last_saved_time = time.time()
        # Starting point for cpu times
        self._prev_cpu_times = self._get_cpu_times()
        # Sorted list of ServiceCoord running in the same machine
        self._local_services = self._find_local_services()
        # Dict service with bool to mark if we will restart them.
        self._will_restart = dict((service,
                                   None if self.contest_id is None else True)
                                  for service in self._local_services)
        # Found process associate to the ServiceCoord.
        self._procs = dict((service, None)
                           for service in self._local_services)
        # Previous cpu time for each service.
        self._services_prev_cpu_times = \
            dict((service, (0.0, 0.0)) for service in self._local_services)
        # Start finding processes and their cputimes.
        self._store_resources(store=False)

        self.add_timeout(self._store_resources, None, 5.0)
        if self.contest_id is not None:
            self._launched_processes = set([])
            self.add_timeout(self._restart_services, None, 5.0,
                             immediately=True)

    def _restart_services(self):
        """Check if the services that are supposed to run on this
        machine are actually running. If not, start them.

        """
        # To avoid zombies, we poll the process we launched. Anyway we
        # use the information from psutil to see if the process we are
        # interested in are alive (since if the user has already
        # launched another instance, we don't want to duplicate
        # services).
        new_launched_processes = set([])
        for process in self._launched_processes:
            if process.poll() is None:
                new_launched_processes.add(process)
        self._launched_processes = new_launched_processes

        # Look for dead processes, and restart them.
        for service in self._local_services:
            # We let the user start logservice and resourceservice.
            if service.name == "LogService" or \
                    service.name == "ResourceService":
                continue

            # If the user specified not to restart some service, we
            # ignore it.
            if not self._will_restart[service]:
                continue

            running = True
            proc = self._procs[service]
            # If we don't have a previously found process for the
            # service, we find it
            if proc is None:
                proc = self._find_proc(service)
            if proc is None:
                running = False
            else:
                self._procs[service] = proc
                # We have a process, but maybe it has been shut down
                if not proc.is_running():
                    # If so, let us find the new one
                    proc = self._find_proc(service)
                    # If there is no new one, continue
                    if proc is None:
                        running = False
                    else:
                        self._procs[service] = proc

            if not running:
                # We give contest_id even if the service doesn't need
                # it, since it causes no trouble.
                logger.info("Restarting (%s, %s)..." % (service.name,
                                                        service.shard))
                devnull = os.open(os.devnull, os.O_WRONLY)
                command = "cms%s" % service.name
                if not config.installed:
                    command = os.path.join(
                        ".",
                        "scripts",
                        "cms%s" % service.name)
                process = subprocess.Popen([command,
                                            str(service.shard),
                                            "-c",
                                            str(self.contest_id)],
                                           stdout=devnull,
                                           stderr=subprocess.STDOUT
                                           )
                self._launched_processes.add(process)

        # Run forever.
        return True

    def _find_local_services(self):
        """Returns the services that are running on the same machine
        as us.

        returns (list): a list of ServiceCoord elements, sorted by
                        name and shard

        """
        logger.debug("ResourceService._find_local_services")
        services = config.async.core_services
        local_machine = services[self._my_coord].ip
        local_services = [x
                          for x in services
                          if services[x].ip == local_machine]
        return sorted(local_services)

    @staticmethod
    def _is_service_proc(service, cmdline):
        """Returns if cmdline can be the command line of service.

        service (ServiceCoord): the service.
        cmdline ([string]): a command line.

        return (bool): whether service could have been launched with
            the command line cmdline.

        """
        if not cmdline:
            return False

        start_index = 0
        if cmdline[0] == "/usr/bin/env":
            start_index = 1

        if len(cmdline) - start_index < 2:
            return False

        cl_interpreter = cmdline[start_index]
        cl_service = cmdline[start_index + 1]
        if "python" not in cl_interpreter or \
                not cl_service.endswith("cms%s" % service.name):
            return False

        # We assume that apart from the shard, all other
        # options are in the form "-<something> <something>".
        shard = None
        for i in xrange(start_index + 2, len(cmdline), 2):
            if cmdline[i].isdigit():
                shard = int(cmdline[i])
                break
        try:
            if get_safe_shard(service.name, shard) != service.shard:
                return False
        except ValueError:
            return False

        return True

    def _find_proc(self, service):
        """Returns the pid of a given service running on this machine.

        service (ServiceCoord): the service we are interested in
        returns (psutil.Process): the process of service, or None if
                                  not found

        """
        logger.debug("ResourceService._find_proc")
        for proc in psutil.get_process_list():
            try:
                proc_info = proc.as_dict(attrs=PSUTIL_PROC_ATTRS)
                if ResourceService._is_service_proc(
                        service, proc_info["cmdline"]):
                    self._services_prev_cpu_times[service] = \
                        proc_info["cpu_times"]
                    return proc
            except psutil.NoSuchProcess:
                continue
        return None

    @staticmethod
    def _get_cpu_times():
        """Wrapper of psutil.cpu_times to get the format we like.

        return (dict): dictionary of cpu times information.

        """
        cpu_times = psutil.cpu_times()
        return {"user": cpu_times.user,
                "nice": cpu_times.nice,
                "system": cpu_times.system,
                "idle": cpu_times.idle,
                "iowait": cpu_times.iowait,
                "irq": cpu_times.irq,
                "softirq": cpu_times.softirq}

    def _store_resources(self, store=True):
        """Looks at the resources usage and store the data locally.

        store (bool): if False, run the method but do not store the
                      resulting values - useful for initializing the
                      previous values

        """
        logger.debug("ResourceService._store_resources")
        # We use the precise time to compute the delta
        now = time.time()
        delta = now - self._last_saved_time
        self._last_saved_time = now
        now = int(now)

        data = {}

        # CPU
        cpu_times = self._get_cpu_times()
        data["cpu"] = dict((x, int(round((cpu_times[x] -
                                          self._prev_cpu_times[x])
                                   / delta * 100.0)))
                           for x in cpu_times)
        data["cpu"]["num_cpu"] = \
            psutil.cpu_count() if PSUTIL2 else psutil.NUM_CPUS
        self._prev_cpu_times = cpu_times

        # Memory. The following relations hold (I think... I only
        # verified them experimentally on a swap-less system):
        # * vmem.free == vmem.available - vmem.cached - vmem.buffers
        # * vmem.total == vmem.used + vmem.free
        # That means that cache & buffers are counted both in .used
        # and in .available. We want to partition the memory into
        # types that sum up to vmem.total.
        vmem = psutil.virtual_memory()
        swap = psutil.swap_memory()
        data["memory"] = {
            "ram_total": vmem.total / B_TO_MB,
            "ram_available": vmem.free / B_TO_MB,
            "ram_cached": vmem.cached / B_TO_MB,
            "ram_buffers": vmem.buffers / B_TO_MB,
            "ram_used": (vmem.used - vmem.cached - vmem.buffers) / B_TO_MB,
            "swap_total": swap.total / B_TO_MB,
            "swap_available": swap.free / B_TO_MB,
            "swap_used": swap.used / B_TO_MB,
            }

        data["services"] = {}
        # Details of our services
        for service in self._local_services:
            dic = {"autorestart": self._will_restart[service],
                   "running": True}
            proc = self._procs[service]
            # If we don't have a previously found process for the
            # service, we find it
            if proc is None:
                proc = self._find_proc(service)
            # If we still do not find it, there is no process
            if proc is None:
                dic["running"] = False
            # We have a process, but maybe it has been shut down
            elif not proc.is_running():
                # If so, let us find the new one
                proc = self._find_proc(service)
                # If there is no new one, continue
                if proc is None:
                    dic["running"] = False
            # If the process is not running, we have nothing to do.
            if not dic["running"]:
                data["services"][str(service)] = dic
                continue

            try:
                proc_info = proc.as_dict(attrs=PSUTIL_PROC_ATTRS)
                dic["since"] = self._last_saved_time - proc_info["create_time"]
                dic["resident"], dic["virtual"] = \
                    (x // B_TO_MB for x in proc_info["memory_info"])
                cpu_times = proc_info["cpu_times"]
                dic["user"] = int(
                    round((cpu_times[0] -
                           self._services_prev_cpu_times[service][0])
                          / delta * 100))
                dic["sys"] = int(
                    round((cpu_times[1] -
                           self._services_prev_cpu_times[service][1])
                          / delta * 100))
                self._services_prev_cpu_times[service] = cpu_times
                try:
                    dic["threads"] = proc_info["num_threads"]
                except AttributeError:
                    dic["threads"] = 0  # 0 = Not implemented

                self._procs[service] = proc
            except psutil.NoSuchProcess:
                # Shut down while we operated?
                dic = {"autorestart": self._will_restart[service],
                       "running": False}
            data["services"][str(service)] = dic

        if store:
            if len(self._local_store) >= 5000:  # almost 7 hours
                self._local_store = self._local_store[1:]
            self._local_store.append((now, data))

        return True

    @rpc_method
    def get_resources(self, last_time=0.0):
        """Returns the resurce usage information from last_time to
        now.

        last_time (float): timestamp of the last time the caller
            called this method.

        """
        logger.debug("ResourceService._get_resources")
        index = bisect.bisect_right(self._local_store, (last_time, 0))
        return self._local_store[index:]

    @rpc_method
    def kill_service(self, service):
        """Restart the service. Note that after calling successfully
        this method, get_resource could still report the service
        running untile we call _store_resources again.

        service (string): format: name,shard.

        """
        logger.info("Killing %s as asked." % service)
        try:
            idx = service.rindex(",")
        except ValueError:
            logger.error("Unable to decode service string.")
        name = service[:idx]
        try:
            shard = int(service[idx + 1:])
        except ValueError:
            logger.error("Unable to decode service shard.")

        remote_service = RemoteServiceClient(ServiceCoord(name, shard))
        remote_service.quit(reason="Asked by ResourceService")

    @rpc_method
    def toggle_autorestart(self, service):
        """If the service is scheduled for autorestart, disable it,
        otherwise enable it.

        service (string): format: name,shard.

        return (bool/None): current status of will_restart.

        """
        # If the contest_id is not set, we cannot autorestart.
        if self.contest_id is None:
            return None

        # Decode name,shard
        try:
            idx = service.rindex(",")
        except ValueError:
            logger.error("Unable to decode service string.")
        name = service[:idx]
        try:
            shard = int(service[idx + 1:])
        except ValueError:
            logger.error("Unable to decode service shard.")
        service = ServiceCoord(name, shard)

        self._will_restart[service] = not self._will_restart[service]
        logger.info("Will restart %s,%s is now %s." %
                    (service.name, service.shard, self._will_restart[service]))

        return self._will_restart[service]

########NEW FILE########
__FILENAME__ = ScoringService
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Bernard Blackham <bernard@largestprime.net>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A service that assigns a score to submission results.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging

import gevent
from gevent.queue import JoinableQueue
from gevent.event import Event

from cms import ServiceCoord
from cms.io import Service, rpc_method
from cms.db import SessionGen, Submission, Dataset
from cms.grading.scoretypes import get_score_type
from cms.service import get_submission_results
from cmscommon.datetime import monotonic_time


logger = logging.getLogger(__name__)


class ScoringService(Service):
    """A service that assigns a score to submission results.

    A submission result is ready to be scored when its compilation is
    unsuccessful (in this case, no evaluation will be performed) or
    after it has been evaluated. The goal of scoring is to use the
    evaluations to determine score, score_details, public_score,
    public_score_details and ranking_score_details (all non-null).
    Scoring is done by the compute_score method of the ScoreType
    defined by the dataset of the result.

    ScoringService keeps a queue of (submission_id, dataset_id) pairs
    identifying submission results to score. A greenlet is spawned to
    consume this queue, one item at a time. The queue is filled by the
    new_evaluation and the invalidate_submissions RPC methods, and by a
    sweeper greenlet, whose duty is to regularly check all submissions
    in the database and put the unscored ones in the queue (this check
    can also be forced by the search_jobs_not_done RPC method).

    """

    # How often we look for submission results not scored.
    SWEEPER_TIMEOUT = 347.0

    def __init__(self, shard):
        """Initialize the ScoringService.

        """
        Service.__init__(self, shard)

        # Set up communication with ProxyService.
        self.proxy_service = self.connect_to(ServiceCoord("ProxyService", 0))

        # Set up and spawn the scorer.
        # TODO Link to greenlet: when it dies, log CRITICAL and exit.
        self._scorer_queue = JoinableQueue()
        gevent.spawn(self._scorer_loop)

        # Set up and spawn the sweeper.
        # TODO Link to greenlet: when it dies, log CRITICAL and exit.
        self._sweeper_start = None
        self._sweeper_event = Event()
        gevent.spawn(self._sweeper_loop)

    def _scorer_loop(self):
        """Monitor the queue, scoring its top element.

        This is an infinite loop that, at each iteration, gets an item
        from the queue (blocking until there is one, if the queue is
        empty) and scores it. Any error during the scoring is sent to
        the logger and then suppressed, because the loop must go on.

        """
        while True:
            submission_id, dataset_id = self._scorer_queue.get()
            try:
                self._score(submission_id, dataset_id)
            except Exception:
                logger.error("Unexpected error when scoring submission %d on "
                             "dataset %d.", submission_id, dataset_id,
                             exc_info=True)
            finally:
                self._scorer_queue.task_done()

    def _score(self, submission_id, dataset_id):
        """Assign a score to a submission result.

        This is the core of ScoringService: here we retrieve the result
        from the database, check if it is in the correct status,
        instantiate its ScoreType, compute its score, store it back in
        the database and tell ProxyService to update RWS if needed.

        submission_id (int): the id of the submission that has to be
            scored.
        dataset_id (int): the id of the dataset to use.

        """
        with SessionGen() as session:
            # Obtain submission.
            submission = Submission.get_from_id(submission_id, session)
            if submission is None:
                raise ValueError("Submission %d not found in the database." %
                                 submission_id)

            # Obtain dataset.
            dataset = Dataset.get_from_id(dataset_id, session)
            if dataset is None:
                raise ValueError("Dataset %d not found in the database." %
                                 dataset_id)

            # Obtain submission result.
            submission_result = submission.get_result(dataset)

            # It means it was not even compiled (for some reason).
            if submission_result is None:
                raise ValueError("Submission result %d(%d) was not found." %
                                 (submission_id, dataset_id))

            # Check if it's ready to be scored.
            if not submission_result.needs_scoring():
                if submission_result.scored():
                    logger.info("Submission result %d(%d) is already scored.",
                                submission_id, dataset_id)
                    return
                else:
                    raise ValueError("The state of the submission result "
                                     "%d(%d) doesn't allow scoring." %
                                     (submission_id, dataset_id))

            # Instantiate the score type.
            score_type = get_score_type(dataset=dataset)

            # Compute score and fill it in the database.
            submission_result.score, \
                submission_result.score_details, \
                submission_result.public_score, \
                submission_result.public_score_details, \
                submission_result.ranking_score_details = \
                score_type.compute_score(submission_result)

            # Store it.
            session.commit()

            # If dataset is the active one, update RWS.
            if dataset is submission.task.active_dataset:
                self.proxy_service.submission_scored(
                    submission_id=submission.id)

    def _sweeper_loop(self):
        """Regularly check the database for unscored results.

        Try to sweep the database once every SWEEPER_TIMEOUT seconds
        but make sure that no two sweeps run simultaneously. That is,
        start a new sweep SWEEPER_TIMEOUT seconds after the previous
        one started or when the previous one finished, whatever comes
        last.

        The search_jobs_not_done RPC method can interfere with this
        regularity, as it tries to run a sweeper as soon as possible:
        immediately, if no sweeper is running, or as soon as the
        current one terminates.

        Any error during the sweep is sent to the logger and then
        suppressed, because the loop must go on.

        """
        while True:
            self._sweeper_start = monotonic_time()
            self._sweeper_event.clear()

            try:
                self._sweep()
            except Exception:
                logger.error("Unexpected error when searching for unscored "
                             "submissions.", exc_info=True)

            self._sweeper_event.wait(max(self._sweeper_start +
                                         self.SWEEPER_TIMEOUT -
                                         monotonic_time(), 0))

    def _sweep(self):
        """Check the database for unscored submission results.

        Obtain a list of all the submission results in the database,
        check each of them to see if it's still unscored and, in case,
        put it in the queue.

        """
        counter = 0

        with SessionGen() as session:
            for sr in get_submission_results(session=session):
                if sr is not None and sr.needs_scoring():
                    self._scorer_queue.put((sr.submission_id, sr.dataset_id))
                    counter += 1

        if counter > 0:
            logger.info("Found %d unscored submissions.", counter)

    @rpc_method
    def search_jobs_not_done(self):
        """Make the sweeper loop fire the sweeper as soon as possible.

        """
        self._sweeper_event.set()

    @rpc_method
    def new_evaluation(self, submission_id, dataset_id):
        """Schedule the given submission result for scoring.

        Put it in the queue to have it scored, sooner or later. Usually
        called by EvaluationService when it's done with a result.

        submission_id (int): the id of the submission that has to be
            scored.
        dataset_id (int): the id of the dataset to use.

        """
        self._scorer_queue.put((submission_id, dataset_id))

    @rpc_method
    def invalidate_submission(self, submission_id=None, dataset_id=None,
                              user_id=None, task_id=None, contest_id=None):
        """Invalidate (and re-score) some submission results.

        Invalidate the scores of the submission results that:
        - belong to submission_id or, if None, to any submission of
          user_id and/or task_id or, if both None, to any submission
          of contest_id or, if None, to any submission in the database.
        - belong to dataset_id or, if None, to any dataset of task_id
          or, if None, to any dataset of contest_id or, if None, to any
          dataset in the database.

        submission_id (int): id of the submission whose results should
            be invalidated, or None.
        dataset_id (int): id of the dataset whose results should be
            invalidated, or None.
        user_id (int): id of the user whose results should be
            invalidated, or None.
        task_id (int): id of the task whose results should be
            invalidated, or None.
        contest_id (int): id of the contest whose results should be
            invalidated, or None.

        """
        logger.info("Invalidation request received.")

        # We can put results in the scorer queue only after they have
        # been invalidated (and committed to the database). Therefore
        # we temporarily save them somewhere else.
        temp_queue = list()

        with SessionGen() as session:
            submission_results = \
                get_submission_results(contest_id, user_id, task_id,
                                       submission_id, dataset_id,
                                       session=session)

            for sr in submission_results:
                if sr.scored():
                    sr.invalidate_score()
                    temp_queue.append((sr.submission_id, sr.dataset_id))

            session.commit()

        for item in temp_queue:
            self._scorer_queue.put(item)

        logger.info("Invalidated %d submissions.", len(temp_queue))

########NEW FILE########
__FILENAME__ = Worker
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""The service that actually compiles and executes code.

"""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import logging

import gevent.coros

from cms.io import Service, rpc_method
from cms.db import SessionGen, Contest
from cms.db.filecacher import FileCacher
from cms.grading import JobException
from cms.grading.tasktypes import get_task_type
from cms.grading.Job import JobGroup


logger = logging.getLogger(__name__)


class Worker(Service):
    """This service implement the possibility to compile and evaluate
    submissions in a sandbox. The instructions to follow for the
    operations are in the TaskType classes, while the sandbox is in
    the Sandbox module.

    """

    JOB_TYPE_COMPILATION = "compile"
    JOB_TYPE_EVALUATION = "evaluate"

    def __init__(self, shard):
        Service.__init__(self, shard)
        self.file_cacher = FileCacher(self)

        self.work_lock = gevent.coros.RLock()
        self._ignore_job = False

    @rpc_method
    def ignore_job(self):
        """RPC that inform the worker that its result for the current
        action will be discarded. The worker will try to return as
        soon as possible even if this means that the result are
        inconsistent.

        """
        # We remember to quit as soon as possible.
        logger.info("Trying to interrupt job as requested.")
        self._ignore_job = True

    @rpc_method
    def precache_files(self, contest_id):
        """RPC to ask the worker to precache of files in the contest.

        contest_id (int): the id of the contest

        """
        # Lock is not needed if the admins correctly placed cache and
        # temp directories in the same filesystem. This is what
        # usually happens since they are children of the same,
        # cms-created, directory.
        logger.info("Precaching files for contest %d." % contest_id)
        with SessionGen() as session:
            contest = Contest.get_from_id(contest_id, session)
            for digest in contest.enumerate_files(skip_submissions=True,
                                                  skip_user_tests=True):
                self.file_cacher.load(digest)
        logger.info("Precaching finished.")

    @rpc_method
    def execute_job_group(self, job_group_dict):
        """Receive a group of jobs in a dict format and executes them
        one by one.

        job_group_dict (dict): a dictionary suitable to be imported
            from JobGroup.

        """
        job_group = JobGroup.import_from_dict(job_group_dict)

        if self.work_lock.acquire(False):

            try:
                self._ignore_job = False

                for k, job in job_group.jobs.iteritems():
                    logger.info("Starting job.",
                                extra={"operation": job.info})

                    job.shard = self.shard

                    # FIXME This is actually kind of a workaround...
                    # The only TaskType that needs it is OutputOnly.
                    job._key = k

                    # FIXME We're creating a new TaskType for each Job
                    # even if, at the moment, a JobGroup always uses
                    # the same TaskType and the same parameters. Yet,
                    # this could change in the future, so the best
                    # solution is to keep a cache of TaskTypes objects
                    # (like ScoringService does with ScoreTypes, except
                    # that we cannot index by Dataset ID here...).
                    task_type = get_task_type(job.task_type,
                                              job.task_type_parameters)
                    task_type.execute_job(job, self.file_cacher)

                    logger.info("Finished job.",
                                extra={"operation": job.info})

                    if not job.success or self._ignore_job:
                        job_group.success = False
                        break
                else:
                    job_group.success = True

                return job_group.export_to_dict()

            except:
                err_msg = "Worker failed."
                logger.error(err_msg, exc_info=True)
                raise JobException(err_msg)

            finally:
                self.work_lock.release()

        else:
            err_msg = "Request received, but declined because of acquired " \
                "lock (Worker is busy executing another job group, this " \
                "should not happen: check if there are more than one ES " \
                "running, or for bugs in ES."
            logger.warning(err_msg)
            raise JobException(err_msg)

########NEW FILE########
__FILENAME__ = util
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2014 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import errno
import logging
import netifaces
import os
import sys
from argparse import ArgumentParser
from collections import namedtuple

import gevent.socket


logger = logging.getLogger(__name__)


def mkdir(path):
    """Make a directory without complaining for errors.

    path (string): the path of the directory to create
    returns (bool): True if the dir is ok, False if it is not

    """
    try:
        os.mkdir(path)
    except OSError as error:
        if error.errno != errno.EEXIST:
            return False
    return True


class Address(namedtuple("Address", "ip port")):
    def __repr__(self):
        return "%s:%d" % (self.ip, self.port)


class ServiceCoord(namedtuple("ServiceCoord", "name shard")):
    """A compact representation for the name and the shard number of a
    service (thus identifying it).

    """
    def __repr__(self):
        return "%s,%d" % (self.name, self.shard)


class Config(object):
    """This class will contain the configuration for the
    services. This needs to be populated at the initilization stage.

    The *_services variables are dictionaries indexed by ServiceCoord
    with values of type Address.

    Core services are the ones that are supposed to run whenever the
    system is up.

    Other services are not supposed to run when the system is up, or
    anyway not constantly.

    """
    core_services = {}
    other_services = {}


async_config = Config()


def get_safe_shard(service, provided_shard):
    """Return a safe shard number for the provided service, or raise.

    service (string): the name of the service trying to get its shard,
        for looking it up in the config.
    provided_shard (int|None): the shard number provided by the admin
        via command line, or None (the default value).

    return (int): the provided shard number if it makes sense,
        otherwise the shard number found matching the IP address with
        the configuration.

    raise (ValueError): if no safe shard can be returned.

    """
    if provided_shard is None:
        addrs = _find_local_addresses()
        computed_shard = _get_shard_from_addresses(service, addrs)
        if computed_shard is None:
            logger.critical("Couldn't autodetect shard number and "
                            "no shard specified for service %s, "
                            "quitting.", service)
            raise ValueError("No safe shard found for %s." % service)
        else:
            return computed_shard
    else:
        coord = ServiceCoord(service, provided_shard)
        if coord not in async_config.core_services:
            logger.critical("The provided shard number for service %s "
                            "cannot be found in the configuration, "
                            "quitting.", service)
            raise ValueError("No safe shard found for %s." % service)
        else:
            return provided_shard


def get_service_address(key):
    """Give the Address of a ServiceCoord.

    key (ServiceCoord): the service needed.
    returns (Address): listening address of key.

    """
    if key in async_config.core_services:
        return async_config.core_services[key]
    elif key in async_config.other_services:
        return async_config.other_services[key]
    else:
        raise KeyError("Service not found.")


def get_service_shards(service):
    """Returns the number of shards that a service has.

    service (string): the name of the service.
    returns (int): the number of shards defined in the configuration.

    """
    i = 0
    while True:
        try:
            get_service_address(ServiceCoord(service, i))
        except KeyError:
            return i
        i += 1


def default_argument_parser(description, cls, ask_contest=None):
    """Default argument parser for services - in two versions: needing
    a contest_id, or not.

    description (string): description of the service.
    cls (type): service's class.
    ask_contest (function): None if the service does not require a
                            contest, otherwise a function that returns
                            a contest_id (after asking the admins?)

    return (object): an instance of a service.

    """
    parser = ArgumentParser(description=description)
    parser.add_argument("shard", nargs="?", type=int, default=None)

    # We need to allow using the switch "-c" also for services that do
    # not need the contest_id because RS needs to be able to restart
    # everything without knowing which is which.
    contest_id_help = "id of the contest to automatically load"
    if ask_contest is None:
        contest_id_help += " (ignored)"
    parser.add_argument("-c", "--contest-id", help=contest_id_help,
                        nargs="?", type=int)
    args = parser.parse_args()

    try:
        args.shard = get_safe_shard(cls.__name__, args.shard)
    except ValueError:
        sys.exit(1)

    if ask_contest is not None:
        if args.contest_id is not None:
            # Test if there is a contest with the given contest id.
            from cms.db import is_contest_id
            if not is_contest_id(args.contest_id):
                print("There is no contest with the specified id. "
                      "Please try again.", file=sys.stderr)
                sys.exit(1)
            return cls(args.shard, args.contest_id)
        else:
            return cls(args.shard, ask_contest())
    else:
        return cls(args.shard)


def _find_local_addresses():
    """Returns the list of IPv4 and IPv6 addresses configured on the
    local machine.

    returns ([(int, str)]): a list of tuples, each representing a
                            local address; the first element is the
                            protocol and the second one is the
                            address.

    """
    addrs = []
    # Based on http://stackoverflow.com/questions/166506/
    # /finding-local-ip-addresses-using-pythons-stdlib
    for iface_name in netifaces.interfaces():
        for proto in [netifaces.AF_INET, netifaces.AF_INET6]:
            addrs += [(proto, i['addr'])
                      for i in netifaces.ifaddresses(iface_name).
                      setdefault(proto, [])]
    return addrs


def _get_shard_from_addresses(service, addrs):
    """Returns the first shard of a service that listens at one of the
    specified addresses.

    service (string): the name of the service.
    addrs ([(int, str)]): a list like the one returned by
        find_local_addresses().

    returns (int|None): the found shard, or None in case it doesn't
        exist.

    """
    i = 0
    ipv4_addrs = set()
    ipv6_addrs = set()
    for proto, addr in addrs:
        if proto == gevent.socket.AF_INET:
            ipv4_addrs.add(addr)
        elif proto == gevent.socket.AF_INET6:
            ipv6_addrs.add(addr)
    while True:
        try:
            host, port = get_service_address(ServiceCoord(service, i))
            res_ipv4_addrs = set()
            res_ipv6_addrs = set()
            # For magic numbers, see getaddrinfo() documentation
            try:
                res_ipv4_addrs = set([x[4][0] for x in
                                      gevent.socket.getaddrinfo(
                                          host, port,
                                          family=gevent.socket.AF_INET,
                                          socktype=gevent.socket.SOCK_STREAM)])
            except (gevent.socket.gaierror, gevent.socket.error):
                res_ipv4_addrs = set()

            try:
                res_ipv6_addrs = set([x[4][0] for x in
                                      gevent.socket.getaddrinfo(
                                          host, port,
                                          family=gevent.socket.AF_INET6,
                                          socktype=gevent.socket.SOCK_STREAM)])
            except (gevent.socket.gaierror, gevent.socket.error):
                res_ipv6_addrs = set()

            if not ipv4_addrs.isdisjoint(res_ipv4_addrs) or \
                    not ipv6_addrs.isdisjoint(res_ipv6_addrs):
                return i
        except KeyError:
            return None
        i += 1

########NEW FILE########
__FILENAME__ = crypto
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2013 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2012 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Utilities dealing with encryption and randomness."""

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import base64
import binascii

from Crypto.Cipher import AES


__all__ = [
    "is_random_secure",

    "get_random_key", "get_hex_random_key",

    "encrypt_string", "decrypt_string",
    "encrypt_number", "decrypt_number",
    ]


# Some older versions of pycrypto don't provide a Random module
# If that's the case, fallback to weak standard library PRG
try:
    from Crypto import Random
    _rndfile = Random.new()
    get_random_bits = lambda x: _rndfile.read(x)
    is_random_secure = True
except ImportError:
    import random
    get_random_bits = lambda x: binascii.unhexlify("%032x" %
                                                   random.getrandbits(x * 8))
    is_random_secure = False


def _get_secret_key_unhex():
    # Only import this if we need it. Otherwise, we would prefer to
    # remain independent of the rest of CMS.
    from cms import config
    return binascii.unhexlify(config.secret_key)


def get_random_key():
    """Generate 16 random bytes, safe to be used as AES key.

    """
    return get_random_bits(16)


def get_hex_random_key():
    """Generate 16 random bytes, safe to be used as AES key.
    Return it encoded in hexadecimal.

    """
    return binascii.hexlify(get_random_key())


def encrypt_string(pt, key=None):
    """Encrypt the plaintext (pt) with the 16-bytes key. Moreover, it
    encrypts it using a random IV, so that encrypting repeatedly the
    same string gives different outputs. This way no analisys can made
    when the same number is used in different contexts. The generated
    string uses the alphabet { 'a', ..., 'z', 'A', ..., 'Z', '0', ...,
    '9', '.', '-', '_' }, so it is safe to use in URLs.

    If key is not specified, it is obtained from the configuration.

    """
    if key is None:
        key = _get_secret_key_unhex()
    # Pad the plaintext to make its length become a multiple of the block size
    # (that is, for AES, 16 bytes), using a byte 0x01 followed by as many bytes
    # 0x00 as needed. If the length of the message is already a multiple of 16
    # bytes, add a new block.
    pt_pad = bytes(pt) + b'\01' + b'\00' * (16 - (len(pt) + 1) % 16)
    # The IV is a random block used to differentiate messages encrypted with
    # the same key. An IV should never be used more than once in the lifetime
    # of the key. In this way encrypting the same plaintext twice will produce
    # different ciphertexts.
    iv = get_random_key()
    # Initialize the AES cipher with the given key and IV.
    aes = AES.new(key, AES.MODE_CBC, iv)
    ct = aes.encrypt(pt_pad)
    # Convert the ciphertext in a URL-safe base64 encoding
    ct_b64 = base64.urlsafe_b64encode(iv + ct).replace(b'=', b'.')
    return ct_b64


def decrypt_string(ct_b64, key=None):
    """Decrypt a ciphertext (ct_b64) encrypted with encrypt_string and
    return the corresponding plaintext.

    If key is not specified, it is obtained from the configuration.

    """
    if key is None:
        key = _get_secret_key_unhex()
    try:
        # Convert the ciphertext from a URL-safe base64 encoding to a
        # bytestring, which contains both the IV (the first 16 bytes) as well
        # as the encrypted padded plaintext.
        iv_ct = base64.urlsafe_b64decode(bytes(ct_b64).replace(b'.', b'='))
        aes = AES.new(key, AES.MODE_CBC, iv_ct[:16])
        # Get the padded plaintext.
        pt_pad = aes.decrypt(iv_ct[16:])
        # Remove the padding.
        # TODO check that the padding is correct, i.e. that it contains at most
        # 15 bytes 0x00 preceded by a byte 0x01.
        pt = pt_pad.rstrip(b'\x00')[:-1]
        return pt
    except TypeError:
        raise ValueError('Could not decode from base64.')
    except ValueError:
        raise ValueError('Wrong AES cryptogram length.')


def encrypt_number(num, key=None):
    """Encrypt an integer number, with the same properties as
    encrypt_string().

    If key is not specified, it is obtained from the configuration.

    """
    hexnum = b"%x" % num
    return encrypt_string(hexnum, key)


def decrypt_number(enc, key=None):
    """Decrypt an integer number encrypted with encrypt_number().

    If key is not specified, it is obtained from the configuration.

    """
    return int(decrypt_string(enc, key), 16)

########NEW FILE########
__FILENAME__ = datetime
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import time
import platform
from datetime import tzinfo, timedelta, datetime
from pytz import timezone, all_timezones


__all__ = [
    "make_datetime", "make_timestamp",
    "get_timezone", "get_system_timezone",

    "utc",

    "monotonic_time",
    ]


def make_datetime(timestamp=None):
    """Return the datetime object associated with the given timestamp

    timestamp (int|float): a POSIX timestamp
    returns (datetime): the datetime representing the UTC time of the
        given timestamp, or now if timestamp is None.

    """
    if timestamp is None:
        return datetime.utcnow()
    else:
        return datetime.utcfromtimestamp(timestamp)


EPOCH = datetime(1970, 1, 1)


def make_timestamp(_datetime=None):
    """Return the timestamp associated with the given datetime object

    _datetime (datetime): a datetime object
    returns (float): the POSIX timestamp corresponding to the given
                     datetime ("read" in UTC), or now if datetime is
                     None.

    """
    if _datetime is None:
        return time.time()
    else:
        return (_datetime - EPOCH).total_seconds()


def get_timezone(user, contest):
    """Return the timezone for the given user and contest

    """
    if user.timezone is not None and user.timezone in all_timezones:
        return timezone(user.timezone)
    if contest.timezone is not None and contest.timezone in all_timezones:
        return timezone(contest.timezone)
    return local


def get_system_timezone():
    """Return the timezone of the system.

    See http://stackoverflow.com/questions/7669938/
               get-the-olson-tz-name-for-the-local-timezone

    return (string): one among the possible timezone description
                     strings in the form Europe/Rome, or None if
                     nothing is found.

    """
    if time.daylight:
        local_offset = time.altzone
        localtz = time.tzname[1]
    else:
        local_offset = time.timezone
        localtz = time.tzname[0]

    local_offset = timedelta(seconds=-local_offset)

    for name in all_timezones:
        tz = timezone(name)
        if not hasattr(tz, '_tzinfos'):
            continue
        for (utcoffset, daylight, tzname), _ in tz._tzinfos.items():
            if utcoffset == local_offset and tzname == localtz:
                return name

    return None


# The following code provides some sample timezone implementations
# (i.e. tzinfo subclasses). It has been copied (almost) verbatim
# from the official datetime module documentation:
# http://docs.python.org/library/datetime.html#tzinfo-objects

ZERO = timedelta(0)
HOUR = timedelta(hours=1)


# A UTC class.

class UTC(tzinfo):
    """UTC"""

    def utcoffset(self, dt):
        return ZERO

    def tzname(self, dt):
        return "UTC"

    def dst(self, dt):
        return ZERO

utc = UTC()


# A class capturing the platform's idea of local time.

STDOFFSET = timedelta(seconds=-time.timezone)
if time.daylight:
    DSTOFFSET = timedelta(seconds=-time.altzone)
else:
    DSTOFFSET = STDOFFSET

DSTDIFF = DSTOFFSET - STDOFFSET


class LocalTimezone(tzinfo):

    def utcoffset(self, dt):
        if self._isdst(dt):
            return DSTOFFSET
        else:
            return STDOFFSET

    def dst(self, dt):
        if self._isdst(dt):
            return DSTDIFF
        else:
            return ZERO

    def tzname(self, dt):
        return time.tzname[self._isdst(dt)]

    def _isdst(self, dt):
        tt = (dt.year, dt.month, dt.day,
              dt.hour, dt.minute, dt.second,
              dt.weekday(), 0, 0)
        stamp = time.mktime(tt)
        tt = time.localtime(stamp)
        return tt.tm_isdst > 0

local = LocalTimezone()


# A monotonic clock, i.e., the time elapsed since an arbitrary and
# unknown starting moment, that doesn't change when setting the real
# clock time. It is guaranteed to be increasing (it's not clear to me
# whether to very close call can return the same number).
# Taken from http://bugs.python.org/file19461/monotonic.py
if platform.system() not in ('Windows', 'Darwin'):
    from ctypes import Structure, c_long, CDLL, c_int, POINTER, byref
    from ctypes.util import find_library

    if platform.system() == 'FreeBSD':
        CLOCK_MONOTONIC = 4
    else:
        CLOCK_MONOTONIC = 1

    class timespec(Structure):
        _fields_ = [
            ('tv_sec', c_long),
            ('tv_nsec', c_long)
            ]

    librt_filename = find_library('rt')
    if not librt_filename:
        # On Debian Lenny (Python 2.5.2), find_library() is unable
        # to locate /lib/librt.so.1
        librt_filename = 'librt.so.1'
    librt = CDLL(librt_filename)
    _clock_gettime = librt.clock_gettime
    _clock_gettime.argtypes = (c_int, POINTER(timespec))

    def monotonic_time():
        """
        Clock that cannot be set and represents monotonic time since some
        unspecified starting point. The unit is a second.
        """
        t = timespec()
        _clock_gettime(CLOCK_MONOTONIC, byref(t))
        return t.tv_sec + t.tv_nsec / 1e9
else:
    try:
        from win32api import GetTickCount

        def monotonic_time():
            return GetTickCount / 1000.0

    except ImportError:
        from time import time as monotonic_time

########NEW FILE########
__FILENAME__ = eventsource
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import re
import time
from collections import deque
from weakref import WeakSet

import six

from gevent import Timeout
from gevent.queue import Queue, Empty
from gevent.pywsgi import WSGIHandler

from werkzeug.wrappers import Request
from werkzeug.exceptions import NotAcceptable


__all__ = [
    "format_event",
    "Publisher", "Subscriber", "EventSource",
    ]


def format_event(id_, event, data):
    """Format the parameters to be sent on an event stream.

    Produce a text that, written on a Server-Sent Events connection,
    will cause the client to receive an event of the given type with
    the given data, and set the last event ID to the given id. No colon
    nor line breaks (i.e. "\\r\\n", "\\r", "\\n") are allowed in the
    event name and all line breaks in the event data will become "\\n".

    id_ (unicode): the ID of the event.
    event (unicode): the name of the event, or None.
    data (unicode): the content of the event, or None.

    return (bytes): the value to write on the stream.

    raise (TypeError): if any parameter isn't unicode.
    raise (ValueError): if event contains illegal characters.

    """
    if not isinstance(id_, six.text_type):
        raise TypeError("Id isn't unicode.")
    result = [b"id:%s" % id_.encode('utf-8')]

    if event is not None and event != "message":
        if not isinstance(event, six.text_type):
            raise TypeError("Event isn't unicode.")
        if not set("\r\n:").isdisjoint(event):
            raise ValueError("Event cannot contain '\\r', '\\n' or ':'.")
        result += [b"event:%s" % event.encode('utf-8')]

    if data is not None:
        if not isinstance(data, six.text_type):
            raise TypeError("Data isn't unicode.")
        for line in re.split("\r\n|(?<!\r)\n|\r(?!\n)", data):
            result += [b"data:%s" % line.encode('utf-8')]

    result += [b'\n']

    return b'\n'.join(result)


class Publisher(object):
    """The publish part of a pub-sub broadcast system.

    Publish-subscribe is actually an improper name, as there's just one
    "topic", making it a simple broadcast system. The publisher class
    is responsible for receiving messages to be sent, keeping them in
    a cache for a while, instantiating subscribers, each with its own
    queue, and pushing new messages to all these queues.

    """
    def __init__(self, size):
        """Instantiate a new publisher.

        size (int): the number of messages to keep in cache.

        """
        # We use a deque as it's efficient to add messages to one end
        # and have the ones at the other end be dropped when the total
        # number exceeds the given limit.
        self._cache = deque(maxlen=size)
        # We use a WeakSet as we want queues to be vanish automatically
        # when no one else is using (i.e. fetching from) them.
        self._sub_queues = WeakSet()

    def put(self, event, data):
        """Dispatch a new item to all subscribers.

        See format_event for details about the parameters.

        event (unicode): the type of event the client will receive.
        data (unicode): the associated data.

        """
        # Number of microseconds since epoch.
        key = int(time.time() * 1000000)
        msg = format_event("%x" % key, event, data)
        # Put into cache.
        self._cache.append((key, msg))
        # Send to all subscribers.
        for queue in self._sub_queues:
            queue.put(msg)

    def get_subscriber(self, last_event_id=None):
        """Obtain a new subscriber.

        The returned subscriber will receive all messages after the one
        with the given index (if they are still in the cache).

        last_event_id (unicode): the ID of the last message the client
            did receive, to request the one generated since then to be
            sent again. If not given no past message will be sent.

        return (Subscriber): a new subscriber instance.

        """
        queue = Queue()
        # If a valid last_event_id is provided see if cache can supply
        # missed events.
        if last_event_id is not None and \
                re.match("^[0-9A-Fa-f]+$", last_event_id):
            last_event_key = int(last_event_id, 16)
            if len(self._cache) > 0 and last_event_key >= self._cache[0][0]:
                # All missed events are in cache.
                for key, msg in self._cache:
                    if key > last_event_key:
                        queue.put(msg)
            else:
                # Some events may be missing. Ask to reinit.
                queue.put(b"event:reinit\n\n")
        # Store the queue and return a subscriber bound to it.
        self._sub_queues.add(queue)
        return Subscriber(queue)


class Subscriber(object):
    """The subscribe part of a pub-sub broadcast system.

    This class receives the messages sent to the Publisher that created
    it.

    """
    def __init__(self, queue):
        """Create a new subscriber.

        Make it wait for messages on the given queue, managed by the
        Publisher.

        queue (Queue): a message queue.

        """
        self._queue = queue

    def get(self):
        """Retrieve new messages.

        Obtain all messages that were put in the associated publisher
        since this method was last called, or (on the first call) since
        the last_event_id given to get_subscriber.

        return ([objects]): the items put in the publisher, in order
            (actually, returns a generator, not a list).

        raise (OutdatedError): if some of the messages it's supposed to
            retrieve have already been removed from the cache.

        """
        # Block until we have something to do.
        self._queue.peek()
        # Fetch all items that are immediately available.
        try:
            while True:
                yield self._queue.get_nowait()
        except Empty:
            pass


class EventSource(object):
    """A class that implements a Server-Sent Events [1] handler.

    This class is intended to be extended: it takes charge of all the
    hard work of managing a stream of events, leaving to the subclass
    only the fun of determining which events to send.

    Server-Sent Events are a way to make server push using long-polling
    over HTTP connections, preferably with chunked transfer encoding.
    This use wasn't a design goal of WSGI but this class, which is a
    WSGI application, should be able to manage it. It has been written
    to work on a gevent.pywsgi server, but should handle other servers
    as well.

    """
    _GLOBAL_TIMEOUT = 600
    _WRITE_TIMEOUT = 30
    _PING_TIMEOUT = 15

    _CACHE_SIZE = 250

    def __init__(self):
        """Create an event source.

        """
        self._pub = Publisher(self._CACHE_SIZE)

    def send(self, event, data):
        """Send the event to the stream.

        Intended for subclasses to push new events to clients. See
        format_event for the meaning of the parameters.

        event (unicode): the type of the event.
        data (unicode): the data of the event.

        """
        self._pub.put(event, data)

    def __call__(self, environ, start_response):
        """Execute this instance as a WSGI application.

        See the PEP for the meaning of parameters. The separation of
        __call__ and wsgi_app eases the insertion of middlewares.

        """
        return self.wsgi_app(environ, start_response)

    def wsgi_app(self, environ, start_response):
        """Execute this instance as a WSGI application.

        See the PEP for the meaning of parameters. The separation of
        __call__ and wsgi_app eases the insertion of middlewares.

        """
        request = Request(environ)
        request.encoding_errors = "strict"

        # The problem here is that we'd like to send an infinite stream
        # of events, but WSGI has been designed to handle only finite
        # responses. Hence, to do this we will have to "abuse" the API
        # a little. This works well with gevent's pywsgi implementation
        # but it may not with others (still PEP-compliant). Therefore,
        # just to be extra-safe, we will terminate the response anyway,
        # after a long timeout, to make it finite.

        # The first such "hack" is the mechanism to trigger the chunked
        # transfer-encoding. The PEP states just that "the server *may*
        # use chunked encoding" to send each piece of data we give it,
        # if we don't specify a Content-Length header and if both the
        # client and the server support it. Accoring to the HTTP spec.
        # all (and only) HTTP/1.1 compliant clients have to support it.
        # We'll assume that the server software supports it too, and
        # actually uses it (gevent does!) even if we have no way to
        # check it. We cannot try to force such behavior as the PEP
        # doesn't even allow us to set the Transfer-Encoding header.

        # The second abuse is the use of the write() callable, returned
        # by start_response, even if the PEP strongly discourages its
        # use in new applications. We do it because we need a way to
        # detect when the client disconnects, and we hope to achieve
        # this by seeing when a call to write() fails, i.e. raises an
        # exception. This behavior isn't documented by the PEP, but it
        # seems reasonable and it's present in gevent (which raises a
        # socket.error).

        # The third non-standard behavior that we expect (related to
        # the previous one) is that no one in the application-to-client
        # chain does response buffering: neither any middleware not the
        # server (gevent doesn't!). This should also hold outside the
        # server realm (i.e. no proxy buffering) but that's definetly
        # not our responsibility.

        # The fourth "hack" is to avoid an error to be printed on the
        # logs. If the client terminates the connection, we catch and
        # silently ignore the exception and return gracefully making
        # the server try to write the last zero-sized chunk (used to
        # mark the end of the stream). This will fail and produce an
        # error. To avoid this we detect if we're running on a gevent
        # server and make it "forget" this was a chunked response.

        # Check if the client will understand what we will produce.
        if request.accept_mimetypes.quality(b"text/event-stream") <= 0:
            return NotAcceptable()(environ, start_response)

        # Initialize the response and get the write() callback. The
        # Cache-Control header is useless for conforming clients, as
        # the spec. already imposes that behavior on them, but we set
        # it explictly to avoid unwanted caching by unaware proxies and
        # middlewares.
        write = start_response(
            b"200 OK",
            [(b"Content-Type", b"text/event-stream; charset=utf-8"),
             (b"Cache-Control", b"no-cache")])

        # This is a part of the fourth hack (see above).
        if hasattr(start_response, "__self__") and \
                isinstance(start_response.__self__, WSGIHandler):
            handler = start_response.__self__
        else:
            handler = None

        # One-shot means that we will terminate the request after the
        # first batch of sent events. We do this when we believe the
        # client doesn't support chunked transfer. As this encoding has
        # been introduced in HTTP/1.1 (as mandatory!) we restrict to
        # requests in that HTTP version. Also, if it comes from an
        # XMLHttpRequest it has been probably sent from a polyfill (not
        # from the native browser implementation) which will be able to
        # read the response body only when it has been fully received.
        if environ[b"SERVER_PROTOCOL"] != b"HTTP/1.1" or request.is_xhr:
            one_shot = True
        else:
            one_shot = False

        # As for the Server-Sent Events [1] spec., this is the way for
        # the client to tell us the ID of the last event it received
        # and to ask us to send it the ones that happened since then.
        # [1] http://www.w3.org/TR/eventsource/
        # The spec. requires implementations to retry the connection
        # when it fails, adding the "Last-Event-ID" HTTP header. But in
        # case of an error they stop, and we have to (manually) delete
        # the EventSource and create a new one. To obtain that behavior
        # again we give the "last_event_id" as a URL query parameter
        # (with lower priority, to have the header override it).
        last_event_id = request.headers.get(b"Last-Event-ID",
                                            type=lambda x: x.decode('utf-8'))
        if last_event_id is None:
            last_event_id = request.args.get(b"last_event_id",
                                             type=lambda x: x.decode('utf-8'))

        # We subscribe to the publisher to receive events.
        sub = self._pub.get_subscriber(last_event_id)

        # Send some data down the pipe. We need that to make the user
        # agent announce the connection (see the spec.). Since it's a
        # comment it will be ignored.
        write(b":\n")

        # XXX We could make the client change its reconnection timeout
        # by sending a "retry:" line.

        # As a last line of defence from very bad-behaving servers we
        # don't want to the request to last longer than _GLOBAL_TIMEOUT
        # seconds (see above). We use "False" to just cause the control
        # exit the with block, instead of raising an exception.
        with Timeout(self._GLOBAL_TIMEOUT, False):
            # Repeat indefinitely.
            while True:
                # Proxies often have a read timeout. We try not to hit
                # it by not being idle for more than _PING_TIMEOUT
                # seconds, sending a ping (i.e. a comment) if there's
                # no real data.
                try:
                    with Timeout(self._PING_TIMEOUT):
                        data = b"".join(sub.get())
                        got_sth = True
                except Timeout:
                    data = b":\n"
                    got_sth = False

                try:
                    with Timeout(self._WRITE_TIMEOUT):
                        write(data)
                # The PEP doesn't tell what has to happen when a write
                # fails. We're conservative, and allow any unexpected
                # event to interrupt the request. We hope it's enough
                # to detect when the client disconnects. It is with
                # gevent, which raises a socket.error. The timeout (we
                # catch that too) is just an extra precaution.
                except Exception:
                    # This is part of the fourth hack (see above).
                    if handler is not None:
                        handler.response_use_chunked = False
                    break

                # If we decided this is one-shot, stop the long-poll as
                # soon as we sent the client some real data.
                if one_shot and got_sth:
                    break

        # An empty iterable tells the server not to send anything.
        return []

########NEW FILE########
__FILENAME__ = isocodes
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import os.path
from xml.sax import parse
from xml.sax.handler import ContentHandler


__all__ = [
    "is_language_code", "translate_language_code",
    "is_country_code", "translate_country_code",
    "is_language_country_code", "translate_country_code",
    ]


# We need the config to access the iso_codes_prefix value. It would be
# better not to depend on cms (i.e. be standalone). The best solution
# would be to get the prefix at installation-time (using pkgconfig),
# like many C libraries/applications do, and store it somehow. Yet, I
# don't know what's the best way to do this in Python...
from cms import config


class _make_dict (ContentHandler):
    def __init__(self, path, key, value, result):
        self.path = path
        self.key = key
        self.value = value
        self.index = 0
        self.result = result

    def startElement(self, name, attrs):
        if self.index < len(self.path) and name == self.path[self.index]:
            self.index += 1
        if self.index == len(self.path):
            if self.key in attrs and self.value in attrs:
                self.result[attrs[self.key]] = attrs[self.value]

    def endElement(self, name):
        if self.index > 0 and name == self.path[self.index - 1]:
            self.index -= 1


_language_codes = dict()
_country_codes = dict()

parse(os.path.join(config.iso_codes_prefix,
                   'share', 'xml', 'iso-codes', 'iso_639.xml'),
      _make_dict(["iso_639_entries", "iso_639_entry"],
                 "iso_639_1_code", "name", _language_codes))
parse(os.path.join(config.iso_codes_prefix,
                   'share', 'xml', 'iso-codes', 'iso_3166.xml'),
      _make_dict(["iso_3166_entries", "iso_3166_entry"],
                 "alpha_2_code", "name", _country_codes))


def is_language_code(code):
    return code in _language_codes


def translate_language_code(code, locale):
    if code not in _language_codes:
        raise ValueError("Language code not recognized.")

    return locale.translate(_language_codes[code]).split(';')[0]


def is_country_code(code):
    return code in _country_codes


def translate_country_code(code, locale):
    if code not in _country_codes:
        raise ValueError("Country code not recognized.")

    return locale.translate(_country_codes[code]).split(';')[0]


def is_language_country_code(code):
    tokens = code.split('_')
    if len(tokens) != 2 or \
            tokens[0] not in _language_codes or \
            tokens[1] not in _country_codes:
        return False
    return True


def translate_language_country_code(code, locale):
    tokens = code.split('_')
    if len(tokens) != 2 or \
            tokens[0] not in _language_codes or \
            tokens[1] not in _country_codes:
        raise ValueError("Language and country code not recognized.")

    return "%s (%s)" % (translate_language_code(tokens[0], locale),
                        translate_country_code(tokens[1], locale))

########NEW FILE########
__FILENAME__ = mimetypes
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import os.path
import fnmatch
import mimetypes
from xml import sax
from xml.sax.handler import ContentHandler
from xml.dom import XML_NAMESPACE as _XML_NS


__all__ = [
    "get_icon_for_type", "get_name_for_type",
    "get_type_for_file_name",
    ]


_XDG_NS = "http://www.freedesktop.org/standards/shared-mime-info"

# We need the config to access the shared_mime_info_prefix value. It
# would be better not to depend on cms (i.e. be standalone). The best
# solution would be to get the prefix at installation-time (using
# pkgconfig), like many C libraries/applications do, and store it
# somehow. Yet, I don't know what's the best way to do this in Python...
from cms import config


# FIXME the following code doesn't take comments into account.
# FIXME the specification requires to look in XDG_DATA_HOME and
# XDG_DATA_DIRS instead of the installation dir of shared-mime-info...
# TODO use python-xdg (or similar libraries) instead of doing the
# parsing ourselves. they also provide ways to find the MIME type.

_aliases = dict(tuple(l.strip().split()) for l in
                open(os.path.join(config.shared_mime_info_prefix,
                                  "share", "mime", "aliases")).readlines())

_icons = dict(tuple(l.strip().split(':')) for l in
              open(os.path.join(config.shared_mime_info_prefix,
                                "share", "mime", "generic-icons")).readlines())

_types = list(l.strip() for l in
              open(os.path.join(config.shared_mime_info_prefix,
                                "share", "mime", "types")).readlines())

_comments = dict()


class _get_comment (ContentHandler):
    def __init__(self):
        self.inside = False
        self.result = None

    def startElementNS(self, name, qname, attrs):
        if name == (_XDG_NS, "comment") and \
                ((_XML_NS, "lang") not in attrs or
                 attrs[(_XML_NS, "lang")] in ["en", "en_US"]):
            self.inside = True
            self.result = ''

    def endElementNS(self, name, qname):
        self.inside = False

    def characters(self, content):
        if self.inside:
            self.result += content


def get_icon_for_type(name):
    if name in _aliases:
        name = _aliases[name]
    if name not in _types:
        return None

    if name in _icons:
        return _icons[name]
    return name[:name.index('/')] + "-x-generic"


def get_name_for_type(name):
    if name in _aliases:
        name = _aliases[name]
    if name not in _types:
        return None

    if name not in _comments:
        try:
            media, subtype = name.split('/')
            path = os.path.join(config.shared_mime_info_prefix,
                                'share', 'mime', media, "%s.xml" % subtype)

            handler = _get_comment()
            parser = sax.make_parser()
            parser.setContentHandler(handler)
            parser.setFeature(sax.handler.feature_namespaces, 1)
            parser.parse(path)

            _comments[name] = handler.result
        except:
            pass

    if name in _comments:
        return _comments[name]


def get_type_for_file_name(name):
    # Provide support for some commonly used types and fallback on
    # Python's mimetypes module. In the future we could be using a
    # proper library here (i.e. an interface to shared-mime-info).
    for glob, mime in [('*.tar.gz', 'application/x-compressed-tar'),
                       ('*.tar.bz2', 'application/x-bzip-compressed-tar'),
                       ('*.c', 'text/x-csrc'),
                       ('*.h', 'text/x-chdr'),
                       ('*.cpp', 'text/x-c++src'),
                       ('*.hpp', 'text/x-c++hdr'),
                       ('*.pas', 'text/x-pascal')]:
        if fnmatch.fnmatchcase(name, glob):
            return mime
    return mimetypes.guess_type(name)[0]

########NEW FILE########
__FILENAME__ = YamlImporter
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals


def main():
    print("YamlImporter has been removed from CMS starting from version 1.1.")
    print("Please, use program Importer.")
    print("Check CMS documentation for instructions.")


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = YamlReimporter
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals


def main():
    print("YamlReimporter has been removed from CMS starting from version "
          "1.1.")
    print("Please, use program Reimporter.")
    print("Check CMS documentation for instructions.")


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = AddUser
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Bernard Blackham <bernard@largestprime.net>
# Copyright © 2010-2011 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2011 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2011 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Utility to add a user to a contest.

"""

from __future__ import absolute_import
from __future__ import print_function

import argparse
import sys

from cms.db import SessionGen, User, Contest, ask_for_contest


def add_user(contest_id, first_name, last_name, username,
             password, ip_address, email, hidden):
    with SessionGen() as session:
        contest = Contest.get_from_id(contest_id, session)
        user = User(first_name=first_name,
                    last_name=last_name,
                    username=username,
                    password=password,
                    email=email,
                    ip=ip_address,
                    hidden=hidden,
                    contest=contest)
        session.add(user)
        session.commit()


def main():
    """Parse arguments and launch process.

    """
    parser = argparse.ArgumentParser(
        description="Adds a user to a contest in CMS.")
    parser.add_argument("first_name",
                        help="first name of the user")
    parser.add_argument("last_name",
                        help="last name of the user")
    parser.add_argument("username",
                        help="username of the user")
    parser.add_argument("-c", "--contest-id",
                        help="id of contest where to add the user",
                        action="store", type=int)
    parser.add_argument("-p", "--password", help="password of the user",
                        action="store")
    parser.add_argument("-i", "--ip-address", help="ip address of the user",
                        action="store")
    parser.add_argument("-e", "--email", help="email address of the user",
                        action="store")
    parser.add_argument("-H", "--hidden", help="if the user is hidden",
                        action="store_true")
    args = parser.parse_args()

    if args.contest_id is None:
        args.contest_id = ask_for_contest()

    add_user(contest_id=args.contest_id,
             first_name=args.first_name,
             last_name=args.last_name,
             username=args.username,
             password=args.password,
             ip_address=args.ip_address,
             email=args.email,
             hidden=args.hidden)

    return 0


if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = BaseLoader
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Base class for deriving loaders.

"""

from __future__ import absolute_import
from __future__ import print_function


class Loader(object):
    """Base class for deriving loaders.

    Each loader must extend this class and support the following
    access pattern:

      * The class method detect() can be called at any time.

      * Once a loader is instatiated, get_contest() can be called on
        it, only once.

      * After get_contest() has been called, at the caller's will,
        get_task() and get_user() can be called, in any order and for
        how many times the caller want. The resource intensive
        operations that are not needed for get_contest() are better
        left in get_task() or get_user(), so that no time is wasted if
        the caller isn't interested in users or tasks.

    """

    # Short name of this loader, meant to be a unique identifier.
    short_name = None

    # Description of this loader, meant to be human readable.
    description = None

    def __init__(self, path, file_cacher):
        """Initialize the Loader.

        path (str): the filesystem location given by the user.
        file_cacher (FileCacher): the file cacher to use to store
                                  files (i.e. statements, managers,
                                  testcases, etc.).

        """
        self.path = path
        self.file_cacher = file_cacher

    @classmethod
    def detect(cls, path):
        """Detect whether this loader is able to interpret a path.

        If the loader chooses to not support autodetection, just
        always return False.

        path (string): the path to scan.

        return (bool): True if the loader is able to interpret the
                       given path.

        """
        raise NotImplementedError("Please extend Loader")

    def get_contest(self):
        """Produce a Contest object.

        Do what is needed (i.e. search directories and explore files
        in the location given to the constructor) to produce a Contest
        object. Also get a minimal amount of information on tasks and
        users, at least enough to produce the list of all task names
        and the list of all usernames.

        return (tuple): the Contest object and the two lists described
                        above.

        """
        raise NotImplementedError("Please extend Loader")

    def get_user(self, username):
        """Produce a User object.

        username (string): the username.

        return (User): the User object.

        """
        raise NotImplementedError("Please extend Loader")

    def get_task(self, name):
        """Produce a Task object.

        name (string): the task name.

        return (Task): the Task object.

        """
        raise NotImplementedError("Please extend Loader")

    def has_changed(self, name):
        """Detect if a Task has been changed since its last import.

        This is expected to happen by saving, at every import, some
        piece of data about the last importation time. Then, when
        has_changed() is called, such time is compared with the last
        modification time of the files describing the task. Anyway,
        the Loader may choose the heuristic better suited for its
        case.

        If this task is being imported for the first time or if the
        Loader decides not to support changes detection, just return
        True.

        name (string): the task name.

        return (bool): True if the task was changed, False otherwise.

        """
        raise NotImplementedError("Please extend Loader")

########NEW FILE########
__FILENAME__ = ComputeComplexity
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/.
# Copyright © 2012 Bernard Blackham <bernard@largestprime.net>
# Copyright © 2010-2011 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2011 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2011 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""This module tries to estimate the complexity of the submissions
related to a single task, using a simple least mean square method.

In particular, it computes the best coefficients of an expression of
the form N^i log_2^j(N) (2^N)^l, where i, k, l are small integers, to
approximate the times obtained by the solution, where N is the number
of bytes of the input file.

More precise results can be obtained using a task-specific model for
the dimensions of the testcases.

"""

from __future__ import absolute_import
from __future__ import print_function

import argparse
import imp
import numpy
import sys

from cms.db import SessionGen, Task
from cms.db.filecacher import FileCacher


MAXP = 4
MAXL = 1
MAXE = 1


def ijk_to_idx(i, j, k):
    """Return the linear index associated to the triple i, j, k."""
    return (MAXE + 1) * ((MAXL + 1) * i + j) + k


def ijk_to_func(i, j, k):
    """Return the function associated to the triple i, j, k."""
    return lambda x: (x ** i) * (numpy.log2(x) ** j) * ((2 ** x) ** k)


class FileLengther(object):
    """A simple file-like object to count the bytes written to the
    file.

    """
    def __init__(self):
        """Initialize the file object."""
        self.bytes = 0

    def open(self, unused_name, unused_mode):
        """Initialize the file object."""
        self.bytes = 0

    def write(self, string):
        """Add string to the content of the file."""
        self.bytes += len(string)

    def tell(self):
        """Return the current position in the file."""
        return self.bytes

    def close(self):
        """Close the file object."""
        self.bytes = 0


def file_length(digest, file_cacher=None, file_lengther=None):
    """Compute the length of the file identified by digest.

    digest (string): the digest of the file.
    file_cacher (FileCacher): the cacher to use, or None.
    file_lengther (type): a File-like object that tell the dimension
        of the input (see example above for how to write one).

    return (int): the length of the tile.

    """
    if file_cacher is None:
        file_cacher = FileCacher()
    if file_lengther is None:
        file_lengther = FileLengther
    lengther = file_lengther()
    file_cacher.get_file_to_fobj(digest, lengther)
    return lengther.tell()


def complexity_to_string(complexity):
    """Given a tuple, return a string representing the complexity.

    complexity ([int]): a tuple of exponents.

    return (string): the complexity as a string.

    """
    string = ""
    if complexity == (0, 0, 0):
        string = "1"
    else:
        if complexity[0] > 0:
            string += " x^%d" % complexity[0]
        if complexity[1] > 0:
            string += " log^%d(x)" % complexity[1]
        if complexity[2] > 0:
            string += " (2^x)^%d" % complexity[2]
    return string


def extract_meaningful_points(testcases_lengths, submission):
    """Extract the meaningful points (consider the most expensive of
    the ones with common dimension, and throw away the ones without a
    time).

    testcases_lengths ([float]): the dimensions of the testcases.
    submission (Submission): the submission to grade.

    return (([float], [float])): x and y coordinates of the points.

    """
    points_x = []
    points_y = []
    last_length = -1
    for idx, length in enumerate(testcases_lengths):
        evaluation = submission.evaluations[idx]
        if float(evaluation.outcome) == 1.0 and \
                evaluation.execution_time is not None:
            if length == last_length:
                points_y[-1] = max(points_y[-1], evaluation.execution_time)
            else:
                points_x.append(length)
                points_y.append(evaluation.execution_time)
                last_length = length
    return points_x, points_y


def extract_complexity_submission(testcases_lengths, submission):
    """Extract the complexity of a submission, writing two files in
    the cwd, sub_<id>.dat and sub_<id>.info. In the first, there are
    three columns, X, Y, YP, where X is the dimension of the testcase,
    Y is the time used by the submission and YP is the time predicted
    by the complexity model; in the seconds, there are the
    coefficients and the residues computed for all possible
    complexities. The first can be plotted using gnuplot:

    plot "sub_<id>.dat" using 1:2, "sub_<id>.dat" using 1:3;

    testcases_lengths ([float]): the dimensions of the testcases.
    submission (Submission): the submission to grade.

    return ([float, (int), float]): score, tuple representing the
                                    complexity, confidence.

    """
    result = [None, None, None]

    points_x, points_y = extract_meaningful_points(testcases_lengths,
                                                   submission)
    print(submission.user.username, len(points_x))
    if len(points_x) <= 6:
        return result

    # Rescaling.
    x_scale = max(points_x)
    points_x = [x * 1.0 / x_scale for x in points_x]
    y_scale = max(points_y)
    if y_scale > 0:
        points_y = [y * 1.0 / y_scale for y in points_y]

    res = []
    residues = []
    best_residue = -1
    best_idxs = (-1, -1, -1)
    sbest_residue = -1

    for i in xrange(MAXP + 1):
        for j in xrange(MAXL + 1):
            for k in xrange(MAXE + 1):
                matrix = []
                for point_x in points_x:
                    matrix.append([ijk_to_func(i, j, k)(point_x)])
                points_y = numpy.array(points_y)
                matrix = numpy.vstack(matrix)
                res.append(numpy.linalg.lstsq(matrix, points_y)[0][0])

                residues.append(0.0)
                for idx, point_y in enumerate(points_y):
                    residues[-1] += (matrix[idx][0] * res[-1] - point_y) ** 2
                if best_residue == -1 or best_residue > residues[-1]:
                    sbest_residue = best_residue
                    best_residue = residues[-1]
                    best_idxs = (i, j, k)
                elif sbest_residue == -1 or sbest_residue > residues[-1]:
                    sbest_residue = residues[-1]

    result[0] = submission.score
    result[1] = best_idxs

    with open("sub_%s.info" % submission.id, "wt") as info:
        for i in xrange(MAXP + 1):
            for j in xrange(MAXL + 1):
                for k in xrange(MAXE + 1):
                    info.write(
                        "%+20.13lf x^%d log^%d(x) (2^x)^%d  -->  %+20.13lf\n" %
                        (res[ijk_to_idx(i, j, k)], i, j, k,
                         residues[ijk_to_idx(i, j, k)]))
        info.write("Complexity: %s\n" % complexity_to_string(best_idxs))
        if sbest_residue != 0.0:
            confidence = (100.0 * (1.0 - best_residue / sbest_residue))
            info.write("Confidence: %5.2lf (%20.13lf, %20.13lf)\n" % (
                confidence, best_residue, sbest_residue))
            result[2] = confidence
        if submission.score is not None:
            info.write("Score: %3d\n" % submission.score)

    computed_y = []
    for point_x in points_x:
        i, j, k = best_idxs
        computed_y.append(ijk_to_func(i, j, k)(point_x) *
                          res[ijk_to_idx(i, j, k)])

    with open("sub_%s.dat" % submission.id, "wt") as dat:
        for point_x, point_y, computed_y in zip(points_x, points_y,
                                                computed_y):
            dat.write("%15.8lf %+15.8lf %+15.8lf\n" % (point_x * x_scale,
                                                       point_y * y_scale,
                                                       computed_y * y_scale))
    print(submission.user.username, result)
    return result


def extract_complexity(task_id, file_lengther=None):
    """Extract the complexity of all submissions of the task. The
    results are stored in a file task_<id>.info

    task_id (int): the id of the task we are interested in.
    file_lengther (type): a File-like object that tell the dimension
        of the input (see example above for how to write one).

    return (int): 0 if operation was successful.

    """
    with SessionGen() as session:
        task = Task.get_from_id(task_id, session)
        if task is None:
            return -1

        # Extracting the length of the testcase.
        file_cacher = FileCacher()
        testcases_lengths = [file_length(testcase.input,
                                         file_cacher, file_lengther)
                             for testcase in task.testcases]
        file_cacher.purge_cache()

        # Compute the complexity of the solutions.
        with open("task_%s.info" % task_id, "wt") as info:
            for submission in task.contest.get_submissions():
                if submission.task_id == task_id and \
                        submission.evaluated():
                    print(submission.user.username)
                    result = extract_complexity_submission(testcases_lengths,
                                                           submission)
                    if result[1] is None:
                        continue
                    info.write("Submission: %s" % submission.id)
                    info.write(" - user: %15s" % submission.user.username)
                    info.write(" - task: %s" % task.name)
                    if result[0] is not None:
                        info.write(" - score: %6.2lf" % result[0])
                    info.write(" - complexity: %20s" %
                               complexity_to_string(result[1]))
                    if result[2] is not None:
                        info.write(" - confidence %5.1lf" % result[2])
                    info.write("\n")

    return 0


def main():
    """Parse arguments and launch process.

    """
    parser = argparse.ArgumentParser(
        description="Extract the complexity of submissions of a task.")
    parser.add_argument("task_id",
                        help="id of the task in the DB")
    parser.add_argument("-l", "--lengther",
                        help="filename of a Python source "
                        "with a FileLengther class")
    args = parser.parse_args()

    file_lengther = None
    if args.lengther is not None:
        if args.lengther.endswith(".py"):
            args.lengther = args.lengther[:-3]
        try:
            file_, file_name, description = imp.find_module(args.lengther)
            module = imp.load_module(args.lengther,
                                     file_, file_name, description)
            file_lengther = module.FileLengther
        except ImportError as error:
            print("Unable to import module %s.\n%r" % (args.lengther, error))
            return -1
        except AttributeError:
            print("Module %s must have a class named FileLengther." %
                  args.lengther)

    return extract_complexity(int(args.task_id), file_lengther=file_lengther)


if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = ComputeComplexityLengtherExample
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/.
# Copyright © 2012 Bernard Blackham <bernard@largestprime.net>
# Copyright © 2010-2011 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2011 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2011 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""This is an example of a FileLengther, that is a file-like object
that receives a file (via write) and uses tell() to give the 'size' of
the file as an input file. In this example, we return the first
integer in the file as the measure for the complexity of the input.

"""

from __future__ import absolute_import
from __future__ import print_function


class FileLengther(object):
    """A simple file-like object to extract the first number of the
    file.

    """
    def __init__(self):
        """Initialize the file object."""
        self.string = ""
        self.state = 0

    def open(self, unused_name, unused_mode):
        """Initialize the file object."""
        self.string = ""
        self.state = 0

    def write(self, string):
        """Add string to the content of the file."""
        if self.state == 0:
            self.string += string
            if " " in self.string or "\n" in self.string:
                self.state = 1

    def tell(self):
        """Return the current position in the file."""
        return int(self.string.split()[0])

    def close(self):
        """Close the file object."""
        self.string = ""
        self.state = 0

########NEW FILE########
__FILENAME__ = ContestExporter
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2014 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""This service exports every data about the contest that CMS
knows. The process of exporting and importing again should be
idempotent.

"""

from __future__ import absolute_import
from __future__ import print_function

# We enable monkey patching to make many libraries gevent-friendly
# (for instance, urllib3, used by requests)
import gevent.monkey
gevent.monkey.patch_all()

import argparse
import io
import json
import logging
import os
import tarfile
import tempfile

from sqlalchemy.types import \
    Boolean, Integer, Float, String, Unicode, DateTime, Interval, Enum

from cms.db import version as model_version
from cms.db import SessionGen, Contest, ask_for_contest, \
    Submission, UserTest, SubmissionResult, UserTestResult, \
    RepeatedUnicode
from cms.db.filecacher import FileCacher
from cms.io.GeventUtils import rmtree

from cmscontrib import sha1sum
from cmscommon.datetime import make_timestamp


logger = logging.getLogger(__name__)


def get_archive_info(file_name):

    """Return information about the archive name.

    file_name (string): the file name of the archive to analyze.

    return (dict): dictionary containing the following keys:
                   "basename", "extension", "write_mode"

    """

    # TODO - This method doesn't seem to be a masterpiece in terms of
    # cleanness...
    ret = {"basename": "",
           "extension": "",
           "write_mode": "",
           }
    if not (file_name.endswith(".tar.gz")
            or file_name.endswith(".tar.bz2")
            or file_name.endswith(".tar")
            or file_name.endswith(".zip")):
        return ret

    if file_name.endswith(".tar"):
        ret["basename"] = os.path.basename(file_name[:-4])
        ret["extension"] = "tar"
        ret["write_mode"] = "w:"
    elif file_name.endswith(".tar.gz"):
        ret["basename"] = os.path.basename(file_name[:-7])
        ret["extension"] = "tar.gz"
        ret["write_mode"] = "w:gz"
    elif file_name.endswith(".tar.bz2"):
        ret["basename"] = os.path.basename(file_name[:-8])
        ret["extension"] = "tar.bz2"
        ret["write_mode"] = "w:bz2"
    elif file_name.endswith(".zip"):
        ret["basename"] = os.path.basename(file_name[:-4])
        ret["extension"] = "zip"
        ret["write_mode"] = ""

    return ret


class ContestExporter(object):

    """This service exports every data about the contest that CMS
    knows. The process of exporting and importing again should be
    idempotent.

    """

    def __init__(self, contest_id, export_target,
                 dump_files, dump_model, skip_generated,
                 skip_submissions, skip_user_tests):
        self.contest_id = contest_id
        self.dump_files = dump_files
        self.dump_model = dump_model
        self.skip_generated = skip_generated
        self.skip_submissions = skip_submissions
        self.skip_user_tests = skip_user_tests
        self.export_target = export_target

        # If target is not provided, we use the contest's name.
        if export_target == "":
            with SessionGen() as session:
                contest = Contest.get_from_id(self.contest_id, session)
                if contest is None:
                    logger.critical("Please specify a valid contest id.")
                    self.contest_id = None
                else:
                    self.export_target = "dump_%s.tar.gz" % contest.name
                    logger.warning("export_target not given, using \"%s\""
                                   % self.export_target)

        self.file_cacher = FileCacher()

    def do_export(self):
        """Run the actual export code."""
        if self.contest_id is None:
            return

        logger.info("Starting export.")

        export_dir = self.export_target
        archive_info = get_archive_info(self.export_target)

        if archive_info["write_mode"] != "":
            # We are able to write to this archive.
            if os.path.exists(self.export_target):
                logger.critical("The specified file already exists, "
                                "I won't overwrite it.")
                return False
            export_dir = os.path.join(tempfile.mkdtemp(),
                                      archive_info["basename"])

        logger.info("Creating dir structure.")
        try:
            os.mkdir(export_dir)
        except OSError:
            logger.critical("The specified directory already exists, "
                            "I won't overwrite it.")
            return False

        files_dir = os.path.join(export_dir, "files")
        descr_dir = os.path.join(export_dir, "descriptions")
        os.mkdir(files_dir)
        os.mkdir(descr_dir)

        with SessionGen() as session:

            contest = Contest.get_from_id(self.contest_id, session)

            # Export files.
            if self.dump_files:
                logger.info("Exporting files.")
                files = contest.enumerate_files(self.skip_submissions,
                                                self.skip_user_tests,
                                                self.skip_generated)
                for file_ in files:
                    if not self.safe_get_file(file_,
                                              os.path.join(files_dir, file_),
                                              os.path.join(descr_dir, file_)):
                        return False

            # Export the contest in JSON format.
            if self.dump_model:
                logger.info("Exporting the contest to a JSON file.")

                # We use strings because they'll be the keys of a JSON
                # object; the contest will have ID 0.
                self.ids = {contest.sa_identity_key: "0"}
                self.queue = [contest]

                data = dict()
                while len(self.queue) > 0:
                    obj = self.queue.pop(0)
                    data[self.ids[obj.sa_identity_key]] = \
                        self.export_object(obj)

                # Specify the "root" of the data graph
                data["_objects"] = ["0"]

                data["_version"] = model_version

                with io.open(os.path.join(export_dir,
                                          "contest.json"), "wb") as fout:
                    json.dump(data, fout, encoding="utf-8",
                              indent=4, sort_keys=True)

        # If the admin requested export to file, we do that.
        if archive_info["write_mode"] != "":
            archive = tarfile.open(self.export_target,
                                   archive_info["write_mode"])
            archive.add(export_dir, arcname=archive_info["basename"])
            archive.close()
            rmtree(export_dir)

        logger.info("Export finished.")

        return True

    def get_id(self, obj):
        obj_key = obj.sa_identity_key
        if obj_key not in self.ids:
            # We use strings because they'll be the keys of a JSON object
            self.ids[obj_key] = str(len(self.ids))
            self.queue.append(obj)

        return self.ids[obj_key]

    def export_object(self, obj):

        """Export the given object, returning a JSON-encodable dict.

        The returned dict will contain a "_class" item (the name of the
        class of the given object), an item for each column property
        (with a value properly translated to a JSON-compatible type)
        and an item for each relationship property (which will be an ID
        or a collection of IDs).

        The IDs used in the exported dict aren't related to the ones
        used in the DB: they are newly generated and their scope is
        limited to the exported file only. They are shared among all
        classes (that is, two objects can never share the same ID, even
        if they are of different classes).

        If, when exporting the relationship, we find an object without
        an ID we generate a new ID, assign it to the object and append
        the object to the queue of objects to export.

        The self.skip_submissions flag controls wheter we export
        submissions (and all other objects that can be reached only by
        passing through a submission) or not.

        """

        cls = type(obj)

        data = {"_class": cls.__name__}

        for prp in cls._col_props:
            col, = prp.columns
            col_type = type(col.type)

            val = getattr(obj, prp.key)
            if col_type in \
                    [Boolean, Integer, Float, Unicode, RepeatedUnicode, Enum]:
                data[prp.key] = val
            elif col_type is String:
                data[prp.key] = \
                    val.decode('latin1') if val is not None else None
            elif col_type is DateTime:
                data[prp.key] = \
                    make_timestamp(val) if val is not None else None
            elif col_type is Interval:
                data[prp.key] = \
                    val.total_seconds() if val is not None else None
            else:
                raise RuntimeError("Unknown SQLAlchemy column type: %s"
                                   % col_type)

        for prp in cls._rel_props:
            other_cls = prp.mapper.class_

            # Skip submissions if requested
            if self.skip_submissions and other_cls is Submission:
                continue

            # Skip user_tests if requested
            if self.skip_user_tests and other_cls is UserTest:
                continue

            # Skip generated data if requested
            if self.skip_generated and other_cls in (SubmissionResult,
                                                     UserTestResult):
                continue

            val = getattr(obj, prp.key)
            if val is None:
                data[prp.key] = None
            elif isinstance(val, other_cls):
                data[prp.key] = self.get_id(val)
            elif isinstance(val, list):
                data[prp.key] = list(self.get_id(i) for i in val)
            elif isinstance(val, dict):
                data[prp.key] = \
                    dict((k, self.get_id(v)) for k, v in val.iteritems())
            else:
                raise RuntimeError("Unknown SQLAlchemy relationship type: %s"
                                   % type(val))

        return data

    def safe_get_file(self, digest, path, descr_path=None):

        """Get file from FileCacher ensuring that the digest is
        correct.

        digest (string): the digest of the file to retrieve.
        path (string): the path where to save the file.
        descr_path (string): the path where to save the description.

        return (bool): True if all ok, False if something wrong.

        """

        # TODO - Probably this method could be merged in FileCacher

        # First get the file
        try:
            self.file_cacher.get_file_to_path(digest, path)
        except Exception as error:
            logger.error("File %s could not retrieved from file server (%r)."
                         % (digest, error))
            return False

        # Then check the digest
        calc_digest = sha1sum(path)
        if digest != calc_digest:
            logger.critical("File %s has wrong hash %s."
                            % (digest, calc_digest))
            return False

        # If applicable, retrieve also the description
        if descr_path is not None:
            with io.open(descr_path, 'wt', encoding='utf-8') as fout:
                fout.write(self.file_cacher.describe(digest))

        return True


def main():
    """Parse arguments and launch process."""
    parser = argparse.ArgumentParser(description="Exporter of CMS contests.")
    parser.add_argument("-c", "--contest-id", action="store", type=int,
                        help="id of contest to export")
    group = parser.add_mutually_exclusive_group()
    group.add_argument("-f", "--files", action="store_true",
                       help="only export files, ignore database structure")
    group.add_argument("-F", "--no-files", action="store_true",
                       help="only export database structure, ignore files")
    parser.add_argument("-G", "--no-generated", action="store_true",
                        help="don't export data and files that can be "
                             "automatically generated")
    parser.add_argument("-S", "--no-submissions", action="store_true",
                        help="don't export submissions")
    parser.add_argument("-U", "--no-user-tests", action="store_true",
                        help="don't export user tests")
    parser.add_argument("export_target", nargs='?', default="",
                        help="target directory or archive for export")

    args = parser.parse_args()

    if args.contest_id is None:
        args.contest_id = ask_for_contest()

    ContestExporter(contest_id=args.contest_id,
                    export_target=args.export_target,
                    dump_files=not args.no_files,
                    dump_model=not args.files,
                    skip_generated=args.no_generated,
                    skip_submissions=args.no_submissions,
                    skip_user_tests=args.no_user_tests).do_export()


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = ContestImporter
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2014 Artem Iglikov <artem.iglikov@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""This service imports a contest from a directory that has been the
target of a ContestExport. The process of exporting and importing
again should be idempotent.

"""

from __future__ import absolute_import
from __future__ import print_function

# We enable monkey patching to make many libraries gevent-friendly
# (for instance, urllib3, used by requests)
import gevent.monkey
gevent.monkey.patch_all()

import argparse
import io
import json
import logging
import os
import tarfile
import tempfile
import zipfile
from datetime import timedelta

from sqlalchemy.types import \
    Boolean, Integer, Float, String, Unicode, DateTime, Interval, Enum

import cms.db as class_hook

from cms.db import version as model_version
from cms.db import SessionGen, init_db, drop_db, Submission, UserTest, \
    SubmissionResult, UserTestResult, RepeatedUnicode
from cms.db.filecacher import FileCacher
from cms.io.GeventUtils import rmtree

from cmscontrib import sha1sum
from cmscommon.datetime import make_datetime


logger = logging.getLogger(__name__)


def find_root_of_archive(file_names):

    """Given a list of file names (the content of an archive) find the
    name of the root directory, i.e., the only file that would be
    created in a directory if we extract there the archive.

    file_names (list of strings): the list of file names in the
                                  archive

    return (string): the root directory, or None if unable to find
                     (for example if there is more than one).

    """

    current_root = None
    for file_name in file_names:
        if '/' not in file_name or '/' not in file_name[0:-1]:
            if current_root is None:
                current_root = file_name
            else:
                return None
    return current_root


class ContestImporter(object):

    """This service imports a contest from a directory that has been
    the target of a ContestExport. The process of exporting and
    importing again should be idempotent.

    """

    def __init__(self, drop, import_source,
                 load_files, load_model, skip_generated,
                 skip_submissions, skip_user_tests):
        self.drop = drop
        self.load_files = load_files
        self.load_model = load_model
        self.skip_generated = skip_generated
        self.skip_submissions = skip_submissions
        self.skip_user_tests = skip_user_tests

        self.import_source = import_source
        self.import_dir = import_source

        self.file_cacher = FileCacher()

    def do_import(self):
        """Run the actual import code."""
        logger.info("Starting import.")

        if not os.path.isdir(self.import_source):
            if self.import_source.endswith(".zip"):
                archive = zipfile.ZipFile(self.import_source, "r")
                file_names = archive.namelist()

                self.import_dir = tempfile.mkdtemp()
                archive.extractall(self.import_dir)
            elif self.import_source.endswith(".tar.gz") \
                    or self.import_source.endswith(".tgz") \
                    or self.import_source.endswith(".tar.bz2") \
                    or self.import_source.endswith(".tbz2") \
                    or self.import_source.endswith(".tar"):
                archive = tarfile.open(name=self.import_source)
                file_names = archive.getnames()
            elif self.import_source.endswith(".tar.xz") \
                    or self.import_source.endswith(".txz"):
                try:
                    import lzma
                except ImportError:
                    logger.critical("LZMA compression format not "
                                    "supported. Please install package "
                                    "lzma.")
                    return False
                archive = tarfile.open(
                    fileobj=lzma.LZMAFile(self.import_source))
                file_names = archive.getnames()
            else:
                logger.critical("Unable to import from %s." %
                                self.import_source)
                return False

            root = find_root_of_archive(file_names)
            if root is None:
                logger.critical("Cannot find a root directory in %s." %
                                self.import_source)
                return False

            self.import_dir = tempfile.mkdtemp()
            archive.extractall(self.import_dir)
            self.import_dir = os.path.join(self.import_dir, root)

        if self.drop:
            logger.info("Dropping and recreating the database.")
            try:
                if not (drop_db() and init_db()):
                    logger.critical("Unexpected error while dropping "
                                    "and recreating the database.",
                                    exc_info=True)
                    return False
            except Exception as error:
                logger.critical("Unable to access DB.\n%r" % error)
                return False

        with SessionGen() as session:

            # Import the contest in JSON format.
            if self.load_model:
                logger.info("Importing the contest from a JSON file.")

                with io.open(os.path.join(self.import_dir,
                                          "contest.json"), "rb") as fin:
                    # TODO - Throughout all the code we'll assume the
                    # input is correct without actually doing any
                    # validations.  Thus, for example, we're not
                    # checking that the decoded object is a dict...
                    self.datas = json.load(fin, encoding="utf-8")

                # If the dump has been exported using a data model
                # different than the current one (that is, a previous
                # one) we try to update it.
                # If no "_version" field is found we assume it's a v1.0
                # export (before the new dump format was introduced).
                dump_version = self.datas.get("_version", 0)

                if dump_version < model_version:
                    logger.warning(
                        "The dump you're trying to import has been created "
                        "by an old version of CMS (it declares data model "
                        "version %d). It may take a while to adapt it to "
                        "the current data model (which is version %d). You "
                        "can use cmsDumpUpdater to update the on-disk dump "
                        "and speed up future imports."
                        % (dump_version, model_version))

                if dump_version > model_version:
                    logger.critical(
                        "The dump you're trying to import has been created "
                        "by a version of CMS newer than this one (it "
                        "declares data model version %d) and there is no "
                        "way to adapt it to the current data model (which "
                        "is version %d). You probably need to update CMS to "
                        "handle it. It is impossible to proceed with the "
                        "importation." % (dump_version, model_version))
                    return False

                for version in range(dump_version, model_version):
                    # Update from version to version+1
                    updater = __import__(
                        "cmscontrib.updaters.update_%d" % (version + 1),
                        globals(), locals(), ["Updater"]).Updater(self.datas)
                    self.datas = updater.run()
                    self.datas["_version"] = version + 1

                assert self.datas["_version"] == model_version

                self.objs = dict()
                for id_, data in self.datas.iteritems():
                    if not id_.startswith("_"):
                        self.objs[id_] = self.import_object(data)
                for id_, data in self.datas.iteritems():
                    if not id_.startswith("_"):
                        self.add_relationships(data, self.objs[id_])

                for k, v in list(self.objs.iteritems()):

                    # Skip submissions if requested
                    if self.skip_submissions and isinstance(v, Submission):
                        del self.objs[k]

                    # Skip user_tests if requested
                    if self.skip_user_tests and isinstance(v, UserTest):
                        del self.objs[k]

                    # Skip generated data if requested
                    if self.skip_generated and \
                            isinstance(v, (SubmissionResult, UserTestResult)):
                        del self.objs[k]

                contest_id = list()
                contest_files = set()

                # Add each base object and all its dependencies
                for id_ in self.datas["_objects"]:
                    contest = self.objs[id_]

                    # We explictly add only the contest since all child
                    # objects will be automatically added by cascade.
                    # Adding each object individually would also add
                    # orphaned objects like the ones that depended on
                    # submissions or user_tests that we (possibly)
                    # removed above.
                    session.add(contest)
                    session.flush()

                    contest_id += [contest.id]
                    contest_files |= contest.enumerate_files(
                        self.skip_submissions, self.skip_user_tests,
                        self.skip_generated)

                session.commit()
            else:
                contest_id = None
                contest_files = None

            # Import files.
            if self.load_files:
                logger.info("Importing files.")

                files_dir = os.path.join(self.import_dir, "files")
                descr_dir = os.path.join(self.import_dir, "descriptions")

                files = set(os.listdir(files_dir))
                descr = set(os.listdir(descr_dir))

                if not descr <= files:
                    logger.warning("Some files do not have an associated "
                                   "description.")
                if not files <= descr:
                    logger.warning("Some descriptions do not have an "
                                   "associated file.")

                if not (contest_files is None or files <= contest_files):
                    # FIXME Check if it's because this is a light import
                    # or because we're skipping submissions or user_tests
                    logger.warning("The dump contains some files that are "
                                   "not needed by the contest.")
                if not (contest_files is None or contest_files <= files):
                    # The reason for this could be that it was a light
                    # export that's not being reimported as such.
                    logger.warning("The contest needs some files that are "
                                   "not contained in the dump.")

                # Limit import to files we actually need.
                if contest_files is not None:
                    files &= contest_files

                for digest in files:
                    file_ = os.path.join(files_dir, digest)
                    desc = os.path.join(descr_dir, digest)
                    if not self.safe_put_file(file_, desc):
                        logger.critical("Unable to put file `%s' in the DB. "
                                        "Aborting. Please remove the contest "
                                        "from the database." % file_)
                        # TODO: remove contest from the database.
                        return False

        if contest_id is not None:
            logger.info("Import finished (contest id: %s)." %
                        ", ".join(str(id_) for id_ in contest_id))
        else:
            logger.info("Import finished.")

        # If we extracted an archive, we remove it.
        if self.import_dir != self.import_source:
            rmtree(self.import_dir)

        return True

    def import_object(self, data):

        """Import objects from the given data (without relationships).

        The given data is assumed to be a dict in the format produced by
        ContestExporter. This method reads the "_class" item and tries
        to find the corresponding class. Then it loads all column
        properties of that class (those that are present in the data)
        and uses them as keyword arguments in a call to the class
        constructor (if a required property is missing this call will
        raise an error).

        Relationships are not handled by this method, since we may not
        have all referenced objects available yet. Thus we prefer to add
        relationships in a later moment, using the add_relationships
        method.

        Note that both this method and add_relationships don't check if
        the given data has more items than the ones we understand and
        use.

        """

        cls = getattr(class_hook, data["_class"])

        args = dict()

        for prp in cls._col_props:
            if prp.key not in data:
                # We will let the __init__ of the class check if any
                # argument is missing, so it's safe to just skip here.
                continue

            col = prp.columns[0]
            col_type = type(col.type)

            val = data[prp.key]
            if col_type in \
                    [Boolean, Integer, Float, Unicode, RepeatedUnicode, Enum]:
                args[prp.key] = val
            elif col_type is String:
                args[prp.key] = \
                    val.encode('latin1') if val is not None else None
            elif col_type is DateTime:
                args[prp.key] = \
                    make_datetime(val) if val is not None else None
            elif col_type is Interval:
                args[prp.key] = \
                    timedelta(seconds=val) if val is not None else None
            else:
                raise RuntimeError(
                    "Unknown SQLAlchemy column type: %s" % col_type)

        return cls(**args)

    def add_relationships(self, data, obj):

        """Add the relationships to the given object, using the given data.

        Do what we didn't in import_objects: importing relationships.
        We already now the class of the object so we simply iterate over
        its relationship properties trying to load them from the data (if
        present), checking wheter they are IDs or collection of IDs,
        dereferencing them (i.e. getting the corresponding object) and
        reflecting all on the given object.

        Note that both this method and import_object don't check if the
        given data has more items than the ones we understand and use.

        """

        cls = type(obj)

        for prp in cls._rel_props:
            if prp.key not in data:
                # Relationships are always optional
                continue

            val = data[prp.key]
            if val is None:
                setattr(obj, prp.key, None)
            elif type(val) == unicode:
                setattr(obj, prp.key, self.objs[val])
            elif type(val) == list:
                setattr(obj, prp.key, list(self.objs[i] for i in val))
            elif type(val) == dict:
                setattr(obj, prp.key,
                        dict((k, self.objs[v]) for k, v in val.iteritems()))
            else:
                raise RuntimeError(
                    "Unknown RelationshipProperty value: %s" % type(val))

    def safe_put_file(self, path, descr_path):

        """Put a file to FileCacher signaling every error (including
        digest mismatch).

        path (string): the path from which to load the file.
        descr_path (string): same for description.

        return (bool): True if all ok, False if something wrong.

        """

        # TODO - Probably this method could be merged in FileCacher

        # First read the description.
        try:
            with io.open(descr_path, 'rt', encoding='utf-8') as fin:
                description = fin.read()
        except IOError:
            description = ''

        # Put the file.
        try:
            digest = self.file_cacher.put_file_from_path(path, description)
        except Exception as error:
            logger.critical("File %s could not be put to file server (%r), "
                            "aborting." % (path, error))
            return False

        # Then check the digest.
        calc_digest = sha1sum(path)
        if digest != calc_digest:
            logger.critical("File %s has hash %s, but the server returned %s, "
                            "aborting." % (path, calc_digest, digest))
            return False

        return True


def main():
    """Parse arguments and launch process."""
    parser = argparse.ArgumentParser(description="Importer of CMS contests.")
    parser.add_argument("-d", "--drop", action="store_true",
                        help="drop everything from the database "
                        "before importing")
    group = parser.add_mutually_exclusive_group()
    group.add_argument("-f", "--files", action="store_true",
                       help="only import files, ignore database structure")
    group.add_argument("-F", "--no-files", action="store_true",
                       help="only import database structure, ignore files")
    parser.add_argument("-G", "--no-generated", action="store_true",
                        help="don't import data and files that can be "
                             "automatically generated")
    parser.add_argument("-S", "--no-submissions", action="store_true",
                        help="don't import submissions")
    parser.add_argument("-U", "--no-user-tests", action="store_true",
                        help="don't import user tests")
    parser.add_argument("import_source",
                        help="source directory or compressed file")

    args = parser.parse_args()

    ContestImporter(drop=args.drop,
                    import_source=args.import_source,
                    load_files=not args.no_files,
                    load_model=not args.files,
                    skip_generated=args.no_generated,
                    skip_submissions=args.no_submissions,
                    skip_user_tests=args.no_user_tests).do_import()


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = DumpUpdater
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A script to update a dump created by CMS.

This script updates a dump (i.e. a set of exported contest data, as
created by ContestExporter) of the Contest Management System from any
of the old supported versions to the current one.

"""

from __future__ import absolute_import
from __future__ import print_function

# We enable monkey patching to make many libraries gevent-friendly
# (for instance, urllib3, used by requests)
import gevent.monkey
gevent.monkey.patch_all()

import argparse
import io
import json
import logging
import os

from cms.db import version as model_version


logger = logging.getLogger(__name__)


def main():
    parser = argparse.ArgumentParser(
        description="Updater of CMS contest dumps.")
    parser.add_argument(
        "-V", "--to-version", action="store", type=int, default=-1,
        help="Update to given version number")
    parser.add_argument(
        "path", help="location of the dump or of the 'contest.json' file")

    args = parser.parse_args()
    path = args.path

    to_version = args.to_version
    if to_version == -1:
        to_version = model_version

    if not path.endswith("contest.json"):
        path = os.path.join(path, "contest.json")

    if not os.path.exists(path):
        logger.critical(
            "The given path doesn't exist or doesn't contain a contest "
            "dump in a format CMS is able to understand.")
        return

    with io.open(path, 'rb') as fin:
        data = json.load(fin, encoding="utf-8")

    # If no "_version" field is found we assume it's a v1.0
    # export (before the new dump format was introduced).
    dump_version = data.get("_version", 0)

    if dump_version == to_version:
        logger.info(
            "The dump you're trying to update is already stored using "
            "the most recent format supported by this version of CMS.")
        return

    if dump_version > to_version:
        logger.critical(
            "The dump you're trying to update is stored using a format "
            "that's more recent than the one supported by this version "
            "of CMS. You probably need to update CMS to handle it.")
        return

    for version in range(dump_version, to_version):
        # Update from version to version+1
        updater = __import__(
            "cmscontrib.updaters.update_%d" % (version + 1),
            globals(), locals(), ["Updater"]).Updater(data)
        data = updater.run()
        data["_version"] = version + 1

    assert data["_version"] == to_version

    with io.open(path, 'wb') as fout:
        json.dump(data, fout, encoding="utf-8", indent=4, sort_keys=True)


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = Importer
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""This script imports a contest from disk using one of the available
loaders.

The data parsed by the loader is used to create a new Contest in the
database.

"""

from __future__ import absolute_import
from __future__ import print_function

# We enable monkey patching to make many libraries gevent-friendly
# (for instance, urllib3, used by requests)
import gevent.monkey
gevent.monkey.patch_all()

import argparse
import datetime
import logging
import os
import os.path

from cms.db import SessionGen, User, init_db, drop_db
from cms.db.filecacher import FileCacher

from cmscontrib.Loaders import choose_loader, build_epilog


logger = logging.getLogger(__name__)


class Importer(object):

    """This script imports a contest from disk using one of the
    available loaders.

    The data parsed by the loader is used to create a new Contest in
    the database.

    """

    def __init__(self, path, drop, test, zero_time, user_number, loader_class):
        self.drop = drop
        self.test = test
        self.zero_time = zero_time
        self.user_number = user_number

        self.file_cacher = FileCacher()

        self.loader = loader_class(os.path.realpath(path), self.file_cacher)

    def _prepare_db(self):
        logger.info("Creating database structure.")
        if self.drop:
            try:
                if not (drop_db() and init_db()):
                    logger.critical("Unexpected error while dropping and "
                                    "recreating the database.",
                                    exc_info=True)
                    return False
            except Exception as error:
                logger.critical("Unable to access DB.\n%r" % error)
                return False
        return True

    def do_import(self):
        """Get the contest from the Loader and store it."""
        if not self._prepare_db():
            return False

        # Get the contest
        contest, tasks, users = self.loader.get_contest()

        # Get the tasks
        for task in tasks:
            contest.tasks.append(self.loader.get_task(task))

        # Get the users or, if asked, generate them
        if self.user_number is None:
            for user in users:
                contest.users.append(self.loader.get_user(user))
        else:
            logger.info("Generating %s random users." % self.user_number)
            contest.users = [User("User %d" % i,
                                  "Last name %d" % i,
                                  "user%03d" % i)
                             for i in xrange(self.user_number)]

        # Apply the modification flags
        if self.zero_time:
            contest.start = datetime.datetime(1970, 1, 1)
            contest.stop = datetime.datetime(1970, 1, 1)
        elif self.test:
            contest.start = datetime.datetime(1970, 1, 1)
            contest.stop = datetime.datetime(2100, 1, 1)

            for user in contest.users:
                user.password = 'a'
                user.ip = None

        # Store
        logger.info("Creating contest on the database.")
        with SessionGen() as session:
            session.add(contest)
            session.commit()
            contest_id = contest.id

        logger.info("Import finished (new contest id: %s)." % contest_id)


def main():
    """Parse arguments and launch process."""

    parser = argparse.ArgumentParser(
        description="Import a contest from disk",
        epilog=build_epilog(),
        formatter_class=argparse.RawDescriptionHelpFormatter)
    group = parser.add_mutually_exclusive_group()
    group.add_argument("-z", "--zero-time", action="store_true",
                       help="set to zero contest start and stop time")
    group.add_argument("-t", "--test", action="store_true",
                       help="setup a contest for testing "
                       "(times: 1970, 2100; ips: unset, passwords: a)")
    parser.add_argument("-d", "--drop", action="store_true",
                        help="drop everything from the database "
                        "before importing")
    parser.add_argument("-n", "--user-number", action="store", type=int,
                        help="put N random users instead of importing them")
    parser.add_argument("-L", "--loader", action="store", default=None,
                        help="use the specified loader (default: autodetect)")
    parser.add_argument("import_directory",
                        help="source directory from where import")

    args = parser.parse_args()
    loader_class = choose_loader(args.loader,
                                 args.import_directory,
                                 parser.error)

    Importer(path=args.import_directory,
             drop=args.drop,
             test=args.test,
             zero_time=args.zero_time,
             user_number=args.user_number,
             loader_class=loader_class).do_import()


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = Loaders
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""List of loaders known by CMS, with some support functions.

"""

from __future__ import absolute_import
from __future__ import print_function

from cmscontrib.YamlLoader import YamlLoader

LOADERS = dict((loader_class.short_name, loader_class)
               for loader_class in [YamlLoader])


def choose_loader(arg, path, error_callback):
    """Decide which loader to use.

    The choice depends upon the specified argument and possibly
    performing an autodetection.

    The autodetection is done by calling detect() on all the known
    loaders and returning the only one that returns True. If no one or
    more than one return True, then the autodetection is considered
    failed and None is returned.

    arg (string): the argument, possibly None, passed to the program
                  as loader specification.
    path (string): the path passed to the program from which to
                   perform the loading.
    error_callback (method): a method to call to report errors.

    return (type): the chosen loader class.

    """
    if arg is not None:
        try:
            return LOADERS[arg]
        except KeyError:
            error_callback("Specified loader doesn't exist")
    else:
        res = None
        for loader in LOADERS.itervalues():
            if loader.detect(path):
                if res is None:
                    res = loader
                else:
                    error_callback(
                        "Couldn't autodetect the loader, "
                        "please specify it: more than one "
                        "loader accepted the detection")
        if res is None:
            error_callback("Couldn't autodetect the loader, "
                           "please specify it: no "
                           "loader accepted the detection")
        return res


def build_epilog():
    """Build the ArgumentParser epilog.

    Basically, list the known loaders' short names.

    """
    epilog = "The following loaders are supported:\n"
    for short_name, loader_class in sorted(LOADERS.items()):
        epilog += " * %s (%s)\n" % (short_name, loader_class.description)
    return epilog

########NEW FILE########
__FILENAME__ = Reimporter
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Versari <veluca93@gmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""This script reimports a contest from disk using one of the
available loaders.

The data parsed by the loader is used to update a Contest that's
already existing in the database.

"""

from __future__ import absolute_import
from __future__ import print_function

# We enable monkey patching to make many libraries gevent-friendly
# (for instance, urllib3, used by requests)
import gevent.monkey
gevent.monkey.patch_all()

import argparse
import logging
import os
import os.path

from cms.db import SessionGen, Base, Contest, User, Task, Submission, \
    ask_for_contest
from cms.db.filecacher import FileCacher

from cmscontrib.Loaders import choose_loader, build_epilog


logger = logging.getLogger(__name__)


def _is_rel(prp, attr):
    # The target of the relationship is in prp.mapper.class_
    return prp.parent.class_ == attr.class_ and prp.key == attr.key


class Reimporter(object):

    """This script reimports a contest from disk using the specified
    loader.

    The data parsed by the loader is used to update a Contest that's
    already existing in the database.

    """

    def __init__(self, path, contest_id, force, loader_class, full):
        self.old_contest_id = contest_id
        self.force = force
        self.full = full

        self.file_cacher = FileCacher()

        self.loader = loader_class(os.path.realpath(path), self.file_cacher)

    def _update_columns(self, old_object, new_object):
        for prp in old_object._col_props:
            if hasattr(new_object, prp.key):
                setattr(old_object, prp.key, getattr(new_object, prp.key))

    def _update_object(self, old_object, new_object):
        # This method copies the scalar column properties from the new
        # object into the old one, and then tries to do the same for
        # relationships too. The data model isn't a tree: for example
        # there are two distinct paths from Contest to Submission, one
        # through User and one through Task. Yet, at the moment, if we
        # ignore Submissions and UserTest (and thus their results, too)
        # we get a tree-like structure and Task.active_dataset and
        # Submission.token are the only scalar relationships that don't
        # refer to the parent. Therefore, if we catch these as special
        # cases, we can use a simple DFS to explore the whole data
        # graph, recursing only on "vector" relationships.
        # TODO Find a better way to handle all of this.

        self._update_columns(old_object, new_object)

        for prp in old_object._rel_props:
            old_value = getattr(old_object, prp.key)
            new_value = getattr(new_object, prp.key)

            # Special case #1: Contest.announcements, User.questions,
            #                  User.messages
            if _is_rel(prp, Contest.announcements) or \
                    _is_rel(prp, User.questions) or \
                    _is_rel(prp, User.messages):
                # A loader should not provide new Announcements,
                # Questions or Messages, since they are data generated
                # by the users during the contest: don't update them.
                # TODO Warn the admin if these attributes are non-empty
                # collections.
                pass

            # Special case #2: Task.datasets
            elif _is_rel(prp, Task.datasets):
                old_datasets = dict((d.description, d) for d in old_value)
                new_datasets = dict((d.description, d) for d in new_value)

                for key in set(new_datasets.keys()):
                    if key not in old_datasets:
                        # create
                        temp = new_datasets[key]
                        new_value.remove(temp)
                        old_value.append(temp)
                    else:
                        # update
                        self._update_object(old_datasets[key],
                                            new_datasets[key])

            # Special case #3: Task.active_dataset
            elif _is_rel(prp, Task.active_dataset):
                # We don't want to update the existing active dataset.
                pass

            # Special case #4: User.submissions, Task.submissions,
            #                  User.user_tests, Task.user_tests
            elif _is_rel(prp, User.submissions) or \
                    _is_rel(prp, Task.submissions) or \
                    _is_rel(prp, User.user_tests) or \
                    _is_rel(prp, Task.user_tests):
                # A loader should not provide new Submissions or
                # UserTests, since they are data generated by the users
                # during the contest: don't update them.
                # TODO Warn the admin if these attributes are non-empty
                # collections.
                pass

            # Special case #5: Submission.token
            elif _is_rel(prp, Submission.token):
                # We should never reach this point! We should never try
                # to update Submissions! We could even assert False...
                pass

            # General case #1: a dict
            elif isinstance(old_value, dict):
                for key in set(old_value.keys()) | set(new_value.keys()):
                    if key in new_value:
                        if key not in old_value:
                            # create
                            # FIXME This hack is needed because of some
                            # funny behavior of SQLAlchemy-instrumented
                            # collections when copying values, that
                            # resulted in new objects being added to
                            # the session. We need to investigate it.
                            temp = new_value[key]
                            del new_value[key]
                            old_value[key] = temp
                        else:
                            # update
                            self._update_object(old_value[key], new_value[key])
                    else:
                        # delete
                        del old_value[key]

            # General case #2: a list
            elif isinstance(old_value, list):
                old_len = len(old_value)
                new_len = len(new_value)
                for i in xrange(min(old_len, new_len)):
                    self._update_object(old_value[i], new_value[i])
                if old_len > new_len:
                    del old_value[new_len:]
                elif new_len > old_len:
                    for i in xrange(old_len, new_len):
                        # FIXME This hack is needed because of some
                        # funny behavior of SQLAlchemy-instrumented
                        # collections when copying values, that
                        # resulted in new objects being added to the
                        # session. We need to investigate it.
                        temp = new_value[i]
                        del new_value[i]
                        old_value.append(temp)

            # General case #3: a parent object
            elif isinstance(old_value, Base):
                # No need to climb back up the recursion tree...
                pass

            # General case #4: None
            elif old_value is None:
                # That should only happen in case of a scalar
                # relationship (i.e. a many-to-one or a one-to-one)
                # that is nullable. "Parent" relationships aren't
                # nullable, so the only possible cases are the active
                # datasets and the tokens, but we should have already
                # caught them. We could even assert False...
                pass

            else:
                raise RuntimeError(
                    "Unknown type of relationship for %s.%s." %
                    (prp.parent.class_.__name__, prp.key))

    def do_reimport(self):
        """Get the contest from the Loader and merge it."""
        with SessionGen() as session:
            # Load the old contest from the database.
            old_contest = Contest.get_from_id(self.old_contest_id, session)
            old_users = dict((x.username, x) for x in old_contest.users)
            old_tasks = dict((x.name, x) for x in old_contest.tasks)

            # Load the new contest from the filesystem.
            new_contest, new_tasks, new_users = self.loader.get_contest()

            # Updates contest-global settings that are set in new_contest.
            self._update_columns(old_contest, new_contest)

            # Do the actual merge: compare all users of the old and of
            # the new contest and see if we need to create, update or
            # delete them. Delete only if authorized, fail otherwise.
            users = set(old_users.keys()) | set(new_users)
            for username in users:
                old_user = old_users.get(username, None)

                if old_user is None:
                    # Create a new user.
                    logger.info("Creating user %s" % username)
                    new_user = self.loader.get_user(username)
                    old_contest.users.append(new_user)
                elif username in new_users:
                    # Update an existing user.
                    logger.info("Updating user %s" % username)
                    new_user = self.loader.get_user(username)
                    self._update_object(old_user, new_user)
                else:
                    # Delete an existing user.
                    if self.force:
                        logger.info("Deleting user %s" % username)
                        old_contest.users.remove(old_user)
                    else:
                        logger.critical(
                            "User %s exists in old contest, but "
                            "not in the new one. Use -f to force." %
                            username)
                        return False

            # The same for tasks. Setting num for tasks requires a bit
            # of trickery, since we have to avoid triggering a
            # duplicate key constraint violation while we're messing
            # with the task order. To do that we just set sufficiently
            # high number on the first pass and then fix it on a
            # second pass.
            tasks = set(old_tasks.keys()) | set(new_tasks)
            current_num = max(len(old_tasks), len(new_tasks))
            for task in tasks:
                old_task = old_tasks.get(task, None)

                if old_task is None:
                    # Create a new task.
                    logger.info("Creating task %s" % task)
                    new_task = self.loader.get_task(task)
                    new_task.num = current_num
                    current_num += 1
                    old_contest.tasks.append(new_task)
                elif task in new_tasks:
                    # Update an existing task.
                    if self.full or self.loader.has_changed(task):
                        logger.info("Updating task %s" % task)
                        new_task = self.loader.get_task(task)
                        new_task.num = current_num
                        current_num += 1
                        self._update_object(old_task, new_task)
                    else:
                        logger.info("Task %s has not changed" % task)
                        # Even unchanged tasks should use a temporary number
                        # to avoid duplicate numbers when we fix them.
                        old_task.num = current_num
                        current_num += 1
                else:
                    # Delete an existing task.
                    if self.force:
                        logger.info("Deleting task %s" % task)
                        session.delete(old_task)
                    else:
                        logger.critical(
                            "Task %s exists in old contest, but "
                            "not in the new one. Use -f to force." %
                            task)
                        return False

                session.flush()

            # And finally we fix the numbers; old_contest must be
            # refreshed because otherwise SQLAlchemy doesn't get aware
            # that some tasks may have been deleted
            tasks_order = dict((name, num)
                               for num, name in enumerate(new_tasks))
            session.refresh(old_contest)
            for task in old_contest.tasks:
                task.num = tasks_order[task.name]

            session.commit()

        logger.info("Reimport finished (contest id: %s)." %
                    self.old_contest_id)

        return True


def main():
    """Parse arguments and launch process."""
    parser = argparse.ArgumentParser(
        description="Reimport a contest from disk",
        epilog=build_epilog(),
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("-c", "--contest-id", action="store", type=int,
                        help="id of contest to overwrite")
    parser.add_argument("-f", "--force", action="store_true",
                        help="force the reimport even if some users or tasks "
                        "may get lost")
    parser.add_argument("-L", "--loader", action="store", default=None,
                        help="use the specified loader (default: autodetect)")
    parser.add_argument("-F", "--full", action="store_true",
                        help="reimport tasks even if they haven't changed")
    parser.add_argument("import_directory",
                        help="source directory from where import")

    args = parser.parse_args()
    loader_class = choose_loader(args.loader,
                                 args.import_directory,
                                 parser.error)

    if args.contest_id is None:
        args.contest_id = ask_for_contest()

    Reimporter(path=args.import_directory,
               contest_id=args.contest_id,
               force=args.force,
               loader_class=loader_class,
               full=args.full).do_reimport()


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = RemoveTask
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Utility to remove a task.

"""

from __future__ import absolute_import
from __future__ import print_function

import argparse
import sys

from cms.db import SessionGen, Task, ask_for_contest


def remove_task(contest_id, task_name):
    with SessionGen() as session:
        task = session.query(Task)\
            .filter(Task.contest_id == contest_id)\
            .filter(Task.name == task_name).first()
        session.delete(task)
        session.commit()


def main():
    """Parse arguments and launch process.

    """
    parser = argparse.ArgumentParser(
        description="Remove a task from a contest in CMS.")
    parser.add_argument("task_name", help="short name of the task")
    parser.add_argument("-c", "--contest-id",
                        help="id of contest the task is in",
                        action="store", type=int)
    args = parser.parse_args()

    if args.contest_id is None:
        args.contest_id = ask_for_contest()

    remove_task(contest_id=args.contest_id,
                task_name=args.task_name)

    return 0


if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = RemoveUser
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Utility to remove a user.

"""

from __future__ import absolute_import
from __future__ import print_function

import argparse
import sys

from cms.db import SessionGen, User, ask_for_contest


def remove_user(contest_id, username):
    with SessionGen() as session:
        user = session.query(User)\
            .filter(User.contest_id == contest_id)\
            .filter(User.username == username).first()
        session.delete(user)
        session.commit()


def main():
    """Parse arguments and launch process.

    """
    parser = argparse.ArgumentParser(
        description="Remove a user from a contest in CMS.")
    parser.add_argument("username", help="username of the user")
    parser.add_argument("-c", "--contest-id",
                        help="id of contest the user is in",
                        action="store", type=int)
    args = parser.parse_args()

    if args.contest_id is None:
        args.contest_id = ask_for_contest()

    remove_user(contest_id=args.contest_id,
                username=args.username)

    return 0


if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = RWSHelper
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A script to interact with RWSs using HTTP requests

Provide a handy command-line interface to do common operations on
entities stored on RankingWebServers. Particularly useful to delete an
entity that has been deleted in the DB without any downtime.

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function

# We enable monkey patching to make many libraries gevent-friendly
# (for instance, urllib3, used by requests)
import gevent.monkey
gevent.monkey.patch_all()

import argparse
import logging
import sys

import six

if six.PY3:
    from urllib.parse import quote, urljoin, urlsplit
else:
    from urllib import quote
    from urlparse import urljoin, urlsplit

from six.moves import xrange

from requests import Session, Request
from requests.exceptions import RequestException

from cms import config


logger = logging.getLogger(__name__)


ACTION_METHODS = {
    'get': 'GET',
    'create': 'PUT',  # Create is actually an update.
    'update': 'PUT',
    'delete': 'DELETE',
    }

ENTITY_TYPES = ['contest',
                'task',
                'team',
                'user',
                'submission',
                'subchange',
                ]


def get_url(shard, entity_type, entity_id):
    return urljoin(config.rankings[shard], '%ss/%s' % (entity_type, entity_id))


def main():
    parser = argparse.ArgumentParser(prog='cmsRWSHelper')
    parser.add_argument(
        '-v', '--verbose', action='store_true',
        help="tell on stderr what's happening")
    # FIXME It would be nice to use '--rankings' with action='store'
    # and nargs='+' but it doesn't seem to work with subparsers...
    parser.add_argument(
        '-r', '--ranking', dest='rankings', action='append', default=None,
        choices=list(xrange(len(config.rankings))), metavar='shard',
        type=int, help="select which RWS to connect to (omit for 'all')")
    subparsers = parser.add_subparsers(
        title='available actions', metavar='action',
        help='what to ask the RWS to do with the entity')

    # Create the parser for the "get" command
    parser_get = subparsers.add_parser('get', help="retrieve the entity")
    parser_get.set_defaults(action='get')

    # Create the parser for the "create" command
    parser_create = subparsers.add_parser('create', help="create the entity")
    parser_create.set_defaults(action='create')
    parser_create.add_argument(
        'file', type=argparse.FileType('rb'),
        help="file holding the entity body to send ('-' for stdin)")

    # Create the parser for the "update" command
    parser_update = subparsers.add_parser('update', help='update the entity')
    parser_update.set_defaults(action='update')
    parser_update.add_argument(
        'file', type=argparse.FileType('rb'),
        help="file holding the entity body to send ('-' for stdin)")

    # Create the parser for the "delete" command
    parser_delete = subparsers.add_parser('delete', help='delete the entity')
    parser_delete.set_defaults(action='delete')

    # Create the group for entity-related arguments
    group = parser.add_argument_group(
        title='entity reference')
    group.add_argument(
        'entity_type', action='store', choices=ENTITY_TYPES, metavar='type',
        help="type of the entity (e.g. contest, user, task, etc.)")
    group.add_argument(
        'entity_id', action='store', type=six.text_type, metavar='id',
        help='ID of the entity (usually a short codename)')

    # Parse the given arguments
    args = parser.parse_args()

    args.entity_id = quote(args.entity_id)

    if args.verbose:
        verb = args.action[:4] + 'ting'
        logger.info("%s entity '%ss/%s'" % (verb.capitalize(),
                                            args.entity_type, args.entity_id))

    if args.rankings is not None:
        shards = args.rankings
    else:
        shards = list(xrange(len(config.rankings)))

    s = Session()
    had_error = False

    for shard in shards:
        url = get_url(shard, args.entity_type, args.entity_id)
        # XXX With requests-1.2 auth is automatically extracted from
        # the URL: there is no need for this.
        auth = urlsplit(url)

        if args.verbose:
            logger.info(
                "Preparing %s request to %s" %
                (ACTION_METHODS[args.action], url))

        if hasattr(args, 'file'):
            if args.verbose:
                logger.info("Reading file contents to use as message body")
            body = args.file.read()
        else:
            body = None

        req = Request(ACTION_METHODS[args.action], url, data=body,
                      auth=(auth.username, auth.password),
                      headers={'content-type': 'application/json'}).prepare()

        if args.verbose:
            logger.info("Sending request")

        try:
            res = s.send(req, verify=config.https_certfile)
        except RequestException as error:
            logger.error("Failed")
            logger.info(repr(error))
            had_error = True
            continue

        if args.verbose:
            logger.info("Response received")

        if 400 <= res.status_code < 600:
            logger.error("Unexpected status code: %d" % res.status_code)
            had_error = True
            continue

        if args.action == "get":
            print(res.content)

    if had_error:
        sys.exit(1)


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = SpoolExporter
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""This service creates a tree structure "similar" to the one used in
Italian IOI repository for storing the results of a contest.

"""

from __future__ import absolute_import
from __future__ import print_function

# We enable monkey patching to make many libraries gevent-friendly
# (for instance, urllib3, used by requests)
import gevent.monkey
gevent.monkey.patch_all()

import argparse
import io
import logging
import os
import time

from cms.db import SessionGen, Contest, ask_for_contest
from cms.db.filecacher import FileCacher
from cms.grading.scoretypes import get_score_type


logger = logging.getLogger(__name__)


# TODO: review this file to avoid print.
class SpoolExporter(object):
    """This service creates a tree structure "similar" to the one used
    in Italian IOI repository for storing the results of a contest.

    """
    def __init__(self, contest_id, spool_dir):
        self.contest_id = contest_id
        self.spool_dir = spool_dir
        self.upload_dir = os.path.join(self.spool_dir, "upload")
        self.contest = None
        self.submissions = None

        self.file_cacher = FileCacher()

    def run(self):
        """Interface to make the class do its job."""
        return self.do_export()

    def do_export(self):
        """Run the actual export code.

        """
        logger.operation = "exporting contest %s" % self.contest_id
        logger.info("Starting export.")

        logger.info("Creating dir structure.")
        try:
            os.mkdir(self.spool_dir)
        except OSError:
            logger.critical("The specified directory already exists, "
                            "I won't overwrite it.")
            return False
        os.mkdir(self.upload_dir)

        with SessionGen() as session:
            self.contest = Contest.get_from_id(self.contest_id, session)
            self.submissions = sorted(
                (submission
                 for submission in self.contest.get_submissions()
                 if not submission.user.hidden),
                key=lambda submission: submission.timestamp)

            # Creating users' directory.
            for user in self.contest.users:
                if not user.hidden:
                    os.mkdir(os.path.join(self.upload_dir, user.username))

            try:
                self.export_submissions()
                self.export_ranking()
            except Exception:
                logger.critical("Generic error.", exc_info=True)
                return False

        logger.info("Export finished.")
        logger.operation = ""

        return True

    def export_submissions(self):
        """Export submissions' source files.

        """
        logger.info("Exporting submissions.")

        queue_file = io.open(os.path.join(self.spool_dir, "queue"), "w",
                             encoding="utf-8")
        for submission in sorted(self.submissions, key=lambda x: x.timestamp):
            logger.info("Exporting submission %s." % submission.id)
            username = submission.user.username
            task = submission.task.name
            timestamp = time.mktime(submission.timestamp.timetuple())

            # Get source files to the spool directory.
            submission_dir = os.path.join(
                self.upload_dir, username, "%s.%d.%s" %
                (task, timestamp, submission.language))
            os.mkdir(submission_dir)
            for filename, file_ in submission.files.iteritems():
                self.file_cacher.get_file_to_path(
                    file_.digest,
                    os.path.join(submission_dir, filename))
            last_submission_dir = os.path.join(
                self.upload_dir, username, "%s.%s" %
                (task, submission.language))
            try:
                os.unlink(last_submission_dir)
            except OSError:
                pass
            os.symlink(os.path.basename(submission_dir), last_submission_dir)
            print("./upload/%s/%s.%d.%s" %
                  (username, task, timestamp, submission.language),
                  file=queue_file)

            # Write results file for the submission.
            active_dataset = submission.task.active_dataset
            result = submission.get_result(active_dataset)
            if result.evaluated():
                res_file = io.open(os.path.join(
                    self.spool_dir,
                    "%d.%s.%s.%s.res" % (timestamp, username,
                                         task, submission.language)),
                    "w", encoding="utf-8")
                res2_file = io.open(
                    os.path.join(self.spool_dir,
                                 "%s.%s.%s.res" % (username, task,
                                                   submission.language)),
                    "w", encoding="utf-8")
                total = 0.0
                for evaluation in result.evaluations:
                    outcome = float(evaluation.outcome)
                    total += outcome
                    line = "Executing on file with codename '%s' %s (%.4f)" % \
                        (evaluation.testcase.codename,
                         evaluation.text, outcome)
                    print(line, file=res_file)
                    print(line, file=res2_file)
                line = "Score: %.6f" % total
                print(line, file=res_file)
                print(line, file=res2_file)
                res_file.close()
                res2_file.close()

        print(file=queue_file)
        queue_file.close()

    def export_ranking(self):
        """Exports the ranking in csv and txt (human-readable) form.

        """
        logger.info("Exporting ranking.")

        # Create the structure to store the scores.
        scores = dict((user.username, 0.0)
                      for user in self.contest.users
                      if not user.hidden)
        task_scores = dict((task.id, dict((user.username, 0.0)
                                          for user in self.contest.users
                                          if not user.hidden))
                           for task in self.contest.tasks)
        last_scores = dict((task.id, dict((user.username, 0.0)
                                          for user in self.contest.users
                                          if not user.hidden))
                           for task in self.contest.tasks)

        # Make the score type compute the scores.
        scorers = {}
        for task in self.contest.tasks:
            scorers[task.id] = get_score_type(dataset=task.active_dataset)

        for submission in self.submissions:
            active_dataset = submission.task.active_dataset
            result = submission.get_result(active_dataset)
            scorers[submission.task_id].add_submission(
                submission.id, submission.timestamp,
                submission.user.username,
                result.evaluated(),
                dict((ev.codename,
                      {"outcome": ev.outcome,
                       "text": ev.text,
                       "time": ev.execution_time,
                       "memory": ev.execution_memory})
                     for ev in result.evaluations),
                submission.tokened())

        # Put together all the scores.
        for submission in self.submissions:
            task_id = submission.task_id
            username = submission.user.username
            details = scorers[task_id].pool[submission.id]
            last_scores[task_id][username] = details["score"]
            if details["tokened"]:
                task_scores[task_id][username] = max(
                    task_scores[task_id][username],
                    details["score"])

        # Merge tokened and last submissions.
        for username in scores:
            for task_id in task_scores:
                task_scores[task_id][username] = max(
                    task_scores[task_id][username],
                    last_scores[task_id][username])
            # print(username, [task_scores[task_id][username]
            #                  for task_id in task_scores])
            scores[username] = sum(task_scores[task_id][username]
                                   for task_id in task_scores)

        sorted_usernames = sorted(scores.keys(),
                                  key=lambda username: (scores[username],
                                                        username),
                                  reverse=True)
        sorted_tasks = sorted(self.contest.tasks,
                              key=lambda task: task.num)

        ranking_file = io.open(
            os.path.join(self.spool_dir, "classifica.txt"),
            "w", encoding="utf-8")
        ranking_csv = io.open(
            os.path.join(self.spool_dir, "classifica.csv"),
            "w", encoding="utf-8")

        # Write rankings' header.
        n_tasks = len(sorted_tasks)
        print("Classifica finale del contest `%s'" %
              self.contest.description, file=ranking_file)
        points_line = " %10s" * n_tasks
        csv_points_line = ",%s" * n_tasks
        print(("%20s %10s" % ("Utente", "Totale")) +
              (points_line % tuple([t.name for t in sorted_tasks])),
              file=ranking_file)
        print(("%s,%s" % ("utente", "totale")) +
              (csv_points_line % tuple([t.name for t in sorted_tasks])),
              file=ranking_csv)

        # Write rankings' content.
        points_line = " %10.3f" * n_tasks
        csv_points_line = ",%.6f" * n_tasks
        for username in sorted_usernames:
            user_scores = [task_scores[task.id][username]
                           for task in sorted_tasks]
            print(("%20s %10.3f" % (username, scores[username])) +
                  (points_line % tuple(user_scores)),
                  file=ranking_file)
            print(("%s,%.6f" % (username, scores[username])) +
                  (csv_points_line % tuple(user_scores)),
                  file=ranking_csv)

        ranking_file.close()
        ranking_csv.close()


def main():
    """Parse arguments and launch process.

    """
    parser = argparse.ArgumentParser(
        description="Exporter for the Italian repository for CMS.")
    parser.add_argument("-c", "--contest-id", help="id of contest to export",
                        action="store", type=int)
    parser.add_argument("export_directory",
                        help="target directory where to export")
    args = parser.parse_args()

    if args.contest_id is None:
        args.contest_id = ask_for_contest()

    SpoolExporter(contest_id=args.contest_id,
                  spool_dir=args.export_directory).run()


if __name__ == "__main__":
    main()

########NEW FILE########
__FILENAME__ = update_1
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This converts the dump to the new format introduced in commit
db4adada08d66b4797d0569d95e8f0c028a4e5e0.

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function

from functools import partial


class Updater(object):

    def __init__(self, data):
        self.data = data
        self.objs = dict()
        self.next_id = 0

    def run(self):
        self.parse_contest(self.data)
        self.objs["_objects"] = ["0"]
        return self.objs

    def get_id(self):
        ret = unicode(self.next_id)
        self.next_id += 1
        return ret

    def parse_list(self, list_, fun, num=False, **kwargs):
        ret = list()
        for i, item in enumerate(list_):
            item_id = fun(item)
            if num:
                self.objs[item_id]['num'] = i
            for k, v in kwargs.iteritems():
                self.objs[item_id][k] = v
            ret.append(item_id)
        return ret

    def parse_dict(self, list_, fun, key, **kwargs):
        ret = dict()
        for item in list_:
            item_id = fun(item)
            for k, v in kwargs.iteritems():
                self.objs[item_id][k] = v
            ret[item[key]] = item_id
        return ret

    def parse_generic(self, data, cls):
        id_ = self.get_id()
        data['_class'] = cls
        self.objs[id_] = data
        return id_

    def parse_contest(self, data):
        id_ = self.get_id()

        data['tasks'] = self.parse_list(
            data['tasks'], self.parse_task, contest=id_)

        tasks_by_name = dict((self.objs[i]['name'], i) for i in data['tasks'])

        data['users'] = self.parse_list(data['users'],
                                        partial(self.parse_user,
                                                tasks_by_name=tasks_by_name),
                                        contest=id_)
        data['announcements'] = self.parse_list(
            data['announcements'],
            partial(self.parse_generic, cls='Announcement'), contest=id_)

        data['_class'] = 'Contest'
        self.objs[id_] = data
        return id_

    def parse_task(self, data):
        id_ = self.get_id()

        data['statements'] = self.parse_dict(
            data['statements'], partial(self.parse_generic, cls='Statement'),
            'language', task=id_)
        data['attachments'] = self.parse_dict(
            data['attachments'], partial(self.parse_generic, cls='Attachment'),
            'filename', task=id_)
        data['submission_format'] = self.parse_list(
            data['submission_format'],
            partial(self.parse_generic, cls='SubmissionFormatElement'),
            task=id_)
        data['managers'] = self.parse_dict(
            data['managers'], partial(self.parse_generic, cls='Manager'),
            'filename', task=id_)
        data['testcases'] = self.parse_list(
            data['testcases'], partial(self.parse_generic, cls='Testcase'),
            True, task=id_)

        data['submissions'] = []
        data['user_tests'] = []

        # Handle some pre-1.0 dumps
        if "score_parameters" in data:
            data["score_type_parameters"] = data["score_parameters"]
            del data["score_parameters"]

        data['_class'] = 'Task'
        self.objs[id_] = data
        return id_

    def parse_user(self, data, tasks_by_name):
        id_ = self.get_id()

        data['messages'] = self.parse_list(
            data['messages'],
            partial(self.parse_generic, cls='Message'), user=id_)
        data['questions'] = self.parse_list(
            data['questions'],
            partial(self.parse_generic, cls='Question'), user=id_)
        data['submissions'] = self.parse_list(
            data['submissions'],
            partial(self.parse_submission, tasks_by_name=tasks_by_name),
            user=id_)
        # Because of a bug in older versions of CMS, some dumps may
        # lack the user_tests key; unfortunately user tests for such
        # contests have been lost
        if 'user_tests' in data:
            data['user_tests'] = self.parse_list(
                data['user_tests'],
                partial(self.parse_user_test, tasks_by_name=tasks_by_name),
                user=id_)
        else:
            data['user_tests'] = []

        data['_class'] = 'User'
        self.objs[id_] = data
        return id_

    def parse_submission(self, data, tasks_by_name):
        id_ = self.get_id()

        data['files'] = self.parse_dict(
            data['files'],
            partial(self.parse_generic, cls='File'), 'filename',
            submission=id_)
        data['executables'] = self.parse_dict(
            data['executables'],
            partial(self.parse_generic, cls='Executable'), 'filename',
            submission=id_)
        data['evaluations'] = self.parse_list(
            data['evaluations'],
            partial(self.parse_generic, cls='Evaluation'), submission=id_)
        if data['token'] is not None:
            data['token'] = self.parse_generic(data['token'], 'Token')

        task_id = tasks_by_name[data['task']]
        data['task'] = task_id
        self.objs[task_id]['submissions'].append(id_)

        data['_class'] = 'Submission'
        self.objs[id_] = data
        return id_

    def parse_user_test(self, data, tasks_by_name):
        id_ = self.get_id()

        data['files'] = self.parse_dict(
            data['files'], partial(self.parse_generic, cls='UserTestFile'),
            'filename', user_test=id_)
        data['executables'] = self.parse_dict(
            data['executables'],
            partial(self.parse_generic, cls='UserTestExecutable'),
            'filename', user_test=id_)
        data['managers'] = self.parse_dict(
            data['managers'],
            partial(self.parse_generic, cls='UserTestManager'),
            'filename', user_test=id_)

        task_id = tasks_by_name[data['task']]
        data['task'] = task_id
        self.objs[task_id]['user_tests'].append(id_)

        data['_class'] = 'UserTest'
        self.objs[id_] = data
        return id_

########NEW FILE########
__FILENAME__ = update_10
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This adapts the dump to some changes in the model introduced in the
commit that created this same file.

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function


class Updater(object):

    def __init__(self, data):
        assert data["_version"] == 9
        self.objs = data

    def run(self):
        for k, v in self.objs.iteritems():
            if k.startswith("_"):
                continue
            if v["_class"] == "Contest":
                if v["start"] is None:
                    # make_timestamp(datetime(2000, 01, 01))
                    v["start"] = 946684800.0
                if v["stop"] is None:
                    # make_timestamp(datetime(2100, 01, 01))
                    v["stop"] = 4102444800.0

        return self.objs

########NEW FILE########
__FILENAME__ = update_11
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2014 Artem Iglikov <artem.iglikov@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This adapts the dump to some changes in the model introduced in the
commit that created this same file.

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function


class Updater(object):

    def __init__(self, data):
        assert data["_version"] == 10
        self.objs = data

    def run(self):
        for k, v in self.objs.iteritems():
            if k.startswith("_"):
                continue
            if v["_class"] == "Contest":
                v["allowed_localizations"] = []

        return self.objs

########NEW FILE########
__FILENAME__ = update_2
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This converts the dump to the new schema introduced to support task
versioning in commits
bd80d0c930e25972eeda861719f96990de6e7822
8ee8fa7496d53ff4a8638804eb3aa497586fc6a3
6ee99114f5aca4ad1a94a76f0b11b5363ba2ffd5

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function


def split_dict(src, *keys):
    ret = dict()
    for k in list(src.iterkeys()):
        v = src[k]
        if k in keys:
            ret[k] = v
            del src[k]
    return ret


class Updater(object):

    def __init__(self, data):
        assert data["_version"] == 1
        self.objs = data
        self.next_id = len(data)

    def get_id(self):
        while unicode(self.next_id) in self.objs:
            self.next_id += 1
        return unicode(self.next_id)

    def run(self):
        for k in list(self.objs.iterkeys()):
            if k.startswith("_"):
                continue
            v = self.objs[k]
            if v["_class"] == "Task":
                self.split_task(k, v)
        return self.objs

    def split_task(self, task_id, task_data):
        dataset_id = self.get_id()
        dataset_data = split_dict(
            task_data,
            "time_limit", "memory_limit",
            "task_type", "task_type_parameters",
            "score_type", "score_type_parameters",
            "managers", "testcases")
        self.objs[dataset_id] = dataset_data

        task_data["_class"] = "Task"
        dataset_data["_class"] = "Dataset"

        task_data["datasets"] = [dataset_id]
        task_data["active_dataset"] = dataset_id
        dataset_data["task"] = task_id
        dataset_data["description"] = "Default"
        dataset_data["autojudge"] = False

        for id_ in dataset_data["managers"].itervalues():
            del self.objs[id_]["task"]
            self.objs[id_]["dataset"] = dataset_id

        for id_ in dataset_data["testcases"]:
            del self.objs[id_]["task"]
            self.objs[id_]["dataset"] = dataset_id

        for id_ in task_data["submissions"]:
            self.split_submission(id_, self.objs[id_], dataset_id)

        for id_ in task_data["user_tests"]:
            self.split_user_test(id_, self.objs[id_], dataset_id)

    def split_submission(self, submission_id, sr_data, dataset_id):
        sr_id = self.get_id()
        submission_data = split_dict(
            sr_data,
            "user", "task",
            "timestamp", "language",
            "files", "token")
        self.objs[submission_id] = submission_data
        self.objs[sr_id] = sr_data

        submission_data["_class"] = "Submission"
        sr_data["_class"] = "SubmissionResult"

        submission_data["results"] = [sr_id]
        sr_data["submission"] = submission_id
        sr_data["dataset"] = dataset_id

        for id_ in sr_data["executables"].itervalues():
            self.objs[id_]["submission"] = submission_id
            self.objs[id_]["dataset"] = dataset_id
            self.objs[id_]["submission_result"] = sr_id

        for id_ in sr_data["evaluations"]:
            self.objs[id_]["submission"] = submission_id
            self.objs[id_]["dataset"] = dataset_id
            self.objs[id_]["submission_result"] = sr_id

    def split_user_test(self, user_test_id, ur_data, dataset_id):
        ur_id = self.get_id()
        user_test_data = split_dict(
            ur_data,
            "user", "task",
            "timestamp", "language", "input",
            "files", "managers")
        self.objs[user_test_id] = user_test_data
        self.objs[ur_id] = ur_data

        user_test_data["_class"] = "UserTest"
        ur_data["_class"] = "UserTestResult"

        user_test_data["results"] = [ur_id]
        ur_data["user_test"] = user_test_id
        ur_data["dataset"] = dataset_id

        for id_ in ur_data["executables"].itervalues():
            self.objs[id_]["submission"] = user_test_id
            self.objs[id_]["dataset"] = dataset_id
            self.objs[id_]["submission_result"] = ur_id

########NEW FILE########
__FILENAME__ = update_3
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This adapts the dump to some changes in the model introduced in commits
483ee85965f527cbc459ebe7e010c6854661b6eb
e051ee4d2667ba381a24c7b1764a6d9c3d792b45
d66951d3149a954fb0b81b6015e8e0b060020152

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function


class Updater(object):

    def __init__(self, data):
        assert data["_version"] == 2
        self.objs = data

    def run(self):
        for k, v in self.objs.iteritems():
            if k.startswith("_"):
                continue
            if v["_class"] == "User":
                if v["email"] == "":
                    v["email"] = None
                if v["ip"] == "0.0.0.0":
                    v["ip"] = None

        return self.objs

########NEW FILE########
__FILENAME__ = update_4
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This adapts the dump to some changes in the model introduced in commit
006de13788e380c284ef4ec96d31b50e547d8c45

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function


class Updater(object):

    def __init__(self, data):
        assert data["_version"] == 3
        self.objs = data
        self.testcases = dict()

    def run(self):
        for k, v in self.objs.iteritems():
            if k.startswith("_"):
                continue
            if v["_class"] == "Testcase":
                self.testcases[(v["dataset"], v["num"])] = k
                v["codename"] = "%03d" % v["num"]
                del v["num"]

        for k, v in self.objs.iteritems():
            if k.startswith("_"):
                continue
            if v["_class"] == "Evaluation":
                # Hmm... assuming it exists may be not so wise.
                v["testcase"] = self.testcases[(v["dataset"], v["num"])]
                del v["num"]

        return self.objs

########NEW FILE########
__FILENAME__ = update_5
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This adapts the dump to some changes in the model introduced in the
commit that created this same file.

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function


class Updater(object):

    def __init__(self, data):
        assert data["_version"] == 4
        self.objs = data

    def run(self):
        for k, v in self.objs.iteritems():
            if k.startswith("_"):
                continue
            if v["_class"] == "UserTestResult":
                v["execution_wall_clock_time"] = None

        return self.objs

########NEW FILE########
__FILENAME__ = update_6
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This adapts the dump to some changes in the model introduced in the
commit that created this same file and in commit
af2338b9a22df8a19671c7fee78d9dc4b35c49ea.

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function

import json


def parse_compilation_text(s):
    if s is None:
        return None, None, None, None, None
    if s == "No compilation needed.":
        return "[\"No compilation needed\"]", None, None, None, None
    s = s.split('\n')
    if s[0].startswith('Killed with signal '):
        del s[1]
    status, stats = s[0].split(' [')
    time, memory = stats.rstrip(']').split(' - ')
    stdout, _, stderr = "\n".join(s[1:])[len("Compiler standard output:\n"):]\
                        .partition("\nCompiler standard error:\n")
    status = status.split(' ')
    text = [{'OK': "Compilation succeeded",
             'Failed': "Compilation failed",
             'Time': "Compilation timed out",
             'Killed': "Compilation killed with signal %d (could be triggered "
                       "by violating memory limits)"}[status[0]]]
    if status[0] == "Killed":
        text += [int(status[-1])]
    if time == "(time unknown)":
        time = None
    else:
        time = float(time.partition(' ')[0])
    if memory == "(memory usage unknown)":
        memory = None
    else:
        memory = int(float(memory.partition(' ')[0]) * 1024 * 1024)
    if stdout == "(empty)\n":
        stdout = ""
    if stderr == "(empty)\n":
        stderr = ""
    return json.dumps(text), time, memory, stdout, stderr


def parse_evaluation_text(s):
    if s is None:
        return None
    # Catch both "Evaluation" and "Execution".
    if "tion didn't produce file " in s:
        res = ["Evaluation didn't produce file %s", ' '.join(s.split(' ')[4:])]
    elif s.startswith("Execution killed with signal "):
        res = ["Execution killed with signal %d (could be triggered by "
               "violating memory limits)", int(s.rpartition(' ')[2][:-1])]
    elif s.startswith("Execution killed because of forbidden syscall "):
        res = ["Execution killed because of forbidden syscall %s",
               s.rpartition(' ')[2][:-1]]
    elif s in ["Execution failed because the return code was nonzero.",
               "Execution killed because of forbidden file access.",
               "Execution timed out."]:
        res = [s.rstrip('.')]
    else:
        res = [s]
    return json.dumps(res)


def parse_tc_details(s):
    for i in s:
        if "text" in i:
            i["text"] = parse_evaluation_text(i["text"])
    return s


def parse_st_details(s):
    for i in s:
        parse_tc_details(i["testcases"])
    return s


class Updater(object):

    def __init__(self, data):
        assert data["_version"] == 5
        self.objs = data

    def run(self):
        for k, v in self.objs.iteritems():
            if k.startswith("_"):
                continue

            # Compilation
            if v["_class"] in ("SubmissionResult", "UserTestResult"):
                # Parse compilation_text
                v["compilation_text"], v["compilation_time"], \
                    v["compilation_memory"], v["compilation_stdout"], \
                    v["compilation_stderr"] = \
                    parse_compilation_text(v["compilation_text"])
                v["compilation_wall_clock_time"] = None

            # Evaluation
            if v["_class"] == "Evaluation":
                v["text"] = parse_evaluation_text(v["text"])
                v["execution_memory"] = v["memory_used"]
                del v["memory_used"]

            if v["_class"] == "UserTestResult":
                v["evaluation_text"] = \
                    parse_evaluation_text(v["evaluation_text"])
                v["evaluation_memory"] = v["memory_used"]
                del v["memory_used"]

            # Scoring
            if v["_class"] == "SubmissionResult":
                s = v.get("score")
                sd = v.get("score_details")
                ps = v.get("public_score")
                psd = v.get("public_score_details")
                rsd = v.get("ranking_score_details")

                if self.objs[v["dataset"]]["score_type"] == "Sum":
                    if sd is not None:
                        sd = json.dumps(parse_tc_details(json.loads(sd)))
                    if psd is not None:
                        psd = json.dumps(parse_tc_details(json.loads(psd)))
                else:  # Group*
                    if sd is not None:
                        sd = json.dumps(parse_st_details(json.loads(sd)))
                    if psd is not None:
                        psd = json.dumps(parse_st_details(json.loads(psd)))

                if rsd is not None:
                    rsd = json.dumps(
                        list(i.strip() for i in rsd[1:-1].split(',')))

                v["score"] = s
                v["score_details"] = sd
                v["public_score"] = ps
                v["public_score_details"] = psd
                v["ranking_score_details"] = rsd

        return self.objs

########NEW FILE########
__FILENAME__ = update_7
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This adapts the dump to the addition of configurable languages per
contest done in the same commit.

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function


class Updater(object):

    def __init__(self, data):
        assert data["_version"] == 6
        self.objs = data

    def run(self):
        for k, v in self.objs.iteritems():
            if k.startswith("_"):
                continue
            if v["_class"] == "Contest":
                v["languages"] = [u"c", u"cpp", u"pas"]

        return self.objs

########NEW FILE########
__FILENAME__ = update_8
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This adapts the dump to some changes in the model introduced in the
commit that created this same file.

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function

from datetime import timedelta


class Updater(object):

    def __init__(self, data):
        assert data["_version"] == 7
        self.objs = data

    def run(self):
        for k, v in self.objs.iteritems():
            if k.startswith("_"):
                continue
            if v["_class"] == "Contest" or v["_class"] == "Task":
                if v["token_initial"] is None:
                    v["token_mode"] = "disabled"
                elif v["token_gen_number"] > 0 and v["token_gen_time"] == 0:
                    v["token_mode"] = "infinite"
                else:
                    v["token_mode"] = "finite"

                v["token_max_number"] = v.pop("token_total")
                v["token_gen_initial"] = v.pop("token_initial")
                v["token_gen_interval"] = v.pop("token_gen_time")
                v["token_gen_max"] = v.pop("token_max")

                if v["token_gen_initial"] is None:
                    v["token_gen_initial"] = 0
                if v["token_gen_interval"] == 0:
                    v["token_gen_interval"] = 60

        return self.objs

########NEW FILE########
__FILENAME__ = update_9
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2014 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""A class to update a dump created by CMS.

Used by ContestImporter and DumpUpdater.

This is a fake updater that warns about a change in the way Java
submissions are compiled. This applies only to batch tasks with no
grader (as it was the only task type supporting Java). The difference
is that before the public class submitted by contestant had to be
named "Task", whereas now it must be called with the task short name.

"""

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function

import logging


logger = logging.getLogger(__name__)


class Updater(object):

    def __init__(self, data):
        assert data["_version"] == 8
        self.objs = data

    def run(self):
        for k, v in self.objs.iteritems():
            if k.startswith("_"):
                continue
            if v["_class"] == "Submission" and v["language"] == "java":
                logger.warning(
                    "The way Java submissions are compiled has changed, and\n"
                    "previously valid submissions will now fail to compile.\n"
                    "If you want to obtain again the same results, you have\n"
                    "to (manually) change the name of the public class, from\n"
                    "the literal \"Task\" to the short name of the task,\n"
                    "in each submitted Java file.")
                break

        return self.objs

########NEW FILE########
__FILENAME__ = YamlLoader
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013-2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import unicode_literals
from __future__ import print_function

import io
import logging
import os
import os.path
import sys
import yaml
from datetime import timedelta

from cms import LANGUAGES
from cmscommon.datetime import make_datetime
from cms.db import Contest, User, Task, Statement, Attachment, \
    SubmissionFormatElement, Dataset, Manager, Testcase
from cmscontrib.BaseLoader import Loader
from cmscontrib import touch


logger = logging.getLogger(__name__)


# Patch PyYAML to make it load all strings as unicode instead of str
# (see http://stackoverflow.com/questions/2890146).
def construct_yaml_str(self, node):
    return self.construct_scalar(node)
yaml.Loader.add_constructor("tag:yaml.org,2002:str", construct_yaml_str)
yaml.SafeLoader.add_constructor("tag:yaml.org,2002:str", construct_yaml_str)


def load(src, dst, src_name, dst_name=None, conv=lambda i: i):
    """Execute:
      dst[dst_name] = conv(src[src_name])
    with the following features:

      * If src_name is a list, it tries each of its element as
        src_name, stopping when the first one succedes.

      * If dst_name is None, it is set to src_name; if src_name is a
        list, dst_name is set to src_name[0] (_not_ the one that
        succedes).

      * By default conv is the identity function.

      * If dst is None, instead of assigning the result to
        dst[dst_name] (which would cast an exception) it just returns
        it.

      * If src[src_name] doesn't exist, the behavior is different
        depending on whether dst is None or not: if dst is None,
        conv(None) is returned; if dst is not None, nothing is done
        (in particular, dst[dst_name] is _not_ assigned to conv(None);
        it is not assigned to anything!).

    """
    if dst is not None and dst_name is None:
        if isinstance(src_name, list):
            dst_name = src_name[0]
        else:
            dst_name = src_name
    res = None
    found = False
    if isinstance(src_name, list):
        for this_src_name in src_name:
            try:
                res = src[this_src_name]
            except KeyError:
                pass
            else:
                found = True
                break
    else:
        if src_name in src:
            found = True
            res = src[src_name]
    if dst is not None:
        if found:
            dst[dst_name] = conv(res)
    else:
        return conv(res)


def make_timedelta(t):
    return timedelta(seconds=t)


class YamlLoader(Loader):
    """Load a contest stored using the Italian IOI format.

    Given the filesystem location of a contest saved in the Italian IOI
    format, parse those files and directories to produce data that can
    be consumed by CMS, i.e. a hierarchical collection of instances of
    the DB classes, headed by a Contest object, and completed with all
    needed (and available) child objects.

    """

    short_name = 'italy_yaml'
    description = 'Italian YAML-based format'

    @classmethod
    def detect(cls, path):
        """See docstring in class Loader.

        """
        # TODO - Not really refined...
        return os.path.exists(os.path.join(path, "contest.yaml"))

    def get_contest(self):
        """See docstring in class Loader.

        """

        name = os.path.split(self.path)[1]
        conf = yaml.safe_load(
            io.open(os.path.join(self.path, "contest.yaml"),
                    "rt", encoding="utf-8"))

        logger.info("Loading parameters for contest %s." % name)

        args = {}

        load(conf, args, ["name", "nome_breve"])
        load(conf, args, ["description", "nome"])

        assert name == args["name"]

        # Use the new token settings format if detected.
        if "token_mode" in conf:
            load(conf, args, "token_mode")
            load(conf, args, "token_max_number")
            load(conf, args, "token_min_interval", conv=make_timedelta)
            load(conf, args, "token_gen_initial")
            load(conf, args, "token_gen_number")
            load(conf, args, "token_gen_interval", conv=make_timedelta)
            load(conf, args, "token_gen_max")
        # Otherwise fall back on the old one.
        else:
            logger.warning(
                "contest.yaml uses a deprecated format for token settings "
                "which will soon stop being supported, you're advised to "
                "update it.")
            # Determine the mode.
            if conf.get("token_initial", None) is None:
                args["token_mode"] = "disabled"
            elif conf.get("token_gen_number", 0) > 0 and \
                    conf.get("token_gen_time", 0) == 0:
                args["token_mode"] = "infinite"
            else:
                args["token_mode"] = "finite"
            # Set the old default values.
            args["token_gen_initial"] = 0
            args["token_gen_number"] = 0
            args["token_gen_interval"] = timedelta()
            # Copy the parameters to their new names.
            load(conf, args, "token_total", "token_max_number")
            load(conf, args, "token_min_interval", conv=make_timedelta)
            load(conf, args, "token_initial", "token_gen_initial")
            load(conf, args, "token_gen_number")
            load(conf, args, "token_gen_time", "token_gen_interval",
                 conv=make_timedelta)
            load(conf, args, "token_max", "token_gen_max")
            # Remove some corner cases.
            if args["token_gen_initial"] is None:
                args["token_gen_initial"] = 0
            if args["token_gen_interval"].total_seconds() == 0:
                args["token_gen_interval"] = timedelta(minutes=1)

        load(conf, args, ["start", "inizio"], conv=make_datetime)
        load(conf, args, ["stop", "fine"], conv=make_datetime)
        load(conf, args, ["per_user_time"], conv=make_timedelta)

        load(conf, args, "max_submission_number")
        load(conf, args, "max_user_test_number")
        load(conf, args, "min_submission_interval", conv=make_timedelta)
        load(conf, args, "min_user_test_interval", conv=make_timedelta)

        logger.info("Contest parameters loaded.")

        tasks = load(conf, None, ["tasks", "problemi"])
        self.tasks_order = dict((name, num)
                                for num, name in enumerate(tasks))
        self.users_conf = dict((user['username'], user)
                               for user
                               in load(conf, None, ["users", "utenti"]))
        users = self.users_conf.keys()

        return Contest(**args), tasks, users

    def has_changed(self, name):
        """See docstring in class Loader

        """
        path = os.path.realpath(os.path.join(self.path, name))

        try:
            conf = yaml.safe_load(
                io.open(os.path.join(path, "task.yaml"),
                        "rt", encoding="utf-8"))
        except IOError:
            conf = yaml.safe_load(
                io.open(os.path.join(self.path, name + ".yaml"),
                        "rt", encoding="utf-8"))

        # If there is no .itime file, we assume that the task has changed
        if not os.path.exists(os.path.join(path, ".itime")):
            return True

        getmtime = lambda fname: os.stat(fname).st_mtime

        itime = getmtime(os.path.join(path, ".itime"))

        # Generate a task's list of files
        # Testcases
        files = []
        for filename in os.listdir(os.path.join(path, "input")):
            files.append(os.path.join(path, "input", filename))

        for filename in os.listdir(os.path.join(path, "output")):
            files.append(os.path.join(path, "output", filename))

        # Attachments
        if os.path.exists(os.path.join(path, "att")):
            for filename in os.listdir(os.path.join(path, "att")):
                files.append(os.path.join(path, "att", filename))

        # Score file
        files.append(os.path.join(path, "gen", "GEN"))

        # Statement
        files.append(os.path.join(path, "statement", "statement.pdf"))
        files.append(os.path.join(path, "testo", "testo.pdf"))

        # Managers
        files.append(os.path.join(path, "check", "checker"))
        files.append(os.path.join(path, "cor", "correttore"))
        files.append(os.path.join(path, "check", "manager"))
        files.append(os.path.join(path, "cor", "manager"))
        if not conf.get('output_only', False) and \
                os.path.isdir(os.path.join(path, "sol")):
            for lang in LANGUAGES:
                files.append(os.path.join(path, "sol", "grader.%s" % lang))
            for other_filename in os.listdir(os.path.join(path, "sol")):
                if other_filename.endswith('.h') or \
                        other_filename.endswith('lib.pas'):
                    files.append(os.path.join(path, "sol", other_filename))

        # Yaml
        files.append(os.path.join(path, "task.yaml"))
        files.append(os.path.join(self.path, name + ".yaml"))

        # Check is any of the files have changed
        for fname in files:
            if os.path.exists(fname):
                if getmtime(fname) > itime:
                    return True

        if os.path.exists(os.path.join(path, ".import_error")):
            logger.warning("Last attempt to import task %s failed,"
                           " I'm not trying again." % name)
        return False

    def get_user(self, username):
        """See docstring in class Loader.

        """
        logger.info("Loading parameters for user %s." % username)
        conf = self.users_conf[username]
        assert username == conf['username']

        args = {}

        load(conf, args, "username")

        load(conf, args, "password")
        load(conf, args, "ip")

        load(conf, args, ["first_name", "nome"])
        load(conf, args, ["last_name", "cognome"])

        if "first_name" not in args:
            args["first_name"] = ""
        if "last_name" not in args:
            args["last_name"] = args["username"]

        load(conf, args, ["hidden", "fake"],
             conv=lambda a: a is True or a == "True")

        logger.info("User parameters loaded.")

        return User(**args)

    def get_task(self, name):
        """See docstring in class Loader.

        """
        try:
            num = self.tasks_order[name]

        # Here we expose an undocumented behavior, so that cmsMake can
        # import a task even without the whole contest; this is not to
        # be relied upon in general
        except AttributeError:
            num = 1

        task_path = os.path.join(self.path, name)

        # We first look for the yaml file inside the task folder,
        # and eventually fallback to a yaml file in its parent folder.
        try:
            conf = yaml.safe_load(
                io.open(os.path.join(task_path, "task.yaml"),
                        "rt", encoding="utf-8"))
        except IOError:
            conf = yaml.safe_load(
                io.open(os.path.join(self.path, name + ".yaml"),
                        "rt", encoding="utf-8"))

        logger.info("Loading parameters for task %s." % name)

        # Here we update the time of the last import
        touch(os.path.join(task_path, ".itime"))
        # If this file is not deleted, then the import failed
        touch(os.path.join(task_path, ".import_error"))

        args = {}

        args["num"] = num
        load(conf, args, ["name", "nome_breve"])
        load(conf, args, ["title", "nome"])

        assert name == args["name"]

        if args["name"] == args["title"]:
            logger.warning("Short name equals long name (title). "
                           "Please check.")

        primary_language = load(conf, None, "primary_language")
        if primary_language is None:
            primary_language = 'it'
        paths = [os.path.join(task_path, "statement", "statement.pdf"),
                 os.path.join(task_path, "testo", "testo.pdf")]
        for path in paths:
            if os.path.exists(path):
                digest = self.file_cacher.put_file_from_path(
                    path,
                    "Statement for task %s (lang: %s)" % (name,
                                                          primary_language))
                break
        else:
            logger.critical("Couldn't find any task statement, aborting...")
            sys.exit(1)
        args["statements"] = [Statement(primary_language, digest)]

        args["primary_statements"] = '["%s"]' % (primary_language)

        args["attachments"] = []  # FIXME Use auxiliary

        args["submission_format"] = [
            SubmissionFormatElement("%s.%%l" % name)]

        # Use the new token settings format if detected.
        if "token_mode" in conf:
            load(conf, args, "token_mode")
            load(conf, args, "token_max_number")
            load(conf, args, "token_min_interval", conv=make_timedelta)
            load(conf, args, "token_gen_initial")
            load(conf, args, "token_gen_number")
            load(conf, args, "token_gen_interval", conv=make_timedelta)
            load(conf, args, "token_gen_max")
        # Otherwise fall back on the old one.
        else:
            logger.warning(
                "%s.yaml uses a deprecated format for token settings which "
                "will soon stop being supported, you're advised to update it.",
                name)
            # Determine the mode.
            if conf.get("token_initial", None) is None:
                args["token_mode"] = "disabled"
            elif conf.get("token_gen_number", 0) > 0 and \
                    conf.get("token_gen_time", 0) == 0:
                args["token_mode"] = "infinite"
            else:
                args["token_mode"] = "finite"
            # Set the old default values.
            args["token_gen_initial"] = 0
            args["token_gen_number"] = 0
            args["token_gen_interval"] = timedelta()
            # Copy the parameters to their new names.
            load(conf, args, "token_total", "token_max_number")
            load(conf, args, "token_min_interval", conv=make_timedelta)
            load(conf, args, "token_initial", "token_gen_initial")
            load(conf, args, "token_gen_number")
            load(conf, args, "token_gen_time", "token_gen_interval",
                 conv=make_timedelta)
            load(conf, args, "token_max", "token_gen_max")
            # Remove some corner cases.
            if args["token_gen_initial"] is None:
                args["token_gen_initial"] = 0
            if args["token_gen_interval"].total_seconds() == 0:
                args["token_gen_interval"] = timedelta(minutes=1)

        load(conf, args, "max_submission_number")
        load(conf, args, "max_user_test_number")
        load(conf, args, "min_submission_interval", conv=make_timedelta)
        load(conf, args, "min_user_test_interval", conv=make_timedelta)

        # Attachments
        args["attachments"] = []
        if os.path.exists(os.path.join(task_path, "att")):
            for filename in os.listdir(os.path.join(task_path, "att")):
                digest = self.file_cacher.put_file_from_path(
                    os.path.join(task_path, "att", filename),
                    "Attachment %s for task %s" % (filename, name))
                args["attachments"] += [Attachment(filename, digest)]

        task = Task(**args)

        args = {}
        args["task"] = task
        args["description"] = conf.get("version", "Default")
        args["autojudge"] = False

        load(conf, args, ["time_limit", "timeout"], conv=float)
        load(conf, args, ["memory_limit", "memlimit"])

        # Builds the parameters that depend on the task type
        args["managers"] = []
        infile_param = conf.get("infile", "input.txt")
        outfile_param = conf.get("outfile", "output.txt")

        # If there is sol/grader.%l for some language %l, then,
        # presuming that the task type is Batch, we retrieve graders
        # in the form sol/grader.%l
        graders = False
        for lang in LANGUAGES:
            if os.path.exists(os.path.join(
                    task_path, "sol", "grader.%s" % lang)):
                graders = True
                break
        if graders:
            # Read grader for each language
            for lang in LANGUAGES:
                grader_filename = os.path.join(
                    task_path, "sol", "grader.%s" % lang)
                if os.path.exists(grader_filename):
                    digest = self.file_cacher.put_file_from_path(
                        grader_filename,
                        "Grader for task %s and language %s" % (name, lang))
                    args["managers"] += [
                        Manager("grader.%s" % lang, digest)]
                else:
                    logger.warning("Grader for language %s not found " % lang)
            # Read managers with other known file extensions
            for other_filename in os.listdir(os.path.join(task_path, "sol")):
                if other_filename.endswith('.h') or \
                        other_filename.endswith('lib.pas'):
                    digest = self.file_cacher.put_file_from_path(
                        os.path.join(task_path, "sol", other_filename),
                        "Manager %s for task %s" % (other_filename, name))
                    args["managers"] += [
                        Manager(other_filename, digest)]
            compilation_param = "grader"
        else:
            compilation_param = "alone"

        # If there is check/checker (or equivalent), then, presuming
        # that the task type is Batch or OutputOnly, we retrieve the
        # comparator
        paths = [os.path.join(task_path, "check", "checker"),
                 os.path.join(task_path, "cor", "correttore")]
        for path in paths:
            if os.path.exists(path):
                digest = self.file_cacher.put_file_from_path(
                    path,
                    "Manager for task %s" % name)
                args["managers"] += [
                    Manager("checker", digest)]
                evaluation_param = "comparator"
                break
        else:
            evaluation_param = "diff"

        # Detect subtasks by checking GEN
        gen_filename = os.path.join(task_path, 'gen', 'GEN')
        try:
            with io.open(gen_filename, "rt", encoding="utf-8") as gen_file:
                subtasks = []
                testcases = 0
                points = None
                for line in gen_file:
                    line = line.strip()
                    splitted = line.split('#', 1)

                    if len(splitted) == 1:
                        # This line represents a testcase, otherwise it's
                        # just a blank
                        if splitted[0] != '':
                            testcases += 1

                    else:
                        testcase, comment = splitted
                        testcase_detected = False
                        subtask_detected = False
                        if testcase.strip() != '':
                            testcase_detected = True
                        comment = comment.strip()
                        if comment.startswith('ST:'):
                            subtask_detected = True

                        if testcase_detected and subtask_detected:
                            raise Exception("No testcase and subtask in the"
                                            " same line allowed")

                        # This line represents a testcase and contains a
                        # comment, but the comment doesn't start a new
                        # subtask
                        if testcase_detected:
                            testcases += 1

                        # This line starts a new subtask
                        if subtask_detected:
                            # Close the previous subtask
                            if points is None:
                                assert(testcases == 0)
                            else:
                                subtasks.append([points, testcases])
                            # Open the new one
                            testcases = 0
                            points = int(comment[3:].strip())

                # Close last subtask (if no subtasks were defined, just
                # fallback to Sum)
                if points is None:
                    args["score_type"] = "Sum"
                    total_value = float(conf.get("total_value", 100.0))
                    input_value = 0.0
                    n_input = testcases
                    if n_input != 0:
                        input_value = total_value / n_input
                    args["score_type_parameters"] = str(input_value)
                else:
                    subtasks.append([points, testcases])
                    assert(100 == sum([int(st[0]) for st in subtasks]))
                    n_input = sum([int(st[1]) for st in subtasks])
                    args["score_type"] = "GroupMin"
                    args["score_type_parameters"] = str(subtasks)

                if "n_input" in conf:
                    assert int(conf['n_input']) == n_input

        # If gen/GEN doesn't exist, just fallback to Sum
        except IOError:
            args["score_type"] = "Sum"
            total_value = float(conf.get("total_value", 100.0))
            input_value = 0.0
            n_input = int(conf['n_input'])
            if n_input != 0:
                input_value = total_value / n_input
            args["score_type_parameters"] = str(input_value)

        # If output_only is set, then the task type is OutputOnly
        if conf.get('output_only', False):
            args["task_type"] = "OutputOnly"
            args["time_limit"] = None
            args["memory_limit"] = None
            args["task_type_parameters"] = '["%s"]' % evaluation_param
            task.submission_format = [
                SubmissionFormatElement("output_%03d.txt" % i)
                for i in xrange(n_input)]

        # If there is check/manager (or equivalent), then the task
        # type is Communication
        else:
            paths = [os.path.join(task_path, "check", "manager"),
                     os.path.join(task_path, "cor", "manager")]
            for path in paths:
                if os.path.exists(path):
                    args["task_type"] = "Communication"
                    args["task_type_parameters"] = '[]'
                    digest = self.file_cacher.put_file_from_path(
                        path,
                        "Manager for task %s" % name)
                    args["managers"] += [
                        Manager("manager", digest)]
                    for lang in LANGUAGES:
                        stub_name = os.path.join(
                            task_path, "sol", "stub.%s" % lang)
                        if os.path.exists(stub_name):
                            digest = self.file_cacher.put_file_from_path(
                                stub_name,
                                "Stub for task %s and language %s" % (name,
                                                                      lang))
                            args["managers"] += [
                                Manager("stub.%s" % lang, digest)]
                        else:
                            logger.warning("Stub for language %s not "
                                           "found." % lang)
                    break

            # Otherwise, the task type is Batch
            else:
                args["task_type"] = "Batch"
                args["task_type_parameters"] = \
                    '["%s", ["%s", "%s"], "%s"]' % \
                    (compilation_param, infile_param, outfile_param,
                     evaluation_param)

        args["testcases"] = []
        for i in xrange(n_input):
            input_digest = self.file_cacher.put_file_from_path(
                os.path.join(task_path, "input", "input%d.txt" % i),
                "Input %d for task %s" % (i, name))
            output_digest = self.file_cacher.put_file_from_path(
                os.path.join(task_path, "output", "output%d.txt" % i),
                "Output %d for task %s" % (i, name))
            args["testcases"] += [
                Testcase("%03d" % i, False, input_digest, output_digest)]
            if args["task_type"] == "OutputOnly":
                task.attachments += [
                    Attachment("input_%03d.txt" % i, input_digest)]
        public_testcases = load(conf, None, ["public_testcases", "risultati"],
                                conv=lambda x: "" if x is None else x)
        if public_testcases != "":
            for x in public_testcases.split(","):
                args["testcases"][int(x.strip())].public = True

        dataset = Dataset(**args)
        task.active_dataset = dataset

        # Import was successful
        os.remove(os.path.join(task_path, ".import_error"))

        logger.info("Task parameters loaded.")

        return task

########NEW FILE########
__FILENAME__ = Config
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import io
import json
import os
import pkg_resources
import sys


class Config(object):
    """An object holding the current configuration.

    """
    def __init__(self):
        """Fill this object with the default values for each key.

        """
        # Connection.
        self.bind_address = ''
        self.http_port = 8890
        self.https_port = None
        self.https_certfile = None
        self.https_keyfile = None
        self.timeout = 600  # 10 minutes (in seconds)

        # Authentication.
        self.realm_name = 'Scoreboard'
        self.username = 'usern4me'
        self.password = 'passw0rd'

        # Buffers
        self.buffer_size = 100  # Needs to be strictly positive.

        # File system.
        self.installed = sys.argv[0].startswith("/usr/") and \
            sys.argv[0] != '/usr/bin/ipython' and \
            sys.argv[0] != '/usr/bin/python2' and \
            sys.argv[0] != '/usr/bin/python'

        self.web_dir = pkg_resources.resource_filename("cmsranking", "static")
        if self.installed:
            self.log_dir = os.path.join("/", "var", "local", "log",
                                        "cms", "ranking")
            self.lib_dir = os.path.join("/", "var", "local", "lib",
                                        "cms", "ranking")
            paths = [os.path.join("/", "usr", "local", "etc",
                                  "cms.ranking.conf"),
                     os.path.join("/", "etc", "cms.ranking.conf")]
        else:
            self.log_dir = os.path.join("log", "ranking")
            self.lib_dir = os.path.join("lib", "ranking")
            paths = [os.path.join(".", "examples", "cms.ranking.conf"),
                     os.path.join("/", "usr", "local", "etc",
                                  "cms.ranking.conf"),
                     os.path.join("/", "etc", "cms.ranking.conf")]

        try:
            os.makedirs(self.lib_dir)
        except OSError:
            pass  # We assume the directory already exists...

        try:
            os.makedirs(self.web_dir)
        except OSError:
            pass  # We assume the directory already exists...

        try:
            os.makedirs(self.log_dir)
        except OSError:
            pass  # We assume the directory already exists...

        self._load(paths)

    def get(self, key):
        """Get the config value for the given key.

        """
        return getattr(self, key)

    def _load(self, paths):
        """Try to load the config files one at a time, until one loads
        correctly.

        """
        for conf_file in paths:
            try:
                self._load_unique(conf_file)
            except IOError:
                # We cannot access the file, we skip it.
                pass
            except ValueError as exc:
                print("Unable to load JSON configuration file %s, probably "
                      "because of a JSON decoding error.\n%r" % (conf_file,
                                                                 exc))
            else:
                print("Using configuration file %s." % conf_file)
                return
        print("Warning: no configuration file found.")

    def _load_unique(self, path):
        """Populate the Config class with everything that sits inside
        the JSON file path (usually something like /etc/cms.conf). The
        only pieces of data treated differently are the elements of
        core_services and other_services that are sent to async
        config.

        path (string): the path of the JSON config file.

        """
        # Load config file
        with io.open(path, 'rb') as fobj:
            data = json.load(fobj)

            # Put everything.
            for key, value in data.iteritems():
                setattr(self, key, value)


# Create an instance of the Config class.
config = Config()

########NEW FILE########
__FILENAME__ = Contest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import six

from cmsranking.Entity import Entity, InvalidData
from cmsranking.Store import Store
from cmsranking.Task import store as task_store


class Contest(Entity):
    """The entity representing a contest.

    It consists of the following properties:
    - name (unicode): the human-readable name of the contest
    - begin (int): the unix timestamp at which the contest begins
    - end (int): the unix timestamp at which the contest ends
    - score_precision (int): how many decimal places to show in scores

    """
    def __init__(self):
        """Set the properties to some default values.

        """
        Entity.__init__(self)
        self.name = None
        self.begin = None
        self.end = None
        self.score_precision = None

    @staticmethod
    def validate(data):
        """Validate the given dictionary.

        See if it contains a valid representation of this entity.

        """
        try:
            assert isinstance(data, dict), \
                "Not a dictionary"
            assert isinstance(data['name'], six.text_type), \
                "Field 'name' isn't a string"
            assert isinstance(data['begin'], six.integer_types), \
                "Field 'begin' isn't an integer"
            assert isinstance(data['end'], six.integer_types), \
                "Field 'end' isn't an integer"
            assert data['begin'] <= data['end'], \
                "Field 'begin' is greater than 'end'"
            assert isinstance(data['score_precision'], six.integer_types), \
                "Field 'score_precision' isn't an integer"
            assert data['score_precision'] >= 0, \
                "Field 'score_precision' is negative"
        except KeyError as exc:
            raise InvalidData("Field %s is missing" % exc.message)
        except AssertionError as exc:
            raise InvalidData(exc.message)

    def set(self, data):
        self.validate(data)
        self.name = data['name']
        self.begin = data['begin']
        self.end = data['end']
        self.score_precision = data['score_precision']

    def get(self):
        result = self.__dict__.copy()
        del result['key']
        return result


store = Store(Contest, 'contests', [task_store])

########NEW FILE########
__FILENAME__ = Entity
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals


class InvalidKey(Exception):
    """Exception raised in case of invalid key."""
    pass


class InvalidData(Exception):
    """Exception raised in case of invalid data."""
    pass


class Entity(object):
    """Base virtual class which all entities should extend.

    Provide some virtual methods that other classes should implement.

    """
    def set(self, data):
        """Set all properties using the given data.

        Accept the data format used on the HTTP interface.

        data (dict): the properties of the entity, in the "external"
            format

        Raise InvalidData if not able to parse the data argument.

        """
        pass

    def get(self):
        """Get all properties.

        Produce the data format used on the HTTP interface.

        return (dict): the properties of the entity, in the "external"
            format

        """
        pass

    def consistent(self):
        """Check if the entity is consistent.

        Verify that all references to other entities are correct (i.e.
        those entities actually exist).

        return (bool): the result of this check

        """
        return True

########NEW FILE########
__FILENAME__ = Logger
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import curses
import logging
import os.path
import sys
import time
from traceback import format_tb

import gevent.coros

from cmsranking.Config import config


class StreamHandler(logging.StreamHandler):
    """Subclass to make gevent-aware.

    Use a gevent lock instead of a threading one to block only the
    current greenlet.

    """
    def createLock(self):
        """Set self.lock to a new gevent RLock.

        """
        self.lock = gevent.coros.RLock()


class FileHandler(logging.FileHandler):
    """Subclass to make gevent-aware.

    Use a gevent lock instead of a threading one to block only the
    current greenlet.

    """
    def createLock(self):
        """Set self.lock to a new gevent RLock.

        """
        self.lock = gevent.coros.RLock()


def has_color_support(stream):
    """Try to determine if the given stream supports colored output.

    Return True only if the stream declares to be a TTY, if it has a
    file descriptor on which ncurses can initialize a terminal and if
    that terminal's entry in terminfo declares support for colors.

    stream (fileobj): a file-like object (that adheres to the API
        declared in the `io' package).

    return (bool): True if we're sure that colors are supported, False
        if they aren't or if we can't tell.

    """
    if stream.isatty():
        try:
            curses.setupterm(fd=stream.fileno())
            # See `man terminfo` for capabilities' names and meanings.
            if curses.tigetnum("colors") > 0:
                return True
        # fileno() can raise IOError or OSError (since Python 3.3).
        except Exception:
            pass
    return False


## ANSI utilities. See for reference:
# http://pueblo.sourceforge.net/doc/manual/ansi_color_codes.html
# http://en.wikipedia.org/wiki/ANSI_escape_code
#

ANSI_FG_COLORS = {'black': 30,
                  'red': 31,
                  'green': 32,
                  'yellow': 33,
                  'blue': 34,
                  'magenta': 35,
                  'cyan': 36,
                  'white': 37}

ANSI_BG_COLORS = {'black': 40,
                  'red': 41,
                  'green': 42,
                  'yellow': 43,
                  'blue': 44,
                  'magenta': 45,
                  'cyan': 46,
                  'white': 47}

ANSI_RESET_CMD = 0
ANSI_FG_DEFAULT_CMD = 39
ANSI_BG_DEFAULT_CMD = 49
ANSI_BOLD_ON_CMD = 1
ANSI_BOLD_OFF_CMD = 22
ANSI_FAINT_ON_CMD = 2
ANSI_FAINT_OFF_CMD = 22
ANSI_ITALICS_ON_CMD = 3
ANSI_ITALICS_OFF_CMD = 23
ANSI_UNDERLINE_ON_CMD = 4
ANSI_UNDERLINE_OFF_CMD = 24
ANSI_STRIKETHROUGH_ON_CMD = 9
ANSI_STRIKETHROUGH_OFF_CMD = 29
ANSI_INVERSE_ON_CMD = 7
ANSI_INVERSE_OFF_CMD = 27

# TODO missing:
# - distinction between single and double underline
# - "slow blink on", "rapid blink on" and "blink off"
# - "conceal on" and "conceal off" (also called reveal)


class CustomFormatter(logging.Formatter):
    """A custom Formatter for our logs.

    """
    def __init__(self, color=True, *args, **kwargs):
        """Initialize the formatter.

        Based on the 'color' parameter we set the tags for many
        elements of our formatted output.

        """
        logging.Formatter.__init__(self, *args, **kwargs)

        self.color = color

        self.time_prefix = self.ansi_command(ANSI_BOLD_ON_CMD)
        self.time_suffix = self.ansi_command(ANSI_BOLD_OFF_CMD)
        self.cri_prefix = self.ansi_command(ANSI_BOLD_ON_CMD,
                                            ANSI_FG_COLORS['white'],
                                            ANSI_BG_COLORS['red'])
        self.cri_suffix = self.ansi_command(ANSI_BOLD_OFF_CMD,
                                            ANSI_FG_DEFAULT_CMD,
                                            ANSI_BG_DEFAULT_CMD)
        self.err_prefix = self.ansi_command(ANSI_BOLD_ON_CMD,
                                            ANSI_FG_COLORS['red'])
        self.err_suffix = self.ansi_command(ANSI_BOLD_OFF_CMD,
                                            ANSI_FG_DEFAULT_CMD)
        self.wrn_prefix = self.ansi_command(ANSI_BOLD_ON_CMD,
                                            ANSI_FG_COLORS['yellow'])
        self.wrn_suffix = self.ansi_command(ANSI_BOLD_OFF_CMD,
                                            ANSI_FG_DEFAULT_CMD)
        self.inf_prefix = self.ansi_command(ANSI_BOLD_ON_CMD,
                                            ANSI_FG_COLORS['green'])
        self.inf_suffix = self.ansi_command(ANSI_BOLD_OFF_CMD,
                                            ANSI_FG_DEFAULT_CMD)
        self.dbg_prefix = self.ansi_command(ANSI_BOLD_ON_CMD,
                                            ANSI_FG_COLORS['blue'])
        self.dbg_suffix = self.ansi_command(ANSI_BOLD_OFF_CMD,
                                            ANSI_FG_DEFAULT_CMD)

    def ansi_command(self, *args):
        """Produce the escape string that corresponds to the given
        ANSI command.

        """
        return "\033[%sm" % ';'.join([str(x)
                                      for x in args]) if self.color else ''

    def formatException(self, exc_info):
        exc_type, exc_value, traceback = exc_info

        result = "%sException%s %s.%s%s%s:\n\n    %s\n\n" % \
            (self.ansi_command(ANSI_BOLD_ON_CMD),
             self.ansi_command(ANSI_BOLD_OFF_CMD),
             exc_type.__module__,
             self.ansi_command(ANSI_BOLD_ON_CMD),
             exc_type.__name__,
             self.ansi_command(ANSI_BOLD_OFF_CMD),
             exc_value)
        result += "%sTraceback (most recent call last):%s\n%s" % \
            (self.ansi_command(ANSI_FAINT_ON_CMD),
             self.ansi_command(ANSI_FAINT_OFF_CMD),
             '\n'.join(map(lambda a: self.ansi_command(ANSI_FAINT_ON_CMD) +
                           a + self.ansi_command(ANSI_FAINT_OFF_CMD),
                           ''.join(format_tb(traceback)).strip().split('\n'))))

        return result

    def format(self, record):
        """Do the actual formatting.

        Prepend a timestamp and an abbreviation of the logging level,
        followed by the message, the request_body (if present) and the
        exception details (if present).

        """
        result = '%s%s.%03d%s' % \
            (self.time_prefix,
             self.formatTime(record, '%Y-%m-%d %H:%M:%S'), record.msecs,
             self.time_suffix)

        if record.levelno == logging.CRITICAL:
            result += ' %s CRI %s ' % (self.cri_prefix, self.cri_suffix)
        elif record.levelno == logging.ERROR:
            result += ' %s ERR %s ' % (self.err_prefix, self.err_suffix)
        elif record.levelno == logging.WARNING:
            result += ' %s WRN %s ' % (self.wrn_prefix, self.wrn_suffix)
        elif record.levelno == logging.INFO:
            result += ' %s INF %s ' % (self.inf_prefix, self.inf_suffix)
        else:  # DEBUG
            result += ' %s DBG %s ' % (self.dbg_prefix, self.dbg_suffix)

        try:
            message = record.getMessage()
        except Exception, exc:
            message = 'Bad message (%r): %r' % (exc, record.__dict__)

        result += message.strip()

        if "location" in record.__dict__:
            result += "\n%s" % record.location.strip()

        if "details" in record.__dict__:
            result += "\n\n%s" % record.details.strip()

        if record.exc_info:
            result += "\n\n%s" % self.formatException(record.exc_info).strip()

        return result.replace("\n", "\n    ") + '\n'


# Create a global reference to the root logger.
root_logger = logging.getLogger()
# Catch all logging messages (we'll filter them on the handlers).
root_logger.setLevel(logging.DEBUG)

# Define the stream handler to output on stderr.
shell_handler = StreamHandler(sys.stdout)
shell_handler.setLevel(logging.INFO)
shell_handler.setFormatter(CustomFormatter(has_color_support(sys.stdout)))
root_logger.addHandler(shell_handler)

# Define the file handler to output on the specified log directory.
log_filename = time.strftime("%Y-%m-%d-%H-%M-%S.log")
file_handler = FileHandler(os.path.join(config.log_dir, log_filename),
                           mode='w', encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(CustomFormatter(False))
root_logger.addHandler(file_handler)

########NEW FILE########
__FILENAME__ = RankingWebServer
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import argparse
import functools
import io
import json
import logging
import os
import pprint
import re
import shutil
import time
from datetime import datetime

import gevent
from gevent.pywsgi import WSGIServer

from werkzeug.wrappers import Request, Response
from werkzeug.routing import Map, Rule
from werkzeug.exceptions import HTTPException, BadRequest, Unauthorized, \
    Forbidden, NotFound, NotAcceptable, UnsupportedMediaType
from werkzeug.wsgi import responder, wrap_file, SharedDataMiddleware, \
    DispatcherMiddleware
from werkzeug.utils import redirect

# Needed for initialization. Do not remove.
import cmsranking.Logger

from cmscommon.eventsource import EventSource
from cmsranking.Config import config
from cmsranking.Entity import InvalidData
import cmsranking.Contest as Contest
import cmsranking.Task as Task
import cmsranking.Team as Team
import cmsranking.User as User
import cmsranking.Submission as Submission
import cmsranking.Subchange as Subchange
import cmsranking.Scoring as Scoring


logger = logging.getLogger(__name__)


class CustomUnauthorized(Unauthorized):
    def get_response(self, environ=None):
        response = Unauthorized.get_response(self, environ)
        # XXX With werkzeug-0.9 a full-featured Response object is
        # returned: there is no need for this.
        response = Response.force_type(response)
        response.www_authenticate.set_basic(config.realm_name)
        return response


class StoreHandler(object):
    def __init__(self, store):
        self.store = store

        self.router = Map(
            [Rule("/<key>", methods=["GET"], endpoint="get"),
             Rule("/", methods=["GET"], endpoint="get_list"),
             Rule("/<key>", methods=["PUT"], endpoint="put"),
             Rule("/", methods=["PUT"], endpoint="put_list"),
             Rule("/<key>", methods=["DELETE"], endpoint="delete"),
             Rule("/", methods=["DELETE"], endpoint="delete_list"),
             ], encoding_errors="strict")

    def __call__(self, environ, start_response):
        return self.wsgi_app(environ, start_response)

    @responder
    def wsgi_app(self, environ, start_response):
        route = self.router.bind_to_environ(environ)
        try:
            endpoint, args = route.match()
        except HTTPException as exc:
            return exc

        request = Request(environ)
        request.encoding_errors = "strict"

        response = Response()

        try:
            if endpoint == "get":
                self.get(request, response, args["key"])
            elif endpoint == "get_list":
                self.get_list(request, response)
            elif endpoint == "put":
                self.put(request, response, args["key"])
            elif endpoint == "put_list":
                self.put_list(request, response)
            elif endpoint == "delete":
                self.delete(request, response, args["key"])
            elif endpoint == "delete_list":
                self.delete_list(request, response)
            else:
                raise RuntimeError()
        except HTTPException as exc:
            return exc

        return response

    def authorized(self, request):
        return request.authorization is not None and \
            request.authorization.type == "basic" and \
            request.authorization.username == config.username and \
            request.authorization.password == config.password

    def get(self, request, response, key):
        # Limit charset of keys.
        if re.match("^[A-Za-z0-9_]+$", key) is None:
            return NotFound()
        if key not in self.store:
            raise NotFound()
        if request.accept_mimetypes.quality("application/json") <= 0:
            raise NotAcceptable()

        response.status_code = 200
        response.headers[b'Timestamp'] = b"%0.6f" % time.time()
        response.mimetype = "application/json"
        response.data = json.dumps(self.store.retrieve(key))

    def get_list(self, request, response):
        if request.accept_mimetypes.quality("application/json") <= 0:
            raise NotAcceptable()

        response.status_code = 200
        response.headers[b'Timestamp'] = b"%0.6f" % time.time()
        response.mimetype = "application/json"
        response.data = json.dumps(self.store.retrieve_list())

    def put(self, request, response, key):
        # Limit charset of keys.
        if re.match("^[A-Za-z0-9_]+$", key) is None:
            return Forbidden()
        if not self.authorized(request):
            logger.warning("Unauthorized request.",
                           extra={'location': request.url,
                                  'details': repr(request.authorization)})
            raise CustomUnauthorized()
        if request.mimetype != "application/json":
            logger.warning("Unsupported MIME type.",
                           extra={'location': request.url,
                                  'details': request.mimetype})
            raise UnsupportedMediaType()

        try:
            data = json.load(request.stream)
        except (TypeError, ValueError):
            logger.warning("Wrong JSON.",
                           extra={'location': request.url})
            raise BadRequest()

        try:
            if key not in self.store:
                self.store.create(key, data)
            else:
                self.store.update(key, data)
        except InvalidData:
            logger.warning("Invalid data.", exc_info=True,
                           extra={'location': request.url,
                                  'details': pprint.pformat(data)})
            raise BadRequest()

        response.status_code = 204

    def put_list(self, request, response):
        if not self.authorized(request):
            logger.info("Unauthorized request.",
                        extra={'location': request.url,
                               'details': repr(request.authorization)})
            raise CustomUnauthorized()
        if request.mimetype != "application/json":
            logger.warning("Unsupported MIME type.",
                           extra={'location': request.url,
                                  'details': request.mimetype})
            raise UnsupportedMediaType()

        try:
            data = json.load(request.stream)
        except (TypeError, ValueError):
            logger.warning("Wrong JSON.",
                           extra={'location': request.url})
            raise BadRequest()

        try:
            self.store.merge_list(data)
        except InvalidData:
            logger.warning("Invalid data.", exc_info=True,
                           extra={'location': request.url,
                                  'details': pprint.pformat(data)})
            raise BadRequest()

        response.status_code = 204

    def delete(self, request, response, key):
        # Limit charset of keys.
        if re.match("^[A-Za-z0-9_]+$", key) is None:
            return NotFound()
        if key not in self.store:
            raise NotFound()
        if not self.authorized(request):
            logger.info("Unauthorized request.",
                        extra={'location': request.url,
                               'details': repr(request.authorization)})
            raise CustomUnauthorized()

        self.store.delete(key)

        response.status_code = 204

    def delete_list(self, request, response):
        if not self.authorized(request):
            logger.info("Unauthorized request.",
                        extra={'location': request.url,
                               'details': repr(request.authorization)})
            raise CustomUnauthorized()

        self.store.delete_list()

        response.status_code = 204


class DataWatcher(EventSource):
    """Receive the messages from the entities store and redirect them."""
    def __init__(self):
        self._CACHE_SIZE = config.buffer_size
        EventSource.__init__(self)

        Contest.store.add_create_callback(
            functools.partial(self.callback, "contest", "create"))
        Contest.store.add_update_callback(
            functools.partial(self.callback, "contest", "update"))
        Contest.store.add_delete_callback(
            functools.partial(self.callback, "contest", "delete"))

        Task.store.add_create_callback(
            functools.partial(self.callback, "task", "create"))
        Task.store.add_update_callback(
            functools.partial(self.callback, "task", "update"))
        Task.store.add_delete_callback(
            functools.partial(self.callback, "task", "delete"))

        Team.store.add_create_callback(
            functools.partial(self.callback, "team", "create"))
        Team.store.add_update_callback(
            functools.partial(self.callback, "team", "update"))
        Team.store.add_delete_callback(
            functools.partial(self.callback, "team", "delete"))

        User.store.add_create_callback(
            functools.partial(self.callback, "user", "create"))
        User.store.add_update_callback(
            functools.partial(self.callback, "user", "update"))
        User.store.add_delete_callback(
            functools.partial(self.callback, "user", "delete"))

        Scoring.store.add_score_callback(self.score_callback)

    def callback(self, entity, event, key, *args):
        self.send(entity, "%s %s" % (event, key))

    def score_callback(self, user, task, score):
        # FIXME Use score_precision.
        self.send("score", "%s %s %0.2f" % (user, task, score))


def SubListHandler(request, response, user_id):
    if request.accept_mimetypes.quality("application/json") <= 0:
        raise NotAcceptable()

    result = list()
    for task_id in Task.store._store.iterkeys():
        result.extend(Scoring.store.get_submissions(user_id, task_id).values())
    result.sort(key=lambda x: (x.task, x.time))
    result = list(a.__dict__ for a in result)

    response.status_code = 200
    response.mimetype = "application/json"
    response.data = json.dumps(result)


def HistoryHandler(request, response):
    if request.accept_mimetypes.quality("application/json") <= 0:
        raise NotAcceptable()

    result = list(Scoring.store.get_global_history())

    response.status_code = 200
    response.mimetype = "application/json"
    response.data = json.dumps(result)


def ScoreHandler(request, response):
    if request.accept_mimetypes.quality("application/json") <= 0:
        raise NotAcceptable()

    result = dict()
    for u_id, tasks in Scoring.store._scores.iteritems():
        for t_id, score in tasks.iteritems():
            if score.get_score() > 0.0:
                result.setdefault(u_id, dict())[t_id] = score.get_score()

    response.status_code = 200
    response.headers[b'Timestamp'] = b"%0.6f" % time.time()
    response.mimetype = "application/json"
    response.data = json.dumps(result)


class ImageHandler(object):
    EXT_TO_MIME = {
        'png': 'image/png',
        'jpg': 'image/jpeg',
        'gif': 'image/gif',
        'bmp': 'image/bmp'
    }

    MIME_TO_EXT = dict((v, k) for k, v in EXT_TO_MIME.iteritems())

    def __init__(self, location, fallback):
        self.location = location
        self.fallback = fallback

        self.router = Map(
            [Rule("/<name>", methods=["GET"], endpoint="get"),
             ], encoding_errors="strict")

    def __call__(self, environ, start_response):
        return self.wsgi_app(environ, start_response)

    @responder
    def wsgi_app(self, environ, start_response):
        route = self.router.bind_to_environ(environ)
        try:
            endpoint, args = route.match()
        except HTTPException as exc:
            return exc

        location = self.location % args

        request = Request(environ)
        request.encoding_errors = "strict"

        response = Response()

        available = list()
        for extension, mimetype in self.EXT_TO_MIME.iteritems():
            if os.path.isfile(location + '.' + extension):
                available.append(mimetype)
        mimetype = request.accept_mimetypes.best_match(available)
        if mimetype is not None:
            path = "%s.%s" % (location, self.MIME_TO_EXT[mimetype])
        else:
            path = self.fallback
            mimetype = 'image/png'  # FIXME Hardcoded type.

        response.status_code = 200
        response.mimetype = mimetype
        response.last_modified = \
            datetime.utcfromtimestamp(os.path.getmtime(path))\
                    .replace(microsecond=0)

        # TODO check for If-Modified-Since and If-None-Match

        response.response = wrap_file(environ, io.open(path, 'rb'))
        response.direct_passthrough = True

        return response


class RoutingHandler(object):
    def __init__(self, event_handler, logo_handler):
        self.router = Map(
            [Rule("/", methods=["GET"], endpoint="root"),
             Rule("/sublist/<user_id>", methods=["GET"], endpoint="sublist"),
             Rule("/history", methods=["GET"], endpoint="history"),
             Rule("/scores", methods=["GET"], endpoint="scores"),
             Rule("/events", methods=["GET"], endpoint="events"),
             Rule("/logo", methods=["GET"], endpoint="logo"),
             ], encoding_errors="strict")

        self.event_handler = event_handler
        self.logo_handler = logo_handler
        self.root_handler = redirect("Ranking.html")

    def __call__(self, environ, start_response):
        return self.wsgi_app(environ, start_response)

    def wsgi_app(self, environ, start_response):
        route = self.router.bind_to_environ(environ)
        try:
            endpoint, args = route.match()
        except HTTPException as exc:
            return exc(environ, start_response)

        if endpoint == "events":
            return self.event_handler(environ, start_response)
        elif endpoint == "logo":
            return self.logo_handler(environ, start_response)
        elif endpoint == "root":
            return self.root_handler(environ, start_response)
        else:
            request = Request(environ)
            request.encoding_errors = "strict"

            response = Response()

            if endpoint == "sublist":
                SubListHandler(request, response, args["user_id"])
            elif endpoint == "scores":
                ScoreHandler(request, response)
            elif endpoint == "history":
                HistoryHandler(request, response)

            return response(environ, start_response)


def main():
    """Entry point for RWS.

    return (bool): True if executed successfully.

    """
    parser = argparse.ArgumentParser(
        description="Ranking for CMS.")
    parser.add_argument("-d", "--drop",
                        help="drop the data already stored",
                        action="store_true")
    args = parser.parse_args()

    if args.drop:
        print("Are you sure you want to delete directory %s? [y/N]" %
              config.lib_dir, end='')
        ans = raw_input().lower()
        if ans in ['y', 'yes']:
            print("Removing directory %s." % config.lib_dir)
            shutil.rmtree(config.lib_dir)
        else:
            print("Not removing directory %s." % config.lib_dir)
        return False

    toplevel_handler = RoutingHandler(DataWatcher(), ImageHandler(
        os.path.join(config.lib_dir, '%(name)s'),
        os.path.join(config.web_dir, 'img', 'logo.png')))

    wsgi_app = SharedDataMiddleware(DispatcherMiddleware(
        toplevel_handler,
        {'/contests': StoreHandler(Contest.store),
         '/tasks': StoreHandler(Task.store),
         '/teams': StoreHandler(Team.store),
         '/users': StoreHandler(User.store),
         '/submissions': StoreHandler(Submission.store),
         '/subchanges': StoreHandler(Subchange.store),
         '/faces': ImageHandler(
             os.path.join(config.lib_dir, 'faces', '%(name)s'),
             os.path.join(config.web_dir, 'img', 'face.png')),
         '/flags': ImageHandler(
             os.path.join(config.lib_dir, 'flags', '%(name)s'),
             os.path.join(config.web_dir, 'img', 'flag.png')),
         }), {'/': config.web_dir})

    servers = list()
    if config.http_port is not None:
        http_server = WSGIServer(
            (config.bind_address, config.http_port), wsgi_app)
        servers.append(http_server)
    if config.https_port is not None:
        https_server = WSGIServer(
            (config.bind_address, config.https_port), wsgi_app,
            certfile=config.https_certfile, keyfile=config.https_keyfile)
        servers.append(https_server)

    try:
        gevent.joinall(list(gevent.spawn(s.serve_forever) for s in servers))
    except KeyboardInterrupt:
        pass
    finally:
        gevent.joinall(list(gevent.spawn(s.stop) for s in servers))
    return True

########NEW FILE########
__FILENAME__ = Scoring
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import heapq
import logging

from cmsranking.Submission import store as submission_store
from cmsranking.Subchange import store as subchange_store


logger = logging.getLogger(__name__)


class NumberSet(object):
    """A fast data structure on numbers.

    It supports:
    - inserting a value
    - removing a value
    - querying the maximum value

    It can hold the same value multiple times.

    This data structure could be implemented with a binary tree, but
    at the moment we're actually using a standard python list.

    """
    def __init__(self):
        self._impl = list()

    def insert(self, val):
        self._impl.append(val)

    def remove(self, val):
        self._impl.remove(val)

    def query(self):
        return max(self._impl + [0.0])

    def clear(self):
        del self._impl[:]


class Score(object):
    """The score of a user for a task.

    It computes the current score (and its history) for this
    user/task.  It gets notified in case a submission is created,
    updated and deleted.

    """
    # We assume that the submissions will all have different times,
    # since cms enforces a minimum delay between two submissions of
    # the same user for the same task.
    # On the other hand, different subchanges may have the same time
    # but cms assures that the order in which the subchanges have to
    # be processed is the ascending order of their keys (actually,
    # this is enforced only for suchanges with the same time).
    def __init__(self):
        # The submissions in their current status.
        self._submissions = dict()

        # The list of changes of the submissions.
        self._changes = list()

        # The set of the scores of the currently released submissions.
        self._released = NumberSet()

        # The last submitted submission (with at least one subchange).
        self._last = None

        # The history of score changes (the actual "output" of this
        # object).
        self._history = list()

    def append_change(self, change):
        # Remove from released submission (if needed), apply changes,
        # add back to released submissions (if needed) and check if
        # it's the last. Compute the new score and, if it changed,
        # append it to the history.
        s_id = change.submission
        if self._submissions[s_id].token:
            self._released.remove(self._submissions[s_id].score)
        if change.score is not None:
            self._submissions[s_id].score = change.score
        if change.token is not None:
            self._submissions[s_id].token = change.token
        if change.extra is not None:
            self._submissions[s_id].extra = change.extra
        if self._submissions[s_id].token:
            self._released.insert(self._submissions[s_id].score)
        if change.score is not None and \
                (self._last is None or
                 self._submissions[s_id].time > self._last.time):
            self._last = self._submissions[s_id]

        score = max(self._released.query(),
                    self._last.score if self._last is not None else 0.0)

        if score != self.get_score():
            self._history.append((change.time, score))

    def get_score(self):
        return self._history[-1][1] if len(self._history) > 0 else 0.0

    def reset_history(self):
        # Delete everything except the submissions and the subchanges.
        self._last = None
        self._released.clear()
        del self._history[:]

        # Reset the submissions at their default value.
        for sub in self._submissions.itervalues():
            sub.score = 0.0
            sub.token = False
            sub.extra = list()

        # Append each change, one at a time.
        for change in self._changes:
            self.append_change(change)

    def create_subchange(self, key, subchange):
        # Insert the subchange at the right position inside the
        # (sorted) list and call the appropriate method (append_change
        # or reset_history)
        if len(self._changes) == 0 or \
                subchange.time > self._changes[-1].time or \
                (subchange.time == self._changes[-1].time and
                 subchange.key > self._changes[-1].key):
            self._changes.append(subchange)
            self.append_change(subchange)
        else:
            for idx, val in enumerate(self._changes):
                if subchange.time < val.time or \
                   (subchange.time == val.time and subchange.key < val.key):
                    self._changes.insert(idx, subchange)
                    break
            self.reset_history()
            logger.info("Reset history for user '%s' and task '%s' after "
                        "creating subchange '%s' for submission '%s'" %
                        (self._submissions[subchange.submission].user,
                         self._submissions[subchange.submission].task,
                         key, subchange.submission))

    def update_subchange(self, key, subchange):
        # Update the subchange inside the (sorted) list and,
        # regardless of its position in that list, reset the history.
        for i in range(len(self._changes)):
            if self._changes[i].key == key:
                self._changes[i] = subchange
        self.reset_history()
        logger.info("Reset history for user '%s' and task '%s' after "
                    "creating subchange '%s' for submission '%s'" %
                    (self._submissions[subchange.submission].user,
                     self._submissions[subchange.submission].task,
                     key, subchange.submission))

    def delete_subchange(self, key):
        # Delete the subchange from the (sorted) list and reset the
        # history.
        self._changes = filter(lambda a: a.key != key, self._changes)
        self.reset_history()
        logger.info("Reset history after deleting subchange '%s'" % key)

    def create_submission(self, key, submission):
        # A new submission never triggers an update in the history,
        # since it doesn't have a score.
        submission.score = 0.0
        submission.token = False
        submission.extra = list()
        self._submissions[key] = submission

    def update_submission(self, key, submission):
        # An updated submission may cause an update in history because
        # it may change the "last" submission at some point in
        # history.
        self._submissions[key] = submission
        self.reset_history()

    def delete_submission(self, key):
        # A deleted submission shouldn't cause any history changes
        # (because its associated subchanges are deleted before it)
        # but we reset it just to be sure...
        if key in self._submissions:
            del self._submissions[key]
            # Delete all its subchanges.
            self._changes = filter(lambda a: a.submission != key,
                                   self._changes)
            self.reset_history()


class ScoringStore(object):
    """A manager for all instances of Scoring.

    It listens to the events of submission_store and subchange_store and
    redirects them to the corresponding Score (based on their user/task).
    When asked to provide a global history of score changes it takes the
    ones of each Score and combines them toghether (using a binary heap).

    """
    # We can do an important assumption here too: since the data has
    # to be consistent we are sure that if there's at least one
    # subchange there's also at least one submission (and if there's
    # no submission there's no subchange). We can also assume that
    # when a submission is deleted its subchanges have already been
    # deleted. So we are sure that we can delete the Score after we
    # delete the last submission, but we cannot after we delete the
    # last subchange.
    def __init__(self):
        submission_store.add_create_callback(self.create_submission)
        submission_store.add_update_callback(self.update_submission)
        submission_store.add_delete_callback(self.delete_submission)
        subchange_store.add_create_callback(self.create_subchange)
        subchange_store.add_update_callback(self.update_subchange)
        subchange_store.add_delete_callback(self.delete_subchange)

        self._scores = dict()

        self._callbacks = list()

        for key, value in submission_store._store.iteritems():
            self.create_submission(key, value)
        for key, value in sorted(subchange_store._store.iteritems()):
            self.create_subchange(key, value)

    def add_score_callback(self, callback):
        """Add a callback to be called when a score changes.

        Callbacks can be any kind of callable objects. They must
        accept three arguments: the user, the task and the new score.

        """
        self._callbacks.append(callback)

    def notify_callbacks(self, user, task, score):
        for call in self._callbacks:
            call(user, task, score)

    def create_submission(self, key, submission):
        if submission.user not in self._scores:
            self._scores[submission.user] = dict()
        if submission.task not in self._scores[submission.user]:
            self._scores[submission.user][submission.task] = Score()

        score_obj = self._scores[submission.user][submission.task]
        old_score = score_obj.get_score()
        score_obj.create_submission(key, submission)
        new_score = score_obj.get_score()
        if old_score != new_score:
            self.notify_callbacks(submission.user, submission.task, new_score)

    def update_submission(self, key, old_submission, submission):
        if old_submission.user != submission.user or \
                old_submission.task != submission.task:
            # TODO Delete all subchanges from the Score of the old
            # submission and create them on the new one.
            self.delete_submission(key, old_submission)
            self.create_submission(key, submission)
            return

        score_obj = self._scores[submission.user][submission.task]
        old_score = score_obj.get_score()
        score_obj.update_submission(key, submission)
        new_score = score_obj.get_score()
        if old_score != new_score:
            self.notify_callbacks(submission.user, submission.task, new_score)

    def delete_submission(self, key, submission):
        score_obj = self._scores[submission.user][submission.task]
        old_score = score_obj.get_score()
        score_obj.delete_submission(key)
        new_score = score_obj.get_score()
        if old_score != new_score:
            self.notify_callbacks(submission.user, submission.task, new_score)

        if len(self._scores[submission.user][submission.task]
               ._submissions) == 0:
            del self._scores[submission.user][submission.task]
        if len(self._scores[submission.user]) == 0:
            del self._scores[submission.user]

    def create_subchange(self, key, subchange):
        submission = submission_store._store[subchange.submission]
        score_obj = self._scores[submission.user][submission.task]
        old_score = score_obj.get_score()
        score_obj.create_subchange(key, subchange)
        new_score = score_obj.get_score()
        if old_score != new_score:
            self.notify_callbacks(submission.user, submission.task, new_score)

    def update_subchange(self, key, old_subchange, subchange):
        if old_subchange.submission != subchange.submission:
            self.delete_subchange(key, old_subchange)
            self.create_subchange(key, subchange)
            return

        submission = submission_store._store[subchange.submission]
        score_obj = self._scores[submission.user][submission.task]
        old_score = score_obj.get_score()
        score_obj.update_subchange(key, subchange)
        new_score = score_obj.get_score()
        if old_score != new_score:
            self.notify_callbacks(submission.user, submission.task, new_score)

    def delete_subchange(self, key, subchange):
        if subchange.submission not in submission_store:
            # Submission has just been deleted. We cannot retrieve the
            # user and the task, so we cannot clean up the Score obj.
            # But the delete_submission callback will do it for us!
            return
        submission = submission_store._store[subchange.submission]
        score_obj = self._scores[submission.user][submission.task]
        old_score = score_obj.get_score()
        score_obj.delete_subchange(key)
        new_score = score_obj.get_score()
        if old_score != new_score:
            self.notify_callbacks(submission.user, submission.task, new_score)

    def get_score(self, user, task):
        if user not in self._scores or task not in self._scores[user]:
            # We may want to raise an exception to distinguish between
            # "no submissions" and "submission with 0 points"
            return 0
        return self._scores[user][task].get_score()

    def get_submissions(self, user, task):
        if user not in self._scores or task not in self._scores[user]:
            return dict()
        return self._scores[user][task]._submissions

    def get_global_history(self):
        """Merge all individual histories into a global one.

        Take all per-user/per-task histories and merge them, providing
        a global history of all schore changes and return it using a
        generator.  Returned data is in the form (user_id, task_id,
        time, score).

        """
        # Use a priority queue, containing only one entry
        # per-user/per-task.
        queue = list()
        for user, dic in self._scores.iteritems():
            for task, scoring in dic.iteritems():
                if scoring._history:
                    heapq.heappush(queue, (scoring._history[0],
                                           user, task, scoring, 0))

        # When an entry is popped, push the next entry for that
        # user/task (if any).
        while len(queue) != 0:
            (time, score), user, task, scoring, index = heapq.heappop(queue)
            yield (user, task, time, score)
            if len(scoring._history) > index + 1:
                heapq.heappush(queue, (scoring._history[index + 1],
                                       user, task, scoring, index + 1))


store = ScoringStore()

########NEW FILE########
__FILENAME__ = Store
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import io
import json
import logging
import os
import re

from gevent.lock import RLock

from cmsranking.Config import config
from cmsranking.Entity import Entity, InvalidKey, InvalidData


logger = logging.getLogger(__name__)


# Global shared lock for all Store instances.
LOCK = RLock()


class Store(object):
    """A store for entities.

    Provide methods to perform the CRUD operations (create, retrieve,
    update, delete) on a set of entities, accessed by their unique key.
    It's very similar to a dict, except that keys are strings, values
    are of a single type (defined at init-time) and it's possible to
    get notified when something changes by providing appropriate
    callbacks.

    """
    def __init__(self, entity, dir_name, depends=None):
        """Initialize an empty EntityStore.

        The entity definition given as argument will define what kind
        of entities will be stored. It cannot be changed.

        entity (type): the class definition of the entities that will
            be stored

        """
        if not issubclass(entity, Entity):
            raise ValueError("The 'entity' parameter "
                             "isn't a subclass of Entity")
        self._entity = entity
        self._path = os.path.join(config.lib_dir, dir_name)
        self._depends = depends if depends is not None else []
        self._store = dict()
        self._create_callbacks = list()
        self._update_callbacks = list()
        self._delete_callbacks = list()

        try:
            os.mkdir(self._path)
        except OSError:
            # it's ok: it means the directory already exists
            pass

        try:
            for name in os.listdir(self._path):
                # TODO check that the key is '[A-Za-z0-9_]+'
                if name[-5:] == '.json' and name[:-5] != '':
                    with io.open(os.path.join(self._path, name), 'rb') as rec:
                        item = self._entity()
                        item.set(json.load(rec, encoding='utf-8'))
                        item.key = name[:-5]
                        self._store[name[:-5]] = item
        except OSError:
            # the path isn't a directory or is inaccessible
            logger.error("Path is not a directory or is not accessible",
                         exc_info=True)
        except IOError:
            logger.error("I/O error occured", exc_info=True)
        except ValueError:
            logger.error("Invalid JSON", exc_info=False,
                         extra={'location': os.path.join(self._path, name)})
        except InvalidData as exc:
            logger.error(exc.message, exc_info=False,
                         extra={'location': os.path.join(self._path, name)})

    def add_create_callback(self, callback):
        """Add a callback to be called when entities are created.

        Callbacks can be any kind of callable objects. They must accept
        a single argument: the key of the entity.

        """
        self._create_callbacks.append(callback)

    def add_update_callback(self, callback):
        """Add a callback to be called when entities are updated.

        Callbacks can be any kind of callable objects. They must accept
        a single argument: the key of the entity.

        """
        self._update_callbacks.append(callback)

    def add_delete_callback(self, callback):
        """Add a callback to be called when entities are deleted.

        Callbacks can be any kind of callable objects. They must accept
        a single argument: the key of the entity.

        """
        self._delete_callbacks.append(callback)

    def create(self, key, data):
        """Create a new entity.

        Create a new entity with the given key and the given data.

        key (unicode): the key with which the entity will be later
            accessed
        data (dict): the properties of the entity

        raise (InvalidKey): if key isn't a unicode or if an entity
            with the same key is already present in the store.
        raise (InvalidData): if data cannot be parsed, if it's missing
            some properties or if properties are of the wrong type.

        """
        if not isinstance(key, unicode) or key in self._store:
            raise InvalidKey("Key already in store.")

        # create entity
        with LOCK:
            item = self._entity()
            item.set(data)
            if not item.consistent():
                raise InvalidData("Inconsistent data")
            item.key = key
            self._store[key] = item
            # notify callbacks
            for callback in self._create_callbacks:
                callback(key, item)
            # reflect changes on the persistent storage
            try:
                path = os.path.join(self._path, key + '.json')
                with io.open(path, 'wb') as rec:
                    json.dump(self._store[key].get(), rec, encoding='utf-8')
            except IOError:
                logger.error("I/O error occured while creating entity",
                             exc_info=True)

    def update(self, key, data):
        """Update an entity.

        Update an existing entity with the given key and the given
        data.

        key (unicode): the key of the entity that has to be updated
        data (dict): the new properties of the entity

        raise (InvalidKey): if key isn't a unicode or if no entity
            with that key is present in the store.
        raise (InvalidData): if data cannot be parsed, if it's missing
            some properties or if properties are of the wrong type.

        """
        if not isinstance(key, unicode) or key not in self._store:
            raise InvalidKey("Key not in store.")

        # update entity
        with LOCK:
            item = self._entity()
            item.set(data)
            if not item.consistent():
                raise InvalidData("Inconsistent data")
            item.key = key
            old_item = self._store[key]
            self._store[key] = item
            # notify callbacks
            for callback in self._update_callbacks:
                callback(key, old_item, item)
            # reflect changes on the persistent storage
            try:
                path = os.path.join(self._path, key + '.json')
                with io.open(path, 'wb') as rec:
                    json.dump(self._store[key].get(), rec, encoding='utf-8')
            except IOError:
                logger.error("I/O error occured while updating entity",
                             exc_info=True)

    def merge_list(self, data_dict):
        """Merge a list of entities.

        Take a dictionary of entites and, for each of them:
         - if it's not present in the store, create it
         - if it's present, update it

        data_dict (dict): the dictionary of entities

        raise (InvalidData) if data cannot be parsed, if an entity is
            missing some properties or if properties are of the wrong
            type.

        """
        with LOCK:
            if not isinstance(data_dict, dict):
                raise InvalidData("Not a dictionary")
            item_dict = dict()
            for key, value in data_dict.iteritems():
                try:
                    # FIXME We should allow keys to be arbitrary unicode
                    # strings, so this just needs to be a non-empty check.
                    if not re.match('[A-Za-z0-9_]+', key):
                        raise InvalidData("Invalid key")
                    item = self._entity()
                    item.set(value)
                    if not item.consistent():
                        raise InvalidData("Inconsistent data")
                    item.key = key
                    item_dict[key] = item
                except InvalidData as exc:
                    exc.message = "[entity %s] %s" % (key, exc)
                    exc.args = exc.message,
                    raise exc

            for key, value in item_dict.iteritems():
                is_new = key not in self._store
                old_value = self._store.get(key)
                # insert entity
                self._store[key] = value
                # notify callbacks
                if is_new:
                    for callback in self._create_callbacks:
                        callback(key, value)
                else:
                    for callback in self._update_callbacks:
                        callback(key, old_value, value)
                # reflect changes on the persistent storage
                try:
                    path = os.path.join(self._path, key + '.json')
                    with io.open(path, 'wb') as rec:
                        json.dump(value.get(), rec, encoding='utf-8')
                except IOError:
                    logger.error(
                        "I/O error occured while merging entity lists",
                        exc_info=True)

    def delete(self, key):
        """Delete an entity.

        Delete an existing entity from the store.

        key (unicode): the key of the entity that has to be deleted

        raise (InvalidKey): if key isn't a unicode or if no entity
            with that key is present in the store.

        """
        if not isinstance(key, unicode) or key not in self._store:
            raise InvalidKey("Key not in store.")

        with LOCK:
            # delete entity
            old_value = self._store[key]
            del self._store[key]
            # enforce consistency
            for depend in self._depends:
                for o_key, o_value in list(depend._store.iteritems()):
                    if not o_value.consistent():
                        depend.delete(o_key)
            # notify callbacks
            for callback in self._delete_callbacks:
                callback(key, old_value)
            # reflect changes on the persistent storage
            try:
                os.remove(os.path.join(self._path, key + '.json'))
            except OSError:
                logger.error("Unable to delete entity", exc_info=True)

    def delete_list(self):
        """Delete all entities.

        Delete all existing entities from the store.

        """
        with LOCK:
            # delete all entities
            for key in list(self._store.iterkeys()):
                self.delete(key)

    def retrieve(self, key):
        """Retrieve an entity.

        Retrieve an existing entity from the store.

        key (unicode): the key of the entity that has to be retrieved

        raise (InvalidKey): if key isn't a unicode or if no entity
            with that key is present in the store.

        """
        if not isinstance(key, unicode) or key not in self._store:
            raise InvalidKey("Key not in store.")

        # retrieve entity
        return self._store[key].get()

    def retrieve_list(self):
        """Retrieve a list of all entities."""
        result = dict()
        for key, value in self._store.iteritems():
            result[key] = value.get()
        return result

    def __contains__(self, key):
        return key in self._store

########NEW FILE########
__FILENAME__ = Subchange
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import six

from cmsranking.Entity import Entity, InvalidData
from cmsranking.Store import Store


class Subchange(Entity):
    """The entity representing a change in the status of a submission.

    It consists of the following properties:
    - submission (unicode): the key of the affected submission
    - time (int): the time the change takes effect
    - score (float): optional, the new score
    - token (bool): optional, the new token status
    - extra ([unicode]): optional, the new details

    """
    def __init__(self):
        """Set the properties to some default values.

        """
        Entity.__init__(self)
        self.submission = None
        self.time = None
        self.score = None
        self.token = None
        self.extra = None

    @staticmethod
    def validate(data):
        """Validate the given dictionary.

        See if it contains a valid representation of this entity.

        """
        try:
            assert isinstance(data, dict), \
                "Not a dictionary"
            assert isinstance(data['submission'], six.text_type), \
                "Field 'submission' isn't a string"
            assert isinstance(data['time'], six.integer_types), \
                "Field 'time' isn't an integer (unix timestamp)"
            if 'score' in data:
                assert isinstance(data['score'], float), \
                    "Field 'score' isn't a float"
            if 'token' in data:
                assert isinstance(data['token'], bool), \
                    "Field 'token' isn't a boolean"
            if 'extra' in data:
                assert isinstance(data['extra'], list), \
                    "Field 'extra' isn't a list of strings"
                for i in data['extra']:
                    assert isinstance(i, six.text_type), \
                        "Field 'extra' isn't a list of strings"
        except KeyError as exc:
            raise InvalidData("Field %s is missing" % exc.message)
        except AssertionError as exc:
            raise InvalidData(exc.message)

    def set(self, data):
        self.validate(data)
        self.submission = data['submission']
        self.time = data['time']
        self.score = (data['score'] if 'score' in data else None)
        self.token = (data['token'] if 'token' in data else None)
        self.extra = (data['extra'] if 'extra' in data else None)

    def get(self):
        result = self.__dict__.copy()
        del result['key']
        for field in ['score', 'token', 'extra']:
            if result[field] is None:
                del result[field]
        return result

    def consistent(self):
        from cmsranking.Submission import store as submission_store
        return self.submission in submission_store


store = Store(Subchange, 'subchanges')

########NEW FILE########
__FILENAME__ = Submission
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import six

from cmsranking.Entity import Entity, InvalidData
from cmsranking.Store import Store
from cmsranking.Subchange import store as subchange_store


class Submission(Entity):
    """The entity representing a submission.

    It consists of the following properties:
    - user (unicode): the key of the user who submitted
    - task (unicode): the key of the task of the submission
    - time (int): the time the submission has been submitted

    """
    def __init__(self):
        """Set the properties to some default values.

        """
        Entity.__init__(self)
        self.user = None
        self.task = None
        self.time = None

    @staticmethod
    def validate(data):
        """Validate the given dictionary.

        See if it contains a valid representation of this entity.

        """
        try:
            assert isinstance(data, dict), \
                "Not a dictionary"
            assert isinstance(data['user'], six.text_type), \
                "Field 'user' isn't a string"
            assert isinstance(data['task'], six.text_type), \
                "Field 'task' isn't a string"
            assert isinstance(data['time'], six.integer_types), \
                "Field 'time' isn't an integer (unix timestamp)"
        except KeyError as exc:
            raise InvalidData("Field %s is missing" % exc.message)
        except AssertionError as exc:
            raise InvalidData(exc.message)

    def set(self, data):
        self.validate(data)
        self.user = data['user']
        self.task = data['task']
        self.time = data['time']

    def get(self):
        result = self.__dict__.copy()
        del result['key']
        del result['score']
        del result['token']
        del result['extra']
        return result

    def consistent(self):
        from cmsranking.Task import store as task_store
        from cmsranking.User import store as user_store
        return self.task in task_store and self.user in user_store


store = Store(Submission, 'submissions', [subchange_store])

########NEW FILE########
__FILENAME__ = Task
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import six

from cmsranking.Entity import Entity, InvalidData
from cmsranking.Store import Store
from cmsranking.Submission import store as submission_store


class Task(Entity):
    """The entity representing a task.

    It consists of the following properties:
    - name (unicode): the human-readable name of the task
    - short_name (unicode): a shorter name for the task, usually a
        code-name
    - contest (unicode): the id of the contest the task belongs to
    - max_score (float): the maximum achievable score for the task
    - score_precision (int): how many decimal places to show in scores
    - data_headers ([unicode]): a list with the descriptions of the
        extra fields that will be provided with each submission for the
        task
    - order (int): the order of the tasks inside of the contest

    """
    def __init__(self):
        """Set the properties to some default values.

        """
        Entity.__init__(self)
        self.name = None
        self.short_name = None
        self.contest = None
        self.max_score = None
        self.extra_headers = None
        self.order = None

    @staticmethod
    def validate(data):
        """Validate the given dictionary.

        See if it contains a valid representation of this entity.

        """
        try:
            assert isinstance(data, dict), \
                "Not a dictionary"
            assert isinstance(data['name'], six.text_type), \
                "Field 'name' isn't a string"
            assert isinstance(data['short_name'], six.text_type), \
                "Field 'short_name' isn't a string"
            assert isinstance(data['contest'], six.text_type), \
                "Field 'contest' isn't a string"
            assert isinstance(data['max_score'], float), \
                "Field 'max_score' isn't a float"
            assert isinstance(data['score_precision'], six.integer_types), \
                "Field 'score_precision' isn't an integer"
            assert data['score_precision'] >= 0, \
                "Field 'score_precision' is negative"
            assert isinstance(data['extra_headers'], list), \
                "Field 'extra_headers' isn't a list of strings"
            for i in data['extra_headers']:
                assert isinstance(i, six.text_type), \
                    "Field 'extra_headers' isn't a list of strings"
            assert isinstance(data['order'], six.integer_types), \
                "Field 'order' isn't an integer"
        except KeyError as exc:
            raise InvalidData("Field %s is missing" % exc.message)
        except AssertionError as exc:
            raise InvalidData(exc.message)

    def set(self, data):
        self.validate(data)
        self.name = data['name']
        self.short_name = data['short_name']
        self.contest = data['contest']
        self.max_score = data['max_score']
        self.score_precision = data['score_precision']
        self.extra_headers = data['extra_headers']
        self.order = data['order']

    def get(self):
        result = self.__dict__.copy()
        del result['key']
        return result

    def consistent(self):
        from cmsranking.Contest import store as contest_store
        return self.contest in contest_store


store = Store(Task, 'tasks', [submission_store])

########NEW FILE########
__FILENAME__ = Team
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import six

from cmsranking.Entity import Entity, InvalidData
from cmsranking.Store import Store
from cmsranking.User import store as user_store


class Team(Entity):
    """The entity representing a team.

    It consists of the following properties:
    - name (unicode): the human-readable name of the team

    """
    def __init__(self):
        """Set the properties to some default values.

        """
        Entity.__init__(self)
        self.name = None

    @staticmethod
    def validate(data):
        """Validate the given dictionary.

        See if it contains a valid representation of this entity.

        """
        try:
            assert isinstance(data, dict), \
                "Not a dictionary"
            assert isinstance(data['name'], six.text_type), \
                "Field 'name' isn't a string"
        except KeyError as exc:
            raise InvalidData("Field %s is missing" % exc.message)
        except AssertionError as exc:
            raise InvalidData(exc.message)

    def set(self, data):
        self.validate(data)
        self.name = data['name']

    def get(self):
        result = self.__dict__.copy()
        del result['key']
        return result


store = Store(Team, 'teams', [user_store])

########NEW FILE########
__FILENAME__ = User
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2011-2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function
from __future__ import unicode_literals

import six

from cmsranking.Entity import Entity, InvalidData
from cmsranking.Store import Store
from cmsranking.Submission import store as submission_store


class User(Entity):
    """The entity representing a user.

    It consists of the following properties:
    - f_name (unicode): the first name of the user
    - l_name (unicode): the last name of the user
    - team (unicode): the id of the team the user belongs to

    """
    def __init__(self):
        """Set the properties to some default values.

        """
        Entity.__init__(self)
        self.f_name = None
        self.l_name = None
        self.team = None

    @staticmethod
    def validate(data):
        """Validate the given dictionary.

        See if it contains a valid representation of this entity.

        """
        try:
            assert isinstance(data, dict), \
                "Not a dictionary"
            assert isinstance(data['f_name'], six.text_type), \
                "Field 'f_name' isn't a string"
            assert isinstance(data['l_name'], six.text_type), \
                "Field 'l_name' isn't a string"
            assert data['team'] is None or \
                isinstance(data['team'], six.text_type), \
                "Field 'team' isn't a string (or null)"
        except KeyError as exc:
            raise InvalidData("Field %s is missing" % exc.message)
        except AssertionError as exc:
            raise InvalidData(exc.message)

    def set(self, data):
        self.validate(data)
        self.f_name = data['f_name']
        self.l_name = data['l_name']
        self.team = data['team']

    def get(self):
        result = self.__dict__.copy()
        del result['key']
        return result

    def consistent(self):
        from cmsranking.Team import store as team_store
        return self.team is None or self.team in team_store


store = Store(User, 'users', [submission_store])

########NEW FILE########
__FILENAME__ = cmsMake
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
# Copyright © 2014 Luca Versari <veluca93@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import argparse
import os
import sys
import subprocess
import copy
import functools
import shutil
import tempfile
import yaml

from cms.grading import get_compilation_commands
from cmstaskenv.Test import test_testcases, clean_test_env


SOL_DIRNAME = 'sol'
SOL_FILENAME = 'soluzione'
SOL_EXTS = ['.cpp', '.c', '.pas']
CHECK_DIRNAME = 'cor'
CHECK_EXTS = SOL_EXTS
TEXT_DIRNAME = 'testo'
TEXT_TEX = 'testo.tex'
TEXT_PDF = 'testo.pdf'
TEXT_AUX = 'testo.aux'
TEXT_LOG = 'testo.log'
INPUT0_TXT = 'input0.txt'
OUTPUT0_TXT = 'output0.txt'
GEN_DIRNAME = 'gen'
GEN_GEN = 'GEN'
GEN_BASENAME = 'generatore'
GEN_EXTS = ['.py', '.sh', '.cpp', '.c', '.pas']
VALIDATOR_BASENAME = 'valida'
GRAD_BASENAME = 'grader'
INPUT_DIRNAME = 'input'
OUTPUT_DIRNAME = 'output'
RESULT_DIRNAME = 'result'

DATA_DIRS = [os.path.join('.', 'cmstaskenv', 'data'),
             os.path.join('/', 'usr', 'local', 'share', 'cms', 'cmsMake')]


def detect_data_dir():
    for _dir in DATA_DIRS:
        if os.path.exists(_dir):
            return os.path.abspath(_dir)


DATA_DIR = detect_data_dir()


def endswith2(string, suffixes):
    """True if string ends with one of the given suffixes.

    """
    return any(filter(lambda x: string.endswith(x), suffixes))


def basename2(string, suffixes):
    """If string ends with one of the specified suffixes, returns its
    basename (i.e., itself after removing the suffix) and the suffix
    packed in a tuple. Otherwise returns None.

    """
    try:
        idx = map(lambda x: string.endswith(x), suffixes).index(True)
    except ValueError:
        return None
    return (string[:-len(suffixes[idx])], string[-len(suffixes[idx]):])


def call(base_dir, args, stdin=None, stdout=None, stderr=None, env=None):
    print("> Executing command %s in dir %s" %
          (" ".join(args), base_dir), file=sys.stderr)
    if env is None:
        env = {}
    env2 = copy.copy(os.environ)
    env2.update(env)
    res = subprocess.call(args, stdin=stdin, stdout=stdout, stderr=stderr,
                          cwd=base_dir, env=env2)
    if res != 0:
        print("Subprocess returned with error", file=sys.stderr)
        sys.exit(1)


def detect_task_name(base_dir):
    return os.path.split(os.path.realpath(base_dir))[1]


def parse_task_yaml(base_dir):
    parent_dir = os.path.split(os.path.realpath(base_dir))[0]

    # We first look for the yaml file inside the task folder,
    # and eventually fallback to a yaml file in its parent folder.
    yaml_path = os.path.join(base_dir, "task.yaml")

    try:
        with open(yaml_path) as yaml_file:
            conf = yaml.load(yaml_file)
    except IOError:
        yaml_path = os.path.join(parent_dir, "%s.yaml" %
                                 (detect_task_name(base_dir)))

        with open(yaml_path) as yaml_file:
            conf = yaml.load(yaml_file)
    return conf


def detect_task_type(base_dir):
    sol_dir = os.path.join(base_dir, SOL_DIRNAME)
    check_dir = os.path.join(base_dir, CHECK_DIRNAME)
    grad_present = os.path.exists(sol_dir) and \
        any(filter(lambda x: x.startswith(GRAD_BASENAME + '.'),
                   os.listdir(sol_dir)))
    stub_present = os.path.exists(sol_dir) and \
        any(filter(lambda x: x.startswith('stub.'),
                   os.listdir(sol_dir)))
    cor_present = os.path.exists(check_dir) and \
        any(filter(lambda x: x.startswith('correttore.'),
                   os.listdir(check_dir)))
    man_present = os.path.exists(check_dir) and \
        any(filter(lambda x: x.startswith('manager.'),
                   os.listdir(check_dir)))

    if not (cor_present or man_present or stub_present or grad_present):
        return ["Batch", "Diff"]  # TODO Could also be an OutputOnly
    elif not (man_present or stub_present or grad_present) and cor_present:
        return ["Batch", "Comp"]  # TODO Could also be an OutputOnly
    elif not (cor_present or man_present or stub_present) and grad_present:
        return ["Batch", "Grad"]
    elif not (man_present or stub_present) and cor_present and grad_present:
        return ["Batch", "GradComp"]
    elif not (cor_present or grad_present) and man_present and stub_present:
        return ["Communication", ""]
    else:
        return ["Invalid", ""]


def noop():
    pass


def build_sols_list(base_dir, task_type, in_out_files, yaml_conf):
    if yaml_conf.get('only_gen', False):
        return []

    sol_dir = os.path.join(base_dir, SOL_DIRNAME)
    entries = map(lambda x: os.path.join(SOL_DIRNAME, x), os.listdir(sol_dir))
    sources = filter(lambda x: endswith2(x, SOL_EXTS), entries)

    actions = []
    test_actions = []
    for src in sources:
        exe, lang = basename2(src, SOL_EXTS)
        # Delete the dot
        lang = lang[1:]
        exe_EVAL = "%s_EVAL" % (exe)

        # Ignore things known to be auxiliary files
        if exe == os.path.join(SOL_DIRNAME, GRAD_BASENAME):
            continue
        if lang == 'pas' and exe.endswith('lib'):
            continue

        srcs = []
        # The grader, when present, must be in the first position of
        # srcs; see docstring of get_compilation_commands().
        if task_type == ['Batch', 'Grad'] or \
                task_type == ['Batch', 'GradComp']:
            srcs.append(os.path.join(SOL_DIRNAME,
                                     GRAD_BASENAME + '.%s' % (lang)))
        srcs.append(src)

        test_deps = \
            [exe_EVAL, os.path.join(TEXT_DIRNAME, TEXT_PDF)] + in_out_files
        if task_type == ['Batch', 'Comp'] or \
                task_type == ['Batch', 'GradComp']:
            test_deps.append('cor/correttore')

        def compile_src(srcs, exe, for_evaluation, lang, assume=None):
            if lang != 'pas' or len(srcs) == 1:
                compilation_commands = get_compilation_commands(
                    lang,
                    srcs,
                    exe,
                    for_evaluation=for_evaluation)
                for command in compilation_commands:
                    call(base_dir, command)

            # When using Pascal with graders, file naming conventions
            # require us to do a bit of trickery, i.e., performing the
            # compilation in a separate temporary directory
            else:
                tempdir = tempfile.mkdtemp()
                task_name = detect_task_name(base_dir)
                new_srcs = [os.path.split(srcs[0])[1],
                            '%s.pas' % (task_name)]
                new_exe = os.path.split(srcs[1])[1][:-4]
                shutil.copyfile(os.path.join(base_dir, srcs[0]),
                                os.path.join(tempdir, new_srcs[0]))
                shutil.copyfile(os.path.join(base_dir, srcs[1]),
                                os.path.join(tempdir, new_srcs[1]))
                lib_filename = '%slib.pas' % (task_name)
                if os.path.exists(os.path.join(SOL_DIRNAME, lib_filename)):
                    shutil.copyfile(os.path.join(SOL_DIRNAME, lib_filename),
                                    os.path.join(tempdir, lib_filename))
                compilation_commands = get_compilation_commands(
                    lang,
                    new_srcs,
                    new_exe,
                    for_evaluation=for_evaluation)
                for command in compilation_commands:
                    call(tempdir, command)
                shutil.copyfile(os.path.join(tempdir, new_exe),
                                os.path.join(base_dir, exe))
                shutil.copymode(os.path.join(tempdir, new_exe),
                                os.path.join(base_dir, exe))
                shutil.rmtree(tempdir)

        def test_src(exe, lang, assume=None):
            print("Testing solution %s" % (exe))
            test_testcases(
                base_dir,
                exe,
                language=lang,
                assume=assume)

        actions.append(
            (srcs,
             [exe],
             functools.partial(compile_src, srcs, exe, False, lang),
             'compile solution'))
        actions.append(
            (srcs,
             [exe_EVAL],
             functools.partial(compile_src, srcs, exe_EVAL, True, lang),
             'compile solution with -DEVAL'))

        test_actions.append((test_deps,
                             ['test_%s' % (os.path.split(exe)[1])],
                             functools.partial(test_src, exe_EVAL, lang),
                             'test solution (compiled with -DEVAL)'))

    return actions + test_actions


def build_checker_list(base_dir, task_type):
    check_dir = os.path.join(base_dir, CHECK_DIRNAME)
    actions = []

    if os.path.exists(check_dir):
        entries = map(lambda x: os.path.join(CHECK_DIRNAME, x),
                      os.listdir(check_dir))
        sources = filter(lambda x: endswith2(x, SOL_EXTS), entries)
        for src in sources:
            exe, lang = basename2(src, CHECK_EXTS)
            # Delete the dot
            lang = lang[1:]

            def compile_check(src, exe, assume=None):
                commands = get_compilation_commands(lang, [src], exe)
                for command in commands:
                    call(base_dir, command)

            actions.append(([src], [exe],
                            functools.partial(compile_check, src, exe),
                            'compile checker'))

    return actions


def build_text_list(base_dir, task_type):
    text_tex = os.path.join(TEXT_DIRNAME, TEXT_TEX)
    text_pdf = os.path.join(TEXT_DIRNAME, TEXT_PDF)
    text_aux = os.path.join(TEXT_DIRNAME, TEXT_AUX)
    text_log = os.path.join(TEXT_DIRNAME, TEXT_LOG)

    def make_pdf(assume=None):
        call(base_dir,
             ['pdflatex', '-output-directory', TEXT_DIRNAME,
              '-interaction', 'batchmode', text_tex],
             env={'TEXINPUTS': '.:%s:%s/file:' % (TEXT_DIRNAME, TEXT_DIRNAME)})

    actions = []
    if os.path.exists(text_tex):
        actions.append(([text_tex], [text_pdf, text_aux, text_log],
                        make_pdf, 'compile to PDF'))
    return actions


def iter_GEN(name):
    st = 0
    for l in open(name, "r"):
        if l[:4] == "#ST:":
            st += 1
        l = (" " + l).split("#")[0][1:].strip("\n")
        if l != "":
            yield (l, st)


def build_gen_list(base_dir, task_type):
    input_dir = os.path.join(base_dir, INPUT_DIRNAME)
    output_dir = os.path.join(base_dir, OUTPUT_DIRNAME)
    gen_dir = os.path.join(base_dir, GEN_DIRNAME)
    entries = os.listdir(gen_dir)
    sources = filter(lambda x: endswith2(x, GEN_EXTS), entries)
    gen_exe = None
    validator_exe = None

    for src in sources:
        base, lang = basename2(src, GEN_EXTS)
        if base == GEN_BASENAME:
            gen_exe = os.path.join(GEN_DIRNAME, base)
            gen_src = os.path.join(GEN_DIRNAME, base + lang)
            gen_lang = lang[1:]
        elif base == VALIDATOR_BASENAME:
            validator_exe = os.path.join(GEN_DIRNAME, base)
            validator_src = os.path.join(GEN_DIRNAME, base + lang)
            validator_lang = lang[1:]
    if gen_exe is None:
        raise Exception("Couldn't find generator")
    if validator_exe is None:
        raise Exception("Couldn't find validator")
    gen_GEN = os.path.join(GEN_DIRNAME, GEN_GEN)

    sol_exe = os.path.join(SOL_DIRNAME, SOL_FILENAME)

    # Count non-trivial lines in GEN
    testcase_num = len(list(iter_GEN(os.path.join(base_dir, gen_GEN))))

    def compile_src(src, exe, lang, assume=None):
        if lang in ['cpp', 'c', 'pas']:
            commands = get_compilation_commands(lang, [src], exe,
                                                for_evaluation=False)
            for command in commands:
                call(base_dir, command)
        elif lang in ['py', 'sh']:
            os.symlink(os.path.basename(src), exe)
        else:
            raise Exception("Wrong generator/validator language!")

    # Question: why, differently from outputs, inputs have to be
    # created all together instead of selectively over those that have
    # been changed since last execution? This is a waste of time,
    # usually generating inputs is a pretty long thing. Answer:
    # because cmsMake architecture, which is based on file timestamps,
    # doesn't make us able to understand which lines of gen/GEN have
    # been changed. Douch! We'll have to thing better this thing for
    # the new format we're developing.
    def make_input(assume=None):
        n = 0
        try:
            os.makedirs(input_dir)
        except OSError:
            pass
        for (line, st) in iter_GEN(os.path.join(base_dir, gen_GEN)):
            print("Generating input # %d" % (n), file=sys.stderr)
            with open(os.path.join(input_dir,
                                   'input%d.txt' % (n)), 'w') as fout:
                call(base_dir,
                     [gen_exe] + line.split(),
                     stdout=fout)
            command = [validator_exe, os.path.join(input_dir,
                                                   'input%d.txt' % (n))]
            if st != 0:
                command.append(str(st))
            call(base_dir, command)
            n += 1

    def make_output(n, assume=None):
        try:
            os.makedirs(output_dir)
        except OSError:
            pass
        print("Generating output # %d" % (n), file=sys.stderr)
        with open(os.path.join(input_dir, 'input%d.txt' % (n))) as fin:
            with open(os.path.join(output_dir,
                                   'output%d.txt' % (n)), 'w') as fout:
                call(base_dir, [sol_exe], stdin=fin, stdout=fout)

    actions = []
    actions.append(([gen_src],
                    [gen_exe],
                    functools.partial(compile_src, gen_src, gen_exe, gen_lang),
                    "compile the generator"))
    actions.append(([validator_src],
                    [validator_exe],
                    functools.partial(compile_src, validator_src,
                                      validator_exe, validator_lang),
                    "compile the validator"))
    actions.append(([gen_GEN, gen_exe, validator_exe],
                    map(lambda x: os.path.join(INPUT_DIRNAME,
                                               'input%d.txt' % (x)),
                        range(0, testcase_num)),
                    make_input,
                    "input generation"))

    for n in xrange(testcase_num):
        actions.append(([os.path.join(INPUT_DIRNAME, 'input%d.txt' % (n)),
                         sol_exe],
                        [os.path.join(OUTPUT_DIRNAME, 'output%d.txt' % (n))],
                        functools.partial(make_output, n),
                        "output generation"))
    in_out_files = [os.path.join(INPUT_DIRNAME, 'input%d.txt' % (n))
                    for n in xrange(testcase_num)] + \
                   [os.path.join(OUTPUT_DIRNAME, 'output%d.txt' % (n))
                    for n in xrange(testcase_num)]
    return actions, in_out_files


def build_action_list(base_dir, task_type, yaml_conf):
    """Build a list of actions that cmsMake is able to do here. Each
    action is described by a tuple (infiles, outfiles, callable,
    description) where:

    1) infiles is a list of files this action depends on;

    2) outfiles is a list of files this action produces; it is
    intended that this action can be skipped if all the outfiles is
    newer than all the infiles; moreover, the outfiles get deleted
    when the action is cleaned;

    3) callable is a callable Python object that, when called,
    performs the action;

    4) description is a human-readable description of what this
    action does.

    """
    actions = []
    gen_actions, in_out_files = build_gen_list(base_dir, task_type)
    actions += gen_actions
    actions += build_sols_list(base_dir, task_type, in_out_files, yaml_conf)
    actions += build_checker_list(base_dir, task_type)
    actions += build_text_list(base_dir, task_type)
    return actions


def clean(base_dir, generated_list):
    # Delete all generated files
    for f in generated_list:
        try:
            os.remove(os.path.join(base_dir, f))
        except OSError:
            pass

    # Delete other things
    try:
        os.rmdir(os.path.join(base_dir, INPUT_DIRNAME))
    except OSError:
        pass
    try:
        os.rmdir(os.path.join(base_dir, OUTPUT_DIRNAME))
    except OSError:
        pass
    try:
        shutil.rmtree(os.path.join(base_dir, RESULT_DIRNAME))
    except OSError:
        pass

    # Delete backup files
    os.system("find %s -name '*.pyc' -delete" % (base_dir))
    os.system("find %s -name '*~' -delete" % (base_dir))


def build_execution_tree(actions):
    """Given a set of actions as described in the docstring of
    build_action_list(), builds an execution tree and the list of all
    the buildable files. The execution tree is a dictionary that maps
    each builable or source file to the tuple (infiles, callable),
    where infiles and callable are as in the docstring of
    build_action_list().

    """
    exec_tree = {}
    generated_list = []
    src_list = set()
    for action in actions:
        for exe in action[1]:
            if exe in exec_tree:
                raise Exception("Target %s not unique" % (exe))
            exec_tree[exe] = (action[0], action[2])
            generated_list.append(exe)
        for src in action[0]:
            src_list.add(src)
    for src in src_list:
        if src not in exec_tree:
            exec_tree[src] = ([], noop)
    return exec_tree, generated_list


def execute_target(base_dir, exec_tree, target,
                   already_executed=None, stack=None,
                   debug=False, assume=None):
    # Initialization
    if debug:
        print(">> Target %s is requested" % (target))
    if already_executed is None:
        already_executed = set()
    if stack is None:
        stack = set()

    # Get target information
    deps = exec_tree[target][0]
    action = exec_tree[target][1]

    # If this target is already in the stack, we have a circular
    # dependency
    if target in stack:
        raise Exception("Circular dependency detected")

    # If the target was already made in another subtree, we have
    # nothing to do
    if target in already_executed:
        if debug:
            print(">> Target %s has already been built, ignoring..." %
                  (target))
        return

    # Otherwise, do a step of the DFS to make dependencies
    if debug:
        print(">> Building dependencies for target %s" % (target))
    already_executed.add(target)
    stack.add(target)
    for dep in deps:
        execute_target(base_dir, exec_tree, dep,
                       already_executed, stack, assume=assume)
    stack.remove(target)
    if debug:
        print(">> Dependencies built for target %s" % (target))

    # Check if the action really needs to be done (i.e., there is one
    # dependency more recent than the generated file)
    dep_times = max([0] + map(lambda dep: os.stat(
        os.path.join(base_dir, dep)).st_mtime, deps))
    try:
        gen_time = os.stat(os.path.join(base_dir, target)).st_mtime
    except OSError:
        gen_time = 0
    if gen_time >= dep_times:
        if debug:
            print(">> Target %s is already new enough, not building" %
                  (target))
        return

    # At last: actually make the so long desired action :-)
    if debug:
        print(">> Acutally building target %s" % (target))
    action(assume=assume)
    if debug:
        print(">> Target %s finished to build" % (target))


def execute_multiple_targets(base_dir, exec_tree, targets,
                             debug=False, assume=None):
    already_executed = set()
    for target in targets:
        execute_target(base_dir, exec_tree, target,
                       already_executed, debug=debug, assume=assume)


def main():
    # Parse command line options
    parser = argparse.ArgumentParser()
    group = parser.add_mutually_exclusive_group()
    parser.add_argument("-D", "--base-dir",
                        help="base directory for problem to make "
                        "(CWD by default)",
                        dest="base_dir", action="store", default=None)
    parser.add_argument("-l", "--list",
                        help="list actions that cmsMake is aware of",
                        dest="list", action="store_true", default=False)
    parser.add_argument("-c", "--clean",
                        help="clean all generated files",
                        dest="clean", action="store_true", default=False)
    parser.add_argument("-a", "--all",
                        help="make all targets",
                        dest="all", action="store_true", default=False)
    group.add_argument("-y", "--yes",
                       help="answer yes to all questions", const='y',
                       dest="assume", action="store_const", default=None)
    group.add_argument("-n", "--no",
                       help="answer no to all questions", const='n',
                       dest="assume", action="store_const")
    parser.add_argument("-d", "--debug",
                        help="enable debug messages",
                        dest="debug", action="store_true", default=False)
    parser.add_argument("targets", metavar="target", nargs="*",
                        help="target to build", type=str)
    options = parser.parse_args()

    base_dir = options.base_dir
    if base_dir is None:
        base_dir = os.getcwd()
    else:
        base_dir = os.path.abspath(base_dir)

    assume = options.assume

    task_type = detect_task_type(base_dir)
    yaml_conf = parse_task_yaml(base_dir)
    actions = build_action_list(base_dir, task_type, yaml_conf)
    exec_tree, generated_list = build_execution_tree(actions)

    if [len(options.targets) > 0, options.list, options.clean,
            options.all].count(True) > 1:
        parser.error("Too many commands")

    if options.list:
        print("Task name: %s" % (detect_task_name(base_dir)))
        print("Task type: %s %s" % (task_type[0], task_type[1]))
        print("Available operations:")
        for entry in actions:
            print("  %s: %s -> %s" %
                  (entry[3], ", ".join(entry[0]), ", ".join(entry[1])))

    elif options.clean:
        print("Cleaning")
        clean(base_dir, generated_list)

    elif options.all:
        print("Making all targets")
        try:
            execute_multiple_targets(base_dir, exec_tree,
                                     generated_list, debug=options.debug,
                                     assume=assume)

        # After all work, possibly clean the left-overs of testing
        finally:
            clean_test_env()

    else:
        try:
            execute_multiple_targets(base_dir, exec_tree,
                                     options.targets, debug=options.debug,
                                     assume=assume)

        # After all work, possibly clean the left-overs of testing
        finally:
            clean_test_env()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = Test
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2013 Luca Versari <veluca93@gmail.com>
# Copyright © 2013 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import json
import os
import sys

from cmscontrib.YamlLoader import YamlLoader
from cms.db import Executable
from cms.db.filecacher import FileCacher
from cms.grading import format_status_text
from cms.grading.Job import EvaluationJob
from cms.grading.tasktypes import get_task_type


# TODO - Use a context object instead of global variables
task = None
file_cacher = None


def usage():
    print("""%s base_dir executable [assume]"
base_dir:   directory of the task
executable: solution to test (relative to the task's directory)
language:   programming language the solution is written in
assume:     if it's y, answer yes to every question
            if it's n, answer no to every question
""" % sys.argv[0])


def mem_human(mem):
    if mem is None:
        return 'None'
    if mem > 2 ** 30:
        return "%4.3gG" % (float(mem) / (2 ** 30))
    if mem > 2 ** 20:
        return "%4.3gM" % (float(mem) / (2 ** 20))
    if mem > 2 ** 10:
        return "%4dK" % (mem / (2 ** 10))
    return "%4d" % mem


def test_testcases(base_dir, soluzione, language, assume=None):
    global task, file_cacher

    # Use a FileCacher with a NullBackend in order to avoid to fill
    # the database with junk
    if file_cacher is None:
        file_cacher = FileCacher(null=True)

    # Load the task
    # TODO - This implies copying a lot of data to the FileCacher,
    # which is annoying if you have to do it continuously; it would be
    # better to use a persistent cache (although local, possibly
    # filesystem-based instead of database-based) and somehow detect
    # when the task has already been loaded
    if task is None:
        loader = YamlLoader(
            os.path.realpath(os.path.join(base_dir, "..")),
            file_cacher)
        # Normally we should import the contest before, but YamlLoader
        # accepts get_task() even without previous get_contest() calls
        task = loader.get_task(os.path.split(os.path.realpath(base_dir))[1])

    # Prepare the EvaluationJob
    dataset = task.active_dataset
    digest = file_cacher.put_file_from_path(
        os.path.join(base_dir, soluzione),
        "Solution %s for task %s" % (soluzione, task.name))
    executables = {task.name: Executable(filename=task.name, digest=digest)}
    jobs = [(t, EvaluationJob(
        language=language,
        task_type=dataset.task_type,
        task_type_parameters=json.loads(dataset.task_type_parameters),
        managers=dict(dataset.managers),
        executables=executables,
        input=dataset.testcases[t].input, output=dataset.testcases[t].output,
        time_limit=dataset.time_limit,
        memory_limit=dataset.memory_limit)) for t in dataset.testcases]
    tasktype = get_task_type(dataset=dataset)

    ask_again = True
    last_status = "ok"
    status = "ok"
    stop = False
    info = []
    points = []
    comments = []
    tcnames = []
    for jobinfo in sorted(jobs):
        print(jobinfo[0], end='')
        sys.stdout.flush()
        job = jobinfo[1]
        # Skip the testcase if we decide to consider everything to
        # timeout
        if stop:
            info.append("Time limit exceeded")
            points.append(0.0)
            comments.append("Timeout.")
            continue

        # Evaluate testcase
        last_status = status
        tasktype.evaluate(job, file_cacher)
        status = job.plus["exit_status"]
        info.append("Time: %5.3f   Wall: %5.3f   Memory: %s" %
                   (job.plus["execution_time"],
                    job.plus["execution_wall_clock_time"],
                    mem_human(job.plus["execution_memory"])))
        points.append(float(job.outcome))
        comments.append(format_status_text(job.text))
        tcnames.append(jobinfo[0])

        # If we saw two consecutive timeouts, ask wether we want to
        # consider everything to timeout
        if ask_again and status == "timeout" and last_status == "timeout":
            print()
            print("Want to stop and consider everything to timeout? [y/N]",
                  end='')
            if assume is not None:
                print(assume)
                tmp = assume
            else:
                tmp = raw_input().lower()
            if tmp in ['y', 'yes']:
                stop = True
            else:
                ask_again = False

    # Result pretty printing
    print()
    clen = max(len(c) for c in comments)
    ilen = max(len(i) for i in info)
    for (i, p, c, b) in zip(tcnames, points, comments, info):
        print("%s) %5.2lf --- %s [%s]" % (i, p, c.ljust(clen), b.center(ilen)))

    return zip(points, comments, info)


def clean_test_env():
    """Clean the testing environment, mostly to reclaim disk space.

    """
    # We're done: since we have no way to reuse this cache, we destroy
    # it to free space. See the TODO above.
    global file_cacher, task
    if file_cacher is not None:
        file_cacher.destroy_cache()
        file_cacher = None
        task = None

if __name__ == "__main__":
    if len(sys.argv) < 4:
        usage()
    if len(sys.argv) == 4:
        assume = None
    else:
        assume = sys.argv[4]
    test_testcases(sys.argv[1], sys.argv[2], sys.argv[3], assume=assume)

########NEW FILE########
__FILENAME__ = AdaptContest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Create a events file (almost) in the format of ReplayContest from a
filesystem tree. Almost mean that the column containing the task_id is
not included because there is no way this script is able to know
that. Task ids must be filled by hand int the events file after being
imported in CMS.

Let / be the root of the tree in the filesystem, then there are:
* directories /<task_short_name> (not starting with "test_")
* directories /<task_short_name>/<username>
* files /<task_short_name>/<username>/<s_id>.data
* files /<task_short_name>/<username>/<s_id>.zip

In the data file, there are one or two lines. The first line is the
timestamp of the submission in the format %H:%M:%S, the second line
(if present) is the timestamp of the token used on that submission.

The zip file contains all files submitted.

"""

from __future__ import absolute_import
from __future__ import print_function

import os
import shutil
import sys
import zipfile

from argparse import ArgumentParser
from glob import glob


def to_timestamp(time_tuple):
    """Translate a tuple (H, M, S) in a timestamp.

    time_tuple ((int)): time formatted as (H, M, S).

    return (int): the corresponding timestamp.

    """
    return time_tuple[2] + \
        time_tuple[1] * 60 + \
        time_tuple[0] * 3600


def main():
    """Main routine for translating. Manage command line arguments and
    walk around the filesystem building the events file.

    """
    parser = ArgumentParser(
        description="Translate from a filesystem format to the Replay format.")
    parser.add_argument("source", type=str, help="source directory")
    args = parser.parse_args()

    all_events = []

    tasks_short = [os.path.basename(x)
                   for x in glob("%s/*" % args.source)
                   if not os.path.basename(x).startswith("test_")]
    for t_short in tasks_short:
        print(t_short, file=sys.stderr)
        users = [os.path.basename(x)
                 for x in glob("%s/%s/*" % (args.source, t_short))
                 if len(os.path.basename(x)) == 4]
        for user in users:
            userdir = "%s/%s/%s" % (args.source, t_short, user)
            submission_nums = []
            submissions = [os.path.splitext(os.path.basename(x))[0]
                           for x in glob("%s/*.data" % userdir)]
            for sid in submissions:
                content = open("%s/%s.data" % (userdir, sid)).readlines()
                submit = to_timestamp(
                    [int(x)
                     for x in content[0].strip().split()[1].split(":")])

                token = None
                if len(content) >= 2:
                    token = to_timestamp(
                        [int(x)
                         for x in content[1].strip().split()[1].split(":")])

                zip_file = zipfile.ZipFile("%s/%s.zip" % (userdir, sid))
                try:
                    [filename] = [x for x in zip_file.filelist
                                  if x.filename.startswith(t_short + ".")]
                except ValueError:
                    filename = zip_file.filelist[0]
                filename = filename.filename
                extracted = zip_file.extract(filename, "/tmp/")
                _, ext = os.path.splitext(extracted)
                newname = "%s/%s%s" % (userdir, sid, ext)
                shutil.move("/tmp/%s" % filename, newname)

                submission_nums.append((submit, sid, token, newname))

            submission_nums.sort()
            for i, (timestamp, sid, token, filename) \
                    in enumerate(submission_nums):
                all_events.append((timestamp,
                                   user,
                                   t_short,
                                   "submit",
                                   filename))
                if token is not None:
                    all_events.append((timestamp,
                                       user,
                                       t_short,
                                       "token",
                                       i + 1))

    all_events.sort()
    for event in all_events:
        print(" ".join([str(x) for x in event]))

    return 0


if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = compile-fail
wibble monster

########NEW FILE########
__FILENAME__ = correct-fileio
n = int(open("input.txt").readline().strip())
f = open("output.txt", "w")
f.write("correct %d\n" % n)
# f intentionally left opened.

########NEW FILE########
__FILENAME__ = correct-stdio
import sys

n = int(sys.stdin.readline().strip())
sys.stdout.write("correct %d\n" % n)

########NEW FILE########
__FILENAME__ = half-correct-fileio
n = int(open("input.txt").readline().strip())
f = open("output.txt", "w")
if n % 2 == 0:
    f.write("correct 0\n")
else:
    f.write("correct %d\n" % n)

########NEW FILE########
__FILENAME__ = half-correct-stdio
import sys

n = int(sys.stdin.readline().strip())
if n % 2 == 0:
    sys.stdout.write("correct 0\n")
else:
    sys.stdout.write("correct %d\n" % n)

########NEW FILE########
__FILENAME__ = incorrect-fileio-with-stdio
correct-stdio.py
########NEW FILE########
__FILENAME__ = incorrect-fileio
n = int(open("input.txt").readline().strip())
f = open("output.txt", "w")
f.write("incorrect %d\n" % n)
f.close()

########NEW FILE########
__FILENAME__ = incorrect-stdio
import sys

n = int(sys.stdin.readline().strip())
sys.stdout.write("incorrect %d\n" % n)

########NEW FILE########
__FILENAME__ = nonzero-return-fileio
import sys

n = int(open("input.txt").readline().strip())
f = open("output.txt", "w")
f.write("correct %d\n" % n)
f.close()
sys.exit(1)

########NEW FILE########
__FILENAME__ = nonzero-return-stdio
import sys

n = int(sys.stdin.readline().strip())
sys.stdout.write("correct %d\n" % n)
sys.exit(1)

########NEW FILE########
__FILENAME__ = oom-heap
import sys

big = [0] * (128 * 1024 * 1024)
big[10000] = int(sys.stdin.readline().strip())
sys.stdout.write("correct %d\n" % big[10000])

########NEW FILE########
__FILENAME__ = timeout-cputime
import sys

n = int(sys.stdin.readline().strip())
sys.stdout.write("correct %d\n" % n)
while True:
    pass

########NEW FILE########
__FILENAME__ = parse_msgfmt
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2014 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import sys


def main():
    for line in sys.stdin:
        filename, data = [x.strip() for x in line.split(':')]
        filename = filename.strip('.')
        pieces = [x.strip() for x in data.split(',')]
        stats = {'translated': 0, 'untranslated': 0, 'fuzzy': 0}
        for piece in pieces:
            words = piece.split(' ')
            stats[words[1]] = int(words[0])
        print("%s,%d,%d,%d" % (filename,
                               stats['translated'],
                               stats['untranslated'],
                               stats['fuzzy']))

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = ReplayContest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""
ReplayContest takes as input a contest exported with ContestExported
(in the extracted form), and replays the contest, meaning that it asks
CWS for all submissions and tokens asked by the contestants, at the
right timing. The time can be increased in order to stress-test CMS.

TODO:
- currently only works with tasks with one file per submission (this
  is a limitation of SubmitRequest).
- implement handling of user tests.
- handle KeyboardInterrupt and notify of the correct commandline to
  resume the contest.
- set the correct parameters for the contest and tasks automatically.
- use a nicer graphics (ncurses based?).

"""

from __future__ import absolute_import
from __future__ import print_function

import os
import shutil
import json
import sys
import tempfile
import time

from argparse import ArgumentParser
from mechanize import Browser
from threading import Thread, RLock

from cms import config, logger
from cmscontrib.ContestImporter import ContestImporter
from cmstestsuite.web.CWSRequests import \
    LoginRequest, SubmitRequest, TokenRequest


def to_time(seconds):
    """Convert a relative timestamp in seconds to a human-readable
    format.

    seconds (int): timestamp.

    return (string): time formatted as %H:%M:%S.

    """
    hours = seconds // 3600
    minutes = seconds // 60 % 60
    seconds = seconds % 60
    return "%02d:%02d:%02d" % (hours, minutes, seconds)


def step(request):
    """Prepare and execute the request in a single instruction.

    request (GenericRequest): some request to *WS.

    """
    request.prepare()
    request.execute()


class ContestReplayer(object):

    def __init__(self, import_source, cws_address, no_import=False,
                 start_from=0):
        self.import_source = import_source
        self.cws_address = cws_address
        self.no_import = no_import
        self.start_from = start_from

        self.start = None
        self.speed = 1
        self.speed_lock = RLock()
        self.events = []

        self.importer = ContestImporter(drop=False,
                                        import_source=import_source,
                                        only_files=False, no_files=False,
                                        no_submissions=True)

    def run(self):
        """Main routine for replaying a contest, handling arguments from
        command line, and managing the speed of the replayer.

        """
        if not self.no_import:
            logger.info("Importing contest...")
            self.importer.run()
            logger.info("Contest imported.")

        logger.info("Please run CMS against the contest (with ip_lock=False).")
        logger.info("Please ensure that:")
        logger.info("- the contest is active (we are between start and stop);")
        logger.info("- the minimum interval for submissions and usertests ")
        logger.info("  (contest- and task-wise) is None.")
        logger.info("Then press enter to start.")
        raw_input()

        with open(os.path.join(self.import_source,
                               "contest.json")) as fin:
            self.compute_events(json.load(fin))

        thread = Thread(target=self.replay)
        thread.daemon = True
        thread.start()

        logger.info("Loading submission data...")
        while self.start is None:
            time.sleep(1)
        while thread.isAlive():
            new_speed = raw_input("Write the speed multiplier or q to quit "
                                  "(time %s, multiplier %s):\n" %
                                  (to_time((time.time() - self.start) *
                                           self.speed),
                                   self.speed))
            if new_speed == "q":
                return 0
            elif new_speed != "":
                try:
                    new_speed = int(new_speed)
                except ValueError:
                    logger.warning("Speed multiplier could not be parsed.")
                else:
                    self.recompute_start(new_speed)
        return 0

    def compute_events(self, contest):
        tasks = dict((task["name"], task["num"]) for task in contest["tasks"])
        for user in contest["users"]:
            tasks_num = dict((task["name"], 1) for task in contest["tasks"])
            for submission in sorted(user["submissions"],
                                     key=lambda x: x["timestamp"]):
                num = tasks_num[submission["task"]]
                tasks_num[submission["task"]] += 1
                self.events.append([
                    submission["timestamp"] - contest["start"],
                    user["username"],
                    user["password"],
                    tasks[submission["task"]],
                    submission["task"],
                    "s",  # For submit events.
                    (submission["files"], submission["language"]),
                    ])
                if submission["token"] is not None:
                    self.events.append([
                        submission["token"]["timestamp"] - contest["start"],
                        user["username"],
                        user["password"],
                        tasks[submission["task"]],
                        submission["task"],
                        "t",  # For token events.
                        num,
                        ])
        # TODO: add user test events.
        self.events.sort()

    def recompute_start(self, new_speed):
        """Utility to recompute the start time of a contest passing
        from a speed of self.speed to a speed of new_speed.

        new_speed(int): the new speed for the contest replayer.

        """
        with self.speed_lock:
            if self.speed != new_speed:
                self.start = self.start \
                    + (time.time() - self.start) * (new_speed - self.speed) \
                    * 1.0 / new_speed
                self.speed = new_speed

    def submit(self, timestamp, username, password, t_id, t_short,
               files, language):
        """Execute the request for a submission.

        timestamp (int): seconds from the start.
        username (string): username issuing the submission.
        password (string): password of username.
        t_id (string): id of the task.
        t_short (string): short name of the task.
        files ([dict]): list of dictionaries with keys 'filename' and
                        'digest'.
        language (string): the extension the files should have.

        """
        logger.info("%s - Submitting for %s on task %s."
                    % (to_time(timestamp), username, t_short))
        if len(files) != 1:
            logger.error("We cannot submit more than one file.")
            return

        # Copying submission files into a temporary directory with the
        # correct name. Otherwise, SubmissionRequest does not know how
        # to interpret the file (and which language are they in).
        temp_dir = tempfile.mkdtemp(dir=config.temp_dir)
        for file_ in files:
            temp_filename = os.path.join(temp_dir,
                                         file_["filename"].replace("%l",
                                                                   language))
            shutil.copy(
                os.path.join(self.import_source, "files", files[0]["digest"]),
                temp_filename
                )
            file_["filename"] = temp_filename

        filename = os.path.join(files[0]["filename"])
        browser = Browser()
        browser.set_handle_robots(False)
        step(LoginRequest(browser, username, password,
                          base_url=self.cws_address))
        step(SubmitRequest(browser,
                           (int(t_id), t_short),
                           filename=filename,
                           base_url=self.cws_address))
        shutil.rmtree(temp_dir)

    def token(self, timestamp, username, password, t_id, t_short,
              submission_num):
        """Execute the request for releasing test a submission.

        timestamp (int): seconds from the start.
        username (string): username issuing the submission.
        password (string): password of username.
        t_id (string): id of the task.
        t_short (string): short name of the task.
        submission_num (string): id of the submission to release test.

        """
        logger.info("%s - Playing token for %s on task %s"
                    % (to_time(timestamp), username, t_short))
        browser = Browser()
        browser.set_handle_robots(False)
        step(LoginRequest(browser, username, password,
                          base_url=self.cws_address))
        step(TokenRequest(browser,
                          (int(t_id), t_short),
                          submission_num=submission_num,
                          base_url=self.cws_address))

    def replay(self):
        """Start replaying the events in source on the CWS at the
        specified address.

        """
        with self.speed_lock:
            index = 0
            if self.start_from is not None:
                while index < len(self.events) \
                        and float(self.events[index][0]) < self.start_from:
                    index += 1
                self.start = time.time() - self.start_from
            else:
                self.start = time.time()

        while index < len(self.events):
            timestamp, username, password, task_id, task_name, type_, data \
                = self.events[index]
            to_wait = (timestamp / self.speed - (time.time() - self.start))
            while to_wait > .5:
                if 0 < to_wait % 10 <= .5:
                    logger.info("Next event in %d seconds." % int(to_wait))
                time.sleep(.5)
                to_wait = (timestamp / self.speed - (time.time() - self.start))
            if to_wait > 0:
                time.sleep(to_wait)

            if type_ == "s":  # Submit.
                files, language = data
                self.submit(timestamp=timestamp,
                            username=username,
                            password=password,
                            t_id=task_id,
                            t_short=task_name,
                            files=files,
                            language=language)
            elif type_ == "t":  # Token.
                self.token(timestamp=timestamp,
                           username=username,
                           password=password,
                           t_id=task_id,
                           t_short=task_name,
                           submission_num=data)
            else:
                logger.warning("Unexpected type `%s', ignoring." % type_)

            index += 1


def main():
    parser = ArgumentParser(description="Replayer of CMS contests.")
    parser.add_argument("cws_address", type=str, help="http address of CWS",
                        default="http://127.0.0.1:8888")
    parser.add_argument("import_source",
                        help="source directory or compressed file")
    parser.add_argument("-i", "--no-import", action="store_true",
                        help="assume the contest is already in the database")
    parser.add_argument("-r", "--resume", type=str,
                        help="start from (%%H:%%M:%%S)")
    args = parser.parse_args()
    start_from = None
    if args.resume is not None:
        try:
            start_from = int(args.resume[6:8]) + \
                int(args.resume[3:5]) * 60 + \
                int(args.resume[0:2]) * 3600
        except:
            logger.critical("Invalid resume time %s, format is %%H:%%M:%%S"
                            % args.resume)
            return 1

    if not os.path.isdir(args.import_source):
        logger.critical("Please extract the contest "
                        "before using ReplayContest.")
        return 1

    ContestReplayer(
        import_source=args.import_source,
        no_import=args.no_import,
        start_from=start_from,
        cws_address=args.cws_address
        ).run()

    return 0


if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = ReplayContestFromAdapter
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""
The file with the events has one row per event, with these format
(space separated):
timestamp (seconds relative to the contest)
username
task_id
task_shortname
action (submit|release)

Next, for submit:
comma-separated list of file paths

For token:
submission_num

All users have empty passwords.

TODO: currently only works with tasks with one file per submission
(this is a limitation of SubmitRequest).

"""

from __future__ import absolute_import
from __future__ import print_function

import sys
import time

from argparse import ArgumentParser
from mechanize import Browser
from threading import Thread

from cmstestsuite.web.CWSRequests import \
    LoginRequest, SubmitRequest, TokenRequest


start = None
speed = 1
old_speed = 1


def to_time(seconds):
    """Convert a relative timestamp in seconds to a human-readable
    format.

    seconds (int): timestamp.

    return (string): time formatted as %H:%M:%S.

    """
    hours = seconds // 3600
    minutes = seconds // 60 % 60
    seconds = seconds % 60
    return "%02d:%02d:%02d" % (hours, minutes, seconds)


def recompute_start(start, speed, old_speed):
    """Utility to recompute the start time of a contest passing from a
    speed of old_speed to a speed of speed.

    start (int): old start time of the contest with speed old_speed.
    speed (int): new speed of the replayer.
    old_speed (int): previous speed of the replayer.

    return (int): new start time using the new speed.

    """
    if speed != old_speed:
        start += (time.time() - start) * (speed - old_speed) * 1.0 / speed
        old_speed = speed
    return start, speed, old_speed


def step(request):
    """Prepare and execute the request in a single instruction.

    request (GenericRequest): some request to *WS.

    """
    request.prepare()
    request.execute()


def submit(timestamp, username, t_id, t_short, files, base_url):
    """Execute the request for a submission.

    timestamp (int): seconds from the start.
    username (string): username issuing the submission.
    t_id (string): id of the task.
    t_short (string): short name of the task.
    files ([string]): list of filenames of submitted files.
    base_url (string): http address of CWS.

    """
    print("\n%s - Submitting for %s on task %s" %
          (to_time(timestamp), username, t_short), end='')
    browser = Browser()
    browser.set_handle_robots(False)
    step(LoginRequest(browser, username, "", base_url=base_url))
    step(SubmitRequest(browser,
                       (int(t_id), t_short),
                       filename=files[0],
                       base_url=base_url))


def token(timestamp, username, t_id, t_short, submission_num, base_url):
    """Execute the request for releasing test a submission.

    timestamp (int): seconds from the start.
    username (string): username issuing the submission.
    t_id (string): id of the task.
    t_short (string): short name of the task.
    submission_num (string): id of the submission to release test.
    base_url (string): http address of CWS.

    """
    print("\n%s - Playing token for %s on task %s" %
          (to_time(timestamp), username, t_short), end='')
    browser = Browser()
    browser.set_handle_robots(False)
    step(LoginRequest(browser, username, "", base_url=base_url))
    step(TokenRequest(browser,
                      (int(t_id), t_short),
                      submission_num=submission_num,
                      base_url=base_url))


def replay(base_url, source="./source.txt", start_from=None):
    """Start replaying the events in source on the CWS at the
    specified address.

    base_url (string): http address of CWS.
    source (string): events file.

    """
    global start, speed, old_speed

    content = [x.strip().split() for x in open(source).readlines()]
    events = len(content)
    index = 0
    if start_from is not None:
        while index < events and float(content[index][0]) < start_from:
            index += 1
        start = time.time() - start_from
    else:
        start = time.time()

    while index < events:
        next_time = float(content[index][0])
        start, speed, old_speed = recompute_start(start, speed, old_speed)
        to_wait = (next_time / speed - (time.time() - start))
        while to_wait > .5:
            time.sleep(.5)
            start, speed, old_speed = recompute_start(start, speed, old_speed)
            to_wait = (next_time / speed - (time.time() - start))
        if to_wait > 0:
            time.sleep(to_wait)
        start, speed, old_speed = recompute_start(start, speed, old_speed)

        if content[index][4] == "submit":
            submit(timestamp=next_time,
                   username=content[index][1],
                   t_id=content[index][2],
                   t_short=content[index][3],
                   files=content[index][5].split(","),
                   base_url=base_url)
        elif content[index][4] == "token":
            token(timestamp=next_time,
                  username=content[index][1],
                  t_id=content[index][2],
                  t_short=content[index][3],
                  submission_num=int(content[index][5]),
                  base_url=base_url)

        index += 1


def main():
    """Main routine for replaying a contest, handling arguments from
    command line, and managing the speed of the replayer.

    """
    global start, speed, old_speed

    parser = ArgumentParser(description="Replay a contest.")
    parser.add_argument("address", type=str, help="http address of CWS",
                        default="http://127.0.0.1:8888")
    parser.add_argument("source", type=str, help="events file")
    parser.add_argument("-r", "--resume", type=str,
                        help="start from (%%H:%%M:%%S)")
    args = parser.parse_args()
    start_from = None
    if args.resume is not None:
        try:
            start_from = int(args.resume[6:8]) + \
                int(args.resume[3:5]) * 60 + \
                int(args.resume[0:2]) * 3600
        except:
            print("Invalid resume time %s, format is %%H:%%M:%%S" %
                  args.resume)

    thread = Thread(target=replay,
                    args=(args.address, args.source, start_from))
    thread.start()
    print("Wait for data to load...")
    while start is None:
        time.sleep(1)
    while thread.isAlive():
        command = raw_input("\nWrite the speed multiplier "
                            "(time %s, multiplier %s): " %
                            (to_time((time.time() - start) * speed), speed))
        try:
            command = int(command)
        except ValueError:
            print("Speed multiplier could not be parsed.")
        else:
            start, speed, old_speed = \
                recompute_start(start, command, old_speed)
    return 0


if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = RunFunctionalTests
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Bernard Blackham <bernard@largestprime.net>
# Copyright © 2013-2014 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import os
import sys
import subprocess
import datetime
import re

from argparse import ArgumentParser

from cms import LANGUAGES
from cmstestsuite import get_cms_config, CONFIG, info, sh
from cmstestsuite import add_contest, add_existing_user, add_existing_task, \
    add_user, add_task, add_testcase, add_manager, combine_coverage, \
    get_tasks, get_users, start_service, start_server, \
    start_ranking_web_server, shutdown_services, restart_service
from cmstestsuite.Test import TestFailure
import cmstestsuite.Tests
from cmscommon.datetime import get_system_timezone


FAILED_TEST_FILENAME = '.testfailures'

# This stores a mapping from task name to (task id, task_module)
task_id_map = {}


def start_generic_services():
    start_service("LogService")
    start_service("ResourceService")
    start_service("Checker")
    start_service("Worker")
    start_service("ScoringService")
    start_server("AdminWebServer")
    # Just to verify it starts successfully.
    start_ranking_web_server()


def create_contest():
    start_time = datetime.datetime.utcnow()
    stop_time = start_time + datetime.timedelta(1, 0, 0)
    contest_id = add_contest(
        name="testcontest1",
        description="A test contest #1.",
        languages=LANGUAGES,
        start=start_time.strftime("%Y-%m-%d %H:%M:%S.%f"),
        stop=stop_time.strftime("%Y-%m-%d %H:%M:%S.%f"),
        timezone=get_system_timezone(),
        token_mode="finite",
        token_max_number="100",
        token_min_interval="0",
        token_gen_initial="100",
        token_gen_number="0",
        token_gen_interval="1",
        token_gen_max="100",
    )

    info("Created contest %d." % contest_id)

    return contest_id


def start_contest(contest_id):
    start_service("EvaluationService", contest=contest_id)
    start_server("ContestWebServer", contest=contest_id)
    # Just to verify it starts successfully.
    start_service("ProxyService", contest=contest_id)


global num_users
num_users = 0


def create_or_get_user(contest_id):
    global num_users
    num_users += 1

    def enumerify(x):
        if 11 <= x <= 13:
            return 'th'
        return {1: 'st', 2: 'nd', 3: 'rd'}.get(x % 10, 'th')

    username = "testrabbit%d" % num_users

    # Find a user that may already exist (from a previous contest).
    users = get_users(contest_id)
    user_create_args = {
        "username": username,
        "password": "kamikaze",
        "first_name": "Ms. Test",
        "last_name": "Wabbit the %d%s" % (num_users, enumerify(num_users)),
    }
    if username in users:
        user_id = users[username]['id']
        add_existing_user(contest_id, user_id, **user_create_args)
        info("Using existing user with id %d." % user_id)
    else:
        user_id = add_user(contest_id, **user_create_args)
        info("Created user with id %d." % user_id)
    return user_id


def get_task_id(contest_id, user_id, task_module):
    name = task_module.task_info['name']

    # Have we done this before? Pull it out of our cache if so.
    if task_module in task_id_map:
        # Ensure we don't have multiple modules with the same task name.
        assert task_id_map[task_module][1] == task_module

        return task_id_map[name][0]

    task_create_args = {
        "token_mode": "finite",
        "token_max_number": "100",
        "token_min_interval": "0",
        "token_gen_initial": "100",
        "token_gen_number": "0",
        "token_gen_interval": "1",
        "token_gen_max": "100",
        "max_submission_number": "100",
        "max_user_test_number": "100",
        "min_submission_interval": None,
        "min_user_test_interval": None,
    }
    task_create_args.update(task_module.task_info)

    # Find if the task already exists in the contest.
    tasks = get_tasks(contest_id)
    if name in tasks:
        # Then just use the existing one.
        task = tasks[name]
        task_id = task['id']
        task_id_map[name] = (task_id, task_module)
        add_existing_task(contest_id, task_id, **task_create_args)
        return task_id

    # Otherwise, we need to add the task ourselves.
    task_id = add_task(contest_id, **task_create_args)

    # add any managers
    code_path = os.path.join(
        os.path.dirname(task_module.__file__),
        "code")
    if hasattr(task_module, 'managers'):
        for manager in task_module.managers:
            mpath = os.path.join(code_path, manager)
            add_manager(task_id, mpath)

    # add the task's test data.
    data_path = os.path.join(
        os.path.dirname(task_module.__file__),
        "data")
    for num, (input_file, output_file, public) \
            in enumerate(task_module.test_cases):
        ipath = os.path.join(data_path, input_file)
        opath = os.path.join(data_path, output_file)
        add_testcase(task_id, str(num), ipath, opath, public)

    task_id_map[name] = (task_id, task_module)

    info("Created task %s as id %d" % (name, task_id))

    # We need to restart ProxyService to ensure it reinitializes,
    # picking up the new task and sending it to RWS.
    restart_service("ProxyService", contest=contest_id)

    return task_id


def get_all_tests():
    tests = []
    for i, test in enumerate(cmstestsuite.Tests.ALL_TESTS):
        for lang in test.languages:
            tests.append((test, lang))

    return tests


def load_test_list_from_file(filename):
    """Load a list of tests to execute from the given file. Each line of the
    file should be of the format:

    testname language1

    """
    if not os.path.exists(filename):
        return []
    try:
        with open(filename) as f:
            lines = f.readlines()
    except (IOError, OSError) as e:
        print("Failed to read test list. %s." % (e))
        return None

    errors = False

    name_to_test_map = {}
    for test in cmstestsuite.Tests.ALL_TESTS:
        if test.name in name_to_test_map:
            print("ERROR: Multiple tests with the same name `%s'." % test.name)
            errors = True
        name_to_test_map[test.name] = test

    tests = []
    for i, line in enumerate(lines):
        bits = [x.strip() for x in line.split()]
        if len(bits) != 2:
            print("ERROR: %s:%d invalid line: %s" % (filename, i + 1, line))
            errors = True
            continue

        name, lang = bits

        if name not in name_to_test_map:
            print("ERROR: %s:%d invalid test case: %s" %
                  (filename, i + 1, name))
            errors = True
            continue

        test = name_to_test_map[name]
        if lang not in test.languages:
            print("ERROR: %s:%d test `%s' does not have language `%s'" %
                  (filename, i + 1, name, lang))
            errors = True
            continue

        tests.append((test, lang))

    if errors:
        sys.exit(1)

    return tests


def load_failed_tests():
    list = load_test_list_from_file(FAILED_TEST_FILENAME)
    if list is None:
        sys.exit(1)

    return list


def filter_testcases(orig_test_list, regexes, languages):
    """Filter out skipped test cases from a list."""
    # Define a function that returns true if the given test case matches the
    # criteria.
    def use(test, lang):
        # No regexes means no constraint on test names.
        ok = not regexes
        for regex in regexes:
            if regex.search(test.name):
                ok = True
                break
        if ok and languages:
            ok = lang in languages
        return ok

    # Select only those (test, language) pairs that pass our checks.
    return [(test, lang) for test, lang in orig_test_list if use(test, lang)]


def write_test_case_list(test_list, filename):
    with open(filename, 'w') as f:
        for test, lang in test_list:
            f.write('%s %s\n' % (test.name, lang))


def run_testcases(contest_id, user_id, test_list):
    """Run all test cases specified by the Tests module.

    contest_id and user_id must specify an already-created contest and user
    under which the tests are submitted.

    test_list should be a list of 2-tuples, each representing a test. The first
    element of each tuple is a Test object, and the second is the language for
    which it should be executed.
    """
    info("Running test cases ...")

    failures = []
    num_tests_to_execute = len(test_list)

    # For all tests...
    for i, (test, lang) in enumerate(test_list):
        # This installs the task into the contest if we haven't already.
        task_id = get_task_id(contest_id, user_id, test.task_module)

        info("Running test %d/%d: %s (%s)" % (
            i + 1, num_tests_to_execute,
            test.name, lang))

        try:
            test.run(contest_id, task_id, user_id, lang)
        except TestFailure as f:
            info("  (FAILED: %s)" % f.message)

            # Add this case to our list of failures, if we haven't already.
            failures.append((test, lang, f.message))

    results = "\n\n"
    if not failures:
        results += "================== ALL TESTS PASSED! ==================\n"
    else:
        results += "------ TESTS FAILED: ------\n"

    results += " Executed: %d\n" % num_tests_to_execute
    results += "   Failed: %d\n" % len(failures)
    results += "\n"

    for test, lang, msg in failures:
        results += " %s (%s): %s\n" % (test.name, lang, msg)

    if failures:
        write_test_case_list(
            [(test, lang) for test, lang, _ in failures],
            FAILED_TEST_FILENAME)
        results += "\n"
        results += "Failed tests stored in %s.\n" % FAILED_TEST_FILENAME
        results += "Run again with --retry-failed (or -r) to retry.\n"

    return len(failures) == 0, results


def time_difference(start_time, end_time):
    secs = int((end_time - start_time).total_seconds())
    mins = secs / 60
    secs = secs % 60
    hrs = mins / 60
    mins = mins % 60
    return "Time elapsed: %02d:%02d:%02d" % (hrs, mins, secs)


def config_is_usable(cms_config):
    """Determine if this configuration is suitable for testing."""

    return True


def main():
    parser = ArgumentParser(description="Runs the CMS functional test suite.")
    parser.add_argument(
        "regex", metavar="regex",
        type=str, nargs='*',
        help="a regex to match to run a subset of tests")
    parser.add_argument(
        "-l", "--languages",
        type=str, action="store", default="",
        help="a comma-separated list of languages to test")
    parser.add_argument(
        "-c", "--contest", action="store",
        help="use an existing contest (and the tasks in it)")
    parser.add_argument(
        "-r", "--retry-failed", action="store_true",
        help="only run failed tests from the previous run (stored in %s)" %
        FAILED_TEST_FILENAME)
    parser.add_argument(
        "-n", "--dry-run", action="store_true",
        help="show what tests would be run, but do not run them")
    parser.add_argument(
        "-v", "--verbose", action="count",
        help="print debug information (use multiple times for more)")
    args = parser.parse_args()

    CONFIG["VERBOSITY"] = args.verbose

    start_time = datetime.datetime.now()

    # Pre-process our command-line arugments to figure out which tests to run.
    regexes = [re.compile(s) for s in args.regex]
    if args.languages:
        languages = frozenset(args.languages.split(','))
    else:
        languages = frozenset()
    if args.retry_failed:
        test_list = load_failed_tests()
    else:
        test_list = get_all_tests()
    test_list = filter_testcases(test_list, regexes, languages)

    if not test_list:
        info("There are no tests to run! (was your filter too restrictive?)")
        return 0

    if args.dry_run:
        for t in test_list:
            print(t[0].name, t[1])
        return 0

    if args.retry_failed:
        info("Re-running %d failed tests from last run." % len(test_list))

    # Load config from cms.conf.
    try:
        git_root = subprocess.check_output(
            "git rev-parse --show-toplevel", shell=True,
            stderr=open(os.devnull, "w")).strip()
    except subprocess.CalledProcessError:
        git_root = None
    CONFIG["TEST_DIR"] = git_root
    CONFIG["CONFIG_PATH"] = "%s/examples/cms.conf" % CONFIG["TEST_DIR"]
    if CONFIG["TEST_DIR"] is None:
        CONFIG["CONFIG_PATH"] = "/usr/local/etc/cms.conf"
    cms_config = get_cms_config()

    if not config_is_usable(cms_config):
        return 1

    if CONFIG["TEST_DIR"] is not None:
        # Set up our expected environment.
        os.chdir("%(TEST_DIR)s" % CONFIG)
        os.environ["PYTHONPATH"] = "%(TEST_DIR)s" % CONFIG

        # Clear out any old coverage data.
        info("Clearing old coverage data.")
        sh("python-coverage erase")

    # Fire us up!
    start_generic_services()
    if args.contest is None:
        contest_id = create_contest()
    else:
        contest_id = int(args.contest)
    user_id = create_or_get_user(contest_id)

    start_contest(contest_id)

    # Run all of our test cases.
    passed, test_results = run_testcases(contest_id, user_id, test_list)

    # And good night!
    shutdown_services()
    combine_coverage()

    print(test_results)

    end_time = datetime.datetime.now()
    print(time_difference(start_time, end_time))

    if passed:
        return 0
    else:
        return 1

if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = RunTests
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import sys

from cmstestsuite import FrameworkException, sh


def main():
    try:
        sh(["./cmstestsuite/RunUnitTests.py"] + sys.argv[1:])
        sh(["./cmstestsuite/RunFunctionalTests.py"] + sys.argv[1:])
    except FrameworkException:
        return 1
    return 0


if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = RunUnitTests
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import os
import sys
import subprocess
import datetime
from argparse import ArgumentParser

from cmstestsuite import CONFIG, FrameworkException, info, sh
from cmstestsuite import combine_coverage


FAILED_UNITTEST_FILENAME = '.unittestfailures'


def run_unittests(test_list):
    """Run all needed unit tests.

    test_list ([(string, string)]): a list of test to run in the
                                    format (path, filename.py).
    return (int):
    """
    info("Running unit tests...")

    failures = []
    num_tests_to_execute = len(test_list)

    # For all tests...
    for i, (path, filename) in enumerate(test_list):
        info("Running test %d/%d: %s.%s" % (
            i + 1, num_tests_to_execute,
            path, filename))
        try:
            sh('python-coverage run -p --source=cms %s' %
               os.path.join(path, filename))
        except FrameworkException:
            info("  (FAILED: %s)" % filename)

            # Add this case to our list of failures, if we haven't already.
            failures.append((path, filename))

    results = "\n\n"
    if not failures:
        results += "================== ALL TESTS PASSED! ==================\n"
    else:
        results += "------ TESTS FAILED: ------\n"

    results += " Executed: %d\n" % num_tests_to_execute
    results += "   Failed: %d\n" % len(failures)
    results += "\n"

    for path, filename in failures:
        results += " %s.%s\n" % (path, filename)

    if failures:
        with open(FAILED_UNITTEST_FILENAME, "w") as failed_filename:
            for path, filename in failures:
                failed_filename.write("%s %s\n" % (path, filename))
        results += "\n"
        results += "Failed tests stored in %s.\n" % FAILED_UNITTEST_FILENAME
        results += "Run again with --retry-failed (or -r) to retry.\n"

    return len(failures) == 0, results


def load_test_list_from_file(filename):
    """Load path and names of unittest files from a filename.

    filename (string): the file to load, containing strings in the
                       format <path> <test_filename>.
    return ([(string, string)]): the content of the file.
    """
    if not os.path.exists(filename):
        return []
    try:
        lines = open(filename).readlines()
        return [line.strip().split(" ") for line in lines]
    except (IOError, OSError) as error:
        print("Failed to read test list. %s." % error)
        return None


def get_all_tests():
    tests = []
    for path, _, names in os.walk(os.path.join("cmstestsuite", "unit_tests")):
        for name in names:
            if name.endswith(".py"):
                tests.append((path, name))
    return tests


def load_failed_tests():
    failed_tests = load_test_list_from_file(FAILED_UNITTEST_FILENAME)
    if failed_tests is None:
        sys.exit(1)

    return failed_tests


def time_difference(start_time, end_time):
    secs = int((end_time - start_time).total_seconds())
    mins = secs / 60
    secs = secs % 60
    hrs = mins / 60
    mins = mins % 60
    return "Time elapsed: %02d:%02d:%02d" % (hrs, mins, secs)


def main():
    parser = ArgumentParser(description="Runs the CMS unittest suite.")
    parser.add_argument(
        "-n", "--dry-run", action="store_true",
        help="show what tests would be run, but do not run them")
    parser.add_argument(
        "-v", "--verbose", action="count",
        help="print debug information (use multiple times for more)")
    parser.add_argument(
        "-r", "--retry-failed", action="store_true",
        help="only run failed tests from the previous run (stored in %s)" %
        FAILED_UNITTEST_FILENAME)

    # Unused parameters.
    parser.add_argument(
        "regex", metavar="regex",
        type=str, nargs='*', help="unused")
    parser.add_argument(
        "-l", "--languages",
        type=str, action="store", default="", help="unused")
    parser.add_argument("-c", "--contest", action="store", help="unused")

    args = parser.parse_args()

    CONFIG["VERBOSITY"] = args.verbose

    start_time = datetime.datetime.now()

    try:
        git_root = subprocess.check_output(
            "git rev-parse --show-toplevel", shell=True,
            stderr=open(os.devnull, "w")).strip()
    except subprocess.CalledProcessError:
        print("Please run the unit tests from the git repository.")
        return 1

    if args.retry_failed:
        test_list = load_failed_tests()
    else:
        test_list = get_all_tests()

    if args.dry_run:
        for t in test_list:
            print(t[0].name, t[1])
        return 0

    if args.retry_failed:
        info("Re-running %d failed tests from last run." % len(test_list))

    # Load config from cms.conf.
    CONFIG["TEST_DIR"] = git_root
    CONFIG["CONFIG_PATH"] = "%s/examples/cms.conf" % CONFIG["TEST_DIR"]
    if CONFIG["TEST_DIR"] is None:
        CONFIG["CONFIG_PATH"] = "/usr/local/etc/cms.conf"

    if CONFIG["TEST_DIR"] is not None:
        # Set up our expected environment.
        os.chdir("%(TEST_DIR)s" % CONFIG)
        os.environ["PYTHONPATH"] = "%(TEST_DIR)s" % CONFIG

        # Clear out any old coverage data.
        info("Clearing old coverage data.")
        sh("python-coverage erase")

    # Run all of our test cases.
    passed, test_results = run_unittests(test_list)

    combine_coverage()

    print(test_results)

    end_time = datetime.datetime.now()
    print(time_difference(start_time, end_time))

    if passed:
        return 0
    else:
        return 1

if __name__ == "__main__":
    sys.exit(main())

########NEW FILE########
__FILENAME__ = StressTest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2014 Artem Iglikov <artem.iglikov@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import ast
import os
import sys
import mechanize
import threading
import optparse
import random
import time
import io

from cms import config, ServiceCoord, get_service_address
from cms.db import Contest, SessionGen

import cmstestsuite.web
from cmstestsuite.web.CWSRequests import HomepageRequest, LoginRequest, \
    TaskRequest, TaskStatementRequest, SubmitRandomRequest


cmstestsuite.web.debug = True


class RequestLog(object):

    def __init__(self, log_dir=None):
        self.total = 0
        self.success = 0
        self.failure = 0
        self.error = 0
        self.undecided = 0
        self.total_time = 0.0
        self.max_time = 0.0

        self.log_dir = log_dir
        if self.log_dir is not None:
            try:
                os.makedirs(self.log_dir)
            except OSError:
                pass

    def print_stats(self):
        print("TOTAL:          %5d" % (self.total), file=sys.stderr)
        print("SUCCESS:        %5d" % (self.success), file=sys.stderr)
        print("FAIL:           %5d" % (self.failure), file=sys.stderr)
        print("ERROR:          %5d" % (self.error), file=sys.stderr)
        print("UNDECIDED:      %5d" % (self.undecided), file=sys.stderr)
        print("Total time:   %7.3f" % (self.total_time), file=sys.stderr)
        print("Average time: %7.3f" % (self.total_time / self.total),
              file=sys.stderr)
        print("Max time:     %7.3f" % (self.max_time), file=sys.stderr)

    def merge(self, log2):
        self.total += log2.total
        self.success += log2.success
        self.failure += log2.failure
        self.error += log2.error
        self.undecided += log2.undecided
        self.total_time += log2.total_time
        self.max_time = max(self.max_time, log2.max_time)

    def store_to_file(self, request):
        if self.log_dir is None:
            return

        filename = "%s_%s.log" % (request.start_time,
                                  request.__class__.__name__)
        filepath = os.path.join(self.log_dir, filename)
        linkpath = os.path.join(self.log_dir, request.__class__.__name__)
        with io.open(filepath, 'w', encoding='utf-8') as fd:
            request.store_to_file(fd)
        try:
            os.remove(linkpath)
        except OSError:
            pass
        os.symlink(filename, linkpath)


class ActorDying(Exception):
    """Exception to be raised when an Actor is going to die soon. See
    Actor class.

    """
    pass


class Actor(threading.Thread):
    """Class that simulates the behaviour of a user of the system. It
    performs some requests at randomized times (checking CMS pages,
    doing submissions, ...), checking for their success or failure.

    The probability that the users doing actions depends on the value
    specified in an object called "metrics".

    """

    def __init__(self, username, password, metrics, tasks,
                 log=None, base_url=None, submissions_path=None):
        threading.Thread.__init__(self)

        self.username = username
        self.password = password
        self.metric = metrics
        self.tasks = tasks
        self.log = log
        self.base_url = base_url
        self.submissions_path = submissions_path

        self.name = "Actor thread for user %s" % (self.username)

        self.browser = mechanize.Browser()
        self.die = False

    def run(self):
        try:
            print("Starting actor for user %s" % (self.username),
                  file=sys.stderr)
            self.act()

        except ActorDying:
            print("Actor dying for user %s" % (self.username), file=sys.stderr)

    def act(self):
        """Define the behaviour of the actor. Subclasses are expected
        to overwrite this stub method properly.

        """
        raise Exception("Not implemented. Please subclass Action"
                        "and overwrite act().")

    def do_step(self, request):
        self.wait_next()
        self.log.total += 1
        try:
            request.prepare()
        except Exception as exc:
            print("Unhandled exception while preparing the request: %s" %
                  (str(exc)), file=sys.stderr)
            return
        try:
            request.execute()
        except Exception as exc:
            print("Unhandled exception while executing the request %s" %
                  (str(exc)), file=sys.stderr)
            return
        self.log.__dict__[request.outcome] += 1
        self.log.total_time += request.duration
        self.log.max_time = max(self.log.max_time, request.duration)
        self.log.store_to_file(request)

    def wait_next(self):
        """Wait some time. At the moment it waits c*X seconds, where c
        is the time_coeff parameter in metrics and X is an
        exponentially distributed random variable, with parameter
        time_lambda in metrics.

        The total waiting time is divided in lots of little sleep()
        call each one of 0.1 seconds, so that the waiting gets
        interrupted if a die signal arrives.

        If a die signal is received, an ActorDying exception is
        raised.

        """
        SLEEP_PERIOD = 0.1
        time_to_wait = self.metric['time_coeff'] * \
            random.expovariate(self.metric['time_lambda'])
        sleep_num = int(time_to_wait / SLEEP_PERIOD)
        for i in xrange(sleep_num):
            time.sleep(SLEEP_PERIOD)
            if self.die:
                raise ActorDying()


class RandomActor(Actor):

    def act(self):
        # Start with logging in and checking to be logged in
        self.do_step(HomepageRequest(self.browser,
                                     self.username,
                                     loggedin=False,
                                     base_url=self.base_url))
        self.do_step(LoginRequest(self.browser,
                                  self.username,
                                  self.password,
                                  base_url=self.base_url))
        self.do_step(HomepageRequest(self.browser,
                                     self.username,
                                     loggedin=True,
                                     base_url=self.base_url))

        # Then keep forever stumbling across user pages
        while True:
            choice = random.random()
            task = random.choice(self.tasks)
            if choice < 0.1 and self.submissions_path is not None:
                self.do_step(SubmitRandomRequest(
                    self.browser,
                    task,
                    base_url=self.base_url,
                    submissions_path=self.submissions_path))
            elif choice < 0.6 and task[2] != []:
                self.do_step(TaskStatementRequest(self.browser,
                                                  task[1],
                                                  random.choice(task[2]),
                                                  base_url=self.base_url))
            else:
                self.do_step(TaskRequest(self.browser,
                                         task[1],
                                         base_url=self.base_url))


def harvest_contest_data(contest_id):
    """Retrieve the couples username, password and the task list for a
    given contest.

    contest_id (int): the id of the contest we want.
    return (tuple): the first element is a dictionary mapping
                    usernames to passwords; the second one is the list
                    of the task names.

    """
    users = {}
    tasks = []
    with SessionGen() as session:
        contest = Contest.get_from_id(contest_id, session)
        for user in contest.users:
            users[user.username] = {'password': user.password}
        for task in contest.tasks:
            tasks.append((task.id, task.name, task.statements.keys()))
    return users, tasks


DEFAULT_METRICS = {'time_coeff': 10.0,
                   'time_lambda': 2.0}


def main():
    parser = optparse.OptionParser(usage="usage: %prog [options]")
    parser.add_option("-c", "--contest",
                      help="contest ID to export", dest="contest_id",
                      action="store", type="int", default=None)
    parser.add_option("-n", "--actor-num",
                      help="the number of actors to spawn", dest="actor_num",
                      action="store", type="int", default=None)
    parser.add_option("-s", "--sort-actors",
                      help="sort usernames alphabetically "
                      "instead of randomizing before slicing them",
                      action="store_true", default=False, dest="sort_actors")
    parser.add_option("-u", "--base-url",
                      help="base URL for placing HTTP requests",
                      action="store", default=None, dest="base_url")
    parser.add_option("-S", "--submissions-path",
                      help="base path for submission to send",
                      action="store", default=None, dest="submissions_path")
    parser.add_option("-p", "--prepare-path",
                      help="file to put contest info to",
                      action="store", default=None, dest="prepare_path")
    parser.add_option("-r", "--read-from",
                      help="file to read contest info from",
                      action="store", default=None, dest="read_from")
    options = parser.parse_args()[0]

    # If prepare_path is specified we only need to save some useful
    # contest data and exit.
    if options.prepare_path is not None:
        users, tasks = harvest_contest_data(options.contest_id)
        contest_data = dict()
        contest_data['users'] = users
        contest_data['tasks'] = tasks
        with open(options.prepare_path, "w") as file_:
            file_.write(str(contest_data))
        return

    users = []
    tasks = []

    # If read_from is not specified, read contest data from database
    # if it is specified - read contest data from the file
    if options.read_from is None:
        users, tasks = harvest_contest_data(options.contest_id)
    else:
        with open(options.read_from, "r") as file_:
            contest_data = ast.literal_eval(file_.read())
        users = contest_data['users']
        tasks = contest_data['tasks']

    if options.actor_num is not None:
        user_items = users.items()
        if options.sort_actors:
            user_items.sort()
        else:
            random.shuffle(user_items)
        users = dict(user_items[:options.actor_num])

    # If the base URL is not specified, we try to guess it; anyway,
    # the guess code isn't very smart...
    if options.base_url is not None:
        base_url = options.base_url
    else:
        base_url = "http://%s:%d/" % \
            (get_service_address(ServiceCoord('ContestWebServer', 0))[0],
             config.contest_listen_port[0])

    actors = [RandomActor(username, data['password'], DEFAULT_METRICS, tasks,
                          log=RequestLog(log_dir=os.path.join('./test_logs',
                                                              username)),
                          base_url=base_url,
                          submissions_path=options.submissions_path)
              for username, data in users.iteritems()]
    for actor in actors:
        actor.start()

    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("Taking down actors", file=sys.stderr)
        for actor in actors:
            actor.die = True

    # Uncomment to turn on some memory profiling.
    # from meliae import scanner
    # print("Dumping")
    # scanner.dump_all_objects('objects.json')
    # print("Dump finished")

    finished = False
    while not finished:
        for actor in actors:
            actor.join()
        else:
            finished = True

    print("Test finished", file=sys.stderr)

    great_log = RequestLog()
    for actor in actors:
        great_log.merge(actor.log)

    great_log.print_stats()

if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = Submit
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 202 Bernard Blackham <bernard@largestprime.net>
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import mechanize
import optparse

from cms.db import Contest, SessionGen

from cmstestsuite.web.CWSRequests import \
    LoginRequest, SubmitRequest, TokenRequest


def harvest_contest_data(contest_id):
    """Retrieve the couples username, password and the task list for a
    given contest.

    contest_id (int): the id of the contest we want.
    return (tuple): the first element is a dictionary mapping
                    usernames to passwords; the second one is the list
                    of the task names.

    """
    users = {}
    tasks = []
    with SessionGen() as session:
        contest = Contest.get_from_id(contest_id, session)
        for user in contest.users:
            users[user.username] = {'password': user.password}
        for task in contest.tasks:
            tasks.append((task.id, task.name))
    return users, tasks


def submit_solution(username, password, task, files, base_url=None):
    def step(request):
        request.prepare()
        request.execute()

    browser = mechanize.Browser()
    browser.set_handle_robots(False)

    step(LoginRequest(browser, username, password, base_url=base_url))
    step(SubmitRequest(browser, task, base_url=base_url, filename=files[0]))


def release_test(username, password, task, submission_num, base_url=None):
    def step(request):
        request.prepare()
        request.execute()

    browser = mechanize.Browser()
    browser.set_handle_robots(False)

    step(LoginRequest(browser, username, password, base_url=base_url))
    step(TokenRequest(
        browser, task, base_url=base_url, submission_num=submission_num))


def main():
    parser = optparse.OptionParser(
        usage="usage: %prog [options] <user name> <task name> <files>")
    parser.add_option("-c", "--contest",
                      help="contest ID to export", dest="contest_id",
                      action="store", type="int", default=None)
    parser.add_option("-u", "--base-url",
                      help="base URL for placing HTTP requests",
                      action="store", default=None, dest="base_url")
    options, args = parser.parse_args()

    if len(args) < 3:
        parser.error("Not enough arguments.")
    username = args[0]
    taskname = args[1]
    files = args[2:]

    users, tasks = harvest_contest_data(options.contest_id)
    if username not in users:
        parser.error("User '%s' unknown." % username)

    task = [(tid, tname) for tid, tname in tasks if tname == taskname]
    if len(task) != 1:
        parser.error("Task '%s' does not identify a unique task." % taskname)
    task = task[0]

    password = users[username]['password']

    submit_solution(username, password, task, files, base_url=options.base_url)


if __name__ == '__main__':
    main()

########NEW FILE########
__FILENAME__ = Test
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Bernard Blackham <bernard@largestprime.net>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import os
import re

from cmstestsuite import cws_submit, get_evaluation_result


class TestFailure(Exception):
    pass


class Check(object):
    def check(self, *args, **kwargs):
        raise NotImplementedError


class CheckOverallScore(Check):
    # This check searches for a string such :
    #   Evaluated (100.0 / 100.0)
    # in status and checks the score.

    score_re = re.compile(r'^Evaluated \(([0-9.]+) / ([0-9/.]+)\)')

    def __init__(self, expected_score, expected_total):
        self.expected_score = expected_score
        self.expected_total = expected_total

    def check(self, result_info):
        g = CheckOverallScore.score_re.match(result_info['status'])
        if not g:
            raise TestFailure("Expected total score, got status: %s" %
                              result_info['status'])

        score, total = g.groups()
        try:
            score = float(score)
            total = float(total)
        except ValueError:
            raise TestFailure("Expected readable score, got: %s/%s" %
                              (score, total))

        if score != self.expected_score or \
                total != self.expected_total:
            raise TestFailure("Expected score of %g/%g, but got %g/%g" %
                              (self.expected_score, self.expected_total,
                               score, total))


class CheckCompilationFail(Check):
    def check(self, result_info):
        if 'Compilation failed' not in result_info['status']:
            raise TestFailure("Expected compilation to fail, got: %s" %
                              result_info['status'])


class CheckAbstractEvaluationFailure(Check):
    def __init__(self, short_adjective, failure_string):
        self.short_adjective = short_adjective
        self.failure_string = failure_string

    def check(self, result_info):
        if 'Evaluated' not in result_info['status']:
            raise TestFailure("Expected a successful evaluation, got: %s" %
                              result_info['status'])
        if not result_info['evaluations']:
            raise TestFailure("No evaluations found.")
        for evaluation in result_info['evaluations']:
            score = float(evaluation['outcome'])
            text = evaluation['text']
            if score != 0.0:
                raise TestFailure("Should have %s. Scored %g." %
                                  (self.short_adjective, score))
            if self.failure_string not in text:
                raise TestFailure("Should have %s, got %s" %
                                  (self.short_adjective, text))


class CheckTimeout(CheckAbstractEvaluationFailure):
    def __init__(self):
        CheckAbstractEvaluationFailure.__init__(
            self, "timed out", "Execution timed out")


class CheckForbiddenSyscall(CheckAbstractEvaluationFailure):
    def __init__(self, syscall_name=''):
        CheckAbstractEvaluationFailure.__init__(
            self, "executed a forbidden syscall",
            "Execution killed because of forbidden syscall %s" % syscall_name)


class CheckSignal(CheckAbstractEvaluationFailure):
    def __init__(self, signal_number):
        CheckAbstractEvaluationFailure.__init__(
            self, "died on a signal",
            "Execution killed with signal %d" % signal_number)


class CheckNonzeroReturn(CheckAbstractEvaluationFailure):
    def __init__(self):
        CheckAbstractEvaluationFailure.__init__(
            self, "nonzero return",
            "Execution failed because the return code was nonzero")


class Test(object):
    def __init__(self, name, task, filename, languages, checks):
        self.name = name
        self.task_module = task
        self.filename = filename
        self.languages = languages
        self.checks = checks

    def run(self, contest_id, task_id, user_id, language):
        # Source files are stored under cmstestsuite/code/.
        path = os.path.join(os.path.dirname(__file__), 'code')

        # Choose the correct file to submit.
        filename = self.filename.replace("%l", language)

        full_path = os.path.join(path, filename)

        # Submit our code.
        submission_id = cws_submit(contest_id, task_id, user_id,
                                   full_path, language)

        # Wait for evaluation to complete.
        result_info = get_evaluation_result(contest_id, submission_id)

        # Run checks.
        for check in self.checks:
            try:
                check.check(result_info)
            except TestFailure:
                # Our caller can deal with these.
                raise

########NEW FILE########
__FILENAME__ = TestCleanCheckout
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Bernard Blackham <bernard@largestprime.net>
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import atexit
import tempfile
import subprocess
import shutil
import os
from argparse import ArgumentParser

from cmstestsuite import info, sh, configure_cms, read_cms_config, CONFIG

# These settings are only used within this file.
CONFIG.update({
    "DB_HOST": "localhost",
    "DB_USER": "cmsuser",
    "DB_PASSWORD": "cmsuser",
    "DB_NAME": "cmstestdb",
    "GIT_ORIGIN": None,
    "GIT_REVISION": None,
})


def drop_old_data():
    info("Dropping any old databases called %(DB_NAME)s." % CONFIG)
    sh("sudo -u postgres dropdb %(DB_NAME)s" % CONFIG, ignore_failure=True)

    info("Purging old checkout from %(TEST_DIR)s." % CONFIG)
    shutil.rmtree("%(TEST_DIR)s" % CONFIG)


def setup_cms():
    info("Creating database called %(DB_NAME)s accessible by %(DB_USER)s." %
         CONFIG)
    sh("sudo -u postgres createdb %(DB_NAME)s -O %(DB_USER)s" % CONFIG)

    info("Checking out code.")
    sh(["git", "clone", CONFIG["GIT_ORIGIN"], CONFIG["TEST_DIR"]])
    os.chdir("%(TEST_DIR)s" % CONFIG)
    sh(["git", "checkout", CONFIG["GIT_REVISION"]])

    info("Configuring CMS.")
    configure_cms(
        {"database": '"postgresql+psycopg2://'
         '%(DB_USER)s:%(DB_PASSWORD)s@'
         '%(DB_HOST)s/%(DB_NAME)s"' % CONFIG,
         "keep_sandbox": "false",
         "contest_listen_address": '["127.0.0.1"]',
         "admin_listen_address": '"127.0.0.1"',
         "min_submission_interval": '0',
         })

    info("Setting environment.")
    os.environ["PYTHONPATH"] = "%(TEST_DIR)s" % CONFIG

    info("Building cms.")
    sh("./setup.py build")
    # Add permission bits to isolate.
    sh("sudo chown root:root isolate/isolate")
    sh("sudo chmod 4755 isolate/isolate")

    # Ensure our logs get preserved. Point them into the checkout instead of
    # the tempdir that we blow away.
    sh(["mkdir", "-p", "%(GIT_ORIGIN)s/log" % CONFIG])
    sh(["ln", "-s", "%(GIT_ORIGIN)s/log" % CONFIG, "log"])

    info("Creating tables.")
    sh("python scripts/cmsInitDB")


if __name__ == "__main__":
    parser = ArgumentParser(
        description="This utility tests a clean checkout of CMS.")
    parser.add_argument(
        "-r", "--revision",
        type=str, default=None, action="store",
        help="Test a specific git revision.")
    parser.add_argument(
        "-k", "--keep-working",
        default=False, action="store_true",
        help="Do not delete the working directory.")
    parser.add_argument(
        "arguments", nargs="*",
        help="All remaining arguments are passed to the test script.")
    args = parser.parse_args()

    CONFIG["TEST_DIR"] = tempfile.mkdtemp()
    CONFIG["CONFIG_PATH"] = "%s/examples/cms.conf" % CONFIG["TEST_DIR"]
    CONFIG["GIT_ORIGIN"] = subprocess.check_output(
        "git rev-parse --show-toplevel", shell=True).strip()
    if args.revision is None:
        CONFIG["GIT_REVISION"] = \
            subprocess.check_output("git rev-parse HEAD", shell=True).strip()
    else:
        CONFIG["GIT_REVISION"] = args.revision

    if not args.keep_working:
        def _cleanup():
            try:
                # Clean up tree.
                info("Cleaning up test directory %(TEST_DIR)s" % CONFIG)
                shutil.rmtree("%(TEST_DIR)s" % CONFIG)
            except:
                pass
        atexit.register(_cleanup)

    info("Testing `%(GIT_REVISION)s' in %(TEST_DIR)s" % CONFIG)

    reinitialize_everything = True

    if reinitialize_everything:
        drop_old_data()
        setup_cms()
    else:
        os.chdir("%(TEST_DIR)s" % CONFIG)
        os.environ["PYTHONPATH"] = "%(TEST_DIR)s" % CONFIG
        read_cms_config()

    # Now run the tests from the checkout.
    sh(["./cmstestsuite/RunTests.py"] + args.arguments)

    # We export the contest, import it again and re-run the tests on the
    # existing contest. Hard-coded contest indicies should be correct, as we
    # own the database.
    sh(["./cmscontrib/ContestExporter.py", "-c", "1"])
    sh(["./cmscontrib/ContestImporter.py", "dump_testcontest1.tar.gz"])
    sh(["./cmstestsuite/RunTests.py", "-c", "2"] + args.arguments)

    # Export coverage results.
    sh("python -m coverage xml --include 'cms*'")
    shutil.copyfile("coverage.xml", "%(GIT_ORIGIN)s/coverage.xml" % CONFIG)

########NEW FILE########
__FILENAME__ = Tests
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Bernard Blackham <bernard@largestprime.net>
# Copyright © 2013-2014 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2014 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import cmstestsuite.tasks.batch_stdio as batch_stdio
import cmstestsuite.tasks.batch_fileio as batch_fileio
import cmstestsuite.tasks.batch_fileio_managed as batch_fileio_managed
import cmstestsuite.tasks.communication as communication

from cms import LANGUAGES, LANG_C, LANG_CPP, LANG_PASCAL, LANG_JAVA, \
    LANG_PYTHON
from cmstestsuite.Test import Test, CheckOverallScore, CheckCompilationFail, \
    CheckTimeout, CheckNonzeroReturn


ALL_LANGUAGES = tuple(LANGUAGES)
NON_INTERPRETED_LANGUAGES = (LANG_C, LANG_CPP, LANG_PASCAL)
COMPILED_LANGUAGES = (LANG_C, LANG_CPP, LANG_PASCAL, LANG_JAVA, LANG_PYTHON)

ALL_TESTS = [

    # Correct solutions to batch tasks.

    Test('correct-stdio',
         task=batch_stdio, filename='correct-stdio.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(100, 100)]),

    Test('correct-freopen',
         task=batch_fileio, filename='correct-freopen.%l',
         languages=(LANG_C,),
         checks=[CheckOverallScore(100, 100)]),

    Test('correct-fileio',
         task=batch_fileio, filename='correct-fileio.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(100, 100)]),

    # Various incorrect solutions to batch tasks.

    Test('incorrect-stdio',
         task=batch_stdio, filename='incorrect-stdio.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(0, 100)]),

    Test('half-correct-stdio',
         task=batch_stdio, filename='half-correct-stdio.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(50, 100)]),

    Test('incorrect-fileio',
         task=batch_fileio, filename='incorrect-fileio.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(0, 100)]),

    Test('half-correct-fileio',
         task=batch_fileio, filename='half-correct-fileio.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(50, 100)]),

    Test('incorrect-fileio-nooutput',
         task=batch_fileio, filename='incorrect-fileio-nooutput.%l',
         languages=(LANG_C,),
         checks=[CheckOverallScore(0, 100)]),

    Test('incorrect-fileio-emptyoutput',
         task=batch_fileio, filename='incorrect-fileio-emptyoutput.%l',
         languages=(LANG_C,),
         checks=[CheckOverallScore(0, 100)]),

    Test('incorrect-fileio-with-stdio',
         task=batch_fileio, filename='incorrect-fileio-with-stdio.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(0, 100)]),

    # Failed compilation.

    Test('compile-fail',
         task=batch_fileio, filename='compile-fail.%l',
         languages=COMPILED_LANGUAGES,
         checks=[CheckCompilationFail()]),

    # Various timeout conditions.

    Test('timeout-cputime',
         task=batch_stdio, filename='timeout-cputime.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(0, 100), CheckTimeout()]),

    Test('timeout-pause',
         task=batch_stdio, filename='timeout-pause.%l',
         languages=(LANG_CPP,),
         checks=[CheckOverallScore(0, 100), CheckTimeout()]),

    Test('timeout-sleep',
         task=batch_stdio, filename='timeout-sleep.%l',
         languages=(LANG_CPP,),
         checks=[CheckOverallScore(0, 100), CheckTimeout()]),

    Test('timeout-sigstop',
         task=batch_stdio, filename='timeout-sigstop.%l',
         languages=(LANG_CPP,),
         checks=[CheckOverallScore(0, 100), CheckTimeout()]),

    Test('timeout-select',
         task=batch_stdio, filename='timeout-select.%l',
         languages=(LANG_CPP,),
         checks=[CheckOverallScore(0, 100), CheckTimeout()]),

    # Nonzero return status.

    Test('nonzero-return-stdio',
         task=batch_stdio, filename='nonzero-return-stdio.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(0, 100), CheckNonzeroReturn()]),

    Test('nonzero-return-fileio',
         task=batch_fileio, filename='nonzero-return-fileio.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(0, 100), CheckNonzeroReturn()]),

    # Fork

    # We can't really check for a specific error, because forking
    # doesn't cause an exceptional stop: it just returns -1 to the
    # caller; we rely on the fact that the test program is designed to
    # produce output only inside the child process

    Test('fork',
         task=batch_stdio, filename='fork.%l',
         languages=(LANG_C, LANG_CPP),
         checks=[CheckOverallScore(0, 100)]),

    # OOM problems.

    Test('oom-static',
         task=batch_stdio, filename='oom-static.%l',
         languages=NON_INTERPRETED_LANGUAGES,
         checks=[CheckOverallScore(0, 100)]),

    Test('oom-heap',
         task=batch_stdio, filename='oom-heap.%l',
         languages=ALL_LANGUAGES,
         checks=[CheckOverallScore(0, 100)]),

    # Tasks with graders. Python and PHP are not yet supported.

    Test('managed-correct',
         task=batch_fileio_managed, filename='managed-correct.%l',
         languages=(LANG_C, LANG_CPP, LANG_PASCAL, LANG_JAVA),
         checks=[CheckOverallScore(100, 100)]),

    Test('managed-incorrect',
         task=batch_fileio_managed, filename='managed-incorrect.%l',
         languages=(LANG_C, LANG_CPP, LANG_PASCAL, LANG_JAVA),
         checks=[CheckOverallScore(0, 100)]),

    # Communication tasks. Python and PHP are not yet supported.

    Test('communication-correct',
         task=communication, filename='communication-correct.%l',
         languages=(LANG_C, LANG_CPP, LANG_PASCAL, LANG_JAVA),
         checks=[CheckOverallScore(100, 100)]),

    Test('communication-incorrect',
         task=communication, filename='communication-incorrect.%l',
         languages=(LANG_C, LANG_CPP, LANG_PASCAL, LANG_JAVA),
         checks=[CheckOverallScore(0, 100)]),

]

########NEW FILE########
__FILENAME__ = TestService
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""This file defines a class built on top of Service in order to
provide facilities to build a service that does test a production
service.

"""

from __future__ import absolute_import
from __future__ import print_function

import logging
import time

from cms.io import Service


logger = logging.getLogger(__name__)


class TestService(Service):
    """Runs automatically a suite of tests defined on the subclass.

    """
    def __init__(self, shard):
        Service.__init__(self, shard)

        self.start = 0
        self.total_time = 0
        self.allright = 0
        self.current = -1
        self.ongoing = False
        self.failed = False
        self.retry = False
        self.add_timeout(self.test, None, 0.2, immediately=True)
        self.initialized = False

    def test(self):
        """Runs the test suite.

        """
        # Call the prepare method, but just the first time
        if not self.initialized:
            self.initialized = True
            try:
                prepare_method = self.prepare
            except AttributeError:
                pass
            else:
                prepare_method()

        if self.ongoing:
            return True
        elif self.current >= 0 and not self.failed and not self.retry:
            self.total_time += self.delta
            logger.info("Test #%03d performed in %.2lf seconds." %
                        (self.current, self.delta))

        if not self.retry:
            self.current += 1
        else:
            self.retry = False
        if self.allright != self.current:
            self.failed = True

        try:
            method = getattr(self, "test_%03d" % self.current)
        except AttributeError:
            total = self.current
            if total == 0:
                logger.info("Test suite completed.")
                return False
            else:
                logger.info(("Test suite completed in %.2f seconds. " +
                             "Result: %d/%d (%.2f%%).") %
                            (self.total_time, self.allright, total,
                             self.allright * 100.0 / total))
                self.exit()
                return False

        if not self.failed:
            self.ongoing = True
            logger.info("Performing Test #%03d..." % self.current)
            self.start = time.time()
            method()
        else:
            logger.info("Not performing Test #%03d." % self.current)
        return True

    def test_end(self, success, message=None, retry=False):
        """This method is to be called when finishing a test.

        success (bool): True if the test was successful.
        message (string): optional message to log.
        retry (bool): if success is False, and retry is True, we try
                      again the same test.

        """
        if message is not None:
            if success:
                logger.info("  " + message)
            else:
                logger.error("  " + message)
        self.ongoing = False
        if success:
            self.allright += 1
        else:
            self.retry = retry
        self.delta = time.time() - self.start

########NEW FILE########
__FILENAME__ = filecacher_test
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2014 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Testing suite for FileCacher

"""

from __future__ import absolute_import
from __future__ import print_function

import os
import random
from StringIO import StringIO
import hashlib
import shutil
import unittest

from cms.db.filecacher import FileCacher


class RandomFile(object):
    """Simulate a random file with dim bytes, calculating its
    SHA1 hash.

    """
    def __init__(self, dim):
        self.dim = dim
        self.source = open('/dev/urandom')
        self.hasher = hashlib.sha1()

    def read(self, byte_num):
        """Read byte_num bytes from the source and return them,
        updating the hashing.

        byte_num (int): number of bytes to read.

        return (string): byte_num bytes of content.

        """
        if byte_num > self.dim:
            byte_num = self.dim
        if byte_num == 0:
            return ''
        buf = self.source.read(byte_num)
        self.dim -= len(buf)
        self.hasher.update(buf)
        return buf

    def close(self):
        """Close the source file.

        """
        self.source.close()

    @property
    def digest(self):
        """Digest of the data read from the source file.

        return (string): digest.

        """
        return self.hasher.hexdigest()


class HashingFile(object):
    """Hashes the content written to this files.

    """
    def __init__(self):
        self.hasher = hashlib.sha1()

    def write(self, buf):
        """Update the hashing with the content of buf.

        buf (string): new content for the file.

        return (int): length of buf.

        """
        self.hasher.update(buf)
        return len(buf)

    @property
    def digest(self):
        """Digest of the data written in the file.

        return (string): digest.

        """
        return self.hasher.hexdigest()

    def close(self):
        """Do nothing, because there is no hidden file we are writing
        to.

        """
        pass


class TestFileCacher(unittest.TestCase):
    """Service that performs automatically some tests for the
    FileCacher service.

    """

    def setUp(self):
        self.file_cacher = FileCacher()
        #self.file_cacher = FileCacher(self, path="fs-storage")
        self.cache_base_path = self.file_cacher.file_dir
        self.cache_path = None
        self.content = None
        self.fake_content = None
        self.digest = None
        self.file_obj = None

    def tearDown(self):
        shutil.rmtree(self.cache_base_path, ignore_errors=True)

    def test_file_life(self):
        """Send a ~100B random binary file to the storage through
        FileCacher as a file-like object. FC should cache the content
        locally.

        Then retrieve it.

        Then check its size.

        Then get it back.

        Then delete it.

        """
        self.size = 100
        self.content = "".join(chr(random.randint(0, 255))
                               for unused_i in xrange(self.size))

        data = self.file_cacher.put_file_from_fobj(StringIO(self.content),
                                                   u"Test #000")

        if not os.path.exists(os.path.join(self.cache_base_path, data)):
            self.fail("File not stored in local cache.")
        elif open(os.path.join(self.cache_base_path, data), "rb").read() != \
                self.content:
            self.fail("Local cache's content differ "
                      "from original file.")
        else:
            self.cache_path = os.path.join(self.cache_base_path, data)
            self.digest = data

        # Retrieve the file.
        self.fake_content = "Fake content.\n"
        with open(self.cache_path, "wb") as cached_file:
            cached_file.write(self.fake_content)
        try:
            data = self.file_cacher.get_file(self.digest)
        except Exception as error:
            self.fail("Error received: %r." % error)
            return

        received = data.read()
        data.close()
        if received != self.fake_content:
            if received == self.content:
                self.fail("Did not use the cache even if it could.")
            else:
                self.fail("Content differ.")

        # Check the size of the file.
        try:
            size = self.file_cacher.get_size(self.digest)
        except Exception as error:
            self.fail("Error received: %r." % error)
            return

        if size != self.size:
            self.fail("The size is wrong: %d instead of %d" %
                      (size, self.size))

        # Get file from FileCacher.
        os.unlink(self.cache_path)
        try:
            data = self.file_cacher.get_file(self.digest)
        except Exception as error:
            self.fail("Error received: %r." % error)
            return

        received = data.read()
        data.close()
        if received != self.content:
            self.fail("Content differ.")
        elif not os.path.exists(self.cache_path):
            self.fail("File not stored in local cache.")
        elif open(self.cache_path).read() != self.content:
            self.fail("Local cache's content differ " +
                      "from original file.")

        # Delete the file through FS and tries to get it again through
        # FC.
        try:
            self.file_cacher.delete(digest=self.digest)
        except Exception as error:
            self.fail("Error received: %s." % error)
            return

        else:
            with self.assertRaises(Exception):
                self.file_cacher.get_file(self.digest)

    def test_fetch_missing_file(self):
        """Get unexisting file from FileCacher.

        """
        with self.assertRaises(Exception):
            self.file_cacher.get_file(self.digest)

    def test_file_as_content(self):
        """Send a ~100B random binary file to the storage through
        FileCacher as a string. FC should cache the content locally.

        Then retrieve it as a string.

        """
        self.content = "".join(chr(random.randint(0, 255))
                               for unused_i in xrange(100))

        try:
            data = self.file_cacher.put_file_content(self.content,
                                                     u"Test #005")
        except Exception as error:
            self.fail("Error received: %r." % error)
            return

        if not os.path.exists(os.path.join(self.cache_base_path, data)):
            self.fail("File not stored in local cache.")
        elif open(os.path.join(self.cache_base_path, data),
                  "rb").read() != self.content:
            self.fail("Local cache's content differ "
                      "from original file.")
        else:
            self.cache_path = os.path.join(self.cache_base_path, data)
            self.digest = data

        # Retrieve the file as a string.
        self.fake_content = "Fake content.\n"
        with open(self.cache_path, "wb") as cached_file:
            cached_file.write(self.fake_content)
        try:
            data = self.file_cacher.get_file_content(self.digest)
        except Exception as error:
            self.fail("Error received: %r." % error)
            return

        if data != self.fake_content:
            if data == self.content:
                self.fail("Did not use the cache even if it could.")
            else:
                self.fail("Content differ.")

    def test_big_file(self):
        """Put a ~10MB file into the storage (using a specially
        crafted file-like object).

        Then get it back.

        """
        rand_file = RandomFile(10000000)
        try:
            data = self.file_cacher.put_file_from_fobj(rand_file, u"Test #007")
        except Exception as error:
            self.fail("Error received: %r." % error)
            return
        if rand_file.dim != 0:
            self.fail("The input file wasn't read completely.")
        my_digest = rand_file.digest
        rand_file.close()

        if not os.path.exists(os.path.join(self.cache_base_path, data)):
            self.fail("File not stored in local cache.")
        elif my_digest != data:
            self.fail("File received with wrong hash.")
        else:
            self.cache_path = os.path.join(self.cache_base_path, data)
            self.digest = data

        # Get the ~100MB file from FileCacher.
        os.unlink(self.cache_path)
        hash_file = HashingFile()
        try:
            self.file_cacher.get_file_to_fobj(self.digest, hash_file)
        except Exception as error:
            self.fail("Error received: %r." % error)
            return
        my_digest = hash_file.digest
        hash_file.close()

        try:
            if self.digest != my_digest:
                self.fail("Content differs.")
            elif not os.path.exists(self.cache_path):
                self.fail("File not stored in local cache.")
        finally:
            self.file_cacher.delete(self.digest)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = SandboxTest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2014 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2014 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Tests for general utility functions."""

from __future__ import absolute_import
from __future__ import print_function

import unittest
import io

from cms.grading.Sandbox import Truncator


class TestTruncator(unittest.TestCase):
    """Test the class Truncator."""
    def setUp(self):
        pass

    def tearDown(self):
        pass

    def perform_truncator_test(self, orig_len, trunc_len, read_chunk_size):
        back_file = io.BytesIO('a' * orig_len)
        truncator = Truncator(back_file, trunc_len)
        buf = truncator.read(read_chunk_size)
        read_len = 0
        while buf != '':
            read_len += len(buf)
            buf = truncator.read(read_chunk_size)
        self.assertEqual(read_len, min(trunc_len, orig_len))

    def test_long_file(self):
        """Read a file longer than requested truncation length.

        """
        self.perform_truncator_test(100, 40, 1000)

    def test_short_file(self):
        """Read a file shorter than requested truncation length.

        """
        self.perform_truncator_test(100, 400, 1000)

    def test_chunked_file(self):
        """Read a file in little chunks.

        """
        self.perform_truncator_test(100, 40, 7)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = rpc_test
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2014 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Tests for RPC system.

"""

from __future__ import absolute_import
from __future__ import print_function

import unittest

import gevent
import gevent.socket
import gevent.event
from gevent.server import StreamServer

from mock import Mock, patch

from cms import Address, ServiceCoord
from cms.io import RPCError, rpc_method, RemoteServiceServer, \
    RemoteServiceClient


class MockService(object):
    def not_rpc_callable(self):
        pass

    @rpc_method
    def raise_exception(self):
        raise RuntimeError()

    @rpc_method
    def return_unencodable(self):
        return RuntimeError()

    @rpc_method
    def echo(self, value):
        return value

    @rpc_method
    def echo_slow(self, value):
        gevent.sleep(0.01)
        return value

    @rpc_method
    def infinite(self):
        event = gevent.event.Event()
        event.wait()


class TestRPC(unittest.TestCase):

    def setUp(self):
        patcher = patch("cms.io.rpc.get_service_address")
        self.mock = patcher.start()
        self.addCleanup(patcher.stop)

        self.service = MockService()
        self.servers = list()
        self.clients = list()
        self.spawn_listener()

    def spawn_listener(self, host="127.0.0.1", port=0):
        """Start listening on the given host and port.

        Each incoming connection will cause a RemoteServiceServer to be
        instantiated (and therefore a greenlet to be spawned) and to be
        inserted in self.servers. The listening host and port will also
        be stored as self.host and self.port.

        host (string): the hostname or IP address
        port (int): the port (0 causes any available port to be chosen)

        """
        self._server = StreamServer((host, port), self.get_server)
        self._server.start()
        self.host = self._server.server_host
        self.port = self._server.server_port
        self.mock.return_value = Address(self.host, self.port)

    def kill_listener(self):
        """Stop listening."""
        self._server.stop()
        del self._server
        # We leave self.host and self.port.

    def get_server(self, socket_, address):
        """Obtain a new RemoteServiceServer to handle a new connection.

        Instantiate a RemoteServiceServer, spawn its greenlet and add
        it to self.servers. It will listen on the given socket, that
        represents a connection opened by a remote host at the given
        address.

        socket_ (socket): the socket to use
        address (tuple): the (ip address, port) of the remote part
        return (RemoteServiceServer): a server

        """
        server = RemoteServiceServer(self.service, address)
        server.handle(socket_)
        self.servers.append(server)
        return server

    def get_client(self, coord, auto_retry=None):
        """Obtain a new RemoteServiceClient to connect to a server.

        Instantiate a RemoteServiceClient, spawn its greenlet and add
        it to self.clients. It will try to connect to the service at
        the given coordinates.

        coord (ServiceCoord): the (name, shard) of the service
        auto_retry (float|None): how long to wait after a disconnection
            before trying to reconnect
        return (RemoteServiceClient): a client

        """
        client = RemoteServiceClient(coord, auto_retry)
        client.connect()
        self.clients.append(client)
        return client

    def disconnect_servers(self):
        """Disconnect all registered servers from their clients."""
        for server in self.servers:
            if server.connected:
                server.disconnect()

    def disconnect_clients(self):
        """Disconnect all registered clients from their servers."""
        for client in self.clients:
            if client.connected:
                client.disconnect()

    def tearDown(self):
        self.kill_listener()
        self.disconnect_clients()
        self.disconnect_servers()
        gevent.sleep(0.002)

    def test_method_not_existent(self):
        client = self.get_client(ServiceCoord("Foo", 0))
        result = client.not_existent()
        result.wait()
        self.assertFalse(result.successful())
        self.assertIsInstance(result.exception, RPCError)

    def test_method_not_rpc_callable(self):
        client = self.get_client(ServiceCoord("Foo", 0))
        result = client.not_rpc_callable()
        result.wait()
        self.assertFalse(result.successful())
        self.assertIsInstance(result.exception, RPCError)

    def test_method_raise_exception(self):
        client = self.get_client(ServiceCoord("Foo", 0))
        result = client.raise_exception()
        result.wait()
        self.assertFalse(result.successful())
        self.assertIsInstance(result.exception, RPCError)

    def test_method_send_unencodable(self):
        client = self.get_client(ServiceCoord("Foo", 0))
        result = client.echo(value=RuntimeError())
        result.wait()
        self.assertFalse(result.successful())
        self.assertIsInstance(result.exception, RPCError)

    # TODO Are we sure we want this to be the correct behavior? That
    # means that if the server (by mistake?) sends unencodable data
    # then the client will never get any data nor any error. It will
    # get stuck until it goes in timeout.
    def test_method_return_unencodable(self):
        client = self.get_client(ServiceCoord("Foo", 0))
        result = client.return_unencodable()
        result.wait(timeout=0.002)
        self.assertFalse(result.ready())

    def test_method_return_bool(self):
        client = self.get_client(ServiceCoord("Foo", 0))
        result = client.echo(value=True)
        result.wait()
        self.assertTrue(result.successful())
        self.assertIs(result.value, True)

    def test_method_return_int(self):
        client = self.get_client(ServiceCoord("Foo", 0))
        result = client.echo(value=42)
        result.wait()
        self.assertTrue(result.successful())
        self.assertEqual(result.value, 42)

    def test_method_return_string(self):
        client = self.get_client(ServiceCoord("Foo", 0))
        result = client.echo(value="Hello World")
        result.wait()
        self.assertTrue(result.successful())
        self.assertEqual(result.value, "Hello World")

    def test_method_return_list(self):
        client = self.get_client(ServiceCoord("Foo", 0))
        result = client.echo(value=["Hello", 42, "World"])
        result.wait()
        self.assertTrue(result.successful())
        self.assertEqual(result.value, ["Hello", 42, "World"])

    def test_autoreconnect1(self):
        client = self.get_client(ServiceCoord("Foo", 0), 0.002)
        gevent.sleep(0.002)
        self.assertTrue(client.connected)
        self.disconnect_servers()
        gevent.sleep(0.002)
        self.assertTrue(client.connected, "Autoreconnect didn't kick in "
                                          "after server disconnected")

    def test_autoreconnect2(self):
        client = self.get_client(ServiceCoord("Foo", 0), 0.002)
        gevent.sleep(0.002)
        self.assertTrue(client.connected)
        self.disconnect_servers()
        self.kill_listener()
        gevent.sleep(0.002)
        self.assertFalse(client.connected)
        self.spawn_listener(port=self.port)
        gevent.sleep(0.002)
        self.assertTrue(client.connected, "Autoreconnect didn't kick in "
                                          "after server came back online")

    def test_autoreconnect3(self):
        client = self.get_client(ServiceCoord("Foo", 0), 0.002)
        gevent.sleep(0.002)
        self.assertTrue(client.connected)
        self.disconnect_clients()
        gevent.sleep(0.002)
        self.assertFalse(client.connected, "Autoreconnect still active "
                                           "after explicit disconnection")

    def test_concurrency(self):
        client = self.get_client(ServiceCoord("Foo", 0))
        result1 = client.infinite()
        gevent.sleep(0.002)
        result2 = client.echo(value=True)
        gevent.sleep(0.002)
        self.assertTrue(result2.successful())
        self.assertIs(result2.value, True)
        self.assertFalse(result1.ready())
        self.disconnect_clients()
        self.assertTrue(result1.ready())
        self.assertFalse(result1.successful())
        self.assertIsInstance(result1.exception, RPCError)

    def test_callbacks(self):
        coord = ServiceCoord("Foo", 0)
        client = self.get_client(coord)
        gevent.sleep(0.002)
        client.disconnect()
        on_connect_handler = Mock()
        client.add_on_connect_handler(on_connect_handler)
        on_disconnect_handler = Mock()
        client.add_on_disconnect_handler(on_disconnect_handler)

        client.connect()
        gevent.sleep(0.002)
        self.assertTrue(client.connected)
        on_connect_handler.assert_called_once_with(coord)
        on_connect_handler.reset_mock()
        self.assertFalse(on_disconnect_handler.called)

        client.disconnect()
        gevent.sleep(0.002)
        self.assertFalse(client.connected)
        self.assertFalse(on_connect_handler.called)
        on_disconnect_handler.assert_called_once_with()
        on_disconnect_handler.reset_mock()

        client.connect()
        gevent.sleep(0.002)
        self.assertTrue(client.connected)
        on_connect_handler.assert_called_once_with(coord)
        on_connect_handler.reset_mock()
        self.assertFalse(on_disconnect_handler.called)

        self.disconnect_servers()
        gevent.sleep(0.1)
        self.assertFalse(client.connected)
        self.assertFalse(on_connect_handler.called)
        on_disconnect_handler.assert_called_once_with()
        on_disconnect_handler.reset_mock()

    def test_reusability(self):
        client = self.get_client(ServiceCoord("Foo", 0))

        on_connect_handler = Mock()
        client.add_on_connect_handler(on_connect_handler)
        on_disconnect_handler = Mock()
        client.add_on_disconnect_handler(on_disconnect_handler)

        for i in range(10):
            self.assertTrue(client.connected)
            result = client.echo(value=42)
            result.wait()
            self.assertTrue(result.successful())
            self.assertEqual(result.value, 42)
            self.assertTrue(client.connected)

            gevent.sleep(0.002)
            client.disconnect()
            self.assertFalse(client.connected)
            gevent.sleep(0.002)
            client.connect()
            self.assertTrue(client.connected)

        gevent.sleep(0.002)

        self.assertEqual(on_connect_handler.call_count, 10)
        self.assertEqual(on_disconnect_handler.call_count, 10)

    def test_double_connect_client(self):
        # Check that asking an already-connected client to connect
        # again causes an error.
        client = self.get_client(ServiceCoord("Foo", 0))
        self.assertRaises(Exception, client.connect)

    def test_double_connect_server(self):
        # Check that asking an already-connected server to initialize
        # again its connection causes an error.
        client = self.get_client(ServiceCoord("Foo", 0))
        gevent.sleep(0.002)
        self.assertRaises(Exception, self.servers[0].initialize, "foo")

    def test_double_disconnect_client(self):
        # Check that asking a non-connected client to disconnect is
        # harmless (i.e. disconnection is idempotent).
        client = self.get_client(ServiceCoord("Foo", 0))
        client.disconnect()
        gevent.sleep(0.002)
        client.disconnect()
        gevent.sleep(0.002)

    def test_double_disconnect_server(self):
        # Check that asking a non-connected server to disconnect is
        # harmless (i.e. disconnection is idempotent).
        client = self.get_client(ServiceCoord("Foo", 0))
        gevent.sleep(0.002)
        self.servers[0].disconnect()
        gevent.sleep(0.002)
        self.servers[0].disconnect()
        gevent.sleep(0.002)

    def test_send_invalid_json(self):
        sock = gevent.socket.create_connection((self.host, self.port))
        sock.sendall("foo\r\n")
        gevent.sleep(0.002)
        self.assertTrue(self.servers[0].connected)
        # Verify the server resumes normal operation.
        self.test_method_return_int()

    def test_send_incomplete_json(self):
        sock = gevent.socket.create_connection((self.host, self.port))
        sock.sendall('{"__id": "foo"}\r\n')
        gevent.sleep(0.002)
        self.assertTrue(self.servers[0].connected)
        # Verify the server resumes normal operation.
        self.test_method_return_int()


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = LogServiceTest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Tests for the logger service.

"""

from __future__ import absolute_import
from __future__ import print_function

import logging
import unittest

from cms.service.LogService import LogService


class TestLogService(unittest.TestCase):

    MSG = "Random message"
    SERVICE_NAME = "RandomService"
    SERVICE_SHARD = 0
    OPERATION = "Random operation"
    CREATED = 1234567890.123
    EXC_TEXT = "Random exception"

    def setUp(self):
        self.service = LogService(0)

    def test_last_messages(self):
        for severity in ["CRITICAL",
                         "ERROR",
                         "WARNING"]:
            self.helper_test_last_messages(severity)
        for severity in ["INFO",
                         "DEBUG"]:
            self.helper_test_last_messages(severity, saved=False)

    def helper_test_last_messages(self, severity, saved=True):
        self.service.Log(
            msg=TestLogService.MSG + severity,
            levelname=severity,
            levelno=getattr(logging, severity),
            created=TestLogService.CREATED,
            service_name=TestLogService.SERVICE_NAME + severity,
            service_shard=TestLogService.SERVICE_SHARD,
            operation=TestLogService.OPERATION + severity,
            exc_text=TestLogService.EXC_TEXT + severity)
        last_message = self.service.last_messages()[-1]
        if saved:
            self.assertEquals(last_message["message"],
                              TestLogService.MSG + severity)
            self.assertEquals(last_message["coord"],
                              TestLogService.SERVICE_NAME + severity +
                              "," + str(TestLogService.SERVICE_SHARD))
            self.assertEquals(last_message["operation"],
                              TestLogService.OPERATION + severity)
            self.assertEquals(last_message["severity"],
                              severity)
            self.assertEquals(last_message["timestamp"],
                              TestLogService.CREATED)
            self.assertEquals(last_message["exc_text"],
                              TestLogService.EXC_TEXT + severity)
            pass
        else:
            self.assertNotEquals(last_message["severity"], severity)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = ResourceServiceTest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2014 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Tests for ResourceService.

"""

from __future__ import absolute_import
from __future__ import print_function

import unittest

from cms import ServiceCoord
from cms.service.ResourceService import ResourceService


class TestResourceService(unittest.TestCase):

    def setUp(self):
        pass

    def test_is_service_proc(self):
        """Several tests for identifying the command line of a service.

        """
        service = ServiceCoord("Worker", 0)
        good_command_lines = [
            "/usr/bin/python2 cmsWorker 0",
            "/usr/bin/python2 cmsWorker",
            "python2 cmsWorker 0 -c 1",
            "python2 cmsWorker -c 1",
            "python2 cmsWorker -c 1 0",
            "/usr/bin/env python2 cmsWorker 0",
            "/usr/bin/env python2 cmsWorker",
            "/usr/bin/env python2 cmsWorker 0 -c 1",
            "/usr/bin/env python2 cmsWorker -c 1",
            "/usr/bin/env python2 cmsWorker -c 1 0",
            ]
        bad_command_lines = [
            "ps",
            "less cmsWorker 0",
            "less /usr/bin/python2 cmsWorker 0",
            "/usr/bin/python2 cmsWorker 1",
            "/usr/bin/python2 cmsAdminWebServer 0",
            ]
        for cmdline in good_command_lines:
            self.assertTrue(ResourceService._is_service_proc(
                service, cmdline.split(" ")), cmdline)
        for cmdline in bad_command_lines:
            self.assertFalse(ResourceService._is_service_proc(
                service, cmdline.split(" ")), cmdline)

        # Test we do not pick the wrong shard.
        service = ServiceCoord("Worker", 1)
        cmdline = "/usr/bin/python2 cmsWorker"
        self.assertFalse(ResourceService._is_service_proc(
            service, cmdline.split(" ")), cmdline)

        # Test that an empty command line does not cause problems.
        self.assertFalse(ResourceService._is_service_proc(
            service, []), "Empty command line.")

        # Simulate a service not running on the same machine.
        service = ServiceCoord("FakeWorker", 0)
        cmdline = "/usr/bin/python2 cmsFakeWorker 0"
        self.assertFalse(ResourceService._is_service_proc(
            service, cmdline.split(" ")), cmdline)

if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = ScoringServiceTest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Tests for the scoring service.

"""

from __future__ import absolute_import
from __future__ import print_function

# We enable monkey patching to make many libraries gevent-friendly
# (for instance, urllib3, used by requests)
import gevent.monkey
gevent.monkey.patch_all()

import gevent
import random
import unittest
from mock import Mock, call

import cms.service.ScoringService
from cms.service.ScoringService import ScoringService


class TestScoringService(unittest.TestCase):

    def setUp(self):
        self.service = ScoringService(0)

    # Testing new_evaluation.

    def test_new_evaluation(self):
        """One submission is scored.

        """
        score_info = self.new_score_info()
        sr = TestScoringService.new_sr_to_score()
        score_type = Mock()
        score_type.compute_score.return_value = score_info
        TestScoringService.set_up_db([sr], score_type)

        self.service.new_evaluation(123, 456)

        gevent.sleep(0)  # Needed to trigger the score loop.
        # Asserts that compute_score was called.
        assert score_type.compute_score.mock_calls == [call(sr)]
        assert (sr.score,
                sr.score_details,
                sr.public_score,
                sr.public_score_details,
                sr.ranking_score_details) == score_info

    def test_new_evaluation_two(self):
        """More than one submissions in the queue.

        """
        score_type = Mock()
        score_type.compute_score.return_value = (1, "1", 2, "2", ["1", "2"])
        sr_a = TestScoringService.new_sr_to_score()
        sr_b = TestScoringService.new_sr_to_score()
        TestScoringService.set_up_db([sr_a, sr_b], score_type)

        self.service.new_evaluation(123, 456)
        self.service.new_evaluation(124, 456)

        gevent.sleep(0)  # Needed to trigger the score loop.
        # Asserts that compute_score was called.
        assert score_type.compute_score.mock_calls == [call(sr_a), call(sr_b)]

    def test_new_evaluation_already_scored(self):
        """One submission is not re-scored if already scored.

        """
        sr = TestScoringService.new_sr_scored()
        score_type = Mock()
        score_type.compute_score.return_value = (1, "1", 2, "2", ["1", "2"])
        TestScoringService.set_up_db([sr], score_type)

        self.service.new_evaluation(123, 456)

        gevent.sleep(0)  # Needed to trigger the score loop.
        # Asserts that compute_score was called.
        assert score_type.compute_score.mock_calls == []

    @staticmethod
    def new_sr_to_score():
        sr = Mock()
        sr.needs_scoring.return_value = True
        sr.scored.return_value = False
        return sr

    @staticmethod
    def new_sr_scored():
        sr = Mock()
        sr.needs_scoring.return_value = False
        sr.scored.return_value = True
        return sr

    @staticmethod
    def new_score_info():
        return (
            random.randint(1, 1000),
            str(random.randint(1, 1000)),
            random.randint(1, 1000),
            str(random.randint(1, 1000)),
            [str(random.randint(1, 1000)), str(random.randint(1, 1000))]
        )

    @staticmethod
    def set_up_db(srs, score_type):
        submission = Mock()
        submission.get_result = Mock(side_effect=srs)
        cms.service.ScoringService.Submission.get_from_id = \
            Mock(return_value=submission)
        cms.service.ScoringService.Dataset.get_from_id = \
            Mock(return_value=Mock())
        cms.service.ScoringService.get_score_type = \
            Mock(return_value=score_type)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = WorkerTest
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2013 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Tests for the worker.

"""

from __future__ import absolute_import
from __future__ import print_function

import gevent
import unittest
from mock import Mock, call

import cms.service.Worker
from cms.grading import JobException
from cms.grading.Job import EvaluationJob, JobGroup
from cms.service.Worker import Worker


class TestWorker(unittest.TestCase):

    def setUp(self):
        self.service = Worker(0)

    # Testing execute_job_group.

    def test_execute_job_group_success(self):
        """Executes a job group with three successful jobs.

        """
        jobgroup, calls = TestWorker.new_jobgroup(3)
        task_type = FakeTaskType([True, True, True])
        cms.service.Worker.get_task_type = Mock(return_value=task_type)

        JobGroup.import_from_dict(
            self.service.execute_job_group(jobgroup.export_to_dict()))

        cms.service.Worker.get_task_type.assert_has_calls(
            calls, any_order=True)
        self.assertEquals(task_type.call_count, 3)

    def test_execute_job_group_jobs_failure(self):
        """Executes a job group with three unsuccessful jobs.

        """
        jobgroup, unused_calls = TestWorker.new_jobgroup(2)
        task_type = FakeTaskType([False, False])
        cms.service.Worker.get_task_type = Mock(return_value=task_type)

        new_group = JobGroup.import_from_dict(
            self.service.execute_job_group(jobgroup.export_to_dict()))

        self.assertFalse(new_group.success)
        # Does not continue after failure, so just one call.
        self.assertEquals(cms.service.Worker.get_task_type.call_count, 1)
        self.assertEquals(task_type.call_count, 1)

    def test_execute_job_group_tasktype_raise(self):
        """Executes a job group with three jobs raising exceptions.

        """
        jobgroup, unused_calls = TestWorker.new_jobgroup(2)
        task_type = FakeTaskType([Exception(), Exception()])
        cms.service.Worker.get_task_type = Mock(return_value=task_type)

        try:
            JobGroup.import_from_dict(
                self.service.execute_job_group(jobgroup.export_to_dict()))
        except JobException:
            # Expected
            pass
        else:
            self.fail("Expected JobException from the tasktype.")

        # Does not continue after failure so just one call.
        self.assertEquals(cms.service.Worker.get_task_type.call_count, 1)
        self.assertEquals(task_type.call_count, 1)

    def test_execute_job_group_subsequent_success(self):
        """Executes a job group with three successful jobs, then another one.

        """
        jobgroup_a, calls_a = TestWorker.new_jobgroup(3, prefix="a")
        task_type_a = FakeTaskType([True, True, True])
        cms.service.Worker.get_task_type = Mock(return_value=task_type_a)

        JobGroup.import_from_dict(
            self.service.execute_job_group(jobgroup_a.export_to_dict()))

        cms.service.Worker.get_task_type.assert_has_calls(
            calls_a, any_order=True)
        self.assertEquals(task_type_a.call_count, 3)

        jobgroup_b, calls_b = TestWorker.new_jobgroup(3, prefix="b")
        task_type_b = FakeTaskType([True, True, True])
        cms.service.Worker.get_task_type = Mock(return_value=task_type_b)

        JobGroup.import_from_dict(
            self.service.execute_job_group(jobgroup_b.export_to_dict()))

        cms.service.Worker.get_task_type.assert_has_calls(
            calls_b, any_order=True)
        self.assertEquals(task_type_b.call_count, 3)

    def test_execute_job_group_subsequent_locked(self):
        """Executes a job group with one long job, then another one
        that should fail because of the lock.

        """
        # Because of how gevent works, the interval here can be very small.
        task_type = FakeTaskType([0.01])
        cms.service.Worker.get_task_type = Mock(return_value=task_type)

        jobgroup_a, calls_a = TestWorker.new_jobgroup(1, prefix="a")
        jobgroup_b, calls_b = TestWorker.new_jobgroup(1, prefix="b")

        def first_call():
            JobGroup.import_from_dict(
                self.service.execute_job_group(jobgroup_a.export_to_dict()))

        first_greenlet = gevent.spawn(first_call)
        gevent.sleep(0)  # To ensure we call jobgroup_a first.

        try:
            JobGroup.import_from_dict(
                self.service.execute_job_group(jobgroup_b.export_to_dict()))
        except JobException:
            # Expected
            pass
        else:
            self.fail("Expected JobException from the lock.")

        first_greenlet.get()

        cms.service.Worker.get_task_type.assert_has_calls(
            calls_a, any_order=True)

    def test_execute_job_group_failure_releases_lock(self):
        """After a failure, the worker should be able to accept another job.

        """
        jobgroup_a, calls_a = TestWorker.new_jobgroup(1)
        task_type_a = FakeTaskType([Exception()])
        cms.service.Worker.get_task_type = Mock(return_value=task_type_a)

        try:
            JobGroup.import_from_dict(
                self.service.execute_job_group(jobgroup_a.export_to_dict()))
        except JobException:
            # Expected.
            pass
        else:
            self.fail("Expected Jobexception from tasktype.")
        cms.service.Worker.get_task_type.assert_has_calls(
            calls_a, any_order=True)
        self.assertEquals(task_type_a.call_count, 1)

        jobgroup_b, calls_b = TestWorker.new_jobgroup(3)
        task_type_b = FakeTaskType([True, True, True])
        cms.service.Worker.get_task_type = Mock(return_value=task_type_b)

        JobGroup.import_from_dict(
            self.service.execute_job_group(jobgroup_b.export_to_dict()))

        cms.service.Worker.get_task_type.assert_has_calls(
            calls_b, any_order=True)
        self.assertEquals(task_type_b.call_count, 3)

    # Testing ignore_job.

    def test_ignore_job(self):
        """Executes a job group with two jobs, and sends an ignore_job
        request that should discard the second job.

        """
        jobgroup, calls = TestWorker.new_jobgroup(2)
        task_type = FakeTaskType([0.01])
        cms.service.Worker.get_task_type = Mock(return_value=task_type)

        def first_call():
            JobGroup.import_from_dict(
                self.service.execute_job_group(jobgroup.export_to_dict()))

        first_greenlet = gevent.spawn(first_call)
        gevent.sleep(0)  # Ensure it enters into the first job.

        self.service.ignore_job()

        first_greenlet.get()

        # Only one call should have been made, the other skipped.
        self.assertEquals(cms.service.Worker.get_task_type.call_count, 1)
        self.assertEquals(task_type.call_count, 1)

    @staticmethod
    def new_jobgroup(number_of_jobs, prefix=None):
        prefix = prefix if prefix is not None else ""
        jobgroup_dict = {}
        calls = []
        for i in xrange(number_of_jobs):
            job_params = ("fake_task_type", "fake_parameters_%s" % i)
            job = EvaluationJob(*job_params, info=prefix + str(i))
            jobgroup_dict[str(i)] = job
            calls.append(call(*job_params))
        return JobGroup(jobgroup_dict), calls


class FakeTaskType(object):
    def __init__(self, execute_results):
        self.execute_results = execute_results
        self.index = 0
        self.call_count = 0

    def execute_job(self, job, file_cacher):
        self.call_count += 1
        result = self.execute_results[self.index]
        self.index += 1
        if isinstance(result, bool):
            # Boolean: it is the success of the job.
            job.success = result
        elif isinstance(result, Exception):
            # Exception: raise.
            raise result
        else:
            # Float: wait the number of seconds.
            job.success = True
            gevent.sleep(result)

    def set_results(self, results):
        self.execute_results = results


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = util_test
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2014 Stefano Maggiolo <s.maggiolo@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

"""Tests for general utility functions."""

from __future__ import absolute_import
from __future__ import print_function

import netifaces
import unittest

from mock import Mock

import cms.util
from cms import Address, ServiceCoord, \
    get_safe_shard, get_service_address, get_service_shards


class FakeAsyncConfig(object):
    """Fake class for the configuration of service addresses."""
    core_services = {
        ServiceCoord("Service", 0): Address("0.0.0.0", 0),
        ServiceCoord("Service", 1): Address("0.0.0.1", 1),
        }
    other_services = {}


def _set_up_async_config(restore=False):
    """Fake the async config."""
    if not restore:
        if not hasattr(_set_up_async_config, "original"):
            _set_up_async_config.original = cms.util.async_config
        cms.util.async_config = FakeAsyncConfig()
    else:
        cms.util.async_config = _set_up_async_config.original


def _set_up_ip_addresses(addresses=None, restore=False):
    """Instruct the netifaces module to return the specific ips."""
    if not restore:
        if not hasattr(_set_up_ip_addresses, "original"):
            _set_up_ip_addresses.original = \
                (netifaces.interfaces, netifaces.ifaddresses)
        dict_addresses = {
            netifaces.AF_INET: [{"addr": address} for address in addresses]}
        netifaces.interfaces = Mock(return_value="eth0")
        netifaces.ifaddresses = Mock(return_value=dict_addresses)
    else:
        netifaces.interfaces, netifaces.ifaddresses = \
            _set_up_ip_addresses.original


class TestGetSafeShard(unittest.TestCase):
    """Test the function cms.util.get_safe_shard."""
    def setUp(self):
        """Set up the default mocks."""
        _set_up_async_config()
        _set_up_ip_addresses(["1.1.1.1", "0.0.0.1"])

    def tearDown(self):
        """Restore the mocks to ensure normal operations."""
        _set_up_async_config(restore=True)
        _set_up_ip_addresses(restore=True)

    def test_success(self):
        """Test success cases.

        This tests for both giving explicitly the shard number, and
        for autodetecting it.

        """
        self.assertEqual(get_safe_shard("Service", 0), 0)
        self.assertEqual(get_safe_shard("Service", 1), 1)
        self.assertEqual(get_safe_shard("Service", None), 1)

    def test_shard_not_present(self):
        """Test failure when the given shard is not in the config."""
        with self.assertRaises(ValueError):
            get_safe_shard("Service", 2)

    def test_service_not_present(self):
        """Test failure when the given service is not in the config."""
        with self.assertRaises(ValueError):
            get_safe_shard("ServiceNotPresent", 0)

    def test_no_autodetect(self):
        """Test failure when no shard is given and autodetect fails."""
        # Setting up non-matching IPs.
        _set_up_ip_addresses(["1.1.1.1", "0.0.0.2"])
        with self.assertRaises(ValueError):
            get_safe_shard("Service", None)


class TestGetServiceAddress(unittest.TestCase):
    """Test the function cms.util.get_service_address.

    """
    def setUp(self):
        """Set up the default mocks."""
        _set_up_async_config()

    def tearDown(self):
        """Restore the mocks to ensure normal operations."""
        _set_up_async_config(restore=True)

    def test_success(self):
        """Test success cases."""
        self.assertEqual(
            get_service_address(ServiceCoord("Service", 0)),
            Address("0.0.0.0", 0))
        self.assertEqual(
            get_service_address(ServiceCoord("Service", 1)),
            Address("0.0.0.1", 1))

    def test_shard_not_present(self):
        """Test failure when the shard of the service is invalid."""
        with self.assertRaises(KeyError):
            get_service_address(ServiceCoord("Service", 2))

    def test_service_not_present(self):
        """Test failure when the service is invalid."""
        with self.assertRaises(KeyError):
            get_service_address(ServiceCoord("ServiceNotPresent", 0))


class TestGetServiceShards(unittest.TestCase):
    """Test the function cms.util.get_service_shards.

    """
    def setUp(self):
        """Set up the default mocks."""
        _set_up_async_config()

    def tearDown(self):
        """Restore the mocks to ensure normal operations."""
        _set_up_async_config(restore=True)

    def test_success(self):
        """Test success cases."""
        self.assertEqual(get_service_shards("Service"), 2)
        self.assertEqual(get_service_shards("ServiceNotPresent"), 0)


if __name__ == "__main__":
    unittest.main()

########NEW FILE########
__FILENAME__ = AWSRequests
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2012 Bernard Blackham <bernard@largestprime.net>
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import re

from BeautifulSoup import BeautifulSoup

from cmstestsuite.web import GenericRequest


class AWSSubmissionViewRequest(GenericRequest):
    """Load the view of a submission in AWS.

    """
    def __init__(self, browser, submission_id, base_url=None):
        GenericRequest.__init__(self, browser, base_url)
        self.submission_id = submission_id
        self.url = "%ssubmission/%s" % (self.base_url, submission_id)

    def describe(self):
        return "check submission %s" % self.submission_id

    def test_success(self):
        if not GenericRequest.test_success(self):
            return False
        try:
            self.get_submission_info()
            return True
        except:
            return False

    def get_submission_info(self):
        # Only valid after self.execute()
        # Parse submission information out of response.

        soup = BeautifulSoup(self.res_data)

        info = {}

        # Get submission status.
        tag = soup.findAll(id="submission_status")[0]
        info['status'] = tag.text.strip()

        # Get compilation text.
        tags = soup.findAll(id="compilation")
        if tags:
            content = tags[0]
            info['compile_output'] = content.pre.text.strip()
        else:
            info['compile_output'] = None

        # Get evaluation results.
        evaluations = []
        tags = soup.findAll(id=re.compile(r"^eval_outcome_"))
        for outcome_tag in sorted(tags, key=lambda t: t['id']):
            index = int(outcome_tag['id'][len("eval_outcome_"):])

            # Get evaluation text also.
            text_tag = soup.findAll(id="eval_text_%d" % index)[0]

            evaluations.append({
                'outcome': outcome_tag.text.strip(),
                'text': text_tag.text.strip(),
            })

        info['evaluations'] = evaluations

        return info

########NEW FILE########
__FILENAME__ = CWSRequests
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Contest Management System - http://cms-dev.github.io/
# Copyright © 2010-2012 Giovanni Mascellani <mascellani@poisson.phc.unipi.it>
# Copyright © 2010-2012 Stefano Maggiolo <s.maggiolo@gmail.com>
# Copyright © 2010-2012 Matteo Boscariol <boscarim@hotmail.com>
# Copyright © 2014 Artem Iglikov <artem.iglikov@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.

from __future__ import absolute_import
from __future__ import print_function

import re
import os
import random

from cmscommon.crypto import decrypt_number
from cmstestsuite.web import GenericRequest


class HomepageRequest(GenericRequest):
    """Load the main page of CWS.

    """
    def __init__(self, browser, username, loggedin, base_url=None):
        GenericRequest.__init__(self, browser, base_url)
        self.url = self.base_url
        self.username = username
        self.loggedin = loggedin

    def describe(self):
        return "check the main page"

    def test_success(self):
        if not GenericRequest.test_success(self):
            return False
        username_re = re.compile(self.username)
        if self.loggedin:
            if username_re.search(self.res_data) is None:
                return False
        else:
            if username_re.search(self.res_data) is not None:
                return False
        return True


class LoginRequest(GenericRequest):
    """Try to login to CWS with given credentials.

    """
    def __init__(self, browser, username, password, base_url=None):
        GenericRequest.__init__(self, browser, base_url)
        self.username = username
        self.password = password
        self.url = '%slogin' % self.base_url
        self.data = {'username': self.username,
                     'password': self.password,
                     'next': '/'}

    def describe(self):
        return "try to login"

    def test_success(self):
        if not GenericRequest.test_success(self):
            return False
        fail_re = re.compile('Failed to log in.')
        if fail_re.search(self.res_data) is not None:
            return False
        username_re = re.compile(self.username)
        if username_re.search(self.res_data) is None:
            return False
        return True

    def specific_info(self):
        return 'Username: %s\nPassword: %s\n' % \
               (self.username, self.password) + \
            GenericRequest.specific_info(self)


class TaskRequest(GenericRequest):
    """Load a task page in CWS.

    """
    def __init__(self, browser, task_id, base_url=None):
        GenericRequest.__init__(self, browser, base_url)
        self.url = "%stasks/%s/description" % (self.base_url, task_id)
        self.task_id = task_id

    def describe(self):
        return "load page for task %s (%s)" % (self.task_id, self.url)


class TaskStatementRequest(GenericRequest):
    """Load a task statement in CWS.

    """
    def __init__(self, browser, task_id, language_code, base_url=None):
        GenericRequest.__init__(self, browser, base_url)
        self.url = "%stasks/%s/statements/%s" % (self.base_url,
                                                 task_id, language_code)
        self.task_id = task_id

    def describe(self):
        return "load statement for task %s (%s)" % (self.task_id, self.url)

    def specific_info(self):
        return '\nNO DATA DUMP FOR TASK STATEMENTS\n'


class SubmitRequest(GenericRequest):
    """Submit a solution in CWS.

    """
    def __init__(self, browser, task, filename, base_url=None):
        GenericRequest.__init__(self, browser, base_url)
        self.url = "%stasks/%s/submit" % (self.base_url, task[1])
        self.task = task
        self.filename = filename
        self.data = {}

    def prepare(self):
        GenericRequest.prepare(self)
        self.files = [('%s.%%l' % (self.task[1]), self.filename)]

    def describe(self):
        return "submit source %s for task %s (ID %d) %s" % \
            (self.filename, self.task[1], self.task[0], self.url)

    def specific_info(self):
        return 'Task: %s (ID %d)\nFile: %s\n' % \
            (self.task[1], self.task[0], self.filename) + \
            GenericRequest.specific_info(self)

    def test_success(self):
        if not GenericRequest.test_success(self):
            return False

        return self.get_submission_id() is not None

    def get_submission_id(self):
        # Only valid after self.execute()
        # Parse submission ID out of response.
        p = self.browser.geturl().split("?")[-1]
        try:
            submission_id = decrypt_number(p)
        except Exception:
            return None
        return submission_id


class TokenRequest(GenericRequest):
    """Release test a submission.

    """
    def __init__(self, browser, task, submission_num, base_url=None):
        GenericRequest.__init__(self, browser, base_url)
        self.url = "%stasks/%s/submissions/%s/token" % (self.base_url,
                                                        task[1],
                                                        submission_num)
        self.task = task
        self.submission_num = submission_num
        self.data = {}

    def prepare(self):
        GenericRequest.prepare(self)

    def describe(self):
        return "release test the %s-th submission for task %s (ID %d)" % \
            (self.submission_num, self.task[1], self.task[0])

    def specific_info(self):
        return 'Task: %s (ID %d)\nSubmission: %s\n' % \
            (self.task[1], self.task[0], self.submission_num) + \
            GenericRequest.specific_info(self)

    def test_success(self):
        if not GenericRequest.test_success(self):
            return False

        return True


class SubmitRandomRequest(SubmitRequest):
    """Submit a solution in CWS.

    """
    def __init__(self, browser, task, base_url=None,
                 submissions_path=None):
        GenericRequest.__init__(self, browser, base_url)
        self.url = "%stasks/%s/submit" % (self.base_url, task[1])
        self.task = task
        self.submissions_path = submissions_path
        self.data = {}

    def prepare(self):
        GenericRequest.prepare(self)
        task_path = os.path.join(self.submissions_path, self.task[1])
        sources = os.listdir(task_path)
        source = random.choice(sources)
        self.source_path = os.path.join(task_path, source)
        self.files = [('%s.%%l' % (self.task[1]), self.source_path)]

    def describe(self):
        return "submit source %s for task %s (ID %d) %s" % \
            (self.source_path, self.task[1], self.task[0], self.url)

    def specific_info(self):
        return 'Task: %s (ID %d)\nFile: %s\n' % \
            (self.task[1], self.task[0], self.source_path) + \
            GenericRequest.specific_info(self)

########NEW FILE########
__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# cms documentation build configuration file, created by
# sphinx-quickstart on Tue Jan  1 11:59:01 2013.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = ['sphinx.ext.autodoc', 'gh_links']

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'CMS'
copyright = u'2013, The CMS development team'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '1.1'
# The full version, including alpha/beta/rc tags.
release = '1.1.0pre'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'cmsdoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
'papersize': 'a4paper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'cms.tex', u'CMS Documentation',
   u'The CMS development team', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'CMS', u'CMS Documentation',
     [u'The CMS development team'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'CMS', u'CMS Documentation',
   u'The CMS development team', 'CMS', 'Contest Management System.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = gh_links
#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Sphinx extension to add roles for some GitHub features
# Copyright © 2013 Luca Wehrstedt <luca.wehrstedt@gmail.com>
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as
# published by the Free Software Foundation, either version 3 of the
# License, or (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.


from docutils import nodes, utils

from sphinx.util.nodes import split_explicit_title

def gh_issue(typ, rawtext, text, lineno, inliner, options={}, content=[]):
    text = utils.unescape(text)
    has_explicit_title, title, part = split_explicit_title(text)
    if not has_explicit_title:
        title = 'issue #%s' % part
    full_url = 'https://github.com/cms-dev/cms/issues/%s' % part

    retnode = nodes.reference(title, title, internal=False, refuri=full_url, **options)
    return [retnode], []

def make_gh_download(app):
    def gh_download(typ, rawtext, text, lineno, inliner, options={}, content=[]):
        title = utils.unescape(text)
        full_url = 'https://github.com/cms-dev/cms/archive/v%s.tar.gz' % app.config.release

        retnode = nodes.reference(title, title, internal=False, refuri=full_url, **options)
        return [retnode], []
    return gh_download

def make_gh_tree(app):
    def gh_tree(typ, rawtext, text, lineno, inliner, options={}, content=[]):
        text = utils.unescape(text)
        has_explicit_title, title, part = split_explicit_title(text)
        if not has_explicit_title:
            title = part
        full_url = 'https://github.com/cms-dev/cms/tree/v%s/%s' % (app.config.release, part)

        refnode = nodes.reference(title, title, internal=False, refuri=full_url, **options)
        retnode = nodes.literal(role=typ.lower(), classes=[typ])
        retnode += refnode
        return [retnode], []
    return gh_tree

def make_gh_blob(app):
    def gh_blob(typ, rawtext, text, lineno, inliner, options={}, content=[]):
        text = utils.unescape(text)
        has_explicit_title, title, part = split_explicit_title(text)
        if not has_explicit_title:
            title = part
        full_url = 'https://github.com/cms-dev/cms/blob/v%s/%s' % (app.config.release, part)

        refnode = nodes.reference(title, title, internal=False, refuri=full_url, **options)
        retnode = nodes.literal(role=typ.lower(), classes=[typ])
        retnode += refnode
        return [retnode], []
    return gh_blob

def setup_roles(app):
    app.add_role("gh_issue", gh_issue)
    app.add_role("gh_download", make_gh_download(app))
    app.add_role("gh_tree", make_gh_tree(app))
    app.add_role("gh_blob", make_gh_blob(app))

def setup(app):
    app.connect('builder-inited', setup_roles)

########NEW FILE########
