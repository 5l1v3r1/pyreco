__FILENAME__ = conf
# -*- coding: utf-8 -*-
#
# redis-completion documentation build configuration file, created by
# sphinx-quickstart on Wed Jun  6 14:51:55 2012.
#
# This file is execfile()d with the current directory set to its containing dir.
#
# Note that not all possible configuration values are present in this
# autogenerated file.
#
# All configuration values have a default; values that are commented out
# serve to show the default.

import sys, os

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#sys.path.insert(0, os.path.abspath('.'))

# -- General configuration -----------------------------------------------------

# If your documentation needs a minimal Sphinx version, state it here.
#needs_sphinx = '1.0'

# Add any Sphinx extension module names here, as strings. They can be extensions
# coming with Sphinx (named 'sphinx.ext.*') or your custom ones.
extensions = []

# Add any paths that contain templates here, relative to this directory.
templates_path = ['_templates']

# The suffix of source filenames.
source_suffix = '.rst'

# The encoding of source files.
#source_encoding = 'utf-8-sig'

# The master toctree document.
master_doc = 'index'

# General information about the project.
project = u'redis-completion'
copyright = u'2012, charles leifer'

# The version info for the project you're documenting, acts as replacement for
# |version| and |release|, also used in various other places throughout the
# built documents.
#
# The short X.Y version.
version = '0.4.0'
# The full version, including alpha/beta/rc tags.
release = '0.4.0'

# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#language = None

# There are two options for replacing |today|: either, you set today to some
# non-false value, then it is used:
#today = ''
# Else, today_fmt is used as the format for a strftime call.
#today_fmt = '%B %d, %Y'

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
exclude_patterns = ['_build']

# The reST default role (used for this markup: `text`) to use for all documents.
#default_role = None

# If true, '()' will be appended to :func: etc. cross-reference text.
#add_function_parentheses = True

# If true, the current module name will be prepended to all description
# unit titles (such as .. function::).
#add_module_names = True

# If true, sectionauthor and moduleauthor directives will be shown in the
# output. They are ignored by default.
#show_authors = False

# The name of the Pygments (syntax highlighting) style to use.
pygments_style = 'sphinx'

# A list of ignored prefixes for module index sorting.
#modindex_common_prefix = []


# -- Options for HTML output ---------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
html_theme = 'default'

# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#html_theme_options = {}

# Add any paths that contain custom themes here, relative to this directory.
#html_theme_path = []

# The name for this set of Sphinx documents.  If None, it defaults to
# "<project> v<release> documentation".
#html_title = None

# A shorter title for the navigation bar.  Default is the same as html_title.
#html_short_title = None

# The name of an image file (relative to this directory) to place at the top
# of the sidebar.
#html_logo = None

# The name of an image file (within the static path) to use as favicon of the
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32
# pixels large.
#html_favicon = None

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ['_static']

# If not '', a 'Last updated on:' timestamp is inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
#html_use_smartypants = True

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

# If false, no index is generated.
#html_use_index = True

# If true, the index is split into individual pages for each letter.
#html_split_index = False

# If true, links to the reST sources are added to the pages.
#html_show_sourcelink = True

# If true, "Created using Sphinx" is shown in the HTML footer. Default is True.
#html_show_sphinx = True

# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True.
#html_show_copyright = True

# If true, an OpenSearch description file will be output, and all pages will
# contain a <link> tag referring to it.  The value of this option must be the
# base URL from which the finished HTML is served.
#html_use_opensearch = ''

# This is the file name suffix for HTML files (e.g. ".xhtml").
#html_file_suffix = None

# Output file base name for HTML help builder.
htmlhelp_basename = 'redis-completiondoc'


# -- Options for LaTeX output --------------------------------------------------

latex_elements = {
# The paper size ('letterpaper' or 'a4paper').
#'papersize': 'letterpaper',

# The font size ('10pt', '11pt' or '12pt').
#'pointsize': '10pt',

# Additional stuff for the LaTeX preamble.
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'redis-completion.tex', u'redis-completion Documentation',
   u'charles leifer', 'manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For "manual" documents, if this is true, then toplevel headings are parts,
# not chapters.
#latex_use_parts = False

# If true, show page references after internal links.
#latex_show_pagerefs = False

# If true, show URL addresses after external links.
#latex_show_urls = False

# Documents to append as an appendix to all manuals.
#latex_appendices = []

# If false, no module index is generated.
#latex_domain_indices = True


# -- Options for manual page output --------------------------------------------

# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [
    ('index', 'redis-completion', u'redis-completion Documentation',
     [u'charles leifer'], 1)
]

# If true, show URL addresses after external links.
#man_show_urls = False


# -- Options for Texinfo output ------------------------------------------------

# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
  ('index', 'redis-completion', u'redis-completion Documentation',
   u'charles leifer', 'redis-completion', 'One line description of project.',
   'Miscellaneous'),
]

# Documents to append as an appendix to all manuals.
#texinfo_appendices = []

# If false, no module index is generated.
#texinfo_domain_indices = True

# How to display URL addresses: 'footnote', 'no', or 'inline'.
#texinfo_show_urls = 'footnote'

########NEW FILE########
__FILENAME__ = stocks
import urllib2
from redis_completion import RedisEngine

engine = RedisEngine(prefix='stocks')

def load_data():
    url = 'http://media.charlesleifer.com/downloads/misc/NYSE.txt'
    contents = urllib2.urlopen(url).read()
    for row in contents.splitlines()[1:]:
        ticker, company = row.split('\t')
        engine.store_json(ticker, company, {'ticker': ticker, 'company': company}) # id, search phrase, data

def search(p, **kwargs):
    return engine.search_json(p, **kwargs)

if __name__ == '__main__':
    engine.flush()
    print 'Loading data (may take a few seconds...)'
    load_data()

    print 'Search data by typing a partial phrase, like "uni sta"'
    print 'Type "q" at any time to quit'

    while 1:
        cmd = raw_input('? ')
        if cmd == 'q':
            break
        results = search(cmd)
        print 'Found %s matches' % len(results)
        for result in results:
            print '%s: %s' % (result['ticker'], result['company'])

########NEW FILE########
__FILENAME__ = engine
try:
    import simplejson as json
except ImportError:
    import json
from itertools import izip_longest
import re
from redis import Redis

from redis_completion.stop_words import STOP_WORDS as _STOP_WORDS


# aggressive stop words will be better when the length of the document is longer
AGGRESSIVE_STOP_WORDS = _STOP_WORDS

# default stop words should work fine for titles and things like that
DEFAULT_STOP_WORDS = set(['a', 'an', 'of', 'the'])


# Keep track of empty objects.
_sentinel = object()


class RedisEngine(object):
    """
    References
    ----------

    http://antirez.com/post/autocomplete-with-redis.html
    http://stackoverflow.com/questions/1958005/redis-autocomplete/1966188#1966188
    http://patshaughnessy.net/2011/11/29/two-ways-of-using-redis-to-build-a-nosql-autocomplete-search-index
    """
    def __init__(self, prefix='ac', stop_words=None, cache_timeout=300,redis_client=None,
                 **conn_kwargs):
        self.prefix = prefix
        self.stop_words = (stop_words is None) and DEFAULT_STOP_WORDS or stop_words
        self.cache_timeout = cache_timeout

        self.conn_kwargs = conn_kwargs
        if redis_client:
            self.client = redis_client
        else:
            self.client = self.get_client()

        self.boost_key = '%s:b' % self.prefix
        self.data_key = '%s:d' % self.prefix
        self.title_key = '%s:t' % self.prefix
        self.search_key = lambda k: '%s:s:%s' % (self.prefix, k)
        self.cache_key = lambda pk, bk: '%s:c:%s:%s' % (self.prefix, pk, bk)

        self.kcombine = lambda _id, _type: ''.join([str(_id), '\x01', str(_type)])
        self.ksplit = lambda k: k.split('\x01', 1)
        self._offset = 27**20

    def get_client(self):
        return Redis(**self.conn_kwargs)

    def score_key(self, k, max_size=20):
        k_len = len(k)
        a = ord('a') - 2
        score = 0

        for i in range(max_size):
            if i < k_len:
                c = (ord(k[i]) - a)
                if c < 2 or c > 27:
                    c = 1
            else:
                c = 1
            score += c*(27**(max_size-i))
        return score

    def clean_phrase(self, phrase):
        phrase = re.sub('[^a-z0-9_\-\s]', '', phrase.lower())
        return [w for w in phrase.split() if w not in self.stop_words]

    def create_key(self, phrase):
        return ' '.join(self.clean_phrase(phrase))

    def autocomplete_keys(self, w):
        for i in range(1, len(w)):
            yield w[:i]
        yield w

    def flush(self, everything=False, batch_size=1000):
        if everything:
            return self.client.flushdb()

        # this could be expensive :-(
        keys = self.client.keys('%s:*' % self.prefix)

        # batch keys
        for i in range(0, len(keys), batch_size):
            self.client.delete(*keys[i:i+batch_size])

    def store(self, obj_id, title=None, data=None, obj_type=None, check_exist=True):
        if title is None:
            title = obj_id
        if data is None:
            data = title

        title_score = self.score_key(self.create_key(title))

        combined_id = self.kcombine(obj_id, obj_type or '')

        if check_exist and self.exists(obj_id, obj_type):
            stored_title = self.client.hget(self.title_key, combined_id)

            # if the stored title is the same, we can simply update the data key
            # since everything else will have stayed the same
            if stored_title == title:
                self.client.hset(self.data_key, combined_id, data)
                return
            else:
                self.remove(obj_id, obj_type)

        pipe = self.client.pipeline()
        pipe.hset(self.data_key, combined_id, data)
        pipe.hset(self.title_key, combined_id, title)

        for i, word in enumerate(self.clean_phrase(title)):
            word_score = self.score_key(word) + self._offset
            key_score = (word_score * (i + 1)) + (title_score)
            for partial_key in self.autocomplete_keys(word):
                pipe.zadd(self.search_key(partial_key), combined_id, key_score)

        pipe.execute()

    def store_json(self, obj_id, title, data_dict, obj_type=None):
        return self.store(obj_id, title, json.dumps(data_dict), obj_type)

    def remove(self, obj_id, obj_type=None):
        obj_id = self.kcombine(obj_id, obj_type or '')
        title = self.client.hget(self.title_key, obj_id) or ''
        keys = []

        for word in self.clean_phrase(title):
            for partial_key in self.autocomplete_keys(word):
                key = self.search_key(partial_key)
                if not self.client.zrange(key, 1, 2):
                    self.client.delete(key)
                else:
                    self.client.zrem(key, obj_id)

        self.client.hdel(self.data_key, obj_id)
        self.client.hdel(self.title_key, obj_id)
        self.client.hdel(self.boost_key, obj_id)

    def boost(self, obj_id, multiplier=1.1, negative=False):
        # take the existing boost for this item and increase it by the multiplier
        current = self.client.hget(self.boost_key, obj_id)
        current_f = float(current or 1.0)
        if negative:
            multiplier = 1 / multiplier
        self.client.hset(self.boost_key, obj_id, current_f * multiplier)

    def exists(self, obj_id, obj_type=None):
        obj_id = self.kcombine(obj_id, obj_type or '')
        return self.client.hexists(self.data_key, obj_id)

    def get_cache_key(self, phrases, boosts):
        if boosts:
            boost_key = '|'.join('%s:%s' % (k, v) for k, v in sorted(boosts.items()))
        else:
            boost_key = ''
        phrase_key = '|'.join(phrases)
        return self.cache_key(phrase_key, boost_key)

    def _chunked(self, iterable, n):
        for group in (list(g) for g in izip_longest(*[iter(iterable)] * n,
                                                    fillvalue=_sentinel)):
            if group[-1] is _sentinel:
                del group[group.index(_sentinel):]
            yield group

    def _process_ids(self, id_list, limit, filters, mappers):
        ct = 0
        data = []
        if not id_list:
            return data

        if limit is not None:
            chunks = self._chunked(id_list, limit)
        else:
            chunks = [id_list]

        for chunk in chunks:
            for raw_data in self.client.hmget(self.data_key, chunk):
                if not raw_data:
                    continue

                if mappers:
                    for m in mappers:
                        raw_data = m(raw_data)

                if filters:
                    passes = True
                    for f in filters:
                        if not f(raw_data):
                            passes = False
                            break

                    if not passes:
                        continue

                data.append(raw_data)
                ct += 1
                if limit and ct == limit:
                    return data

        return data

    def search(self, phrase, limit=None, filters=None, mappers=None, boosts=None, autoboost=False):
        cleaned = self.clean_phrase(phrase)
        if not cleaned:
            return []

        if autoboost:
            boosts = boosts or {}
            stored = self.client.hgetall(self.boost_key)
            for obj_id in stored:
                if obj_id not in boosts:
                    boosts[obj_id] = float(stored[obj_id])

        if len(cleaned) == 1 and not boosts:
            new_key = self.search_key(cleaned[0])
        else:
            new_key = self.get_cache_key(cleaned, boosts)
            if not self.client.exists(new_key):
                # zinterstore also takes {k1: wt1, k2: wt2}
                self.client.zinterstore(new_key, map(self.search_key, cleaned))
                self.client.expire(new_key, self.cache_timeout)

        if boosts:
            pipe = self.client.pipeline()
            for raw_id, score in self.client.zrange(new_key, 0, -1, withscores=True):
                orig_score = score
                for part in self.ksplit(raw_id):
                    if part and part in boosts:
                        score *= 1 / boosts[part]
                if orig_score != score:
                    pipe.zadd(new_key, raw_id, score)
            pipe.execute()

        id_list = self.client.zrange(new_key, 0, -1)
        return self._process_ids(id_list, limit, filters, mappers)

    def search_json(self, phrase, limit=None, filters=None, mappers=None, boosts=None, autoboost=False):
        if not mappers:
            mappers = []
        mappers.insert(0, json.loads)
        return self.search(phrase, limit, filters, mappers, boosts, autoboost)

########NEW FILE########
__FILENAME__ = stop_words
words = """a
a's
able
about
above
according
accordingly
across
actually
after
afterwards
again
against
ain't
all
allow
allows
almost
alone
along
already
also
although
always
am
among
amongst
amoungst
amount
an
and
another
any
anybody
anyhow
anyone
anything
anyway
anyways
anywhere
apart
appear
appreciate
appropriate
are
aren't
around
as
aside
ask
asking
associated
at
available
away
awfully
back
be
became
because
become
becomes
becoming
been
before
beforehand
behind
being
believe
below
beside
besides
best
better
between
beyond
bill
both
bottom
brief
but
by
c'mon
c's
call
came
can
can't
cannot
cant
cause
causes
certain
certainly
changes
clearly
co
com
come
comes
computer
con
concerning
consequently
consider
considering
contain
containing
contains
corresponding
could
couldn't
couldnt
course
cry
currently
de
definitely
describe
described
despite
detail
did
didn't
different
do
does
doesn't
doing
don't
done
down
downwards
due
during
each
edu
eg
eight
either
eleven
else
elsewhere
empty
enough
entirely
especially
et
etc
even
ever
every
everybody
everyone
everything
everywhere
ex
exactly
example
except
far
few
fifteen
fifth
fify
fill
find
fire
first
five
followed
following
follows
for
former
formerly
forth
forty
found
four
from
front
full
further
furthermore
get
gets
getting
give
given
gives
go
goes
going
gone
got
gotten
greetings
had
hadn't
happens
hardly
has
hasn't
hasnt
have
haven't
having
he
he's
hello
help
hence
her
here
here's
hereafter
hereby
herein
hereupon
hers
herself
hi
him
himself
his
hither
hopefully
how
howbeit
however
hundred
i
i'd
i'll
i'm
i've
ie
if
ignored
immediate
in
inasmuch
inc
indeed
indicate
indicated
indicates
inner
insofar
instead
interest
into
inward
is
isn't
it
it'd
it'll
it's
its
itself
just
keep
keeps
kept
know
known
knows
last
lately
later
latter
latterly
least
less
lest
let
let's
like
liked
likely
little
look
looking
looks
ltd
made
mainly
many
may
maybe
me
mean
meanwhile
merely
might
mill
mine
more
moreover
most
mostly
move
much
must
my
myself
name
namely
nd
near
nearly
necessary
need
needs
neither
never
nevertheless
new
next
nine
no
nobody
non
none
noone
nor
normally
not
nothing
novel
now
nowhere
obviously
of
off
often
oh
ok
okay
old
on
once
one
ones
only
onto
or
other
others
otherwise
ought
our
ours
ourselves
out
outside
over
overall
own
part
particular
particularly
per
perhaps
placed
please
plus
possible
presumably
probably
provides
put
que
quite
qv
rather
rd
re
really
reasonably
regarding
regardless
regards
relatively
respectively
right
said
same
saw
say
saying
says
second
secondly
see
seeing
seem
seemed
seeming
seems
seen
self
selves
sensible
sent
serious
seriously
seven
several
shall
she
should
shouldn't
show
side
since
sincere
six
sixty
so
some
somebody
somehow
someone
something
sometime
sometimes
somewhat
somewhere
soon
sorry
specified
specify
specifying
still
sub
such
sup
sure
system
t's
take
taken
tell
ten
tends
th
than
thank
thanks
thanx
that
that's
thats
the
their
theirs
them
themselves
then
thence
there
there's
thereafter
thereby
therefore
therein
theres
thereupon
these
they
they'd
they'll
they're
they've
thick
thin
think
third
this
thorough
thoroughly
those
though
three
through
throughout
thru
thus
to
together
too
took
top
toward
towards
tried
tries
truly
try
trying
twelve
twenty
twice
two
un
under
unfortunately
unless
unlikely
until
unto
up
upon
us
use
used
useful
uses
using
usually
value
various
very
via
viz
vs
want
wants
was
wasn't
way
we
we'd
we'll
we're
we've
welcome
well
went
were
weren't
what
what's
whatever
when
whence
whenever
where
where's
whereafter
whereas
whereby
wherein
whereupon
wherever
whether
which
while
whither
who
who's
whoever
whole
whom
whose
why
will
willing
wish
with
within
without
won't
wonder
would
wouldn't
yes
yet
you
you'd
you'll
you're
you've
your
yours
yourself
yourselves
zero"""
STOP_WORDS = set([
    w.strip() for w in words.splitlines() if w
])

########NEW FILE########
__FILENAME__ = tests
import random
from unittest import TestCase

from redis_completion.engine import RedisEngine


stop_words = set(['a', 'an', 'the', 'of'])

class RedisCompletionTestCase(TestCase):
    def setUp(self):
        self.engine = self.get_engine()
        self.engine.flush()

    def get_engine(self):
        return RedisEngine(prefix='testac', db=15)

    def store_data(self, id=None):
        test_data = (
            (1, 'testing python'),
            (2, 'testing python code'),
            (3, 'web testing python code'),
            (4, 'unit tests with python'),
        )
        for obj_id, title in test_data:
            if id is None or id == obj_id:
                self.engine.store_json(obj_id, title, {
                    'obj_id': obj_id,
                    'title': title,
                    'secret': obj_id % 2 == 0 and 'derp' or 'herp',
                })

    def sort_results(self, r):
        return sorted(r, key=lambda i:i['obj_id'])

    def test_search(self):
        self.store_data()

        results = self.engine.search_json('testing python')
        self.assertEqual(self.sort_results(results), [
            {'obj_id': 1, 'title': 'testing python', 'secret': 'herp'},
            {'obj_id': 2, 'title': 'testing python code', 'secret': 'derp'},
            {'obj_id': 3, 'title': 'web testing python code', 'secret': 'herp'},
        ])

        results = self.engine.search_json('test')
        self.assertEqual(self.sort_results(results), [
            {'obj_id': 1, 'title': 'testing python', 'secret': 'herp'},
            {'obj_id': 2, 'title': 'testing python code', 'secret': 'derp'},
            {'obj_id': 3, 'title': 'web testing python code', 'secret': 'herp'},
            {'obj_id': 4, 'title': 'unit tests with python', 'secret': 'derp'},
        ])

        results = self.engine.search_json('unit')
        self.assertEqual(results, [
            {'obj_id': 4, 'title': 'unit tests with python', 'secret': 'derp'},
        ])

        results = self.engine.search_json('')
        self.assertEqual(results, [])

        results = self.engine.search_json('missing')
        self.assertEqual(results, [])

    def test_boosting(self):
        test_data = (
            (1, 'test alpha', 't1'),
            (2, 'test beta', 't1'),
            (3, 'test gamma', 't1'),
            (4, 'test delta', 't1'),
            (5, 'test alpha', 't2'),
            (6, 'test beta', 't2'),
            (7, 'test gamma', 't2'),
            (8, 'test delta', 't2'),
            (9, 'test alpha', 't3'),
            (10, 'test beta', 't3'),
            (11, 'test gamma', 't3'),
            (12, 'test delta', 't3'),
        )
        for obj_id, title, obj_type in test_data:
            self.engine.store_json(obj_id, title, {
                'obj_id': obj_id,
                'title': title,
            }, obj_type)

        def assertExpected(results, id_list):
            self.assertEqual([r['obj_id'] for r in results], id_list)

        results = self.engine.search_json('alp')
        assertExpected(results, [1, 5, 9])

        results = self.engine.search_json('alp', boosts={'t2': 1.1})
        assertExpected(results, [5, 1, 9])

        results = self.engine.search_json('test', boosts={'t3': 1.5, 't2': 1.1})
        assertExpected(results, [9, 10, 12, 11, 5, 6, 8, 7, 1, 2, 4, 3])

        results = self.engine.search_json('alp', boosts={'t1': 0.5})
        assertExpected(results, [5, 9, 1])

        results = self.engine.search_json('alp', boosts={'t1': 1.5, 't3': 1.6})
        assertExpected(results, [9, 1, 5])

        results = self.engine.search_json('alp', boosts={'t3': 1.5, '5': 1.6})
        assertExpected(results, [5, 9, 1])

    def test_autoboost(self):
        self.engine.store('t1', 'testing 1')
        self.engine.store('t2', 'testing 2')
        self.engine.store('t3', 'testing 3')
        self.engine.store('t4', 'testing 4')
        self.engine.store('t5', 'testing 5')

        def assertExpected(results, id_list):
            self.assertEqual(results, ['testing %s' % i for i in id_list])

        results = self.engine.search('testing', autoboost=True)
        assertExpected(results, [1, 2, 3, 4, 5])

        self.engine.boost('t3')
        results = self.engine.search('testing', autoboost=True)
        assertExpected(results, [3, 1, 2, 4, 5])

        self.engine.boost('t2')
        results = self.engine.search('testing', autoboost=True)
        assertExpected(results, [2, 3, 1, 4, 5])

        self.engine.boost('t1', negative=True)
        results = self.engine.search('testing', autoboost=True)
        assertExpected(results, [2, 3, 4, 5, 1])

        results = self.engine.search('testing', boosts={'t5': 4.0}, autoboost=True)
        assertExpected(results, [5, 2, 3, 4, 1])

        results = self.engine.search('testing', boosts={'t3': 1.5}, autoboost=True)
        assertExpected(results, [3, 2, 4, 5, 1])

    def test_limit(self):
        self.store_data()

        results = self.engine.search_json('testing', limit=1)
        self.assertEqual(results, [
            {'obj_id': 1, 'title': 'testing python', 'secret': 'herp'},
        ])

    def test_filters(self):
        self.store_data()

        f = lambda i: i['secret'] == 'herp'
        results = self.engine.search_json('testing python', filters=[f])

        self.assertEqual(self.sort_results(results), [
            {'obj_id': 1, 'title': 'testing python', 'secret': 'herp'},
            {'obj_id': 3, 'title': 'web testing python code', 'secret': 'herp'},
        ])

    def test_simple(self):
        self.engine.print_scores = True
        self.engine.store('testing python')
        self.engine.store('testing python code')
        self.engine.store('web testing python code')
        self.engine.store('unit tests with python')

        results = self.engine.search('testing')
        self.assertEqual(results, ['testing python', 'testing python code', 'web testing python code'])

        results = self.engine.search('code')
        self.assertEqual(results, ['testing python code', 'web testing python code'])

        self.engine.store('z python code')
        results = self.engine.search('cod')
        self.assertEqual(results, ['testing python code', 'z python code', 'web testing python code'])

    def test_correct_sorting(self):
        strings = []
        for i in range(26):
            strings.append('aaaa%s' % chr(i + ord('a')))
            if i > 0:
                strings.append('aaa%sa' % chr(i + ord('a')))

        random.shuffle(strings)

        for s in strings:
            self.engine.store(s)

        results = self.engine.search('aaa')
        self.assertEqual(results, sorted(strings))

        results = self.engine.search('aaa', limit=30)
        self.assertEqual(results, sorted(strings)[:30])

    def test_removing_objects(self):
        self.store_data()

        self.engine.remove(1)

        results = self.engine.search_json('testing')
        self.assertEqual(self.sort_results(results), [
            {'obj_id': 2, 'title': 'testing python code', 'secret': 'derp'},
            {'obj_id': 3, 'title': 'web testing python code', 'secret': 'herp'},
        ])

        self.store_data(1)
        self.engine.remove(2)

        results = self.engine.search_json('testing')
        self.assertEqual(self.sort_results(results), [
            {'obj_id': 1, 'title': 'testing python', 'secret': 'herp'},
            {'obj_id': 3, 'title': 'web testing python code', 'secret': 'herp'},
        ])

    def test_clean_phrase(self):
        self.assertEqual(self.engine.clean_phrase('abc def ghi'), ['abc', 'def', 'ghi'])

        self.assertEqual(self.engine.clean_phrase('a A tHe an a'), [])
        self.assertEqual(self.engine.clean_phrase(''), [])

        self.assertEqual(
            self.engine.clean_phrase('The Best of times, the blurst of times'),
            ['best', 'times', 'blurst', 'times'])

    def test_exists(self):
        self.assertFalse(self.engine.exists('test'))
        self.engine.store('test')
        self.assertTrue(self.engine.exists('test'))

    def test_removing_objects_in_depth(self):
        # want to ensure that redis is cleaned up and does not become polluted
        # with spurious keys when objects are removed
        redis_client = self.engine.client
        prefix = self.engine.prefix

        initial_key_count = len(redis_client.keys())

        # store the blog "testing python"
        self.store_data(1)

        # see how many keys we have in the db - check again in a bit
        key_len = len(redis_client.keys())

        self.store_data(2)
        key_len2 = len(redis_client.keys())

        self.assertTrue(key_len != key_len2)
        self.engine.remove(2)

        # back to the original amount of keys
        self.assertEqual(len(redis_client.keys()), key_len)

        self.engine.remove(1)
        self.assertEqual(len(redis_client.keys()), initial_key_count)

    def test_updating(self):
        self.engine.store('id1', 'title one', 'd1', 't1')
        self.engine.store('id2', 'title two', 'd2', 't2')
        self.engine.store('id3', 'title three', 'd3', 't3')

        results = self.engine.search('tit')
        self.assertEqual(results, ['d1', 'd3', 'd2'])

        # overwrite the data for id1
        self.engine.store('id1', 'title one', 'D1', 't1')

        results = self.engine.search('tit')
        self.assertEqual(results, ['D1', 'd3', 'd2'])

        # overwrite the data with a new title, will remove the title one refs
        self.engine.store('id1', 'Herple One', 'done', 't1')

        results = self.engine.search('tit')
        self.assertEqual(results, ['d3', 'd2'])

        results = self.engine.search('her')
        self.assertEqual(results, ['done'])

        self.engine.store('id1', 'title one', 'Done', 't1', False)
        results = self.engine.search('tit')
        self.assertEqual(results, ['Done', 'd3', 'd2'])

        # this shows that when we don't clean up crap gets left around
        results = self.engine.search('her')
        self.assertEqual(results, ['Done'])

    def test_issue9_ordering(self):
        self.engine.store('aaaa bbbb')
        self.engine.store('bbbb cccc')
        self.engine.store('bbbb aaaa')
        self.engine.store('aaaa bbbb')

        results = self.engine.search('bb')
        self.assertEqual(results, ['bbbb aaaa', 'bbbb cccc', 'aaaa bbbb'])

        results = self.engine.search('aa')
        self.assertEqual(results, ['aaaa bbbb', 'bbbb aaaa'])

        self.engine.store('aabb bbbb')

        results = self.engine.search('bb')
        self.assertEqual(results, ['bbbb aaaa', 'bbbb cccc', 'aaaa bbbb', 'aabb bbbb'])

        results = self.engine.search('aa')
        self.assertEqual(results, ['aaaa bbbb', 'aabb bbbb', 'bbbb aaaa'])

        # Verify issue 9 is fixed.
        self.engine.store('foo one')
        self.engine.store('bar foo one')

        results = self.engine.search('foo')
        self.assertEqual(results, ['foo one', 'bar foo one'])

########NEW FILE########
__FILENAME__ = runtests
#!/usr/bin/env python
import sys
import unittest

from redis_completion import tests

def runtests(*test_args):
    suite = unittest.TestLoader().loadTestsFromModule(tests)
    result = unittest.TextTestRunner(verbosity=2).run(suite)
    if result.failures:
        sys.exit(1)
    elif result.errors:
        sys.exit(2)
    sys.exit(0)

if __name__ == '__main__':
    runtests(*sys.argv[1:])

########NEW FILE########
